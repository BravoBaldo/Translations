---
bibliography: efficient_ai.bib
---

# IA Efficiente {#sec-efficient_ai}

::: {.content-visible when-format="html"}
Risorse: [Slide](#sec-efficient-ai-resource), [Video](#sec-efficient-ai-resource), [Esercizi](#sec-efficient-ai-resource), [Laboratori](#sec-efficient-ai-resource)
:::

![_DALL·E 3 Prompt: Un'illustrazione concettuale che raffigura l'efficienza nell'intelligenza artificiale usando un'analogia con il cantiere navale. La scena mostra un cantiere navale in fermento dove i container rappresentano bit o byte di dati. Questi container vengono spostati in modo efficiente da gru e veicoli, a simboleggiare l'elaborazione semplificata e rapida delle informazioni nei sistemi di intelligenza artificiale. Il cantiere navale è organizzato meticolosamente, a dimostrazione del concetto di prestazioni ottimali entro i vincoli delle risorse limitate. Sullo sfondo, le navi sono attraccate, a rappresentare diverse piattaforme e scenari in cui viene applicata l'intelligenza artificiale. L'atmosfera dovrebbe trasmettere una tecnologia avanzata con la sostenibilità come tema di fondo e un'ampia applicabilità._](images/png/cover_efficient_ai.png)

L'efficienza nell'intelligenza artificiale (IA) non è semplicemente un lusso, ma una necessità. In questo capitolo, approfondiamo i concetti chiave alla base dell'efficienza dei sistemi di IA. Le richieste computazionali sulle reti neurali possono essere scoraggianti, anche per i sistemi minimali. Per integrare perfettamente l'IA nei dispositivi quotidiani e nei sistemi essenziali, deve funzionare in modo ottimale entro i vincoli delle risorse limitate, mantenendo al contempo la sua efficacia. La ricerca dell'efficienza garantisce che i modelli di IA siano semplificati, rapidi e sostenibili, ampliando così la loro applicabilità su varie piattaforme e scenari.

::: {.callout-tip}

## Obiettivi dell'Apprendimento

- Riconoscere la necessità di un'intelligenza artificiale efficiente nei dispositivi TinyML/edge.

- Comprendere la necessità di architetture di modelli efficienti come MobileNets e SqueezeNet.

- Comprendere perché le tecniche per la compressione dei modelli sono importanti.

- Apprezzare per il valore di un hardware AI efficiente.

- Riconoscere l'importanza delle rappresentazioni numeriche e della loro precisione.

- Comprendere le sfumature del confronto dei modelli oltre la semplice accuratezza.

- Riconoscere che il confronto dei modelli coinvolge memoria, elaborazione, potenza e velocità, non solo accuratezza.

- Riconoscere che l'efficienza comprende tecnologia, costi ed etica.

:::

L'attenzione è rivolta all'acquisizione di una comprensione concettuale delle motivazioni e del significato delle varie strategie per raggiungere un'intelligenza artificiale efficiente, sia in termini di tecniche che di prospettiva olistica. I capitoli successivi forniscono un'analisi più approfondita di questi molteplici concetti.

## Introduzione

I modelli di training possono consumare molta energia, a volte equivalente all'impatto ambientale di processi industriali considerevoli. Tratteremo alcuni di questi dettagli sulla sostenibilità nel capitolo [Sostenibilità dell'IA](../sustainable_ai/sustainable_ai.qmd). Dal punto di vista dell'implementazione, se questi modelli non sono ottimizzati per l'efficienza, possono esaurire rapidamente le batterie dei dispositivi, richiedere una memoria eccessiva o non soddisfare le esigenze di elaborazione in tempo reale. In questo capitolo, miriamo a chiarire le sfumature dell'efficienza, gettando le basi per un'esplorazione completa nei capitoli successivi.

## La Necessità di un'IA Efficiente

L'efficienza assume connotazioni diverse a seconda di dove si verificano i calcoli dell'IA. Rivediamo Cloud, Edge e TinyML (come discusso in [Sistemi di ML](../ml_systems/ml_systems.qmd)) e distinguiamoli in termini di efficienza. @fig-platforms fornisce un confronto generale delle tre diverse piattaforme.

![Cloud, Mobile e TinyML. Fonte: @schizas2022tinyml.](https://www.mdpi.com/futureinternet/futureinternet-14-00363/article_deploy/html/images/futureinternet-14-00363-g001-550.jpg){#fig-platforms}

**IA Cloud:** I modelli IA tradizionali vengono spesso eseguiti in data center su larga scala dotati di potenti GPU e TPU [@barroso2019datacenter]. Qui, l'efficienza riguarda l'ottimizzazione delle risorse di calcolo, la riduzione dei costi e la garanzia di elaborazione e restituzione tempestive dei dati. Tuttavia, fare affidamento sul cloud introduce latenza, soprattutto quando si ha a che fare con flussi di dati di grandi dimensioni che richiedono caricamento, elaborazione e download.

**IA Edge:** L'edge computing avvicina l'intelligenza artificiale alla fonte dei dati, elaborando le informazioni direttamente su dispositivi locali come smartphone, fotocamere o macchine industriali [@li2019edge]. Qui, l'efficienza comprende risposte rapide in tempo reale e ridotte esigenze di trasmissione dei dati. Tuttavia, i vincoli sono più severi: questi dispositivi, sebbene più potenti dei microcontrollori, hanno una potenza di calcolo limitata rispetto alle configurazioni cloud.

**TinyML:** TinyML supera i limiti consentendo ai modelli di intelligenza artificiale di funzionare su microcontrollori o ambienti con risorse estremamente limitate. La differenza di prestazioni del processore e della memoria tra TinyML e i sistemi cloud o mobili può essere di diversi ordini di grandezza [@warden2019tinyml]. L'efficienza in TinyML consiste nell'assicurare che i modelli siano sufficientemente leggeri da adattarsi a questi dispositivi, consumino il minimo di energia (fondamentale per i dispositivi alimentati a batteria) e continuino a svolgere le loro attività in modo efficace.

Lo spettro da Cloud a TinyML rappresenta un passaggio da vaste risorse di elaborazione centralizzate ad ambienti distribuiti, localizzati e limitati. Passando dall'uno all'altro, i problemi e le strategie relative all'efficienza evolvono, sottolineando la necessità di approcci specializzati su misura per ogni scenario. Dopo aver stabilito la necessità di un'intelligenza artificiale efficiente, in particolare nel contesto di TinyML, passeremo all'esplorazione delle metodologie ideate per rispondere a queste sfide. Le sezioni seguenti delineano i concetti principali che approfondiremo in seguito. Dimostreremo l'ampiezza e la profondità dell'innovazione necessarie per ottenere un'intelligenza artificiale efficiente mentre esploriamo queste strategie.

## Architetture di Modelli Efficienti

Selezionare un'architettura del modello ottimale è tanto cruciale quanto ottimizzarla. Negli ultimi anni, i ricercatori hanno compiuto passi da gigante nell'esplorazione di architetture innovative che possono avere intrinsecamente meno parametri pur mantenendo prestazioni elevate.

**MobileNet:** MobileNet sono modelli di applicazioni di visione mobile ed embedded efficienti [@howard2017mobilenets]. L'idea chiave che ha portato al loro successo sono le convoluzioni separabili in profondità, che riducono significativamente il numero di parametri e calcoli nella rete. MobileNetV2 e V3 migliorano ulteriormente questo design introducendo residui invertiti e colli di bottiglia lineari.

**SqueezeNet:** SqueezeNet è una classe di modelli ML noti per le sue dimensioni ridotte senza sacrificare la precisione. Ciò si ottiene utilizzando un "modulo fire" che riduce il numero di canali di input a filtri 3x3, riducendo così i parametri [@iandola2016squeezenet]. Inoltre, impiega il downsampling [sottocampionamento] ritardato per aumentare la precisione mantenendo una mappa delle feature più ampia.

**Varianti ResNet:** L'architettura Residual Network (ResNet) consente l'introduzione di connessioni skip o scorciatoie [@he2016deep]. Alcune varianti di ResNet sono progettate per essere più efficienti. Ad esempio, ResNet-SE incorpora il meccanismo "squeeze and excitation" per ricalibrare le feature map [@hu2018squeeze], mentre ResNeXt offre convoluzioni raggruppate per l'efficienza [@xie2017aggregated].

## Compressione Efficiente del Modello {#sec-efficient-model-compression}

I metodi di compressione dei modelli sono essenziali per portare i modelli di apprendimento profondo su dispositivi con risorse limitate. Queste tecniche riducono le dimensioni dei modelli, il consumo energetico e le richieste di elaborazione senza perdere significativamente la precisione. Ad alto livello, i metodi possono essere categorizzati nei seguenti metodi fondamentali:

**Pruning:** L'Abbiamo menzionato un paio di volte nei capitoli precedenti, ma non l'abbiamo ancora formalmente introdotta. Il pruning è simile alla potatura dei rami di un albero. Questo è stato pensato per la prima volta nel documento [Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf) [@lecun1989optimal] ed è stato successivamente reso popolare nel contesto del deep learning da @han2016deep. Determinati pesi o interi neuroni vengono rimossi dalla rete nella potatura in base a criteri specifici. Questo può ridurre significativamente le dimensioni del modello. In @sec-pruning esploreremo due delle principali strategie di potatura, quella strutturata e quella non-strutturata. @fig-pruning è un esempio di potatura della rete neurale, in cui la rimozione di alcuni nodi negli strati interni (in base a criteri specifici) riduce il numero di rami tra i nodi e, a sua volta, le dimensioni del modello.

![Neural Network Pruning.](images/jpg/pruning.jpeg){#fig-pruning}

**Quantizzazione:** La quantizzazione è il processo di limitazione di un input da un set ampio a un output in un set più piccolo, principalmente nel deep learning; ciò significa ridurre il numero di bit che rappresentano i pesi e i bias del modello. Ad esempio, l'utilizzo di rappresentazioni a 16 o 8 bit anziché a 32 bit può ridurre la dimensione del modello e velocizzare i calcoli, con un piccolo compromesso in termini di accuratezza. Esploreremo questi aspetti più in dettaglio in @sec-quant. @fig-quantization mostra un esempio di quantizzazione mediante arrotondamento al numero più vicino. La conversione da virgola mobile a 32 bit a 16 bit riduce l'utilizzo della memoria del 50%. Passare da un intero a 32 bit a uno a 8 bit riduce l'utilizzo della memoria del 75%. Mentre la perdita di precisione numerica e, di conseguenza, di prestazioni del modello è minima, l'efficienza nell'utilizzo della memoria è significativa.

![Diverse forme di quantizzazione.](images/jpg/quantization.jpeg){#fig-quantization}

**Knowledge Distillation:** La "distillazione della conoscenza" comporta l'addestramento di un modello più piccolo (studente) per replicare il comportamento di un modello più grande (insegnante). L'idea è quella di trasferire la conoscenza dal modello ingombrante a quello leggero. Quindi, il modello più piccolo raggiunge prestazioni vicine alla sua controparte più grande ma con parametri significativamente inferiori. Esploreremo la "distillazione della conoscenza" in modo più dettagliato in @sec-kd.

## Hardware di Inferenza Efficiente

Nel capitolo [Training](../training/training.qmd), abbiamo discusso il processo di training dei modelli di intelligenza artificiale. Ora, dal punto di vista dell'efficienza, è importante notare che il training è un'attività che richiede molte risorse e molto tempo, spesso richiede hardware potente e impiega da ore a settimane per essere completato. L'inferenza, d'altra parte, deve essere il più veloce possibile, soprattutto nelle applicazioni in tempo reale. È qui che entra in gioco un hardware di inferenza efficiente. Ottimizzando l'hardware specificamente per le attività di inferenza, possiamo ottenere tempi di risposta rapidi e un funzionamento efficiente dal punto di vista energetico, il che è particolarmente cruciale per i dispositivi edge e i sistemi embedded.

**TPU (Tensor Processing Unit):** Le [TPU](https://cloud.google.com/tpu) sono ASIC (Application-Specific Integrated Circuits) personalizzati da Google per accelerare i carichi di lavoro di apprendimento automatico [@jouppi2017datacenter]. Sono ottimizzate per le operazioni tensoriali, offrono un throughput elevato per l'aritmetica a bassa precisione e sono progettate specificamente per il machine learning delle reti neurali. Le TPU accelerano significativamente l'addestramento e l'inferenza del modello rispetto alle GPU/CPU generiche. Questo potenziamento si traduce in un addestramento più rapido dei modelli e in capacità di inferenza in tempo reale o quasi reale, fondamentali per applicazioni come la ricerca vocale e la realtà aumentata.

Le [Edge TPU](https://cloud.google.com/edge-tpu) sono una versione più piccola e a basso consumo delle TPU di Google, studiate appositamente per i dispositivi edge. Forniscono un'inferenza ML veloce sul dispositivo per i modelli TensorFlow Lite. Le Edge TPU consentono un'inferenza a bassa latenza e ad alta efficienza su dispositivi edge come smartphone, dispositivi IoT e sistemi embedded. Le capacità di IA possono essere implementate in applicazioni in tempo reale senza comunicare con un server centrale, risparmiando così larghezza di banda e riducendo la latenza. Si consideri la tabella in @fig-edge-tpu-perf. Mostra le differenze di prestazioni tra l'esecuzione di modelli diversi su CPU rispetto a un acceleratore Coral USB. L'acceleratore Coral USB è un accessorio della piattaforma Coral AI di Google che consente agli sviluppatori di collegare le Edge TPU ai computer Linux. L'esecuzione dell'inferenza sulle Edge TPU è stata da 70 a 100 volte più veloce rispetto alle CPU.

![Confronto delle prestazioni tra acceleratore e CPU in diverse configurazioni hardware. Desktop CPU: 64-bit Intel(R) Xeon(R) E5–1650 v4 @ 3.60GHz. Embedded CPU: Quad-core Cortex-A53 @ 1.5GHz, †Dev Board: Quad-core Cortex-A53 @ 1.5GHz + Edge TPU. Fonte: [TensorFlow Blog.](https://blog.tensorflow.org/2019/03/build-ai-that-works-offline-with-coral.html)](images/png/tflite_edge_tpu_perf.png){#fig-edge-tpu-perf}



**Acceleratori NN (Neural Network):** Gli acceleratori di reti neurali a funzione fissa sono acceleratori hardware progettati esplicitamente per i calcoli di reti neurali. Possono essere chip standalone o far parte di una soluzione di system-on-chip (SoC) più ampia. Ottimizzando l'hardware per le operazioni specifiche richieste dalle reti neurali, come moltiplicazioni di matrici e convoluzioni, gli acceleratori NN possono ottenere tempi di inferenza più rapidi e consumi energetici inferiori rispetto alle CPU e alle GPU per uso generico. Sono particolarmente utili nei dispositivi TinyML con vincoli di potenza o termici, come smartwatch, micro-droni o robotica.

Ma questi sono solo gli esempi più comuni. Stanno emergendo diversi altri tipi di hardware che hanno il potenziale per offrire vantaggi significativi per l'inferenza. Questi includono, ma non solo, hardware neuromorfico, elaborazione fotonica, ecc. In [@sec-aihw], esploreremo questi aspetti in modo più dettagliato.

Un hardware efficiente per l'inferenza velocizza il processo, risparmia energia, prolunga la durata della batteria e può funzionare in condizioni di tempo reale. Man mano che l'intelligenza artificiale viene integrata in innumerevoli applicazioni, dalle telecamere intelligenti agli assistenti vocali, il ruolo dell'hardware ottimizzato diventerà sempre più importante. Sfruttando questi componenti hardware specializzati, sviluppatori e ingegneri possono portare la potenza dell'intelligenza artificiale a dispositivi e situazioni che prima erano impensabili.

## Matematica Efficiente {#sec-efficient-numerics}

L'apprendimento automatico, e in particolare il deep learning, comporta enormi quantità di elaborazione. I modelli possono avere milioni o miliardi di parametri, spesso addestrati su vasti set di dati. Ogni operazione, ogni moltiplicazione o addizione, richiede risorse di elaborazione. Pertanto, la precisione dei numeri utilizzati in queste operazioni può avere un impatto significativo sulla velocità di elaborazione, sul consumo di energia e sui requisiti di memoria. È qui che entra in gioco il concetto di numeri efficienti.

### Formati Numerici {#sec-numerical-formats}

Esistono molti tipi diversi di numeri. I numeri hanno una lunga storia nei sistemi di elaborazione.

**Floating point:** Noto come "virgola mobile" a precisione singola, FP32 utilizza 32 bit per rappresentare un numero, incorporandone segno, esponente e mantissa. Comprendere come i numeri in virgola mobile sono rappresentati in modo approfondito è fondamentale per comprendere le varie ottimizzazioni possibili nei calcoli numerici. Il bit del segno determina se il numero è positivo o negativo, l'esponente controlla l'intervallo di valori che possono essere rappresentati e la mantissa determina la precisione del numero. La combinazione di questi componenti consente ai numeri in virgola mobile di rappresentare un'ampia gamma di valori con vari gradi di precisione.

@vid-floating-point-numbers fornisce una panoramica completa di questi tre componenti principali, segno, esponente e mantissa, e di come funzionano insieme per rappresentare i numeri in virgola mobile.

:::{#vid-floating-point-numbers .callout-important}

# Numeri in Virgola Mobile

{{< video https://youtu.be/gc1Nl3mmCuY?si=nImcymfbE5H392vu >}}

:::


FP32 è ampiamente adottato in molti framework di deep learning e bilancia accuratezza e requisiti computazionali. È prevalente nella fase di training per molte reti neurali grazie alla sua sufficiente precisione nel catturare dettagli minuti durante gli aggiornamenti dei pesi.
Noto anche come virgola mobile a mezza precisione, FP16 utilizza 16 bit per rappresentare un numero, inclusi il segno, l'esponente e la frazione. Offre un buon equilibrio tra precisione e risparmio di memoria. FP16 è particolarmente popolare nella training di deep learning su GPU che supportano l'aritmetica a precisione mista, combinando i vantaggi di velocità di FP16 con la precisione di FP32 quando necessario.

Diversi altri formati numerici rientrano in una classe esotica. Un esempio esotico è BF16 o Brain Floating Point. È un formato numerico a 16 bit progettato esplicitamente per applicazioni di deep learning. È un compromesso tra FP32 e FP16, che mantiene l'esponente a 8 bit di FP32 riducendo la mantissa a 7 bit (rispetto alla mantissa a 23 bit di FP32). Questa struttura dà priorità al range rispetto alla precisione. BF16 ha ottenuto risultati di training paragonabili in accuratezza a FP32, utilizzando significativamente meno memoria e risorse computazionali [@kalamkar2019study]. Ciò lo rende adatto non solo per l'inferenza, ma anche per il training di reti neurali profonde.

Mantenendo l'esponente a 8 bit di FP32, BF16 offre un range simile, che è fondamentale per le attività di deep learning in cui determinate operazioni possono generare numeri molto grandi o molto piccoli. Allo stesso tempo, troncando la precisione, BF16 consente requisiti di memoria e computazionali ridotti rispetto a FP32. BF16 è emerso come una promettente via di mezzo nel panorama dei formati numerici per il deep learning, fornendo un'alternativa efficiente ed efficace ai formati FP32 e FP16 più tradizionali.

@fig-float-point-formats mostra tre diversi formati in virgola mobile: Float32, Float16 e BFloat16.

![Tre formati a virgola mobile.](images/png/three_float_types.png){#fig-float-point-formats width=90%}

**Intero:** Si tratta di rappresentazioni di numeri interi che utilizzano 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocità e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attività di inferenza, in particolare sui dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione è spesso accettabile, dati i guadagni di efficienza. Una forma estrema di numeri interi è per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno dei due valori: +1 o -1.

**Larghezze di bit variabili:** Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezze di bit estremamente basse possono offrire accelerazioni significative e ridurre ulteriormente il consumo di energia. Sebbene permangano dei problemi nel mantenere l'accuratezza del modello con una quantizzazione così drastica, si continuano a fare progressi in quest'area.

L'efficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano più pervasivi, soprattutto in ambienti reali con risorse limitate, l'attenzione su una numerica efficiente continuerà a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, è possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocità, memoria ed energia. @tbl-precision riassume questi compromessi.

+----------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+
| Precisione           | Pro                                                                                            | Contro                                                                                                            |
+:=====================+:===============================================================================================+:==================================================================================================================+
| FP32 (virgola mobile | * Precisione standard utilizzata nella maggior parte dei framework di deep learning.           | * Elevato utilizzo di memoria.                                                                                    |
| a 32 bit)            | * Elevata accuratezza grazie all'ampia capacità di rappresentazione.                           | * Tempi di inferenza più lenti rispetto ai modelli quantizzati.                                                   |
|                      | * Adatto per il training                                                                       | * Maggiore consumo energetico.                                                                                    |
+----------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+
| FP16 (virgola mobile | * Riduce l'utilizzo di memoria rispetto a FP32.                                                | * Minore capacità di rappresentazione rispetto a FP32.                                                            |
| a 16 bit)            | * Velocizza i calcoli su hardware che supporta FP16.                                           | * Rischio di instabilità numerica in alcuni modelli o livelli.                                                    |
|                      | * Spesso utilizzato nel training a precisione mista per bilanciare velocità                    |                                                                                                                   |
|                      |   e accuratezza.                                                                               |                                                                                                                   |
+----------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+
| INT8 (intero         | * Impronta di memoria notevolmente ridotta rispetto alle rappresentazioni                      | * La quantizzazione può comportare una certa perdita di accuratezza.                                              |
| a 8 bit)             |   in virgola mobile.                                                                           | * Richiede una calibrazione attenta durante la quantizzazione per                                                 |
|                      | * Inferenza più rapida se l'hardware supporta i calcoli INT8.                                  |   ridurre al minimo il degrado della precisione.                                                                  |
|                      | * Adatto a molti scenari di quantizzazione post-training.                                      |                                                                                                                   |
+----------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+
| INT4 (intero         | * Utilizzo di memoria ancora inferiore rispetto a INT8.                                        | * Rischio di perdita di precisione più elevato rispetto a INT8.                                                   |
| a 4 bit)             | * Ulteriore potenziale di accelerazione per l'inferenza.                                       | * La calibrazione durante la quantizzazione diventa più critica.                                                  |
+----------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+
| Binario              | * Ingombro di memoria minimo (solo 1 bit per parametro).                                       | * Calo significativo della precisione per molte attività.                                                         |
|                      | * Inferenza estremamente rapida grazie alle operazioni bit a bit.                              | * Dinamiche di training complesse grazie alla quantizzazione estrema.                                             |
|                      | * Efficienza energetica.                                                                       |                                                                                                                   |
+----------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+
| Ternario             | * Basso utilizzo di memoria ma leggermente superiore a quello binario.                         | * L'accuratezza potrebbe essere ancora inferiore a quella dei                                                     |
|                      | * Offre una via di mezzo tra rappresentazione ed efficienza.                                   |   modelli di precisione più elevata.                                                                              |
|                      |                                                                                                | * Le dinamiche di addestramento possono essere complesse.                                                         |
+----------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+

: Confronto dei livelli di precisione nel deep learning. {#tbl-precision .striped .hover}

### Vantaggi dell'Efficienza {#sec-efficiency-benefits}

L'efficienza numerica è importante per i carichi di lavoro di machine learning per diversi motivi:

**Efficienza Computazionale:** I calcoli ad alta precisione (come FP32 o FP64) possono essere lenti e richiedere molte risorse. Ridurre la precisione numerica può ottenere tempi di calcolo più rapidi, specialmente su hardware specializzato che supporta una precisione inferiore.

**Efficienza della Memoria:** I requisiti di archiviazione diminuiscono con una precisione numerica ridotta. Ad esempio, FP16 richiede metà della memoria di FP32. Ciò è fondamentale quando si distribuiscono modelli su dispositivi edge con memoria limitata o si lavora con modelli di grandi dimensioni.

**Efficienza Energetica:** I calcoli a precisione inferiore spesso consumano meno energia, il che è particolarmente importante per i dispositivi alimentati a batteria.

**Introduzione del Rumore:** È interessante notare che il rumore introdotto utilizzando una precisione inferiore può talvolta fungere da regolarizzatore, contribuendo a prevenire l'overfitting in alcuni modelli.

**Accelerazione Hardware:** Molti acceleratori di IA e GPU moderni sono ottimizzati per operazioni di precisione inferiore, sfruttando i vantaggi dell'efficienza di tali numeri.

## Valutazione dei Modelli

Vale la pena notare che i vantaggi e i compromessi effettivi possono variare in base all'architettura specifica della rete neurale, al set di dati, all'attività e all'hardware utilizzato. Prima di decidere una precisione numerica, è consigliabile eseguire esperimenti per valutare l'impatto sull'applicazione desiderata.

### Metriche di Efficienza

Una profonda comprensione dei metodi di valutazione dei modelli è importante per guidare questo processo in modo sistematico. Quando si valuta l'efficacia e l'idoneità dei modelli di intelligenza artificiale per varie applicazioni, le metriche di efficienza vengono in primo piano.

I **FLOP (Floating Point Operations)**, introdotti in [Training](../training/training.html), misurano le esigenze computazionali di un modello. Ad esempio, una moderna rete neurale come BERT ha miliardi di FLOP, che potrebbero essere gestibili su un potente server cloud ma sarebbero gravosi su uno smartphone. FLOP più elevati possono portare a tempi di inferenza più prolungati e a un notevole consumo di energia, soprattutto su dispositivi senza acceleratori hardware specializzati. Quindi, per applicazioni in tempo reale come lo streaming video o i giochi, potrebbero essere più desiderabili modelli con FLOP più bassi.

L'**Utilizzo della Memoria** riguarda la quantità di spazio di archiviazione richiesta dal modello, che influisce sia sullo spazio di archiviazione del dispositivo che sulla RAM. Si prenda in considerazione l'implementazione di un modello su uno smartphone: un modello che occupa diversi gigabyte di spazio non solo consuma prezioso spazio di archiviazione, ma potrebbe anche essere più lento a causa della necessità di caricare grandi pesi nella memoria. Ciò diventa particolarmente cruciale per dispositivi edge come telecamere di sicurezza o droni, dove impronte di memoria minime sono vitali per l'archiviazione e l'elaborazione rapida dei dati.

Il **Consumo Energetico** diventa particolarmente cruciale per i dispositivi che si basano sulle batterie. Ad esempio, un monitor sanitario indossabile che utilizza un modello ad alto consumo energetico potrebbe esaurire la batteria in poche ore, rendendolo poco pratico per il monitoraggio continuo. L'ottimizzazione dei modelli per un basso consumo energetico diventa essenziale mentre ci muoviamo verso un'era dominata dai dispositivi IoT, dove molti dispositivi funzionano a batteria.

Il **Tempo di Inferenza** riguarda la rapidità con cui un modello può produrre risultati. In applicazioni come la guida autonoma, dove decisioni in frazioni di secondo fanno la differenza tra sicurezza e calamità, i modelli devono funzionare rapidamente. Se il modello di un'auto a guida autonoma impiega anche solo pochi secondi in più per riconoscere un ostacolo, le conseguenze potrebbero essere disastrose. Quindi, garantire che il tempo di inferenza di un modello sia allineato con le richieste in tempo reale della sua applicazione è fondamentale.

In sostanza, queste metriche di efficienza sono più che dei numeri che stabiliscono dove e come un modello può essere distribuito in modo efficace. Un modello potrebbe vantare un'elevata accuratezza, ma se i suoi FLOP, l'utilizzo della memoria, il consumo energetico o il tempo di inferenza lo rendono inadatto alla piattaforma prevista o agli scenari del mondo reale, la sua utilità pratica diventa limitata.

### Confronti di Efficienza

Il panorama dei modelli di machine learning è vasto, con ogni modello che offre un set unico di punti di forza e considerazioni di implementazione. Sebbene le cifre di accuratezza grezza o le velocità di training e inferenza possano essere parametri di riferimento allettanti, forniscono un quadro incompleto. Un'analisi comparativa più approfondita rivela diversi fattori critici che influenzano l'idoneità di un modello per le applicazioni TinyML.
Spesso, incontriamo il delicato equilibrio tra accuratezza ed efficienza. Ad esempio, mentre un modello di deep learning e denso e una variante MobileNet leggera potrebbero eccellere nella classificazione delle immagini, le loro richieste di calcolo potrebbero essere ad estremi opposti. Questa differenziazione è particolarmente pronunciata quando si confrontano le distribuzioni su server cloud con risorse abbondanti rispetto ai limitati dispositivi TinyML. In molti scenari del mondo reale, i guadagni marginali in termini di accuratezza potrebbero essere oscurati dalle inefficienze di un modello ad alta intensità di risorse richieste.

Inoltre, la scelta del modello ottimale non è sempre universale, ma spesso dipende dalle specifiche di un'applicazione. Ad esempio, un modello che eccelle in scenari di rilevamento di oggetti generali potrebbe avere difficoltà in ambienti di nicchia, come il rilevamento di difetti di fabbricazione in una fabbrica. Questa adattabilità, o la sua mancanza, può influenzare l'utilità reale di un modello.

Un'altra considerazione importante è la relazione tra la complessità del modello e i suoi vantaggi pratici. Prendiamo gli assistenti attivati tramite comando vocale, come "Alexa" o "OK Google". Mentre un modello complesso potrebbe dimostrare una comprensione marginalmente superiore del parlato dell'utente se è più lento a rispondere rispetto a una controparte più semplice, l'esperienza utente potrebbe essere compromessa. Pertanto, l'aggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.

Un'altra considerazione importante è la relazione tra la complessità del modello e i suoi vantaggi pratici. Prendiamo gli assistenti vocali come "Alexa" o "OK Google". Mentre un modello complesso potrebbe dimostrare una comprensione leggermente superiore del parlato dell'utente se è più lento a rispondere rispetto a una controparte più semplice, l'esperienza utente potrebbe essere compromessa. Pertanto, l'aggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.

Inoltre, mentre i set di dati di riferimento, come ImageNet [@russakovsky2015imagenet], COCO [@lin2014microsoft], Visual Wake Words [@chowdhery2019visual], Google Speech Commands [@warden2018speech], ecc. forniscono una metrica di prestazioni standardizzata, potrebbero non catturare la diversità e l'imprevedibilità dei dati del mondo reale. Due modelli di riconoscimento facciale con punteggi di riferimento simili potrebbero mostrare competenze diverse quando si trovano di fronte a background etnici diversi o condizioni di illuminazione difficili. Tali disparità sottolineano l'importanza di robustezza e coerenza tra dati diversi. Ad esempio, @fig-stoves dal set di dati Dollar Street mostra immagini di stufe su redditi mensili estremi. Le stufe hanno forme e livelli tecnologici diversi in diverse regioni e livelli di reddito. Un modello che non è addestrato su set di dati diversi potrebbe funzionare bene su un benchmark ma fallire nelle applicazioni del mondo reale. Quindi, se un modello fosse addestrato solo su immagini di stufe trovate nei paesi ricchi, non riuscirebbe a riconoscere le stufe delle regioni più povere.

![Diversi tipi di stufe. Fonte: Immagini di stufe di Dollar Street.](images/jpg/ds_stoves.jpg){#fig-stoves}

In sostanza, un'analisi comparativa approfondita trascende le metriche numeriche. È una valutazione olistica intrecciata con applicazioni del mondo reale, costi e le intricate sottigliezze che ogni modello porta con sé. Ecco perché avere parametri di riferimento e metriche standard ampiamente stabiliti e adottati dalla comunità diventa importante.

## Conclusione

L'intelligenza artificiale efficiente è fondamentale mentre ci spingiamo verso un'implementazione più ampia e diversificata del machine learning nel mondo reale. Questo capitolo ha fornito una panoramica, esplorando le varie metodologie e considerazioni alla base del raggiungimento di un'intelligenza artificiale efficiente, a partire dall'esigenza fondamentale, dalle somiglianze e dalle differenze tra i sistemi cloud, Edge e TinyML.

Abbiamo esaminato le architetture dei modelli efficienti e la loro utilità per l'ottimizzazione. Le tecniche di compressione dei modelli come pruning, quantizzazione e distillazione della conoscenza esistono per aiutare a ridurre le richieste di calcolo e l'ingombro della memoria senza influire in modo significativo sulla precisione. Hardware specializzati come TPU e acceleratori NN offrono chip ottimizzati per le operazioni di rete neurale e il flusso di dati. I numeri efficienti bilanciano precisione ed efficienza, consentendo ai modelli di ottenere prestazioni robuste utilizzando risorse minime. Esploreremo questi argomenti in modo approfondito e dettagliato nei capitoli successivi.

Insieme, questi formano un quadro olistico per un'intelligenza artificiale efficiente. Ma il viaggio non finisce qui. Il raggiungimento di un'intelligenza efficiente in modo ottimale richiede ricerca e innovazione continue. Man mano che i modelli diventano più sofisticati, i set di dati crescono e le applicazioni si diversificano in domini specializzati, l'efficienza deve evolversi di pari passo. La misura dell'impatto nel mondo reale richiede parametri di riferimento adatti e metriche standardizzate che vadano oltre le semplicistiche cifre dell'accuratezza.

Inoltre, l'intelligenza artificiale efficiente si espande oltre l'ottimizzazione tecnologica e comprende costi, impatto ambientale e considerazioni etiche per il bene della società in senso più ampio. Man mano che l'intelligenza artificiale permea i settori e la vita quotidiana, una prospettiva completa sull'efficienza sostiene il suo progresso sostenibile e responsabile. I capitoli successivi si baseranno su questi concetti fondamentali, fornendo approfondimenti concreti e norme pratiche per lo sviluppo e l'implementazione di soluzioni di intelligenza artificiale efficienti.

## Risorse {#sec-efficient-ai-resource}

Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.

:::{.callout-note collapse="false"}

# Slide

Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.

- [Deploying on Edge Devices: challenges and techniques.](https://docs.google.com/presentation/d/1tvSiOfQ1lYPXsvHcFVs8R1lYZPei_Nb7/edit?usp=drive_link&ouid=102419556060649178683&rtpof=true&sd=true)

- [Model Evaluation.](https://docs.google.com/presentation/d/1jdBnIxgNovG3b8frTl3DwqiIOw_K4jvp3kyv2GoKfYQ/edit?usp=drive_link&resourcekey=0-PN8sYpltO1nP_xePynJn9w)

- [Continuous Evaluation Challenges for TinyML.](https://docs.google.com/presentation/d/1OuhwH5feIwPivEU6pTDyR3QMs7AFstHLiF_LB8T5qYQ/edit?usp=drive_link&resourcekey=0-DZxIuVBUbJawuFh0AO-Pvw)
:::

:::{.callout-important collapse="false"}

#### Video

- _Prossimamente._
:::

:::{.callout-caution collapse="false"}

#### Esercizi

Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.

- _Prossimamente._
:::

:::{.callout-warning collapse="false"}

#### Laboratori

Oltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l'esperienza di apprendimento.

- _Prossimamente._
:::

