---
bibliography: privacy_security.bib
---

# Sicurezza e Privacy {#sec-security_privacy}

::: {.content-visible when-format="html"}
Risorse: [Slide](#sec-security-and-privacy-resource),  [Video](#sec-security-and-privacy-resource), [Esercizi](#sec-security-and-privacy-resource), [Laboratori](#sec-security-and-privacy-resource)
:::

![_DALL·E 3 Prompt: Un'illustrazione sulla privacy e la sicurezza nei sistemi di apprendimento automatico. L'immagine mostra un paesaggio digitale con una rete di nodi interconnessi e flussi di dati, che simboleggiano gli algoritmi di apprendimento automatico. In primo piano, c'è un grande lucchetto sovrapposto alla rete, che rappresenta la privacy e la sicurezza. Il lucchetto è semi-trasparente, consentendo alla rete sottostante di essere parzialmente visibile. Lo sfondo presenta codice binario e simboli di crittografia digitale, che enfatizzano il tema della sicurezza informatica. La combinazione di colori è un mix di blu, verdi e grigi, che suggerisce un ambiente digitale ad alta tecnologia._](images/png/cover_security_privacy.png)

Sicurezza e privacy sono fondamentali quando si sviluppano sistemi di apprendimento automatico nel mondo reale. Poiché l'apprendimento automatico viene sempre più applicato a domini sensibili come sanità, finanza e dati personali, proteggere la riservatezza e prevenire l'uso improprio di dati e modelli diventa imperativo. Chiunque intenda creare sistemi di apprendimento automatico solidi e responsabili deve comprendere i potenziali rischi per la sicurezza e la privacy, come perdite di dati, furto di modelli, attacchi avversari, bias [pregiudizi] e accesso involontario a informazioni private. Dobbiamo anche comprendere le best practice per mitigare questi rischi. Ancora più importante, sicurezza e privacy non possono essere un ripensamento e devono essere affrontate in modo proattivo durante tutto il ciclo di vita dello sviluppo del sistema di apprendimento automatico, dalla raccolta e dall'etichettatura dei dati al training, valutazione e deployment [distribuzione] del modello. Incorporare considerazioni sulla sicurezza e sulla privacy in ogni fase di creazione, distribuzione e gestione dei sistemi di apprendimento automatico è essenziale per sbloccare in modo sicuro i vantaggi dell'intelligenza artificiale.

::: {.callout-tip}

## Obiettivi dell'Apprendimento

* Comprendere i principali rischi per la privacy e la sicurezza del ML, come perdite di dati, furto di modelli, attacchi avversari, pregiudizi e accesso involontario ai dati.

* Imparare dagli incidenti storici di sicurezza di sistemi hardware ed embedded.

* Identificare le minacce ai modelli ML come avvelenamento dei dati, estrazione di modelli, inferenza di appartenenza ed esempi avversari.

* Riconoscere le minacce alla sicurezza hardware per il ML embedded che abbracciano bug hardware, attacchi fisici, canali laterali, componenti contraffatti, ecc.

* Esplorare le difese ML embedded, come ambienti di esecuzione affidabili, avvio sicuro, funzioni fisiche non clonabili e moduli di sicurezza hardware.

* Discutere i problemi di privacy nella gestione di dati utente sensibili con ML embedded, comprese le normative.

* Apprendere tecniche ML che preservano la privacy come la privacy differenziale, apprendimento federato, crittografia omomorfica e generazione di dati sintetici.

* Comprendere i compromessi tra privacy, accuratezza, efficienza, modelli di minaccia e ipotesi di fiducia.

* Riconoscere la necessità di una prospettiva multilivello che abbracci progettazione elettrica, firmware, software e fisica quando si proteggono dispositivi ML embedded.

:::

## Introduzione

Il "Machine learning " [apprendimento automatico] si è evoluto notevolmente dalle sue origini accademiche, in cui la privacy non era una preoccupazione primaria. Con la migrazione del ML in applicazioni commerciali e consumer, i dati sono diventati più sensibili, comprendendo informazioni personali come comunicazioni, acquisti e dati sanitari. Questa esplosione di disponibilità di dati ha alimentato rapidi progressi nelle capacità del ML. Tuttavia, ha anche esposto nuovi rischi per la privacy, come dimostrato da incidenti come la [fuga di dati di AOL nel 2006](https://en.wikipedia.org/wiki/AOL_search_log_release) e lo scandalo [Cambridge Analytica](https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html).

Questi eventi hanno evidenziato la crescente necessità di affrontare la privacy nei sistemi ML. In questo capitolo, esploriamo insieme considerazioni sulla privacy e sulla sicurezza, poiché sono intrinsecamente collegate nel ML:

* La privacy si riferisce al controllo dell'accesso ai dati sensibili degli utenti, come informazioni finanziarie o dati biometrici raccolti da un'applicazione ML.

* La sicurezza protegge i sistemi e i dati ML da hacking, furto e uso improprio.

Ad esempio, una telecamera di sicurezza domestica basata su ML deve proteggere i flussi video da accessi non autorizzati e fornire protezioni della privacy per garantire che solo gli utenti previsti possano visualizzare il filmato. Una violazione della sicurezza o della privacy potrebbe esporre momenti privati degli utenti.

I sistemi ML embedded come assistenti intelligenti e dispositivi indossabili sono onnipresenti ed elaborano dati intimi degli utenti. Tuttavia, i loro vincoli computazionali spesso impediscono protocolli di sicurezza pesanti. I progettisti devono bilanciare le esigenze di prestazioni con rigorosi standard di sicurezza e privacy adattati alle limitazioni dell'hardware embedded.

Questo capitolo fornisce conoscenze essenziali per affrontare il complesso panorama di privacy e sicurezza dell'ML embedded. Esploreremo le vulnerabilità e tratteremo varie tecniche che migliorano la privacy e la sicurezza all'interno dei vincoli di risorse dei sistemi embedded.

Ci auguriamo che sviluppando una comprensione olistica dei rischi e delle misure di sicurezza, si acquisiranno i principi per sviluppare applicazioni ML embedded sicure ed etiche.

## Terminologia

In questo capitolo parleremo insieme di sicurezza e privacy, quindi ci sono termini chiave su cui dobbiamo essere chiari.

* **Privacy:** Si consideri una telecamera di sicurezza domestica basata su ML che identifica e registra potenziali minacce. Questa telecamera registra informazioni identificabili di individui che si avvicinano e potenzialmente entrano in questa casa, compresi i volti. Le preoccupazioni sulla privacy potrebbero riguardare chi può accedere a questi dati.

* **Sicurezza:** Si consideri una telecamera di sicurezza domestica basata su ML che identifica e registra potenziali minacce. L'aspetto della sicurezza garantirebbe che gli hacker non possano accedere a questi video e ai modelli di riconoscimento.

* **Minaccia:** Utilizzando il nostro esempio della telecamera di sicurezza domestica, una minaccia potrebbe essere un hacker che tenta di accedere a video live o archiviati o che utilizza falsi input per ingannare il sistema.

* **Vulnerabilità:** Una vulnerabilità comune potrebbe essere una rete scarsamente protetta tramite la quale la telecamera si connette a Internet, che potrebbe essere sfruttata per accedere ai dati.

## Precedenti Storici

Sebbene le specifiche della sicurezza hardware dell'apprendimento automatico possano essere distinte, il campo dei sistemi embedded ha una storia di incidenti di sicurezza che forniscono lezioni fondamentali per tutti i sistemi connessi, compresi quelli che utilizzano ML. Ecco analisi dettagliate di violazioni passate:

### Stuxnet

Nel 2010, qualcosa di inaspettato è stato trovato su un computer in Iran: un virus informatico molto complicato che gli esperti non avevano mai visto prima. [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf) era un worm informatico dannoso che prendeva di mira i sistemi di controllo di supervisione e acquisizione dati (SCADA) ed era progettato per danneggiare il programma nucleare iraniano [@farwell2011stuxnet]. Stuxnet stava utilizzando quattro "[exploit zero-day](https://en.wikipedia.org/wiki/Zero-day_(computing))", attacchi che sfruttano debolezze segrete nel software di cui nessuno è ancora a conoscenza. Ciò ha reso Stuxnet molto subdolo e difficile da rilevare.

Ma Stuxnet non è stato progettato per rubare informazioni o spiare le persone. Il suo obiettivo era la distruzione fisica, sabotare le centrifughe della centrale nucleare iraniana di Natanz! Quindi come ha fatto il virus a raggiungere i computer della centrale di Natanz, che avrebbe dovuto essere disconnessa dal mondo esterno per motivi di sicurezza? Gli esperti pensano che qualcuno abbia inserito una chiavetta USB contenente Stuxnet nella rete interna di Natanz. Ciò ha permesso al virus di "saltare" da un sistema esterno ai sistemi di controllo nucleare isolati e scatenare il caos.

Stuxnet era un malware incredibilmente avanzato creato dai governi nazionali per passare dal regno digitale alle infrastrutture del mondo reale. Ha preso di mira in modo specifico importanti macchine industriali, dove l'apprendimento automatico embedded è altamente applicabile in un modo mai visto prima. Il virus ha lanciato un segnale di allarme su come i sofisticati attacchi informatici potrebbero ora distruggere fisicamente apparecchiature e strutture.

Questa violazione è stata significativa a causa della sua sofisticatezza; Stuxnet ha preso di mira in modo specifico i "programmable logic controllers (PLC)" utilizzati per automatizzare processi elettromeccanici come la velocità delle centrifughe per l'arricchimento dell'uranio. Il worm sfruttava le vulnerabilità del sistema operativo Windows per ottenere l'accesso al software Siemens Step7 che controlla i PLC. Nonostante non sia un attacco diretto ai sistemi ML, Stuxnet è rilevante per tutti i sistemi embedded in quanto mostra il potenziale degli attori a livello statale per progettare attacchi che collegano il mondo informatico e quello fisico con effetti devastanti. @fig-stuxnet spiega Stuxnet in modo più dettagliato.

![Spiegazione di Stuxnet. Fonte: [IEEE Spectrum](https://spectrum.ieee.org/the-real-story-of-stuxnet)](images/png/stuxnet.png){#fig-stuxnet}

### Hack della Jeep Cherokee

L'hack della Jeep Cherokee è stato un evento rivoluzionario che ha dimostrato i rischi insiti nelle automobili sempre più connesse [@miller2019lessons]. In una dimostrazione controllata, i ricercatori della sicurezza hanno sfruttato da remoto una vulnerabilità nel sistema di entertainment Uconnect, che aveva una connessione cellulare a Internet. Sono stati in grado di controllare il motore, la trasmissione e i freni del veicolo, allarmando l'industria automobilistica e spingendola a riconoscere le gravi implicazioni per la sicurezza delle vulnerabilità informatiche nei veicoli. @vid-jeephack di seguito è riportato un breve documentario dell'attacco.

:::{#vid-jeephack .callout-important}

# Hack della Jeep Cherokee

{{< video https://www.youtube.com/watch?v=MK0SrxBC1xs&ab_channel=WIRED >}}

:::

Sebbene non si sia trattato di un attacco a un sistema ML in sé, l'affidamento dei veicoli moderni ai sistemi embedded per funzioni critiche per la sicurezza presenta parallelismi significativi con l'implementazione di ML nei sistemi embedded, sottolineando la necessità di una sicurezza robusta a livello hardware.

### Botnet Mirai

La botnet Mirai ha coinvolto l'infezione di dispositivi in rete come fotocamere digitali e lettori DVR [@antonakakis2017understanding]. Nell'ottobre 2016, la botnet è stata utilizzata per condurre uno dei più grandi attacchi [DDoS](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/), interrompendo l'accesso a Internet negli Stati Uniti. L'attacco è stato possibile perché molti dispositivi utilizzavano nomi utente e password predefiniti, che sono stati facilmente sfruttati dal malware Mirai per controllare i dispositivi. @vid-mirai spiega come funziona la botnet Mirai.

:::{#vid-mirai .callout-important}

# Botnet Mirai

{{< video https://www.youtube.com/watch?v=1pywzRTJDaY >}}

:::

Sebbene i dispositivi non fossero basati su ML, l'incidente è un duro promemoria di ciò che può accadere quando numerosi dispositivi embedded con scarsi controlli di sicurezza vengono collegati in rete, cosa che sta diventando sempre più comune con la crescita dei dispositivi IoT basati su ML.

### Implicazioni

Queste violazioni storiche dimostrano gli effetti a cascata delle vulnerabilità hardware nei sistemi embedded. Ogni incidente offre un precedente per comprendere i rischi e progettare protocolli di sicurezza migliori. Ad esempio, la botnet Mirai evidenzia l'immenso potenziale distruttivo quando gli autori delle minacce possono ottenere il controllo su dispositivi in rete con sicurezza debole, una situazione che sta diventando sempre più comune con i sistemi ML. Molti dispositivi ML attuali funzionano come dispositivi "edge" pensati per raccogliere ed elaborare dati localmente prima di inviarli al cloud. Proprio come le telecamere e i DVR compromessi da Mirai, i dispositivi ML edge spesso si basano su hardware embedded come processori ARM ed eseguono sistemi operativi leggeri come Linux. Proteggere le credenziali del dispositivo è fondamentale.

Allo stesso modo, l'hacking della Jeep Cherokee è stato un momento spartiacque per l'industria automobilistica. Ha esposto gravi vulnerabilità nei crescenti sistemi di veicoli connessi in rete e la loro mancanza di isolamento dai sistemi di guida principali come freni e sterzo. In risposta, i produttori di automobili hanno investito molto in nuove misure di sicurezza informatica, anche se probabilmente permangono delle lacune.

Chrysler ha effettuato un richiamo per correggere il software vulnerabile Uconnect, che consentiva l'exploit remoto. Ciò includeva l'aggiunta di protezioni a livello di rete per impedire l'accesso esterno non autorizzato e la compartimentazione dei sistemi di bordo per limitare i movimenti laterali. Sono stati aggiunti ulteriori livelli di crittografia per i comandi inviati tramite il bus CAN all'interno dei veicoli.

L'incidente ha anche stimolato la creazione di nuovi standard e best practice per la sicurezza informatica. L'[Auto-ISAC](https://automotiveisac.com/) è stato istituito per consentire alle case automobilistiche di condividere informazioni e la NHTSA ha guidato i rischi di gestione. Sono state sviluppate nuove procedure di test e audit per valutare le vulnerabilità in modo proattivo. Gli effetti collaterali continuano a guidare il cambiamento nel settore automobilistico poiché le auto diventano sempre più definite dal software.

Sfortunatamente, i produttori spesso trascurano la sicurezza quando sviluppano nuovi dispositivi edge ML, utilizzando password predefinite, comunicazioni non crittografate, aggiornamenti firmware non protetti, ecc. Tali vulnerabilità potrebbero consentire agli aggressori di ottenere l'accesso e controllare i dispositivi su larga scala infettandoli con malware. Con una botnet di dispositivi ML compromessi, gli aggressori potrebbero sfruttare la loro potenza di calcolo aggregata per attacchi DDoS su infrastrutture critiche.

Sebbene questi eventi non abbiano coinvolto direttamente hardware di machine learning, i principi degli attacchi si estendono ai sistemi ML, che spesso coinvolgono dispositivi embedded e architetture di rete simili. Poiché l'hardware ML è sempre più integrato con il mondo fisico, proteggerlo da tali violazioni è fondamentale. L'evoluzione delle misure di sicurezza in risposta a questi incidenti fornisce preziose informazioni sulla protezione dei sistemi ML attuali e futuri da vulnerabilità analoghe.

La natura distribuita dei dispositivi edge ML significa che le minacce possono propagarsi rapidamente attraverso le reti. E se i dispositivi vengono utilizzati per scopi critici come dispositivi medici, controlli industriali o veicoli a guida autonoma, il potenziale danno fisico dei bot ML armati potrebbe essere grave. Proprio come Mirai ha dimostrato il potenziale pericoloso dei dispositivi IoT scarsamente protetti, la prova del nove per la sicurezza dell'hardware ML sarà quanto questi dispositivi siano vulnerabili o resilienti ad attacchi simili a worm. La posta in gioco aumenta man mano che il ML si diffonde in ambiti critici per la sicurezza, ponendo l'onere sui produttori e sugli operatori di sistema di incorporare le lezioni di Mirai.

La lezione è l'importanza di progettare per la sicurezza fin dall'inizio e di avere difese stratificate. Il caso Jeep evidenzia potenziali vulnerabilità per i sistemi ML in merito alle interfacce software esterne e all'isolamento tra sottosistemi. I produttori di dispositivi e piattaforme ML dovrebbero assumere un approccio proattivo e completo simile alla sicurezza piuttosto che lasciarlo come un ripensamento. Una risposta rapida e la diffusione delle best practice saranno cruciali man mano che le minacce si evolvono.

## Minacce alla Sicurezza per i Modelli ML

I modelli ML affrontano rischi per la sicurezza che possono comprometterne l'integrità, le prestazioni e l'affidabilità se non affrontati adeguatamente. Sebbene esistano diverse minacce, le principali includono:
Furto di modelli, in cui gli avversari rubano i parametri proprietari del modello e i dati sensibili in essi contenuti.
Avvelenamento dei dati, che compromette i modelli tramite manomissione dei dati.
Gli attacchi avversari ingannano il modello per fare previsioni errate o indesiderate.

### Furto di Modelli

Il furto di modelli si verifica quando un aggressore ottiene l'accesso non autorizzato a un modello ML distribuito. La preoccupazione in questo caso è il furto della struttura del modello e dei parametri addestrati, nonché dei dati proprietari in esso contenuti [@ateniese2015hacking]. Il furto di modelli è una minaccia reale e crescente, come dimostrato da casi come quello dell'ex ingegnere di Google Anthony Levandowski, che [presumibilmente ha rubato i progetti di auto a guida autonoma di Waymo](https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html) e ha fondato un'azienda concorrente. Oltre all'impatto economico, il furto di modelli può seriamente compromettere la privacy e consentire ulteriori attacchi.

Ad esempio, si consideri un modello ML sviluppato per raccomandazioni personalizzate in un'applicazione di e-commerce. Se un concorrente ruba questo modello, ottiene informazioni su analisi aziendali, preferenze dei clienti e persino segreti commerciali racchiusi nei dati del modello. Gli aggressori potrebbero sfruttare i modelli rubati per creare input più efficaci per attacchi di "inversione del modello", deducendo dettagli privati sui dati di addestramento del modello. Un modello di raccomandazione di e-commerce clonato potrebbe rivelare i comportamenti di acquisto e i dati demografici dei clienti.

Per comprendere gli attacchi di "inversione del modello", si consideri un sistema di riconoscimento facciale utilizzato per concedere l'accesso a strutture protette. Il sistema viene addestrato su un set di dati di foto dei dipendenti. Un aggressore potrebbe dedurre le caratteristiche del set di dati originale osservando l'output del modello su vari input. Ad esempio, supponiamo che il livello di confidenza del modello per un particolare volto sia significativamente più alto per un dato set di caratteristiche. In tal caso, un aggressore potrebbe dedurre che qualcuno con quelle caratteristiche è probabile che sia nel set di dati di addestramento.

La metodologia di "inversione del modello" in genere prevede i seguenti passaggi:

* **Accesso agli Output del Modello:** L'aggressore interroga il modello ML con dati di input e osserva gli output. Ciò avviene spesso tramite un'interfaccia legittima, come un'API pubblica.

* **Analisi dei Confidence Score:** Per ogni input, il modello fornisce un "punteggio di confidenza" che riflette quanto l'input sia simile ai dati di training.

* **Reverse-Engineering:** Analizzando i punteggi di confidenza o le probabilità di output, gli aggressori possono utilizzare tecniche di ottimizzazione per ricostruire ciò che ritengono sia vicino ai dati di input originali.

Un esempio storico di tale vulnerabilità esplorata è stata la ricerca sugli attacchi di inversione contro il set di dati del premio Netflix degli Stati Uniti, in cui i ricercatori hanno dimostrato che era possibile conoscere le preferenze cinematografiche di un individuo, il che potrebbe portare a violazioni della privacy [@narayanan2006break].

Il furto di modelli implica che potrebbe portare a perdite economiche, minare il vantaggio competitivo e violare la privacy degli utenti. C'è anche il rischio di attacchi di inversione del modello, in cui un avversario potrebbe immettere vari dati nel modello rubato per dedurre informazioni sensibili sui dati di addestramento.

In base alla risorsa desiderata, gli attacchi con furto di modelli possono essere suddivisi in due categorie: proprietà esatte del modello e comportamento approssimativo del modello.

##### Furto di Proprietà Esatte del Modello

In questi attacchi, l'obiettivo è estrarre informazioni su metriche concrete, come i parametri appresi di una rete, gli iperparametri ottimizzati e l'architettura interna dei layer del modello [@oliynyk2023know].

* **Parametri Appresi:** Gli avversari mirano a rubare la conoscenza appresa di un modello (pesi e bias) per replicarla. Il furto di parametri è generalmente utilizzato con altri attacchi, come il furto di architettura, che non hanno conoscenza dei parametri.
* **Iperparametri Ottimizzati:** L'addestramento è costoso e l'identificazione della configurazione ottimale degli iperparametri (come velocità di apprendimento e regolarizzazione) può richiedere molto tempo e risorse. Di conseguenza, rubare gli iperparametri ottimizzati di un modello consente agli avversari di replicare il modello senza sostenere gli stessi costi di sviluppo.

* **Architettura del Modello:** Questo attacco riguarda la progettazione e la struttura specifiche del modello, come strati, neuroni e pattern di connettività. Oltre a ridurre i costi di training associati, questo furto rappresenta un grave rischio per la proprietà intellettuale, potenzialmente compromettendo il vantaggio competitivo di un'azienda. Il furto di architettura può essere ottenuto sfruttando attacchi side-channel (discussi più avanti).

##### Furto del Comportamento Approssimativo del Modello

Invece di estrarre valori numerici esatti dei parametri del modello, questi attacchi mirano a riprodurre il comportamento del modello (previsioni ed efficacia), il processo decisionale e le caratteristiche di alto livello [@oliynyk2023know]. Queste tecniche mirano a ottenere risultati simili pur consentendo deviazioni interne nei parametri e nell'architettura. I tipi di furto di comportamento approssimativo includono l'ottenimento dello stesso livello di efficacia e l'ottenimento di coerenza di previsione.

* **Livello di efficacia:** Gli aggressori mirano a replicare le capacità decisionali del modello piuttosto che concentrarsi sui valori precisi dei parametri. Ciò avviene attraverso la comprensione del comportamento complessivo del modello. Consideriamo uno scenario in cui un aggressore desidera copiare il comportamento di un modello di classificazione delle immagini. Analizzando i limiti decisionali del modello, l'attacco ottimizza il suo modello per raggiungere un'efficacia paragonabile al modello originale. Ciò potrebbe comportare l'analisi di 1) la matrice di confusione per comprendere l'equilibrio delle metriche di previsione (vero positivo, vero negativo, falso positivo, falso negativo) e 2) altre metriche di prestazione, come punteggio F1 e precisione, per garantire che i due modelli siano comparabili.

* **Coerenza della Previsione:** L'attaccante cerca di allineare i pattern di previsione del proprio modello con quelli del modello target. Ciò comporta l'abbinamento degli output di previsione (sia positivi che negativi) sullo stesso set di input e la garanzia della coerenza distributiva tra classi diverse. Ad esempio, prendiamo in considerazione un modello di elaborazione del linguaggio naturale (NLP) che genera un'analisi del sentiment per le recensioni di film (etichettando le recensioni come positive, neutre o negative). L'attaccante cercherà di mettere a punto il proprio modello per adattarlo alla previsione dei modelli originali sullo stesso set di recensioni di film. Ciò include la garanzia che il modello commetta gli stessi errori (previsioni errate) commessi dal modello target.

#### Caso di Studio

Nel 2018, Tesla ha intentato una [causa](https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf) contro la startup di auto a guida autonoma [Zoox](https://zoox.com/), sostenendo che ex dipendenti avevano rubato dati riservati e segreti commerciali relativi al sistema di assistenza alla guida autonoma di Tesla.

Tesla ha affermato che diversi suoi ex dipendenti hanno sottratto oltre 10 GB. di dati proprietari, inclusi modelli ML e codice sorgente, prima di unirsi a Zoox. Ciò avrebbe incluso uno dei modelli di riconoscimento delle immagini cruciali di Tesla per l'identificazione degli oggetti.

Il furto di questo modello proprietario sensibile potrebbe aiutare Zoox ad abbreviare anni di sviluppo ML e duplicare le capacità di Tesla. Tesla ha sostenuto che questo furto di I.P. ha causato danni finanziari e competitivi significativi. C'erano anche preoccupazioni che potesse consentire attacchi di inversione del modello per dedurre dettagli privati sui dati di test di Tesla.

I dipendenti di Zoox hanno negato di aver rubato informazioni proprietarie. Tuttavia, il caso evidenzia i rischi significativi del furto di modelli, che consente la clonazione di modelli commerciali, causando ripercussioni economiche e aprendo la porta a ulteriori violazioni della privacy dei dati.

### Avvelenamento dei Dati

L'avvelenamento dei dati è un attacco in cui i dati di training vengono manomessi, portando a un modello compromesso [@biggio2012poisoning]. Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ciò può essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.

Il processo di solito prevede i seguenti passaggi:

* **Injection:** L'aggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un'ispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.

* **Training:** Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei pattern di dati.

* **Deployment:** Una volta distribuito il modello, l'addestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilità prevedibili che l'aggressore può sfruttare.

Gli impatti dell'avvelenamento dei dati vanno oltre i semplici errori di classificazione o cali di accuratezza. Ad esempio, se dati errati o dannosi vengono introdotti nel set di addestramento di un sistema di riconoscimento dei segnali stradali, il modello potrebbe imparare a classificare erroneamente i segnali di stop come segnali di precedenza, il che può avere pericolose conseguenze nel mondo reale, specialmente nei sistemi autonomi embedded come i veicoli autonomi.

L'avvelenamento dei dati può degradare l'accuratezza di un modello, costringerlo a fare previsioni errate o farlo comportare in modo imprevedibile. In applicazioni critiche come l'assistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza.

Esistono sei categorie principali di avvelenamento dei dati [@oprea2022poisoning]:

* **Attacchi di Disponibilità:** Questi attacchi cercano di compromettere la funzionalità complessiva di un modello. Fanno sì che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio è il "label flipping", in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.

* **Attacchi Mirati:** A differenza degli attacchi alla disponibilità, gli attacchi mirati mirano a compromettere un numero limitato di campioni di test. Quindi, l'effetto è localizzato su un numero limitato di classi, mentre il modello mantiene lo stesso livello di accuratezza originale sulla maggior parte delle classi. La natura mirata dell'attacco richiede che l'aggressore conosca le classi del modello, rendendo più difficile il rilevamento di questi attacchi.

* **Attacchi Backdoor:** In questi attacchi, un avversario prende di mira pattern specifici nei dati. L'aggressore introduce una backdoor (un trigger o pattern nascosto e dannoso) nei dati di training, ad esempio modificando determinate feature nei dati strutturati o un pattern di pixel in una posizione fissa. Ciò fa sì che il modello associ il pattern dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di test che contengono un pattern dannoso, fa false previsioni, evidenziando l'importanza della cautela e della prevenzione nel ruolo dei professionisti della sicurezza dei dati.

* **Attacchi di Sotto-popolazione:** Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l'accuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilità e mirati: eseguire attacchi di disponibilità (degrado delle prestazioni) nell'ambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:

* **Scope:** Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un attore inserisce immagini manipolate di un cartello di avvertimento "rallentamenti" (con perturbazioni o pattern attentamente studiati), che fa sì che un'auto autonoma non riconosca tale cartello e non rallenti. D'altro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica è un esempio di attacco di sotto-popolazione.

* **Conoscenza:** Mentre gli attacchi mirati richiedono un alto grado di familiarità con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.

#### Caso di Studio 1

Nel 2017, i ricercatori hanno dimostrato un attacco di avvelenamento dei data contro un modello di classificazione della tossicità popolare chiamato Perspective [@hosseini2017deceiving]. Questo modello ML rileva commenti tossici online.

I ricercatori hanno aggiunto commenti tossici generati sinteticamente con lievi errori di ortografia e grammaticali ai dati di training del modello. Ciò ha lentamente corrotto il modello, facendogli classificare erroneamente un numero crescente di input gravemente tossici come non tossici nel tempo.

Dopo il ri-addestramento sui dati avvelenati, il tasso di falsi negativi del modello è aumentato dall'1,4% al 27%, consentendo ai commenti estremamente tossici di aggirare il rilevamento. I ricercatori hanno avvertito che questo furtivo "data poisoning" potrebbe consentire la diffusione di discorsi di odio, molestie e abusi se implementato contro sistemi di moderazione reali.

Questo caso evidenzia come l'avvelenamento dei dati possa degradare l'accuratezza e l'affidabilità del modello. Per le piattaforme di social media, un attacco di avvelenamento che compromette il rilevamento della tossicità potrebbe portare alla proliferazione di contenuti dannosi e alla sfiducia nei sistemi di moderazione ML. L'esempio dimostra perché proteggere l'integrità dei dati di training e monitorare l'avvelenamento è fondamentale in tutti i domini applicativi.

#### Caso di Studio 2

È interessante notare che gli attacchi di "data poisoning" non sono sempre dannosi [@shan2023prompt]. Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l'Università di Chicago, utilizza il data poisoning per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di intelligenza artificiale generativa. Gli artisti possono utilizzare lo strumento per modificare le proprie immagini in modo sottile prima di caricarle online.

Sebbene queste modifiche siano impercettibili all'occhio umano, possono degradare significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando integrate nei dati di addestramento. I modelli generativi possono essere manipolati per produrre output irrealistici o privi di senso. Ad esempio, con solo 300 immagini corrotte, i ricercatori dell'Università di Chicago sono riusciti a ingannare l'ultimo modello "Stable Diffusion" per generare immagini di cani che assomigliano a felini o bovini quando richiesto per le automobili.

Con l'aumento della quantità di immagini corrotte online, l'efficacia dei modelli addestrati su dati estratti diminuirà esponenzialmente. Inizialmente, identificare i dati corrotti è difficile e richiede un intervento manuale. Successivamente, la contaminazione si diffonde rapidamente ai concetti correlati, poiché i modelli generativi stabiliscono connessioni tra le parole e le loro rappresentazioni visive. Di conseguenza, un'immagine corrotta di un'"auto" potrebbe propagarsi in immagini generate collegate a termini come "camion", "treno" e "autobus".

D'altro canto, questo strumento può essere utilizzato in modo dannoso e influenzare le applicazioni legittime del modello generativo. Ciò dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.

@fig-poisoning mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in varie categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un'auto genera una mucca.

![Avvelenamento dei Dati. Fonte: @shan2023prompt.](images/png/Data_poisoning.png){#fig-poisoning}

### Attacchi Avversari

Gli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) [@parrish2023adversarial]. Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono "hackerare" il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui lievi, spesso impercettibili alterazioni dei dati di input possono indurre un modello ML a fare una previsione errata.

È possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE [@ramesh2021zero] o Stable Diffusion [@rombach2022highresolution]. Ad esempio, alterando i valori dei pixel di un'immagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.

Gli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l'inferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input dannosi con perturbazioni per fuorviare il riconoscimento di pattern del modello, essenzialmente "hackerando" le percezioni del modello.

Gli attacchi avversari rientrano in diversi scenari:

* **Attacchi Whitebox:** L'attaccante ha una conoscenza completa del funzionamento interno del modello target, inclusi i dati di addestramento, i parametri e l'architettura. Questo ampio accesso facilita lo sfruttamento delle vulnerabilità del modello. L'attaccante può sfruttare debolezze specifiche e sottili per costruire esempi avversari altamente efficaci.

* **Attacchi Blackbox:** A differenza degli attacchi Whitebox, in quelli Blackbox l'attaccante ha poca o nessuna conoscenza del modello target. L'attore avversario deve osservare attentamente il comportamento di output del modello per eseguire l'attacco.

* **Attacchi Greybox:** Questi attacchi occupano uno spettro tra gli attacchi Blackbox e Whitebox. L'avversario possiede una conoscenza parziale della struttura interna del modello target. Ad esempio, l'attaccante potrebbe conoscere i dati di training ma non avere informazioni sull'architettura o sui parametri del modello. In scenari pratici, la maggior parte degli attacchi rientra in questa zona grigia.

Il panorama dei modelli di apprendimento automatico è complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilità all'interno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:

* I **Generative Adversarial Network (GAN)** sono modelli di deep learning costituiti da due reti in competizione tra loro: un generatore e un discriminatore [@goodfellow2020generative]. Il generatore cerca di sintetizzare dati realistici mentre il discriminatore valuta se sono reali o falsi. Le GAN possono essere utilizzate per creare esempi avversari. La rete del generatore è addestrata per produrre input che il modello target classifica erroneamente. Queste immagini generate da GAN possono quindi attaccare un classificatore target o un modello di rilevamento. Il generatore e il modello target sono impegnati in un processo competitivo, con il generatore che migliora continuamente la sua capacità di creare esempi ingannevoli e il modello target che aumenta la sua resistenza a tali esempi. Le GAN forniscono un framework robusto per creare input avversari complessi e diversi, illustrando l'adattabilità dei modelli generativi nel panorama avversario.

* I **Transfer Learning Adversarial Attacks** [attacchi avversari di apprendimento di trasferimento] sfruttano la conoscenza trasferita da un modello pre-addestrato a un modello target, creando esempi avversari che possono ingannare entrambi i modelli. Questi attacchi rappresentano una preoccupazione crescente, in particolare quando gli avversari hanno conoscenza dell'estrattore di feature ma non hanno accesso alla testa di classificazione (la parte o il layer responsabile della creazione delle classificazioni finali). Denominate "attacchi headless", queste strategie avversarie trasferibili sfruttano le capacità espressive degli estrattori di feature per creare perturbazioni, ignare dello spazio delle etichette o dei dati di training. L'esistenza di tali attacchi sottolinea l'importanza di sviluppare difese robuste per le applicazioni di apprendimento tramite trasferimento, soprattutto perché i modelli pre-addestrati sono comunemente utilizzati [@ahmed2020headless].

#### Caso di Studio

Nel 2017, i ricercatori hanno condotto esperimenti posizionando piccoli adesivi bianchi e neri sui segnali di stop [@eykholt2018robust]. Quando visti da un occhio umano normale, gli adesivi non oscuravano il segnale né ne impedivano l'interpretazione. Tuttavia, quando le immagini degli adesivi dei segnali di stop venivano inserite nei modelli ML standard di classificazione dei segnali stradali, venivano classificati erroneamente come segnali di limite di velocità nell'85% dei casi.

Questa dimostrazione ha mostrato come semplici adesivi avversari potrebbero ingannare i sistemi ML facendogli interpretare male i segnali stradali critici. Se implementati in modo realistico, questi attacchi potrebbero mettere a repentaglio la sicurezza pubblica, inducendo i veicoli autonomi a interpretare male i segnali di stop come limiti di velocità. I ricercatori hanno avvertito che ciò potrebbe potenzialmente causare pericolosi semplici rallentamenti o accelerazioni negli incroci.

Questo caso di studio fornisce un'illustrazione concreta di come gli esempi avversari sfruttano i meccanismi di riconoscimento di pattern dei modelli ML. Alterando in modo sottile i dati di input, gli aggressori possono indurre previsioni errate e rappresentare rischi significativi per applicazioni critiche per la sicurezza come le auto a guida autonoma. La semplicità dell'attacco dimostra come anche cambiamenti minori e impercettibili possano sviare i modelli. Di conseguenza, gli sviluppatori devono implementare difese robuste contro tali minacce.

## Minacce alla Sicurezza Per l'Hardware ML

Un esame sistematico delle minacce alla sicurezza dell'hardware di apprendimento automatico embedded è essenziale per comprendere in modo completo le potenziali vulnerabilità nei sistemi ML. Inizialmente, verranno esplorate le vulnerabilità hardware derivanti da difetti di progettazione intrinseci che possono essere sfruttati. Questa conoscenza di base è fondamentale per riconoscere le origini delle debolezze hardware. Successivamente, verranno esaminati gli attacchi fisici, che rappresentano i metodi più diretti e palesi per compromettere l'integrità hardware. Sulla base di ciò, verranno analizzati gli attacchi di "injection" di guasti, dimostrando come le manipolazioni deliberate possano indurre guasti del sistema.

Passando agli attacchi "side-channel", verrà mostrata la crescente complessità, poiché questi si basano sullo sfruttamento di "leakage" [perdite] di informazioni indirette, che richiedono una comprensione sfumata delle operazioni hardware e delle interazioni ambientali. Le interfacce che "perdono" mostreranno come i canali di comunicazione esterni possono diventare vulnerabili, portando a esposizioni accidentali di dati. Le discussioni sull'hardware contraffatto traggono vantaggio dalle precedenti esplorazioni dell'integrità dell'hardware e dalle tecniche di sfruttamento, poiché spesso aggravano questi problemi con rischi aggiuntivi dovuti alla loro provenienza discutibile. Infine, i rischi della "supply chain" comprendono tutte le preoccupazioni di cui sopra e le inquadrano nel contesto del percorso dell'hardware dalla produzione alla distribuzione, evidenziando la natura multiforme della sicurezza dell'hardware e la necessità di vigilanza in ogni fase.

@tbl-threat_types riassume gli argomenti:

+----------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| Tipo di minaccia                 | Descrizione                                                                                                               | Rilevanza per la sicurezza hardware ML                  |
+:=================================+:==========================================================================================================================+:========================================================+
| Bug Hardware                     | Difetti intrinseci nelle progettazioni hardware che possono compromettere l'integrità del sistema.                        | Fondamento della vulnerabilità hardware.                |
+----------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| Attacchi Fisici                  | Sfruttamento diretto dell'hardware tramite accesso fisico o manipolazione.                                                | Modello di minaccia basilare e palese.                  |
+----------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| Attacchi di Injection di Guasti  | Induzione di guasti per causare errori nel funzionamento dell'hardware, portando a potenziali crash di sistema.           | Manipolazione sistematica che porta al guasto.          |
+----------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| Attacchi a Canale Laterale       | Sfruttamento di informazioni sul funzionamento dell'hardware per estrarre dati sensibili.                                 | Attacco indiretto tramite osservazione ambientale.      |
+----------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| Interfacce con Perdite           | Vulnerabilità derivanti da interfacce che espongono i dati in modo involontario.                                          | Esposizione dei dati tramite canali di comunicazione.   |
+----------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| Hardware Contraffatto            | Utilizzo di componenti hardware non autorizzati che potrebbero presentare falle di sicurezza.                             | Problemi di vulnerabilità aggravati.                    |
+----------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+
| Rischi della Catena di Fornitura | Rischi introdotti durante il ciclo di vita dell'hardware, dalla produzione alla distribuzione.                            | Sfide di sicurezza cumulative e multiformi.             |
+----------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------+

: Tipi di minaccia alla sicurezza hardware. {#tbl-threat_types .striped .hover}

### Bug Hardware

L'hardware non è immune al problema pervasivo di difetti di progettazione o bug. Gli aggressori possono sfruttare queste vulnerabilità per accedere, manipolare o estrarre dati sensibili, violando la riservatezza e l'integrità da cui dipendono utenti e servizi. Un esempio di tali vulnerabilità è venuto alla luce con la scoperta di Meltdown e Spectre, due vulnerabilità hardware che sfruttano vulnerabilità critiche nei processori moderni. Questi bug consentono agli aggressori di aggirare la barriera hardware che separa le applicazioni, consentendo a un programma dannoso di leggere la memoria di altri programmi e del sistema operativo.

Meltdown [@Lipp2018meltdown] e Spectre [@Kocher2018spectre] funzionano sfruttando le ottimizzazioni nelle CPU moderne che consentono loro di eseguire istruzioni speculative fuori ordine prima che i controlli di validità siano stati completati. Ciò rivela dati che dovrebbero essere inaccessibili, che l'attacco cattura tramite canali laterali come le cache. La complessità tecnica dimostra la difficoltà di eliminare le vulnerabilità anche con una validazione estesa.

Se un sistema ML elabora dati sensibili, come informazioni personali degli utenti o analisi aziendali proprietarie, Meltdown e Spectre rappresentano un pericolo reale e presente per la sicurezza dei dati. Si consideri il caso di una scheda acceleratrice ML progettata per accelerare i processi di apprendimento automatico, come quelli di cui abbiamo parlato nel capitolo [I.A. Hardware](../hw_acceleration/hw_acceleration.it.qmd). Questi acceleratori lavorano con la CPU per gestire calcoli complessi, spesso correlati all'analisi dei dati, al riconoscimento delle immagini e all'elaborazione del linguaggio naturale. Se una scheda acceleratrice di questo tipo presenta una vulnerabilità simile a Meltdown o Spectre, potrebbe far trapelare i dati che elabora. Un aggressore potrebbe sfruttare questa falla non solo per sottrarre dati, ma anche per ottenere informazioni sul funzionamento del modello ML, incluso potenzialmente il reverse engineering del modello stesso (tornando quindi al problema del [furto di modelli](@sec-model_theft).

Uno scenario reale in cui ciò potrebbe essere devastante sarebbe nel settore sanitario. I sistemi ML elaborano regolarmente dati altamente sensibili dei pazienti per aiutare a diagnosticare, pianificare il trattamento e prevedere i risultati. Un bug nell'hardware del sistema potrebbe portare alla divulgazione non autorizzata di informazioni sanitarie personali, violando la privacy del paziente e contravvenendo a rigidi standard normativi come l'[Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html)

Le vulnerabilità [Meltdown e Spectre](https://meltdownattack.com/) sono un duro promemoria del fatto che la sicurezza hardware non riguarda solo la prevenzione dell'accesso fisico non autorizzato, ma anche la garanzia che l'architettura dell'hardware non diventi un canale per l'esposizione dei dati. Difetti di progettazione hardware simili emergono regolarmente in CPU, acceleratori, memoria, bus e altri componenti. Ciò richiede continue mitigazioni retroattive e compromessi sulle prestazioni nei sistemi distribuiti. Soluzioni proattive come le architetture di elaborazione confidenziale potrebbero mitigare intere classi di vulnerabilità attraverso una progettazione hardware fondamentalmente più sicura. Contrastare i bug hardware richiede rigore in ogni fase di progettazione, validazione e distribuzione.

### Attacchi Fisici

La manomissione fisica si riferisce alla manipolazione diretta e non autorizzata di risorse informatiche fisiche per minare l'integrità dei sistemi di apprendimento automatico. È un attacco particolarmente insidioso perché aggira le tradizionali misure di sicurezza informatica, che spesso si concentrano più sulle vulnerabilità del software che sulle minacce hardware.

La manomissione fisica può assumere molte forme, da quelle relativamente semplici, come l'inserimento di un dispositivo USB caricato con software dannoso in un server, a quelle altamente sofisticate, come l'inclusione di un Trojan hardware durante il processo di produzione di un microchip (discusso più avanti in dettaglio nella sezione Supply Chain). I sistemi ML sono suscettibili a questo attacco perché si basano sull'accuratezza e l'integrità del loro hardware per elaborare e analizzare correttamente grandi quantità di dati.

Si consideri un drone alimentato da ML utilizzato per la mappatura geografica. Il funzionamento del drone si basa su una serie di sistemi di bordo, tra cui un modulo di navigazione che elabora gli input da vari sensori per determinare il suo percorso. Se un aggressore ottiene l'accesso fisico a questo drone, potrebbe sostituire il modulo di navigazione originale con uno compromesso che include una backdoor. Questo modulo manipolato potrebbe quindi alterare la traiettoria di volo del drone per condurre la sorveglianza su aree riservate o persino contrabbandare merci di contrabbando volando su rotte non rilevate.

Un altro esempio è la manomissione fisica degli scanner biometrici utilizzati per il controllo degli accessi in strutture sicure. Introducendo un sensore modificato che trasmette dati biometrici a un ricevitore non autorizzato, un aggressore può accedere ai dati di identificazione personale per autenticare gli individui.

Esistono diversi modi in cui la manomissione fisica può verificarsi nell'hardware ML:

* **Manipolazione dei sensori:** Si consideri un veicolo autonomo dotato di telecamere e LiDAR per la percezione ambientale. Un malintenzionato potrebbe manipolare deliberatamente l'allineamento fisico di questi sensori per creare zone di occlusione o distorcere le misure della distanza. Ciò potrebbe compromettere le capacità di rilevamento degli oggetti e potenzialmente mettere in pericolo gli occupanti del veicolo.

* **Trojan hardware:** Le modifiche dannose ai circuiti possono introdurre trojan progettati per attivarsi in base a specifiche condizioni di input. Ad esempio, un chip acceleratore ML potrebbe funzionare come previsto fino a quando non incontra un trigger predeterminato, dopodiché si comporta in modo irregolare.

* **Manomissione della memoria:** L'esposizione fisica e la manipolazione dei chip di memoria potrebbero consentire l'estrazione di parametri del modello ML crittografati. Le tecniche di iniezione di guasti possono anche corrompere i dati del modello per degradare l'accuratezza.

* **Introduzione di backdoor:** Ottenendo l'accesso fisico ai server, un avversario potrebbe utilizzare keylogger hardware per catturare password e creare account backdoor per l'accesso persistente. Questi potrebbero poi essere utilizzati per esfiltrare dati di training ML nel tempo.

* **Attacchi alla supply chain:** Manipolare componenti hardware di terze parti o compromettere i canali di produzione e spedizione crea vulnerabilità sistemiche difficili da rilevare e correggere.

### Attacchi di Fault-injection

Introducendo intenzionalmente guasti nell'hardware ML, gli aggressori possono indurre errori nel processo di elaborazione, portando a output non corretti. Questa manipolazione compromette l'integrità delle operazioni ML e può fungere da vettore per ulteriori sfruttamenti, come il reverse engineering del sistema o il bypass del protocollo di sicurezza. L'iniezione di guasti comporta l'interruzione deliberata delle operazioni di elaborazione standard in un sistema tramite interferenze esterne [@joye2012fault]. Attivando con precisione gli errori di elaborazione, gli avversari possono alterare l'esecuzione del programma in modi che degradano l'affidabilità o trapelano informazioni sensibili.

Per l'iniezione di guasti possono essere utilizzate varie tecniche di manomissione fisica. Bassa tensione [@barenghi2010low], picchi di potenza [@hutter2009contact], anomalie di clock [@amiel2006fault], impulsi elettromagnetici [@agrawal2003side], aumento della temperatura [@skorobogatov2009local] e colpi laser [@skorobogatov2003optical] sono comuni vettori di attacco hardware. Sono programmati con precisione per indurre guasti come bit invertiti o istruzioni saltate durante operazioni critiche.

Per i sistemi ML, le conseguenze includono una precisione del modello compromessa, negazione del servizio, estrazione di dati di training privati o parametri del modello e reverse engineering delle architetture del modello. Gli aggressori potrebbero utilizzare l'iniezione di guasti per forzare classificazioni errate, interrompere sistemi autonomi o rubare proprietà intellettuale.

Ad esempio, @breier2018deeplaser ha iniettato con successo un "fault attack" in una rete neurale profonda distribuita su un microcontrollore. Hanno utilizzato un laser per riscaldare transistor specifici, costringendoli a cambiare stato. In un caso, hanno utilizzato questo metodo per attaccare una funzione di attivazione ReLU, con il risultato che la funzione emetteva sempre un valore di 0, indipendentemente dall'input. Nel codice assembly mostrato in @fig-injection, l'attacco ha fatto sì che il programma in esecuzione saltasse sempre l'istruzione `jmp end` alla riga 6. Ciò significa che `HiddenLayerOutput[i]` è sempre impostato su 0, sovrascrivendo eventuali valori scritti su di esso nelle righe 4 e 5. Di conseguenza, i neuroni mirati vengono resi inattivi, con conseguenti classificazioni errate.

![Iniezione di errore dimostrata con codice assembly. Fonte: @breier2018deeplaser.](images/png/Fault-injection_demonstrated_with_assembly_code.png){#fig-injection}

La strategia di un aggressore potrebbe essere quella di dedurre informazioni sulle funzioni di attivazione tramite attacchi side-channel (discussi in seguito). Quindi, l'aggressore potrebbe tentare di colpire più calcoli di funzioni di attivazione iniettando casualmente guasti nei livelli il più vicino possibile al livello di output, aumentando la probabilità e l'impatto dell'attacco.

I dispositivi embedded sono particolarmente vulnerabili a causa di un limitato rafforzamento fisico e di vincoli di risorse che limitano le difese di runtime robuste. Senza un packaging antimanomissione, l'accesso dell'aggressore ai bus di sistema e alla memoria consente di infierire guasti precisi. Anche i modelli ML embedded leggeri mancano di ridondanza per bypassare gli errori.

Questi attacchi possono essere particolarmente insidiosi perché aggirano le tradizionali misure di sicurezza basate su software, spesso non tenendo conto delle interruzioni fisiche. Inoltre, poiché i sistemi ML si basano in larga misura sull'accuratezza e l'affidabilità del loro hardware per attività come il riconoscimento di pattern, il processo decisionale e le risposte automatiche, qualsiasi compromesso nel loro funzionamento dovuto all'iniezione di guasti può avere conseguenze gravi e di vasta portata.

Per mitigare i rischi di iniezione di guasti è necessario un approccio multi-layer. Il rafforzamento fisico tramite custodie antimanomissione e offuscamento del design aiuta a ridurre l'accesso. Il rilevamento di leggere anomalie può identificare input di sensori insoliti o output di modelli errati [@hsiao2023mavfi]. Le memorie con correzione degli errori riducono al minimo le interruzioni, mentre la crittografia dei dati salvaguarda le informazioni. Le tecniche emergenti di watermarking dei modelli tracciano i parametri rubati.

Tuttavia, bilanciare protezioni robuste con i limiti di dimensioni e potenza ristretti dei sistemi embedded rimane una sfida. I limiti della crittografia e la mancanza di coprocessori sicuri su hardware embedded sensibile ai costi limitano le opzioni. In definitiva, la resilienza all'iniezione di guasti richiede una prospettiva multi-layer che abbraccia i layer di progettazione elettrica, firmware, software e fisica.

### Attacchi a canale laterale

Gli attacchi side-channel costituiscono una classe di violazioni della sicurezza che sfruttano informazioni rivelate inavvertitamente tramite l'implementazione fisica dei sistemi informatici. Contrariamente agli attacchi diretti che prendono di mira vulnerabilità software o di rete, questi attacchi sfruttano le caratteristiche hardware intrinseche del sistema per estrarre informazioni sensibili.

La premessa fondamentale di un attacco side-channel è che il funzionamento di un dispositivo può rivelare inavvertitamente informazioni. Tali fughe possono provenire da varie fonti, tra cui l'energia elettrica consumata da un dispositivo [@kocher1999differential], i campi elettromagnetici che emette [@gandolfi2001electromagnetic], il tempo necessario per elaborare determinate operazioni o persino i suoni che produce. Ogni canale può intravedere indirettamente i processi interni del sistema, rivelando informazioni che possono compromettere la sicurezza.

Ad esempio, si consideri un sistema di apprendimento automatico che esegue transazioni crittografate. Gli algoritmi di crittografia dovrebbero proteggere i dati, ma richiedono un lavoro computazionale per crittografare e decrittografare le informazioni. Un aggressore può analizzare i pattern di consumo energetico del dispositivo che esegue la crittografia per scoprire la chiave crittografica. Con metodi statistici sofisticati, piccole variazioni nel consumo energetico durante il processo di crittografia possono essere correlate ai dati in fase di elaborazione, rivelando infine la chiave. Alcune tecniche di attacco di analisi differenziale sono "Differential Power Analysis (DPA)" [@Kocher2011Intro], "Differential Electromagnetic Analysis (DEMA)" e "Correlation Power Analysis (CPA)".

Ad esempio, si consideri un aggressore che cerca di violare l'algoritmo di crittografia AES utilizzando un attacco di analisi differenziale. L'aggressore dovrebbe prima raccogliere molte tracce di potenza o elettromagnetiche (una traccia è una registrazione di consumi o emissioni) del dispositivo durante l'esecuzione della crittografia AES.

Una volta che l'aggressore ha raccolto tracce sufficienti, utilizzerebbe una tecnica statistica per identificare le correlazioni tra le tracce e i diversi valori del testo in chiaro (testo originale non crittografato) e del testo cifrato (testo crittografato). Queste correlazioni verrebbero poi utilizzate per dedurre il valore di un bit nella chiave AES e, infine, l'intera chiave. Gli attacchi di analisi differenziale sono pericolosi perché sono economici, efficaci e non intrusivi, consentendo agli aggressori di aggirare le misure di sicurezza algoritmiche e a livello hardware. I sistemi compromessi da questi attacchi sono anche difficili da rilevare perché non modificano fisicamente il dispositivo o interrompono l'algoritmo di crittografia.

Di seguito, una visualizzazione semplificata illustra come l'analisi dei pattern di consumo energetico del dispositivo di crittografia può aiutare a estrarre informazioni sulle operazioni dell'algoritmo e, a sua volta, sui dati segreti. Consideriamo un dispositivo che accetta una password di 5 byte come input. I diversi pattern di tensione misurati mentre il dispositivo di crittografia esegue operazioni sull'input per autenticare la password verranno analizzati e confrontati.

Innanzitutto, l'analisi della potenza elettrica delle operazioni del dispositivo dopo aver inserito una password corretta è mostrata nella prima immagine in @fig-encryption. Il grafico blu denso restituisce la misura della tensione del dispositivo di crittografia. Ciò che è significativo qui è il confronto tra i diversi grafici di analisi piuttosto che i dettagli specifici di ciò che sta accadendo in ogni scenario.

![Analisi della potenza di un dispositivo di crittografia con una password corretta. Fonte: [Colin O'Flynn.](https://www.youtube.com/watch?v=2iDLfuEBcs8)](images/png/Power_analysis_of_an_encryption_device_with_a_correct_password.png){#fig-encryption}

Quando viene inserita una password errata, il grafico dell'analisi della potenza è mostrato in @fig-encryption2. I primi tre byte della password sono corretti. Di conseguenza, i pattern di tensione sono molto simili o identici tra i due grafici, fino al quarto byte incluso. Dopo che il dispositivo elabora il quarto byte, viene determinata una mancata corrispondenza tra la chiave segreta e l'input tentato. Viene notato un cambiamento nel pattern nel punto di transizione tra il quarto e il quinto byte: la tensione aumenta (la corrente diminuisce) perché il dispositivo ha smesso di elaborare il resto dell'input.

![Analisi della potenza di un dispositivo di crittografia con una password (parzialmente) errata. Fonte: [Colin O'Flynn.](https://www.youtube.com/watch?v=2iDLfuEBcs8)](images/png/Power_analysis_of_an_encryption_device_with_a_(partially)_wrong_password.png){#fig-encryption2}

@fig-encryption3 descrive un altro grafico di una password completamente errata. Dopo che il dispositivo ha terminato l'elaborazione del primo byte, determina che è errato e interrompe l'ulteriore elaborazione: la tensione aumenta e la corrente diminuisce.

![Analisi della potenza di un dispositivo di crittografia con una password errata. Fonte: [Colin O'Flynn.](https://www.youtube.com/watch?v=2iDLfuEBcs8)](images/png/Power_analysis_of_an_encryption_device_with_a_wrong_password.png){#fig-encryption3}

L'esempio sopra dimostra come le informazioni sul processo di crittografia e sulla chiave segreta possono essere dedotte analizzando diversi input e tentando di "intercettare" le operazioni del dispositivo su ogni byte di input. Per una spiegazione più dettagliata, guardare @vid-powerattack di seguito.

:::{#vid-powerattack .callout-important}

# Power Attack

{{< video https://www.youtube.com/watch?v=2iDLfuEBcs8 >}}

:::

Un altro esempio è un sistema ML per il riconoscimento vocale, che elabora i comandi vocali per eseguire azioni. Misurando la latenza del sistema per rispondere ai comandi o la potenza utilizzata durante l'elaborazione, un aggressore potrebbe dedurre quali comandi vengono elaborati e quindi apprendere i pattern operativi del sistema. Ancora più sottilmente, il suono emesso dalla ventola o dal disco rigido di un computer potrebbe cambiare in risposta al carico di lavoro, che un microfono sensibile potrebbe captare e analizzare per determinare che tipo di operazioni vengono eseguite.

In scenari reali, gli attacchi side-channel hanno effettivamente estratto chiavi di crittografia e compromesso comunicazioni sicure. Uno dei primi casi registrati di un simile attacco si è verificato negli anni '60, quando l'agenzia di intelligence britannica MI5 ha affrontato la sfida di decifrare comunicazioni crittografate dall'ambasciata egiziana a Londra. I loro sforzi di decifrazione dei codici sono stati inizialmente ostacolati dalle limitazioni computazionali dell'epoca, fino a quando un'ingegnosa osservazione dell'agente MI5 Peter Wright ha alterato il corso dell'operazione.

L'agente dell'MI5 Peter Wright propose di usare un microfono per catturare le sottili firme acustiche emesse dalla macchina di cifratura del rotore dell'ambasciata durante la crittografia [@Burnet1989Spycatcher]. I distinti clic meccanici dei rotori mentre gli operatori li configuravano quotidianamente facevano trapelare informazioni critiche sulle impostazioni iniziali. Questo semplice "canale laterale" del suono ha permesso all'MI5 di ridurre drasticamente la complessità della decifrazione dei messaggi. Questo primo attacco di "perdita" acustica evidenzia che gli attacchi a canale laterale non sono semplicemente una novità dell'era digitale, ma una continuazione di antichi principi di crittoanalisi. L'idea che dove c'è un segnale, c'è un'opportunità di intercettazione rimane fondamentale. Dai clic meccanici alle fluttuazioni elettriche e oltre, i canali laterali consentono agli avversari di estrarre segreti indirettamente attraverso un'attenta analisi del segnale.

Oggi, la crittoanalisi acustica si è evoluta in attacchi come l'intercettazione della tastiera [@Asonov2004Keyboard]. I canali laterali elettrici spaziano dall'analisi della potenza su hardware crittografico [@gnad2017voltage] alle fluttuazioni di tensione [@zhao2018fpga] su acceleratori di machine learning. Anche tempistiche, emissioni elettromagnetiche e persino impronte di calore possono essere sfruttate. Nuovi e inaspettati canali laterali emergono spesso man mano che l'informatica diventa più interconnessa e miniaturizzata.

Proprio come la "perdita" acustica analogica dell'MI5 ha trasformato la loro decifrazione dei codici, i moderni attacchi ai canali laterali aggirano i confini tradizionali della difesa informatica. Comprendere lo spirito creativo e la persistenza storica degli exploit dei canali laterali è una conoscenza fondamentale per sviluppatori e difensori che cercano di proteggere in modo completo i moderni sistemi di apprendimento automatico dalle minacce digitali e fisiche.

### Interfacce con Perdite

Le interfacce "leaky" nei sistemi embedded sono spesso backdoor trascurate che possono trasformarsi in significative vulnerabilità di sicurezza. Sebbene progettate per scopi legittimi come comunicazione, manutenzione o debug, queste interfacce possono inavvertitamente fornire agli aggressori una finestra attraverso la quale estrarre informazioni sensibili o iniettare dati dannosi.

Un'interfaccia diventa "leaky" quando espone più informazioni del dovuto, spesso a causa della mancanza di rigorosi controlli di accesso o di una schermatura inadeguata dei dati trasmessi. Ecco alcuni esempi concreti di problemi di interfaccia leaky che causano problemi di sicurezza in dispositivi IoT ed embedded:

* **Baby Monitor:** Molti baby monitor abilitati al WiFi hanno interfacce non protette per l'accesso remoto. Ciò ha consentito agli aggressori di ottenere feed audio e video in tempo reale dalle case delle persone, rappresentando una grave [violazione della privacy](https://www.fox19.com/story/25310628/hacked-baby-monitor/).

* **Pacemaker:** Sono state scoperte vulnerabilità dell'interfaccia in alcuni [pacemaker](https://www.fda.gov/medical-devices/medical-device-recalls/abbott-formally-known-st-jude-medical-recalls-assuritytm-and-enduritytm-pacemakers-potential) che potrebbero consentire agli aggressori di manipolare le funzioni cardiache se sfruttate. Ciò presenta uno scenario potenzialmente letale.

* **Lampadine Smart:** Un ricercatore ha scoperto di poter accedere a dati non crittografati da lampadine intelligenti tramite un'interfaccia di debug, comprese le credenziali WiFi, consentendogli di accedere alla rete connessa [@dhanjani2015abusing].

* **Auto Smart:** Se non protetta, la porta di diagnostica OBD-II ha dimostrato di fornire un vettore di attacco ai sistemi automobilistici. Gli aggressori potrebbero usarlo per controllare i freni e altri componenti [@miller2015remote].

Sebbene quanto sopra non sia direttamente collegato al ML, si consideri l'esempio di un sistema di casa intelligente con un componente ML embedded che controlla la sicurezza domestica in base a pattern di comportamento che apprende nel tempo. Il sistema include un'interfaccia di manutenzione accessibile tramite la rete locale per aggiornamenti software e controlli di sistema. Se questa interfaccia non richiede un'autenticazione forte o i dati trasmessi tramite essa non sono crittografati, un aggressore sulla stessa rete potrebbe ottenere l'accesso. Potrebbero quindi intercettare le routine quotidiane del proprietario di casa o riprogrammare le impostazioni di sicurezza manipolando il firmware.

Tali "fughe" rappresentano un problema di privacy e un potenziale punto di ingresso per exploit più dannosi. L'esposizione di dati di training, parametri del modello o output ML da una "fuga" potrebbe aiutare gli avversari a costruire esempi avversari o a sottoporre a reverse engineering i modelli. L'accesso tramite un'interfaccia con "perdite" potrebbe anche essere utilizzato per modificare il firmware di un dispositivo embedded, caricandolo con codice dannoso che potrebbe spegnerlo, intercettare dati o utilizzarlo in attacchi botnet.

Per mitigare questi rischi, è necessario un approccio multi-strato, che comprenda controlli tecnici quali autenticazione, crittografia, rilevamento delle anomalie, policy e processi come inventari di interfaccia, controlli di accesso, auditing e pratiche di sviluppo sicure. Disattivare le interfacce non necessarie e compartimentare i rischi tramite un modello zero-trust fornisce una protezione aggiuntiva.

Come progettisti di sistemi ML embedded, dovremmo valutare le interfacce nelle prime fasi dello sviluppo e monitorarle continuamente dopo l'implementazione come parte di un ciclo di vita della sicurezza end-to-end. Comprendere e proteggere le interfacce è fondamentale per garantire la sicurezza complessiva del ML embedded.

### Hardware Contraffatto

I sistemi ML sono affidabili solo quanto l'hardware sottostante. In un'epoca in cui i componenti hardware sono beni di consumo globali, l'aumento di hardware contraffatti o clonati rappresenta una sfida significativa. L'hardware contraffatto comprende tutti i componenti che sono riproduzioni non autorizzate di parti originali. I componenti contraffatti si infiltrano nei sistemi ML attraverso complesse catene di fornitura che si estendono oltre i confini e coinvolgono numerose fasi dalla produzione alla consegna.

Anche una sola mancanza di integrità nella catena di fornitura può comportare l'inserimento di parti contraffatte, progettate per imitare fedelmente le funzioni e l'aspetto dell'hardware originale. Ad esempio, un sistema di riconoscimento facciale per il controllo degli accessi ad alta sicurezza potrebbe essere compromesso se dotato di processori contraffatti. Questi processori potrebbero non riuscire a elaborare e verificare accuratamente i dati biometrici, consentendo potenzialmente a persone non autorizzate di accedere ad aree riservate.

La sfida con l'hardware contraffatto è multiforme. Compromette la qualità e l'affidabilità dei sistemi ML, poiché questi componenti potrebbero degradarsi più rapidamente o funzionare in modo imprevedibile a causa di una produzione scadente. Anche i rischi per la sicurezza sono profondi; l'hardware contraffatto può contenere vulnerabilità pronte per essere sfruttate da malintenzionati. Ad esempio, un router di rete clonato in un data center ML potrebbe includere una backdoor nascosta, consentendo l'intercettazione dei dati o l'intrusione nella rete senza essere rilevati.

Inoltre, l'hardware contraffatto comporta rischi legali e di conformità. Le aziende che utilizzano inavvertitamente parti contraffatte nei loro sistemi ML possono affrontare gravi ripercussioni legali, tra cui multe e sanzioni per il mancato rispetto delle normative e degli standard del settore. Ciò è particolarmente vero per i settori in cui è obbligatoria la conformità a specifiche normative sulla sicurezza e sulla privacy, come l'assistenza sanitaria e la finanza.

Le pressioni economiche per ridurre i costi aggravano il problema dell'hardware contraffatto e costringono le aziende ad approvvigionarsi da fornitori a basso costo, privi di rigorosi processi di verifica. Questa economia può introdurre inavvertitamente parti contraffatte in sistemi altrimenti sicuri. Inoltre, rilevare queste contraffazioni è intrinsecamente complicato poiché vengono create per passare per componenti originali, il che spesso richiede attrezzature e competenze sofisticate per essere identificate.

Nel campo dell'apprendimento automatico, dove decisioni in tempo reale e calcoli complessi sono la norma, le implicazioni di un guasto hardware possono essere scomode e potenzialmente pericolose. È fondamentale che le parti interessate siano pienamente consapevoli di questi rischi. Le sfide poste dall'hardware contraffatto richiedono una comprensione completa delle attuali minacce all'integrità del sistema di apprendimento automatico. Ciò sottolinea la necessità di una gestione proattiva e informata del ciclo di vita dell'hardware all'interno di questi sistemi avanzati.

### Rischi della Catena di Fornitura

La minaccia dell'hardware contraffatto è strettamente legata alle vulnerabilità più ampie della supply chain [catena di fornitura]. Le supply chain globalizzate e interconnesse creano molteplici opportunità per componenti compromessi di infiltrarsi nel ciclo di vita di un prodotto. Le supply chain coinvolgono numerose entità, dalla progettazione alla produzione, all'assemblaggio, alla distribuzione e all'integrazione. Una mancanza di trasparenza e supervisione di ogni partner rende difficile la verifica dell'integrità a ogni passaggio. Le lacune in qualsiasi punto della catena possono consentire l'inserimento di parti contraffatte.

Ad esempio, un produttore su contratto potrebbe ricevere e includere inconsapevolmente rifiuti elettronici riciclati contenenti contraffazioni pericolose. Un distributore inaffidabile potrebbe introdurre di nascosto componenti clonati. Le minacce interne a qualsiasi fornitore potrebbero deliberatamente mescolare contraffazioni in spedizioni legittime.

Una volta che le contraffazioni entrano nel flusso di fornitura, passano rapidamente attraverso più mani prima di finire nei sistemi ML in cui il rilevamento è difficile. Le contraffazioni avanzate come parti ricondizionate o cloni con esterni riconfezionati possono mascherarsi da componenti autentici, superando l'ispezione visiva.

Per identificare i falsi, spesso è richiesta una profilazione tecnica completa tramite micrografia, screening a raggi X, analisi forense dei componenti e test funzionali. Tuttavia, un'analisi così costosa non è pratica per gli acquisti su larga scala.

Strategie come audit della supply chain, screening dei fornitori, convalida della provenienza dei componenti e aggiunta di protezioni antimanomissione possono aiutare a mitigare i rischi. Tuttavia, date le sfide globali alla sicurezza della supply chain, un approccio zero-trust è prudente. Progettare sistemi ML per utilizzare controlli ridondanti, fail-safe e monitoraggio continuo del runtime fornisce resilienza contro i compromessi dei componenti.

Una rigorosa convalida delle sorgenti hardware abbinata ad architetture di sistema fault-tolerant offre la difesa più solida contro i rischi pervasivi di supply chain globali contorte e opache.

### Caso di Studio

Nel 2018, Bloomberg Businessweek ha pubblicato una [storia](https://www.bloomberg.com/news/features/2018-10-04/the-big-hack-how-china-used-a-tiny-chip-to-infiltrate-america-s-top-companies) allarmante che ha attirato molta attenzione nel mondo della tecnologia. L'articolo sosteneva che Supermicro aveva segretamente impiantato minuscoli chip spia nell'hardware del server. I giornalisti hanno affermato che gli hacker statali cinesi che lavoravano con Supermicro potevano infilare questi minuscoli chip nelle schede madri durante la produzione. I minuscoli chip avrebbero presumibilmente dato agli hacker un accesso backdoor ai server utilizzati da oltre 30 grandi aziende, tra cui Apple e Amazon.

Se fosse vero, ciò consentirebbe agli hacker di spiare dati privati o persino manomettere i sistemi. Tuttavia, dopo aver indagato, Apple e Amazon non hanno trovato prove dell'esistenza di tale hardware Supermicro hackerato. Altri esperti hanno messo in dubbio l'accuratezza dell'articolo di Bloomberg.

Se la storia sia del tutto accurata o meno non è una nostra preoccupazione da un punto di vista pedagogico. Tuttavia, questo incidente ha attirato l'attenzione sui rischi delle catene di fornitura globali per l'hardware prodotto principalmente in Cina. Quando le aziende esternalizzano e acquistano componenti hardware da fornitori in tutto il mondo, è necessario che vi sia maggiore visibilità nel processo. In questa complessa pipeline globale, si teme che hardware contraffatti o manomessi possano essere introdotti da qualche parte lungo il percorso senza che le aziende tecnologiche se ne accorgano. Le aziende che si affidano troppo a singoli produttori o distributori creano rischi. Ad esempio, a causa dell'eccessiva dipendenza da [TSMC](https://www.tsmc.com/english) per la produzione di semiconduttori, gli Stati Uniti hanno investito 50 miliardi di dollari nel [CHIPS Act](https://www.whitehouse.gov/briefing-room/statements-releases/2022/08/09/fact-sheet-chips-and-science-act-will-lower-costs-create-jobs-strengthen-supply-chains-and-counter-china/).

Man mano che l'apprendimento automatico si sposta in sistemi più critici, è fondamentale verificare l'integrità dell'hardware dalla progettazione alla produzione e alla consegna. La backdoor Supermicro segnalata ha dimostrato che per la sicurezza dell'apprendimento automatico non possiamo dare per scontate le catene di fornitura e la produzione globali. Dobbiamo ispezionare e convalidare l'hardware a ogni collegamento della catena.

## Sicurezza Hardware del ML Embedded

### Trusted Execution Environments

#### Informazioni su TEE

Un Trusted Execution Environment (TEE) è un'area protetta all'interno di un processore host che garantisce l'esecuzione sicura del codice e la protezione dei dati sensibili. Isolando le attività critiche dal sistema operativo, i TEE resistono agli attacchi software e hardware, fornendo un ambiente protetto per la gestione di calcoli sensibili.

#### Vantaggi

I TEE sono particolarmente preziosi in scenari in cui devono essere elaborati dati sensibili o in cui l'integrità delle operazioni di un sistema è critica. Nel contesto dell'hardware ML, i TEE assicurano che gli algoritmi e i dati ML siano protetti da manomissioni e "perdite". Ciò è essenziale perché i modelli ML elaborano spesso informazioni private, segreti commerciali o dati che potrebbero essere sfruttati se esposti.

Ad esempio, un TEE può proteggere i parametri del modello ML dall'estrazione da parte di software dannosi sullo stesso dispositivo. Questa protezione è fondamentale per la privacy e il mantenimento dell'integrità del sistema ML, assicurando che i modelli funzionino come previsto e non forniscano output distorti a causa di parametri manipolati. [Secure Enclave di Apple](https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web), presente in iPhone e iPad, è una forma di TEE che fornisce un ambiente isolato per proteggere i dati sensibili degli utenti e le operazioni crittografiche.

I Trusted Execution Environment (TEE) sono essenziali per i settori che richiedono elevati livelli di sicurezza, tra cui telecomunicazioni, finanza, sanità e automotive. I TEE proteggono l'integrità delle reti 5G nelle telecomunicazioni e supportano applicazioni critiche. Nella finanza, proteggono i pagamenti mobili e i processi di autenticazione. L'assistenza sanitaria si affida ai TEE per salvaguardare i dati sensibili dei pazienti, mentre il settore automobilistico dipende da loro per la sicurezza e l'affidabilità dei sistemi autonomi. In tutti i settori, i TEE garantiscono la riservatezza e l'integrità dei dati e delle operazioni.

Nei sistemi ML, i TEE possono:

* Eseguire in modo sicuro l'addestramento e l'inferenza del modello, assicurando che i risultati del calcolo rimangano riservati.

* Proteggere la riservatezza dei dati di input, come le informazioni biometriche, utilizzati per l'identificazione personale o per attività di classificazione sensibili.

* Proteggere i modelli ML impedendo il reverse engineering, che può proteggere le informazioni proprietarie e mantenere un vantaggio competitivo.

* Abilitare aggiornamenti sicuri ai modelli ML, assicurando che gli aggiornamenti provengano da una fonte attendibile e non siano stati manomessi durante il transito.

* Rafforzare la sicurezza della rete salvaguardando la trasmissione dei dati tra componenti ML distribuiti tramite crittografia ed elaborazione sicura in-TEE.

L'importanza dei TEE nella sicurezza hardware ML deriva dalla loro capacità di proteggere da minacce esterne e interne, tra cui le seguenti:

* **Software Dannoso:** I TEE possono impedire al malware ad alto privilegio di accedere alle aree sensibili del sistema ML.

* **Manomissione Fisica:** Integrandosi con le misure di sicurezza hardware, i TEE possono proteggere dalla manomissione fisica che tenta di aggirare la sicurezza del software.

* **Attacchi Side-Channel:** Sebbene non siano impenetrabili, i TEE possono mitigare specifici attacchi side-channel controllando l'accesso a operazioni sensibili e modelli di dati.

* **Minacce di Rete:** I TEE migliorano la sicurezza della rete salvaguardando la trasmissione dei dati tra componenti ML distribuiti tramite crittografia ed elaborazione in-TEE sicura. Ciò impedisce efficacemente gli attacchi "man-in-the-middle" e garantisce che i dati vengano trasmessi tramite canali attendibili.

#### Meccanica

I fondamenti dei TEE contengono quattro parti principali:

* **Esecuzione Isolata:** Il codice all'interno di un TEE viene eseguito in un ambiente separato dal sistema operativo host del dispositivo host. Questo isolamento protegge il codice dall'accesso non autorizzato da parte di altre applicazioni.

* **Archiviazione Sicura:** I TEE possono archiviare in modo sicuro chiavi crittografiche, token di autenticazione e dati sensibili, impedendo alle applicazioni normali di accedervi al di fuori del TEE.

* **Protezione dell'Integrità:** I TEE possono verificare l'integrità del codice e dei dati, assicurando che non siano stati alterati prima dell'esecuzione o durante l'archiviazione.

* **Crittografia dei Dati:** I dati gestiti all'interno di un TEE possono essere crittografati, rendendoli illeggibili per entità senza le chiavi appropriate, che sono anch'esse gestite all'interno del TEE.

Ecco alcuni esempi di TEE che forniscono sicurezza basata su hardware per applicazioni sensibili:

* **[ARMTrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m):** Questa tecnologia crea ambienti di esecuzione sicuri e normali isolati tramite controlli hardware e implementati in molti chipset mobili. mobile chipsets.

* **[IntelSGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html):** Le estensioni Software Guard di Intel forniscono un'enclave per l'esecuzione del codice che protegge da varie minacce basate sul software, prendendo di mira in modo specifico le vulnerabilità del livello del sistema operativo. Vengono utilizzate per salvaguardare i carichi di lavoro nel cloud.

* **[Qualcomm Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions):** Un sandbox hardware su chipset Qualcomm per app di pagamento e autenticazione mobili.

* **[Apple SecureEnclave](https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web):** Un TEE per la gestione dei dati biometrici e delle chiavi crittografiche su iPhone e iPad, che facilita i pagamenti mobili sicuri.

@fig-enclave è un diagramma che mostra un'enclave sicura isolata dal processore host per fornire un ulteriore livello di sicurezza. L'enclave sicura ha una ROM di avvio per stabilire una "root" hardware di attendibilità, un motore AES per operazioni crittografiche efficienti e sicure e memoria protetta. Ha anche un meccanismo per memorizzare le informazioni in modo sicuro su un archivio collegato separato da quello  flash NAND utilizzato dal processore applicativo e dal sistema operativo. Questo design mantiene al sicuro i dati sensibili degli utenti anche quando il kernel dell'Application Processor viene compromesso.

![Enclave sicura System-on-chip. Fonte: [Apple.](https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web)](images/png/System-on-chip_secure_enclave.png){#fig-enclave}

#### Compromessi

Sebbene i "Trusted Execution Environment" offrano notevoli vantaggi in termini di sicurezza, la loro implementazione comporta dei compromessi. Diversi fattori influenzano se un sistema include un TEE:

**Costo:** L'implementazione dei TEE comporta costi aggiuntivi. Ci sono costi diretti per l'hardware e costi indiretti associati allo sviluppo e alla manutenzione di software sicuro per i TEE. Questi costi potrebbero essere giustificabili solo per alcuni dispositivi, in particolare prodotti a basso margine.

**Complessità:** I TEE aggiungono complessità alla progettazione e allo sviluppo del sistema. L'integrazione di un TEE con sistemi esistenti richiede una sostanziale ri-progettazione dello stack hardware e software, che può rappresentare un ostacolo, in particolare per i sistemi legacy.

**Performance Overhead:** I TEE possono introdurre un overhead di prestazioni dovuto ai passaggi aggiuntivi coinvolti nella crittografia e nella verifica dei dati, che potrebbero rallentare le applicazioni sensibili al tempo.

**Sfide di Sviluppo:** Lo sviluppo per i TEE richiede conoscenze specialistiche e spesso deve rispettare rigidi protocolli di sviluppo. Ciò può estendere i tempi di sviluppo e complicare i processi di debug e test.

**Scalabilità e Flessibilità:** I TEE, a causa della loro natura protetta, possono imporre limitazioni di scalabilità e flessibilità. L'aggiornamento dei componenti protetti o il ridimensionamento del sistema per più utenti o dati può essere più impegnativo quando tutto deve passare attraverso un ambiente protetto e chiuso.

**Consumo Energetico:** L'elaborazione aumentata richiesta per la crittografia, la decrittografia e i controlli di integrità possono portare a un maggiore consumo energetico, una preoccupazione significativa per i dispositivi alimentati a batteria.

**Domanda del Mercato:** Non tutti i mercati o le applicazioni richiedono il livello di sicurezza fornito dai TEE. Per molte applicazioni consumer, il rischio percepito potrebbe essere abbastanza basso da indurre i produttori a scegliere di non includere TEE nei loro progetti.

**Certificazione e Garanzia di Sicurezza:** I sistemi con TEE potrebbero aver bisogno di rigorose certificazioni di sicurezza con enti come [Common Criteria](https://www.commoncriteriaportal.org/ccra/index.cfm) (CC) o [European Union Agency for Cybersecurity](https://www.enisa.europa.eu/) (ENISA), che possono essere lunghe e costose. Alcune organizzazioni potrebbero scegliere di astenersi dall'implementare TEE per evitare questi ostacoli.

**Dispositivi con risorse limitate:** I dispositivi con potenza di elaborazione, memoria o archiviazione limitate potrebbero supportare solo TEE senza compromettere la loro funzionalità primaria.

### Avvio Sicuro

#### Informazioni

Secure Boot è uno standard di sicurezza fondamentale che garantisce che un dispositivo si avvii solo tramite software attendibile dal "Original Equipment Manufacturer (OEM)" [produttore di apparecchiature originali]. Durante l'avvio, il firmware controlla la firma digitale di ogni componente software di avvio, inclusi bootloader, kernel e sistema operativo di base. Questo processo verifica che il software non sia stato alterato o manomesso. Se una firma non supera la verifica, il processo di avvio viene interrotto per impedire l'esecuzione di codice non autorizzato che potrebbe compromettere l'integrità della sicurezza del sistema.

#### Vantaggi

L'integrità di un sistema di apprendimento automatico (ML) embedded è fondamentale dal momento in cui viene acceso. Qualsiasi compromissione nel processo di avvio può portare all'esecuzione di software dannoso prima che il sistema operativo e le applicazioni ML inizino, con conseguenti operazioni ML manipolate, accesso ai dati non autorizzato o riutilizzo del dispositivo per attività dannose come botnet o crypto-mining.

Secure Boot offre protezioni vitali per l'hardware ML embedded tramite i seguenti meccanismi critici:

* **Protezione dei Dati ML:** Garantire che i dati utilizzati dai modelli ML, che possono includere informazioni private o sensibili, non siano esposti a manomissioni o furti durante il processo di boot [avvio].

* **Protezione dell'Integrità del Modello:** Mantenere l'integrità dei modelli ML è fondamentale, poiché la loro manomissione potrebbe portare a risultati errati o dannosi.

* **Aggiornamenti Sicuri del Modello:** Abilitare aggiornamenti sicuri per modelli e algoritmi ML, assicurando che gli aggiornamenti siano autenticati e non siano stati alterati.

#### Meccanica

Secure Boot funziona con i TEE per migliorare ulteriormente la sicurezza del sistema. @fig-secure-boot illustra un diagramma di flusso di un sistema embedded affidabile. Nella fase di validazione iniziale, Secure Boot verifica che il codice in esecuzione nel TEE sia la versione corretta e non manomessa autorizzata dal produttore del dispositivo. Controllando le firme digitali del firmware e di altri componenti critici del sistema, Secure Boot impedisce modifiche non autorizzate che potrebbero compromettere le capacità di sicurezza del TEE. Ciò stabilisce una base di fiducia su cui il TEE può eseguire in modo sicuro operazioni sensibili come la gestione delle chiavi crittografiche e l'elaborazione sicura dei dati. Applicando questi livelli di sicurezza, Secure Boot consente operazioni dei dispositivi sicure e resilienti anche negli ambienti con risorse più limitate.

![Flusso di Secure Boot. Fonte: @Rashmi2018Secure.](images/png/Secure_Boot_flow.png){#fig-secure-boot}

#### Caso di Studio: Face ID di Apple

Un esempio concreto dell'applicazione di Secure Boot può essere osservato nella tecnologia Face ID di Apple, che utilizza algoritmi di apprendimento automatico avanzati per abilitare il [riconoscimento facciale](https://support.apple.com/en-us/102381) su iPhone e iPad. Face ID si basa su una sofisticata integrazione di sensori e software per mappare con precisione la geometria del volto di un utente. Affinché Face ID funzioni in modo sicuro e protegga i dati biometrici degli utenti, le operazioni del dispositivo devono essere affidabili fin dall'inizializzazione. È qui che Secure Boot svolge un ruolo fondamentale. Di seguito viene descritto come funziona Secure Boot insieme a Face ID:

**Verifica Iniziale:** All'avvio di un iPhone, il processo Secure Boot inizia all'interno di Secure Enclave, un coprocessore specializzato progettato per aggiungere un ulteriore livello di sicurezza. Secure Enclave gestisce i dati biometrici, come le impronte digitali per Touch ID e i dati di riconoscimento facciale per Face ID. Durante il processo di avvio, il sistema verifica rigorosamente che Apple abbia firmato digitalmente il firmware di Secure Enclave, garantendone l'autenticità. Questa fase di verifica assicura che il firmware utilizzato per elaborare i dati biometrici rimanga sicuro e senza essere compromesso.

**Controlli di sicurezza continui:** Dopo l'inizializzazione e la convalida del sistema da parte di Secure Boot, Secure Enclave comunica con il processore centrale del dispositivo per mantenere una catena di avvio sicura. Durante questo processo, le firme digitali del kernel iOS e di altri componenti di avvio critici vengono meticolosamente verificate per garantirne l'integrità prima di procedere. Questo modello di "catena di fiducia" impedisce efficacemente modifiche non autorizzate al bootloader e al sistema operativo, salvaguardando la sicurezza complessiva del dispositivo.

**Elaborazione dei Dati del Viso:** Una volta completata la sequenza di avvio sicura, Secure Enclave interagisce in modo sicuro con gli algoritmi di apprendimento automatico che alimentano Face ID. Il riconoscimento facciale prevede la proiezione e l'analisi di oltre 30.000 punti invisibili per creare una mappa di profondità del volto dell'utente e un'immagine a infrarossi. Questi dati vengono convertiti in una rappresentazione matematica e confrontati in modo sicuro con i dati del volto registrati e archiviati in Secure Enclave.

**Secure Enclave e Protezione dei Dati:** Secure Enclave è progettato con precisione per proteggere i dati sensibili e gestire le operazioni crittografiche che salvaguardano tali dati. Anche in caso di kernel del sistema operativo compromesso, i dati del volto elaborati tramite Face ID rimangono inaccessibili ad applicazioni non autorizzate o ad aggressori esterni. È importante sottolineare che i dati di Face ID non vengono mai trasmessi dal dispositivo e non vengono archiviati su iCloud o altri server esterni.

**Aggiornamenti Firmware:** Apple rilascia frequentemente aggiornamenti per risolvere le vulnerabilità della sicurezza e migliorare la funzionalità del sistema. Secure Boot garantisce che tutti gli aggiornamenti del firmware siano autenticati, consentendo l'installazione solo di quelli firmati da Apple. Questo processo aiuta a preservare l'integrità e la sicurezza del sistema di Face ID nel tempo.

Integrando Secure Boot con hardware dedicato come Secure Enclave, Apple offre solide garanzie di sicurezza per operazioni critiche come il riconoscimento facciale.

#### Sfide

Nonostante i suoi vantaggi, l'implementazione di Secure Boot presenta diverse sfide, in particolare in distribuzioni complesse e su larga scala:
**Complessità della Gestione delle Chiavi:** Generare, archiviare, distribuire, ruotare e revocare chiavi crittografiche in modo dimostrabilmente sicuro è particolarmente impegnativo ma fondamentale per mantenere la catena di fiducia. Qualsiasi compromissione delle chiavi paralizza le protezioni. Le grandi aziende che gestiscono moltitudini di chiavi di dispositivi affrontano particolari sfide di scala.

**Sovraccarico di Prestazioni:** Il controllo delle firme crittografiche durante il Boot può aggiungere 50-100ms o più per componente verificato. Questo ritardo può essere proibitivo per applicazioni sensibili al tempo o con risorse limitate. Tuttavia, gli impatti sulle prestazioni possono essere ridotti tramite parallelizzazione e accelerazione hardware.

**Signing Burden:** [Onere della firma] Gli sviluppatori devono garantire diligentemente che tutti i componenti software coinvolti nel processo di avvio, ovvero bootloader, firmware, kernel del sistema operativo, driver, applicazioni, ecc., siano firmati correttamente da chiavi attendibili. L'accettazione della firma del codice di terze parti rimane un problema.

**Verifica Crittografica:** Gli algoritmi e i protocolli sicuri devono convalidare la legittimità di chiavi e firme, evitare manomissioni o bypass e supportare la revoca. L'accettazione di chiavi dubbie mina la fiducia.

**Vincoli di Personalizzazione:** Le architetture Secure Boot bloccate dal fornitore limitano il controllo dell'utente e l'aggiornabilità. I bootloader open source come [u-boot](https://source.denx.de/u-boot/u-boot) e [coreboot](https://www.coreboot.org/) abilitano la sicurezza supportando al contempo la personalizzazione.

**Standard Scalabili:** Standard emergenti come [Device Identifier Composition Engine](https://www.microsoft.com/en-us/research/project/dice-device-identifier-composition-engine/) (DICE) e [IDevID](https://1.ieee802.org/security/802-1ar/) promettono di fornire e gestire in modo sicuro identità e chiavi dei dispositivi su larga scala in tutti gli ecosistemi.

L'adozione di Secure Boot richiede di seguire le best practice di sicurezza relative alla gestione delle chiavi, alla convalida della crittografia, agli aggiornamenti firmati e al controllo degli accessi. Secure Boot fornisce una solida base per creare integrità e affidabilità dei dispositivi se implementato con cura.

### Moduli di Sicurezza Hardware

#### HSM

Un "Hardware Security Module (HSM)" è un dispositivo fisico che gestisce le chiavi digitali per un'autenticazione avanzata e fornisce l'elaborazione crittografica. Questi moduli sono progettati per essere resistenti alle manomissioni e fornire un ambiente sicuro per l'esecuzione di operazioni crittografiche. Gli HSM possono essere dispositivi standalone, schede plug-in o circuiti integrati su un altro dispositivo.

Gli HSM sono fondamentali per varie applicazioni sensibili alla sicurezza perché offrono un'enclave rafforzata e sicura per l'archiviazione delle chiavi crittografiche e l'esecuzione di funzioni crittografiche. Sono particolarmente importanti per garantire la sicurezza delle transazioni, le verifiche dell'identità e la crittografia dei dati.

#### Vantaggi

Gli HSM forniscono diverse funzionalità utili per la sicurezza dei sistemi ML:

**Protezione dei Dati Sensibili:** Nelle applicazioni di apprendimento automatico, i modelli spesso elaborano dati sensibili che possono essere proprietari o personali. Gli HSM proteggono le chiavi di crittografia utilizzate per proteggere questi dati, sia a riposo che in transito, dall'esposizione o dal furto.

**Garanzia dell'Integrità del Modello:** L'integrità dei modelli ML è fondamentale per il loro funzionamento affidabile. Gli HSM possono gestire in modo sicuro i processi di firma e verifica per software e firmware ML, assicurando che parti non autorizzate non abbiano alterato i modelli.

**Addestramento e Aggiornamenti Sicuri del Modello:** L'addestramento e l'aggiornamento dei modelli ML comportano l'elaborazione di dati potenzialmente sensibili. Gli HSM garantiscono che questi processi vengano condotti all'interno di un confine crittografico sicuro, proteggendo dall'esposizione dei dati di addestramento e dagli aggiornamenti non autorizzati del modello.

#### Compromessi

Gli HSM comportano diversi compromessi per l'ML embedded. Questi compromessi sono simili ai TEE, ma per completezza, li discuteremo anche qui attraverso la lente dell'HSM.

**Costo:** Gli HSM sono dispositivi specializzati che possono essere costosi da procurare e implementare, aumentando il costo complessivo di un progetto ML. Questo può essere un fattore significativo per i sistemi embedded, dove i vincoli di costo sono spesso più rigidi.

**Sovraccarico di Prestazioni:** Sebbene sicure, le operazioni crittografiche eseguite dagli HSM possono introdurre latenza. Qualsiasi ritardo aggiunto può essere critico nelle applicazioni ML embedded ad alte prestazioni in cui l'inferenza deve avvenire in tempo reale, come nei veicoli autonomi o nei dispositivi di traduzione.

**Spazio Fisico:** I sistemi embedded sono spesso limitati dallo spazio fisico e l'aggiunta di un HSM può essere difficile in ambienti con vincoli rigidi. Ciò è particolarmente vero per l'elettronica di consumo e la tecnologia indossabile, dove le dimensioni e il fattore di forma sono considerazioni chiave.

**Consumo Energetico:** Gli HSM richiedono energia per funzionare, il che può rappresentare uno svantaggio per i dispositivi a batteria con una lunga durata della batteria. L'elaborazione sicura e le operazioni crittografiche possono scaricare la batteria più velocemente, un compromesso significativo per le applicazioni ML embedded mobili o remote.

**Complessità nell'Integrazione:** L'integrazione degli HSM nei sistemi hardware esistenti aggiunge complessità. Spesso sono necessarie conoscenze specialistiche per gestire la comunicazione sicura tra l'HSM e il processore del sistema e sviluppare software in grado di interfacciarsi con l'HSM.

**Scalabilità:** Il ridimensionamento di una soluzione ML che utilizza gli HSM può essere impegnativo. Gestire una flotta di HSM e garantire l'uniformità nelle pratiche di sicurezza tra i dispositivi può diventare complesso e costoso quando aumentano le dimensioni della distribuzione, soprattutto quando si ha a che fare con sistemi embedded in cui la comunicazione è costosa.

**Complessità Operativa:** Gli HSM possono rendere più complesso l'aggiornamento del firmware e dei modelli ML. Ogni aggiornamento deve essere firmato e possibilmente crittografato, il che aggiunge passaggi al processo di aggiornamento e potrebbe richiedere meccanismi sicuri per la gestione delle chiavi e la distribuzione degli aggiornamenti.

**Sviluppo e Manutenzione:** La natura sicura degli HSM implica che solo personale limitato abbia accesso all'HSM per scopi di sviluppo e manutenzione. Ciò può rallentare il processo di sviluppo e rendere più difficile la manutenzione di routine.

**Certificazione e Conformità:** Garantire che un HSM soddisfi specifici standard di settore e requisiti di conformità può aumentare i tempi e i costi di sviluppo. Ciò potrebbe comportare l'esecuzione di rigorosi processi di certificazione e audit.

### Physical Unclonable Functions (PUF)

#### Informazioni

Le "Physical Unclonable Function (PUF)" [funzioni fisiche non clonabili] forniscono un mezzo intrinseco all'hardware per la generazione di chiavi crittografiche e l'autenticazione del dispositivo sfruttando la variabilità di produzione intrinseca nei componenti semiconduttori. Durante la fabbricazione, fattori fisici casuali come variazioni di drogaggio, ruvidità del bordo della linea e spessore dielettrico determinano differenze microscopiche tra i semiconduttori, anche quando prodotti dalle stesse maschere. Questi creano variazioni di temporizzazione e potenza rilevabili che agiscono come una "impronta digitale" unica per ogni chip. Le PUF sfruttano questo fenomeno incorporando circuiti integrati per amplificare piccole differenze di temporizzazione o potenza in uscite digitali misurabili.

Quando stimolato con uno stimolo in input, il circuito PUF produce una risposta di output basata sulle caratteristiche fisiche intrinseche del dispositivo. A causa della loro unicità fisica, lo stesso stimolo produrrà una risposta diversa su altri dispositivi. Questo meccanismo di stimolo-risposta può essere utilizzato per generare chiavi in modo sicuro e identificatori legati all'hardware specifico, eseguire l'autenticazione del dispositivo o archiviare in modo sicuro i segreti. Ad esempio, una chiave derivata da un PUF funzionerà solo su quel dispositivo e non potrà essere clonata o estratta nemmeno con accesso fisico o reverse engineering completo [@Gao2020Physical].

#### Vantaggi

La generazione di chiavi PUF evita l'archiviazione esterna delle chiavi, che rischia di essere esposta. Fornisce inoltre una base per altre primitive di sicurezza hardware come Secure Boot. Le sfide di implementazione includono la gestione di affidabilità ed entropia variabili tra diverse PUF, sensibilità alle condizioni ambientali e suscettibilità agli attacchi di modellazione di apprendimento automatico. Se progettate con cura, le PUF consentono applicazioni promettenti nella protezione IP, nel trusted computing e nell'anticontraffazione.

#### Utilità

I modelli di apprendimento automatico stanno rapidamente diventando una parte fondamentale della funzionalità per molti dispositivi embedded, come smartphone, assistenti domestici intelligenti e droni autonomi. Tuttavia, proteggere l'apprendimento automatico su hardware embedded con risorse limitate può essere difficile. È qui che le funzioni fisiche non clonabili (PUF) risultano particolarmente utili. Diamo un'occhiata ad alcuni esempi di come le PUF possono essere utili.

Le PUF forniscono un modo per generare impronte digitali e chiavi crittografiche univoche legate alle caratteristiche fisiche di ciascun chip sul dispositivo. Facciamo un esempio. Abbiamo un drone con telecamera intelligente che usa ML embedded per tracciare gli oggetti. Un PUF integrato nel processore del drone potrebbe creare una chiave specifica del dispositivo per crittografare il modello ML prima di caricarlo sul drone. In questo modo, anche se un aggressore in qualche modo hackerasse il drone e provasse a rubare il modello, non sarebbe in grado di usarlo su un altro dispositivo!

La stessa chiave PUF potrebbe anche creare una filigrana digitale embedded nel modello ML. Se quel modello dovesse mai trapelare e essere pubblicato online da qualcuno che cercasse di piratarlo, la filigrana potrebbe aiutare a dimostrare che proviene dal drone rubato e non dall'aggressore. Inoltre, si immagini che la telecamera del drone si colleghi al cloud per scaricare parte della sua elaborazione ML. Il PUF può autenticare che la telecamera è legittima prima che il cloud esegua l'inferenza su video sensibili. Il cloud potrebbe verificare che il drone non sia stato manomesso fisicamente controllando che le risposte PUF non siano cambiate.

Le PUF consentono tutta questa sicurezza attraverso la casualità intrinseca del loro comportamento di stimolo-risposta e il binding hardware. Senza dover memorizzare le chiavi esternamente, le PUF sono ideali per proteggere l'ML embedded con risorse limitate. Pertanto, offrono un vantaggio unico rispetto ad altri meccanismi.

#### Meccanica

Il principio di funzionamento alla base dei PUF, illustrato in @fig-pfu, comporta la generazione di una coppia "stimolo-risposta", in cui un input specifico (lo stimolo) al circuito PUF determina un output (la risposta) che è determinato dalle proprietà fisiche uniche di quel circuito. Questo processo può essere paragonato a un meccanismo di impronte digitali per dispositivi elettronici. I dispositivi che utilizzano ML per elaborare i dati dei sensori possono utilizzare i PUF per proteggere la comunicazione tra dispositivi e impedire l'esecuzione di modelli ML su hardware contraffatto.

@fig-pfu illustra una panoramica delle basi dei PUF: a) PUF può essere pensato come un'impronta digitale unica per ogni pezzo di hardware; b) un PUF Ottico è uno speciale token di plastica che viene illuminato, creando un pattern a macchie unico che viene poi registrato; c) in un APUF (Arbiter PUF), i bit di stimolo selezionano percorsi diversi e un giudice decide quale è più veloce, dando una risposta di '1' o '0'; d) in un PUF SRAM, la risposta è determinata dalla mancata corrispondenza nella tensione di soglia dei transistor, dove determinate condizioni portano a una risposta preferita di  '1'. Ognuno di questi metodi utilizza caratteristiche specifiche dell'hardware per creare un identificatore univoco.

![Nozioni di base sui PUF. Fonte: @Gao2020Physical.](images/png/PUF_basics.png){#fig-pfu}

#### Sfide

Ci sono alcune sfide con i PUF. La risposta PUF può essere sensibile alle condizioni ambientali, come fluttuazioni di temperatura e tensione, portando a un comportamento incoerente di cui si deve tenere conto nella progettazione. Inoltre, poiché i PUF possono generare molte coppie stimolo-risposta uniche, gestire e garantire la coerenza di queste coppie per tutta la durata del dispositivo può essere impegnativo. Ultimo ma non meno importante, l'integrazione della tecnologia PUF può aumentare il costo di produzione complessivo di un dispositivo, sebbene possa far risparmiare sui costi di gestione delle chiavi durante il ciclo di vita del dispositivo.

## Problemi di Privacy nella Gestione dei Dati

La gestione sicura ed etica dei dati personali e sensibili è fondamentale poiché l'apprendimento automatico permea dispositivi come smartphone, dispositivi indossabili ed elettrodomestici intelligenti. Per l'hardware medico, la gestione sicura ed etica dei dati è ulteriormente richiesta dalla legge tramite l'[Health Insurance Portability and Accountability Act](https://aspe.hhs.gov/report/health-insurance-portability-and-accountability-act-1996) (HIPAA). Questi sistemi ML embedded presentano rischi unici per la privacy, data la loro intima vicinanza alla vita degli utenti.

### Tipi di Dati Sensibili

I dispositivi ML embedded come quelli indossabili, assistenti domestici intelligenti e veicoli autonomi elaborano spesso dati altamente personali che richiedono un'attenta gestione per preservare la privacy dell'utente e prevenirne l'uso improprio. Esempi specifici includono referti medici e piani di trattamento elaborati da dispositivi indossabili per la salute, conversazioni private costantemente acquisite da assistenti domestici intelligenti e abitudini di guida dettagliate raccolte da auto connesse. La compromissione di tali dati sensibili può portare a gravi conseguenze come furto di identità, manipolazione emotiva, umiliazione pubblica ed abuso di sorveglianza di massa.

I dati sensibili assumono molte forme: registri strutturati come elenchi di contatti e contenuti non strutturati come flussi audio e video conversazionali. In ambito medico, le "protected health information (PHI)" [informazioni sanitarie protette] vengono raccolte dai medici durante ogni interazione e sono fortemente regolamentate da rigide linee guida HIPAA. Anche al di fuori degli ambienti medici, i dati sensibili possono comunque essere raccolti sotto forma di [Personally Identifiable Information](https://www.dol.gov/general/ppii) (PII) [Informazioni di identificazione personale], definite come "qualsiasi rappresentazione di informazioni che consenta di dedurre ragionevolmente l'identità di un individuo a cui si applicano le informazioni con mezzi diretti o indiretti". Esempi di PII includono indirizzi e-mail, numeri di previdenza sociale e numeri di telefono, tra gli altri campi. Le PII vengono raccolte in ambito medico e in altri contesti (applicazioni finanziarie, ecc.) e sono fortemente regolamentate dalle politiche del Dipartimento del Lavoro.

Anche gli output dei modelli derivati potrebbero far trapelare indirettamente dettagli sugli individui. Oltre ai dati personali, anche algoritmi e set di dati proprietari garantiscono la protezione della riservatezza. Nella sezione Data Engineering, abbiamo trattato diversi argomenti in dettaglio.

Tecniche come la de-identificazione, l'aggregazione, l'anonimizzazione e la federazione possono aiutare a trasformare i dati sensibili in forme meno rischiose mantenendo al contempo l'utilità analitica. Tuttavia, controlli diligenti su accesso, crittografia, auditing, consenso, minimizzazione e pratiche di conformità sono ancora essenziali durante tutto il ciclo di vita dei dati. Regolamenti come [GDPR](https://gdpr-info.eu/) categorizzano diverse classi di dati sensibili e prescrivono responsabilità in merito alla loro gestione etica. Standard come [NIST 800-53](https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final) forniscono rigorose linee guida per il controllo della sicurezza per la protezione della riservatezza. Con la crescente dipendenza dal ML embedded, comprendere i rischi dei dati sensibili è fondamentale.

### Regolamenti Applicabili

Molte applicazioni ML embedded gestiscono dati sensibili degli utenti in base alle normative HIPAA, GDPR e CCPA. Comprendere le protezioni imposte da queste leggi è fondamentale per creare sistemi conformi.

* La norma sulla privacy [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/index.html) stabilisce che i fornitori di assistenza che svolgono determinate attività regolano la privacy e la sicurezza dei dati medici negli Stati Uniti, con severe sanzioni per le violazioni. Tutti i dispositivi ML embedded correlati alla salute, come dispositivi indossabili diagnostici o robot di assistenza, dovrebbero implementare controlli come audit trail, controlli di accesso e crittografia prescritti da HIPAA.

* Il [GDPR](https://gdpr-info.eu/) impone trasparenza, limiti di conservazione e diritti degli utenti sui dati dei cittadini dell'UE, anche quando elaborati da aziende al di fuori dell'UE. I sistemi per la casa intelligente che catturano conversazioni familiari o pattern di posizione dovrebbero essere conformi al GDPR. I requisiti chiave includono la minimizzazione dei dati, la crittografia e meccanismi per il consenso e la cancellazione.

* Il [CCPA](https://oag.ca.gov/privacy/ccpa), che si applica in California, protegge la privacy dei dati dei consumatori tramite disposizioni quali divulgazioni obbligatorie e diritti di opt-out: i gadget IoT come gli smart speaker e i fitness tracker utilizzati dai californiani rientrano probabilmente nel suo ambito.

* Il CCPA è stato il primo insieme di regolamenti specifici per stato in merito alle preoccupazioni sulla privacy. Dopo il CCPA, regolamenti simili sono stati emanati anche in [altri 10 stati](https://pro.bloomberglaw.com/brief/state-privacy-legislation-tracker/), con alcuni stati che hanno proposto progetti di legge per la protezione della privacy dei dati dei consumatori.

Inoltre, quando pertinenti all'applicazione, le norme specifiche del settore disciplinano telematica, servizi finanziari, servizi di pubblica utilità, ecc. Le best practice come "Privacy by design", valutazioni di impatto e mantenimento di audit trail aiutano a incorporare la conformità se non è già richiesta dalla legge. Date le sanzioni potenzialmente costose, è consigliabile consultare team legali/di conformità quando si sviluppano sistemi ML embedded regolamentati.

### De-identificazione

Se i dati medici vengono completamente de-identificati, le linee guida HIPAA non si applicano direttamente e ci sono molte meno normative. Tuttavia, i dati medici devono essere de-identificati utilizzando [metodi HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html) (metodi Safe Harbor o metodi Expert Determination) affinché le linee guida HIPAA non siano più applicabili.

#### Metodi Safe Harbor

I metodi Safe Harbor sono più comunemente utilizzati per de-identificare le informazioni sanitarie protette a causa delle risorse limitate necessarie rispetto ai metodi Expert Determination. La de-identificazione Safe Harbor richiede la pulizia dei set di dati di tutti i dati che rientrano in una delle 18 categorie. Le seguenti categorie sono elencate come informazioni sensibili in base allo standard Safe Harbor:

* Nome, Localizzatore geografico, Data di nascita, Numero di telefono, Indirizzo e-mail, Indirizzi, Numeri di previdenza sociale, Numeri di cartella clinica, Numeri di beneficiari sanitari, Identificatori di dispositivi e Numeri di serie, Numeri di certificati/patenti (Certificato di nascita, Patente di guida, ecc.), Numeri di conto, Identificatori di veicoli, URL di siti Web, Foto a pieno facciale e Immagini comparabili, Identificatori biometrici, Qualsiasi altro identificatore univoco

Per la maggior parte di queste categorie, tutti i dati devono essere rimossi indipendentemente dalle circostanze. Per altre categorie, tra cui informazioni geografiche e data di nascita, i dati possono essere rimossi parzialmente quanto basta per rendere le informazioni difficili da re-identificare. Ad esempio, se un codice postale è abbastanza grande, le prime 3 cifre possono rimanere poiché ci sono abbastanza persone nell'area geografica da rendere difficile la re-identificazione. Le date di nascita devono essere ripulite da tutti gli elementi tranne l'anno di nascita e tutte le età superiori a 89 anni devono essere aggregate in una categoria 90+.

#### Metodi di Determinazione degli Esperti

I metodi Safe Harbor funzionano per diversi casi di de-identificazione dei dati medici, sebbene in alcuni casi sia ancora possibile la re-identificazione. Ad esempio, supponiamo che si raccolgano dati su un paziente in una città urbana con un grande codice postale, ma è stata documentata una malattia rara di cui soffre, una malattia che colpisce solo 25 persone in tutta la città. Dati i dati geografici associati all'anno di nascita, è altamente possibile che qualcuno possa re-identificare questo individuo, il che rappresenta una violazione della privacy estremamente dannosa.

In casi unici come questi, sono preferiti metodi di de-identificazione dei dati di determinazione esperta. La de-identificazione di determinazione esperta richiede una "persona con conoscenza ed esperienza appropriate di principi e metodi statistici e scientifici generalmente accettati per rendere le informazioni non identificabili individualmente" per valutare un set di dati e determinare se il rischio di re-identificazione dei dati individuali in un dato set di dati in combinazione con dati disponibili al pubblico (registri di voto, ecc.), è estremamente ridotto.

La de-identificazione tramite Expert Determination è comprensibilmente più difficile da completare rispetto alla de-identificazione tramite Safe Harbor, a causa del costo e della fattibilità dell'accesso a un esperto per verificare la probabilità di re-identificazione di un set di dati. Tuttavia, in molti casi, è richiesta una Expert Determination per garantire che la re-identificazione dei dati sia estremamente improbabile.

### Riduzione al Minimo dei Dati

La riduzione al minimo dei dati comporta la raccolta, la conservazione e l'elaborazione solo dei dati utente necessari per ridurre i rischi per la privacy derivanti dai sistemi ML embedded. Si inizia limitando i tipi di dati e le istanze raccolte al minimo indispensabile per la funzionalità di base del sistema. Ad esempio, un modello di rilevamento degli oggetti raccoglie solo le immagini necessarie per quella specifica attività di visione artificiale. Allo stesso modo, un assistente vocale limiterebbe l'acquisizione audio a specifici comandi vocali anziché registrare in modo persistente i suoni ambientali.

Ove possibile, i dati temporanei che risiedono brevemente nella memoria senza archiviazione persistente forniscono un'ulteriore riduzione al minimo. Dovrebbe essere stabilita una chiara base giuridica, come il consenso dell'utente, per la raccolta e la conservazione. Il sandboxing e i controlli di accesso impediscono l'uso non autorizzato oltre le attività previste. I periodi di conservazione dovrebbero essere definiti in base allo scopo, con procedure di eliminazione sicura che rimuovono i dati scaduti.

La riduzione al minimo dei dati può essere suddivisa in [3 categorie](https://dl.acm.org/doi/pdf/10.1145/3397271.3401034?casa_token=NrOifKo6dPMAAAAA:Gl5NZNpZMiuSRpJblj43c1cNXkXyv7oEOuYlOfX2qvT8e-9mOLoLQQYz29itxVh6xakKm8haWRs):

1. "I dati devono essere _adeguati_ rispetto allo scopo perseguito". L'omissione di dati può limitare l'accuratezza dei modelli addestrati sui dati e qualsiasi utilità generale di un set di dati. La minimizzazione dei dati richiede che una quantità minima di dati venga raccolta dagli utenti durante la creazione di un set di dati che aggiunge valore ad altri.

2. I dati raccolti dagli utenti devono essere _rilevanti_ allo scopo della raccolta dati.

3. I dati degli utenti dovrebbero essere limitati solo ai dati necessari per soddisfare lo scopo della raccolta dati iniziale. Se è possibile ottenere risultati altrettanto solidi e accurati da un set di dati più piccolo, non dovrebbero essere raccolti dati aggiuntivi oltre questo set di dati più piccolo.

Tecniche emergenti come la privacy differenziale, l'apprendimento federato e la generazione di dati sintetici consentono approfondimenti utili derivati da meno dati utente grezzi. L'esecuzione di mappature del flusso di dati e valutazioni di impatto aiutano a identificare le opportunità per ridurre al minimo l'utilizzo di dati grezzi.

Metodologie come Privacy by Design [@cavoukian2009privacy] considerano tale minimizzazione all'inizio dell'architettura del sistema. Anche normative come il GDPR impongono principi di minimizzazione dei dati. Con un approccio multistrato nei regni legale, tecnico e di processo, la minimizzazione dei dati limita i rischi nei prodotti ML embedded.

#### Caso di Studio - Minimizzazione dei Dati Basata sulle Prestazioni

La minimizzazione dei dati basata sulle prestazioni [@Biega2020Oper] si concentra sull'espansione della terza categoria di minimizzazione dei dati menzionata sopra, ovvero la _limitazione_. Definisce specificamente la robustezza dei risultati del modello su un dato set di dati tramite determinate metriche delle prestazioni, in modo che i dati non debbano essere raccolti ulteriormente se non migliorano significativamente le prestazioni. Le metriche delle prestazioni possono essere divise in due categorie:

1. Prestazioni globali di minimizzazione dei dati

a. Soddisfatto se un set di dati riduce al minimo la quantità di dati per utente mentre le sue prestazioni medie su tutti i dati sono paragonabili alle prestazioni medie del set di dati originale non ridotto.

2. Prestazioni di minimizzazione dei dati per utente

a. Soddisfatto se un set di dati riduce al minimo la quantità di dati per utente mentre le prestazioni minime dei dati utente individuali sono paragonabili a quelle dei dati utente individuali nel set di dati originale non ridotto.

La riduzione al minimo dei dati basata sulle prestazioni può essere sfruttata in impostazioni di apprendimento automatico, inclusi algoritmi di raccomandazione di film e impostazioni di e-commerce.

La riduzione al minimo dei dati globali è molto più fattibile della riduzione al minimo dei dati per utente, data la differenza molto più significativa nelle perdite per utente tra i set di dati ridotti al minimo e quelli originali.

### Consenso e Trasparenza

Un consenso e una trasparenza significativi sono fondamentali quando si raccolgono dati utente per prodotti ML embedded come smart speaker, dispositivi indossabili e veicoli autonomi. Quando viene configurato per la prima volta. Idealmente, il dispositivo dovrebbe spiegare chiaramente quali tipi di dati vengono raccolti, per quali scopi, come vengono elaborati e le policy di conservazione. Ad esempio, uno smart speaker potrebbe raccogliere campioni vocali per addestrare il riconoscimento vocale e profili vocali personalizzati. Durante l'uso, promemoria e opzioni della dashboard forniscono una trasparenza continua su come vengono gestiti i dati, come riepiloghi settimanali di frammenti vocali acquisiti. Le opzioni di controllo consentono di revocare o limitare il consenso, come disattivare l'archiviazione dei profili vocali.

I flussi di consenso dovrebbero fornire controlli granulari che vadano oltre le semplici scelte binarie sì/no. Ad esempio, gli utenti potrebbero acconsentire selettivamente a determinati utilizzi dei dati, come l'addestramento al riconoscimento vocale, ma non alla personalizzazione. I focus group e i test di usabilità con gli utenti target modellano le interfacce di consenso e la formulazione delle policy sulla privacy per ottimizzare la comprensione e il controllo. Il rispetto dei diritti degli utenti, come l'eliminazione e la rettifica dei dati, dimostra affidabilità. Un gergo legale vago ostacola la trasparenza. Regolamenti come GDPR e CCPA rafforzano i requisiti di consenso. Un consenso ponderato e la trasparenza forniscono agli utenti l'agenzia sui propri dati, creando al contempo fiducia nei prodotti ML incorporati attraverso una comunicazione e un controllo aperti.

### Problemi di Privacy nell'Apprendimento Automatico

#### IA Generativa

Sono aumentate anche le preoccupazioni sulla privacy e sulla sicurezza con l'uso pubblico di modelli di intelligenza artificiale generativa, tra cui GPT4 di OpenAI e altri LLM. ChatGPT, in particolare, è stato discusso più di recente in merito alla privacy, date tutte le informazioni personali raccolte dagli utenti di ChatGPT. Nel giugno 2023 è [stata intentata una class action](https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/rIZH4FXwShJE/v0) contro ChatGPT a causa del timore che fosse stato addestrato su informazioni mediche e personali proprietarie senza le dovute autorizzazioni o consensi. Come risultato di queste preoccupazioni sulla privacy, [molte aziende](https://www.businessinsider.com/chatgpt-companies-issued-bans-restrictions-openai-ai-amazon-apple-2023-7) hanno proibito ai propri dipendenti di accedere a ChatGPT e di caricare informazioni aziendali private sul chatbot. Inoltre, ChatGPT è suscettibile al "prompt injection" e altri attacchi alla sicurezza che potrebbero compromettere la privacy dei dati proprietari su cui è stato addestrato.

##### Caso di Studio

Mentre ChatGPT ha istituito delle protezioni per impedire alle persone di accedere a informazioni private ed eticamente discutibili, diversi individui sono riusciti a bypassare queste protezioni tramite "prompt injection" e altri attacchi alla sicurezza. Come dimostrato in @fig-role-play, gli utenti possono bypassare le protezioni di ChatGPT per imitare il tono di una "nonna defunta" per imparare come bypassare un firewall per applicazioni Web [@Gupta2023ChatGPT].

![Gioco di ruolo della nonna per bypassare le restrizioni di sicurezza. Fonte: @Gupta2023ChatGPT.](images/png/Grandma_role_play_to_bypass_safety_restrictions.png){#fig-role-play}

Inoltre, gli utenti hanno anche utilizzato con successo la psicologia inversa per manipolare ChatGPT e accedere a informazioni inizialmente proibite dal modello. In @fig-role-play2, a un utente viene inizialmente impedito di scoprire siti Web di pirateria tramite ChatGPT, ma può bypassare queste restrizioni utilizzando la psicologia inversa.

![Psicologia inversa per bypassare le restrizioni di sicurezza. Fonte: @Gupta2023ChatGPT.](images/png/Reverse_psychology_to_bypass_safety_restrictions.png){#fig-role-play2}

La facilità con cui gli attacchi alla sicurezza possono manipolare ChatGPT è preoccupante, date le informazioni private su cui è stato addestrato senza consenso. Ulteriori ricerche sulla privacy dei dati in LLM e sull'intelligenza artificiale generativa dovrebbero concentrarsi sull'impedire al modello di essere così ingenuo da indurre attacchi di "injection".

#### Cancellazione dei Dati

Molte delle normative precedentemente menzionate, incluso il GDPR, includono una clausola sul "diritto all'oblio". Questa [clausola](https://gdpr-info.eu/art-17-gdpr/) afferma essenzialmente che "l'interessato ha il diritto di ottenere dal titolare del trattamento la cancellazione dei dati personali che lo riguardano senza indebito ritardo". Tuttavia, in diversi casi, anche se i dati dell'utente sono stati cancellati da una piattaforma, i dati vengono cancellati solo parzialmente se un modello di apprendimento automatico è stato addestrato su questi dati per scopi separati. Attraverso metodi simili agli attacchi di inferenza di appartenenza, altri individui possono ancora dedurre i dati di addestramento su cui è stato addestrato un modello, anche se la presenza dei dati è stata esplicitamente rimossa online.

Un approccio per affrontare i problemi di privacy con i dati di addestramento dell'apprendimento automatico è stato attraverso metodi di privacy differenziali. Ad esempio, aggiungendo rumore laplaciano nel set di addestramento, un modello può essere robusto agli attacchi di inferenza di appartenenza, impedendo il recupero dei dati eliminati. Un altro approccio per impedire che i dati eliminati vengano dedotti da attacchi alla sicurezza è semplicemente riaddestrare il modello da zero sui dati rimanenti. Poiché questo processo è dispendioso in termini di tempo e di elaborazione dati, altri ricercatori hanno tentato di affrontare le preoccupazioni sulla privacy relative all'inferenza dei dati di training del modello tramite un processo chiamato "machine unlearning", in cui un modello itera attivamente su se stesso per rimuovere l'influenza dei dati "dimenticati" su cui potrebbe essere stato addestrato, come menzionato di seguito.

## Tecniche ML per la Tutela della Privacy

Sono state sviluppate molte tecniche per preservare la privacy, ognuna delle quali affronta diversi aspetti e sfide per la sicurezza dei dati. Questi metodi possono essere ampiamente categorizzati in diverse aree chiave:
**Differential Privacy**, che si concentra sulla privacy statistica negli output dei dati; **Federated Learning**, che enfatizza l'elaborazione decentralizzata dei dati; **Homomorphic Encryption e Secure Multi-party Computation (SMC)**, entrambi abilitanti calcoli sicuri su dati crittografati o privati;
**Data Anonymization** e **Data Masking and Obfuscation**, che alterano i dati per proteggere le identità individuali; **Private Set Intersection** e
**Zero-Knowledge Proofs**, che facilitano confronti e convalide di dati sicuri; **Decentralized Identifiers (DID)** per identità digitali auto-sovrane; **Privacy-Preserving Record Linkage (PPRL)**, che collega i dati tra le fonti senza esposizione; **Synthetic Data Generation**, che crea set di dati artificiali per analisi sicure; e **Adversarial Learning Techniques**, che migliora la resistenza dei dati o dei modelli agli attacchi alla privacy.

Data l'ampia gamma di queste tecniche, non è possibile approfondire ciascuna di esse in un singolo corso o discussione, e tanto meno che qualcuno possa conoscerle tutte nei loro gloriosi dettagli. Pertanto, esploreremo alcune tecniche specifiche in modo relativamente dettagliato, fornendo una comprensione più approfondita dei loro principi, applicazioni e delle sfide uniche per la privacy che affrontano nell'apprendimento automatico. Questo approccio mirato ci fornirà una comprensione più completa e pratica dei principali metodi di tutela della privacy nei moderni sistemi ML.

### Privacy Differenziale

#### Idea Centrale

La "Differential Privacy" [Privacy Differenziale] è un framework per quantificare e gestire la privacy degli individui in un set di dati [@Dwork2006Theory]. Fornisce una garanzia matematica che la privacy degli individui nel set di dati non verrà compromessa, indipendentemente da qualsiasi conoscenza aggiuntiva che un aggressore potrebbe possedere. L'idea fondamentale della privacy differenziale è che il risultato di qualsiasi analisi (come una query statistica) dovrebbe essere essenzialmente lo stesso, indipendentemente dal fatto che i dati di un individuo siano inclusi nel set di dati o meno. Ciò significa che osservando il risultato dell'analisi, non è possibile determinare se i dati di un individuo siano stati utilizzati nel calcolo.

Ad esempio, supponiamo che un database contenga cartelle cliniche di 10 pazienti. Vogliamo pubblicare statistiche sulla prevalenza del diabete in questo campione senza rivelare le condizioni di un paziente. Per fare ciò, potremmo aggiungere una piccola quantità di rumore casuale al conteggio reale prima di pubblicarlo. Se il numero reale di pazienti diabetici è 6, potremmo aggiungere rumore da una distribuzione di Laplace per ottenere casualmente 5, 6 o 7, ciascuno con una certa probabilità. Un osservatore ora non può dire se un singolo paziente ha il diabete basandosi solo sull'output rumoroso. Il risultato della query è simile a se i dati di ogni paziente sono inclusi o esclusi. Questa è la privacy differenziale. Più formalmente, un algoritmo randomizzato soddisfa la privacy differenziale ε se, per qualsiasi database vicino D e Dʹ che differisce solo per una voce, la probabilità di qualsiasi risultato cambia al massimo di un fattore ε. Un ε inferiore fornisce maggiori garanzie di privacy.

Il meccanismo di Laplace è uno dei metodi più semplici e comunemente utilizzati per ottenere la privacy differenziale. Comporta l'aggiunta di rumore che segue una distribuzione di Laplace ai dati o ai risultati delle query. A parte il Meccanismo di Laplace, il principio generale di aggiunta di rumore è fondamentale per la Privacy differenziale. L'idea è di aggiungere rumore casuale ai dati o ai risultati di una query. Il rumore è calibrato per garantire la necessaria garanzia di privacy mantenendo i dati utili.

Mentre la distribuzione di Laplace è comune, possono essere utilizzate anche altre distribuzioni come quella gaussiana. Il rumore di Laplace è utilizzato per la privacy differenziale rigorosa ε per query a bassa sensibilità. Al contrario, le distribuzioni gaussiane possono essere utilizzate quando la privacy non è garantita, nota come privacy differenziale (ϵ, 𝛿). In questa versione rilassata della privacy differenziale, epsilon e delta definiscono la quantità di privacy garantita quando si rilasciano informazioni o un modello correlato a un set di dati. Epsilon stabilisce un limite su quanta informazione può essere appresa sui dati in base all'output. Allo stesso tempo, delta consente una piccola probabilità che la garanzia della privacy venga violata. La scelta tra Laplace, gaussiana e altre distribuzioni dipenderà dai requisiti specifici della query e del set di dati e dal compromesso tra Privacy e accuratezza.

Per illustrare il compromesso tra Privacy e accuratezza nella Privacy differenziale ($\epsilon$, $\delta$), i seguenti grafici in @fig-tradeoffs mostrano i risultati sull'accuratezza per diversi livelli di rumore sul set di dati MNIST, un ampio set di dati di cifre scritte a mano [@abadi2016deep]. Il valore delta (linea nera; asse y destro) indica il livello di rilassamento della privacy (un valore elevato indica che la Privacy è meno rigorosa). Man mano che la Privacy diventa più rilassata, aumenta l'accuratezza del modello.

![Compromesso tra privacy e accuratezza. Fonte: @abadi2016deep.](images/png/Privacy-accuracy_tradeoff.png){#fig-tradeoffs}

I punti chiave da ricordare sulla privacy differenziale sono i seguenti:

* **Aggiunta di Rumore:** La tecnica fondamentale nella Privacy differenziale è l'aggiunta di rumore casuale controllato ai dati o ai risultati delle query. Questo rumore maschera il contributo dei singoli dati.

* **Atto di Bilanciamento:** C'è un equilibrio tra Privacy e accuratezza. Più rumore (ϵ inferiore) nei dati significa maggiore Privacy ma minore accuratezza nei risultati del modello.

* **Universalità:** La privacy differenziale non si basa su ipotesi su ciò che sa un aggressore.s. Ciò lo rende robusto contro gli attacchi di re-identificazione, in cui un aggressore cerca di scoprire dati individuali.

* **Applicabilità:** Può essere applicato a vari tipi di dati e query, rendendolo uno strumento versatile per l'analisi dei dati che preserva la privacy.

#### Compromessi

Ci sono diversi compromessi da fare con la Privacy differenziale, come nel caso di qualsiasi algoritmo. Ma concentriamoci sui compromessi specifici computazionali, poiché ci interessano i sistemi ML. Ci sono alcune considerazioni e compromessi computazionali chiave quando si implementa la privacy differenziale in un sistema di apprendimento automatico:

**Generazione di Rumore:** L'implementazione della privacy differenziale introduce diversi compromessi computazionali importanti rispetto alle tecniche di apprendimento automatico standard. Una considerazione importante è la necessità di generare in modo sicuro rumore casuale da distribuzioni come Laplace o Gaussiana che vengono aggiunte ai risultati delle query e agli output del modello. La generazione di numeri casuali crittografici di alta qualità può essere computazionalmente costosa.

**Analisi di Sensibilità:** Un altro requisito fondamentale è il monitoraggio rigoroso della sensibilità degli algoritmi sottostanti ai singoli punti dati che vengono aggiunti o rimossi. Questa analisi di sensibilità globale è necessaria per calibrare correttamente i livelli di rumore. Tuttavia, l'analisi della sensibilità del caso peggiore può aumentare sostanzialmente la complessità computazionale per complesse procedure di addestramento del modello e pipeline di dati.

**Gestione del budget per la privacy:** La gestione del budget per la perdita della privacy su più query e iterazioni di apprendimento è un altro sovraccarico contabile. Il sistema deve tenere traccia dei costi cumulativi per la privacy e comporli per spiegare le garanzie di privacy complessive. Ciò aggiunge un onere computazionale che va oltre la semplice esecuzione di query o modelli di addestramento.

**Compromessi tra batch e online:** Per i sistemi di apprendimento online con query continue ad alto volume, gli algoritmi differenzialmente privati richiedono nuovi meccanismi per mantenere l'utilità e prevenire troppe perdite di privacy accumulate poiché ogni query può potenzialmente alterare il budget per la privacy. L'elaborazione offline in batch è più semplice da una prospettiva computazionale poiché elabora i dati in grandi batch, dove ogni batch viene trattato come una singola query. I dati sparsi ad alta dimensionalità aumentano anche le difficoltà dell'analisi di sensibilità.

**Addestramento distribuito:** Quando si addestrano modelli utilizzando approcci [distribuiti](../training/training.it.qmd) o [federati](../optimizations/optimizations.it.qmd), sono necessari nuovi protocolli crittografici per tracciare e limitare le "fughe" di privacy tra i nodi. Il calcolo multi-parti sicuro con dati crittografati per la Privacy differenziale aggiunge un carico computazionale sostanziale.

Mentre la Privacy differenziale fornisce solide garanzie formali di privacy, la sua implementazione rigorosa richiede aggiunte e modifiche alla pipeline di apprendimento automatico a un costo computazionale. La gestione di queste spese generali preservando l'accuratezza del modello rimane un'area di ricerca attiva.

#### Caso di Studio

[L'implementazione della Privacy differenziale da parte di Apple](https://machinelearning.apple.com/research/learning-with-privacy-at-scale#DMNS06) in iOS e MacOS fornisce un importante esempio concreto di [come la Privacy differenziale può essere distribuita su larga scala](https://docs-assets.developer.apple.com/ml-research/papers/learning-with-privacy-at-scale.pdf). Apple voleva raccogliere statistiche aggregate sull'utilizzo nel proprio ecosistema per migliorare prodotti e servizi, ma mirava a farlo senza compromettere la privacy dei singoli utenti.

Per raggiungere questo obiettivo, ha implementato tecniche di privacy differenziale direttamente sui dispositivi degli utenti per rendere anonimi i punti dati prima di inviarli ai server Apple. In particolare, Apple utilizza il meccanismo di Laplace per iniettare rumore casuale attentamente calibrato. Ad esempio, supponiamo che la cronologia delle posizioni di un utente contenga [Lavoro, Casa, Lavoro, Palestra, Lavoro, Casa]. In tal caso, la versione privata differenziale potrebbe sostituire le posizioni esatte con un campione rumoroso come [Palestra, Casa, Lavoro, Lavoro, Casa, Lavoro].

Apple regola la distribuzione del rumore di Laplace per fornire un elevato livello di privacy preservando al contempo l'utilità delle statistiche aggregate. L'aumento dei livelli di rumore fornisce maggiori garanzie di privacy (valori ε inferiori nella terminologia DP) ma può ridurre l'utilità dei dati. Gli ingegneri della privacy di Apple hanno ottimizzato empiricamente questo compromesso in base ai loro obiettivi di prodotto.

Apple ottiene statistiche aggregate ad alta fedeltà aggregando centinaia di milioni di punti dati rumorosi dai dispositivi. Ad esempio, possono analizzare le funzionalità delle nuove app iOS mascherando i comportamenti delle app di qualsiasi utente. Il calcolo sul dispositivo evita di inviare dati grezzi ai server Apple.

Il sistema utilizza la generazione di numeri casuali sicuri basata su hardware per campionare in modo efficiente dalla distribuzione di Laplace sui dispositivi. Apple ha anche dovuto ottimizzare i suoi algoritmi e pipeline differenzialmente privati per operare sotto i vincoli computazionali dell'hardware degli utenti.

Numerosi audit di terze parti hanno verificato che il sistema Apple fornisce rigorose protezioni differenziali della privacy in linea con le loro politiche dichiarate. Naturalmente, le ipotesi sulla composizione nel tempo e sui potenziali rischi di reidentificazione sono ancora valide. L'implementazione di Apple mostra come la privacy differenziale può essere realizzata in grandi prodotti del mondo reale quando supportata da sufficienti risorse ingegneristiche.

:::{#exr-dptf .callout-caution collapse="true"}

### Privacy Differenziale - Privacy TensorFlow

Volete addestrare un modello ML senza compromettere i segreti di nessuno? La Privacy differenziale è come un superpotere per i dati! In questo Colab, useremo TensorFlow Privacy per aggiungere rumore speciale durante l'addestramento. Ciò rende molto più difficile per chiunque determinare se sono stati utilizzati i dati di una singola persona, anche se hanno modi furtivi per sbirciare il modello.

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/tensorflow/privacy/blob/master/g3doc/tutorials/classification_privacy.ipynb)

:::

### Il Federated Learning

#### Idea Centrale

Il "Federated Learning (FL)" è un tipo di apprendimento automatico in cui un modello viene creato e distribuito su più dispositivi o server mantenendo localizzati i dati di training. È stato precedentemente discusso nel capitolo [Ottimizzazioni del modello](../optimizations/optimizations.it.qmd). Tuttavia, lo riepilogheremo brevemente qui per completarlo e concentrarci su cose che riguardano questo capitolo.

FL addestra modelli di apprendimento automatico su reti decentralizzate di dispositivi o sistemi, mantenendo tutti i dati di addestramento localizzati. @fig-fl-lifecycle illustra questo processo: ogni dispositivo partecipante sfrutta i propri dati locali per calcolare gli aggiornamenti del modello, che vengono poi aggregati per creare un modello globale migliorato. Tuttavia, i dati di training grezzi non vengono mai condivisi, trasferiti o compilati direttamente. Questo approccio di tutela della privacy consente lo sviluppo congiunto di modelli ML senza centralizzare i dati di training potenzialmente sensibili in un unico posto.

![Ciclo di Vita dell'Apprendimento Federato. Fonte: @jin2020towards.](images/png/Federated_Learning_lifecycle.png){#fig-fl-lifecycle}

Uno degli algoritmi di aggregazione di modelli più comuni è Federated Averaging (FedAvg), in cui il modello globale viene creato calcolando la media di tutti i parametri dai parametri locali. Mentre FedAvg funziona bene con dati indipendenti e distribuiti in modo identico (IID), algoritmi alternativi come Federated Proximal (FedProx) sono fondamentali nelle applicazioni del mondo reale in cui i dati sono spesso non IID. FedProx è progettato per il processo FL quando c'è una significativa eterogeneità negli aggiornamenti client a causa di diverse distribuzioni di dati tra dispositivi, capacità di calcolo o quantità variabili di dati.

Lasciando i dati grezzi distribuiti e scambiando solo aggiornamenti temporanei del modello, l'apprendimento federato fornisce un'alternativa più sicura e che migliora la privacy alle tradizionali pipeline di apprendimento automatico centralizzate. Ciò consente alle organizzazioni e agli utenti di trarre vantaggio in modo collaborativo da modelli condivisi mantenendo al contempo il controllo e la proprietà sui dati sensibili. La natura decentralizzata di FL lo rende anche robusto per singoli punti di errore.

Si immagini un gruppo di ospedali che desidera collaborare a uno studio per prevedere i risultati dei pazienti in base ai loro sintomi. Tuttavia, non possono condividere i dati dei pazienti a causa di problemi di privacy e normative come HIPAA. Ecco come il Federated Learning può aiutare.

* **Addestramento Locale:** Ogni ospedale addestra un modello di apprendimento automatico sui dati dei pazienti. Questo addestramento avviene localmente, il che significa che i dati non lasciano mai i server dell'ospedale.

* **Condivisione del Modello:** Dopo l'addestramento, ogni ospedale invia solo il modello (in particolare, i suoi parametri o pesi) a un server centrale. Non invia alcun dato del paziente.

* **Modelli di Aggregazione:** Il server centrale aggrega questi modelli da tutti gli ospedali in un singolo modello più robusto. Questo processo in genere comporta la media dei parametri del modello.

* **Vantaggio:** Il risultato è un modello di apprendimento automatico che ha appreso da un'ampia gamma di dati dei pazienti senza condividere dati sensibili o rimuoverli dalla loro posizione originale.

#### Compromessi

Ci sono diversi aspetti correlati alle prestazioni del sistema di FL nei sistemi di apprendimento automatico. Sarebbe saggio comprendere questi compromessi perché non esiste un "pranzo gratis" per preservare la privacy tramite FL [@Li2020Federated].

**Sovraccarico di Comunicazione e Vincoli di Rete:** Nel FL, una delle sfide più significative è la gestione del sovraccarico di comunicazione. Ciò comporta la frequente trasmissione di aggiornamenti del modello tra un server centrale e numerosi dispositivi client, che può richiedere molta larghezza di banda. Il numero totale di round di comunicazione e la dimensione dei messaggi trasmessi per round devono essere ridotti per ridurre ulteriormente la comunicazione. Ciò può comportare un traffico di rete sostanziale, soprattutto in scenari con molti partecipanti. Inoltre, la latenza diventa un fattore critico: il tempo impiegato per inviare, aggregare e ridistribuire questi aggiornamenti può causare ritardi. Ciò influisce sul tempo di training complessivo e ha un impatto sulla reattività del sistema e sulle capacità in tempo reale. Gestire questa comunicazione riducendo al minimo l'utilizzo della larghezza di banda e la latenza è fondamentale per implementare FL.

**Carico di Calcolo sui Dispositivi Locali:** FL si basa su dispositivi client (come smartphone o dispositivi IoT, che sono particolarmente importanti in TinyML) per l'addestramento del modello, che spesso hanno una potenza di calcolo e una durata della batteria limitate. L'esecuzione di algoritmi complessi di apprendimento automatico in locale può mettere a dura prova queste risorse, portando a potenziali problemi di prestazioni. Inoltre, le capacità di questi dispositivi possono variare in modo significativo, con conseguenti contributi non uniformi al processo di addestramento del modello. Alcuni dispositivi elaborano gli aggiornamenti in modo più rapido ed efficiente di altri, portando a disparità nel processo di apprendimento. Bilanciare il carico computazionale per garantire una partecipazione e un'efficienza coerenti su tutti i dispositivi è una sfida fondamentale in FL.

**Efficienza dell'Addestramento del Modello:** La natura decentralizzata di FL può influire sull'efficienza dell'addestramento del modello. Raggiungere la convergenza, in cui il modello non migliora più in modo significativo, può essere più lento in FL rispetto ai metodi di addestramento centralizzati. Ciò è particolarmente vero nei casi in cui i dati sono non IID (non indipendenti e distribuiti in modo identico) tra i dispositivi. Inoltre, gli algoritmi utilizzati per aggregare gli aggiornamenti del modello svolgono un ruolo fondamentale nel processo di addestramento. La loro efficienza influisce direttamente sulla velocità e l'efficacia dell'apprendimento. Sviluppare e implementare algoritmi in grado di gestire le complessità di FL garantendo al contempo una convergenza tempestiva è essenziale per le prestazioni del sistema.

**Sfide di Scalabilità:** La scalabilità è una preoccupazione significativa in FL, soprattutto con l'aumento del numero di dispositivi partecipanti. La gestione e il coordinamento degli aggiornamenti del modello da molti dispositivi aggiungono complessità e possono mettere a dura prova il sistema. È fondamentale garantire che l'architettura del sistema possa gestire in modo efficiente questo carico aumentato senza degradare le prestazioni. Ciò implica non solo la gestione degli aspetti computazionali e di comunicazione, ma anche il mantenimento della qualità e della coerenza del modello man mano che aumenta la scala dell'operazione. Una sfida fondamentale è la progettazione di sistemi FL che si adattino in modo efficace mantenendo le prestazioni.

**Sincronizzazione e Coerenza dei Dati:** Garantire la sincronizzazione dei dati e mantenere la coerenza del modello su tutti i dispositivi partecipanti in FL è una sfida. Mantenere tutti i dispositivi sincronizzati con l'ultima versione del modello può essere difficile in ambienti con connettività intermittente o dispositivi che vanno offline periodicamente. Inoltre, è fondamentale mantenere la coerenza nel modello addestrato, soprattutto quando si ha a che fare con un'ampia gamma di dispositivi con diverse distribuzioni dei dati e frequenze di aggiornamento. Ciò richiede sofisticate strategie di sincronizzazione e aggregazione per garantire che il modello finale rifletta accuratamente gli addestramenti da tutti i dispositivi.

**Consumo Energetico:** Il consumo energetico dei dispositivi client in FL è un fattore critico, in particolare per i dispositivi alimentati a batteria come smartphone e altri dispositivi TinyML/IoT. Le richieste di elaborazione dei modelli di training a livello locale possono portare a un notevole consumo della batteria, il che potrebbe scoraggiare la partecipazione continua al processo FL. È essenziale bilanciare i requisiti di elaborazione dei modelli di training con l'efficienza energetica. Ciò comporta l'ottimizzazione di algoritmi e processi di training per ridurre il consumo energetico e ottenere risultati di addestramento efficaci. Garantire un funzionamento efficiente dal punto di vista energetico è fondamentale per l'accettazione da parte dell'utente e la sostenibilità dei sistemi FL.

#### Casi di Studio

Ecco un paio di casi di studio reali che possono illustrare l'uso dell'apprendimento federato:

##### Google Gboard

Google utilizza l'apprendimento federato per migliorare le previsioni sulla sua app per tastiera mobile Gboard. L'app esegue un algoritmo di apprendimento federato sui dispositivi degli utenti per apprendere dai loro pattern di utilizzo locali e dalle previsioni di testo, mantenendo al contempo privati i dati degli utenti. Gli aggiornamenti del modello vengono aggregati nel cloud per produrre un modello globale migliorato. Ciò consente di fornire previsioni della parola successiva personalizzate in base allo stile di digitazione di ciascun utente, evitando al contempo di raccogliere direttamente dati di digitazione sensibili. Google ha segnalato che l'approccio di apprendimento federato ha ridotto gli errori di previsione del 25% rispetto alla baseline, preservando al contempo la privacy.

##### Ricerca Sanitaria

La UK Biobank e l'American College of Cardiology hanno combinato set di dati per addestrare un modello per il rilevamento dell'aritmia cardiaca utilizzando l'apprendimento federato. I set di dati non potevano essere combinati direttamente a causa di restrizioni legali e sulla privacy. L'apprendimento federato ha consentito lo sviluppo di modelli collaborativi senza condividere dati sanitari protetti, con solo aggiornamenti del modello scambiati tra le parti. Questa accuratezza del modello è migliorata in quanto potrebbe sfruttare una più ampia diversità di dati di training, soddisfacendo al contempo i requisiti normativi.

##### Servizi Finanziari

Le banche stanno valutando l'utilizzo dell'apprendimento federato per i modelli di "anti-money laundering (AML)" [rilevamento antiriciclaggio ]. Più banche potrebbero migliorare congiuntamente i modelli AML senza condividere dati riservati sulle transazioni dei clienti con concorrenti o terze parti. Solo gli aggiornamenti del modello devono essere aggregati anziché i dati grezzi sulle transazioni. Ciò consente l'accesso a dati di training più completi da diverse fonti, evitando al contempo problemi normativi e di riservatezza relativi alla condivisione di dati finanziari sensibili dei clienti.

Questi esempi dimostrano come l'apprendimento federato fornisca vantaggi tangibili sulla privacy e consenta un ML collaborativo in contesti in cui la condivisione diretta dei dati è impossibile.

### Machine Unlearning

#### Idea Centrale

Il "Machine unlearning" è un processo abbastanza nuovo che descrive come l'influenza di un sottoinsieme di dati di training può essere rimossa dal modello. Sono stati utilizzati diversi metodi per eseguire l'unlearning automatico e rimuovere l'influenza di un sottoinsieme di dati di training dal modello finale. Un approccio di base potrebbe consistere semplicemente nel perfezionare il modello per più epoche solo sui dati che dovrebbero essere ricordati per ridurre l'influenza dei dati "dimenticati" dal modello. Poiché questo approccio non rimuove esplicitamente l'influenza dei dati che dovrebbero essere cancellati, sono ancora possibili attacchi di inferenza di appartenenza, quindi i ricercatori hanno adottato altri approcci per disimparare i dati da un modello in modo esplicito. Un tipo di approccio adottato dai ricercatori include l'adeguamento della funzione di perdita del modello per trattare le perdite del "set di dimenticanza esplicito" (dati da disimparare) e del "set di conservazione" (dati rimanenti che dovrebbero ancora essere ricordati) in modo diverso [@tarun2023deep; @khan2021knowledgeadaptation]. @fig-machine-unlearning illustra alcune delle applicazioni di Machine-unlearning.

![Applicazioni di Machine Unlearning. Fonte: [BBVA OpenMind](https://www.bbvaopenmind.com/en/technology/artificial-intelligence/ai-and-machine-unlearning-forgotten-path/)](images/png/machineunlearning.png){#fig-machine-unlearning}

#### Caso di Studio

Alcuni ricercatori hanno dimostrato un esempio concreto di approcci di disapprendimento automatico applicati ai modelli di machine learning SOTA attraverso l'addestramento di un LLM, LLaMA2-7b, per disimparare qualsiasi riferimento a Harry Potter [@eldan2023whos]. Sebbene questo modello abbia richiesto 184K ore di GPU per il pre-addestramento, è bastata solo 1 ora di GPU di messa a punto per cancellare la capacità del modello di generare o richiamare contenuti correlati a Harry Potter senza compromettere in modo evidente l'accuratezza della generazione di contenuti non correlati a Harry Potter. @fig-hp-prompts mostra come l'output del modello cambia prima (colonna Llama-7b-chat-hf) e dopo (colonna Llama-b messa a punto) che si è verificato il disapprendimento.

![Llama disapprendimento di Harry Potter. Fonte: @eldan2023whos.](images/png/Llama_unlearning_Harry_Potter.png){#fig-hp-prompts}

#### Altri Utilizzi

##### Rimozione di dati avversari

È stato precedentemente dimostrato che i modelli di deep learning sono vulnerabili ad attacchi avversari, in cui l'aggressore genera dati avversari simili ai dati di training originali, in cui un essere umano non riesce a distinguere tra i dati reali e quelli fabbricati. I dati avversari fanno sì che il modello emetta previsioni errate, il che potrebbe avere conseguenze negative in varie applicazioni, tra cui le previsioni di diagnosi sanitaria. Il disapprendimento automatico è stato utilizzato per [disimparare l'influenza dei dati avversari](https://arxiv.org/pdf/2209.02299.pdf), impedendo così che queste previsioni errate si verifichino e causino danni.

### Crittografia Omomorfica

#### Idea Centrale

La crittografia omomorfica è una forma di crittografia che consente di eseguire calcoli su testo cifrato, generando un risultato crittografato che, una volta decrittografato, corrisponde al risultato delle operazioni eseguite sul testo in chiaro. Ad esempio, moltiplicando due numeri crittografati con crittografia omomorfica si ottiene un prodotto crittografato che decrittografa il prodotto effettivo dei due numeri. Ciò significa che i dati possono essere elaborati in forma crittografata e solo l'output risultante deve essere decrittografato, migliorando significativamente la sicurezza dei dati, in particolare per le informazioni sensibili.

La crittografia omomorfica consente calcoli esternalizzati su dati crittografati senza esporre i dati stessi esternamente per eseguire le operazioni. Tuttavia, solo determinati calcoli come addizione e moltiplicazione sono supportati negli schemi parzialmente omomorfici. La "Fully homomorphic encryption (FHE)" [crittografia completamente omomorfica ] in grado di gestire qualsiasi calcolo è ancora più complessa. Il numero di possibili operazioni è limitato prima che l'accumulo di rumore corrompa il testo cifrato.

Per utilizzare la crittografia omomorfica su diverse entità, le chiavi pubbliche generate con cura devono essere scambiate per le operazioni su dati crittografati separatamente. Questa tecnica di crittografia avanzata consente paradigmi di calcolo sicuri precedentemente impossibili, ma richiede competenze specifiche per essere implementata correttamente nei sistemi del mondo reale.

#### Vantaggi

La crittografia omomorfica consente l'addestramento del modello di apprendimento automatico e l'inferenza sui dati crittografati, assicurando che gli input sensibili e i valori intermedi rimangano riservati. Ciò è fondamentale in ambito sanitario, finanziario, genetico e altri domini, che si affidano sempre di più al ML per analizzare set di dati sensibili e regolamentati contenenti miliardi di dati personali.

La crittografia omomorfica ostacola attacchi come l'estrazione del modello e l'inferenza dell'appartenenza che potrebbero esporre dati privati utilizzati nei flussi di lavoro ML. Fornisce un'alternativa ai TEE che utilizzano enclave hardware per l'elaborazione riservata. Tuttavia, gli schemi attuali hanno elevati overhead computazionali e limitazioni algoritmiche che limitano le applicazioni del mondo reale.

La crittografia omomorfica realizza la visione decennale di elaborazione multi-parti sicura consentendo l'elaborazione su testi cifrati. Concepiti negli anni '70, i primi sistemi crittografici completamente omomorfici sono emersi nel 2009, consentendo elaborazioni arbitrarie. La ricerca in corso sta rendendo queste tecniche più efficienti e pratiche.

La crittografia omomorfica mostra grandi promesse nell'abilitare l'apprendimento automatico che preserva la privacy in base alle normative emergenti sui dati. Tuttavia, dati i vincoli, si dovrebbe valutare attentamente la sua applicabilità rispetto ad altri approcci di elaborazione confidenziale. Esistono ampie risorse per esplorare la crittografia omomorfica e monitorare i progressi nell'attenuare le barriere all'adozione.

#### Meccanica

1. **Crittografia dei Dati:** Prima che i dati vengano elaborati o inviati a un modello ML, vengono crittografati utilizzando uno schema di crittografia omomorfica e una chiave pubblica. Ad esempio, la crittografia dei numeri $x$ e $y$ genera i testi cifrati $E(x)$ e $E(y)$.

2. **Calcolo sul Testo Cifrato:** L'algoritmo ML elabora direttamente i dati crittografati. Ad esempio, la moltiplicazione dei testi cifrati $E(x)$ e $E(y)$ genera $E(xy)$. È possibile eseguire anche un training del modello più complesso sui testi cifrati.

3. **Crittografia del Risultato:** Il risultato $E(xy)$ rimane crittografato e può essere decrittografato solo da qualcuno con la chiave privata corrispondente per rivelare il prodotto effettivo $xy$.

Solo le parti autorizzate con la chiave privata possono decrittografare gli output finali, proteggendo lo stato intermedio. Tuttavia, il rumore si accumula con ogni operazione, impedendo ulteriori calcoli senza decrittazione.

Oltre all'assistenza sanitaria, la crittografia omomorfica consente il calcolo riservato per applicazioni come il rilevamento di frodi finanziarie, analisi assicurative, ricerca genetica e altro ancora. Offre un'alternativa a tecniche come il calcolo multipartitico e i TEE. La ricerca in corso migliora l'efficienza e le capacità.

Strumenti come HElib, SEAL e TensorFlow HE forniscono librerie per esplorare l'implementazione della crittografia omomorfica in pipeline di apprendimento automatico nel mondo reale.

#### Compromessi

Per molte applicazioni in tempo reale ed embedded, la crittografia completamente omomorfica rimane poco pratica per i seguenti motivi.

**Sovraccarico Computazionale:** La crittografia omomorfica impone sovraccarichi computazionali molto elevati, spesso con conseguenti rallentamenti di oltre 100 volte per le applicazioni ML del mondo reale. Ciò la rende poco pratica per molti utilizzi sensibili al tempo o con risorse limitate. L'hardware ottimizzato e la parallelizzazione possono alleviare, ma non eliminare, questo problema.

**Complessità di Implementazione** Gli algoritmi sofisticati richiedono una profonda competenza in crittografia per essere implementati correttamente. Sfumature come la compatibilità del formato con modelli ML in virgola mobile e la gestione scalabile delle chiavi pongono ostacoli. Questa complessità ostacola l'adozione pratica diffusa.

**Limitazioni Algoritmiche:** Gli schemi attuali limitano le funzioni e la profondità dei calcoli supportati, limitando i modelli e i volumi di dati che possono essere elaborati. La ricerca in corso sta spingendo questi limiti, ma permangono delle restrizioni.

**Accelerazione Hardware:** La crittografia omomorfica richiede hardware specializzato, come processori sicuri o coprocessori con TEE, che aggiungono costi di progettazione e infrastruttura.

**Progetti Ibridi:** Anziché crittografare interi flussi di lavoro, l'applicazione selettiva della crittografia omomorfica a sotto-componenti critici può ottenere protezione riducendo al minimo i costi generali.

:::{#exr-he .callout-caution collapse="true"}

### Crittografia Omomorfica

Pronti a sbloccare il potere del calcolo crittografato? La crittografia omomorfica è come un trucco magico per i vostri dati! In questo Colab, impareremo come fare calcoli su numeri segreti senza mai rivelarli. Immaginate di addestrare un modello su dati che non potete nemmeno vedere: questo è il potere di questa tecnologia strabiliante.

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/drive/1GjKT5Lgh9Madjsyr9UiyeogUgVpTEBMp?usp=sharing)

:::

### Secure Multiparty Communication

#### Idea Centrale

L'obiettivo principale della "Multi-Party Communication (MPC)" [Comunicazione Multi-parte] è consentire a diverse parti di calcolare congiuntamente una funzione sui propri input, mantenendo al contempo la riservatezza di tali input. Ad esempio, due organizzazioni potrebbero voler collaborare all'addestramento di un modello di apprendimento automatico combinando i rispettivi set di dati. Tuttavia, non possono rivelare direttamente tali dati a causa di vincoli di privacy o riservatezza. MPC fornisce protocolli e tecniche che consentono di ottenere i vantaggi dei dati aggregati per l'accuratezza del modello senza compromettere la privacy dei dati sensibili di ciascuna organizzazione.

Ad alto livello, MPC funziona suddividendo attentamente il calcolo in porzioni che ciascuna parte può eseguire in modo indipendente utilizzando il proprio input privato. I risultati vengono poi combinati per rivelare solo l'output finale della funzione e nulla sui valori intermedi. Vengono utilizzate tecniche crittografiche per garantire che i risultati parziali rimangano privati in modo dimostrabile.

Prendiamo un semplice esempio di protocollo MPC. Uno dei protocolli MPC più basilari è l'addizione sicura di due numeri. Ogni parte suddivide il suo input in quote casuali che vengono distribuite segretamente. Si scambiano le quote e calcolano localmente la somma delle quote, che ricostruisce la somma finale senza rivelare i singoli input. Ad esempio, se Alice ha input x e Bob ha input y:

1. Alice genera $x_1$ casuale e imposta $x_2 = x - x_1$

2. Bob genera $y_1$ casuale e imposta $y_2 = y - y_1$

3. Alice invia $x_1$ a Bob, Bob invia $y_1$ ad Alice (mantenendo segreti $x_2$ e $y_2$)

4. Alice calcola $x_2 + y_1 = s_1$, Bob calcola $x_1 + y_2 = s_2$

5. $s_1 + s_2 = x + y$ è la somma finale, senza rivelare $x$ o $y$.

Gli input individuali di Alice e Bob ($x$ e $y$) rimangono privati e ciascuna parte rivela solo un numero associato ai propri input originali. Grazie ai risultati casuali, non viene rivelata alcuna informazione sui numeri originali.

**Confronto Sicuro:** Un'altra operazione di base è un confronto sicuro di due numeri, per determinare quale è maggiore dell'altro. Questo può essere fatto usando tecniche come i "Yao's Garbled Circuits" [circuiti distorti di Yao] (https://it.wikipedia.org/wiki/Andrew_Chi-Chih_Yao), dove il circuito di confronto è crittografato per consentire una valutazione congiunta degli input senza trapelare.

**Moltiplicazione Sicura di Matrici:** Le operazioni di matrice come la moltiplicazione sono essenziali per l'apprendimento automatico. Le tecniche MPC (Multiparty Communication) come la condivisione segreta additiva possono essere usate per dividere le matrici in quote casuali, calcolare i prodotti sulle quote e quindi ricostruire il risultato.

**Addestramento Sicuro del Modello:** Gli algoritmi di addestramento dell'apprendimento automatico distribuito come la media federata possono essere resi sicuri usando MPC. Gli aggiornamenti del modello calcolati su dati partizionati in ogni nodo vengono condivisi segretamente tra i nodi e aggregati per addestrare il modello globale senza esporre aggiornamenti individuali.

L'idea fondamentale alla base dei protocolli MPC è quella di dividere il calcolo in passaggi che possono essere eseguiti congiuntamente senza rivelare dati sensibili intermedi. Ciò si ottiene combinando tecniche crittografiche come la condivisione segreta, la crittografia omomorfica, il trasferimento inconsapevole e i circuiti garbled [distorti]. I protocolli MPC consentono il calcolo collaborativo di dati sensibili fornendo al contempo garanzie di privacy dimostrabili. Questa capacità di preservazione della privacy è essenziale per molte applicazioni di apprendimento automatico odierne che coinvolgono più parti che non possono condividere direttamente i propri dati grezzi.

Gli approcci principali utilizzati in MPC includono:

* **Crittografia omomorfica:** La crittografia speciale consente di eseguire calcoli su dati crittografati senza decrittografarli.

* **Condivisione segreta:** I dati privati vengono suddivisi in quote casuali distribuite a ciascuna parte. I calcoli vengono eseguiti localmente sulle quote e infine ricostruiti.

* **Trasferimento inconsapevole:** Un protocollo in cui un ricevitore ottiene un sottoinsieme di dati da un mittente, ma il mittente non sa quali dati specifici sono stati trasferiti.

* **Circuiti Garbled:** La funzione da calcolare è rappresentata come un circuito booleano crittografato ("distorto") per consentire una valutazione congiunta senza rivelare gli input.

#### Compromessi

Sebbene i protocolli MPC forniscano solide garanzie di privacy, hanno un costo computazionale elevato rispetto ai calcoli semplici. Ogni operazione sicura, come addizione, moltiplicazione, confronto, ecc., richiede più ordini di elaborazione rispetto all'operazione equivalente non crittografata. Questo overhead deriva dalle tecniche crittografiche sottostanti:

* Nella crittografia parzialmente omomorfica, ogni calcolo su testi cifrati richiede costose operazioni a chiave pubblica. La crittografia completamente omomorfica ha overhead ancora più elevati.

* La condivisione segreta divide i dati in più porzioni, quindi anche le operazioni di base richiedono la manipolazione di molte porzioni.

* Il trasferimento inconsapevole e i circuiti distorti aggiungono mascheramento e crittografia per nascondere i pattern di accesso ai dati e i flussi di esecuzione.

* I sistemi MPC richiedono un'ampia comunicazione e interazione tra le parti per calcolare congiuntamente condivisioni/testi cifrati.

Di conseguenza, i protocolli MPC possono rallentare i calcoli di 3-4 ordini di grandezza rispetto alle implementazioni semplici. Ciò diventa proibitivo per grandi set di dati e modelli. Pertanto, l'addestramento di modelli di apprendimento automatico su dati crittografati tramite MPC rimane oggi irrealizzabile per dimensioni di set di dati realistiche a causa del sovraccarico. Sono necessarie ottimizzazioni e approssimazioni intelligenti per rendere pratico l'MPC.

La ricerca in corso sull'MPC colma questo divario di efficienza attraverso progressi crittografici, nuovi algoritmi, hardware affidabile come le enclave SGX e sfruttando acceleratori come GPU/TPU. Tuttavia, nel prossimo futuro, sarà necessario un certo grado di approssimazione e compromesso sulle prestazioni per scalare MPC in modo da soddisfare le esigenze dei sistemi di apprendimento automatico del mondo reale.

### Generazione di Dati Sintetici

#### Idea Centrale

La generazione di dati sintetici è emersa come un importante approccio di apprendimento automatico per la tutela della privacy che consente di sviluppare e testare modelli senza esporre dati utente reali. L'idea chiave è quella di addestrare modelli generativi su set di dati reali e poi campionare da questi modelli per sintetizzare dati artificiali che corrispondono statisticamente alla distribuzione dei dati originali ma non contengono informazioni utente reali. Ad esempio, un GAN [Generative Adversarial Network] potrebbe essere addestrato su un set di dati di cartelle cliniche sensibili per apprendere i pattern sottostanti e quindi utilizzato per campionare dati sintetici dei pazienti.

La sfida principale della sintesi dei dati è garantire che gli avversari non siano in grado di identificare nuovamente il set di dati originale. Un approccio semplice per ottenere dati sintetici è aggiungere rumore al set di dati originale, che rischia comunque di far trapelare la privacy. Quando il rumore viene aggiunto ai dati nel contesto della privacy differenziale, vengono utilizzati meccanismi sofisticati basati sulla sensibilità dei dati per calibrare la quantità e la distribuzione del rumore. Attraverso questi framework matematicamente rigorosi, la Privacy differenziale generalmente garantisce la Privacy a un certo livello, che è l'obiettivo principale di questa tecnica di tutela della privacy. Oltre a preservare la privacy, i dati sintetici contrastano molteplici problemi di disponibilità dei dati, come set di dati sbilanciati, set di dati scarsi e rilevamento di anomalie.

I ricercatori possono condividere liberamente questi dati sintetici e collaborare alla modellazione senza rivelare informazioni mediche private. I dati sintetici ben costruiti proteggono la privacy, offrendo al contempo utilità per lo sviluppo di modelli accurati. Le tecniche chiave per impedire la ricostruzione dei dati originali includono l'aggiunta di rumore di privacy differenziale durante l'addestramento, l'applicazione di vincoli di plausibilità e l'utilizzo di più modelli generativi diversi. Ecco alcuni approcci comuni per la generazione di dati sintetici:

* **Generative Adversarial Network (GAN):** Le GAN sono un algoritmo di intelligenza artificiale utilizzato nell'apprendimento non supervisionato in cui due reti neurali competono tra loro in un gioco. @fig-gans è una panoramica del sistema GAN. La rete del generatore (grande riquadro rosso) è responsabile della produzione dei dati sintetici, mentre la rete del discriminatore (riquadro giallo) valuta l'autenticità dei dati distinguendo tra dati falsi creati dalla rete del generatore e dati reali. Le reti del generatore e del discriminatore apprendono e aggiornano i loro parametri in base ai risultati. Il discriminatore funge da metrica su quanto siano simili tra loro i dati falsi e quelli reali. È altamente efficace nel generare dati realistici ed è un approccio popolare per generare dati sintetici.

![Diagramma di flusso delle GAN. Fonte: @rosa2021.](images/png/Flowchart_of_GANs.png){#fig-gans}

* **Variational Autoencoder (VAE):** I VAE sono reti neurali in grado di apprendere complesse distribuzioni di probabilità e di bilanciare la qualità della generazione dei dati e l'efficienza computazionale. Codificano i dati in uno spazio latente in cui apprendono la distribuzione per decodificare i dati.

* **Data Augmentation:** Implica la trasformazione dei dati esistenti per creare nuovi dati modificati. Ad esempio, capovolgere, ruotare e ridimensionare (in modo uniforme o non uniforme) le immagini originali può aiutare a creare un set di dati di immagini più diversificato e robusto prima di addestrare un modello ML.

* **Simulazioni:** I modelli matematici possono simulare sistemi o processi del mondo reale per imitare fenomeni del mondo reale. Ciò è molto utile nella ricerca scientifica, nella pianificazione urbana e nell'economia.

#### Vantaggi

Sebbene i dati sintetici possano essere necessari a causa di rischi per la privacy o la conformità, sono ampiamente utilizzati nei modelli di apprendimento automatico quando i dati disponibili sono di scarsa qualità, scarsi o inaccessibili. I dati sintetici offrono uno sviluppo più efficiente ed efficace semplificando i processi di addestramento, test e distribuzione dei modelli robusti. Consentono ai ricercatori di condividere i modelli più ampiamente senza violare le leggi e le normative sulla privacy. La collaborazione tra gli utenti dello stesso set di dati sarà facilitata, il che aiuterà ad ampliare le capacità e i progressi nella ricerca ML.

Esistono diverse motivazioni per l'utilizzo di dati sintetici nell'apprendimento automatico:

* **Privacy e Conformità:** I dati sintetici evitano di esporre informazioni personali, consentendo una condivisione e una collaborazione più aperte. Ciò è importante quando si lavora con set di dati sensibili come cartelle cliniche o informazioni finanziarie.

* **Scarsità di dati:** Quando non sono disponibili dati reali sufficienti, i dati sintetici possono aumentare i set di dati di addestramento. Ciò migliora l'accuratezza del modello quando i dati limitati rappresentano un collo di bottiglia.

* **Test del modello:** I dati sintetici forniscono sandbox protetti dalla privacy per testare le prestazioni del modello, risolvere i problemi e monitorare i bias.

* **Etichettatura dei dati:** I dati di training etichettati di alta qualità sono spesso scarsi e costosi. I dati sintetici possono aiutare a generare automaticamente esempi etichettati.

#### Compromessi

Sebbene i dati sintetici cerchino di rimuovere qualsiasi prova del set di dati originale, la perdita della privacy rappresenta comunque un rischio, poiché i dati sintetici imitano i dati originali. Le informazioni statistiche e la distribuzione sono simili, se non uguali, tra i dati originali e sintetici. Ricampionando dalla distribuzione, gli avversari potrebbero comunque essere in grado di recuperare i campioni di addestramento originali. A causa dei loro processi di apprendimento e complessità intrinseci, le reti neurali potrebbero rivelare accidentalmente informazioni sensibili sui dati di addestramento originali.

Una sfida fondamentale con i dati sintetici è il potenziale divario tra le distribuzioni dei dati sintetici e quelli del mondo reale. Nonostante i progressi nelle tecniche di modellazione generativa, i dati sintetici potrebbero catturare solo parzialmente la complessità, la diversità e i pattern sfumati dei dati reali. Ciò può limitare l'utilità dei dati sintetici per l'addestramento robusto di modelli di apprendimento automatico. Valutare rigorosamente la qualità dei dati sintetici tramite metodi avversari e confrontare le prestazioni del modello con i benchmark dei dati reali aiuta a valutare e migliorare la fedeltà. Tuttavia, intrinsecamente, i dati sintetici rimangono un'approssimazione.

Un'altra preoccupazione critica sono i rischi per la privacy dei dati sintetici. I modelli generativi possono far trapelare informazioni identificabili sugli individui nei dati di training, il che potrebbe consentire la ricostruzione di informazioni private. Gli attacchi avversari emergenti dimostrano le sfide nel prevenire la perdita di identità dalle pipeline di generazione di dati sintetici. Tecniche come la privacy differenziale possono aiutare a salvaguardare la privacy, ma comportano compromessi nell'utilità dei dati. Esiste una tensione intrinseca tra la produzione di dati sintetici utili e la protezione completa dei dati di training sensibili, che devono essere bilanciati.

Ulteriori insidie dei dati sintetici includono distorsioni amplificate, etichettature errate, sovraccarico computazionale per l'addestramento di modelli generativi, costi di archiviazione e mancata contabilizzazione di nuovi dati fuori distribuzione. Sebbene questi siano secondari rispetto al divario sintetico-reale e ai rischi per la privacy, rimangono considerazioni importanti quando si valuta l'idoneità dei dati sintetici per particolari attività di apprendimento automatico. Come con qualsiasi tecnica, i vantaggi dei dati sintetici comportano compromessi e limitazioni intrinseche che richiedono strategie di mitigazione ponderate.

### Riepilogo

Sebbene tutte le tecniche di cui abbiamo discusso finora mirino a consentire un apprendimento automatico che salvaguardi la privacy, esse implicano meccanismi e compromessi distinti. Fattori come vincoli computazionali, ipotesi di fiducia richieste, modelli di minaccia e caratteristiche dei dati aiutano a guidare il processo di selezione per un caso d'uso particolare. Tuttavia, trovare il giusto equilibrio tra privacy, accuratezza ed efficienza richiede sperimentazione e valutazione empirica per molte applicazioni. @tbl-privacy-techniques è una tabella di confronto delle principali tecniche di apprendimento automatico che salvaguardano la privacy e dei loro pro e contro:

+---------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------+
| Tecnica                         | Pro                                                                    | Contro                                                                       |
+:================================+:=======================================================================+:=============================================================================+
| Privacy Differenziale           | - Forti garanzie formali di privacy                                    | - Perdita di accuratezza dovuta all'aggiunta di rumore                       |
|                                 | - Robusto per attacchi dati ausiliari                                  | - Overhead computazionale per analisi di sensibilità e generazione di rumore |
|                                 | - Versatile per molti tipi di dati e analisi                           |                                                                              |
+---------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------+
| Addestramento Federato          | - Consente l'apprendimento collaborativo senza condividere dati grezzi | - Overhead di comunicazione aumentato                                        |
|                                 | - I dati rimangono decentralizzati migliorando la sicurezza            | - Convergenza del modello potenzialmente più lenta                           |
|                                 | - Nessuna necessità di elaborazione crittografata                      | - Capacità di dispositivi client non uniformi                                |
+---------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------+
| Elaborazione Multi-Parte Sicura | - Consente elaborazione congiunta su dati sensibili                    | - Overhead computazionale molto elevato                                      |
|                                 | - Fornisce garanzie di privacy crittografica                           | - Complessità di implementazione                                             |
|                                 | - Protocolli flessibili per varie funzioni                             | - Vincoli algoritmici sulla profondità della funzione                        |
+---------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------+
| Crittografia Omomorfica         | - Consente il calcolo su dati crittografati                            | - Costi di calcolo estremamente elevati                                      |
|                                 | - Previene l'esposizione allo stato intermedio                         | - Implementazioni crittografiche complesse                                   |
|                                 |                                                                        | - Restrizioni sui tipi di funzione                                           |
+---------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------+
| Generazione di Dati Sintetici   | - Consente la condivisione dei dati senza "fughe"                      | - Divario sintetico-reale nelle distribuzioni                                |
|                                 | - Attenua i problemi di scarsità di dati                               | - Potenziale per la ricostruzione di dati privati                            |
|                                 |                                                                        | - Bias e problemi di etichettatura                                           |
+---------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------------+

: Confronto di tecniche per l'apprendimento automatico che tutela la privacy. {#tbl-privacy-techniques .striped .hover}

## Conclusione

La sicurezza hardware del machine learning è fondamentale poiché i sistemi ML embedded vengono sempre più implementati in domini critici per la sicurezza come dispositivi medici, controlli industriali e veicoli autonomi. Abbiamo esplorato varie minacce che spaziano da bug hardware, attacchi fisici, canali laterali, rischi della supply chain, ecc. Difese come TEE, Secure Boot, PUF e moduli di sicurezza hardware forniscono una protezione multi-livello su misura per dispositivi embedded con risorse limitate.

Tuttavia, una vigilanza continua è essenziale per tracciare i vettori di attacco emergenti e affrontare potenziali vulnerabilità tramite pratiche di ingegneria sicure durante l'intero ciclo di vita dell'hardware. Man mano che ML e ML embedded si diffondono, il mantenimento di rigorose basi di sicurezza che corrispondano al ritmo accelerato di innovazione del settore rimane imperativo.

## Risorse {#sec-security-and-privacy-resource}

Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.

:::{.callout-note collapse="false"}

#### Slide

Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.

* [Security.](https://docs.google.com/presentation/d/1jZBi8DS1NUXFdIwNGwofzA4CYRei6lH2e56BOu098-k/edit#slide=id.g1ff987f3d96_0_0)

* [Privacy.](https://docs.google.com/presentation/d/1Wp-5eO4Bmco2f7ppNKsRkE1utuz22PeLvVoREFSChR8/edit#slide=id.g202a5aaf418_0_0)

* [Monitoring after Deployment.](https://docs.google.com/presentation/d/1WlQdk40zJcW9Bx6ua-vKu3sDrMU_iI89BQGMGk6OEB0/edit?usp=drive_link)

:::

:::{.callout-important collapse="false"}

#### Video

* @vid-jeephack

* @vid-mirai

* @vid-powerattack

:::

:::{.callout-caution collapse="false"}

#### Esercizi

Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.

* @exr-dptf

* @exr-he
:::

:::{.callout-warning collapse="false"}

#### Laboratori

Oltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l'esperienza di apprendimento.

* _Prossimamente._
:::

