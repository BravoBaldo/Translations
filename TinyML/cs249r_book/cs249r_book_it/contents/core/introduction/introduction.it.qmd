---
bibliography: introduction.bib
---

# Introduzione {#sec-introduction}

![_DALL·E 3 Prompt: Un'illustrazione 2D dettagliata, rettangolare e piatta che raffigura una roadmap dei capitoli di un libro sui sistemi di apprendimento automatico, su uno sfondo bianco nitido e pulito. L'immagine presenta una strada tortuosa che attraversa vari punti di riferimento simbolici. Ogni punto di riferimento rappresenta un argomento del capitolo: Introduzione, Sistemi di ML, Avvio al Deep Learning, Workflow dell'IA, Data Engineering, Framework di IA, Addestramento dell'IA, IA Efficiente, Ottimizzazioni dei Modelli, Accelerazione IA, Benchmarking dell'IA, Apprendimento On-Device, Operazioni di ML, Sicurezza e Privacy, IA Responsabile, IA Sostenibile, AI for Good, IA Robusta, IA Generativa. Lo stile è pulito, moderno e piatto, adatto a un libro tecnico, con ogni punto di riferimento chiaramente etichettato con il titolo del capitolo._](images/png/cover_introduction.png)

## Perché i Sistemi di Machine Learning Sono Importanti

L'intelligenza artificiale è ovunque. Si pensi alla routine mattutina: Ci si sveglia con una sveglia intelligente basata sull'IA che ha appreso i propri schemi di sonno. Il telefono suggerisce il percorso per andare al lavoro, avendo appreso dai pattern del traffico. Durante il tragitto, l'app musicale crea automaticamente una playlist che si pensa piacerà. Al lavoro, il client di posta elettronica filtra lo spam e dà priorità ai messaggi importanti. Durante il giorno, lo smartwatch monitora l'attività, suggerendo quando muoversi o fare esercizio. La sera, il servizio di streaming consiglia programmi che potrebbero piacere, mentre i dispositivi smart home regolano l'illuminazione e la temperatura in base alle preferenze apprese.

Ma queste comodità quotidiane sono solo l'inizio. L'IA sta trasformando il mondo in modi straordinari. Oggi, i sistemi di IA rilevano tumori in fase iniziale con una precisione senza precedenti, prevedono e tracciano eventi meteorologici estremi per salvare vite e accelerano la scoperta di farmaci simulando milioni di interazioni molecolari. I veicoli autonomi percorrono strade cittadine complesse elaborando dati di sensori in tempo reale da decine di fonti. I modelli linguistici si impegnano in conversazioni sofisticate, traducono tra centinaia di lingue e aiutano gli scienziati ad analizzare vasti database di ricerca. Nei laboratori scientifici, i sistemi di IA stanno facendo scoperte rivoluzionarie, dalla previsione di strutture proteiche che sbloccano nuovi trattamenti medici all'identificazione di materiali promettenti per celle solari e batterie di nuova generazione. Anche nei campi creativi, l'IA collabora con artisti e musicisti per esplorare nuove forme di espressione, spingendo i confini della creatività umana.

Questa non è fantascienza, è la realtà di come l'intelligenza artificiale, in particolare i sistemi di apprendimento automatico, si sia intrecciata nel tessuto della nostra vita quotidiana. All'inizio degli anni '90, [Mark Weiser](https://en.wikipedia.org/wiki/Mark_Weiser), un pioniere dell'informatica, ha introdotto il mondo a un concetto rivoluzionario che avrebbe cambiato per sempre il modo in cui interagiamo con la tecnologia. Questa visione è stata riassunta in modo sintetico nel suo articolo fondamentale, "The Computer for the 21st Century" (@fig-ubiquitous). Weiser immaginava un futuro in cui l'informatica sarebbe stata perfettamente integrata nei nostri ambienti, diventando una parte invisibile e integrante della vita quotidiana.

![Ubiquitous computing as envisioned di Mark Weiser.](images/png/21st_computer.png){#fig-ubiquitous width=50%}

Ha definito questo concetto "ubiquitous computing", promettendo un mondo in cui la tecnologia ci avrebbe servito senza richiedere la nostra costante attenzione o interazione. Oggi ci ritroviamo a vivere nel futuro immaginato da Weiser, in gran parte reso possibile dai sistemi di apprendimento automatico. La vera essenza della sua visione, ovvero creare un ambiente intelligente in grado di anticipare le nostre esigenze e agire per nostro conto, è diventata realtà attraverso lo sviluppo e l'implementazione di sistemi di ML che abbracciano interi ecosistemi, dai potenti data center cloud ai dispositivi edge fino ai più piccoli sensori IoT.

Eppure la maggior parte di noi raramente pensa ai sistemi complessi che rendono possibile tutto questo. Dietro ciascuna di queste interazioni apparentemente semplici si nasconde una sofisticata infrastruttura di dati, algoritmi e risorse informatiche che lavorano insieme. Comprendere come funzionano questi sistemi, le loro capacità, limitazioni e requisiti, è diventato sempre più critico man mano che si integrano sempre di più nel nostro mondo.

Per apprezzare l'entità di questa trasformazione e la complessità dei moderni sistemi di machine learning, dobbiamo capire come siamo arrivati fin qui. Il viaggio dall'intelligenza artificiale primitiva agli odierni sistemi ML onnipresenti è una storia non solo di evoluzione tecnologica, ma anche di prospettive mutevoli su ciò che è possibile e ciò che è necessario per rendere l'IA pratica e affidabile.

## L'evoluzione dell'IA

L'evoluzione dell'IA, rappresentata nella cronologia mostrata in @fig-ai-timeline, evidenzia traguardi chiave come lo sviluppo del **perceptron**[^defn-perceptron] nel 1957 da parte di Frank Rosenblatt, un elemento fondamentale per le moderne reti neurali. Si immagini di entrare in un laboratorio informatico nel 1965. Si troveranno mainframe delle dimensioni di una stanza che eseguono programmi in grado di dimostrare teoremi matematici di base o di giocare a semplici giochi come il tris. Questi primi sistemi di intelligenza artificiale, pur essendo rivoluzionari per l'epoca, erano ben lontani dagli odierni sistemi di apprendimento automatico in grado di rilevare il cancro nelle immagini mediche o di comprendere il linguaggio umano. La cronologia mostra la progressione dalle prime innovazioni come il chatbot ELIZA nel 1966, a importanti innovazioni come Deep Blue di IBM che ha sconfitto il campione di scacchi Garry Kasparov nel 1997. I progressi più recenti includono l'introduzione di GPT-3 di OpenAI nel 2020 e GPT-4 nel 2023, dimostrando la drammatica evoluzione e la crescente complessità dei sistemi di IA nel corso dei decenni.

[^defn-perceptron]: La prima rete neurale artificiale, un modello semplice che potrebbe imparare a classificare schemi visivi, simile a un singolo neurone che prende una decisione sì/no in base ai suoi input.

![Pietre miliari nell'IA dal 1950 al 2020. Fonte: IEEE Spectrum](https://spectrum.ieee.org/media-library/a-chart-of-milestones-in-ai-from-1950-to-2020.png?id=27547255){#fig-ai-timeline}

Esploriamo come siamo arrivati fin qui.

### IA Simbolica (1956-1974)

La storia del machine learning inizia alla storica conferenza di Dartmouth del 1956, dove pionieri come John McCarthy, Marvin Minsky e Claude Shannon coniarono per primi il termine "intelligenza artificiale". Il loro approccio si basava su un'idea convincente: l'intelligenza poteva essere ridotta alla manipolazione dei simboli. Si consideri il sistema STUDENT di Daniel Bobrow del 1964, uno dei primi programmi di IA in grado di risolvere problemi di algebra:

::: {.callout-note}
### Esempio: STUDENT (1964)

```
Problema: "Se il numero di clienti che Tom ottiene è il doppio del 
quadrato del 20% del numero di pubblicità che gestisce e 
il numero di pubblicità è 45, qual è il numero di
clienti che Tom ottiene?"
 
STUDENT dovrebbe:

1. Analizzare il testo
2. Convertirlo in equazioni algebriche
3. Risolvere l'equazione: n = 2(0.2 × 45)²
4. Fornire la risposta: 162 clienti
```
:::

Le prime IA come STUDENT soffrivano di una limitazione fondamentale: potevano gestire solo input che corrispondevano esattamente ai loro schemi e regole pre-programmati. Si immagini un traduttore linguistico che funziona solo quando le frasi seguono una struttura grammaticale perfetta: anche piccole variazioni come cambiare l'ordine delle parole, usare sinonimi o schemi di linguaggio naturali causerebbero il fallimento di STUDENT. Questa "fragilità" significava che, mentre queste soluzioni potevano apparire intelligenti quando gestivano casi molto specifici per cui erano state progettate, si sarebbero completamente guastate quando si trovavano di fronte anche a piccole variazioni o complessità del mondo reale.
 Questa limitazione non era solo un inconveniente tecnico, ma rivelava un problema più profondo con gli approcci basati su regole all'IA: non potevano realmente comprendere o generalizzare dalla loro programmazione, potevano solo abbinare e manipolare i modelli esattamente come specificato.

### Sistemi Esperti (anni '70-'80)

Verso la metà degli anni '70, i ricercatori si resero conto che l'IA generale era troppo ambiziosa. Si concentraronoinvece, sull'acquisizione di conoscenze esperte umane in domini specifici. MYCIN, sviluppato a Stanford, è stato uno dei primi sistemi esperti su larga scala progettati per diagnosticare le infezioni del sangue:

::: {.callout-note}
### Esempio: MYCIN (1976)
```
Esempio di regola da MYCIN:
IF
    L'infezione è una batteriemia primaria
    Il sito della coltura è uno dei siti sterili
    Il presunto portale di ingresso è il tratto gastrointestinale
THEN
    Ci sono prove suggestive (0,7) che l'infezione è batterioide
```
:::

Mentre MYCIN ha rappresentato un importante progresso nell'IA medica con le sue 600 regole esperte per la diagnosi delle infezioni del sangue, ha rivelato sfide fondamentali che ancora oggi affliggono l'apprendimento automatico. Ottenere la conoscenza del dominio da esperti umani e convertirla in regole precise si è rivelato incredibilmente dispendioso in termini di tempo e difficile: i medici spesso non riuscivano a spiegare esattamente come prendevano le decisioni. MYCIN ha lottato con informazioni incerte o incomplete, a differenza dei medici umani che potevano fare ipotesi istruite. Forse la cosa più importante è che la manutenzione e l'aggiornamento della base di regole sono diventati esponenzialmente più complessi con la crescita di MYCIN: l'aggiunta di nuove regole spesso entrava in conflitto con quelle esistenti e la conoscenza medica stessa continuava a evolversi. Queste stesse sfide di acquisizione della conoscenza, gestione dell'incertezza e manutenzione rimangono preoccupazioni centrali nell'apprendimento automatico moderno, anche se ora utilizziamo approcci tecnici diversi per affrontarle.

### Apprendimento Statistico: Un Cambio di Paradigma (anni '90)

Gli anni '90 hanno segnato una trasformazione radicale nell'intelligenza artificiale, poiché il campo si è spostato dalle regole codificate a mano verso approcci di apprendimento statistico. Questa non è stata una scelta semplice: è stata guidata da tre fattori convergenti che hanno reso i metodi statistici possibili e potenti. La rivoluzione digitale ha significato che enormi quantità di dati erano improvvisamente disponibili per addestrare gli algoritmi. La **legge di Moore**[^defn-mooreslaw] ha fornito la potenza di calcolo necessaria per elaborare questi dati in modo efficace. E i ricercatori hanno sviluppato nuovi algoritmi come le "Support Vector Machines" [macchine a vettori di supporto] e reti neurali migliorate che potevano effettivamente apprendere modelli da questi dati anziché seguire regole pre-programmate. Questa combinazione ha cambiato radicalmente il modo in cui abbiamo costruito l'IA: invece di cercare di codificare direttamente la conoscenza umana, ora potevamo lasciare che le macchine scoprissero automaticamente i pattern da esempi, portando a un'IA più solida e adattabile.

[^defn-mooreslaw]: L'osservazione fatta dal co-fondatore di Intel Gordon Moore nel 1965 secondo cui il numero di transistor su un microchip raddoppia circa ogni due anni, mentre il costo si dimezza. Questa crescita esponenziale della potenza di calcolo è stata un fattore chiave dei progressi nell'apprendimento automatico, sebbene il ritmo abbia iniziato a rallentare negli ultimi anni.

Si consideri come si è evoluto il filtraggio dello spam via e-mail:

::: {.callout-note}
### Esempio: Sistemi di Rilevamento Precoce dello Spam

```
Basati su regole (anni '80):
IF contains("viagra") OR contains("winner") THEN spam

Statistici (anni '90):
P(spam|word) = (frequency in spam emails) / (total frequency)

Combinati usando Naive Bayes:
P(spam|email) ∝ P(spam) × ∏ P(word|spam)
```
:::

Il passaggio agli approcci statistici ha cambiato radicalmente il nostro modo di pensare alla creazione di IA introducendo tre concetti fondamentali che rimangono importanti ancora oggi. In primo luogo, la qualità e la quantità dei dati di addestramento sono diventati importanti quanto gli algoritmi stessi: l'IA poteva apprendere solo i modelli presenti nei suoi esempi di addestramento. In secondo luogo, avevamo bisogno di metodi rigorosi per valutare quanto bene l'IA funzionasse effettivamente, portando a metriche che potessero misurare il successo e confrontare diversi approcci. In terzo luogo, abbiamo scoperto una tensione intrinseca tra precisione (avere ragione quando facciamo una previsione) e richiamo (catturare tutti i casi che dovremmo trovare), costringendo i progettisti a fare compromessi espliciti in base alle esigenze della loro applicazione. Ad esempio, un filtro antispam potrebbe tollerare un po' di spam per evitare di bloccare e-mail importanti, mentre la diagnosi medica potrebbe dover catturare ogni potenziale caso anche se ciò significa più falsi allarmi.

@tbl-ai-evolution-strengths racchiude il percorso evolutivo degli approcci di IA di cui abbiamo discusso finora, evidenziando i punti di forza e le capacità chiave emersi con ogni nuovo paradigma. Spostandoci da sinistra a destra nella tabella, possiamo osservare diverse tendenze importanti. Parleremo di apprendimento "superficiale" e "profondo" in seguito, ma è utile comprendere i compromessi tra gli approcci trattati finora.

+--------------------------+------------------------------------------+-------------------------------------+-----------------------------------+----------------------------------------------+
| Aspetto                  | IA simbolica                             | Sistemi esperti                     | Apprendimento statistico          | Apprendimento superficiale/profondo          |
+:=========================+:=========================================+:====================================+:==================================+:=============================================+
| Punto di forza           | Ragionamento logico                      | Competenza di dominio               | Versatilità                       | Riconoscimento di pattern                    |
+--------------------------+------------------------------------------+-------------------------------------+-----------------------------------+----------------------------------------------+
| Miglior caso d'uso       | Problemi ben definiti e basati su regole | Problemi di dominio specifici       | Vari problemi di dati strutturati | Problemi di dati complessi e non strutturati |
+--------------------------+------------------------------------------+-------------------------------------+-----------------------------------+----------------------------------------------+
| Gestione dei dati        | Dati minimi necessari                    | Basato sulla conoscenza del dominio | Dati moderati richiesti           | Elaborazione dati su larga scala             |
+--------------------------+------------------------------------------+-------------------------------------+-----------------------------------+----------------------------------------------+
| Adattabilità             | Regole fisse                             | Adattabilità specifica del dominio  | Adattabile a vari domini          | Altamente adattabile a diverse attività      |
+--------------------------+------------------------------------------+-------------------------------------+-----------------------------------+----------------------------------------------+
| Complessità del problema | Semplice, basato sulla logica            | Complicato, specifico del dominio   | Complesso, strutturato            | Altamente complesso, non strutturato         |
+--------------------------+------------------------------------------+-------------------------------------+-----------------------------------+----------------------------------------------+

: Evoluzione dell'IA - Aspetti Chiave Positivi {#tbl-ai-evolution-strengths .hover .striped}

La tabella funge da ponte tra i primi approcci di cui abbiamo parlato e gli sviluppi più recenti nell'apprendimento superficiale e profondo che esploreremo in seguito. Pone le basi per comprendere perché determinati approcci hanno acquisito importanza in epoche diverse e come ogni nuovo paradigma si è basato e ha affrontato i limiti dei suoi predecessori. Inoltre, illustra come i punti di forza degli approcci precedenti continuino a influenzare e migliorare le moderne tecniche di IA, in particolare nell'era dei modelli di fondazione.

### Apprendimento Superficiale (anni 2000)

Gli anni 2000 hanno segnato un periodo affascinante nella storia del machine learning che ora chiamiamo l'era dello ""shallow learning" [apprendimento superficiale]. Per capire perché è "superficiale", si immagini di costruire una casa: il "deep learning " [apprendimento profondo] (che è arrivato dopo) è come avere più squadre di costruzione che lavorano a diversi livelli contemporaneamente, ciascuna squadra impara dal lavoro delle squadre sottostanti. Al contrario, l'apprendimento superficiale in genere aveva solo uno o due livelli di elaborazione, come avere solo una squadra di fondazione e una squadra di intelaiatura.

Durante questo periodo, diversi potenti algoritmi hanno dominato il panorama dell'apprendimento automatico. Ognuno ha portato punti di forza unici a problemi diversi: I "decision trees" [alberi decisionali] hanno fornito risultati interpretabili prendendo decisioni molto simili a un diagramma di flusso. I "K-nearest neighbors" hanno fatto previsioni trovando esempi simili nei dati passati, come chiedere consiglio ai vicini più esperti. La regressione lineare e logistica hanno offerto modelli semplici e interpretabili che hanno funzionato bene per molti problemi del mondo reale. Le "Support Vector Machine (SVM)" eccellevano nel trovare confini complessi tra categorie usando il "trucco del kernel": Si immagini di poter districare una ciotola di spaghetti in linee rette sollevandola in una dimensione superiore. Questi algoritmi hanno costituito la base della macchina pratica.

Si consideri una tipica soluzione di visione artificiale del 2005:

::: {.callout-note}
### Esempio: Pipeline di Visione Artificiale Tradizionale
```
1. Estrazione Manuale delle Feature
   - SIFT (Scale-Invariant Feature Transform)
   - HOG (Histogram of Oriented Gradients)
   - Filtri di Gabor
2. Selezione/Ingegnerizzazione delle Feature
3. Modello di Apprendimento "Superficiale" (ad esempio, SVM)
4. Post-elaborazione
```
:::

Ciò che ha reso questa era unica è stato il suo approccio ibrido: caratteristiche ingegnerizzate dall'uomo combinate con l'apprendimento statistico. Avevano solide basi matematiche (i ricercatori potevano dimostrare perché funzionavano). Hanno funzionato bene anche con dati limitati. Erano efficienti dal punto di vista computazionale. Hanno prodotto risultati affidabili e riproducibili.

Prendiamo l'esempio del rilevamento dei volti, in cui l'algoritmo Viola-Jones (2001) ha ottenuto prestazioni in tempo reale utilizzando semplici "feature" [caratteristiche] rettangolari e una cascata di classificatori. Questo algoritmo ha alimentato il rilevamento dei volti delle fotocamere digitali per quasi un decennio.

### Deep Learning (2012-Oggi)

Mentre le "Support Vector Machine" eccellevano nel trovare confini complessi tra categorie usando trasformazioni matematiche, il deep learning ha adottato un approccio radicalmente diverso ispirato all'architettura del cervello umano. Il deep learning è costruito da "layer" [strati] di neuroni artificiali, dove ogni layer impara a trasformare i suoi dati di input in rappresentazioni sempre più astratte. Si immagini di elaborare un'immagine di un gatto: il primo layer potrebbe imparare a rilevare semplici bordi e contrasti, il layer successivo li combina in forme e texture di base, un altro layer potrebbe riconoscere baffi e orecchie a punta e quelli finali assemblano queste caratteristiche nel concetto di "gatto". A differenza dei metodi di apprendimento superficiali che richiedevano agli esseri umani di progettare attentamente le feature, le reti di deep learning possono scoprire automaticamente feature utili direttamente dai dati grezzi. Questa capacità di apprendere rappresentazioni gerarchiche, da semplici a complesse, da concrete ad astratte, è ciò che rende il deep learning "deep" [profondo] e si è rivelato un approccio straordinariamente potente per la gestione di dati complessi del mondo reale come immagini, discorsi e testo.

Nel 2012, una rete neurale profonda chiamata AlexNet, mostrata in @fig-alexnet, ha raggiunto una svolta nella competizione ImageNet che avrebbe trasformato il campo del machine learning. La sfida era formidabile: classificare correttamente 1,2 milioni di immagini ad alta risoluzione in 1.000 categorie diverse. Mentre gli approcci precedenti hanno lottato con tassi di errore superiori al 25%, AlexNet ha raggiunto un tasso di errore del 15,3%, superando notevolmente tutti i metodi esistenti.

![Architettura della rete neurale profonda per Alexnet. Fonte: @krizhevsky2012imagenet](./images/png/alexnet_arch.png){#fig-alexnet}

Il successo di AlexNet non è stato solo un risultato tecnico, è stato un momento spartiacque che ha dimostrato la fattibilità pratica del deep learning. Ha dimostrato che con dati sufficienti, potenza di calcolo e innovazioni architettoniche, le reti neurali potevano superare le funzionalità progettate a mano e i metodi di apprendimento superficiale che avevano dominato il campo per decenni. Questo singolo risultato ha innescato un'esplosione di ricerca e applicazioni nel deep learning che continua ancora oggi.

Da questa base, il deep learning è entrato in un'era di portata senza precedenti. Verso la fine del 2010, aziende come Google, Facebook e OpenAI stavano addestrando reti neurali migliaia di volte più grandi di **AlexNet**[^defn-alexnet]. Questi modelli massicci, spesso chiamati "foundation models" [modelli di base], hanno portato il deep learning a nuovi livelli. GPT-3, rilasciato nel 2020, conteneva 175 miliardi di **parametri**[^defn-parameters]---si immagini uno studente che potesse leggere l'intera Wikipedia più volte e apprendere modelli da ogni articolo. Questi modelli hanno mostrato capacità straordinarie: scrivere testi simili a quelli umani, impegnarsi in conversazioni, generare immagini da descrizioni e persino scrivere codice per computer. L'intuizione chiave era semplice ma potente: man mano che ingrandivamo le reti neurali e fornivamo loro più dati, queste diventavano in grado di risolvere attività sempre più complesse. Tuttavia, questa scala ha portato sfide di sistema senza precedenti: come si addestrano in modo efficiente modelli che richiedono migliaia di GPU che lavorano in parallelo? Come si archiviano e si forniscono modelli di centinaia di gigabyte di dimensioni? Come si gestiscono gli enormi set di dati necessari per l'addestramento?

[^defn-alexnet]: Una rivoluzionaria rete neurale profonda del 2012 che ha vinto la [ImageNet competition](https://www.image-net.org/challenges/LSVRC/) con un ampio margine e ha contribuito a innescare la rivoluzione del deep learning.

[^defn-parameters]: Simile a come le connessioni neurali del cervello diventano più forti man mano che si apprende una nuova abilità, avere più parametri significa generalmente che il modello può apprendere schemi più complessi.

La rivoluzione del deep learning del 2012 non è emersa dal nulla, ma è stata costruita sulla ricerca sulle reti neurali risalente agli anni '50. La storia inizia con il Perceptron di Frank Rosenblatt nel 1957, che ha catturato l'immaginazione dei ricercatori mostrando come un semplice neurone artificiale potesse imparare a classificare gli schemi. Sebbene potesse gestire solo problemi linearmente separabili, una limitazione drammaticamente evidenziata dal libro del 1969 di Minsky e Papert "Perceptrons", ha introdotto il concetto fondamentale di reti neurali addestrabili. Gli anni '80 portarono altre importanti scoperte: Rumelhart, Hinton e Williams introdussero la backpropagation nel 1986, fornendo un modo sistematico per addestrare reti multi-layer, mentre Yann LeCun ne dimostrò l'applicazione pratica nel riconoscimento di cifre scritte a mano utilizzando "**convolutional neural networks (CNN)" [reti neurali convoluzionali]**[^defn-cnn].

[^defn-cnn]: Un tipo di rete neurale appositamente progettata per l'elaborazione di immagini, ispirata al funzionamento del sistema visivo umano. La parte "convoluzionale" si riferisce al modo in cui analizza le immagini in piccoli blocchi, in modo simile a come i nostri occhi si concentrano su diverse parti di una scena.

:::{#vid-tl .callout-important}

# Demo di Rete Convoluzionale del 1989

{{< video https://www.youtube.com/watch?v=FwFduRA_L6Q&ab_channel=YannLeCun >}}

:::

Tuttavia, queste reti sono rimaste in gran parte inattive negli anni '90 e 2000, non perché le idee fossero sbagliate, ma perché erano avanti coi tempi: il campo mancava di tre ingredienti importanti: dati sufficienti per addestrare reti complesse, potenza di calcolo sufficiente per elaborare questi dati e le innovazioni tecniche necessarie per addestrare efficacemente reti molto profonde.

Il campo ha dovuto attendere la convergenza dei big data, hardware di elaborazione migliore e innovazioni algoritmiche prima che il potenziale del deep learning potesse essere sbloccato. Questo lungo periodo di gestazione aiuta a spiegare perché il momento ImageNet del 2012 è stato meno una rivoluzione improvvisa e più il culmine di decenni di ricerca accumulata che ha finalmente trovato il suo momento. Come esploreremo nelle sezioni seguenti, questa evoluzione ha portato a due sviluppi significativi nel campo. In primo luogo, ha dato origine alla definizione del campo dell'ingegneria dei sistemi di apprendimento automatico, una disciplina che insegna come colmare il divario tra progressi teorici e implementazione pratica. In secondo luogo, ha reso necessaria una definizione più completa dei sistemi di apprendimento automatico, che comprenda non solo gli algoritmi, ma anche i dati e l'infrastruttura informatica. Le attuali sfide di scala riecheggiano molte delle stesse domande fondamentali su metodi di calcolo, dati e apprendimento con cui i ricercatori si sono confrontati sin dall'inizio del campo, ma ora all'interno di un quadro più complesso e interconnesso.

## L'Ascesa dell'Ingegneria dei Sistemi di ML

La storia che abbiamo tracciato, dai primi giorni del Perceptron alla rivoluzione del deep learning, è stata in gran parte una storia di innovazioni algoritmiche. Ogni epoca ha portato nuove intuizioni matematiche e approcci di modellazione che hanno ampliato i confini di ciò che l'IA poteva raggiungere. Ma qualcosa di importante è cambiato nell'ultimo decennio: il successo dei sistemi di IA è diventato sempre più dipendente non solo dalle innovazioni algoritmiche, ma anche da un'ingegneria sofisticata.

Questo cambiamento rispecchia l'evoluzione dell'informatica e dell'ingegneria alla fine degli anni '60 e all'inizio degli anni '70. Durante quel periodo, man mano che i sistemi informatici diventavano più complessi, emerse una nuova disciplina: la "Computer Engineering" [ingegneria informatica]. Questo campo colmò il divario tra l'esperienza hardware dell'Ingegneria Elettrica e l'attenzione dell'Informatica su algoritmi e software. L'Ingegneria Informatica nacque perché le sfide della progettazione e della costruzione di sistemi informatici complessi richiedevano un approccio integrato che nessuna delle due discipline poteva affrontare completamente da sola.

Oggi, stiamo assistendo a una transizione simile nel campo dell'IA. Mentre l'Informatica continua a spingere i confini degli algoritmi di ML e l'Ingegneria Elettrica fa progredire l'hardware di IA specializzato, nessuna delle due discipline affronta completamente i principi di ingegneria necessari per distribuire, ottimizzare e sostenere i sistemi di ML su larga scala. Questa lacuna evidenzia la necessità di una nuova disciplina: la "Machine Learning Systems Engineering" [ingegneria dei sistemi di apprendimento automatico].

Non esiste una definizione esplicita di cosa sia questo campo oggi, ma può essere ampiamente definito come tale:

:::{.callout-tip}
### Definizione di Machine Learning Systems Engineering

La "Machine Learning Systems Engineering (MLSysEng)" è la disciplina di progettazione, implementazione e gestione di sistemi di intelligenza artificiale su scale di elaborazione, dai dispositivi embedded con risorse limitate ai computer su scala industriale. Questo campo integra i principi delle discipline ingegneristiche che spaziano dall'hardware al software per creare sistemi affidabili, efficienti e ottimizzati per il loro contesto di distribuzione. Comprende il ciclo di vita completo delle applicazioni AI: dall'ingegneria dei requisiti e dalla raccolta dati allo sviluppo di modelli, all'integrazione di sistemi, alla distribuzione, al monitoraggio e alla manutenzione. Il campo enfatizza i principi ingegneristici di progettazione sistematica, vincoli di risorse, requisiti di prestazioni e affidabilità operativa.
:::

Consideriamo l'esplorazione spaziale. Mentre gli astronauti si avventurano in nuove frontiere ed esplorano le vaste incognite dell'universo, le loro scoperte sono possibili solo grazie ai complessi sistemi di ingegneria che li supportano: i razzi che li sollevano nello spazio, i sistemi di supporto vitale che li mantengono in vita e le reti di comunicazione che li mantengono connessi alla Terra. Allo stesso modo, mentre i ricercatori di IA spingono i confini di ciò che è possibile con gli algoritmi di apprendimento, le loro scoperte diventano realtà pratica solo attraverso un'attenta ingegneria dei sistemi. I moderni sistemi di IA necessitano di un'infrastruttura solida per raccogliere e gestire i dati, di potenti sistemi di elaborazione per addestrare i modelli e di piattaforme di distribuzione affidabili per servire milioni di utenti.

Questa emergenza dell'ingegneria dei sistemi di apprendimento automatico come disciplina importante riflette una realtà più ampia: trasformare gli algoritmi di IA in sistemi del mondo reale richiede di colmare il divario tra possibilità teoriche e implementazione pratica. Non basta avere un algoritmo brillante se non si riesce a raccogliere ed elaborare in modo efficiente i dati necessari, distribuirne il calcolo su centinaia di macchine, servirlo in modo affidabile a milioni di utenti o monitorarne le prestazioni in produzione.

Comprendere questa interazione tra algoritmi e ingegneria è diventato fondamentale per i moderni professionisti dell'IA. Mentre i ricercatori continuano a spingere i confini di ciò che è algoritmicamente possibile, gli ingegneri stanno affrontando la complessa sfida di far funzionare questi algoritmi in modo affidabile ed efficiente nel mondo reale. Questo ci porta a una domanda fondamentale: cos'è esattamente un sistema di "machine learning" [apprendimento automatico] e cosa lo rende diverso dai tradizionali sistemi software?

## Definizione di un sistema ML

Non esiste una definizione univoca e universalmente accettata di un sistema di apprendimento automatico. Questa ambiguità deriva dal fatto che diversi professionisti, ricercatori e settori spesso fanno riferimento ai sistemi di apprendimento automatico in contesti diversi e con ambiti diversi. Alcuni potrebbero concentrarsi esclusivamente sugli aspetti algoritmici, mentre altri potrebbero includere l'intera pipeline dalla raccolta dati all'implementazione del modello. Questo uso approssimativo del termine riflette la natura multidisciplinare e in rapida evoluzione del campo.

Data questa diversità di prospettive, è importante stabilire una definizione chiara e completa che comprenda tutti questi aspetti. In questo libro, adottiamo un approccio olistico ai sistemi di apprendimento automatico, considerando non solo gli algoritmi ma anche l'intero ecosistema in cui operano. Pertanto, definiamo un sistema di apprendimento automatico come segue:

:::{.callout-tip}
### Definizione di un Sistema di Machine Learning

Un sistema di "machine learning" [apprendimento automatico] è un sistema di elaborazione integrato che comprende tre componenti principali: (1) dati che guidano il comportamento algoritmico, (2) algoritmi di apprendimento che estraggono modelli da questi dati e (3) infrastruttura di elaborazione che consente sia il processo di apprendimento (ad esempio, addestramento) sia l'applicazione della conoscenza appresa (ad esempio, inferenza/servizio). Insieme, questi componenti creano un sistema di elaborazione in grado di fare previsioni, generare contenuti o intraprendere azioni in base a modelli appresi.
:::

Il nucleo di qualsiasi sistema di apprendimento automatico è costituito da tre componenti interrelati, come illustrato in @fig-ai-triangle: modelli/algoritmi, dati e infrastruttura informatica. Questi componenti formano una dipendenza triangolare in cui ogni elemento plasma fondamentalmente le possibilità degli altri. L'architettura del modello detta sia le richieste computazionali per l'addestramento e l'inferenza, sia il volume e la struttura dei dati richiesti per un apprendimento efficace. La scala e la complessità dei dati influenzano l'infrastruttura necessaria per l'archiviazione e l'elaborazione, determinando contemporaneamente quali architetture del modello sono fattibili. Le capacità dell'infrastruttura stabiliscono limiti pratici sia sulla scala del modello che sulla capacità di elaborazione dei dati, creando un framework all'interno del quale devono operare gli altri componenti.

![I sistemi di apprendimento automatico coinvolgono algoritmi, dati e calcoli, tutti interconnessi tra loro.](images/png/triangle.png){#fig-ai-triangle}

Ciascuno di questi componenti ha uno scopo distinto ma interconnesso:

- **Algoritmi:** Modelli matematici e metodi che apprendono pattern dai dati per fare previsioni o decisioni

- **Dati:** Processi e infrastrutture per la raccolta, l'archiviazione, l'elaborazione, la gestione e la fornitura di dati sia per l'addestramento che per l'inferenza.

- **Calcolo:** Infrastruttura hardware e software che consente l'addestramento, la fornitura e il funzionamento efficienti di modelli su larga scala.

L'interdipendenza di questi componenti significa che nessun singolo elemento può funzionare in modo isolato. L'algoritmo più sofisticato non può apprendere senza dati o risorse di elaborazione su cui eseguire. I set di dati più grandi sono inutili senza algoritmi per estrarre modelli o infrastrutture per elaborarli. E l'infrastruttura di elaborazione più potente non serve a nulla senza algoritmi da eseguire o dati da elaborare.

Per illustrare queste relazioni, possiamo fare un paragone con l'esplorazione spaziale. Gli sviluppatori di algoritmi sono come gli astronauti: esplorano nuove frontiere e fanno scoperte. I team di data science funzionano come specialisti del controllo missione, assicurando il flusso costante di informazioni e risorse critiche necessarie per far funzionare la missione. Gli ingegneri delle infrastrutture informatiche sono come gli ingegneri missilistici: progettano e costruiscono i sistemi che rendono possibile la missione. Proprio come una missione spaziale richiede l'integrazione perfetta di astronauti, controllo missione e sistemi missilistici, un sistema di apprendimento automatico richiede l'attenta orchestrazione di algoritmi, dati e infrastrutture informatiche.

## Il ciclo di vita dei sistemi ML

I sistemi software tradizionali seguono un ciclo di vita prevedibile in cui gli sviluppatori scrivono istruzioni esplicite che i computer devono eseguire. Questi sistemi sono basati su decenni di consolidate pratiche di ingegneria del software. I sistemi di controllo delle versioni mantengono cronologie precise delle modifiche del codice. Le pipeline di integrazione e distribuzione continue automatizzano i processi di test e rilascio. Gli strumenti di analisi statica misurano la qualità del codice e identificano potenziali problemi. Questa infrastruttura consente uno sviluppo, un test e una distribuzione affidabili di sistemi software, seguendo principi ben definiti di ingegneria del software.

I sistemi di apprendimento automatico rappresentano una deviazione fondamentale da questo paradigma tradizionale. Mentre i sistemi tradizionali eseguono una logica di programmazione esplicita, i sistemi di apprendimento automatico derivano il loro comportamento da pattern nei dati. Questo passaggio dal codice ai dati come driver principale del comportamento del sistema introduce nuove complessità.

Come illustrato in @fig-ml_lifecycle_overview, il ciclo di vita ML è costituito da fasi interconnesse dalla raccolta dati al monitoraggio del modello, con cicli di feedback per il miglioramento continuo quando le prestazioni si degradano o i modelli necessitano di miglioramenti.

![Il tipico ciclo di vita di un sistema di apprendimento automatico.](./images/png/ml_lifecycle_overview.png){#fig-ml_lifecycle_overview}

A differenza del codice sorgente, che cambia solo quando gli sviluppatori lo modificano, i dati riflettono la natura dinamica del mondo reale. Le modifiche nelle distribuzioni dei dati possono alterare silenziosamente il comportamento del sistema. Gli strumenti tradizionali di ingegneria del software, progettati per sistemi basati su codice deterministico, si dimostrano insufficienti per la gestione di questi sistemi dipendenti dai dati. Ad esempio, i sistemi di controllo delle versioni che eccellono nel tracciare modifiche discrete del codice hanno difficoltà a gestire grandi set di dati in evoluzione. I framework di test progettati per output deterministici devono essere adattati per previsioni probabilistiche. Questa natura dipendente dai dati crea un ciclo di vita più dinamico, che richiede un monitoraggio e un adattamento continui per mantenere la pertinenza del sistema man mano che i modelli di dati del mondo reale si evolvono.

Per comprendere il ciclo di vita del sistema di apprendimento automatico è necessario esaminarne le fasi distinte. Ogni fase presenta requisiti unici sia dal punto di vista dell'apprendimento che da quello dell'infrastruttura. Questa duplice considerazione, delle esigenze di apprendimento e del supporto dei sistemi, è estremamente importante per la creazione di sistemi di machine learning efficaci.

Tuttavia, le varie fasi del ciclo di vita ML in produzione non sono isolate; sono, infatti, profondamente interconnesse. Questa interconnessione può creare circoli virtuosi o viziosi. In un circolo virtuoso, dati di alta qualità consentono un apprendimento efficace, infrastrutture robuste supportano un'elaborazione efficiente e sistemi ben progettati facilitano la raccolta di dati ancora migliori. Tuttavia, in un circolo vizioso, una scarsa qualità dei dati mina l'apprendimento, infrastrutture inadeguate ostacolano l'elaborazione e le limitazioni del sistema impediscono il miglioramento della raccolta dati: ogni problema aggrava gli altri.

## Lo Spettro dei Sistemi ML

La complessità della gestione dei sistemi di machine learning diventa ancora più evidente se consideriamo l'ampio spettro in cui il ML viene distribuito oggi. I sistemi di ML esistono su scale molto diverse e in ambienti diversi, ognuno dei quali presenta sfide e vincoli unici.

Da un lato, abbiamo sistemi di ML basati su cloud in esecuzione in enormi data center. Questi sistemi, come i grandi modelli linguistici o i motori di raccomandazione, elaborano petabyte di dati e servono milioni di utenti contemporaneamente. Possono sfruttare risorse di elaborazione virtualmente illimitate, ma devono gestire un'enorme complessità operativa e costi.

Dall'altro lato, troviamo sistemi TinyML in esecuzione su microcontrollori e dispositivi embedded. Questi sistemi devono eseguire attività di ML con gravi vincoli di memoria, potenza di elaborazione e consumo energetico. Si immagini un dispositivo per la casa intelligente, come Alexa o Google Assistant, che deve riconoscere i comandi vocali utilizzando meno energia di una lampadina a LED, o un sensore che deve rilevare anomalie mentre funziona a batteria per mesi o addirittura anni.

Tra questi estremi, troviamo una ricca varietà di sistemi di ML adattati a diversi contesti. I sistemi Edge ML avvicinano il calcolo alle fonti dei dati, riducendo i requisiti di latenza e larghezza di banda e gestendo al contempo le risorse di elaborazione locali. I sistemi ML mobili devono bilanciare capacità sofisticate con limitazioni di durata della batteria e del processore su smartphone e tablet. I sistemi ML aziendali spesso operano entro vincoli aziendali specifici, concentrandosi su attività particolari e integrandosi con l'infrastruttura esistente. Alcune organizzazioni impiegano approcci ibridi, distribuendo le capacità ML su più livelli per bilanciare vari requisiti.

## Implicazioni del Sistema di ML sul Ciclo di Vita ML

La diversità dei sistemi ML nell'intero spettro rappresenta una complessa interazione di requisiti, vincoli e compromessi. Queste decisioni hanno un impatto fondamentale su ogni fase del ciclo di vita ML di cui abbiamo parlato in precedenza, dalla raccolta dati al funzionamento continuo.

I requisiti di prestazioni spesso guidano le decisioni architettoniche iniziali. Le applicazioni sensibili alla latenza, come i veicoli autonomi o il rilevamento delle frodi in tempo reale, potrebbero richiedere architetture edge o embedded nonostante i loro vincoli di risorse. Al contrario, le applicazioni che richiedono un'enorme potenza di calcolo per l'addestramento, come i grandi modelli linguistici, gravitano naturalmente verso architetture cloud centralizzate. Tuttavia, le mere prestazioni sono solo una considerazione in uno spazio decisionale complesso.

La gestione delle risorse varia notevolmente tra le architetture. I sistemi cloud devono ottimizzare l'efficienza dei costi su larga scala, bilanciando costosi cluster GPU, sistemi di archiviazione e larghezza di banda di rete. I sistemi edge affrontano limiti di risorse fissate e devono gestire attentamente l'elaborazione e l'archiviazione locali. I sistemi mobili ed embedded operano con i vincoli più rigorosi, in cui ogni byte di memoria e milliwatt di potenza sono importanti. Queste considerazioni sulle risorse influenzano direttamente sia la progettazione del modello che l'architettura del sistema.

La complessità operativa aumenta con la distribuzione del sistema. Mentre le architetture cloud centralizzate traggono vantaggio da strumenti di distribuzione maturi e servizi gestiti, i sistemi edge e ibridi devono gestire la complessità della gestione del sistema distribuito. Questa complessità si manifesta durante l'intero ciclo di vita del ML, dalla raccolta dati e dal controllo delle versioni alla distribuzione e al monitoraggio del modello. Come discusso nell'esame del debito tecnico, questa complessità operativa può aumentare nel tempo se non gestita attentamente.

Le considerazioni sui dati spesso introducono pressioni concorrenti. I requisiti sulla privacy o le normative sulla sovranità dei dati potrebbero spingere verso architetture edge o integrate, mentre la necessità di dati di formazione su larga scala potrebbe favorire approcci cloud. Anche la velocità e il volume dei dati influenzano le scelte architettoniche: i dati dei sensori in tempo reale potrebbero richiedere l'elaborazione edge per gestire la larghezza di banda, mentre l'analisi batch potrebbe essere più adatta all'elaborazione cloud.

I requisiti di evoluzione e manutenzione devono essere considerati fin dall'inizio. Le architetture cloud offrono flessibilità per l'evoluzione del sistema, ma possono comportare costi continui significativi. I sistemi edge ed embedded potrebbero essere più difficili da aggiornare, ma potrebbero offrire un sovraccarico operativo inferiore. Il ciclo continuo dei sistemi ML di cui abbiamo parlato in precedenza diventa particolarmente impegnativo nelle architetture distribuite, dove l'aggiornamento dei modelli e il mantenimento dello stato di salute del sistema richiedono un'attenta orchestrazione su più piani.

Questi compromessi sono raramente semplici scelte binarie. I moderni sistemi ML adottano spesso approcci ibridi, bilanciando attentamente queste considerazioni in base a casi d'uso e vincoli specifici. La chiave è comprendere come queste decisioni influenzeranno il sistema durante tutto il suo ciclo di vita, dallo sviluppo iniziale al funzionamento continuo e all'evoluzione.

### Tendenze Emergenti

Siamo solo all'inizio. Man mano che i sistemi di apprendimento automatico continuano a evolversi, diverse tendenze chiave stanno rimodellando il panorama della progettazione e dell'implementazione dei sistemi ML.

L'ascesa dei "agentic system" [sistemi agenti] segna una profonda evoluzione nei sistemi ML. I sistemi ML tradizionali erano principalmente reattivi: facevano previsioni o classificazioni basate sui dati di input. Al contrario, i "agentic system" possono intraprendere azioni, imparare dai loro risultati e adattare il loro comportamento di conseguenza. Questi sistemi, esemplificati da agenti autonomi in grado di pianificare, ragionare ed eseguire attività complesse, introducono nuove sfide architettoniche. Richiedono framework sofisticati per il processo decisionale, vincoli di sicurezza e interazione in tempo reale con l'ambiente.

L'evoluzione architettonica è guidata da nuovi pattern di hardware e di distribuzione. Acceleratori AI specializzati stanno emergendo in tutto lo spettro, dai potenti chip dei data center agli efficienti processori edge alle minuscole unità di elaborazione neurale nei dispositivi mobili. Questo panorama informatico eterogeneo sta abilitando nuove possibilità architettoniche, come la distribuzione dinamica dei modelli tra livelli in base alle capacità di elaborazione e alle condizioni attuali. I confini tradizionali tra cloud, edge e sistemi embedded stanno diventando sempre più fluidi.

L'efficienza delle risorse sta acquisendo importanza man mano che i costi ambientali ed economici del ML su larga scala diventano più evidenti. Ciò ha innescato l'innovazione nella compressione dei modelli, nelle tecniche di addestramento efficienti e nell'elaborazione consapevole dei consumi energetici. I sistemi futuri dovranno probabilmente bilanciare la spinta verso modelli più potenti con le crescenti preoccupazioni per la sostenibilità. Questa enfasi sull'efficienza è particolarmente rilevante data la nostra precedente discussione sul debito tecnico e sui costi operativi.

L'intelligenza di sistema si sta muovendo verso un funzionamento più autonomo. I futuri sistemi ML probabilmente incorporeranno un auto-monitoraggio più sofisticato, una gestione automatizzata delle risorse e strategie di distribuzione adattive. Questa evoluzione si basa sul ciclo continuo discusso in precedenza, ma con una maggiore automazione nella gestione dei turni di distribuzione dei dati, degli aggiornamenti dei modelli e dell'ottimizzazione del sistema.

Le sfide dell'integrazione stanno diventando più complesse man mano che i sistemi ML interagiscono con ecosistemi tecnologici più ampi. La necessità di integrarsi con i sistemi software esistenti, gestire diverse fonti di dati e operare oltre i confini organizzativi sta guidando nuovi approcci alla progettazione del sistema. Questa complessità di integrazione aggiunge nuove dimensioni alle considerazioni sul debito tecnico esplorato in precedenza.

Queste tendenze suggeriscono che i futuri sistemi ML dovranno essere sempre più adattabili ed efficienti, gestendo al contempo una crescente complessità. Comprendere queste direzioni è importante per costruire sistemi che possano evolversi con il settore, evitando al contempo l'accumulo di debito tecnico di cui abbiamo parlato in precedenza.

## Applicazioni e Impatto nel Mondo Reale

La capacità di creare e rendere operativi sistemi ML su diverse scale e ambienti ha portato a cambiamenti trasformativi in numerosi settori. Questa sezione presenta alcuni esempi in cui i concetti teorici e le considerazioni pratiche di cui abbiamo discusso si manifestano in applicazioni tangibili e di impatto nel mondo reale.

This section showcases a few examples where theoretical concepts and practical considerations we have discussed manifest in tangible, impactful applications and real-world impact.

### Caso di Studio: FarmBeats: ML Edge ed Embedded per l'Agricoltura

FarmBeats, un progetto sviluppato da Microsoft Research, mostrato in @fig-farmbeats-overview, è un progresso significativo nell'applicazione dell'apprendimento automatico all'agricoltura. Questo sistema mira ad aumentare la produttività agricola e ridurre i costi sfruttando le tecnologie AI e IoT. FarmBeats esemplifica come i sistemi ML edge ed embedded possono essere distribuiti in ambienti reali e difficili per risolvere problemi pratici. Portando le capacità ML direttamente in azienda agricola, FarmBeats dimostra il potenziale dei sistemi AI distribuiti nella trasformazione delle industrie tradizionali.

![Microsoft Farmbeats: AI, Edge e IoT per l'Agricoltura.](./images/png/farmbeats.png){#fig-farmbeats-overview}

**Aspetti dei Dati**

L'ecosistema di dati in FarmBeats è diversificato e distribuito. I sensori distribuiti nei campi raccolgono dati in tempo reale su umidità del suolo, temperatura e livelli di nutrienti. I droni dotati di telecamere multispettrali catturano immagini ad alta risoluzione delle colture, fornendo informazioni sulla salute delle piante e sui pattern di crescita. Le stazioni meteorologiche forniscono dati climatici locali, mentre le registrazioni agricole storiche offrono un contesto per le tendenze a lungo termine. La sfida non risiede solo nella raccolta di questi dati eterogenei, ma anche nella gestione del loro flusso da luoghi dispersi, spesso remoti, con connettività limitata. FarmBeats impiega tecniche innovative di trasmissione dati, come l'utilizzo di spazi TV vuoti (frequenze di trasmissione inutilizzate) per estendere la connettività Internet a sensori distanti. Questo approccio alla raccolta e alla trasmissione dei dati incarna i principi dell'edge computing discussi in precedenza, in cui l'elaborazione dei dati inizia alla fonte per ridurre i requisiti di larghezza di banda e consentire il processo decisionale in tempo reale.

**Aspetti Algoritmo/Modello**

FarmBeats utilizza una varietà di algoritmi ML su misura per applicazioni agricole. Per la previsione dell'umidità del suolo, utilizza reti neurali temporali in grado di catturare le complesse dinamiche del movimento dell'acqua nel suolo. Gli algoritmi di visione artificiale elaborano le immagini dei droni per rilevare lo stress delle colture, le infestazioni di parassiti e le stime della resa. Questi modelli devono essere robusti ai dati rumorosi e in grado di funzionare con risorse computazionali limitate. I metodi di apprendimento automatico come il "transfer learning" consentono ai modelli di addestrarsi su aziende agricole ricche di dati per essere adattati all'uso in aree con dati storici limitati. Il sistema incorpora anche una combinazione di metodi che combinano gli output di più algoritmi per migliorare l'accuratezza e l'affidabilità delle previsioni. Una sfida chiave che FarmBeats affronta è la personalizzazione del modello, ovvero l'adattamento di modelli generali alle condizioni specifiche delle singole aziende agricole, che possono avere composizioni del suolo, microclimi e pratiche agricole uniche.

**Aspetti dell'Infrastruttura Informatica**

FarmBeats esemplifica il paradigma di edge computing esplorato nella discussione sullo spettro del sistema ML. Al livello più basso, i modelli ML embedded vengono eseguiti direttamente su dispositivi e sensori IoT, eseguendo il filtraggio dei dati di base e il rilevamento delle anomalie. I dispositivi edge, come i "ruggedized field gateway" [gateway di campo rinforzati], aggregano i dati da più sensori ed eseguono modelli più complessi per il processo decisionale locale. Questi dispositivi edge operano in condizioni difficili, richiedendo progetti hardware robusti e una gestione efficiente dell'alimentazione per funzionare in modo affidabile in contesti agricoli remoti. Il sistema impiega un'architettura gerarchica, con attività più intensive dal punto di vista computazionale scaricate su server locali o sul cloud. Questo approccio a livelli consente a FarmBeats di bilanciare la necessità di elaborazione in tempo reale con i vantaggi dell'analisi centralizzata dei dati e del training del modello. L'infrastruttura include anche meccanismi per gli aggiornamenti dei modelli over-the-air, assicurando che i dispositivi edge possano ricevere modelli migliorati man mano che diventano disponibili più dati e gli algoritmi vengono perfezionati.

**Impatto e Implicazioni Future**

FarmBeats mostra come i sistemi ML possono essere distribuiti in ambienti reali con risorse limitate per guidare miglioramenti significativi nei settori tradizionali. Fornendo agli agricoltori informazioni basate sull'IA, il sistema ha dimostrato il potenziale per aumentare le rese delle colture, ridurre l'uso dell'acqua e ottimizzare l'allocazione delle risorse. Guardando al futuro, l'approccio FarmBeats potrebbe essere esteso per affrontare le sfide globali in materia di sicurezza alimentare e agricoltura sostenibile. Il successo di questo sistema evidenzia anche la crescente importanza dell'edge e dell'embedded ML nelle applicazioni IoT, dove avvicinare l'intelligenza alla sorgente dei dati può portare a soluzioni più reattive, efficienti e scalabili. Man mano che le capacità di edge computing continuano a progredire, possiamo aspettarci di vedere simili architetture ML distribuite applicate ad altri domini, dalle smart city al monitoraggio ambientale.

### Caso di Studio: AlphaFold: ML Scientifico su Larga Scala

AlphaFold, sviluppato da DeepMind, è un traguardo storico nell'applicazione dell'apprendimento automatico a problemi scientifici complessi. Questo sistema di IA è progettato per prevedere la struttura tridimensionale delle proteine, come mostrato in @fig-alphafold-overview, dalle loro sequenze di amminoacidi, una sfida nota come "problema del ripiegamento delle proteine" che ha lasciato perplessi gli scienziati per decenni. Il successo di AlphaFold dimostra come i sistemi di ML su larga scala possano accelerare la scoperta scientifica e potenzialmente rivoluzionare campi come la biologia strutturale e la progettazione di farmaci. Questo studio di caso esemplifica l'uso di tecniche di ML avanzate e di enormi risorse computazionali per affrontare problemi alle frontiere della scienza.

::: {.content-visible when-format="html"}
![Esempi di target proteici nella categoria di modellazione libera. Fonte: Google DeepMind](images/png/alphafold.gif){#fig-alphafold-overview}
:::

::: {.content-visible when-format="docx"}
![Esempi di target proteici nella categoria di modellazione libera. Fonte: Google DeepMind](images/png/alphafold.png){#fig-alphafold-overview}
:::

::: {.content-visible when-format="pdf"}
![Esempi di target proteici nella categoria di modellazione libera. Fonte: Google DeepMind](images/png/alphafold.png){#fig-alphafold-overview}
:::

**Aspetti dei Dati**

I dati alla base del successo di AlphaFold sono vasti e sfaccettati. Il set di dati principale è il Protein Data Bank (PDB), che contiene le strutture determinate sperimentalmente di oltre 180.000 proteine. Questo è completato da database di sequenze proteiche, che assommano a centinaia di milioni. AlphaFold utilizza anche dati evolutivi sotto forma di allineamenti di sequenze multiple (MSA), che forniscono informazioni sui modelli di conservazione degli amminoacidi nelle proteine correlate. La sfida non risiede solo nel volume dei dati, ma anche nella loro qualità e rappresentazione. Le strutture proteiche sperimentali possono contenere errori o essere incomplete, il che richiede sofisticati processi di pulizia e convalida dei dati. Inoltre, la rappresentazione delle strutture e delle sequenze proteiche in una forma adatta all'apprendimento automatico è una sfida significativa di per sé. La pipeline di dati di AlphaFold prevede complessi passaggi di pre-elaborazione per convertire i dati grezzi di sequenza e strutturali in feature significative che catturano le proprietà fisiche e chimiche rilevanti per il ripiegamento delle proteine.

**Aspetti Algoritmo/Modello**

L'approccio algoritmico di AlphaFold rappresenta un "tour de force" nell'applicazione del deep learning ai problemi scientifici. Al centro, AlphaFold utilizza una nuova architettura di rete neurale che si combina con tecniche di biologia computazionale. Il modello impara a prevedere distanze inter-residuo e angoli di torsione, che vengono poi utilizzati per costruire una struttura proteica 3D completa. Un'innovazione fondamentale è l'uso di layer di "attenzione equivariante" che rispettano le simmetrie intrinseche nelle strutture proteiche. Il processo di apprendimento coinvolge più fasi, tra cui il "pre-addestramento" iniziale su un ampio corpus di sequenze proteiche, seguito da una messa a punto su strutture note. AlphaFold incorpora anche la conoscenza del dominio sotto forma di vincoli basati sulla fisica e funzioni di punteggio, creando un sistema ibrido che sfrutta sia l'apprendimento basato sui dati sia la conoscenza scientifica pregressa. La capacità del modello di generare stime di confidenza accurate per le sue previsioni è fondamentale, consentendo ai ricercatori di valutare l'affidabilità delle strutture previste.

**Aspetti dell'Infrastruttura Informatica**

Le esigenze computazionali di AlphaFold esemplificano le sfide dei sistemi ML scientifici su larga scala. L'addestramento del modello richiede enormi risorse di elaborazione parallela, sfruttando cluster di GPU o TPU (Tensor Processing Unit) in un ambiente di elaborazione distribuito. DeepMind ha utilizzato l'infrastruttura cloud di Google, con la versione finale di AlphaFold addestrata su 128 core TPUv3 per diverse settimane. Il processo di inferenza, sebbene meno intensivo dal punto di vista computazionale rispetto all'addestramento, richiede comunque risorse significative, soprattutto quando si prevedono strutture per proteine di grandi dimensioni o si elaborano molte proteine in parallelo. Per rendere AlphaFold più accessibile alla comunità scientifica, DeepMind ha collaborato con l'European Bioinformatics Institute per creare un [database pubblico](https://alphafold.ebi.ac.uk/) di strutture proteiche previste, che di per sé rappresenta una sfida sostanziale per l'elaborazione e la gestione dei dati. Questa infrastruttura consente ai ricercatori di tutto il mondo di accedere alle previsioni di AlphaFold senza dover eseguire il modello in proprio, dimostrando come le risorse di elaborazione centralizzate e ad alte prestazioni possano essere sfruttate per democratizzare l'accesso alle funzionalità ML avanzate.

**Impatto e Implicazioni Future**

L'impatto di AlphaFold sulla biologia strutturale è stato profondo, con il potenziale di accelerare la ricerca in aree che vanno dalla biologia fondamentale alla scoperta di farmaci. Fornendo previsioni strutturali accurate per proteine che hanno resistito ai metodi sperimentali, AlphaFold apre nuove strade per comprendere i meccanismi delle malattie e progettare terapie mirate. Il successo di AlphaFold serve anche come una potente dimostrazione di come il ML può essere applicato ad altri complessi problemi scientifici, portando potenzialmente a scoperte in campi come la scienza dei materiali o la modellazione climatica. Tuttavia, solleva anche importanti questioni sul ruolo dell'IA nella scoperta scientifica e sulla natura mutevole dell'indagine scientifica nell'era dei sistemi ML su larga scala. Mentre guardiamo al futuro, l'approccio AlphaFold suggerisce un nuovo paradigma per il ML scientifico, in cui enormi risorse computazionali vengono combinate con conoscenze specifiche del dominio per spingere i confini della comprensione umana.

### Caso di Studio: Veicoli Autonomi: Abbracciare lo Spettro ML

Waymo, una sussidiaria di Alphabet Inc., è all'avanguardia nella tecnologia dei veicoli autonomi, rappresentando una delle applicazioni più ambiziose dei sistemi di apprendimento automatico fino ad oggi. Evoluzione del Google Self-Driving Car Project avviato nel 2009, l'approccio di Waymo alla guida autonoma esemplifica come i sistemi ML possano abbracciare l'intero spettro, dai sistemi embedded all'infrastruttura cloud. Questo caso di studio mostra l'implementazione pratica di sistemi ML complessi in un ambiente reale critico per la sicurezza, integrando il processo decisionale in tempo reale con l'addestramento e l'adattamento a lungo termine.

{{< video https://youtu.be/hA_-MkU0Nfw?si=6DIH7qwMbeMicnJ5 >}}

**Aspetti dei Dati**

L'ecosistema di dati alla base della tecnologia Waymo è vasto e dinamico. Ogni veicolo funge da data center itinerante, la sua suite di sensori, composta da LiDAR, radar e telecamere ad alta risoluzione, genera circa un terabyte di dati per ora di guida. Questi dati del mondo reale sono integrati da un set di dati simulato ancora più esteso, con i veicoli Waymo che hanno percorso oltre 20 miliardi di miglia in simulazione e più di 20 milioni di miglia su strade pubbliche. La sfida non risiede solo nel volume di dati, ma nella loro eterogeneità e nella necessità di elaborazione in tempo reale. Waymo deve gestire contemporaneamente sia dati strutturati (ad esempio, coordinate GPS) che non strutturati (ad esempio, immagini della telecamera). La pipeline di dati si estende dall'elaborazione edge sul veicolo stesso a enormi sistemi di archiviazione ed elaborazione basati su cloud. Sono necessari sofisticati processi di validazione e pulizia dei dati, data la natura critica per la sicurezza dell'applicazione. Inoltre, la rappresentazione dell'ambiente del veicolo in una forma adatta all'apprendimento automatico presenta sfide significative, che richiedono una pre-elaborazione complessa per convertire i dati grezzi dei sensori in caratteristiche significative che catturino le dinamiche degli scenari di traffico.

**Aspetti Algoritmo/Modello**

Lo stack ML di Waymo rappresenta un sofisticato insieme di algoritmi su misura per la sfida multiforme della guida autonoma. Il sistema di percezione impiega tecniche di deep learning, tra cui reti neurali convoluzionali, per elaborare dati visivi per il rilevamento e il tracciamento di oggetti. I modelli di previsione, necessari per anticipare il comportamento di altri utenti della strada, sfruttano reti neurali ricorrenti per comprendere sequenze temporali. Waymo ha sviluppato modelli ML personalizzati come VectorNet per prevedere le traiettorie dei veicoli. I sistemi di pianificazione e decisionali possono incorporare tecniche di apprendimento per rinforzo o apprendimento per imitazione per navigare in scenari di traffico complessi. Un'innovazione fondamentale nell'approccio di Waymo è l'integrazione di questi diversi modelli in un sistema coerente in grado di funzionare in tempo reale. I modelli ML devono anche essere interpretabili in una certa misura, poiché comprendere il ragionamento alla base delle decisioni di un veicolo è fondamentale per la sicurezza e la conformità alle normative. Il processo di apprendimento di Waymo implica un continuo perfezionamento basato su esperienze di guida nel mondo reale e simulazioni approfondite, creando un ciclo di feedback che migliora costantemente le prestazioni del sistema.

**Aspetti dell'Infrastruttura Informatica**

L'infrastruttura informatica che supporta i veicoli autonomi di Waymo incarna le sfide dell'implementazione di sistemi ML nell'intero spettro, dall'edge al cloud. Ogni veicolo è dotato di una piattaforma informatica personalizzata in grado di elaborare dati dei sensori e prendere decisioni in tempo reale, spesso sfruttando hardware specializzato come GPU o acceleratori AI personalizzati. Questo edge computing è completato da un ampio utilizzo dell'infrastruttura cloud, sfruttando la potenza dei data center di Google per la formazione di modelli, l'esecuzione di simulazioni su larga scala e l'esecuzione di apprendimento a livello di flotta. La connettività tra questi livelli è fondamentale, con veicoli che richiedono comunicazioni affidabili e ad alta larghezza di banda per aggiornamenti in tempo reale e caricamento dati. L'infrastruttura di Waymo deve essere progettata per robustezza e tolleranza agli errori, garantendo un funzionamento sicuro anche in caso di guasti hardware o interruzioni di rete. La portata delle operazioni di Waymo presenta sfide significative nella gestione dei dati, nell'implementazione dei modelli e nel monitoraggio del sistema in una flotta di veicoli distribuita geograficamente.

**Impatto e Implicazioni Future**

L'impatto di Waymo si estende oltre il progresso tecnologico, rivoluzionando potenzialmente i trasporti, la pianificazione urbana e numerosi aspetti della vita quotidiana. Il lancio di Waymo One, un servizio di "ride-hailing" [a chiamata] commerciale che utilizza veicoli autonomi a Phoenix, in Arizona, rappresenta una pietra miliare significativa nell'implementazione pratica dei sistemi di IA in applicazioni critiche per la sicurezza. I progressi di Waymo hanno implicazioni più ampie per lo sviluppo di sistemi di IA solidi e reali, guidando innovazioni nella tecnologia dei sensori, nell'edge computing e nella sicurezza dell'IA che hanno applicazioni ben oltre l'industria automobilistica. Tuttavia, solleva anche importanti questioni su responsabilità, etica e interazione tra sistemi di IA e società umana. Mentre Waymo continua ad espandere le sue operazioni ed esplorare applicazioni nei trasporti su camion e nelle consegne dell'ultimo miglio, funge da importante banco di prova per sistemi di ML avanzati, guidando i progressi in aree come l'apprendimento continuo, la percezione robusta e l'interazione uomo-IA. Il caso di studio di Waymo sottolinea sia l'enorme potenziale dei sistemi di ML per trasformare i settori sia le complesse sfide implicate nell'implementazione dell'IA nel mondo reale.

## Sfide e Considerazioni

La creazione e l'implementazione di sistemi di apprendimento automatico presentano sfide uniche che vanno oltre lo sviluppo software tradizionale. Queste sfide aiutano a spiegare perché la creazione di sistemi ML efficaci non riguarda solo la scelta dell'algoritmo giusto o la raccolta di dati sufficienti. Esploriamo le aree chiave in cui i professionisti del ML affrontano ostacoli significativi.

### Sfide dei Dati

Il fondamento di qualsiasi sistema ML sono i suoi dati e la gestione di questi dati presenta diverse sfide fondamentali. Innanzitutto, c'è la questione di base della qualità dei dati: i dati del mondo reale sono spesso disordinati e incoerenti. Si immagini un'applicazione sanitaria che deve elaborare le cartelle cliniche dei pazienti di diversi ospedali. Ogni ospedale potrebbe registrare le informazioni in modo diverso, utilizzare unità di misura diverse o avere standard diversi per i dati da raccogliere. Alcune cartelle potrebbero avere informazioni mancanti, mentre altre potrebbero contenere errori o incongruenze che devono essere eliminate prima che i dati possano essere utili.

Man mano che i sistemi ML crescono, spesso devono gestire quantità di dati sempre maggiori. Un servizio di streaming video come Netflix, ad esempio, deve elaborare miliardi di interazioni con gli spettatori per alimentare il suo sistema di raccomandazione. Questa scala introduce nuove sfide su come archiviare, elaborare e gestire in modo efficiente set di dati così grandi.

Un'altra sfida critica è il modo in cui i dati cambiano nel tempo. Questo fenomeno, noto come "data drift", si verifica quando i modelli nei nuovi dati iniziano a differire dai modelli da cui il sistema ha appreso originariamente. Ad esempio, molti modelli predittivi hanno avuto difficoltà durante la pandemia di COVID-19 perché il comportamento dei consumatori è cambiato così drasticamente che i modelli storici sono diventati meno rilevanti. I sistemi ML hanno bisogno di modi per rilevare quando ciò accade e adattarsi di conseguenza.

### Sfide del Modello

La creazione e la manutenzione dei modelli ML stessi presentano un'altra serie di sfide. I moderni modelli ML, in particolare nel deep learning, possono essere estremamente complessi. Si consideri un modello linguistico come GPT-3, che ha centinaia di miliardi di parametri (le singole impostazioni che il modello apprende durante l'addestramento). Questa complessità crea sfide pratiche: questi modelli richiedono un'enorme potenza di calcolo per l'addestramento e l'esecuzione, rendendo difficile la loro distribuzione in situazioni con risorse limitate, come su telefoni cellulari o dispositivi IoT.

Addestrare questi modelli in modo efficace è di per sé una sfida significativa. A differenza della programmazione tradizionale in cui scriviamo istruzioni esplicite, i modelli ML imparano dagli esempi. Questo processo di apprendimento comporta molte scelte: come dovremmo strutturare il modello? Per quanto tempo dovremmo addestrarlo? Come possiamo sapere se sta imparando le cose giuste? Prendere queste decisioni spesso richiede sia competenza tecnica che notevoli tentativi ed errori.

Una sfida particolarmente importante è garantire che i modelli funzionino bene in condizioni reali. Un modello potrebbe funzionare in modo eccellente sui suoi dati di addestramento ma fallire quando si trova di fronte a situazioni leggermente diverse nel mondo reale. Questo divario tra le prestazioni di formazione e le prestazioni nel mondo reale è una sfida centrale nell'apprendimento automatico, specialmente per applicazioni critiche come veicoli autonomi o sistemi di diagnosi medica.

### Sfide di Sistema

Far funzionare i sistemi ML in modo affidabile nel mondo reale presenta una serie di sfide. A differenza dei software tradizionali che seguono regole fisse, i sistemi ML devono gestire l'incertezza e la variabilità nei loro input e output. In genere necessitano sia di sistemi di formazione (per apprendere dai dati) sia di sistemi di servizio (per fare previsioni), ognuno con requisiti e vincoli diversi.

Si consideri un'azienda che costruisce un sistema di riconoscimento vocale. Hanno bisogno di infrastrutture per raccogliere e archiviare dati audio, sistemi per addestrare modelli su questi dati e quindi sistemi separati per elaborare effettivamente il parlato degli utenti in tempo reale. Ogni parte di questa pipeline deve funzionare in modo affidabile ed efficiente e tutte le parti devono funzionare insieme senza problemi.

Questi sistemi necessitano anche di monitoraggio e aggiornamento costanti. Come facciamo a sapere se il sistema funziona correttamente? Come aggiorniamo i modelli senza interrompere il servizio? Come gestiamo errori o input imprevisti? Queste sfide operative diventano particolarmente complesse quando i sistemi ML servono milioni di utenti.

### Considerazioni Etiche e Sociali

Man mano che i sistemi ML diventano più diffusi nella nostra vita quotidiana, il loro impatto più ampio sulla società diventa sempre più importante da considerare. Una delle principali preoccupazioni è l'equità: i sistemi ML a volte possono imparare a prendere decisioni che discriminano determinati gruppi di persone. Ciò accade spesso in modo involontario, poiché i sistemi rilevano pregiudizi presenti nei loro dati di formazione. Ad esempio, un sistema di screening delle domande di lavoro potrebbe imparare inavvertitamente a favorire determinati dati demografici se tali gruppi avevano storicamente maggiori probabilità di essere assunti.

Un'altra considerazione importante è la trasparenza. Molti modelli ML moderni, in particolare i modelli di deep learning, funzionano come "scatole nere": sebbene possano fare previsioni, è spesso difficile capire come sono arrivati alle loro decisioni. Ciò diventa particolarmente problematico quando i sistemi ML prendono decisioni importanti sulla vita delle persone, come nell'assistenza sanitaria o nei servizi finanziari.

Anche la privacy è una delle principali preoccupazioni. I sistemi ML spesso necessitano di grandi quantità di dati per funzionare in modo efficace, ma questi dati potrebbero contenere informazioni personali sensibili. Come bilanciamo la necessità di dati con la necessità di proteggere la privacy individuale? Come possiamo garantire che i modelli non memorizzino e rivelino inavvertitamente informazioni private?

Queste sfide non sono semplicemente problemi tecnici da risolvere, ma considerazioni in corso che modellano il nostro approccio alla progettazione e all'implementazione del sistema ML. In questo libro, esploreremo queste sfide in dettaglio ed esamineremo le strategie per affrontarle in modo efficace.

## Direzioni Future

Guardando al futuro dei sistemi di apprendimento automatico, diverse tendenze entusiasmanti stanno plasmando il campo. Questi sviluppi promettono sia di risolvere le sfide esistenti sia di aprire nuove possibilità per ciò che i sistemi ML possono realizzare.

Una delle tendenze più significative è la democratizzazione della tecnologia AI. Proprio come i personal computer hanno trasformato l'informatica da mainframe specializzati a strumenti di uso quotidiano, i sistemi ML stanno diventando più accessibili a sviluppatori e organizzazioni di tutte le dimensioni. I provider cloud ora offrono modelli pre-addestrati e piattaforme ML automatizzate che riducono le competenze necessarie per implementare soluzioni AI. Questa democratizzazione sta abilitando nuove applicazioni in tutti i settori, dalle piccole imprese che utilizzano l'AI per il servizio clienti ai ricercatori che applicano l'ML a problemi precedentemente intrattabili.

Con l'aumento delle preoccupazioni sui costi computazionali e sull'impatto ambientale, c'è una crescente attenzione nel rendere i sistemi ML più efficienti. I ricercatori stanno sviluppando nuove tecniche per addestrare modelli con meno dati e potenza di calcolo. L'innovazione nell'hardware specializzato, dalle GPU migliorate ai chip AI personalizzati, sta rendendo i sistemi ML più veloci e più efficienti dal punto di vista energetico. Questi progressi potrebbero rendere disponibili sofisticate capacità AI su più dispositivi, dagli smartphone ai sensori IoT.

Forse la tendenza più trasformativa è lo sviluppo di sistemi ML più autonomi in grado di adattarsi e migliorarsi. Questi sistemi stanno iniziando a gestire le proprie attività di manutenzione, rilevando quando hanno bisogno di essere riqualificati, trovando e correggendo automaticamente gli errori e ottimizzando le proprie prestazioni. Questa automazione potrebbe ridurre drasticamente le spese generali operative dei sistemi ML in esecuzione, migliorandone al contempo l'affidabilità.

Sebbene queste tendenze siano promettenti, è importante riconoscere i limiti del campo. La creazione di un'intelligenza generale veramente artificiale rimane un obiettivo lontano. Gli attuali sistemi ML eccellono in attività specifiche, ma mancano della flessibilità e della comprensione che gli esseri umani danno per scontate. Le sfide relative a pregiudizi, trasparenza e privacy continuano a richiedere un'attenta considerazione. Man mano che i sistemi ML diventano più diffusi, sarà fondamentale affrontare queste limitazioni sfruttando al contempo nuove capacità.

## Percorso di Apprendimento e Struttura del Libro

Questo libro è progettato per guidare dalla comprensione dei fondamenti dei sistemi ML alla progettazione e all'implementazione efficaci. Per affrontare le complessità e le sfide dell'ingegneria dei Sistemi di Machine Learning, abbiamo organizzato il contenuto attorno a cinque pilastri fondamentali che comprendono il ciclo di vita dei sistemi ML. Questi pilastri forniscono un framework per comprendere, sviluppare e mantenere sistemi ML robusti.

![Panoramica dei cinque pilastri fondamentali del sistema dell'ingegneria dei Sistemi di Machine Learning.](images/png/book_pillars.png){#fig-pillars}

Come illustrato nella Figura @fig-pillars, i cinque pilastri centrali del framework sono:

- **Dati**: Enfatizzazione dell'ingegneria dei dati e dei principi fondamentali fondamentali per il funzionamento dell'IA in relazione ai dati.
- **Addestramento**: Esplorazione delle metodologie per l'addestramento dell'IA, concentrandosi su efficienza, ottimizzazione e tecniche di accelerazione per migliorare le prestazioni del modello.
- **Distribuzione**: Inclusione di benchmark, strategie di addestramento su dispositivo e operazioni di apprendimento automatico per garantire un'applicazione efficace del modello.
- **Operazioni**: Evidenziazione delle sfide di manutenzione uniche per i sistemi di apprendimento automatico, che richiedono approcci specializzati distinti dai sistemi di ingegneria tradizionali.
- **Etica e Governance**: Affrontare preoccupazioni quali sicurezza, privacy, pratiche di IA responsabili e le più ampie implicazioni sociali delle tecnologie di IA.

Ogni pilastro rappresenta una fase critica nel ciclo di vita dei sistemi ML ed è composto da elementi fondamentali che si basano l'uno sull'altro. Questa struttura garantisce una comprensione completa di "Machine Learning in Science and Engineering" (MLSE), dai principi di base alle applicazioni avanzate e alle considerazioni etiche.

Per informazioni più dettagliate sulla panoramica del libro, sui contenuti, sui risultati di apprendimento, sul pubblico target, sui prerequisiti e sulla guida alla navigazione, fare riferimento alla sezione [Informazioni sul libro](../about/about.it.qmd). Lì si troveranno anche preziosi dettagli sulla nostra comunità di apprendimento e su come massimizzare l'esperienza con questa risorsa.