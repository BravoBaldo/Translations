---
bibliography: training.bib
---

# Addestramento dell'IA {#sec-ai_training}

::: {.content-visible when-format="html"}
Risorse: [Slide](#sec-ai-training-resource), [Video](#sec-ai-frameworks-resource), [Esercizi](#sec-ai-training-resource), [Laboratori](#sec-ai-training-resource)
:::

![_DALL·E 3 Prompt: Un'illustrazione per l'addestramento dell'IA, raffigurante una rete neurale con neuroni che vengono riparati e attivati. La scena include una vasta rete di neuroni, ognuno dei quali si illumina e si attiva per rappresentare attività e apprendimento. Tra questi neuroni, piccole figure che ricordano ingegneri e scienziati lavorano attivamente, riparando e modificando i neuroni. Questi lavoratori in miniatura simboleggiano il processo di addestramento della rete, regolando pesi e bias per ottenere la convergenza. L'intera scena è una metafora visiva dello sforzo intricato e collaborativo coinvolto nell'addestramento dell'IA, con i lavoratori che rappresentano l'ottimizzazione e l'apprendimento continui all'interno di una rete neurale. Lo sfondo è una serie complessa di neuroni interconnessi, che creano un senso di profondità e complessità._](images/png/ai_training.png)

Il training è fondamentale per sviluppare sistemi di IA accurati e utili utilizzando tecniche di apprendimento automatico. A un livello elevato, il training implica l'inserimento di dati negli algoritmi di machine learning in modo che possano imparare pattern e fare previsioni. Tuttavia, il training efficace dei modelli richiede di affrontare vari problemi relativi a dati, algoritmi, ottimizzazione dei parametri del modello e abilitazione della generalizzazione. Questo capitolo esplorerà le sfumature e le considerazioni relative al training dei modelli di apprendimento automatico.

::: {.callout-tip}

## Obiettivi dell'Apprendimento

* Comprendere la matematica fondamentale delle reti neurali, tra cui le trasformazioni lineari, funzioni di attivazione, funzioni di perdita, backpropagation e ottimizzazione tramite "gradient descent" [discesa del gradiente].

* Scoprire come sfruttare efficacemente i dati per il training del modello tramite la suddivisione appropriata in set di addestramento, convalida e test per abilitare la generalizzazione.

* Scoprire vari algoritmi di ottimizzazione come lo "stochastic gradient descent" e adattamenti come momentum e Adam che accelerano l'addestramento.

* Scoprire tecniche di ottimizzazione e regolarizzazione degli iperparametri per migliorare la generalizzazione del modello riducendo l'overfitting.

* Scoprire strategie di inizializzazione del peso appropriate abbinate alle architetture del modello e alle scelte di attivazione che accelerano la convergenza.

* Identificare i colli di bottiglia posti da operazioni chiave come la moltiplicazione di matrici durante l'addestramento e il deployment.

* Scoprire come i miglioramenti hardware come GPU, TPU e acceleratori specializzati velocizzano le operazioni matematiche critiche per accelerare l'addestramento.

* Scoprire tecniche di parallelizzazione, sia parallelismo dei dati che del modello, per distribuire l'addestramento su più dispositivi e accelerare la produttività del sistema.

:::

## Introduzione

Il training è fondamentale per sviluppare sistemi di intelligenza artificiale accurati e utili tramite il machine learning. Il training crea un modello di apprendimento automatico che può essere generalizzato a dati nuovi e inediti anziché memorizzare gli esempi dell'addestramento. Ciò avviene inserendo dati di training in algoritmi che apprendono pattern da questi esempi regolando i parametri interni.

Gli algoritmi riducono al minimo una "funzione loss" [perdita], che confronta le loro previsioni sui dati di training con le etichette o le soluzioni note, guidando l'apprendimento. Un training efficace richiede spesso set di dati rappresentativi di alta qualità sufficientemente grandi da catturare la variabilità nei casi d'uso del mondo reale.

Richiede inoltre la scelta di un algoritmo adatto all'attività, che si tratti di una rete neurale per la visione artificiale, un algoritmo di apprendimento di rinforzo per il controllo robotico o un metodo basato su alberi per la previsione categoriale. È necessaria un'attenta messa a punto per la struttura del modello, come la profondità e la larghezza della rete neurale e i parametri di apprendimento come la dimensione del passo e la forza della regolarizzazione.

Sono importanti anche le tecniche per prevenire l'overfitting, come le penalità di regolarizzazione nonché la convalida con dati trattenuti. L'overfitting può verificarsi quando un modello si adatta troppo ai dati di training, non riuscendo a generalizzare con i nuovi dati. Ciò può accadere se il modello è troppo complesso o è stato addestrato troppo a lungo.

Per evitare l'overfitting, le tecniche di regolarizzazione possono aiutare a vincolare il modello. Un metodo di regolarizzazione consiste nell'aggiungere un termine di penalità alla funzione di perdita che scoraggia la complessità, come la norma L2 dei pesi. Questo penalizza i valori dei parametri elevati. Un'altra tecnica è il dropout, in cui una percentuale di neuroni viene impostata casualmente a zero durante l'addestramento. Ciò riduce il co-adattamento dei neuroni.

I metodi di validazione aiutano anche a rilevare ed evitare l'overfitting. Una parte dei dati di training viene tenuta fuori dal ciclo di training come un set di validazione. Il modello viene valutato su questi dati. Se l'errore di convalida aumenta mentre l'errore di training diminuisce, si verifica un overfitting. Il training può quindi essere interrotto in anticipo o regolarizzato in modo più forte. La regolarizzazione e la convalida consentono ai modelli di addestrarsi alla massima capacità senza overfitting [sovra-adattare] i dati di training.

Il training richiede notevoli risorse di elaborazione, in particolare per le reti neurali profonde (deep) utilizzate nella visione artificiale, nell'elaborazione del linguaggio naturale e in altre aree. Queste reti hanno milioni di pesi regolabili che devono essere regolati tramite un training esteso. I miglioramenti hardware e le tecniche di training distribuite hanno consentito di addestrare reti neurali sempre più grandi che possono raggiungere prestazioni di livello umano in alcune attività.

In sintesi, alcuni punti chiave sul training:

* **I Dati sono cruciali:** I modelli di machine learning apprendono dagli esempi nei dati di training. Dati più rappresentativi e di qualità elevata portano a migliori prestazioni del modello. I dati devono essere elaborati e formattati per il training.
* **Gli algoritmi imparano dai dati:** Diversi algoritmi (reti neurali, alberi decisionali, ecc.) hanno approcci diversi per trovare dei pattern nei dati. È importante scegliere l'algoritmo giusto per l'attività.
* **L'addestramento affina i parametri del modello:** L'addestramento del modello regola i parametri interni per trovare pattern nei dati. I modelli avanzati come le reti neurali hanno molti pesi regolabili. L'addestramento regola iterativamente i pesi per ridurre al minimo una funzione di perdita.
* **La generalizzazione è l'obiettivo:** Un modello che sovra-adatta i dati di addestramento non generalizzerà bene. Le tecniche di regolarizzazione (dropout, early stopping [arresto anticipato], ecc.) riducono il sovra-adattamento. I dati di validazione vengono utilizzati per valutare la generalizzazione.
* **L'addestramento richiede risorse di elaborazione:** L'addestramento di modelli complessi richiede una notevole potenza di elaborazione e tempo. I miglioramenti hardware e il training distribuito su GPU/TPU hanno consentito dei progressi.

Guideremo attraverso questi dettagli nelle restanti sezioni. Comprendere come sfruttare in modo efficace dati, algoritmi, ottimizzazione dei parametri e generalizzazione attraverso il training è essenziale per sviluppare sistemi di intelligenza artificiale capaci e distribuibili che funzionino in modo robusto nel mondo reale.

## Matematica delle Reti Neurali

Il deep learning ha rivoluzionato l'apprendimento automatico e l'intelligenza artificiale, consentendo ai computer di apprendere pattern complessi e prendere decisioni intelligenti. La rete neurale è al centro della rivoluzione del deep learning e, come discusso nella sezione 3, "Avvio al Deep Learning", è un pilastro in alcuni di questi progressi.

Le reti neurali sono costituite da semplici funzioni stratificate l'una sull'altra. Ogni layer acquisisce alcuni dati, esegue alcuni calcoli e li passa al layer successivo. Questi layer apprendono progressivamente funzionalità di alto livello utili per le attività che la rete è addestrata a svolgere. Ad esempio, in una rete addestrata per il riconoscimento delle immagini, il layer di input può acquisire valori di pixel, mentre i layer successivi possono rilevare forme semplici come i bordi. I layer successivi possono rilevare forme più complesse come nasi, occhi, ecc. Il layer di output finale classifica l'immagine nel suo complesso.

La rete, in una rete neurale, si riferisce al modo in cui questi layer sono connessi. L'output di ogni layer è considerato un set di neuroni, che sono collegati ai neuroni nei layer successivi, formando una "rete". Il modo in cui questi neuroni interagiscono è determinato dai pesi tra di loro, che modellano le forze sinaptiche simili a quelle di un neurone del cervello. La rete neurale viene addestrata regolando questi pesi. Concretamente, i pesi vengono inizialmente impostati in modo casuale, quindi viene immesso l'input, l'output viene confrontato con il risultato desiderato e, infine, i pesi vengono modificati per migliorare la rete. Questo processo viene ripetuto finché la rete non riduce al minimo in modo affidabile la perdita (loss), indicando di aver appreso i pattern nei dati.

Come viene definito matematicamente questo processo? Formalmente, le reti neurali sono modelli matematici costituiti da operazioni lineari e non lineari alternate, parametrizzate da un set di pesi apprendibili che vengono addestrati per minimizzare una qualche funzione di perdita (loss). Questa funzione di perdita misura quanto è buono il nostro modello per quanto riguarda l'adattamento dei nostri dati di addestramento e produce un valore numerico quando viene valutato sul nostro modello rispetto ai dati di addestramento. L'addestramento delle reti neurali comporta la valutazione ripetuta della funzione di perdita su molti dati diversi per misurare quanto è buono il nostro modello, quindi la modifica continua dei pesi del nostro modello utilizzando la backpropagation in modo che la perdita diminuisca, ottimizzando infine il modello per adattarlo ai nostri dati.

### Notazione delle Reti Neurali

Il nucleo di una rete neurale può essere visto come una sequenza di operazioni lineari e non lineari alternate, come mostrato in @fig-neural-net-diagram.

![Diagramma della rete neurale. Fonte: astroML.](images/png/aitrainingnn.png){#fig-neural-net-diagram}

Le reti neurali sono strutturate con layer [strati] di neuroni collegati da pesi (che rappresentano operazioni lineari) e funzioni di attivazione (che rappresentano operazioni non lineari). Esaminando la figura, vediamo come le informazioni fluiscono attraverso la rete, partendo dal layer di input, passando attraverso uno o più layer nascosti, e infine raggiungendo il layer di output. Ogni connessione tra neuroni rappresenta un peso, mentre ciascun neurone applica tipicamente una funzione di attivazione non lineare ai suoi input.

La rete neurale funziona prendendo un vettore di input $x_i$ e passandolo attraverso una serie di layer, ognuno dei quali esegue operazioni lineari e non lineari. L'output della rete a ogni layer $A_j$ può essere rappresentato come:

$$
A_j = f\left(\sum_{i=1}^{N} w_{ij} x_i\right)
$$

Dove:

- $N$ - Il numero totale di feature di input.
- $x_{i}$ - La singola feature di input, dove $i$ varia da $1$ a $N$.
- $w_{ij}$ - I pesi che collegano il neurone $i$ in uno layer al neurone $j$ nel layer successivo, che vengono aggiustati durante l'addestramento.
- $f(\theta)$ - La funzione di attivazione non lineare applicata a ogni layer (ad esempio, ReLU, softmax, ecc.).
- $A_{j}$ - L'output della rete neurale a ogni layer $j$, dove $j$ indica il numero del layer.

Nel contesto di @fig-neural-net-diagram, $x_1, x_2, x_3, x_4,$ e $x_5$ rappresentano le caratteristiche di input. Ogni neurone di input $x_i$ corrisponde a una feature dei dati di input. Le frecce dal layer di input al layer nascosto indicano le connessioni tra i neuroni di input e i neuroni nascosti, con ogni connessione associata a un peso $w_{ij}$.

Il layer nascosto è costituito dai neuroni $a_1, a_2, a_3,$ e $a_4$, ognuno dei quali riceve input da tutti i neuroni nello layer di input. I pesi $w_{ij}$ collegano i neuroni di input ai neuroni nascosti. Ad esempio, $w_{11}$ è il peso che collega l'input $x_1$ al neurone nascosto $a_1$.

Il numero di nodi in ogni layer e il numero totale di layer insieme definiscono l'architettura della rete neurale. Nel primo layer (layer di input), il numero di nodi corrisponde alla dimensionalità dei dati di input, mentre nell'ultimo layer (layer di output), il numero di nodi corrisponde alla dimensionalità dell'output. Il numero di nodi nei layer intermedi può essere impostato arbitrariamente, consentendo flessibilità nella progettazione dell'architettura di rete.

I pesi, che determinano il modo in cui ogni layer della rete neurale interagisce con gli altri, sono matrici di numeri reali. Inoltre, ogni layer in genere include un vettore di bias [polarizzazione], ma qui lo ignoriamo per semplicità. La matrice dei pesi $W_j$ che collega il layer $j-1$ al layer $j$ ha le dimensioni:

$$
W_j \in \mathbb{R}^{d_j \times d_{j-1}}
$$

dove $d_j$ è il numero di nodi nel layer $j$ e $d_{j-1}$ è il numero di nodi nel layer precedente $j-1$.

L'output finale $y_k$ della rete si ottiene applicando un'altra funzione di attivazione $g(\theta)$ alla somma ponderata degli output del layer nascosto:

$$
y = g\left(\sum_{j=1}^{M} w_{jk} A_j\right)
$$

Dove:

- $M$ - Il numero di neuroni nascosti nel layer finale prima dell'output.
- $w_{jk}$ - Il peso tra il neurone nascosto $a_j$ e il neurone di output $y_k$.
- $g(\theta)$ - La funzione di attivazione applicata alla somma ponderata degli output del layer nascosto.

La nostra rete neurale, come definita, esegue una sequenza di operazioni lineari e non lineari sui dati di input ($x_{i}$) per ottenere previsioni ($y_{i}$), che si spera siano una buona risposta a ciò che vogliamo che la rete neurale faccia sull'input (ad esempio, classificare se l'immagine di input è un gatto o meno). La nostra rete neurale può quindi essere rappresentata succintamente come una funzione $N$ che accetta un input $x \in \mathbb{R}^{d_0}$ parametrizzato da $W_1, ..., W_n$ e produce l'output finale $y$:

$$
y = N(x; W_1, ..., W_n) \quad \text{where } A_0 = x
$$

Questa equazione indica che la rete inizia con l'input $A_0 = x$ e calcola iterativamente $A_j$ a ogni layer utilizzando i parametri $W_j$ fino a produrre l'output finale $y$ al layer di output.

Successivamente vedremo come valutare questa rete neurale rispetto ai dati di addestramento introducendo una funzione di perdita.

:::{.callout-note}
Perché sono necessarie le operazioni non lineari? Se avessimo solo layer lineari, l'intera rete sarebbe equivalente a un singolo layer lineare costituito dal prodotto degli operatori lineari. Quindi, le funzioni non lineari svolgono un ruolo chiave nella potenza delle reti neurali poiché migliorano la capacità della rete neurale di adattare le funzioni.
:::

:::{.callout-note}
Anche le convoluzioni sono operatori lineari e possono essere convertite in una moltiplicazione di matrici.
:::

### Funzione Loss come Misura della Bontà di Adattamento Rispetto ai Dati di Addestramento

Dopo aver definito la nostra rete neurale, ci vengono forniti alcuni dati di addestramento, ovvero un set di punti ${(x_j, y_j)}$ per $j=1 \rightarrow M$, dove $M$ è il numero totale di campioni nel set di dati e $j$ indicizza ogni campione. Vogliamo valutare quanto è buona la nostra rete neurale nell'adattare questi dati. Per fare ciò, introduciamo una funzione di perdita, ovvero una funzione che prende l'output della rete neurale su un particolare punto dati $\hat{y_j} = N(x_j; W_1, ..., W_n)$ e lo confronta con la "etichetta" di quel particolare dato (il corrispondente $y_j$) e restituisce un singolo scalare numerico (ovvero un numero reale) che rappresenta quanto è "bene" la rete neurale adatta quel particolare dato; la misura finale di quanto è buona la rete neurale sull'intero set di dati è quindi solo la media delle perdite su tutti i dati.

Esistono molti tipi diversi di funzioni di perdita; ad esempio, nel caso della classificazione delle immagini, potremmo usare la funzione di "cross-entropy loss" [perdita di entropia incrociata], che ci dice quanto bene si confrontano due vettori che rappresentano le previsioni di classificazione (ad esempio, se la nostra previsione prevede che un'immagine sia più probabilmente un cane, ma l'etichetta dice che è un gatto, restituirà una "perdita" elevata, che indica un cattivo adattamento).

Matematicamente, una funzione di perdita è una funzione che prende due vettori con valori reali, uno che rappresenta gli output previsti della rete neurale e l'altro che rappresenta le etichette vere, e restituisce un singolo scalare numerico che rappresenta l'errore o la "perdita".

$$
L: \mathbb{R}^{d_{n}} \times \mathbb{R}^{d_{n}} \longrightarrow \mathbb{R}
$$

Per un singolo esempio di training, la perdita è data da:

$$
L(N(x_j; W_1, ..., W_n), y_j)
$$

dove $\hat{y}_j = N(x_j; W_1, ..., W_n)$ è l'output previsto della rete neurale per l'input $x_j$, and $y_j$ è la vera etichetta.

La perdita totale nell'intero set di dati, $L_{full}$, viene quindi calcolata come la perdita media in tutti i dati di training:

> Funzione di Perdita per l'Ottimizzazione del Modello di Rete Neurale su Dataset
> $$
> L_{full} = \frac{1}{M} \sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)
> $$

### Addestramento di Reti Neurali con Discesa del Gradiente

Ora che possiamo misurare quanto bene la nostra rete si adatta ai dati di training, possiamo ottimizzare i pesi della rete neurale per ridurre al minimo questa perdita. In questo contesto, stiamo denotando $W_i$ come pesi per ogni layer $i$ nella rete. Ad alto livello, modifichiamo i parametri delle matrici a valori reali $W_i$ per ridurre al minimo la funzione di perdita $L_{full}$. Nel complesso, il nostro obiettivo matematico è

> Obiettivo dell'Addestramento della Rete Neurale
> $$
> min_{W_1, ..., W_n} L_{full}
> $$
> $$
> = min_{W_1, ..., W_n} \frac{1}{M} \sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)
> $$

Quindi, come ottimizziamo questo obiettivo? Ricordiamo dal calcolo che la minimizzazione di una funzione può essere eseguita prendendo la derivata della funzione relativa ai parametri di input e modificando i parametri nella direzione del gradiente. Questa tecnica è chiamata a "discesa del gradiente" e concretamente comporta il calcolo della derivata della funzione di perdita $L_{full}$ relativa a $W_1, ..., W_n$ per ottenere un gradiente per questi parametri per fare un passo avanti, poi aggiornare questi parametri nella direzione del gradiente. Quindi, possiamo addestrare la nostra rete neurale utilizzando la discesa del gradiente, che applica ripetutamente la regola di aggiornamento.

> Regola di Aggiornamento della Discesa del Gradiente
> $$
> W_i := W_i - \lambda \frac{\partial L_{full}}{\partial W_i} \mbox{ for } i=1..n
> $$

:::{.callout-note}
In pratica, il gradiente viene calcolato su un mini-batch di punti dati per migliorare l'efficienza computazionale. Questo processo è chiamato "discesa del gradiente stocastico" o "discesa del gradiente batch".
:::

Dove $\lambda$ è la dimensione del passo o il tasso di apprendimento delle nostre modifiche, nell'addestramento della nostra rete neurale, eseguiamo ripetutamente il passaggio precedente fino alla convergenza, o quando la perdita non diminuisce più. @fig-gradient-descent illustra questo processo: vogliamo raggiungere il punto minimo, il che si ottiene seguendo il gradiente (come illustrato con le frecce blu nella figura). Questo precedente approccio è noto come discesa del gradiente completa poiché stiamo calcolando la derivata relativa a tutti i dati di addestramento e solo dopo eseguiamo un singolo passaggio del gradiente; un approccio più efficiente è quello di calcolare il gradiente relativo solo a un batch casuale di dati e poi eseguire un passaggio, un processo noto come discesa del gradiente batch o discesa del gradiente stocastica [@robbins1951stochastic], che è più efficiente poiché ora eseguiamo molti più passi per passaggio di tutti i dati di addestramento. Successivamente, tratteremo la matematica alla base del calcolo del gradiente della funzione di perdita relativa a $W_i$, un processo noto come backpropagation.

![Discesa del gradiente. Fonte: Towards Data Science.](images/png/aitrainingsgd.png){#fig-gradient-descent}

### Backpropagation

L'addestramento delle reti neurali comporta ripetute applicazioni dell'algoritmo di discesa del gradiente, che prevede il calcolo della derivata della funzione di perdita rispetto alle $W_i$. Come calcoliamo la derivata della perdita relativa alle $W_i$, dato che le $W_i$ sono funzioni annidate l'una dell'altra in una rete neurale profonda? Il trucco è sfruttare la **regola della catena:** possiamo calcolare la derivata della perdita relativa alle $W_i$ applicando ripetutamente la regola della catena in un processo completo noto come backpropagation. In particolare, possiamo calcolare i gradienti calcolando la derivata della perdita relativa agli output dell'ultimo layer, poi usarla progressivamente per calcolare la derivata della perdita relativa a ciascun layer precedente a quello di input. Questo processo inizia dalla fine della rete (il layer più vicino all'output) e procede all'indietro, e quindi prende il nome di backpropagation.

Analizziamolo. Possiamo calcolare la derivata della perdita relativa agli _output di ciascun layer della rete neurale_ utilizzando applicazioni ripetute della regola della catena.

$$
\frac{\partial L_{full}}{\partial L_{n}} = \frac{\partial A_{n}}{\partial L_{n}} \frac{\partial L_{full}}{\partial A_{n}}
$$

$$
\frac{\partial L_{full}}{\partial L_{n-1}} = \frac{\partial A_{n-1}}{\partial L_{n-1}} \frac{\partial L_{n}}{\partial A_{n-1}} \frac{\partial A_{n}}{\partial L_{n}} \frac{\partial L_{full}}{\partial A_{n}}  
$$

o più in generale

$$
\frac{\partial L_{full}}{\partial L_{i}} = \frac{\partial A_{i}}{\partial L_{i}} \frac{\partial L_{i+1}}{\partial A_{i}} ... \frac{\partial A_{n}}{\partial L_{n}} \frac{\partial L_{full}}{\partial A_{n}}  
$$

:::{.callout-note}
In quale ordine dovremmo eseguire questo calcolo? Da una prospettiva computazionale, è preferibile eseguire i calcoli dalla fine alla parte frontale.
(ad esempio: prima si calcola $\frac{\partial L_{full}}{\partial A_{n}}$, poi i termini precedenti, anziché iniziare dal centro) poiché ciò evita di materializzare e calcolare grandi jacobiani. Questo perché $\ \frac {\partial L_{full}}{\partial A_{n}}$ è un vettore; quindi, qualsiasi operazione di matrice che include questo termine ha un output che è compresso per essere un vettore. Quindi, eseguire il calcolo dalla fine evita grandi moltiplicazioni matrice-matrice assicurando che i prodotti intermedi siano vettori.
:::

:::{.callout-note}
Nella nostra notazione, assumiamo che le attivazioni intermedie $A_{i}$ siano vettori _colonna_, anziché vettori _riga_, quindi la regola della catena è $\frac{\partial L}{\partial L_{i}} = \frac{\partial L_{i+1}}{\partial L_{i}} ... \frac{\partial L}{\partial L_{n}}$ piuttosto che $\frac{\partial L}{\partial L_{i}} = \frac{\partial L}{\partial L_{n}} ... \frac{\partial L_{i+1}}{\partial L_{i}}$
:::

Dopo aver calcolato la derivata della perdita relativa all'_output di ogni layer_, possiamo facilmente ottenere la derivata della perdita relativa ai _parametri_, utilizzando di nuovo la regola della catena:

$$
\frac{\partial L_{full}}{W_{i}} = \frac{\partial L_{i}}{\partial W_{i}} \frac{\partial L_{full}}{\partial L_{i}}
$$

Ed è in definitiva così che le derivate dei pesi dei layer vengono calcolate usando la backpropagation! Come appare concretamente in un esempio specifico? Di seguito, esaminiamo un esempio specifico di una semplice rete neurale a 2 layer su un'attività di regressione usando una funzione di perdita MSE con input a 100 dimensioni e uno layer nascosto a 30 dimensioni:

> Esempio di backpropagation\
> Supponiamo di avere una rete neurale a due layer
> $$
> L_1 = W_1 A_{0}
> $$
> $$
> A_1 = ReLU(L_1)
> $$
> $$
> L_2 = W_2 A_{1}
> $$
> $$
> A_2 = ReLU(L_2)
> $$
> $$
> NN(x) = \mbox{Let } A_{0} = x \mbox{ then output } A_2
> $$
> dove $W_1 \in \mathbb{R}^{30 \times 100}$ e $W_2 \in \mathbb{R}^{1 \times 30}$. Inoltre, supponiamo di utilizzare la funzione di perdita MSE:
> $$
> L(x, y) = (x-y)^2
> $$
> Vogliamo calcolare
> $$
> \frac{\partial L(NN(x), y)}{\partial W_i} \mbox{ for } i=1,2
> $$
> Notare quanto segue:
> $$
> \frac{\partial L(x, y)}{\partial x} = 2 \times (x-y)
> $$
> $$
> \frac{\partial ReLU(x)}{\partial x} \delta  = \left\{\begin{array}{lr}
> 0 & \text{for } x \leq 0 \\
> 1 & \text{for } x \geq 0 \\
> \end{array}\right\} \odot \delta
> $$
> $$
> \frac{\partial WA}{\partial A} \delta = W^T \delta
> $$
> $$
> \frac{\partial WA}{\partial W} \delta = \delta A^T
> $$
> Quindi abbiamo
> $$
> \frac{\partial L(NN(x), y)}{\partial W_2} = \frac{\partial L_2}{\partial W_2} \frac{\partial A_2}{\partial L_2} \frac{\partial L(NN(x), y)}{\partial A_2}
> $$
> $$
> = (2L(NN(x) - y) \odot ReLU'(L_2)) A_1^T
> $$
> e
> $$
> \frac{\partial L(NN(x), y)}{\partial W_1} = \frac{\partial L_1}{\partial W_1} \frac{\partial A_1}{\partial L_1} \frac{\partial L_2}{\partial A_1} \frac{\partial A_2}{\partial L_2} \frac{\partial L(NN(x), y)}{\partial A_2}
> $$
> $$
> = [ReLU'(L_1) \odot (W_2^T [2L(NN(x) - y) \odot ReLU'(L_2)])] A_0^T
> $$

::: {.callout-tip}
Ricontrollare il lavoro assicurandosi che le forme siano corrette!

* Tutti i prodotti di Hadamard ($\odot$) dovrebbero operare su tensori della stessa forma
* Tutte le moltiplicazioni di matrici dovrebbero operare su matrici che condividono una dimensione comune (ad esempio, m per n, n per k)
* Tutti i gradienti relativi ai pesi dovrebbero avere la stessa forma delle stesse matrici dei pesi
:::

L'intero processo di backpropagation può essere complesso, specialmente per reti molto profonde. Fortunatamente, framework di machine learning come PyTorch supportano la differenziazione automatica, che esegue la backpropagation. In questi framework, dobbiamo semplicemente specificare il passaggio in avanti e le derivate ci verranno calcolate automaticamente. Tuttavia, è utile comprendere il processo teorico che avviene internamente in questi framework di apprendimento automatico.

:::{.callout-note}
Come visto sopra, le attivazioni intermedie $A_i$ vengono riutilizzate nella backpropagation. Per migliorare le prestazioni, queste attivazioni vengono memorizzate nella cache dal passaggio in avanti per evitare di essere ricalcolate. Tuttavia, le attivazioni devono essere mantenute in memoria tra i passaggi in avanti e indietro, il che comporta un maggiore utilizzo della memoria. Se la rete e le dimensioni del batch sono grandi, ciò potrebbe causare problemi di memoria. Analogamente, le derivate rispetto agli output di ogni layer vengono memorizzate nella cache per evitare il ricalcolo.
:::

:::{#exr-nn .callout-caution collapse="true"}

### Reti Neurali con Backpropagation e Discesa del Gradiente

Scoprire la matematica dietro le potenti reti neurali! Il deep learning potrebbe sembrare magico, ma è radicato nei principi matematici. In questo capitolo, abbiamo scomposto la notazione delle reti neurali, le funzioni di perdita e la potente tecnica della backpropagation. Ora, prepariamoci a implementare questa teoria con questi notebook Colab. Immergersi nel cuore di come le reti neurali apprendono. Si vedrà la matematica dietro la backpropagation e la discesa del gradiente, aggiornando quei pesi passo dopo passo.

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/jigsawlabs-student/pytorch-intro-curriculum/blob/main/5-training-mathematically/20-backpropagation-and-gradient-descent.ipynb)

:::

## Grafi del Calcolo Differenziabili

In generale, la discesa del gradiente stocastico mediante backpropagation può essere eseguita su qualsiasi grafo computazionale che un utente può definire, a condizione che le operazioni del calcolo siano differenziabili. Pertanto, le librerie generiche di deep learning come PyTorch e Tensorflow consentono agli utenti di specificare il loro processo computazionale (ad esempio, reti neurali) come grafo computazionale. La backpropagation viene eseguita automaticamente tramite differenziazione automatica quando la discesa del gradiente stocastico viene eseguita su questi grafi computazionali. Inquadrare l'addestramento dell'IA come un problema di ottimizzazione su grafi di calcolo differenziabili è un modo generale per comprendere cosa sta accadendo internamente con i sistemi di deep learning.

La struttura raffigurata in @fig-computational-graph mostra un segmento di un grafo computazionale differenziabile. In questo grafo, l'input 'x' viene elaborato tramite una serie di operazioni: viene prima moltiplicato per una matrice di pesi 'W' (MatMul), poi aggiunto a un bias 'b' (Add) e infine passato a una funzione di attivazione, Rectified Linear Unit (ReLU). Questa sequenza di operazioni ci fornisce l'output C. La natura differenziabile del grafo significa che ogni operazione ha un gradiente ben definito. La differenziazione automatica, come implementata nei framework ML, sfrutta questa proprietà per calcolare in modo efficiente i gradienti della perdita rispetto a ciascun parametro nella rete (ad esempio, 'W' e 'b').

![Grafo Computazionale. Fonte: TensorFlow.](./images/png/graph.png){#fig-computational-graph height=40%}

## Dati di Training

Per consentire un training efficace della rete neurale, i dati disponibili devono essere suddivisi in set di training, di validazione e di test. Il set di training viene utilizzato per addestrare i parametri del modello. Il set di validazione valuta il modello durante il training per ottimizzare gli iperparametri e prevenire l'overfitting. Il set di test fornisce una valutazione finale imparziale delle prestazioni del modello addestrato.

Mantenere chiare suddivisioni tra training, validation e test con dati rappresentativi è fondamentale per addestrare, ottimizzare e valutare correttamente i modelli per ottenere le migliori prestazioni nel mondo reale. A tal fine, scopriremo le insidie o gli errori comuni che le persone commettono quando creano queste suddivisioni dei dati.

@tbl-training_splits confronta le differenze tra le suddivisioni dei dati di training, validazione e test:

+----------------------+------------------------------------------------------------------+------------------------+
| Suddivisione         | Scopo                                                            | Dimensioni tipiche     |
+:=====================+:=================================================================+:=======================+
| Set di addestramento | Addestrare i parametri del modello                               | 60-80% dei dati totali |
+----------------------+------------------------------------------------------------------+------------------------+
| Set di validazione   | Valutare il modello durante l'addestramento per ottimizzare      | ∼20% dei dati totali   |
|                      | gli iperparametri e prevenire l'overfitting                      |                        |
+----------------------+------------------------------------------------------------------+------------------------+
| Set di test          | Fornire una valutazione imparziale del modello finale addestrato | ∼20% dei dati totali   |
+----------------------+------------------------------------------------------------------+------------------------+

: Confronto tra suddivisioni di dati di training, validazione e test. {#tbl-training_splits .striped .hover}

### Suddivisioni di Dataset

#### Set di Training

Il set di training viene utilizzato per addestrare il modello. È il sottoinsieme più grande, in genere il 60-80% dei dati totali. Il modello vede e impara dai dati di addestramento per fare previsioni. È necessario un set di training sufficientemente grande e rappresentativo affinché il modello apprenda efficacemente i pattern sottostanti.

#### Set di Validazione

Il set di validazione valuta il modello durante l'addestramento, in genere dopo ogni epoca. Solitamente, il 20% dei dati viene assegnato a questo set. Il modello non impara né aggiorna i suoi parametri in base ai dati di validazione. Vengono usati per ottimizzare gli iperparametri e apportare altre modifiche per migliorare l'addestramento. Il monitoraggio di metriche come perdita e accuratezza sul set di validazione impedisce l'overfitting solo sui dati di addestramento.

#### Set di Test

Il set di test agisce come un dataset che il modello non ha visto durante l'addestramento. Viene utilizzato per fornire una valutazione imparziale del modello addestrato finale. In genere, il 20% dei dati è riservato ai test. Mantenere un set di test "hold-out" [esterno] è fondamentale per ottenere una stima accurata di come il modello addestrato si comporterebbe su dati non ancora visti del mondo reale. La mancanza di dati dal set di test deve essere evitata a tutti i costi.

Le proporzioni relative dei set di training, validazione e test possono variare in base alle dimensioni dei dati e all'applicazione. Tuttavia, seguire le linee guida generali per una suddivisione 60/20/20 è un buon punto di partenza. Un'attenta suddivisione dei dati garantisce che i modelli siano adeguatamente addestrati, ottimizzati e valutati per ottenere le prestazioni migliori.

@vid-train-dev-test spiega come suddividere correttamente il dataset in set di training, validazione e test, assicurando un processo di training ottimale.

:::{#vid-train-dev-test .callout-important}

# Train/Dev/Test Sets

{{< video https://www.youtube.com/watch?v=1waHlpKiNyY >}}

:::

### Errori e Insidie Comuni

#### Dati di Training Insufficienti

Assegnare troppo pochi dati al set di training è un errore comune quando si suddividono i dati, il che può avere un impatto significativo sulle prestazioni del modello. Se il set di training è troppo piccolo, il modello non avrà campioni sufficienti per apprendere in modo efficace i veri pattern nei dati. Ciò comporta un'elevata varianza e impedisce al modello di generalizzare bene ai nuovi dati.

Ad esempio, se si addestra un modello di classificazione delle immagini per riconoscere cifre scritte a mano, fornire solo 10 o 20 immagini per classe di cifre sarebbe del tutto inadeguato. Il modello avrebbe bisogno di più esempi per catturare le ampie varianze negli stili di scrittura, rotazioni, larghezze dei tratti e altre varianti.

Come regola generale, la dimensione del training set dovrebbe essere di almeno centinaia o migliaia di esempi affinché la maggior parte degli algoritmi di apprendimento automatico funzioni in modo efficace. A causa dell'elevato numero di parametri, il set di training spesso deve essere di decine o centinaia di migliaia per le reti neurali profonde, in particolare quelle che utilizzano layer convoluzionali.

Dati di training insufficienti si manifestano in genere in sintomi quali alti tassi di errore su set di validazione/test, bassa accuratezza del modello, alta varianza e overfitting su campioni di set di training di piccole dimensioni. La soluzione è raccogliere più dati di training di qualità. Le tecniche di data augmentation possono anche aiutare ad aumentare virtualmente le dimensioni dei dati di training per immagini, audio, ecc.

È importante considerare attentamente la complessità del modello e la difficoltà del problema quando si assegnano i campioni di training per garantire che siano disponibili dati sufficienti affinché il modello possa apprendere correttamente. Si consiglia inoltre di seguire le linee guida sulle dimensioni minime dei set di training per diversi algoritmi. Sono necessari più dati di training per mantenere il successo complessivo di qualsiasi applicazione di machine learning.

Si consideri @fig-over-under-fitting dove proviamo a classificare/suddividere i dati in due categorie (qui, per colore): a sinistra, l'overfitting è rappresentato da un modello che ha appreso troppo bene le sfumature nei dati di training (o il set di dati era troppo piccolo o abbiamo eseguito il modello per troppo tempo), facendo sì che segua il rumore insieme al segnale, come indicato dalle eccessive curve della linea. Il lato destro mostra l'underfitting, dove la semplicità del modello gli impedisce di catturare la struttura sottostante del dataset, con conseguente linea che non si adatta bene ai dati. Il grafico centrale rappresenta un adattamento ideale, dove il modello bilancia bene tra generalizzazione e adattamento, catturando la tendenza principale dei dati senza essere influenzato da valori anomali. Sebbene il modello non sia un adattamento perfetto (manca di alcuni punti), ci interessa di più la sua capacità di riconoscere pattern generali piuttosto che valori anomali idiosincratici.

![Adattamento dei dati: overfitting, right fit e underfitting. Fonte: MathWorks.](images/png/fits.png){#fig-over-under-fitting}

@fig-fitting-time il processo di adattamento dei dati nel tempo. Durante l'addestramento, cerchiamo il "punto ottimale" tra underfitting e overfitting. Inizialmente, quando il modello non ha avuto abbastanza tempo per apprendere i pattern nei dati, ci troviamo nella zona di underfitting, indicata da alti tassi di errore sul set di convalida (da ricordare che il modello è addestrato sul set di addestramento e testiamo la sua generalizzabilità sul set di convalida o sui dati che non ha mai visto prima). A un certo punto, raggiungiamo un minimo globale per i tassi di errore e idealmente vogliamo interrompere l'addestramento lì. Se continuiamo l'addestramento, il modello inizierà a "memorizzare" o a conoscere i dati troppo bene, tanto che il tasso di errore inizierà a risalire, poiché il modello non riuscirà a generalizzare a dati che non ha mai visto prima.

![Adattamento dei dati nel tempo. Fonte: IBM.](images/png/aitrainingfit.png){#fig-fitting-time}

Il @vid-bias fornisce una panoramica di bias e varianza e la relazione tra i due concetti e l'accuratezza del modello.

:::{#vid-bias .callout-important}

# Bias/Varianza

{{< video https://www.youtube.com/watch?v=SjQyLhQIXSM >}}

:::

#### Perdita di Dati Tra Set

Il "data leakage" [perdita di dati] si riferisce al trasferimento involontario di informazioni tra i set di training, convalida e test. Ciò viola il presupposto fondamentale che le divisioni siano reciprocamente esclusive. La perdita di dati porta a risultati di valutazione seriamente compromessi e metriche di prestazioni gonfiate.

Un modo comune in cui si verifica la perdita di dati è se alcuni campioni del set di test vengono inavvertitamente inclusi nei dati di training. Quando si valuta il set di test, il modello ha già visto alcuni dati, il che fornisce punteggi eccessivamente ottimistici. Ad esempio, se il 2% dei dati di test trapelano nel set di training di un classificatore binario, può comportare un aumento della precisione fino al 20%!

Se le divisioni dei dati non vengono eseguite con attenzione, possono verificarsi forme di perdita più sottili. Se le divisioni non vengono randomizzate e mescolate correttamente, i campioni che sono vicini tra loro nel set di dati potrebbero finire nella stessa divisione, portando a distorsioni della distribuzione. Ciò crea una fuga di informazioni basata sulla prossimità nel set di dati.

Un altro caso è quando i set di dati hanno campioni collegati, intrinsecamente connessi, come grafici, reti o dati di serie temporali. La suddivisione "ingenua" può isolare nodi o intervalli di tempo connessi in set diversi. I modelli possono fare ipotesi non valide basate su informazioni parziali.

Per prevenire la perdita di dati è necessario creare una solida separazione tra le suddivisioni: nessun campione dovrebbe esistere in più di una suddivisione. Il mescolamento e la suddivisione randomizzata aiutano a creare divisioni robuste. Le tecniche di "cross-validation" [validazione incrociata] possono essere utilizzate per una valutazione più rigorosa. Rilevare la perdita è difficile, ma i segnali rivelatori includono modelli che funzionano molto meglio sui dati di test rispetto a quelli di validazione.

La perdita di dati compromette gravemente la validità della valutazione perché il modello ha già visto parzialmente i dati di test. Nessuna quantità di messa a punto o architetture complesse può sostituire le suddivisioni nette dei dati. È meglio essere prudenti e creare una separazione completa tra le suddivisioni per evitare questo errore fondamentale nelle pipeline di machine learning.

#### Set di Validazione Piccolo o Non Rappresentativo

Il set di validazione viene utilizzato per valutare le prestazioni del modello durante l'addestramento e per ottimizzare gli iperparametri. Per valutazioni affidabili e stabili, il set di validazione deve essere sufficientemente ampio e rappresentativo della distribuzione dei dati reali. Tuttavia, ciò può rendere più impegnativa la selezione e l'ottimizzazione del modello.

Ad esempio, se il set di validazione contiene solo 100 campioni, le metriche calcolate avranno un'elevata varianza. A causa del rumore, l'accuratezza può variare fino al 5-10% tra le epoche. Questo rende difficile sapere se un calo nell'accuratezza della validazione è dovuto a un overfitting o a una varianza naturale. Con un set di validazione più ampio, diciamo 1000 campioni, le metriche saranno molto più stabili.

Inoltre, se il set di validazione non è rappresentativo, forse mancano alcune sottoclassi, la capacità stimata del modello potrebbe essere gonfiata. Ciò potrebbe portare a scelte di iperparametri scadenti o a interruzioni premature dell'addestramento. I modelli selezionati in base a tali set di validazione distorti non si generalizzano bene ai dati reali.

Una buona regola pratica è che la dimensione del set di convalida dovrebbe essere di almeno diverse centinaia di campioni e fino al 10-20% del set di addestramento, lasciando comunque campioni sufficienti per l'addestramento. Le divisioni dovrebbero anche essere stratificate, il che significa che le proporzioni di classe nel set di validazione dovrebbero corrispondere a quelle nel set di dati completo, soprattutto se si lavora con set di dati sbilanciati. Un set di validazione più ampio che rappresenti le caratteristiche dei dati originali è essenziale per una corretta selezione e messa a punto del modello.

#### Riutilizzo del Set di Test Più Volte

Il set di test è progettato per fornire una valutazione imparziale del modello completamente addestrato solo una volta alla fine del processo di sviluppo del modello. Riutilizzare il set di test più volte durante lo sviluppo per la valutazione del modello, la messa a punto degli iperparametri, la selezione del modello, ecc., può causare un overfitting sui dati di test. Invece, si deve riservare il set di test per una valutazione finale del modello completamente addestrato, trattandolo come una scatola nera per simularne le prestazioni su dati reali. Questo approccio fornisce metriche affidabili per determinare se il modello è pronto per la distribuzione in produzione.

Se il set di test viene riutilizzato come parte del processo di validazione, il modello potrebbe iniziare a vedere e imparare dai campioni di test. Questo, insieme all'ottimizzazione intenzionale o meno delle prestazioni del modello sul set di test, può gonfiare artificialmente metriche come l'accuratezza.

Ad esempio, supponiamo che il set di test venga utilizzato ripetutamente per la selezione del modello su 5 architetture. In tal caso, il modello potrebbe raggiungere il 99% di accuratezza del test memorizzando i campioni anziché apprendere pattern generalizzabili. Tuttavia, quando implementati nel mondo reale, l'accuratezza dei nuovi dati potrebbe scendere del 60%.

La prassi migliore è interagire con il set di test solo una volta alla fine per segnalare metriche imparziali su come il modello finale ottimizzato si comporterebbe nel mondo reale.  Durante lo sviluppo del modello, il set di convalida dovrebbe essere utilizzato per tutte le attività di ottimizzazione dei parametri, selezione del modello, arresto anticipato e simili. È importante riservare una parte, come il 20-30% dell'intero set di dati, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l'ottimizzazione o la selezione del modello durante lo sviluppo.

Non mantenere un set "hold-out " non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrità finale sull'efficacia nel mondo reale. Mantenere la completa separazione di addestramento/validazione dal set di test è essenziale per ottenere stime accurate delle prestazioni del modello. Anche piccole deviazioni da un singolo utilizzo del set di test potrebbero falsare positivamente i risultati e le metriche, fornendo una visione eccessivamente ottimistica dell'efficacia nel mondo reale.

#### Stesse Suddivisioni dei Dati negli Esperimenti

Quando si confrontano diversi modelli di machine learning o si sperimentano varie architetture e iperparametri, utilizzare le stesse suddivisioni dei dati per l'addestramento, la validazione e il test nei diversi esperimenti può introdurre distorsioni e invalidare le comparazioni.

Se le stesse suddivisioni vengono riutilizzate, i risultati della valutazione potrebbero essere più bilanciati e misurare accuratamente quale modello funziona meglio. Ad esempio, una certa suddivisione casuale dei dati potrebbe favorire il modello A rispetto al modello B indipendentemente dagli algoritmi. Riutilizzare questa suddivisione causerà quindi distorsioni a favore del modello A.

Invece, le suddivisioni dei dati dovrebbero essere randomizzate o mescolate per ogni iterazione sperimentale. Ciò garantisce che la casualità nel campionamento delle suddivisioni non conferisca un vantaggio ingiusto a nessun modello.

Con diverse suddivisioni per esperimento, la valutazione diventa più solida. Ogni modello viene testato su un'ampia gamma di set di test estratti casualmente dalla popolazione complessiva, attenuando la variazione e rimuovendo la correlazione tra i risultati.

La prassi corretta è quella di impostare un "seed" casuale prima di suddividere i dati per ogni esperimento. La suddivisione dovrebbe avvenire dopo il rimescolamento/ricampionamento come parte della pipeline sperimentale. Eseguire confronti sulle stesse suddivisioni viola l'ipotesi i.i.d (indipendenti e identicamente distribuite) richiesta per la validità statistica.

Le suddivisioni univoche sono essenziali per confronti di modelli equi. Sebbene richieda un'elaborazione più intensiva, l'allocazione randomizzata per esperimento rimuove la distorsione del campionamento e consente un benchmarking valido. Ciò evidenzia le vere differenze nelle prestazioni del modello indipendentemente dalle caratteristiche di una particolare suddivisione.

#### Mancata Stratificazione delle Suddivisioni

Quando si suddividono i dati in set di training, validazione e test, la mancata stratificazione delle suddivisioni può comportare una rappresentazione non uniforme delle classi target tra le suddivisioni e introdurre un bias di campionamento. Ciò è particolarmente problematico per i set di dati sbilanciati.

La suddivisione stratificata implica il campionamento dei dati in modo che la proporzione di classi di output sia approssimativamente preservata in ogni suddivisione. Ad esempio, se si esegue una suddivisione training-test 70/30 su un set di dati con campioni negativi al 60% e positivi al 40%, la stratificazione garantisce esempi negativi al ~60% e positivi al ~40% sia nei set di training che nei set di test.

Senza stratificazione, la casualità potrebbe comportare che la suddivisione di training abbia campioni positivi al 70% mentre il test ha campioni positivi al 30%. Il modello addestrato su questa distribuzione di training distorta non si generalizzerà bene. Lo squilibrio delle classi compromette anche le metriche del modello come l'accuratezza.

La stratificazione funziona meglio quando viene eseguita utilizzando etichette, sebbene proxy come il clustering possano essere utilizzati per l'apprendimento non supervisionato. Diventa essenziale per set di dati altamente distorti con classi rare che potrebbero essere facilmente omesse dalle suddivisioni.

Librerie come Scikit-Learn hanno metodi di suddivisione stratificati nativi. Non utilizzarli potrebbe inavvertitamente introdurre bias di campionamento e danneggiare le prestazioni del modello sui gruppi minoritari. Dopo aver eseguito le suddivisioni, il bilanciamento complessivo delle classi dovrebbe essere esaminato per garantire una rappresentazione uniforme tra le suddivisioni.

La stratificazione fornisce un set di dati bilanciato sia per l'addestramento del modello che per la valutazione. Sebbene la semplice suddivisione casuale sia facile, tenendo conto delle esigenze di stratificazione, specialmente per dati sbilanciati nel mondo reale, si traduce in uno sviluppo e una valutazione del modello più solidi.

#### Ignorare le Dipendenze delle Serie Temporali

I dati delle serie temporali hanno una struttura temporale intrinseca con osservazioni dipendenti dal contesto passato. Suddividere ingenuamente i dati delle serie temporali in set di training e test senza tenere conto di questa dipendenza porta a perdite di dati e bias di lookahead.

Ad esempio, suddividere semplicemente una serie temporale nel primo 70% di training e nell'ultimo 30% come dati di test contaminerà i dati di training con dati futuri. Il modello può usare queste informazioni per "sbirciare" in avanti durante il training.

Ciò si traduce in una valutazione eccessivamente ottimistica delle prestazioni del modello. Il modello può sembrare che preveda il futuro in modo accurato, ma in realtà ha appreso implicitamente in base ai dati futuri, il che non si traduce in prestazioni nel mondo reale.

Dovrebbero essere utilizzate tecniche di validazione incrociata delle serie temporali appropriate, come il concatenamento in avanti, per preservare l'ordine e la dipendenza. Il set di test dovrebbe contenere solo dati da una finestra temporale futura a cui il modello non è stato esposto per il training.

Non tenere conto delle relazioni temporali porta a ipotesi di causalità non valide. Se i dati di training contengono dati futuri, il modello potrebbe anche dover imparare come estrapolare ulteriormente le previsioni.

Mantenere il flusso temporale degli eventi ed evitare il bias di lookahead è fondamentale per addestrare e testare correttamente i modelli di serie temporali. Ciò garantisce che possano davvero prevedere pattern futuri e non solo memorizzare i dati di training passati.

#### Nessun Dato Non Visto per la Valutazione Finale

Un errore comune quando si suddividono i dati è non metterne da parte una porzione solo per la valutazione finale del modello completato. Tutti i dati vengono utilizzati per training, validazione e set di test durante lo sviluppo.

Questo non lascia dati non visti per ottenere una stima imparziale di come il modello finale ottimizzato si comporterebbe nel mondo reale. Le metriche sul set di test utilizzate durante lo sviluppo potrebbero riflettere solo parzialmente le reali capacità del modello.

Ad esempio, scelte come l'arresto anticipato e l'ottimizzazione degli iperparametri sono spesso ottimizzate in base alle prestazioni del set di test. Questo accoppia il modello ai dati di test. È necessario un set di dati non visto per interrompere questo accoppiamento e ottenere metriche reali del mondo reale.

La "best practice" è quella di riservare una parte, come il 20-30% del set di dati completo, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l'ottimizzazione o la selezione del modello durante lo sviluppo.

Il salvataggio di alcuni dati non visti consente di valutare il modello completamente addestrato come una scatola nera su dati del mondo reale. Questo fornisce metriche affidabili per decidere se il modello è pronto per la distribuzione in produzione.

Non mantenere un set "hold-out " non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrità finale sull'efficacia nel mondo reale.

#### Sovra-ottimizzazione del Set di Validazione

Il set di validazione è pensato per guidare il processo di training del modello, non per fungere da dati di training aggiuntivi. L'eccessiva ottimizzazione del set di validazione per massimizzare le metriche delle prestazioni lo tratta più come un set di training secondario, portando a metriche gonfiate e scarsa generalizzazione.

Ad esempio, tecniche come l'ottimizzazione estensiva degli iperparametri o l'aggiunta di incrementi di dati mirati a migliorare l'accuratezza della convalida possono far sì che il modello si adatti troppo ai dati di validazione. Il modello può raggiungere un'accuratezza di validazione del 99% ma solo un'accuratezza di test del 55%.

Analogamente, riutilizzare il set di validazione per un arresto anticipato può anche ottimizzare il modello specificamente per quei dati. L'arresto alle migliori prestazioni di validazione sovra-adatta il rumore e le fluttuazioni causate dalle piccole dimensioni di validazione.

Il set di validazione funge da proxy per ottimizzare e selezionare i modelli. Tuttavia, l'obiettivo rimane massimizzare le prestazioni dei dati del mondo reale, non il set di validazione. Ridurre al minimo la perdita o l'errore sui dati di validazione non si traduce automaticamente in una buona generalizzazione.

Un buon approccio è quello di mantenere l'uso del set di validazione al minimo: gli iperparametri possono essere regolati grossolanamente prima sui dati di training, ad esempio. Il set di validazione guida il training ma non dovrebbe influenzare o alterare il modello stesso. È uno strumento diagnostico, non di ottimizzazione.

Quando si valutano le prestazioni sul set di validazione, bisogna fare attenzione a non sovra-adattare. Sono necessari dei compromessi per costruire modelli che funzionino bene sulla popolazione complessiva e non siano eccessivamente regolati sui campioni di validazione.

## Algoritmi di Ottimizzazione

Stochastic gradient descent (SGD) è un algoritmo di ottimizzazione semplice ma potente per l'addestramento di modelli di machine learning. Funziona stimando il gradiente della funzione di perdita relativa ai parametri del modello utilizzando un singolo esempio di addestramento e poi aggiornando i parametri nella direzione che riduce la perdita.

Sebbene concettualmente semplice, SGD necessita di alcune aree di miglioramento. Innanzitutto, scegliere un tasso di apprendimento appropriato può essere difficile: troppo piccolo e i progressi sono molto lenti; troppo grande e i parametri possono oscillare e non convergere. In secondo luogo, SGD tratta tutti i parametri in modo uguale e indipendente, il che potrebbe non essere l'ideale in tutti i casi. Infine, SGD vanilla [standard] utilizza solo informazioni sul gradiente di primo ordine, il che si traduce in progressi lenti su problemi mal condizionati.

### Ottimizzazioni

Nel corso degli anni, sono state proposte varie ottimizzazioni per accelerare e migliorare l'SGD vanilla. @ruder2016overview fornisce un'eccellente panoramica dei diversi ottimizzatori. In breve, diverse tecniche di ottimizzazione SGD comunemente utilizzate includono:

**Momentum:** Accumula un vettore di velocità in direzioni di gradiente persistente attraverso le iterazioni. Ciò aiuta ad accelerare i progressi smorzando le oscillazioni e mantiene i progressi in direzioni coerenti.

**Nesterov Accelerated Gradient (NAG):** Una variante di momentum che calcola i gradienti in "look ahead" anziché nella posizione del parametro corrente. Questo aggiornamento anticipatorio impedisce l'overshooting mentre il momentum mantiene il progresso accelerato.

**Adagrad:** Un algoritmo di velocità di apprendimento adattivo che mantiene una velocità di apprendimento per parametro ridotta proporzionalmente alla somma storica dei gradienti di ciascun parametro. Aiuta a eliminare la necessità di regolare manualmente i tassi di apprendimento [@john2010adaptive].

**Adadelta:** Una modifica ad Adagrad limita la finestra dei gradienti passati accumulati, riducendo così il decadimento aggressivo dei tassi di apprendimento [@zeiler2012reinforcement].

**RMSProp:** Divide il tasso di apprendimento per una media esponenzialmente decrescente dei gradienti quadrati. Ciò ha un effetto di normalizzazione simile ad Adagrad ma non accumula i gradienti nel tempo, evitando un rapido decadimento dei tassi di apprendimento [@hinton2017overview].

**Adam:** Combinazione di momentum e rmsprop dove rmsprop modifica il tasso di apprendimento in base alla media delle recenti ampiezze dei gradienti. Mostra un progresso iniziale molto rapido e regola automaticamente le dimensioni dei passi [@diederik2015adam].

**AMSGrad:** Una variante di Adam che assicura una convergenza stabile mantenendo il massimo dei gradienti quadratici passati, impedendo al tasso di apprendimento di aumentare durante l'addestramento [@reddi2019convergence].

Tra questi metodi, Adam è ampiamente considerato l'algoritmo di ottimizzazione di riferimento per molte attività di deep-learning. Supera costantemente SGD vanilla in termini di velocità di addestramento e prestazioni. Altri ottimizzatori potrebbero essere più adatti in alcuni casi, in particolare per modelli più semplici.

### Compromessi

@tbl-optim-algos è una tabella di pro e contro per alcuni dei principali algoritmi di ottimizzazione per l'addestramento di reti neurali:

+-------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| Algoritmo                     | Pro                                                                          | Contro                                                                              |
+:==============================+:=============================================================================+:====================================================================================+
| Momentum                      | - Convergenza più rapida dovuta all'accelerazione lungo i gradienti          | - Richiede la messa a punto del parametro momentum                                  |
|                               | - Minore oscillazione rispetto a SGD vanilla                                 |                                                                                     |
+-------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| Nesterov Accelerated Gradient | - Più veloce dello slancio standard in alcuni casi                           | - Più complesso da comprendere intuitivamente                                       |
| (NAG)                         | - Gli aggiornamenti anticipati impediscono il superamento                    |                                                                                     |
+-------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| Adagrad                       | - Elimina la necessità di regolare manualmente i tassi di apprendimento      | - Il tasso di apprendimento potrebbe decadere troppo rapidamente su gradienti densi |
|                               | - Funziona bene su gradienti radi                                            |                                                                                     |
+-------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| Adadelta                      | - Decadimento del tasso di apprendimento meno aggressivo rispetto ad Adagrad | - Ancora sensibile al valore iniziale del tasso di apprendimento                    |
+-------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| RMSProp                       | - Regola automaticamente i tassi di apprendimento                            | - Nessun aspetto negativo importante                                                |
|                               | - Funziona bene nella pratica                                                |                                                                                     |
+-------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| Adam                          | - Combinazione di momentum e tassi di apprendimento adattivo                 | - Prestazioni di generalizzazione leggermente peggiori in alcuni casi               |
|                               | - Convergenza efficiente e veloce                                            |                                                                                     |
+-------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| AMSGrad                       | - Miglioramento di Adam che affronta il problema della generalizzazione      | - Non è stato utilizzato/testato così ampiamente come Adam                          |
+-------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+

: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione. {#tbl-optim-algos .striped .hover}

### Algoritmi di Benchmarking

Non esiste un singolo metodo migliore per tutti i tipi di problemi. Ciò significa che abbiamo bisogno di un benchmarking completo per identificare l'ottimizzatore più efficace per set di dati e modelli specifici. Le prestazioni di algoritmi come Adam, RMSProp e Momentum variano a seconda delle dimensioni del batch, dei programmi di apprendimento, dell'architettura del modello, della distribuzione dei dati e della regolarizzazione. Queste variazioni sottolineano l'importanza di valutare ogni ottimizzatore in diverse condizioni.

Prendiamo ad esempio Adam, che spesso eccelle nelle attività di visione artificiale, a differenza di RMSProp, che potrebbe mostrare una migliore generalizzazione in determinate attività di elaborazione del linguaggio naturale. La forza di Momentum risiede nella sua accelerazione in scenari con direzioni di gradiente coerenti, mentre i tassi di apprendimento adattivo di Adagrad sono più adatti per problemi di gradiente sparso.

Questa vasta gamma di interazioni tra ottimizzatori dimostra la difficoltà di dichiarare un singolo algoritmo universalmente superiore. Ogni ottimizzatore ha punti di forza unici, rendendo fondamentale valutare vari metodi per scoprire empiricamente le loro condizioni di applicazione ottimali.

Un approccio di benchmarking completo dovrebbe valutare la velocità di convergenza e fattori come errore di generalizzazione, stabilità, sensibilità degli iperparametri ed efficienza computazionale, tra gli altri. Ciò comporta il monitoraggio delle curve di apprendimento di training e convalida su più esecuzioni e il confronto degli ottimizzatori su vari set di dati e modelli per comprenderne i punti di forza e di debolezza.

AlgoPerf, introdotto da @dahl2023benchmarking, risponde alla necessità di un sistema di benchmarking robusto. Questa piattaforma valuta le prestazioni dell'ottimizzatore utilizzando criteri quali curve di loss [perdita] di training, errore di generalizzazione, sensibilità agli iperparametri ed efficienza computazionale. AlgoPerf testa vari metodi di ottimizzazione, tra cui Adam, LAMB e Adafactor, su diversi tipi di modelli come CNN e RNN/LSTM su set di dati stabiliti. Utilizza la "containerizzazione" e la raccolta automatica di metriche per ridurre al minimo le incongruenze e consente esperimenti controllati su migliaia di configurazioni, fornendo una base affidabile per confrontare gli ottimizzatori.

Le informazioni ottenute da AlgoPerf e benchmark simili sono inestimabili per guidare la scelta ottimale o la messa a punto degli ottimizzatori. Abilitando valutazioni riproducibili, questi benchmark contribuiscono a una comprensione più approfondita delle prestazioni di ciascun ottimizzatore, aprendo la strada a innovazioni future e progressi accelerati nel settore.

## Ottimizzazione degli Iperparametri

Gli iperparametri sono impostazioni importanti nei modelli di machine learning che incidono notevolmente sulle prestazioni finali dei modelli. A differenza di altri parametri del modello che vengono appresi durante l'addestramento, gli iperparametri vengono specificati dai "data scientist" o dagli ingegneri del machine learning prima dell'addestramento del modello.

La scelta dei valori degli iperparametri corretti consente ai modelli di apprendere pattern dai dati in modo efficace. Alcuni esempi di iperparametri chiave negli algoritmi di apprendimento automatico includono:

* **Reti neurali:** Velocità di apprendimento, dimensione del batch, numero di unità nascoste, funzioni di attivazione
* **Macchine a vettori di supporto:** Forza di regolarizzazione, tipo di kernel e parametri
* **Random forest:** Numero di alberi, profondità dell'albero
* **K-means:** Numero di cluster

Il problema è che non ci sono regole pratiche affidabili per scegliere configurazioni ottimali degli iperparametri: in genere si devono provare valori diversi e valutare le prestazioni. Questo processo è chiamato "hyperparameter tuning" [ottimizzazione degli iperparametri].

Nei primi anni del moderno deep learning, i ricercatori erano ancora alle prese con problemi di convergenza instabile e lenta. I punti dolenti comuni includevano perdite di training che fluttuavano selvaggiamente, gradienti che esplodevano o svanivano e un'ampia serie di tentativi ed errori necessari per addestrare le reti in modo affidabile. Di conseguenza, un punto focale iniziale era l'utilizzo di iperparametri per controllare l'ottimizzazione del modello. Ad esempio, tecniche seminali come la normalizzazione batch consentivano una convergenza più rapida del modello regolando gli aspetti dello spostamento interno delle covariate. I metodi di velocità di apprendimento adattivo hanno anche mitigato la necessità di estese pianificazioni manuali. Questi affrontavano problemi di ottimizzazione durante l'addestramento, come la divergenza incontrollata del gradiente. Le velocità di apprendimento adattate con attenzione sono anche il fattore di controllo primario per ottenere una convergenza rapida e stabile anche oggi.

Con l'espansione esponenziale della capacità computazionale negli anni successivi, modelli molto più grandi potevano essere addestrati senza cadere preda di problemi di pura ottimizzazione numerica. L'attenzione si è spostata verso la generalizzazione, sebbene una convergenza efficiente fosse un prerequisito fondamentale. Tecniche all'avanguardia come "Transformers" hanno introdotto miliardi di parametri. A tali dimensioni, gli iperparametri relativi a capacità, regolarizzazione, ensembling [raggruppamento], ecc., hanno assunto un ruolo centrale per la messa a punto, anziché solo le metriche di convergenza grezze.

La lezione è che comprendere l'accelerazione e la stabilità del processo di ottimizzazione stesso costituisce il lavoro di base. Schemi di inizializzazione, dimensioni dei batch, decadimenti di peso e altri iperparametri di training rimangono indispensabili oggi. Dominare una convergenza rapida e impeccabile consente ai professionisti di espandere la propria attenzione sulle esigenze emergenti relative alla messa a punto di parametri quali accuratezza, robustezza ed efficienza su larga scala.

### Algoritmi di Ricerca

Quando si tratta del processo critico di ottimizzazione degli iperparametri, ci sono diversi algoritmi sofisticati su cui gli specialisti del machine learning si affidano per cercare sistematicamente nel vasto spazio di possibili configurazioni dei modelli. Alcuni degli algoritmi di ricerca degli iperparametri più importanti includono:

* **Grid Search:** Il metodo di ricerca più elementare, in cui si definisce manualmente una griglia di valori da controllare per ogni iperparametro. Ad esempio, controllando `velocità di apprendimento = [0.01, 0.1, 1]` e `dimensioni batch = [32, 64, 128]`. Il vantaggio principale è la semplicità, ma può portare a un'esplosione esponenziale nello spazio di ricerca, rendendolo dispendioso in termini di tempo. È più adatto per l'ottimizzazione di un piccolo numero di parametri.

* **Random Search:** Invece di definire una griglia, si selezionano casualmente valori per ogni iperparametro da un intervallo o set predefinito. Questo metodo è più efficiente nell'esplorazione di un vasto spazio di iperparametri perché non richiede una ricerca esaustiva. Tuttavia, potrebbe comunque non trovare parametri ottimali poiché non esplora sistematicamente tutte le possibili combinazioni.

* **Bayesian Optimization:** Questo è un approccio probabilistico avanzato per l'esplorazione adattiva basato su una funzione surrogata per modellare le prestazioni su iterazioni. È semplice ed efficiente: trova iperparametri altamente ottimizzati in meno passaggi di valutazione. Tuttavia, richiede un maggiore investimento nella configurazione [@jasper2012practical].

* **Evolutionary Algorithms:** Questi algoritmi imitano i principi della selezione naturale. Generano popolazioni di combinazioni di iperparametri e le evolvono nel tempo in base alle prestazioni. Questi algoritmi offrono solide capacità di ricerca più adatte per superfici di risposta complesse. Tuttavia, sono necessarie molte iterazioni per una convergenza ragionevole.

* **Population Based Training (PBT):** Un metodo che ottimizza gli iperparametri addestrando più modelli in parallelo, consentendo loro di condividere e adattare configurazioni di successo durante l'addestramento, combinando elementi di ricerca casuale e algoritmi evolutivi [@jaderberg2017population].

* **Neural Architecture Search:** Un approccio alla progettazione di architetture ad alte prestazioni per reti neurali. Tradizionalmente, gli approcci NAS utilizzano una qualche forma di apprendimento di rinforzo per proporre architetture di reti neurali, che vengono poi ripetutamente valutate [@zoph2023cybernetical].

### Implicazioni di Sistema

La messa a punto degli iperparametri può avere un impatto significativo sul tempo di convergenza durante l'addestramento del modello, influenzando direttamente il runtime complessivo. I valori corretti per gli iperparametri chiave di training sono cruciali per un'efficiente convergenza del modello. Ad esempio, la velocità di apprendimento dell'iperparametro controlla la dimensione del passo durante l'ottimizzazione della discesa del gradiente. Impostando correttamente uno scheduling della velocità di apprendimento assicura che l'algoritmo di ottimizzazione converga rapidamente verso un buon minimo. Una velocità di apprendimento troppo bassa porta a una convergenza dolorosamente lenta, mentre un valore troppo grande causa una fluttuazione selvaggia delle perdite. Una messa a punto corretta assicura un rapido movimento verso pesi e bias ottimali.

Analogamente, la dimensione del batch per la discesa del gradiente stocastica influisce sulla stabilità della convergenza. La giusta dimensione del batch attenua le fluttuazioni negli aggiornamenti dei parametri per avvicinarsi più rapidamente al minimo. Sono necessarie più dimensioni del batch per evitare una convergenza rumorosa, mentre le dimensioni maggiori del batch non riescono a generalizzare e rallentano la convergenza a causa di aggiornamenti dei parametri meno frequenti. La messa a punto degli iperparametri per una convergenza più rapida e una durata di addestramento ridotta ha implicazioni dirette sui costi e sui requisiti di risorse per il ridimensionamento dei sistemi di machine learning:

* **Costi computazionali inferiori:** Tempi di convergenza più brevi significano costi computazionali inferiori per i modelli di training. Il training ML sfrutta spesso grandi istanze di cloud computing come cluster GPU e TPU che comportano pesanti costi orari. Ridurre al minimo i tempi di training riduce direttamente questo costo di noleggio delle risorse, che tende a dominare i budget ML per le organizzazioni. Un'iterazione più rapida consente inoltre agli esperti di dati di sperimentare più liberamente all'interno dello stesso budget.

* **Tempo di training ridotto:** Un tempo di training ridotto sblocca opportunità per addestrare più modelli utilizzando lo stesso budget computazionale. Gli iperparametri ottimizzati estendono ulteriormente le risorse disponibili, consentendo alle aziende di sviluppare e sperimentare più modelli con vincoli di risorse per massimizzare le prestazioni.

* **Efficienza delle risorse:** Un training più rapido consente di allocare istanze di calcolo più piccole nel cloud poiché i modelli richiedono l'accesso alle risorse per una durata più breve. Ad esempio, un job di training di un'ora consente di utilizzare istanze GPU meno potenti rispetto a un training di più ore, che richiede un accesso di elaborazione sostenuto su intervalli più lunghi. Ciò consente di risparmiare sui costi, soprattutto per carichi di lavoro di grandi dimensioni.

Ci sono anche altri vantaggi. Ad esempio, una convergenza più rapida riduce la pressione sui team di ingegneria ML in merito al provisioning delle risorse di training. Le semplici routine di riaddestramento del modello possono utilizzare risorse meno potenti anziché richiedere l'accesso a code ad alta priorità per cluster GPU di livello di produzione vincolati, liberando risorse di distribuzione per altre applicazioni.

### Gli Auto Tuner

Data la sua importanza, esiste un'ampia gamma di offerte commerciali per aiutare con l'ottimizzazione degli iperparametri. Toccheremo brevemente due esempi: uno incentrato sull'ottimizzazione per ML su scala cloud e l'altro per modelli di apprendimento automatico mirati ai microcontrollori. @tbl-platform-comparison delinea le principali differenze:

+------------------------------------+-------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+
| Piattaforma                        | Caso d'Uso Target                         | Tecniche di ottimizzazione                                 | Vantaggi                                                                                                                                |
+:===================================+:==========================================+:===========================================================+:========================================================================================================================================+
| Vertex AI di Google                | Apprendimento automatico su scala cloud   | Ottimizzazione bayesiana, addestramento Population-Based   | Nasconde la complessità, consentendo modelli rapidi e pronti per l'implementazione con ottimizzazione iperparametrica all'avanguardia   |
+------------------------------------+-------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+
| EON Tuner di Edge Impulse          | Modelli di microcontrollori (TinyML)      | Ottimizzazione bayesiana                                   | Adatta i modelli per dispositivi con risorse limitate, semplifica l'ottimizzazione per l'implementazione embedded                       |
+------------------------------------+-------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+

: Confronto di piattaforme di ottimizzazione per diversi casi d'uso di machine learning. {#tbl-platform-comparison .striped .hover}

#### BigML

Sono disponibili diverse piattaforme commerciali di auto-tuning per risolvere questo problema. Una soluzione è Vertex AI Cloud di Google, che offre un ampio supporto integrato per tecniche di ottimizzazione all'avanguardia.

Una delle funzionalità più importanti della piattaforma di apprendimento automatico gestita da Vertex AI di Google è l'ottimizzazione efficiente e integrata degli iperparametri per lo sviluppo del modello. Per addestrare con successo modelli ML performanti è necessario identificare configurazioni ottimali per un set di iperparametri esterni che determinano il comportamento del modello, ponendo un problema di ricerca ad alta dimensione impegnativo. Vertex AI semplifica questo processo tramite strumenti di Automated Machine Learning (AutoML).

In particolare, gli scienziati dei dati possono sfruttare i motori di ottimizzazione degli iperparametri di Vertex AI fornendo un set di dati etichettato e scegliendo un tipo di modello come un classificatore di reti neurali o Random Forest. Vertex avvia un job di "Hyperparameter Search" in modo trasparente sul backend, gestendo completamente il provisioning delle risorse, l'addestramento del modello, il monitoraggio delle metriche e l'analisi dei risultati automaticamente utilizzando algoritmi di ottimizzazione avanzati.

Internamente, Vertex AutoML impiega varie strategie di ricerca per esplorare in modo intelligente le configurazioni di iperparametri più promettenti in base ai risultati delle valutazioni precedenti.  Tra queste, l'ottimizzazione bayesiana è offerta in quanto fornisce un'efficienza di campionamento superiore, richiedendo meno iterazioni di training per ottenere una qualità del modello ottimizzata rispetto ai metodi standard di Grid Search o di Random Search. Per spazi di ricerca di architettura neurale più complessi, Vertex AutoML utilizza il Population-Based Training, che addestra simultaneamente più modelli e regola dinamicamente i loro iperparametri sfruttando le prestazioni di altri modelli nella popolazione, analogamente ai principi di selezione naturale.

Vertex AI democratizza le tecniche di ricerca di iperparametri all'avanguardia su scala cloud per tutti gli sviluppatori ML, astraendo la complessità di esecuzione e di orchestrazione sottostante. Gli utenti si concentrano esclusivamente sul loro set di dati, sui requisiti del modello e sugli obiettivi di accuratezza, mentre Vertex gestisce il ciclo di ottimizzazione, l'allocazione delle risorse, il training del modello, il monitoraggio dell'accuratezza e l'archiviazione degli artefatti internamente. Il risultato è ottenere modelli ML ottimizzati e pronti per la distribuzione più velocemente per il problema target.

#### TinyML

Edge Impulse's Efficient On-device Neural Network Tuner (EON Tuner) è uno strumento di ottimizzazione automatizzata degli iperparametri progettato per sviluppare modelli di apprendimento automatico per microcontrollori. Semplifica il processo di sviluppo del modello trovando automaticamente la migliore configurazione di rete neurale per un'implementazione efficiente e accurata su dispositivi con risorse limitate.

La funzionalità chiave di EON Tuner è la seguente. Innanzitutto, gli sviluppatori definiscono gli iperparametri del modello, come numero di layer, nodi per layer, funzioni di attivazione e pianificazione della velocità di "annealing" [https://it.wikipedia.org/wiki/Ricottura_simulata] dell'apprendimento. Questi parametri costituiscono lo spazio di ricerca che verrà ottimizzato. Successivamente, viene selezionata la piattaforma del microcontrollore target, fornendo vincoli hardware embedded. L'utente può anche specificare obiettivi di ottimizzazione, come la riduzione dell'ingombro di memoria, la riduzione della latenza, la riduzione del consumo energetico o la massimizzazione della precisione.

Con lo spazio di ricerca definito e gli obiettivi di ottimizzazione, EON Tuner sfrutta l'ottimizzazione degli iperparametri bayesiani per esplorare in modo intelligente possibili configurazioni. Ogni configurazione potenziale viene automaticamente implementata come specifica di modello completa, addestrata e valutata per metriche di qualità. Il processo continuo bilancia esplorazione e sfruttamento per arrivare a impostazioni ottimizzate su misura per l'architettura del chip scelta dallo sviluppatore e i requisiti di prestazioni.

EON Tuner libera gli esperti di machine learning dal processo iterativo esigente di messa a punto manuale dei modelli, regolando automaticamente i modelli per il deployment embedded. Lo strumento si integra perfettamente nel flusso di lavoro Edge Impulse, portando i modelli dal concetto a implementazioni ottimizzate in modo efficiente sui microcontrollori. L'esperienza racchiusa in EON Tuner per quanto riguarda l'ottimizzazione del modello ML per i microcontrollori garantisce che sia gli sviluppatori principianti che quelli esperti possano rapidamente iterare per ottenere modelli adatti alle esigenze del loro progetto.

:::{#exr-hpt .callout-caution collapse="true"}

### Ottimizzazione degli Iperparametri

Prepariamoci a scoprire i segreti della messa a punto degli iperparametri e portiamo i modelli PyTorch al livello successivo! Gli iperparametri sono come i quadranti e le manopole nascosti che controllano i superpoteri di apprendimento del modello. In questo notebook Colab, si collaborerà con Ray Tune per trovare le combinazioni perfette di iperparametri. Scopriamo come definire quali valori cercare, impostare il codice di training per l'ottimizzazione e lasciare che Ray Tune faccia il grosso del lavoro. Alla fine, si diventerà professionisti della messa a punto degli iperparametri!

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/30bcc2970bf630097b13789b5cdcea48/hyperparameter_tuning_tutorial.ipynb)

:::

@vid-hyperparameter spiega l'organizzazione sistematica del processo di ottimizzazione degli iperparametri.

:::{#vid-hyperparameter .callout-important}

# Iperparametro

{{< video https://www.youtube.com/watch?v=AXDByU3D1hA&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=24 >}}

:::

## Regolarizzazione

La regolarizzazione è una tecnica critica per migliorare le prestazioni e la generalizzabilità dei modelli di machine learning in impostazioni applicate. Si riferisce alla limitazione matematica o alla penalizzazione della complessità del modello per evitare il sovra-adattamento dei dati di training. Senza regolarizzazione, i modelli ML complessi sono inclini al sovra-adattamento del set di dati e alla memorizzazione di peculiarità e rumore nel set di training anziché all'apprendimento di pattern significativi. Possono raggiungere un'elevata accuratezza di training ma hanno prestazioni scadenti quando valutano nuovi input non ancora visti.

La regolarizzazione aiuta ad affrontare questo problema ponendo vincoli che favoriscono modelli più semplici e più generalizzabili che non si agganciano a errori di campionamento. Tecniche come la regolarizzazione L1/L2 penalizzano direttamente valori di parametri elevati durante il training, costringendo il modello a utilizzare i parametri più piccoli che possono spiegare adeguatamente il segnale. Le regole di arresto anticipato interrompono il training quando le prestazioni del set di validazione smettono di migliorare, prima che il modello inizi a sovra-adattarsi.

Una regolarizzazione appropriata è fondamentale quando si distribuiscono modelli a nuove popolazioni di utenti e ambienti in cui sono probabili cambiamenti di distribuzione. Ad esempio, un modello irregolare di rilevamento delle frodi addestrato presso una banca potrebbe funzionare inizialmente, ma accumulare debiti tecnici nel tempo man mano che emergono nuovi pattern di frode.

La regolarizzazione di reti neurali complesse offre anche vantaggi computazionali: modelli più piccoli richiedono meno "data augmentation", potenza di calcolo e archiviazione dei dati. La regolarizzazione consente anche sistemi di intelligenza artificiale più efficienti, in cui accuratezza, robustezza e gestione delle risorse sono attentamente bilanciate rispetto alle limitazioni del set di addestramento.

Diverse potenti tecniche di regolarizzazione sono comunemente utilizzate per migliorare la generalizzazione del modello. L'architettura della strategia ottimale richiede la comprensione di come ogni metodo influisce sull'apprendimento e sulla complessità del modello.

### L1 e L2

Due delle forme di regolarizzazione più ampiamente utilizzate sono la regolarizzazione L1 e la L2. Entrambe penalizzano la complessità del modello aggiungendo un termine extra alla funzione di costo ottimizzata durante l'addestramento. Questo termine cresce all'aumentare dei parametri del modello.

La regolarizzazione L2, nota anche come "ridge regression" [https://it.wikipedia.org/wiki/Regolarizzazione_di_Tichonov], aggiunge la somma delle grandezze al quadrato di tutti i parametri moltiplicata per un coefficiente α. Questa penalità quadratica riduce i valori dei parametri estremi in modo più aggressivo rispetto alle tecniche L1. L'implementazione richiede solo la modifica della funzione di costo e la messa a punto di α.

$$R_{L2}(\Theta) = \alpha \sum_{i=1}^{n}\theta_{i}^2$$

Dove:

* $R_{L2}(\Theta)$ - Il termine di regolarizzazione L2 che viene aggiunto alla funzione di costo
* $\alpha$ - L'iperparametro di regolarizzazione L2 che controlla la forza della regolarizzazione
* $\theta_{i}$ - L'i-esimo parametro del modello
* $n$ - Il numero di parametri nel modello
* $\theta_{i}^2$ - Il quadrato di ciascun parametro

E la funzione di costo regolarizzata L2 completa è:

$$J(\theta) = L(\theta) + R_{L2}(\Theta)$$

Dove:

* $L(\theta)$ - La funzione di costo non regolarizzata originale
* $J(\theta)$ - La nuova funzione di costo regolarizzata

Sia la regolarizzazione L1 che L2 penalizzano i pesi elevati nella rete neurale. Tuttavia, la differenza fondamentale tra la regolarizzazione L1 e L2 è che la regolarizzazione L2 penalizza i quadrati dei parametri anziché i valori assoluti. Questa differenza fondamentale ha un impatto considerevole sui pesi regolarizzati risultanti. La regolarizzazione L1, o regressione LASSO [https://it.wikipedia.org/wiki/Regolarizzazione_(matematica)], utilizza la somma assoluta delle grandezze anziché il quadrato moltiplicato per α. La penalizzazione del valore assoluto dei pesi induce scarsità poiché il gradiente degli errori estrapola linearmente mentre i termini dei pesi tendono a zero; questo è diverso dalla penalizzazione del valore al quadrato dei pesi, dove la penalità si riduce man mano che i pesi tendono a 0. Inducendo scarsità nel vettore dei parametri, la regolarizzazione L1 esegue automaticamente la selezione delle feature, impostando i pesi delle feature irrilevanti a zero. A differenza della regolarizzazione L2, la L1 porta alla scarsità poiché i pesi sono impostati su 0; nella regolarizzazione L2, i pesi sono impostati su un valore molto vicino a 0 ma generalmente non raggiungono mai esattamente 0. La regolarizzazione L1 incoraggia la scarsità ed è stata utilizzata in alcuni lavori per addestrare reti sparse che potrebbero essere più efficienti in termini di hardware [@torsten2021sparsity].

$$R_{L1}(\Theta) = \alpha \sum_{i=1}^{n}||\theta_{i}||$$

Dove:

* $R_{L1}(\Theta)$ - Il termine di regolarizzazione L1 che viene aggiunto alla funzione di costo
* $\alpha$ - L'iperparametro di regolarizzazione L1 che controlla la forza della regolarizzazione
* $\theta_{i}$ - L'i-esimo parametro del modello
* $n$ - Il numero di parametri nel modello
* $||\theta_{i}||$ - La norma L1, che assume il valore assoluto di ciascun parametro

E la funzione di costo regolarizzata L1 completa è:

$$J(\theta) = L(\theta) + R_{L1}(\Theta)$$

Dove:

* $L(\theta)$ - La funzione di costo non regolarizzata originale
* $J(\theta)$ - La nuova funzione di costo regolarizzata

La scelta tra L1 e L2 dipende dalla complessità del modello prevista e dalla necessità o meno di una selezione di feature intrinseche. Entrambi richiedono una messa a punto iterativa su un set di validazione per selezionare l'iperparametro α ottimale.

@vid-regularization e @vid-whyreg spiegano come funziona la regolarizzazione.

:::{#vid-regularization .callout-important}

# Regolarizzazione

{{< video https://www.youtube.com/watch?v=6g0t3Phly2M&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=4 >}}

:::

@vid-whyreg spiega come la regolarizzazione può aiutare a ridurre l'overfitting del modello per migliorare le prestazioni.

:::{#vid-whyreg .callout-important}

# Perché la Regolarizzazione Riduce l'Overfitting

{{< video https://www.youtube.com/watch?v=NyG-7nRpsW8&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=5 >}}

:::

### Dropout

Un altro metodo di regolarizzazione ampiamente adottato è "dropout" [@srivastava2014dropout]. Durante l'addestramento, dropout imposta casualmente una frazione $p$ di output del nodo o attivazioni nascoste a zero. Questo incoraggia una maggiore distribuzione delle informazioni su più nodi anziché affidarsi a un piccolo numero di nodi. Al momento della previsione, viene utilizzata l'intera rete neurale, con attivazioni intermedie scalate di $1 - p$ per mantenere le ampiezze di output. Le ottimizzazioni GPU semplificano l'implementazione efficiente di dropout tramite framework come PyTorch e TensorFlow.

Siamo più precisi. Durante l'addestramento con dropout, l'output di ogni nodo $a_i$ viene passato attraverso una maschera di dropout $r_i$ prima di essere utilizzato dal layer successivo:

$$ ã_i = r_i \odot a_i $$

Dove:

* $a_i$ - output del nodo $i$
* $ã_i$ - output del nodo $i$ dopo il dropout
* $r_i$ - variabile casuale di Bernoulli indipendente con probabilità $1 - p$ di essere 1
* $\odot$ - moltiplicazione elemento per elemento

Per capire come funziona il dropout, è importante sapere che la maschera di dropout $r_i$ è basata sulle variabili casuali di Bernoulli. Una variabile casuale di Bernoulli assume un valore di 1 con probabilità $1-p$ (mantenendo l'attivazione) e un valore di 0 con probabilità $p$ (dropping [perdendo] l'attivazione). Ciò significa che l'attivazione di ciascun nodo viene mantenuta o eliminata indipendentemente durante l'addestramento. Questa maschera di dropout $r_i$ imposta casualmente una frazione $p$ di attivazioni a 0 durante l'addestramento, costringendo la rete a creare rappresentazioni ridondanti.

Al momento del test, la maschera di dropout viene rimossa e le attivazioni vengono ridimensionate di $1 - p$ per mantenere le ampiezze di output previste:

$$ a_i^{test} = (1 - p)  a_i$$

Dove:

* $a_i^{test}$ - output del nodo al momento del test
* $p$ - la probabilità di effettuare il dropping [eliminare] di un nodo.

L'iperparametro chiave è $p$, la probabilità di eliminare ogni nodo, spesso impostata tra 0.2 e 0.5. Le reti più grandi tendono a trarre vantaggio da un dropout maggiore, mentre le reti più piccole rischiano di non adattarsi se vengono eliminati troppi nodi. Tentativi ed errori combinati con il monitoraggio delle prestazioni di validazione aiutano a regolare il livello di dropout.

@vid-dropout discute l'intuizione alla base della tecnica di regolarizzazione del dropout e il suo funzionamento.

:::{#vid-dropout .callout-important}

# Dropout

{{< video https://www.youtube.com/watch?v=ARq74QuavAo&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=7 >}}

:::

### Arresto Anticipato

L'intuizione alla base di "early stopping" [arresto anticipato] implica il monitoraggio delle prestazioni del modello su un set di validazione "held-out" [esterno] in epoche di addestramento. Inizialmente, gli aumenti nell'idoneità del set di addestramento accompagnano i guadagni nell'accuratezza della validazione man mano che il modello rileva pattern generalizzabili. Dopo un certo punto, tuttavia, il modello inizia a sovradimensionarsi, agganciandosi a peculiarità e rumore nei dati di addestramento che non si applicano più in generale. Le prestazioni di validazione raggiungono il picco e poi si degradano se l'addestramento continua. Le regole di "arresto anticipato" interrompono l'addestramento a questo picco per evitare il sovradimensionamento. Questa tecnica dimostra come le pipeline ML debbano monitorare il feedback del sistema, non solo massimizzare incondizionatamente le prestazioni su un set di addestramento statico. Lo stato del sistema evolve e gli endpoint ottimali cambiano.

Pertanto, i metodi formali di arresto anticipato richiedono il monitoraggio di una metrica come l'accuratezza o la perdita di validazione dopo ogni epoca. Le curve comuni mostrano rapidi guadagni iniziali che si riducono gradualmente, alla fine raggiungendo un plateau e diminuiscono leggermente man mano che si verifica il sovradimensionamento. Il punto di arresto ottimale è spesso compreso tra 5 e 15 epoche oltre il picco, a seconda dei "patient threshold" [limiti della pazienza!]. Il monitoraggio di più metriche può migliorare il segnale poiché esiste una varianza tra le misure.

Le semplici regole di arresto anticipato si interrompono immediatamente alla prima degradazione post-picco. Metodi più robusti introducono un parametro di "pazienza", ovvero il numero di epoche di degradazione consentite prima dell'arresto. Ciò evita di interrompere prematuramente l'addestramento a causa di fluttuazioni transitorie. Le finestre di "pazienza" tipiche vanno da 50 a 200 batch di validazione. Finestre più ampie comportano il rischio di overfitting. Le strategie di ottimizzazione formali possono determinare la "pazienza" ottimale.

:::{#exr-r .callout-caution collapse="true"}

### Regolarizzazione

Combattere l'Overfitting: Scoprire i Segreti della Regolarizzazione! L'overfitting è come se il modello memorizzasse le risposte a un test, per poi fallire l'esame reale. Le tecniche di regolarizzazione sono le guide di studio che aiutano il modello a generalizzare e ad affrontare nuovi problemi. In questo notebook Colab, impareremo come ottimizzare i parametri di regolarizzazione per risultati ottimali utilizzando la regolarizzazione L1 e L2, il dropout e l'arresto anticipato.

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/dphi-official/Deep_Learning_Bootcamp/blob/master/Optimization_Techniques/Regularization_and_Dropout.ipynb)

:::

@vid-otherregs tratta alcuni altri metodi di regolarizzazione che possono ridurre l'overfitting del modello.

:::{#vid-otherregs .callout-important}

# Altri Metodi di Regolarizzazione

{{< video https://www.youtube.com/watch?v=BOCLq2gpcGU&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=8 >}}

:::


## Funzioni di Attivazione

Le funzioni di attivazione svolgono un ruolo cruciale nelle reti neurali. Introducono comportamenti non lineari che consentono alle reti neurali di modellare pattern complessi. Le funzioni di attivazione elemento per elemento vengono applicate alle somme ponderate che arrivano a ciascun neurone nella rete. Senza funzioni di attivazione, le reti neurali sarebbero ridotte a modelli di regressione lineare.

Idealmente, le funzioni di attivazione possiedono alcune qualità desiderabili:

* **Non lineari:** Consentono di modellare relazioni complesse tramite trasformazioni non lineari della somma degli input.
* **Differenziabili:** Devono avere derivate prime ben definite per abilitare la retropropagazione e l'ottimizzazione basata sul gradiente durante l'addestramento.
* **Limitazione dell'Intervallo:** Limitano il segnale di output, impedendo un'esplosione. Ad esempio, la sigmoide schiaccia gli input a (0,1).

Inoltre, proprietà come efficienza computazionale, monotonicità e fluidità rendono alcune attivazioni più adatte di altre in base all'architettura di rete e alla complessità del problema.

Esamineremo brevemente alcune delle funzioni di attivazione più ampiamente adottate e i loro punti di forza e limiti. Forniremo anche linee guida per la selezione di funzioni appropriate abbinate ai vincoli del sistema ML e alle esigenze dei casi d'uso.

### Sigmoide

L'attivazione sigmoide applica una curva a forma di S schiacciante che lega strettamente l'output tra 0 e 1. Ha la forma matematica:

$$ sigmoid(x) = \frac{1}{1+e^{-x}} $$

La trasformazione esponenziale consente alla funzione di passare gradualmente da quasi 0 a quasi 1 quando l'input passa da molto negativo a molto positivo. L'aumento monotono copre l'intero intervallo (0,1).

La funzione sigmoide presenta diversi vantaggi. Fornisce sempre un gradiente uniforme per la retropropagazione e il suo output è limitato tra 0 e 1, il che aiuta a prevenire valori "esplosivi" durante l'addestramento. Inoltre, ha una semplice formula matematica che è facile da calcolare.

Tuttavia, la funzione sigmoide presenta anche alcuni svantaggi. Tende a saturarsi a valori di input estremi, il che può causare la "scomparsa" dei gradienti, rallentando o addirittura interrompendo il processo di apprendimento. Inoltre, la funzione non è centrata sullo zero, il che significa che i suoi output non sono distribuiti simmetricamente attorno allo zero, il che può portare ad aggiornamenti inefficienti durante l'addestramento.

### Tanh

Anche Tanh o "tangente iperbolica" assume una forma a S ma è centrata sullo zero, il che significa che il valore medio dell'output è 0.

$$ tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

La trasformazione numeratore/denominatore sposta l'intervallo da (0,1) in Sigmoide a (-1, 1) in tanh.

La maggior parte dei pro/contro sono condivisi con la Sigmoide, ma Tanh evita alcuni problemi di saturazione dell'output essendo centrata. Tuttavia, soffre ancora di gradienti che svaniscono con molti layer.

### ReLU

La Rectified Linear Unit (ReLU) introduce un semplice comportamento di soglia con la sua forma matematica:

$$ ReLU(x) = max(0, x) $$

Lascia tutti gli input positivi invariati mentre taglia tutti i valori negativi a 0. Questa attivazione sparsa e il calcolo economico rendono ReLU ampiamente favorito rispetto a sigmoide/tanh.

@fig-activation-functions dimostra le 3 funzioni di attivazione di cui abbiamo discusso sopra in confronto a una funzione lineare:

![Funzioni di Attivazione Comuni. Fonte: [AI Wiki.](https://machine-learning.paperspace.com/wiki/activation-function)](images/jpeg/activation-functions3.jpg){width=70% #fig-activation-functions}

### Softmax

La funzione di attivazione softmax è generalmente utilizzata come ultimo layer per le attività di classificazione per normalizzare il vettore del valore di attivazione in modo che i suoi elementi sommino a 1. Questo è utile per le attività di classificazione in cui vogliamo imparare a prevedere probabilità specifiche per classe di un input particolare, nel qual caso la probabilità cumulativa tra le classi è uguale a 1. La funzione di attivazione softmax è definita come

$$\sigma(z_i) = \frac{e^{z_{i}}}{\sum_{j=1}^K e^{z_{j}}} \ \ \ for\ i=1,2,\dots,K$$

### Pro e Contro

@tbl-af sono i pro e i contro riassuntivi di queste varie funzioni di attivazione standard:

+-------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------+
| Attivazione             | Pro                                                                                               | Contro                                               |
+:========================+:==================================================================================================+:=====================================================+
| Sigmoide                | - Gradiente uniforme per il backdrop [sfondo]                                                     | - La saturazione elimina i gradienti                 |
|                         | - Output limitato tra 0 e 1                                                                       | - Non centrato sullo zero                            |
+-------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------+
| Tanh                    | - Gradiente più uniforme della sigmoide                                                           | - Soffre ancora di problemi di gradiente evanescente |
|                         | - Output centrato sullo zero [-1, 1]                                                              |                                                      |
+-------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------+
| ReLU                    | - Efficiente dal punto di vista computazionale                                                    | - Unità "ReLU morenti"                               |
|                         | - Introduce la "sparsity" [scarsità]                                                              | - Non limitato                                       |
|                         | - Evita gradienti evanescenti                                                                     |                                                      |
+-------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------+
| Softmax                 | - Utilizzato per l'ultimo livello per normalizzare gli output in modo che siano una distribuzione |                                                      |
|                         | - In genere utilizzato per attività di classificazione                                            |                                                      |
+-------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------+

: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione. {#tbl-af .striped .hover}

:::{#exr-af .callout-caution collapse="true"}

### Funzioni di Attivazione

Sblocchiamo la potenza delle funzioni di attivazione! Questi piccoli "muletti" matematici sono ciò che rende le reti neurali così incredibilmente flessibili. In questo notebook Colab, ci si cimenterà con funzioni come Sigmoid, tanh e la superstar ReLU. Guardiamo come trasformano gli input e scopriamo quale funziona meglio in diverse situazioni. È la chiave per costruire reti neurali in grado di affrontare problemi complessi!

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/nn_activation_functions.ipynb)

:::

## Inizializzazione dei Pesi

La corretta inizializzazione dei pesi in una rete neurale prima dell'addestramento è un passaggio fondamentale che ha un impatto diretto sulle prestazioni del modello. L'inizializzazione casuale dei pesi a valori molto grandi o molto piccoli può portare a problemi come gradienti che svaniscono/esplodono, convergenza lenta dell'addestramento o intrappolati in minimi locali scadenti. La corretta inizializzazione del peso accelera la convergenza del modello durante l'addestramento e comporta implicazioni per le prestazioni del sistema al momento dell'inferenza negli ambienti di produzione. Alcuni aspetti chiave sono:

* **Tempo di Accuratezza più Rapido:** Un'inizializzazione attentamente calibrata porta a una convergenza più rapida, che si traduce nel raggiungimento da parte dei modelli di traguardi di accuratezza target in anticipo nel ciclo di training. Ad esempio, l'inizializzazione Xavier potrebbe ridurre il tempo di accuratezza del 20% rispetto a un'inizializzazione casuale errata. Poiché l'addestramento è in genere la fase più dispendiosa in termini di tempo e calcolo, ciò migliora direttamente la velocità e la produttività del sistema ML.

* **Efficienza del Ciclo di Iterazione del Modello:** Se i modelli vengono addestrati più rapidamente, il tempo di risposta complessivo per le iterazioni di sperimentazione, valutazione e progettazione del modello diminuisce in modo significativo. I sistemi hanno maggiore flessibilità per esplorare architetture, pipeline di dati, ecc., entro determinati intervalli di tempo.

* **Impatto sulle Epoche di Addestramento Necessarie:** Il processo di addestramento viene eseguito per più epoche, con ogni passaggio completo attraverso i dati che rappresenta un'epoca. Una buona inizializzazione può ridurre le epoche necessarie per far convergere le curve di perdita e accuratezza sul set di addestramento del 10-30%. Ciò significa risparmi tangibili sui costi di risorse e infrastruttura.

* **Effetto sugli Iperparametri di Addestramento:** I parametri di inizializzazione del peso interagiscono fortemente con determinati iperparametri di regolarizzazione che governano le dinamiche di addestramento, come i programmi di velocità di apprendimento e le probabilità di abbandono. Trovare la giusta combinazione di impostazioni non è banale. Un'inizializzazione appropriata semplifica questa ricerca.

L'inizializzazione dei pesi ha vantaggi a cascata per l'efficienza ingegneristica dell'apprendimento automatico e un overhead di risorse di sistema ridotto al minimo. È una tattica facilmente trascurata che ogni professionista dovrebbe padroneggiare. La scelta di quale tecnica di inizializzazione del peso utilizzare dipende da fattori come l'architettura del modello (numero di layer, pattern di connettività, ecc.), le funzioni di attivazione e il problema specifico da risolvere. Nel corso degli anni, i ricercatori hanno sviluppato e verificato empiricamente diverse strategie di inizializzazione mirate alle comuni architetture di reti neurali, di cui parleremo qui.

### Inizializzazione Uniforme e Normale

Quando si inizializzano pesi in modo casuale, vengono comunemente utilizzate due distribuzioni di probabilità standard: uniforme e Gaussiana (normale). La distribuzione uniforme imposta una probabilità uguale che i parametri di peso iniziali rientrino in qualsiasi punto entro i limiti minimi e massimi impostati. Ad esempio, i limiti potrebbero essere -1 e 1, portando a una distribuzione uniforme dei pesi tra questi limiti. La distribuzione gaussiana, d'altra parte, concentra la probabilità attorno a un valore medio, seguendo la forma di una curva a campana. La maggior parte dei valori di peso si raggrupperà nella regione della media specificata, con meno campioni verso le estremità. Il parametro di deviazione standard controlla la distribuzione attorno alla media.

La scelta tra inizializzazione uniforme o normale dipende dall'architettura di rete e dalle funzioni di attivazione. Per reti poco profonde, si consiglia una distribuzione normale con una deviazione standard relativamente piccola (ad esempio, 0.01). La curva a campana impedisce valori di peso elevati che potrebbero innescare l'instabilità di addestramento in reti piccole. Per reti più profonde, una distribuzione normale con deviazione standard più elevata (diciamo 0.5 o superiore) o una distribuzione uniforme può essere preferita per tenere conto dei problemi di gradiente evanescente su molti layer. La maggiore diffusione determina una maggiore differenziazione tra i comportamenti dei neuroni. La messa a punto dei parametri di distribuzione di inizializzazione è fondamentale per una convergenza stabile e rapida del modello. Il monitoraggio dei trend di "loss" [perdita] di addestramento può diagnosticare i problemi per modificare i parametri in modo iterativo.

### Inizializzazione Xavier

Proposta da @glorot2010understanding, questa tecnica di inizializzazione è progettata appositamente per le funzioni di attivazione sigmoide e tanh. Queste attivazioni saturate possono causare gradienti evanescenti o esplosivi durante la retro-propagazione su molti layer.

Il metodo Xavier imposta in modo intelligente la varianza della distribuzione dei pesi in base al numero di input e output per ciascun layer. L'intuizione è che questo bilancia il flusso di informazioni e gradienti in tutta la rete. Ad esempio, si consideri un layer con 300 unità di input e 100 unità di output. Inserendo questo nella formula varianza = 2/(#inputs + #outputs) si ottiene una varianza di 2/(300+100) = 0.01.

Il campionamento dei pesi iniziali da una distribuzione uniforme o normale centrata su 0 con questa varianza fornisce una convergenza di addestramento molto più fluida per reti sigmoide/tanh profonde. I gradienti sono ben condizionati, impedendo la scomparsa o la crescita esponenziale.

### Inizializzazione He

Come proposto da @kaiming2015delving, questa tecnica di inizializzazione è adattata alle funzioni di attivazione ReLU (Rectified Linear Unit). Le ReLU introducono il problema del neurone morente in cui le unità rimangono bloccate e producono solo 0 se inizialmente ricevono forti input negativi. Ciò rallenta e ostacola l'addestramento.

"He" supera questo problema campionando i pesi da una distribuzione con un set di varianza basato solo sul numero di input per layer, ignorando gli output. Ciò mantiene i segnali in arrivo sufficientemente piccoli da attivare le ReLU nel loro regime lineare dall'inizio, evitando unità morte. Per un layer con 1024 input, la formula varianza = 2/1024 = 0.002 mantiene la maggior parte dei pesi concentrati strettamente attorno a 0.

Questa inizializzazione specializzata consente alle reti ReLU di convergere in modo efficiente fin dall'inizio. La scelta tra Xavier e He deve corrispondere alla funzione di attivazione della rete prevista.

:::{#exr-wi .callout-caution collapse="true"}

### Inizializzazione dei Pesi

Facciamo partire la rete neurale col piede giusto con l'inizializzazione dei pesi! Il modo in cui si impostano quei pesi iniziali può fare la differenza nell'addestramento del modello. Si immagini di accordare gli strumenti di un'orchestra prima del concerto. In questo notebook Colab, si imparerà che la giusta strategia di inizializzazione può far risparmiare tempo, migliorare le prestazioni del modello e rendere il percorso di deep-learning molto più fluido.

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/csaybar/DLcoursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week5/Initialization/Initialization.ipynb)

:::

@vid-weightinit sottolinea l'importanza di selezionare deliberatamente i valori di peso iniziale rispetto a scelte casuali.

:::{#vid-weightinit .callout-important}

# Inizializzazione dei Pesi

{{< video https://www.youtube.com/watch?v=s2coXdufOzE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=11 >}}

:::

## "Colli di Bottiglia" del Sistema

Come introdotto in precedenza, le reti neurali comprendono operazioni lineari (moltiplicazioni di matrici) intervallate da funzioni di attivazione non lineari elemento per elemento. La parte computazionalmente più costosa delle reti neurali sono le trasformazioni lineari, in particolare le moltiplicazioni di matrici tra ogni layer. Questi layer lineari mappano le attivazioni dal layer precedente a uno spazio dimensionale superiore che funge da input per la funzione di attivazione del layer successivo.

### Complessità a Runtime della Moltiplicazione di Matrici

#### Moltiplicazioni di Layer vs. Attivazioni

La maggior parte del calcolo nelle reti neurali deriva dalle moltiplicazioni di matrici tra layer. Si consideri un layer di rete neurale con una dimensione di input di $M$ = 500 e una dimensione di output di $N$ = 1000; la moltiplicazione di matrici richiede $O(N \cdot M) = O(1000 \cdot 500) = 500,000$ operazioni di moltiplicazione-accumulazione (MAC) tra quei layer.

Confronta questo col layer precedente, che aveva $M$ = 300 input, che richiedevano $O(500 \cdot 300) = 150,000$ operazioni. Man mano che le dimensioni dei layer aumentano, i requisiti computazionali aumentano quadraticamente con la dimensione del layer. I calcoli totali su $L$ layer possono essere espressi come $\sum_{l=1}^{L-1} O\big(N^{(l)} \cdot M^{(l-1)}\big)$, dove il calcolo richiesto per ogni layer dipende dal prodotto delle dimensioni di input e output delle matrici che vengono moltiplicate.

Ora, confrontando la moltiplicazione della matrice con la funzione di attivazione, che richiede solo $O(N) = 1000$ non linearità elemento per elemento per $N = 1000$ output, possiamo vedere le trasformazioni lineari che dominano le attivazioni computazionalmente.

Queste grandi moltiplicazioni di matrici influiscono sulle scelte hardware, sulla latenza dell'inferenza e sui vincoli di potenza per le applicazioni di reti neurali nel mondo reale. Ad esempio, un tipico layer DNN potrebbe richiedere 500,000 moltiplicazioni-accumulazioni rispetto a solo 1000 attivazioni non lineari, dimostrando un aumento di 500x nelle operazioni matematiche.

Quando si addestrano reti neurali, in genere utilizziamo la discesa del gradiente in mini-batch, operando su piccoli batch di dati contemporaneamente. Considerando una dimensione batch di $B$ esempi di addestramento, l'input per la moltiplicazione di matrice diventa una matrice $M \times B$, mentre l'output è una matrice $N \times B$.

#### Mini-batch

Nell'addestramento delle reti neurali, dobbiamo stimare ripetutamente il gradiente della funzione di perdita rispetto ai parametri di rete (ad esempio, pesi e bias). Questo gradiente indica in quale direzione i parametri devono essere aggiornati per ridurre al minimo la perdita. Come introdotto in precedenza, eseguiamo aggiornamenti su un batch di dati a ogni aggiornamento, noto anche come discesa del gradiente stocastico o "discesa del gradiente mini-batch".

L'approccio più semplice è stimare il gradiente in base a un singolo esempio di addestramento, calcolare l'aggiornamento dei parametri, riassettare tutto e ripetere per l'esempio successivo. Tuttavia, ciò comporta aggiornamenti dei parametri molto piccoli e frequenti che possono essere computazionalmente inefficienti e potrebbero dover essere più accurati in termini di convergenza a causa della stocasticità dell'utilizzo di un solo dato per un aggiornamento del modello.

Invece, la discesa del gradiente in mini-batch bilancia la stabilità della convergenza e l'efficienza computazionale. Invece di calcolare il gradiente su singoli esempi, stimiamo il gradiente in base a piccoli "mini-batch" di dati, solitamente tra 8 e 256 esempi in pratica.

Ciò fornisce una stima del gradiente rumorosa ma coerente che porta a una convergenza più stabile. Inoltre, l'aggiornamento dei parametri deve essere eseguito solo una volta per mini-batch anziché una volta per ogni esempio, riducendo il sovraccarico computazionale.

Regolando la dimensione del mini-batch, possiamo controllare il compromesso tra la fluidità della stima (i batch più grandi sono generalmente migliori) e la frequenza degli aggiornamenti (i batch più piccoli consentono aggiornamenti più frequenti). Le dimensioni del mini-batch sono solitamente potenze di 2, quindi possono sfruttare in modo efficiente il parallelismo tra i core GPU.

Quindi, il calcolo totale esegue una moltiplicazione di matrici $N \times M$ per $M \times B$, producendo $O(N \cdot M \cdot B)$ operazioni in virgola mobile. Come esempio numerico, $N=1000$ unità nascoste, $M=500$ unità di input e una dimensione del batch $B=64$ equivale a 1000 x 500 x 64 = 32 milioni di moltiplicazioni-accumulazioni per iterazione di training!

Al contrario, le funzioni di attivazione vengono applicate elemento per elemento alla matrice di output $N \times B$, richiedendo solo $O(N \cdot B)$ calcoli. Per $N=1000$ e $B=64$, si tratta di sole 64,000 non linearità, ovvero 500 volte meno lavoro della moltiplicazione di matrici.

Man mano che aumentiamo le dimensioni del batch per sfruttare appieno hardware parallelo come le GPU, la discrepanza tra la moltiplicazione di matrici e il costo della funzione di attivazione aumenta ulteriormente. Ciò rivela come l'ottimizzazione delle operazioni di algebra lineare offra enormi guadagni di efficienza.

Pertanto, la moltiplicazione di matrici è fondamentale nell'analisi di dove e come le reti neurali impiegano i calcoli. Ad esempio, le moltiplicazioni di matrici spesso rappresentano oltre il 90% della latenza di inferenza e del tempo di addestramento nelle comuni reti neurali convoluzionali e ricorrenti.

#### Ottimizzazione della Moltiplicazione di Matrici

Diverse tecniche migliorano l'efficienza delle operazioni generali matrice-matrice densa/sparsa e matrice-vettore per migliorare l'efficienza complessiva. Alcuni metodi chiave sono:

* Sfruttamento di librerie matematiche ottimizzate come [cuBLAS](https://developer.nvidia.com/cublas) per l'accelerazione GPU
* Abilitazione di formati di precisione inferiore come FP16 o INT8 dove l'accuratezza lo consente
* Utilizzo di [Tensor Processing Unit](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) con moltiplicazione di matrici in hardware
* Calcoli consapevoli della sparsità e formati di archiviazione dati per sfruttare i parametri zero
* Approssimazione delle moltiplicazioni di matrici con algoritmi come le Fast Fourier Transform
* Progettazione dell'architettura del modello per ridurre le larghezze e le attivazioni degli layer
* Quantizzazione, pruning [potatura], distillazione e altre tecniche di compressione
* Parallelizzazione del calcolo sull'hardware disponibile
* Risultati di caching/pre-calcolo ove possibile per ridurre le operazioni ridondanti

Le potenziali tecniche di ottimizzazione sono vaste, data la porzione sproporzionata di tempo che i modelli trascorrono nella matematica di matrici e vettori. Anche i miglioramenti incrementali velocizzano i tempi di esecuzione e riducono il consumo di energia. Trovare nuovi modi per migliorare queste primitive di algebra lineare rimane un'area di ricerca attiva allineata con le future esigenze di machine learning. Ne parleremo in dettaglio nei capitoli [Ottimizzazioni](../optimizations/optimizations.it.qmd) e [Accelerazione IA](../hw_acceleration/hw_acceleration.it.qmd).

### Calcolo vs. Collo di Bottiglia della Memoria

A questo punto, la moltiplicazione matrice-matrice è l'operazione matematica fondamentale alla base delle reti neurali. Sia l'addestramento che l'inferenza per le reti neurali utilizzano ampiamente queste operazioni di moltiplicazione di matrici. L'analisi mostra che oltre il 90% dei requisiti computazionali nelle reti neurali attuali derivano da moltiplicazioni di matrici. Di conseguenza, le prestazioni della moltiplicazione di matrici hanno un'enorme influenza sul tempo complessivo di addestramento o inferenza del modello.

#### Addestramento vs. Inferenza

Mentre l'addestramento e l'inferenza si basano ampiamente sulle prestazioni della moltiplicazione di matrici, i loro profili computazionali precisi differiscono. In particolare, l'inferenza della rete neurale tende a essere più legata al calcolo rispetto all'addestramento per una dimensione di batch equivalente. La differenza fondamentale risiede nel passaggio di backpropagation, che è richiesto solo durante l'addestramento. La backpropagation implica una sequenza di operazioni di moltiplicazione di matrici per calcolare i gradienti rispetto alle attivazioni su ogni layer della rete. Tuttavia, è fondamentale che qui non sia necessaria alcuna larghezza di banda di memoria aggiuntiva: gli input, gli output e i gradienti vengono letti/scritti dalla cache o dai registri.

Di conseguenza, l'addestramento mostra intensità aritmetiche inferiori, con calcoli del gradiente limitati dall'accesso alla memoria anziché dai **FLOP** (Floating Point Operations Per Second), una misura delle prestazioni computazionali che indica quanti calcoli in virgola mobile un sistema può eseguire al secondo. Al contrario, la propagazione in avanti domina l'inferenza della rete neurale, che corrisponde a una serie di moltiplicazioni matrice-matrice. Senza una retrospettiva del gradiente che richiede molta memoria, le dimensioni dei batch più grandi spingono facilmente l'inferenza a essere estremamente limitata dal calcolo. Le elevate intensità aritmetiche misurate mostrano questo. I tempi di risposta possono essere critici per alcune applicazioni di inferenza, costringendo il fornitore dell'applicazione a utilizzare una dimensione di batch inferiore per soddisfare questi requisiti di tempo di risposta, riducendo così l'efficienza dell'hardware; quindi, le inferenze potrebbero vedere un utilizzo inferiore dell'hardware.

Le implicazioni sono che il provisioning hardware e i compromessi tra larghezza di banda e FLOP differiscono a seconda che un sistema miri al training o all'inferenza. I server ad alta produttività e bassa latenza per l'inferenza dovrebbero enfatizzare la potenza di calcolo anziché la memoria, mentre i cluster di training richiedono un'architettura più bilanciata.

Tuttavia, la moltiplicazione di matrici mostra un'interessante tensione: la larghezza di banda della memoria dell'hardware sottostante o le capacità di throughput aritmetico possono vincolarla. La capacità del sistema di recuperare e fornire dati matriciali rispetto alla sua capacità di eseguire operazioni di calcolo determina questa direzione.

Questo fenomeno ha impatti profondi; l'hardware deve essere progettato giudiziosamente e devono essere prese in considerazione le ottimizzazioni del software. Ottimizzare e bilanciare il calcolo rispetto alla memoria per alleviare questo collo di bottiglia della moltiplicazione di matrici è fondamentale per un training un deployment efficienti del modello.

Infine, la dimensione del batch può avere un impatto sui tassi di convergenza durante l'addestramento della rete neurale, un'altra considerazione importante. Ad esempio, ci sono generalmente rendimenti decrescenti nei benefici della convergenza con dimensioni di batch estremamente grandi (ad esempio, > 16384). Al contrario, dimensioni di batch estremamente grandi possono essere sempre più vantaggiose da una prospettiva di intensità hardware/aritmetica; l'utilizzo di batch così grandi potrebbe non tradursi in una convergenza più rapida rispetto al tempo a causa dei loro benefici decrescenti per la convergenza. Questi compromessi fanno parte delle decisioni di progettazione fondamentali per i sistemi per il tipo di ricerca basata sull'apprendimento automatico.

#### Dimensione del Batch

La dimensione del batch utilizzata durante l'addestramento e l'inferenza della rete neurale ha un impatto significativo sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria. In concreto, la dimensione del batch si riferisce al numero di campioni propagati assieme attraverso la rete in un passaggio avanti/indietro. La moltiplicazione di matrici equivale a dimensioni di matrice maggiori.

In particolare, diamo un'occhiata all'intensità aritmetica della moltiplicazione di matrici durante l'addestramento della rete neurale. Questa misura il rapporto tra operazioni computazionali e trasferimenti di memoria. La moltiplicazione di due matrici di dimensione $N \times M$ e $M \times B$ richiede $N \times M \times B$ operazioni di moltiplicazione-accumulo, ma solo trasferimenti di $N \times M + M \times B$ elementi di matrice.

Man mano che aumentiamo la dimensione del batch $B$, il numero di operazioni aritmetiche cresce più velocemente dei trasferimenti di memoria. Ad esempio, con una dimensione del batch di 1, abbiamo bisogno di $N \times M$ operazioni e $N + M$ trasferimenti, dando un rapporto di intensità aritmetica di circa $\frac{N \times M}{N+M}$. Ma con una dimensione del batch di grandi dimensioni di 128, il rapporto di intensità diventa $\frac{128 \times N \times M}{N \times M + M \times 128} \approx 128$.

L'utilizzo di una dimensione del batch più grande sposta il calcolo complessivo da vincolato alla memoria a più vincolato al calcolo. L'addestramento IA utilizza grandi dimensioni del batch ed è generalmente limitato dalle massime prestazioni di calcolo aritmetiche, ovvero l'Applicazione 3 in @fig-roofline. Pertanto, la moltiplicazione di matrici in batch è molto più intensiva dal punto di vista computazionale rispetto al limite di accesso alla memoria. Ciò ha implicazioni per la progettazione hardware e le ottimizzazioni software, che tratteremo in seguito. L'intuizione chiave è che possiamo modificare in modo significativo il profilo computazionale e i colli di bottiglia posti dall'addestramento e dall'inferenza della rete neurale regolando la dimensione del batch.

![Modello a profilo a di tetto per il training di IA.](images/png/aitrainingroof.png){#fig-roofline}

#### Caratteristiche Hardware

L'hardware moderno come CPU e GPU è altamente ottimizzato per la produttività computazionale piuttosto che per la larghezza di banda della memoria. Ad esempio, le GPU H100 Tensor Core di fascia alta possono fornire oltre 60 TFLOPS di prestazioni a doppia precisione, ma forniscono solo fino a 3 TB/s di larghezza di banda della memoria. Ciò significa che c'è uno squilibrio di quasi 20 volte tra unità aritmetiche e accesso alla memoria; di conseguenza, per hardware come gli acceleratori GPU, i carichi di lavoro di addestramento della rete neurale devono essere resi il più intensivi possibile dal punto di vista computazionale per utilizzare appieno le risorse disponibili.

Ciò motiva ulteriormente la necessità di utilizzare batch di grandi dimensioni durante l'addestramento. Quando si utilizza un batch di piccole dimensioni, la moltiplicazione della matrice è limitata dalla larghezza di banda della memoria, sottoutilizzando le abbondanti risorse di elaborazione. Tuttavia, possiamo spostare il collo di bottiglia verso l'elaborazione e ottenere un'intensità aritmetica molto più elevata con batch sufficientemente grandi. Ad esempio, potrebbero essere necessari batch di 256 o 512 campioni per saturare una GPU di fascia alta. Lo svantaggio è che batch più grandi forniscono aggiornamenti dei parametri meno frequenti, il che può influire sulla convergenza. Tuttavia, il parametro funge da importante manopola di sintonizzazione per bilanciare le limitazioni di memoria e quelle di elaborazione.

Pertanto, date le architetture di elaborazione-memoria sbilanciate dell'hardware moderno, l'impiego di batch di grandi dimensioni è essenziale per alleviare i colli di bottiglia e massimizzare la produttività. Come accennato, anche il software e gli algoritmi successivi devono adattarsi a tali dimensioni di batch, poiché dimensioni di batch più grandi possono avere rendimenti decrescenti verso la convergenza della rete. L'utilizzo di dimensioni di batch molto piccole può portare a un utilizzo non ottimale dell'hardware, limitando in ultima analisi l'efficienza del training. L'aumento di dimensioni dei batch di grandi dimensioni è un argomento di ricerca esplorato in vari lavori che mirano a eseguire una training su larga scala [@yang2018imagenet].

#### Architetture dei Modelli

L'architettura della rete neurale influisce anche sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria maggiore durante l'esecuzione. I trasformatori e gli MLP sono molto più vincolati al calcolo rispetto alle reti neurali convoluzionali CNN. Ciò deriva dai tipi di operazioni di moltiplicazione di matrici coinvolte in ciascun modello. I trasformatori si basano sull'auto-attenzione, moltiplicando grandi matrici di attivazione per enormi matrici di parametri per correlare gli elementi. Gli MLP impilano layer completamente connessi, richiedendo anche grandi moltiplicazioni matriciali.

Al contrario, i layer convoluzionali nelle CNN hanno una finestra scorrevole che riutilizza attivazioni e parametri nell'input, il che significa che sono necessarie meno operazioni matriciali univoche. Tuttavia, le convoluzioni richiedono l'accesso ripetuto a piccole parti di input e lo spostamento di somme parziali per popolare ciascuna finestra. Sebbene le operazioni aritmetiche nelle convoluzioni siano intense, questo spostamento di dati e la manipolazione del buffer impongono enormi overhead di accesso alla memoria. Le CNN comprendono diverse fasi a strati, quindi gli output intermedi devono materializzarsi frequentemente nella memoria.

Di conseguenza, l'addestramento CNN tende a essere più vincolato alla larghezza di banda della memoria rispetto al limite aritmetico in confronto a Transformers e MLP. Pertanto, il profilo di moltiplicazione della matrice e, a sua volta, il collo di bottiglia posto, varia in modo significativo in base alla scelta del modello. Hardware e sistemi devono essere progettati con un appropriato equilibrio di larghezza di banda di elaborazione-memoria a seconda dell'implementazione del modello target. I modelli che si basano maggiormente sull'attenzione e sui layer MLP richiedono una maggiore produttività aritmetica rispetto alle CNN, il che richiede un'elevata larghezza di banda della memoria.

## Parallelizzazione del Training

L'addestramento delle reti neurali comporta richieste di calcolo e memoria intensive. L'algoritmo di backpropagation per il calcolo dei gradienti e l'aggiornamento dei pesi consiste in ripetute moltiplicazioni di matrici e operazioni aritmetiche sull'intero set di dati. Ad esempio, un passaggio di backpropagation scala in complessità temporale con $O(num\_parameters \times batch\_size \times sequence\_length)$.

I requisiti di calcolo aumentano rapidamente con l'aumento delle dimensioni del modello in parametri e layer. Inoltre, l'algoritmo richiede l'archiviazione di output di attivazione e parametri del modello per la fase di backward, che cresce con le dimensioni del modello.

I modelli più grandi non possono adattarsi e addestrarsi su un singolo dispositivo acceleratore come una GPU e l'ingombro di memoria diventa proibitivo. Pertanto, dobbiamo parallelizzare l'addestramento del modello su più dispositivi per fornire elaborazione e memoria sufficienti per addestrare reti neurali all'avanguardia.

Come mostrato in @fig-training-parallelism, i due approcci principali sono il parallelismo dei dati, che replica il modello su più dispositivi suddividendo i dati di input in batch, e il parallelismo del modello, che suddivide l'architettura del modello stesso su diversi dispositivi. Tramite il training in parallelo, possiamo sfruttare maggiori risorse aggregate di elaborazione e memoria per superare le limitazioni del sistema e accelerare i carichi di lavoro di deep learning.

![Parallelismo dei dati e parallelismo del modello.](images/png/aitrainingpara.png){#fig-training-parallelism}

### Parallelismo dei Dati

La parallelizzazione dei dati è un approccio comune per parallelizzare il training di apprendimento automatico su più unità di elaborazione, come GPU o risorse di elaborazione distribuite. Il set di dati di addestramento è suddiviso in batch nel parallelismo dei dati e un'unità di elaborazione separata elabora ogni batch. I parametri del modello vengono poi aggiornati in base ai gradienti calcolati dall'elaborazione di ogni batch. Ecco una descrizione dettagliata della parallelizzazione dei dati per il training ML:

1. **Divisione del Dataset:** Il set di dati di addestramento è suddiviso in batch più piccoli, ciascuno contenente un sottoinsieme degli esempi di training.

2. **Replica del Modello:** Il modello di rete neurale è replicato su tutte le unità di elaborazione e ogni unità di elaborazione ha la sua copia.

3. **Calcolo Parallelo:** Ogni unità di elaborazione prende un batch diverso e calcola in modo indipendente i passaggi in forward e backward. Durante il passaggio forward [in avanti], il modello fa delle previsioni sui dati di input. La funzione di loss [perdita] determina i gradienti per i parametri del modello durante il passaggio backward [all'indietro].

4. **Aggregazione dei Gradienti:** Dopo l'elaborazione dei rispettivi batch, i gradienti di ogni unità di elaborazione vengono aggregati. I metodi di aggregazione comuni includono la sommatoria o la media dei gradienti.

5. **Aggiornamento dei Parametri:** I gradienti aggregati aggiornano i parametri del modello. L'aggiornamento può essere eseguito utilizzando algoritmi di ottimizzazione come SGD o varianti come Adam.

6. **Sincronizzazione:** Dopo l'aggiornamento, tutte le unità di elaborazione sincronizzano i parametri del modello, assicurandosi che ciascuna ne abbia la versione più recente.

I passaggi precedenti vengono ripetuti per diverse iterazioni o fino alla convergenza.

Prendiamo un esempio specifico. Abbiamo 256 dimensioni di batch e 8 GPU; ogni GPU riceverà un micro-batch di 32 campioni. I loro passaggi forward e backward calcolano perdite e gradienti solo in base ai 32 campioni locali. I gradienti vengono aggregati tra i dispositivi con un server dei parametri o una libreria di comunicazioni collettiva per ottenere il gradiente effettivo per il batch globale. Gli aggiornamenti dei pesi avvengono indipendentemente su ogni GPU in base a questi gradienti. Dopo un numero configurato di iterazioni, i pesi aggiornati si sincronizzano e si equalizzano tra i dispositivi prima di passare alle iterazioni successive.

Il parallelismo dei dati è efficace quando il modello è grande e il set di dati è sostanziale, poiché consente l'elaborazione parallela di diverse parti dei dati. È ampiamente utilizzato in framework e librerie di deep learning che supportano il training distribuito, come TensorFlow e PyTorch. Tuttavia, per garantire una parallelizzazione efficiente, è necessario prestare attenzione a gestire problemi come l sovraccarico della comunicazione, bilanciamento del carico e sincronizzazione.

### Parallelismo del Modello

Il parallelismo del modello si riferisce alla distribuzione del modello di rete neurale su più dispositivi anziché alla replica del modello completo come il parallelismo dei dati. Ciò è particolarmente utile quando un modello è troppo grande per essere inserito nella memoria di una singola GPU o di un dispositivo acceleratore. Sebbene ciò potrebbe non essere specificamente applicabile per casi d'uso embedded o TinyML poiché la maggior parte dei modelli è relativamente piccola, è comunque utile saperlo.

Nell'addestramento parallelo del modello, diverse parti o layer del modello vengono assegnati a dispositivi separati. Le attivazioni di input e gli output intermedi vengono partizionati e passati tra questi dispositivi durante i passaggi forward e backward per coordinare i calcoli del gradiente tra le partizioni del modello.

Il "footprint " [impronta] di memoria e le operazioni di calcolo vengono distribuite suddividendo l'architettura del modello su più dispositivi anziché concentrarsi su uno. Ciò consente l'addestramento di modelli molto grandi con miliardi di parametri che altrimenti supererebbero la capacità di un singolo dispositivo. Esistono diversi modi in cui possiamo eseguire il partizionamento:

* **Parallelismo di Layer:** I layer consecutivi sono distribuiti su dispositivi diversi. Ad esempio, il dispositivo 1 contiene i layer 1-3; il dispositivo 2 contiene i layer 4-6. Le attivazioni di output dal layer 3 verrebbero trasferite al dispositivo 2 per avviare i layer successivi per i calcoli della fase di forward.

* **Parallelismo a Livello di Filtro:** Nei layer convoluzionali, i filtri di output possono essere suddivisi tra più dispositivi. Ogni dispositivo calcola gli output di attivazione per un sottoinsieme di filtri, che vengono concatenati prima di propagarsi ulteriormente.

* **Parallelismo Spaziale:** Le immagini di input vengono divise spazialmente, quindi ogni dispositivo elabora una determinata regione come il quarto in alto a sinistra delle immagini. Le regioni di output si combinano poi per formare l'output completo.

Inoltre, le combinazioni ibride possono suddividere il modello a livello di layer e i dati in batch. Il tipo appropriato di parallelismo del modello dipende dai vincoli specifici dell'architettura neurale e dalla configurazione hardware. Ottimizzare il partizionamento e la comunicazione per la topologia del modello è fondamentale per ridurre al minimo il sovraccarico.

Tuttavia, poiché le parti del modello vengono eseguite su dispositivi fisicamente separati, devono comunicare e sincronizzare i loro parametri durante ogni fase di addestramento. La fase di backward deve garantire che gli aggiornamenti del gradiente si propaghino accuratamente tra le partizioni del modello. Quindi, il coordinamento e l'interconnessione ad alta velocità tra i dispositivi sono fondamentali per ottimizzare le prestazioni dell'addestramento parallelo. Sono necessari dei buoni protocolli di partizionamento e comunicazione per ridurre al minimo il sovraccarico di trasferimento.

### Confronto

Riassumendo, @tbl-parallelism illustra alcune delle caratteristiche chiave per confrontare il parallelismo dei dati e quello dei modelli.

+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Caratteristica              | Parallelismo dei dati                                         | Parallelismo del modello                     |
+:============================+:==============================================================+:=============================================+
| Definizione                 | Distribuisce i dati tra i dispositivi con repliche            | Distribuisce il modello tra i dispositivi    |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Obiettivo                   | Accelera il training tramite il ridimensionamento del calcolo | Abilita un training del modello più ampio    |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Metodo di Ridimensionamento | Dispositivi/workers in scala                                  | Dimensioni modello in scala                  |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Vincolo Principale          | Dimensione del modello per ogni dispositivo                   | Overhead di coordinamento dispositivo        |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Requisiti Hardware          | Più GPU/TPU                                                   | Spesso interconnessione specializzata        |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Difficoltà Principale       | Sincronizzazione dei parametri                                | Partizionamento e comunicazione complicati   |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Tipi                        | N/D                                                           | Per livello, per filtro, spaziale            |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Complessità del Codice      | Modifiche minime                                              | Intervento più significativa sul modello     |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+
| Librerie Popolari           | Horovod, PyTorch Distributed                                  | Mesh TensorFlow                              |
+-----------------------------+---------------------------------------------------------------+----------------------------------------------+

: Confronto tra parallelismo dei dati e parallelismo del modello. {#tbl-parallelism .striped .hover}

## Conclusione

In questo capitolo abbiamo trattato le basi fondamentali che consentono un training efficace dei modelli di intelligenza artificiale. Abbiamo esplorato concetti matematici come funzioni di perdita, backpropagation e discesa del gradiente che rendono possibile l'ottimizzazione delle reti neurali. Abbiamo anche discusso tecniche pratiche per sfruttare i dati di training, la regolarizzazione, la messa a punto degli iperparametri, l'inizializzazione dei pesi e strategie di parallelizzazione distribuita che migliorano convergenza, generalizzazione e scalabilità.

Queste metodologie costituiscono il fondamento attraverso cui è stato raggiunto il successo del deep learning nell'ultimo decennio. Padroneggiare questi fondamenti prepara i professionisti a progettare sistemi e perfezionare modelli su misura per il loro contesto. Tuttavia, man mano che modelli e set di dati crescono in modo esponenziale, i sistemi di training devono ottimizzare parametri come tempo, costo e "carbon footprint" [impatto ambientale]. Il ridimensionamento hardware tramite grosse warehouse consente un throughput computazionale enorme, ma le ottimizzazioni relative a efficienza e specializzazione saranno fondamentali. Tecniche software come compressione e sfruttamento delle matrici sparse possono aumentare i guadagni hardware. Ne discuteremo diverse nei prossimi capitoli.

Nel complesso, i fondamenti trattati in questo capitolo preparano i professionisti a costruire, perfezionare e distribuire modelli. Tuttavia, le competenze interdisciplinari che abbracciano teoria, sistemi e hardware differenzieranno gli esperti in grado di portare l'IA al livello successivo in modo sostenibile e responsabile, come richiesto dalla società. Comprendere l'efficienza insieme all'accuratezza costituisce l'approccio ingegneristico bilanciato necessario per addestrare sistemi intelligenti che si integrano senza problemi in molti contesti del mondo reale.

## Risorse {#sec-ai-training-resource}

Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.

:::{.callout-note collapse="false"}

#### Slide

Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.

* [Thinking About Loss.](https://docs.google.com/presentation/d/1X92JqVkUY7k6yJXQcT2u83dpdrx5UzGFAJkkDMDfKe0/edit#slide=id.g94db9f9f78_0_2)

* [Minimizing Loss.](https://docs.google.com/presentation/d/1x3xbZHo4VtaZgoXfueCbOGGXuWRYj0nOsKwAAoGsrD0/edit#slide=id.g94db9f9f78_0_2)

* [Training, Validation, and Test Data.](https://docs.google.com/presentation/d/1G56D0-qG9YWnzQQeje9LMpcLSotMgBCiMyfj53yz7lY/edit?usp=drive_link)

* Continuous Training:
   * [Retraining Trigger.](https://docs.google.com/presentation/d/1jtkcAnFot3VoY6dm8wARtIRPhM1Cfoe8S_8lMMox2To/edit?usp=drive_link)

   * [Data Processing Overview.](https://docs.google.com/presentation/d/1vW4jFv5mqpLo2_G2JXQrKLPMNoWoOvSXhFYotUbg3B0/edit?usp=drive_link)

   * [Data Ingestion.](https://docs.google.com/presentation/d/1e7_JGZH2X9Ha99-UsFy0bgpC4g-Msq1zXogrbQVBKfQ/edit?usp=drive_link)

   * [Data Validation.](https://docs.google.com/presentation/d/1PjilfceaDFp-spnZpTyqfcdvTbbfT0_95Hteqr-twk8/edit?usp=drive_link)

   * [Data Transformation.](https://docs.google.com/presentation/d/1cWMcFTl30Yl1XBYJZcND1USYKtS05TkfFkvwxfImOfY/edit?usp=drive_link)

   * [Training with AutoML.](https://docs.google.com/presentation/d/1SYjvCe_LZ0S3F5MdiDvAiGflpWmffmq7vAgruyXtaHk/edit?usp=drive_link&resourcekey=0-uu6gpFHmuCx56J89oguWMQ)

   * [Continuous Training with Transfer Learning.](https://docs.google.com/presentation/d/12Hhq1WGobzsLdVUzRRD-S1Mm2Z5dINGWtbB6RBmv87c/edit?usp=drive_link)

   * [Continuous Training Use Case Metrics.](https://docs.google.com/presentation/d/1ShpXTuUsf44TW0vXuv1Mk_REeRcAIpQRO2J2EFuWP0g/edit?usp=drive_link&resourcekey=0-6wnzPJ0mFlnJnpzTMGzN3w)

   * [Continuous Training Impact on MLOps.](https://docs.google.com/presentation/d/16kQd5BBCA41gvUauznQRd1ZdW5NI6OgiJVB9cuEmk14/edit#slide=id.g94db9f9f78_0_2)

:::

:::{.callout-important collapse="false"}

#### Video

* @vid-train-dev-test

* @vid-bias

* @vid-hyperparameter

* @vid-regularization

* @vid-whyreg

* @vid-dropout

* @vid-otherregs

* @vid-weightinit

:::

:::{.callout-caution collapse="false"}

#### Esercizi

Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.

* @exr-nn

* @exr-hpt

* @exr-r

* @exr-wi

* @exr-af

:::

:::{.callout-warning collapse="false"}

#### Laboratori

Oltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l'esperienza di apprendimento.

* _Prossimamente._
:::


