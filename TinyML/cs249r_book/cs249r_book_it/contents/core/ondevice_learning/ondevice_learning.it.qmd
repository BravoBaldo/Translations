---
bibliography: ondevice_learning.bib
---

# Apprendimento On-Device {#sec-ondevice_learning}

::: {.content-visible when-format="html"}
Risorse: [Slide](#sec-on-device-learning-resource), [Video](#sec-on-device-learning-resource), [Esercizi](#sec-on-device-learning-resource), [Laboratori](#sec-on-device-learning-resource)
:::

![_DALL·E 3 Prompt: Disegno di uno smartphone con i suoi componenti interni esposti, che mostra diversi ingegneri in miniatura di diversi sessi e tonalità di pelle che lavorano attivamente sul modello di ML. Gli ingegneri, tra cui uomini, donne e individui non binari, stanno regolando i parametri, riparando le connessioni e migliorando la rete al volo. I dati fluiscono nel modello ML, vengono elaborati in tempo reale e generano inferenze di output._](images/png/cover_ondevice_learning.png)

L'apprendimento "On-device" [sul dispositivo] rappresenta un'innovazione significativa per i dispositivi IoT embedded ed edge, consentendo ai modelli di addestrarsi e aggiornarsi direttamente su piccoli dispositivi locali. Ciò contrasta con i metodi tradizionali, in cui i modelli vengono addestrati su ampie risorse di cloud computing prima della distribuzione. Con l'apprendimento On-Device, dispositivi come smart speaker, dispositivi indossabili e sensori industriali possono perfezionare i modelli in tempo reale in base ai dati locali senza dover trasmettere dati esternamente. Ad esempio, uno smart speaker con comando vocale potrebbe apprendere e adattarsi ai pattern di linguaggio e al vocabolario del suo proprietario direttamente sul dispositivo. Tuttavia, non esiste un "pranzo gratis"; pertanto, in questo capitolo, discuteremo sia i vantaggi che i limiti dell'apprendimento sul dispositivo.

::: {.callout-tip}

## Obiettivi dell'Apprendimento

* Comprendere l'apprendimento sul dispositivo e in che modo differisce dal training basato su cloud

* Riconoscere i vantaggi e i limiti dell'apprendimento sul dispositivo

* Esaminare le strategie per adattare i modelli tramite riduzione della complessità, ottimizzazione e compressione dei dati

* Comprendere concetti correlati come apprendimento federato e apprendimento tramite trasferimento

* Analizzare le implicazioni della sicurezza dell'apprendimento sul dispositivo e delle strategie di mitigazione

:::

## Introduzione

L'apprendimento su dispositivo si riferisce all'addestramento di modelli ML direttamente sul dispositivo in cui vengono distribuiti, al contrario dei metodi tradizionali in cui i modelli vengono addestrati su server potenti e poi distribuiti sui dispositivi. Questo metodo è particolarmente rilevante per TinyML, in cui i sistemi ML sono integrati in dispositivi minuscoli e con risorse limitate.

Un esempio di apprendimento su dispositivo può essere visto in un termostato intelligente che si adatta al comportamento dell'utente nel tempo. Inizialmente, il termostato può avere un modello generico che comprende pattern di utilizzo di base. Tuttavia, poiché è esposto a più dati, come gli orari in cui l'utente è a casa o fuori, le temperature preferite e le condizioni meteorologiche esterne, il termostato può perfezionare il suo modello direttamente sul dispositivo per fornire un'esperienza personalizzata. Tutto ciò avviene senza inviare dati a un server centrale per l'elaborazione.

Un altro esempio è nel testo predittivo sugli smartphone. Mentre gli utenti digitano, il telefono impara dai pattern linguistici dell'utente e suggerisce parole o frasi che probabilmente verranno utilizzate in seguito. Questo apprendimento avviene direttamente sul dispositivo e il modello si aggiorna in tempo reale man mano che vengono raccolti più dati. Un esempio pratico di apprendimento su dispositivo ampiamente utilizzato è [Gboard](https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin). Su un telefono Android, Gboard [impara da modelli di digitazione e dettatura](https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/) per migliorare l'esperienza per tutti gli utenti. L'apprendimento "On-device" è anche chiamato "apprendimento federato".

@fig-federated-cycle mostra il ciclo di apprendimento federato sui dispositivi mobili: *A.* il dispositivo impara dai pattern utente; *B.* gli aggiornamenti del modello locale vengono comunicati al cloud; *C.* il server cloud aggiorna il modello globale e invia il nuovo modello a tutti i dispositivi.

![Ciclo di apprendimento federato. Fonte: [Google Research.](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)](images/png/ondevice_intro.png){#fig-federated-cycle}

## Vantaggi e Limiti

L'apprendimento su dispositivo offre diversi vantaggi rispetto al tradizionale ML basato su cloud. Mantenendo dati e modelli sul dispositivo, elimina la necessità di costose trasmissioni di dati e risolve i problemi di privacy. Ciò consente esperienze più personalizzate e reattive, poiché il modello può adattarsi in tempo reale al comportamento dell'utente.

Tuttavia, l'apprendimento su dispositivo presenta anche dei compromessi. Le limitate risorse di elaborazione sui dispositivi dei consumatori possono rendere difficile l'esecuzione di modelli complessi in locale. Anche i set di dati sono più limitati poiché sono costituiti solo da dati generati dall'utente da un singolo dispositivo. Inoltre, l'aggiornamento dei modelli richiede l'invio di nuove versioni anziché aggiornamenti cloud senza interruzioni.

L'apprendimento su dispositivo apre nuove possibilità abilitando l'intelligenza artificiale offline mantenendo al contempo la privacy dell'utente. Tuttavia, richiede una gestione attenta della complessità dei modelli e dei dati entro i limiti dei dispositivi dei consumatori. Trovare il giusto equilibrio tra localizzazione e offload dal cloud è fondamentale per ottimizzare le esperienze su dispositivo.

### Vantaggi

#### Privacy e Sicurezza dei Dati

Uno dei vantaggi significativi dell'apprendimento sul dispositivo è la maggiore privacy e sicurezza dei dati degli utenti. Ad esempio, si consideri uno smartwatch che monitora parametri sanitari sensibili come la frequenza cardiaca e la pressione sanguigna. Elaborando i dati e adattando i modelli direttamente sul dispositivo, i dati biometrici rimangono localizzati, aggirando la necessità di trasmettere dati grezzi ai server cloud dove potrebbero essere soggetti a violazioni.

Le violazioni dei server sono tutt'altro che rare, con milioni di record compromessi ogni anno. Ad esempio, la violazione di Equifax del 2017 ha esposto i dati personali di 147 milioni di persone. Mantenendo i dati sul dispositivo, il rischio di tali esposizioni è drasticamente ridotto. L'apprendimento sul dispositivo elimina la dipendenza dall'archiviazione cloud centralizzata e protegge dall'accesso non autorizzato da varie minacce, tra cui attori malintenzionati, minacce interne ed esposizione accidentale.

Regolamenti come l'Health Insurance Portability and Accountability Act ([HIPAA](https://www.cdc.gov/phlp/publications/topic/hipaa.html)) e il General Data Protection Regulation ([GDPR](https://gdpr.eu/tag/gdpr/)) impongono rigorosi requisiti di riservatezza dei dati che l'apprendimento sul dispositivo affronta abilmente. Garantendo che i dati rimangano localizzati e non vengano trasferiti ad altri sistemi, l'apprendimento sul dispositivo facilita la [conformità a tali regolamenti](https://www.researchgate.net/publication/321515854_The_EU_General_Data_Protection_Regulation_GDPR_A_Practical_Guide).

L'apprendimento sul dispositivo non è solo vantaggioso per i singoli utenti; ha implicazioni significative per le organizzazioni e i settori che gestiscono dati altamente sensibili. Ad esempio, in ambito militare, l'apprendimento sul dispositivo consente ai sistemi di prima linea di adattare modelli e funzioni indipendentemente dalle connessioni ai server centrali che potrebbero essere potenzialmente compromessi. Le informazioni critiche e sensibili sono saldamente protette dalla localizzazione dell'elaborazione e dell'apprendimento dei dati. Tuttavia, ciò comporta il compromesso che i singoli dispositivi assumono più valore e possono incentivare furti o distruzioni poiché diventano gli unici vettori di modelli di intelligenza artificiale specializzati. È necessario prestare attenzione alla protezione dei dispositivi stessi durante la transizione all'apprendimento sul dispositivo.

È inoltre importante preservare la privacy, la sicurezza e la conformità normativa dei dati personali e sensibili. Invece che nel cloud, i modelli di training e operativi aumentano sostanzialmente le misure di privacy a livello locale, assicurando che i dati degli utenti siano protetti da potenziali minacce.

Tuttavia, questo è solo parzialmente intuitivo perché l'apprendimento sul dispositivo potrebbe invece esporre i sistemi a nuovi attacchi alla privacy.
Con preziosi riepiloghi dei dati e aggiornamenti dei modelli archiviati in modo permanente su singoli dispositivi, potrebbe essere molto più difficile proteggerli fisicamente e digitalmente rispetto a un grande cluster di elaborazione. Mentre l'apprendimento sul dispositivo riduce la quantità di dati compromessi in una qualsiasi violazione, potrebbe anche introdurre nuovi pericoli disperdendo informazioni sensibili su molti terminali decentralizzati. Le pratiche di sicurezza attente sono ancora essenziali per i sistemi "on-device".

#### Normativa di Conformità

L'apprendimento sul dispositivo aiuta ad affrontare le principali normative sulla privacy come [GDPR](https://gdpr.eu/tag/gdpr/))e [CCPA](https://oag.ca.gov/privacy/ccpa). Queste normative richiedono la localizzazione dei dati, limitando i trasferimenti di dati transfrontalieri a paesi approvati con controlli adeguati. Il GDPR impone inoltre requisiti di "privacy by design" e consenso per la raccolta dei dati. Mantenendo l'elaborazione dei dati e il training del modello localizzati sul dispositivo, i dati sensibili degli utenti non vengono trasferiti altrove. Ciò evita importanti grattacapi di conformità per le organizzazioni.

Ad esempio, un fornitore di servizi sanitari che monitora i parametri vitali dei pazienti con dispositivi indossabili deve garantire che i trasferimenti di dati transfrontalieri siano conformi a HIPAA e GDPR se utilizza il cloud. Determinare le leggi del paese applicabili e garantire le approvazioni per i flussi di dati internazionali introduce oneri legali e ingegneristici. Con l'apprendimento on-device, nessun dato lascia il dispositivo, semplificando la conformità. Il tempo e le risorse spesi per la conformità vengono ridotti in modo significativo.

Settori come sanità, finanza e governo, che hanno dati altamente regolamentati, possono trarre grandi vantaggi dal training sul dispositivo. Localizzando i dati e l'apprendimento, i requisiti normativi di privacy e sovranità dei dati vengono soddisfatti più facilmente. Le soluzioni su dispositivo forniscono un modo efficiente per creare applicazioni di IA conformi.

Le principali normative sulla privacy impongono restrizioni sullo spostamento transfrontaliero dei dati che l'apprendimento su dispositivo affronta intrinsecamente tramite elaborazione localizzata. Ciò riduce l'onere di conformità per le organizzazioni che lavorano con dati regolamentati.

#### Riduzione della Larghezza di Banda, dei Costi e Maggiore Efficienza

Uno dei principali vantaggi dell'apprendimento su dispositivo è la significativa riduzione dell'utilizzo della larghezza di banda e dei costi associati all'infrastruttura cloud. Mantenendo i dati localizzati per l'addestramento del modello anziché trasmettere dati grezzi al cloud, l'apprendimento su dispositivo può comportare notevoli risparmi di larghezza di banda. Ad esempio, una rete di telecamere che analizzano i filmati video può ottenere significative riduzioni nel trasferimento di dati addestrando i modelli sul dispositivo anziché trasmettere in streaming tutti i filmati video al cloud per l'elaborazione.

Questa riduzione nella trasmissione dei dati consente di risparmiare larghezza di banda e si traduce in costi inferiori per server, reti e archiviazione dei dati nel cloud. Le grandi organizzazioni, che potrebbero spendere milioni in infrastrutture cloud per addestrare modelli di dati sui dispositivi, possono sperimentare drastiche riduzioni dei costi tramite l'apprendimento on-device. Nell'era dell'intelligenza artificiale generativa, in cui [i costi sono aumentati in modo significativo](https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems), trovare modi per contenere le spese è diventato sempre più importante.

Inoltre, anche i costi energetici e ambientali della gestione di grandi server farm sono diminuiti. I data center consumano grandi quantità di energia, contribuendo alle emissioni di gas serra. Riducendo la necessità di un'ampia infrastruttura basata su cloud, l'apprendimento sui dispositivi contribuisce a mitigare l'impatto ambientale dell'elaborazione dei dati [@wu2022sustainable].

Specificamente per le applicazioni endpoint [finali], l'apprendimento sui dispositivi riduce al minimo il numero di chiamate API di rete necessarie per eseguire l'inferenza tramite un provider cloud. I costi cumulativi associati alla larghezza di banda e alle chiamate API possono aumentare rapidamente per le applicazioni con milioni di utenti. Al contrario, eseguire training e inferenze localmente è notevolmente più efficiente e conveniente. Con ottimizzazioni all'avanguardia, è stato dimostrato che l'apprendimento on-device riduce i requisiti di memoria del training, migliora drasticamente l'efficienza della memoria e riduce fino al 20% la latenza per iterazione [@dhar2021survey].

Un altro vantaggio fondamentale dell'apprendimento sul dispositivo è la possibilità per i dispositivi IoT di adattare continuamente il loro modello ML a nuovi dati per un apprendimento continuo e permanente. I modelli sul dispositivo possono rapidamente diventare obsoleti man mano che il comportamento dell'utente, i pattern di dati e le preferenze cambiano. L'apprendimento continuo consente al modello di adattarsi in modo efficiente a nuovi dati e miglioramenti e di mantenere elevate prestazioni del modello nel tempo.

### Limitazioni

Mentre i tradizionali sistemi ML basati su cloud hanno accesso a risorse di elaborazione pressoché infinite, l'apprendimento sul dispositivo è spesso limitato nella potenza di elaborazione e di archiviazione del dispositivo edge su cui viene addestrato il modello. Per definizione, un [dispositivo edge](http://arxiv.org/abs/1911.00623) è un dispositivo con risorse di elaborazione, memoria ed energia limitate che non possono essere facilmente aumentate o diminuite. Pertanto, la dipendenza dai dispositivi edge può limitare la complessità, l'efficienza e le dimensioni dei modelli ML sul dispositivo.

#### Risorse di elaborazione

I tradizionali sistemi ML basati su cloud utilizzano grandi server con più GPU o TPU di fascia alta, che forniscono una potenza di calcolo e una memoria pressoché infinite. Ad esempio, servizi come Amazon Web Services (AWS) [EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html) consentono di configurare cluster di istanze GPU per un training parallelo massiccio.

Al contrario, l'apprendimento sul dispositivo è limitato dall'hardware del dispositivo edge su cui viene eseguito. I dispositivi edge si riferiscono a endpoint come smartphone, elettronica embedded e dispositivi IoT. Per definizione, questi dispositivi hanno risorse di elaborazione, memoria ed energia molto limitate rispetto al cloud.

Ad esempio, uno smartphone tipico o Raspberry Pi può avere solo pochi core CPU, pochi GB di RAM e una piccola batteria. Ancora più limitati in termini di risorse sono i dispositivi microcontrollore TinyML come [Arduino Nano BLE Sense](https://store-usa.arduino.cc/products/arduino-nano-33-ble-sense). Le risorse sono fisse su questi dispositivi e non possono essere facilmente aumentate su richiesta, come il ridimensionamento dell'infrastruttura cloud. Questa dipendenza dai dispositivi edge limita direttamente la complessità, l'efficienza e le dimensioni dei modelli che possono essere distribuiti per l'addestramento sul dispositivo:

* **Complessità:** I limiti di memoria, elaborazione e potenza limitano la progettazione dell'architettura del modello, così come il numero di layer e dei parametri.
* **Efficienza:** I modelli devono essere fortemente ottimizzati tramite metodi come la quantizzazione e la potatura per essere eseguiti più velocemente e consumare meno energia.
* **Dimensioni:** I file del modello effettivo devono essere compressi il più possibile per rientrare nei limiti di archiviazione dei dispositivi edge.

Pertanto, mentre il cloud offre una scalabilità infinita, l'apprendimento sul dispositivo deve operare entro i rigidi vincoli di risorse dell'hardware. Ciò richiede un'attenta progettazione congiunta di modelli semplificati, metodi di addestramento e ottimizzazioni su misura specificamente per i dispositivi edge.

#### Dimensioni, Accuratezza e Generalizzazione del Dataset

Oltre alle risorse di elaborazione limitate, l'apprendimento sul dispositivo è anche limitato dal set di dati disponibile per i modelli di training.

Nel cloud, i modelli vengono addestrati su dataset enormi e diversi come ImageNet o Common Crawl. Ad esempio, ImageNet contiene oltre 14 milioni di immagini attentamente categorizzate in migliaia di classi.

L'apprendimento sul dispositivo si basa invece su "data silos" più piccoli e decentralizzati, unici per ogni dispositivo. Il rullino fotografico di uno smartphone potrebbe contenere solo migliaia di foto degli interessi e degli ambienti degli utenti.

Questi dati decentralizzati portano alla necessità di dati IID (indipendenti e distribuiti in modo identico). Ad esempio, due amici potrebbero scattare molte foto degli stessi luoghi e oggetti, il che significa che le loro distribuzioni di dati sono altamente correlate piuttosto che indipendenti.

Motivi per cui i dati potrebbero essere non IID nelle impostazioni sul dispositivo:

* **Eterogeneità degli utenti:** Utenti diversi hanno interessi e ambienti diversi.
* **Differenze tra dispositivi:** Sensori, regioni e dati demografici influenzano i dati.
* **Effetti temporali:** Ora del giorno, impatti stagionali sui dati.

L'efficacia del ML si basa in gran parte su dati di training ampi e diversificati. Con set di dati piccoli e localizzati, i modelli on-device potrebbero non riuscire a generalizzare tra diverse popolazioni di utenti e ambienti. Ad esempio, un modello di rilevamento delle malattie addestrato solo su immagini di un singolo ospedale non si generalizzerebbe bene ad altri dati demografici dei pazienti. Le prestazioni nel mondo reale non potranno che migliorare con progressi medici estesi e diversificati. Quindi, mentre l'apprendimento basato su cloud sfrutta enormi set di dati, l'apprendimento su dispositivo si basa su "silo di dati" decentralizzati molto più piccoli, unici per ogni utente.

I dati limitati e le ottimizzazioni richieste per l'apprendimento on-device possono avere un impatto negativo sulla precisione e sulla generalizzazione del modello:

* I piccoli dataset aumentano il rischio di overfitting. Ad esempio, un classificatore di frutta addestrato su 100 immagini rischia di overfitting rispetto a uno addestrato su 1 milione di immagini diverse.
* I dati rumorosi generati dall'utente riducono la qualità. Il rumore del sensore o l'etichettatura impropria dei dati da parte di non esperti possono degradare l'addestramento.
* Ottimizzazioni come la potatura e la quantizzazione compromettono la precisione per l'efficienza. Un modello quantizzato a 8 bit funziona più velocemente ma meno accuratamente di un modello a 32 bit.

Quindi, mentre i modelli cloud raggiungono un'elevata precisione con enormi set di dati e senza vincoli, i modelli su dispositivo possono avere difficoltà a generalizzare. Alcuni studi dimostrano che il training sul dispositivo corrisponde all'accuratezza del cloud su determinate attività. Tuttavia, le prestazioni sui carichi di lavoro reali richiedono ulteriori studi [@lin2022device].

Ad esempio, un modello cloud può rilevare con precisione la polmonite nelle radiografie del torace di migliaia di ospedali. Tuttavia, un modello sul dispositivo addestrato solo su una piccola popolazione locale di pazienti potrebbe non riuscire a generalizzare.

Un'accuratezza inaffidabile limita l'applicabilità nel mondo reale dell'apprendimento sul dispositivo per usi critici come la diagnosi di malattie o i veicoli a guida autonoma.

Il training sul dispositivo è anche più lento del cloud a causa delle risorse limitate. Anche se ogni iterazione è più veloce, il processo di training complessivo richiede più tempo.

Ad esempio, un'applicazione di robotica in tempo reale potrebbe richiedere aggiornamenti del modello entro millisecondi. L'On-device training su un piccolo hardware embedded potrebbe richiedere secondi o minuti per l'aggiornamento, troppo lento per l'uso in tempo reale.

Le sfide relative a precisione, generalizzazione e velocità pongono ostacoli all'adozione dell'apprendimento on-device per sistemi di produzione reali, soprattutto quando affidabilità e bassa latenza sono fondamentali.

## Adattamento On-device

In un'attività ML, il consumo di risorse proviene [principalmente](http://arxiv.org/abs/1911.00623) da tre fonti:

* Il modello ML stesso;
* Il processo di ottimizzazione durante l'apprendimento del modello
* Archiviazione ed elaborazione del dataset utilizzato per l'apprendimento.

Di conseguenza, ci sono tre approcci per adattare gli algoritmi ML esistenti su dispositivi con risorse limitate:

* Riduzione della complessità del modello ML
* Modifica delle ottimizzazioni per ridurre i requisiti delle risorse di training
* Creazione di nuove rappresentazioni dei dati più efficienti in termini di archiviazione

Nella sezione seguente, esamineremo questi metodi di adattamento dell'apprendimento on-device. Il capitolo [Ottimizzazioni dei Modelli](../optimizations/optimizations.it.qmd) fornisce maggiori dettagli sulle ottimizzazioni del modello.

### Riduzione della Complessità del Modello

In questa sezione, discuteremo brevemente i modi per ridurre la complessità del modello quando si adattano i modelli ML sul dispositivo. Per i dettagli sulla riduzione della complessità del modello, fare riferimento al capitolo Ottimizzazioni dei Modelli.

#### Algoritmi ML tradizionali

A causa delle limitazioni di elaborazione e memoria dei dispositivi edge, alcuni algoritmi ML tradizionali sono ottimi candidati per applicazioni di apprendimento on-device grazie alla loro natura leggera. Alcuni esempi di algoritmi con basso impatto sulle risorse includono Naive Bayes Classifiers, Support Vector Machines (SVM), Linear Regression, Logistic Regression e algoritmi Decision Tree selezionati.

Con alcuni perfezionamenti, questi algoritmi ML classici possono essere adattati a specifiche architetture hardware ed eseguire attività semplici. I loro bassi requisiti di prestazioni semplificano l'integrazione dell'apprendimento continuo anche su dispositivi edge.

#### Pruning

Il "pruning" [potatura] è una tecnica per ridurre le dimensioni e la complessità di un modello ML per migliorarne l'efficienza e le prestazioni di generalizzazione. Ciò è utile per l'addestramento di modelli su dispositivi edge, in cui vogliamo ridurre al minimo l'utilizzo delle risorse mantenendo un'accuratezza competitiva.

L'obiettivo principale della potatura è rimuovere parti del modello che non contribuiscono in modo significativo al suo potere predittivo, mantenendo al contempo gli aspetti più informativi. Nel contesto degli alberi decisionali, la potatura comporta la rimozione di alcuni rami (sottoalberi) dall'albero, portando a un albero più piccolo e semplice. Allo stesso modo, quando applicato alle Deep Neural Networks (DNN), il pruning riduce il numero di neuroni (unità) o connessioni nella rete. Questo processo è illustrato in @fig-ondevice-pruning, che dimostra come il pruning può semplificare la struttura di una rete neurale eliminando connessioni o unità meno importanti, con conseguente creazione di un modello più compatto ed efficiente.

![Potatura della rete.](images/jpg/pruning.jpeg){#fig-ondevice-pruning}

#### Riduzione della Complessità dei Modelli di Deep Learning

I framework DNN tradizionali basati su cloud hanno un sovraccarico di memoria troppo elevato per essere utilizzati sul dispositivo. [Ad esempio](http://arxiv.org/abs/2206.15472), i sistemi di deep learning come PyTorch e TensorFlow richiedono centinaia di megabyte di overhead di memoria durante l'addestramento di modelli come [MobilenetV2](https://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html) e l'overhead aumenta con l'aumentare del numero di parametri di addestramento.

La ricerca attuale per DNN leggeri esplora principalmente architetture CNN. Esistono anche diversi framework "bare-metal" [tutto in hardware] progettati per eseguire reti neurali su MCU mantenendo bassi l'overhead computazionale e l'ingombro di memoria. Alcuni esempi includono MNN, TVM e TensorFlow Lite. Tuttavia, possono eseguire l'inferenza solo durante i passaggi in avanti e non supportano la backpropagation. Sebbene questi modelli siano progettati per l'implementazione edge, la loro riduzione nei pesi del modello e nelle connessioni architettoniche ha portato a minori requisiti di risorse per l'apprendimento continuo.

Il compromesso tra prestazioni e supporto del modello è chiaro quando si adattano i sistemi DNN più diffusi. Come adattiamo i modelli DNN esistenti a impostazioni con risorse limitate mantenendo il supporto per la backpropagation e l'apprendimento continuo? Le ultime ricerche suggeriscono tecniche di progettazione congiunta di algoritmi e sistemi che aiutano a ridurre il consumo di risorse dell'addestramento ML sui dispositivi edge. Utilizzando tecniche come il "quantization-aware scaling " (QAS) [ridimensionamento consapevole della quantizzazione], aggiornamenti sparsi e altre tecniche all'avanguardia, l'apprendimento sul dispositivo è possibile su sistemi embedded con poche centinaia di kilobyte di RAM senza memoria aggiuntiva mantenendo [un'elevata precisione](http://arxiv.org/abs/2206.15472).

### Modifica dei Processi di Ottimizzazione

La scelta della giusta strategia di ottimizzazione è importante per l'addestramento DNN su un dispositivo, poiché consente di trovare un buon minimo locale. Poiché l'addestramento avviene su un dispositivo, questa strategia deve anche considerare la memoria e la potenza limitate.

#### Quantization-Aware Scaling

La quantizzazione è un metodo comune per ridurre l'impronta di memoria dell'addestramento DNN. Sebbene ciò possa introdurre nuovi errori, questi possono essere mitigati progettando un modello per caratterizzare questo errore statistico. Ad esempio, i modelli potrebbero utilizzare l'arrotondamento stocastico o introdurre l'errore di quantizzazione negli aggiornamenti del gradiente.

Una tecnica algoritmica specifica è Quantization-Aware Scaling (QAS), che migliora le prestazioni delle reti neurali su hardware a bassa precisione, come dispositivi edge, dispositivi mobili o sistemi TinyML, regolando i fattori di scala durante il processo di quantizzazione.

Come abbiamo discusso nel capitolo [Ottimizzazioni dei Modelli](../optimizations/optimizations.it.qmd), la quantizzazione è il processo di mappatura di un intervallo continuo di valori in un set discreto di valori. Nel contesto delle reti neurali, questo spesso comporta la riduzione della precisione di pesi e attivazioni da virgola mobile a 32 bit a formati di precisione inferiore come gli interi a 8 bit. Questa riduzione della precisione può ridurre significativamente il costo computazionale e l'ingombro di memoria del modello, rendendolo adatto per l'implementazione su hardware a bassa precisione. @fig-float-int-quantization illustra questo concetto, mostrando un esempio di quantizzazione da float a intero in cui i valori in virgola mobile ad alta precisione vengono mappati in una rappresentazione intera più compatta. Questa rappresentazione visiva aiuta a chiarire come la quantizzazione può mantenere la struttura essenziale dei dati riducendone al contempo la complessità e i requisiti di archiviazione.

![Quantizzazione float-to-integer. Fonte: [Nvidia.](https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/qat-training-precision.png)](images/png/ondevice_quantization_matrix.png){#fig-float-int-quantization}

Tuttavia, il processo di quantizzazione può anche introdurre errori di quantizzazione che possono degradare le prestazioni del modello. La scalatura basata sulla quantizzazione è una tecnica che riduce al minimo questi errori regolando i fattori di scala utilizzati nel processo di quantizzazione.

Il processo QAS prevede due fasi principali:

* **Addestramento basato sulla quantizzazione:** In questa fase, la rete neurale viene addestrata tenendo conto della quantizzazione, simulandola per imitarne gli effetti durante i passaggi  "forward" e "backward". Ciò consente al modello di imparare a compensare gli errori di quantizzazione e migliorarne le prestazioni su hardware a bassa precisione. Per i dettagli, fare riferimento alla sezione QAT in Ottimizzazioni del modello.

* **Quantizzazione e ridimensionamento:** Dopo l'addestramento, il modello viene quantizzato in un formato a bassa precisione e i fattori di scala vengono regolati per ridurre al minimo gli errori di quantizzazione. I fattori di scala vengono scelti in base alla distribuzione dei pesi e delle attivazioni nel modello e vengono regolati per garantire che i valori quantizzati siano compresi nell'intervallo del formato a bassa precisione.

QAS viene utilizzato per superare le difficoltà di ottimizzazione dei modelli su dispositivi minuscoli senza dover effettuare la messa a punto degli iperparametri; QAS ridimensiona automaticamente i gradienti tensoriali con varie precisioni di bit. Ciò stabilizza il processo di addestramento e corrisponde all'accuratezza della precisione in virgola mobile.

#### Aggiornamenti Sparsi

Sebbene QAS consenta l'ottimizzazione di un modello quantizzato, utilizza una grande quantità di memoria, il che non è realistico per l'addestramento sul dispositivo. Quindi, gli aggiornamenti "spare" vengono utilizzati per ridurre l'ingombro di memoria del calcolo "full backward". Invece di potare i pesi per l'inferenza, l'aggiornamento sparso pota il gradiente durante la "backward propagation " [propagazione all'indietro] per aggiornare il modello in modo sparso. In altre parole, l'aggiornamento sparso salta i gradienti del calcolo di layer e sottotensori meno importanti.

Tuttavia, determinare lo schema di un aggiornamento sparso ottimale dato un budget di memoria vincolante può essere difficile a causa dell'ampio spazio di ricerca. Ad esempio, il modello MCUNet ha 43 layer convoluzionali e uno spazio di ricerca di circa 1030. Una tecnica per affrontare questo problema è l'analisi del contributo. L'analisi del contributo misura il miglioramento dell'accuratezza dai bias (aggiornamento degli ultimi bias rispetto al solo aggiornamento del classificatore) e pesi (aggiornamento del peso di un layer extra rispetto al solo aggiornamento del bias). Cercando di massimizzare questi miglioramenti, l'analisi del contributo deriva automaticamente uno schema di aggiornamento sparso ottimale per abilitare l'addestramento sul dispositivo.

#### Training Layer-Wise

Altri metodi oltre alla quantizzazione possono aiutare a ottimizzare le routine. Uno di questi metodi è l'addestramento "layer-wise". Un consumatore significativo di memoria dell'addestramento DNN è la backpropagation end-to-end, che richiede che tutte le feature map intermedie siano archiviate in modo che il modello possa calcolare i gradienti. Un'alternativa a questo approccio che riduce l'impronta di memoria dell'addestramento DNN è l'addestramento sequenziale "layer-by-layer" [@chen2016training]. Invece dell'addestramento end-to-end, l'addestramento di un singolo layer alla volta aiuta a evitare di dover archiviare le feature map intermedie.

#### Trading Computation for Memory

La strategia "trading computation for memory" [scambio di elaborazione per memoria] comporta il rilascio di parte della memoria utilizzata per archiviare i risultati intermedi. Invece, questi risultati possono essere ricalcolati in base alle necessità. È stato dimostrato che la riduzione della memoria in cambio di più elaborazione riduce l'impronta di memoria dell'addestramento DNN per adattarsi a quasi tutti i budget, riducendo al minimo anche i costi di elaborazione [@gruslys2016memory].

### Sviluppo di Nuove Rappresentazioni dei Dati

La dimensionalità e il volume dei dati di training possono avere un impatto significativo sull'adattamento sul dispositivo. Quindi, un'altra tecnica per adattare i modelli su dispositivi con risorse limitate è quella di rappresentare i set di dati in modo più efficiente.

#### Compressione dei Dati

L'obiettivo della compressione dei dati è raggiungere elevate precisioni limitando al contempo la quantità di dati di training. Un metodo per raggiungere questo obiettivo è dare priorità alla complessità del campione: la quantità di dati di training necessari affinché l'algoritmo raggiunga una precisione target [@dhar2021survey].

Altri metodi più comuni di compressione dei dati si concentrano sulla riduzione della dimensionalità e del volume dei dati di training. Ad esempio, un approccio potrebbe sfruttare la sparsità della matrice per ridurre l'ingombro di memoria per l'archiviazione dei dati di training.
 I dati di training possono essere trasformati in un embedding a dimensione inferiore e fattorizzati in una matrice di dizionario moltiplicata per una matrice di coefficienti blocchi sparsi [@rouhani2017tinydl]. Un altro esempio potrebbe riguardare la rappresentazione di parole provenienti da un ampio set di dati di training linguistica in un formato vettoriale più compresso [@li2016lightrnn].

## Il Transfer Learning

Il transfer learning è una tecnica in cui un modello sviluppato per un'attività specifica viene riutilizzato come punto di partenza per un modello su una seconda attività. Il transfer learning ci consente di sfruttare modelli pre-addestrati che hanno già appreso rappresentazioni utili da grandi set di dati e di perfezionarli per attività specifiche utilizzando set di dati più piccoli direttamente sul dispositivo. Ciò può ridurre significativamente le risorse di calcolo e il tempo necessari per l'addestramento dei modelli da zero.

Può essere compreso attraverso esempi intuitivi del mondo reale, come illustrato in @fig-transfer-learning-apps. La figura mostra scenari in cui le competenze di un dominio possono essere applicate per accelerare l'apprendimento in un campo correlato. Un esempio lampante è la relazione tra andare in bicicletta e andare in moto. Se si sa andare in bicicletta, si ha già l'abilità di stare in equilibrio su un veicolo a due ruote. La conoscenza di base di questa abilità rende significativamente più facile per te imparare a guidare una moto rispetto a qualcuno senza alcuna esperienza ciclistica. La figura illustra questo e altri scenari simili, dimostrando come l'apprendimento per trasferimento sfrutti le conoscenze esistenti per accelerare l'acquisizione di nuove competenze correlate.

![Trasferimento di conoscenze tra attività. Fonte: @zhuang2021comprehensive.](images/png/ondevice_transfer_learning_apps.png){#fig-transfer-learning-apps}

Prendiamo l'esempio di un'applicazione di sensore intelligente che utilizza l'intelligenza artificiale on-device per riconoscere gli oggetti nelle immagini acquisite dal dispositivo. Tradizionalmente, ciò richiederebbe l'invio dei dati dell'immagine a un server, dove un ampio modello di rete neurale elabora i dati e invia i risultati. Con l'intelligenza artificiale on-device, il modello viene archiviato ed eseguito direttamente sul dispositivo, eliminando la necessità di inviare dati a un server.

Per personalizzare il modello per le caratteristiche on-device, addestrare un modello di rete neurale da zero sul dispositivo sarebbe poco pratico a causa delle risorse di calcolo limitate e della durata della batteria. È qui che entra in gioco il "transfer learning" [apprendimento tramite trasferimento]. Invece di addestrare un modello da zero, possiamo prendere un modello pre-addestrato, come una rete neurale convoluzionale (CNN) o una rete di trasformatori addestrata su un ampio set di dati di immagini, e perfezionarlo per la nostra specifica attività di riconoscimento degli oggetti. Questa messa a punto può essere eseguita direttamente sul dispositivo utilizzando un set di dati più piccolo di immagini pertinenti all'attività. Sfruttando il modello pre-addestrato, possiamo ridurre le risorse di calcolo e il tempo necessari per il training, ottenendo comunque un'elevata precisione per l'attività di riconoscimento degli oggetti. @fig-t-learning illustra ulteriormente i vantaggi dell'apprendimento tramite trasferimento rispetto al training da zero.

![Training da zero vs. apprendimento tramite trasferimento.](images/png/transfer_learning.png){#fig-t-learning}

Il transfer learning è importante per rendere praticabile l'intelligenza artificiale on-device, consentendoci di sfruttare modelli pre-addestrati e di perfezionarli per attività specifiche, riducendo così le risorse di calcolo e il tempo necessari per il training. La combinazione di intelligenza artificiale sul dispositivo e il "transfer learning" apre nuove possibilità per applicazioni di intelligenza artificiale più attente alla privacy e più reattive alle esigenze degli utenti.

Il transfer learning ha rivoluzionato il modo in cui i modelli vengono sviluppati e distribuiti, sia nel cloud che nell'edge. Il transfer learning viene utilizzato nel mondo reale. Un esempio del genere è l'uso del transfer learning per sviluppare modelli di intelligenza artificiale in grado di rilevare e diagnosticare malattie da immagini mediche, come raggi X, scansioni MRI [risonanza magnetica] e TAC. Ad esempio, i ricercatori della Stanford University hanno sviluppato un modello di apprendimento di trasferimento in grado di rilevare il cancro nelle immagini della pelle con una precisione del 97% [@esteva2017dermatologist]. Questo modello è stato pre-addestrato su 1.28 milioni di immagini per classificare un'ampia gamma di oggetti e poi specializzato per il rilevamento del cancro tramite l'addestramento su un set di dati di immagini della pelle curato da dermatologi.

L'implementazione negli scenari di produzione può essere ampiamente categorizzata in due fasi: pre-distribuzione e post-distribuzione.

### Specializzazione Pre-Distribuzione

Nella fase di pre-implementazione, il transfer learning funge da catalizzatore per accelerare il processo di sviluppo. Ecco come funziona tipicamente: Si immagini di creare un sistema per riconoscere diverse razze di cani. Invece di partire da zero, possiamo utilizzare un modello pre-addestrato che ha già padroneggiato il compito più ampio di riconoscere gli animali nelle immagini.

Questo modello pre-addestrato funge da solida base e contiene una vasta conoscenza acquisita da dati estesi. Quindi perfezioniamo questo modello utilizzando un set di dati specializzato contenente immagini di varie razze di cani. Questo processo di messa a punto adatta il modello alle nostre esigenze specifiche, ovvero identificare con precisione le razze di cani. Una volta perfezionato e convalidato per soddisfare i criteri di prestazione, questo modello specializzato è pronto per l'implementazione.

Ecco come funziona in pratica:

* **Inizia con un Modello Pre-Addestrato:** Si inizia selezionando un modello che è già stato addestrato su un set di dati completo, solitamente correlato a un'attività generale. Questo modello funge da base per l'attività in questione.
* **Finetuning:** Il modello pre-addestrato viene quindi perfezionato su un set di dati più piccolo e specifico per l'attività desiderata. Questo passaggio consente al modello di adattare e specializzare la sua conoscenza ai requisiti specifici dell'applicazione.
* **Validazione:** Dopo la messa a punto, il modello viene convalidato per garantire che soddisfi i criteri di prestazione per l'attività specializzata.
* **Deployment:** Una volta convalidato, il modello specializzato viene distribuito nell'ambiente di produzione.

Questo metodo riduce significativamente il tempo e le risorse di calcolo necessarie per addestrare un modello da zero [@pan2009survey]. Adottando l'apprendimento tramite trasferimento, i sistemi embedded possono raggiungere un'elevata precisione su attività specializzate senza la necessità di raccogliere dati estesi o di impiegare risorse di calcolo significative per l'addestramento da zero.

### Adattamento Post-Distribuzione

L'implementazione su un dispositivo non deve necessariamente segnare il culmine del percorso educativo di un modello ML. Con l'avvento dell'apprendimento per trasferimento, apriamo le porte all'implementazione di modelli ML adattivi in scenari del mondo reale, soddisfacendo le esigenze personalizzate degli utenti.

Consideriamo un'applicazione reale in cui un genitore desidera identificare il proprio figlio in una raccolta di immagini di un evento scolastico sul proprio smartphone. In questo scenario, il genitore si trova di fronte alla sfida di localizzare il proprio figlio in mezzo alle immagini di molti altri bambini. L'apprendimento per trasferimento può essere impiegato qui per perfezionare il modello di un sistema embedded per questo compito unico e specializzato. Inizialmente, il sistema potrebbe utilizzare un modello generico addestrato per riconoscere i volti nelle immagini. Tuttavia, con l'apprendimento per trasferimento, il sistema può adattare questo modello per riconoscere le caratteristiche specifiche del figlio dell'utente.

Ecco come funziona:

1. **Raccolta Dati:** Il sistema embedded raccoglie immagini che includono il bambino, idealmente con l'input del genitore per garantire accuratezza e pertinenza. Ciò può essere fatto direttamente sul dispositivo, mantenendo la privacy dei dati dell'utente.
2. **Fine Tuning del Modello:** Il modello di riconoscimento facciale preesistente, che è stato addestrato su un set di dati ampio e diversificato, viene quindi perfezionato utilizzando le immagini del bambino appena raccolte. Questo processo adatta il modello per riconoscere le caratteristiche facciali specifiche del bambino, distinguendolo dagli altri bambini nelle immagini.
3. **Validazione:** Il modello rifinito viene poi convalidato per garantire che riconosca accuratamente il bambino in varie immagini. Ciò può comportare che il genitore verifichi le prestazioni del modello e fornisca feedback per ulteriori miglioramenti.
4. **Deployment:** Una volta convalidato, il modello adattato viene distribuito sul dispositivo, consentendo al genitore di identificare facilmente il proprio figlio nelle immagini senza doverle esaminare manualmente.

Questa personalizzazione al volo migliora l'efficacia del modello per il singolo utente, assicurando che tragga vantaggio dalla personalizzazione ML. Questo è, in parte, il modo in cui iPhotos o Google Photos funzionano quando ci chiedono di riconoscere un volto e poi, in base a queste informazioni, indicizzano tutte le foto di quel volto. Poiché l'apprendimento e l'adattamento avvengono sul dispositivo stesso, non ci sono rischi per la privacy personale. Le immagini dei genitori non vengono caricate su un server cloud o condivise con terze parti, proteggendo la privacy della famiglia e continuando a raccogliere i benefici di un modello ML personalizzato. Questo approccio rappresenta un significativo passo avanti nella ricerca per fornire agli utenti soluzioni ML personalizzate che rispettino e sostengano la loro privacy.

### Vantaggi

Il transfer learning è diventato una tecnica importante in ML e intelligenza artificiale, ed è particolarmente prezioso per diversi motivi.

1. **Scarsità di Dati:** In molti scenari reali, acquisire un set di dati etichettato sufficientemente grande per addestrare un modello ML da zero è complicato. Il transfer learning mitiga questo problema consentendo l'uso di modelli pre-addestrati che hanno già appreso funzionalità preziose da un vasto set di dati.
2. **Spese Computazionali:** Addestrare un modello da zero richiede risorse computazionali e tempo significativi, specialmente per modelli complessi come reti neurali profonde. Utilizzando il transfer learning, possiamo sfruttare il calcolo che è già stato eseguito durante l'addestramento del modello sorgente, risparmiando così tempo e potenza computazionale.
3. **Dati Annotati Limitati:** Per alcune attività specifiche, potrebbero essere disponibili ampi dati grezzi, ma il processo di etichettatura di tali dati per l'apprendimento supervisionato può essere costoso e richiedere molto tempo. Il transfer learning ci consente di utilizzare modelli pre-addestrati su un'attività correlata con dati etichettati, quindi richiedendo meno dati annotati per la nuova attività.

Ci sono vantaggi nel riutilizzare le funzionalità:

1. **Hierarchical Feature Learning:** I modelli di deep learning, in particolare le reti neurali convoluzionali (CNN), possono apprendere funzionalità gerarchiche. I layer inferiori in genere apprendono funzionalità generiche come bordi e forme, mentre quelli superiori apprendono funzionalità più complesse e specifiche per l'attività. Il transfer learning ci consente di riutilizzare le funzionalità generiche apprese da un modello e di perfezionare i livelli superiori per la nostra attività specifica.
2. **Aumento delle Prestazioni:** È stato dimostrato che il transfer learning aumenta le prestazioni dei modelli su attività con dati limitati. La conoscenza acquisita dall'attività dall'attività sorgente può fornire un prezioso punto di partenza e portare a una convergenza più rapida e a una maggiore accuratezza nell'attività target.

:::{#exr-tlb .callout-caution collapse="true"}

### Il Transfer Learning

Si immagini di addestrare un'IA a riconoscere i fiori come un professionista, ma senza aver bisogno di un milione di immagini di fiori! Questo è il potere del transfer learning. In questo Colab, prenderemo un'IA che conosce già le immagini e le insegneremo a diventare un'esperta di fiori con meno sforzo. Prepararsi a rendere la propria IA più intelligente, non è più difficile!

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb?force_kitty_mode=1&force_corgi_mode=1)

:::

### Concetti Fondamentali

Comprendere i concetti fondamentali del transfer learning è essenziale per utilizzare efficacemente questo potente approccio in ML. Qui, analizzeremo alcuni dei principi e dei componenti principali che stanno alla base del processo di transfer learning.

#### Attività di Origine e di Destinazione

Nel transfer learning, sono coinvolte due attività principali: l'attività di origine e quella di destinazione. L'attività di origine è quella per la quale il modello è già stato addestrato e ha appreso informazioni preziose. L'attività di destinazione è la nuova attività che vogliamo che il modello esegua. L'obiettivo del transfer learning è sfruttare le conoscenze acquisite dall'attività di origine per migliorare le prestazioni nell'attività di destinazione.

Supponiamo di avere un modello addestrato per riconoscere vari frutti nelle immagini (attività di origine) e di voler creare un nuovo modello per riconoscere diverse verdure nelle immagini (attività di destinazione). In tal caso, possiamo utilizzare il transfer learning per sfruttare le conoscenze acquisite durante l'attività di riconoscimento della frutta per migliorare le prestazioni del modello di riconoscimento della verdura.

#### Trasferimento della Rappresentazione

Il trasferimento della rappresentazione riguarda le rappresentazioni apprese (caratteristiche) dall'attività di origine all'attività di destinazione. Esistono tre tipi principali di trasferimento della rappresentazione:

* **Trasferimento di Istanza:** Implica il riutilizzo delle istanze di dati dall'attività di origine nell'attività di destinazione.
* **Trasferimento della Rappresentazione delle Feature:** Implica il trasferimento delle rappresentazioni di Feature [funzionalità] apprese dall'attività di origine all'attività di destinazione.
* **Trasferimento di Parametri:** Implica il trasferimento dei parametri appresi del modello (pesi) dall'attività di origine all'attività di destinazione.

Nell'elaborazione del linguaggio naturale, un modello addestrato per comprendere la sintassi e la grammatica di una lingua (attività di origine) può trasferire le sue rappresentazioni apprese a un nuovo modello progettato per eseguire l'analisi del sentiment (attività di destinazione).

#### Finetuning

Il "finetuning" [messa a punto] è il processo di regolazione dei parametri di un modello pre-addestrato per adattarlo all'attività di destinazione. In genere, ciò comporta l'aggiornamento dei pesi dei layer del modello, in particolare degli ultimi layer, per rendere il modello più pertinente per la nuova attività. Nella classificazione delle immagini, un modello pre-addestrato su un set di dati generale come ImageNet (attività di origine) può essere messo a punto regolando i pesi dei suoi livelli per ottenere buone prestazioni in un'attività di classificazione specifica, come il riconoscimento di specie animali specifiche (attività di destinazione).

#### Estrazione delle Feature

L'estrazione delle "feature" [caratteristiche] comporta l'utilizzo di un modello pre-addestrato come estrattore di feature fisse, in cui l'output dei layer intermedi del modello viene utilizzato come feature per l'attività di destinazione. Questo approccio è particolarmente utile quando l'attività di destinazione ha un set di dati di piccole dimensioni, poiché le feature apprese dal modello pre-addestrato possono migliorare significativamente le prestazioni. Nell'analisi delle immagini mediche, un modello pre-addestrato su un ampio set di dati di immagini mediche generali (attività di origine) può essere utilizzato come estrattore di feature per fornire funzionalità preziose per un nuovo modello progettato per riconoscere specifici tipi di tumori nelle immagini radiografiche (attività di destinazione).

### Tipi di Apprendimento Tramite Trasferimento

L'apprendimento tramite trasferimento può essere classificato in tre tipi principali in base alla natura delle attività e dei dati di origine e di destinazione. Esploriamo ciascun tipo in dettaglio:

#### Apprendimento Tramite Trasferimento Induttivo

Nell'apprendimento tramite trasferimento induttivo, l'obiettivo è apprendere la funzione predittiva di destinazione con l'aiuto dei dati di origine. In genere comporta la messa a punto di un modello pre-addestrato sull'attività di destinazione con dati etichettati disponibili. Un esempio comune di apprendimento tramite trasferimento induttivo sono le attività di classificazione delle immagini. Ad esempio, un modello pre-addestrato sul set di dati ImageNet (attività di origine) può essere messo a punto per classificare tipi specifici di uccelli (attività di destinazione) utilizzando un set di dati etichettato più piccolo di immagini di uccelli.

#### Apprendimento Tramite Trasferimento Transduttivo

L'apprendimento tramite trasferimento transduttivo comporta l'utilizzo di dati di origine e destinazione, ma solo dell'attività di origine. L'obiettivo principale è trasferire la conoscenza dal dominio di origine al dominio di destinazione, anche se le attività rimangono le stesse. L'analisi del "sentiment" per diverse lingue può servire come esempio di apprendimento tramite trasferimento transduttivo. Un modello addestrato per eseguire l'analisi del sentiment in inglese (attività di origine) può essere adattato per eseguire l'analisi del sentiment in un'altra lingua, come il francese (attività di destinazione), sfruttando set di dati paralleli di frasi in inglese e francese con gli stessi sentimenti.

#### Apprendimento con Trasferimento Non Supervisionato

L'apprendimento con trasferimento non supervisionato viene utilizzato quando le attività di origine e di destinazione sono correlate, ma non sono disponibili dati etichettati per l'attività di destinazione. L'obiettivo è sfruttare la conoscenza acquisita dall'attività di origine per migliorare le prestazioni nell'attività di destinazione, anche senza dati etichettati. Un esempio di apprendimento di trasferimento non supervisionato è la modellazione degli argomenti nei dati di testo. Un modello addestrato per estrarre argomenti da articoli di notizie (attività di origine) può essere adattato per estrarre argomenti da post sui social media (attività di destinazione) senza aver bisogno di dati etichettati per i post sui social media.

#### Confronto e Compromessi

Sfruttando questi diversi tipi di apprendimento per trasferimento, i professionisti possono scegliere l'approccio che meglio si adatta alla natura dei loro compiti e ai dati disponibili, portando infine a modelli di ML più efficaci ed efficienti. Quindi, in sintesi:

* **Induttivo:** Diversi compiti di origine e destinazione, domini diversi
* **Trasduttivo:** diversi compiti di origine e destinazione, stesso dominio
* **Non supervisionato:** dati di origine non etichettati, trasferisce le rappresentazioni delle feature

@tbl-tltypes presenta una matrice che delinea in modo un po' più dettagliato le somiglianze e le differenze tra i tipi di apprendimento per trasferimento:

+-------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Aspetto                                         | Apprendimento Induttivo per Trasferimento                          | Apprendimento Trasduttivo per Trasferimento                    | Apprendimento Non Supervisionato                                                                      |
+:================================================+:===================================================================+:===============================================================+:======================================================================================================+
| Dati Etichettati per l'Attività di Destinazione | Obbligatorio                                                       | Non obbligatorio                                               | Non obbligatorio                                                                                      |
+-------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Attività di origine                             | Può essere diversa                                                 | Lo stesso                                                      | Lo stesso o diverso                                                                                   |
+-------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Attività di destinazione                        | Può essere diversa                                                 | Lo stesso                                                      | Può essere diverso                                                                                    |
+-------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Obiettivo                                       | Migliorare le prestazioni dell'attività target con i dati sorgente | Trasferisci la conoscenza dal dominio sorgente a quello target | Sfrutta l'attività sorgente per migliorare le prestazioni dell'attività target senza dati etichettati |
+-------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Esempio                                         | Da ImageNet alla classificazione degli uccelli                     | Analisi del sentiment in diverse lingue                        | Modellazione degli argomenti per diversi dati di testo                                                |
+-------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+

: Confronto dei tipi di apprendimento per trasferimento. {#tbl-tltypes .striped .hover}

### Vincoli e Considerazioni

Quando si intraprende un apprendimento per trasferimento, ci sono diversi fattori che devono essere considerati per garantire un trasferimento di conoscenze di successo e prestazioni del modello. Ecco una ripartizione di alcuni fattori chiave:

#### Somiglianza dei Domini

La similarità di dominio si riferisce a quanto strettamente correlati sono i domini di origine e di destinazione. Più simili sono i domini, più è probabile che l'apprendimento per trasferimento abbia successo. Trasferire la conoscenza da un modello addestrato su immagini di scene esterne (dominio di origine) a un nuovo compito che prevede il riconoscimento di oggetti in scene interne (dominio di destinazione) potrebbe avere più successo rispetto al trasferire la conoscenza da scene esterne a un compito che prevede l'analisi del testo, poiché i domini (immagini vs. testo) sono piuttosto diversi.

#### Similarità dell'Attività

La similarità dell'attività si riferisce a quanto strettamente correlati sono le attività di origine e di destinazione. È probabile che attività simili traggano maggiori benefici dall'apprendimento per trasferimento. Un modello addestrato per riconoscere diverse razze di cani (attività di origine) può essere adattato più facilmente per riconoscere diverse razze di gatti (attività di destinazione) rispetto a quanto possa essere adattato per eseguire un'attività completamente diversa come la traduzione di una lingua.

#### Qualità e Quantità dei Dati

La qualità e la quantità dei dati disponibili per il compito di destinazione possono avere un impatto significativo sul successo dell'apprendimento per trasferimento. Più dati di alta qualità possono comportare migliori prestazioni del modello. Supponiamo di avere un ampio set di dati con immagini chiare e ben etichettate per riconoscere specie di uccelli specifiche. In tal caso, il processo di apprendimento per trasferimento avrà probabilmente più successo rispetto a un set di dati piccolo e rumoroso.

#### Sovrapposizione dello Spazio delle Feature

La sovrapposizione dello spazio delle feature si riferisce a quanto bene le feature apprese dal modello sorgente si allineano con quelle necessarie per l'attività di destinazione. Una maggiore sovrapposizione può portare a un apprendimento per trasferimento più efficace. Un modello addestrato su immagini ad alta risoluzione (attività di origine) potrebbe non trasferirsi bene a un'attività di destinazione che coinvolge immagini a bassa risoluzione, poiché lo spazio delle feature (alta risoluzione rispetto a bassa risoluzione) è diverso.

#### Complessità del Modello

Anche la complessità del modello sorgente può influire sul successo dell'apprendimento per trasferimento. A volte, un modello più semplice potrebbe trasferirsi meglio di uno complesso, poiché è meno probabile che si adatti eccessivamente all'attività di origine. Ad esempio, un semplice modello di rete neurale convoluzionale (CNN) addestrato su dati di immagini (attività di origine) può essere trasferito con maggiore successo a una nuova attività di classificazione di immagini (attività di destinazione) rispetto a una CNN complessa con molti layer, poiché è meno probabile che il modello più semplice si adatti eccessivamente all'attività di origine.

Considerando questi fattori, i professionisti del ML possono prendere decisioni informate su quando e come utilizzare l'apprendimento per trasferimento, portando infine a prestazioni del modello più efficaci nell'attività di destinazione. Il successo dell'apprendimento per trasferimento dipende dal grado di similarità tra i domini di origine e di destinazione. L'overfitting [adattamento eccessivo] è rischioso, soprattutto quando la messa a punto avviene su un set di dati limitato. Sul fronte computazionale, alcuni modelli pre-addestrati, a causa delle loro dimensioni, potrebbero non adattarsi comodamente ai vincoli di memoria di alcuni dispositivi o potrebbero essere eseguiti in modo proibitivamente lento. Nel tempo, con l'evoluzione dei dati, c'è il potenziale per la "drift" [deriva] del modello, che indica la necessità di un riaddestramento periodico o di un adattamento continuo.

Scoprire di più sull'apprendimento per trasferimento in @vid-tl di seguito.

:::{#vid-tl .callout-important}

# Il Transfer Learning

{{< video https://www.youtube.com/watch?v=FQM13HkEfBk >}}

:::

## Apprendimento Automatico Federato {#sec-fl}

### Panoramica dell'Apprendimento Federato

L'Internet moderna è piena di grandi reti di dispositivi connessi. Che si tratti di telefoni cellulari, termostati, smart speaker o altri prodotti IoT, innumerevoli dispositivi edge sono una miniera d'oro per dati iperpersonalizzati e ricchi. Tuttavia, con quei dati ricchi arriva una serie di problemi con il trasferimento delle informazioni e la privacy. Costruire un set di dati di training nel cloud da questi dispositivi comporterebbe un'ampia larghezza di banda, costi per il trasferimento dati e violazione della privacy degli utenti.

L'apprendimento federato offre una soluzione a questi problemi: addestrare i modelli parzialmente sui dispositivi edge e comunicare solo gli aggiornamenti al cloud. Nel 2016, un team di Google ha progettato un'architettura per l'apprendimento federato che tenta di risolvere questi problemi. Nel loro articolo iniziale, @mcmahan2017communication delinea un algoritmo di apprendimento federato di principio chiamato FederatedAveraging, mostrato in @fig-federated-avg-algo. In particolare, FederatedAveraging esegue la "stochastic gradient descent (SGD) [discesa del gradiente stocastico] su diversi dispositivi edge. In questo processo, ogni dispositivo calcola un gradiente $g_k = \nabla F_k(w_t)$ che viene poi applicato per aggiornare i pesi lato server come (con $\eta$ come tasso di apprendimento su $k$ client):

$$
w_{t+1} \rightarrow w_t - \eta \sum_{k=1}^{K} \frac{n_k}{n}g_k
$$
Questo riassume l'algoritmo di base per l'apprendimento federato sulla destra. Per ogni round di addestramento, il server prende un set casuale di dispositivi client e chiama ogni client per addestrare sul suo batch locale usando i pesi lato server più recenti. Tali pesi vengono poi restituiti al server, dove vengono raccolti individualmente e calcolati per aggiornare i pesi del modello globale.

![Algoritmo FederatedAverage proposto da Google. Fonte: McMahan et al. ([2017](https://arxiv.org/abs/1602.05629)).](images/png/ondevice_fed_averaging.png){#fig-federated-avg-algo}

Con questa struttura proposta, ci sono alcuni vettori chiave per ottimizzare ulteriormente l'apprendimento federato. Descriveremo ciascuno di essi nelle seguenti sottosezioni.

@vid-fl fornisce una panoramica dell'apprendimento federato.

:::{#vid-fl .callout-important}

# Il Transfer Learning

{{< video https://www.youtube.com/watch?v=zqv1eELa7fs >}}

:::

@fig-federated-learning delinea l'impatto trasformativo dell'apprendimento federato sull'apprendimento on-device.

![L'apprendimento federato sta rivoluzionando l'apprendimento on-device.](images/png/federatedvsoil.png){#fig-federated-learning}

### Efficienza della Comunicazione

Uno dei principali colli di bottiglia nell'apprendimento federato è la comunicazione. Ogni volta che un client addestra il modello, deve comunicare i propri aggiornamenti al server. Analogamente, una volta che il server ha calcolato la media di tutti gli aggiornamenti, deve inviarli al client. Ciò comporta enormi costi di larghezza di banda e risorse su grandi reti di milioni di dispositivi. Con l'avanzare del campo dell'apprendimento federato, sono state sviluppate alcune ottimizzazioni per ridurre al minimo questa comunicazione. Per affrontare l'ingombro del modello, i ricercatori hanno sviluppato tecniche di compressione del modello. Nel protocollo client-server, l'apprendimento federato può anche ridurre al minimo la comunicazione tramite la condivisione selettiva degli aggiornamenti sui client. Infine, anche tecniche di aggregazione efficienti possono semplificare il processo di comunicazione.

### Compressione del Modello

Nell'apprendimento federato standard, il server comunica l'intero modello a ciascun client, quindi il client invia tutti i pesi aggiornati. Ciò significa che il modo più semplice per ridurre l'ingombro di memoria e comunicazione del client è ridurre al minimo le dimensioni del modello che deve essere comunicato. Possiamo impiegare tutte le strategie di ottimizzazione del modello discusse in precedenza per farlo.

Nel 2022, un altro team di Google ha proposto che ogni client comunichi tramite un formato compresso e decomprima il modello al volo per l'addestramento [@yang2023online], allocando e deallocando l'intera memoria per il modello solo per un breve periodo durante l'addestramento. Il modello viene compresso tramite una gamma di diverse strategie di quantizzazione elaborate nel loro documento. Nel frattempo, il server può aggiornare il modello non compresso decomprimendolo e applicando gli aggiornamenti man mano che arrivano.

### Condivisione Selettiva degli Aggiornamenti

Esistono molti metodi per condividere selettivamente gli aggiornamenti. Il principio generale è che la riduzione della porzione del modello che i client stanno addestrando lato edge riduce la memoria necessaria per l'addestramento e la dimensione della comunicazione con il server. Nell'apprendimento federato di base, il client addestra l'intero modello. Ciò significa che quando un client invia un aggiornamento al server, ha gradienti per ogni peso nella rete.

Tuttavia, non possiamo semplicemente ridurre la comunicazione inviando parti di quei gradienti da ogni client al server perché i gradienti fanno parte di un intero aggiornamento necessario per migliorare il modello. Invece, si deve progettare architettonicamente il modello in modo che ogni client addestri solo una piccola parte del modello più ampio, riducendo la comunicazione totale e ottenendo comunque il vantaggio dell'addestramento sui dati del client. @shi2022data applica questo concetto a una CNN suddividendo il modello globale in due parti: una superiore e una inferiore, come mostrato in @chen2023learning.

![Suddivisione dell'architettura del modello per la condivisione selettiva. Fonte: Shi et al., ([2022](https://doi.org/10.1145/3517207.3526980)).](images/png/ondevice_split_model.png){#fig-split-model}

La parte inferiore è progettata per concentrarsi sulle funzionalità generiche nel set di dati, mentre la parte superiore, addestrata su tali funzionalità generiche, è progettata per essere più sensibile alle mappe di attivazione. Ciò significa che la parte inferiore del modello viene addestrata tramite la media federata standard su tutti i client. Nel frattempo, la parte superiore del modello viene addestrata interamente sul lato server dalle mappe di attivazione generate dai client. Questo approccio riduce drasticamente la comunicazione per il modello, rendendo comunque la rete robusta a vari tipi di input trovati nei dati sui dispositivi client.

### Aggregazione Ottimizzata

Oltre a ridurre il sovraccarico di comunicazione, l'ottimizzazione della funzione di aggregazione può migliorare la velocità e l'accuratezza dell'addestramento del modello in determinati casi d'uso di apprendimento federato. Mentre lo standard per l'aggregazione è solo la media, vari altri approcci possono migliorare l'efficienza, l'accuratezza e la sicurezza del modello.

Un'alternativa è la "media ritagliata", che limita gli aggiornamenti del modello entro un intervallo specifico. Un'altra strategia per preservare la sicurezza è l'aggregazione media della privacy differenziale. Questo approccio integra la privacy differenziale nella fase di aggregazione per proteggere le identità dei client. Ogni client aggiunge uno strato di rumore casuale ai propri aggiornamenti prima di comunicare al server. Il server si aggiorna quindi con gli aggiornamenti rumorosi, il che significa che la quantità di rumore deve essere regolata attentamente per bilanciare privacy e precisione.

Oltre ai metodi di aggregazione che migliorano la sicurezza, ci sono diverse modifiche ai metodi di aggregazione che possono migliorare la velocità di training e le prestazioni aggiungendo metadati client insieme agli aggiornamenti del peso. Il "Momentum aggregation" è una tecnica che aiuta ad affrontare il problema della convergenza. Nell'apprendimento federato, i dati client possono essere estremamente eterogenei a seconda dei diversi ambienti in cui vengono utilizzati i dispositivi. Ciò significa che molti modelli con dati eterogenei potrebbero aver bisogno di aiuto per convergere. Ogni client memorizza localmente un termine di "momentum", che traccia il ritmo del cambiamento su diversi aggiornamenti. Con i client che comunicano questo "momentum", il server può tenere conto della velocità di cambiamento di ogni aggiornamento quando si modifica il modello globale per accelerare la convergenza. Allo stesso modo, l'aggregazione ponderata può tenere conto delle prestazioni del client o di altri parametri come il tipo di dispositivo o la potenza della connessione di rete per regolare il peso con cui il server dovrebbe incorporare gli aggiornamenti del modello. Ulteriori descrizioni di algoritmi di aggregazione specifici sono fornite da @moshawrab2023reviewing.

### Gestione dei Dati non-IID

Quando si utilizza l'apprendimento federato per addestrare un modello su molti dispositivi client, è conveniente considerare i dati come indipendenti e distribuiti in modo identico (IID) su tutti i client. Quando i dati sono IID, il modello convergerà più velocemente e funzionerà meglio perché ogni aggiornamento locale su un dato client è più rappresentativo del set di dati più ampio. Questo semplifica l'aggregazione, poiché è possibile calcolare direttamente la media di tutti i client. Tuttavia, differisce dal modo in cui i dati spesso appaiono nel mondo reale. Si considerino alcuni dei seguenti modi in cui i dati possono essere non IID:

* Imparando su un set di dispositivi di monitoraggio sanitari, diversi modelli di dispositivi potrebbero significare diverse qualità e proprietà dei sensori. Ciò significa che sensori e dispositivi di bassa qualità possono produrre dati e, pertanto, aggiornamenti del modello nettamente diversi da quelli di alta qualità

* Una tastiera intelligente addestrata per eseguire la correzione automatica. Se si ha una quantità sproporzionata di dispositivi da una determinata regione, lo slang, la struttura della frase o persino il linguaggio che stavano usando potrebbero deviare più aggiornamenti verso un certo stile di digitazione

* Se si hanno sensori per la fauna selvatica in aree remote, la connettività potrebbe non essere distribuita equamente, facendo sì che alcuni client in determinate regioni non siano in grado di inviare più aggiornamenti rispetto ad altri. Se quelle regioni hanno un'attività di fauna selvatica diversa da alcune specie, ciò potrebbe distorcere gli aggiornamenti verso quegli animali

Esistono alcuni approcci per affrontare i dati non IID nell'apprendimento federato. Uno potrebbe essere quello di modificare l'algoritmo di aggregazione. Se si utilizza un algoritmo di aggregazione ponderato, è possibile regolarlo in base a diverse proprietà del client come regione, proprietà del sensore o connettività [@zhao2018federated].

### Selezione del Client

Considerando tutti i fattori che influenzano l'efficacia dell'apprendimento federato, come i dati IID e la comunicazione, la selezione del client è una componente chiave per garantire che un sistema si alleni bene. La selezione dei client sbagliati può distorcere il dataset, con conseguenti dati non IID. Analogamente, la scelta casuale di client con cattive connessioni di rete può rallentare la comunicazione. Pertanto, è necessario considerare diverse caratteristiche chiave quando si seleziona il sottoinsieme corretto di client.

Quando si selezionano i client, ci sono tre componenti principali da considerare: eterogeneità dei dati, allocazione delle risorse e costo della comunicazione. Possiamo selezionare i client in base alle metriche proposte in precedenza nella sezione non IID per affrontare l'eterogeneità dei dati. Nell'apprendimento federato, tutti i dispositivi possono avere diverse quantità di elaborazione, con il risultato che alcuni sono più inefficienti di altri nell'addestramento. Quando si seleziona un sottoinsieme di client per l'addestramento, si deve considerare un equilibrio tra eterogeneità dei dati e risorse disponibili. In uno scenario ideale, è sempre possibile selezionare il sottoinsieme di client con le maggiori risorse. Tuttavia, questo potrebbe distorcere il set di dati, quindi è necessario trovare un equilibrio. Le differenze di comunicazione aggiungono un altro layer; si desidera evitare di essere bloccati dall'attesa che i dispositivi con connessioni scadenti trasmettano tutti i loro aggiornamenti. Pertanto, è anche necessario considerare la scelta di un sottoinsieme di dispositivi diversi ma ben collegati.

### L'Esempio di Gboard

Un esempio primario di un sistema di apprendimento federato distribuito è la tastiera di Google, Gboard, per dispositivi Android. Nell'implementare l'apprendimento federato per la tastiera, Google si è concentrata sull'impiego di tecniche di privacy differenziali per proteggere i dati e l'identità dell'utente. Gboard sfrutta modelli linguistici per diverse funzionalità chiave, come Next Word Prediction (NWP), Smart Compose (SC) e On-The-Fly rescoring (OTF) [@xu2023federated], come mostrato in @fig-gboard-features.

NWP anticiperà la parola successiva che l'utente tenta di digitare in base a quella precedente. SC fornisce suggerimenti in linea per velocizzare la digitazione in base a ciascun carattere. OTF riclassificherà le parole successive proposte in base al processo di digitazione attivo. Tutti e tre questi modelli devono essere eseguiti rapidamente sull'edge e l'apprendimento federato può accelerare l'addestramento sui dati degli utenti. Tuttavia, caricare ogni parola digitata da un utente sul cloud per l'addestramento costituirebbe una violazione massiccia della privacy. Pertanto, l'apprendimento federato enfatizza la privacy differenziale, che protegge l'utente consentendo al contempo una migliore esperienza utente.

![Funzionalità di Google G Board. Fonte: Zheng et al., ([2023](https://arxiv.org/abs/2305.18465)).](images/png/ondevice_gboard_example.png){#fig-gboard-features}

Per raggiungere questo obiettivo, Google ha impiegato il suo algoritmo DP-FTRL, che fornisce una garanzia formale che i modelli addestrati non memorizzeranno dati o identità utente specifici. La progettazione del sistema dell'algoritmo è mostrata in @fig-differential-privacy. DP-FTRL, combinato con l'aggregazione sicura, crittografa gli aggiornamenti del modello e fornisce un equilibrio ottimale tra privacy e utilità. Inoltre, il clipping adattivo viene applicato nel processo di aggregazione per limitare l'impatto dei singoli utenti sul modello globale (passaggio 3 in @fig-differential-privacy). Combinando tutte queste tecniche, Google può perfezionare continuamente la sua tastiera preservando al contempo la privacy dell'utente in un modo formalmente dimostrabile.

![Privacy Differenziale in G Board. Fonte: Zheng et al., ([2023](https://arxiv.org/abs/2305.18465)).](images/png/ondevice_gboard_approach.png){#fig-differential-privacy}

:::{#exr-flg .callout-caution collapse="true"}

### Apprendimento Federato - Generazione di Testo

Avete mai usato quelle tastiere intelligenti che suggeriscono la parola successiva? Con l'apprendimento federato, possiamo renderle ancora migliori senza sacrificare la privacy. In questo Colab, insegneremo a un'IA a prevedere le parole tramite l'addestramento su dati di testo distribuiti su più dispositivi. Prepariamoci a rendere la digitazione ancora più fluida!

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/tensorflow/federated/blob/main/docs/tutorials/federated_learning_for_text_generation.ipynb)

:::

:::{#exr-fli .callout-caution collapse="true"}

### Apprendimento Federato - Classificazione delle Immagini

Vogliamo addestrare un'IA esperta di immagini senza inviare le proprie foto al cloud? L'apprendimento federato è la risposta! In questo Colab, addestreremo un modello su più dispositivi, ognuno dei quali apprende dalle proprie immagini. La privacy è protetta e il lavoro di squadra fa funzionare il sogno dell'IA!

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/tensorflow/federated/blob/v0.5.0/docs/tutorials/federated_learning_for_image_classification.ipynb)

:::

### Benchmarking Federated Learning: MedPerf

I dispositivi medici rappresentano uno degli esempi più ricchi di dati edge. Questi dispositivi memorizzano alcuni dei dati utente più personali, offrendo allo stesso tempo progressi significativi nel trattamento personalizzato e una maggiore accuratezza nell'IA medica. Questa combinazione di dati sensibili e potenziale di innovazione rende i dispositivi medici un caso d'uso ideale per l'apprendimento federato.

Uno sviluppo chiave in questo campo è MedPerf, una piattaforma open source progettata per il benchmarking dei modelli utilizzando la valutazione federata [@karargyris2023federated]. MedPerf va oltre il tradizionale apprendimento federato portando il modello sui dispositivi periferici per testare i dati personalizzati mantenendo la privacy. Questo approccio consente a un comitato di benchmark di valutare vari modelli in scenari reali su dispositivi edge senza compromettere l'anonimato del paziente.

La piattaforma MedPerf, descritta in dettaglio in uno studio recente (https://doi.org/10.1038/s42256-023-00652-2), dimostra come le tecniche federate possano essere applicate non solo all'addestramento dei modelli, ma anche alla valutazione e al benchmarking dei modelli. Questo progresso è particolarmente cruciale nel campo medico, dove l'equilibrio tra lo sfruttamento di grandi set di dati per migliorare le prestazioni dell'IA e la protezione della privacy individuale è di fondamentale importanza.

## Problemi di Sicurezza

L'esecuzione di training e adattamento del modello ML sui dispositivi degli utenti finali introduce anche rischi per la sicurezza che devono essere affrontati. Alcune preoccupazioni chiave per la sicurezza includono:

* **Esposizione di dati privati:** I dati di training potrebbero essere trapelati o rubati dai dispositivi
* **Avvelenamento dei dati:** Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello
* **Estrazione del modello:** Degli aggressori potrebbero tentare di rubare i parametri del modello addestrato
* **Inferenza di appartenenza:** I modelli potrebbero rivelare la partecipazione di dati di utenti specifici
* **Attacchi di evasione:** Input appositamente creati possono causare una classificazione errata

Qualsiasi sistema che esegue l'apprendimento sul dispositivo introduce preoccupazioni per la sicurezza, poiché potrebbe esporre vulnerabilità in modelli su larga scala. Numerosi rischi per la sicurezza sono associati a qualsiasi modello ML, ma questi rischi hanno conseguenze specifiche per l'apprendimento on-device. Fortunatamente, esistono metodi per mitigare questi rischi e migliorare le prestazioni reali dell'apprendimento su dispositivo.

### Avvelenamento dei Dati

L'apprendimento automatico on-device introduce sfide uniche per la sicurezza dei dati rispetto all'addestramento tradizionale basato su cloud. In particolare, gli attacchi di avvelenamento dei dati rappresentano una seria minaccia durante l'apprendimento su dispositivo. Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello quando vengono distribuiti.

Esistono diverse tecniche di attacco di avvelenamento dei dati:

* **Label Flipping:** Comporta l'applicazione di etichette errate ai campioni. Ad esempio, nella classificazione delle immagini, le foto di gatti possono essere etichettate come cani per confondere il modello. Anche capovolgere il [10% delle etichette](https://proceedings.mlr.press/v139/schwarzschild21a.html) può avere conseguenze significative sul modello.
* **Inserimento dei Dati:** Introduce input falsi o distorti nel set di training. Ciò potrebbe includere immagini pixelate, audio rumoroso o testo distorto.
* **Corruzione Logica:** Altera i [pattern] sottostanti (<https://www.worldscientific.com/doi/10.1142/S0218001414600027>) nei dati per fuorviare il modello. Nell'analisi del "sentiment", le recensioni altamente negative possono essere contrassegnate come positive tramite questa tecnica. Per questo motivo, recenti sondaggi hanno dimostrato che molte aziende hanno più [paura dell'avvelenamento dei dati](https://proceedings.mlr.press/v139/schwarzschild21a.html) rispetto ad altre preoccupazioni di ML avversarie.

Ciò che rende l'avvelenamento dei dati allarmante è il modo in cui sfrutta la discrepanza tra set di dati curati e dati di training in tempo reale. Consideriamo un set di dati di foto di gatti raccolti da Internet. Nelle settimane successive, quando questi dati addestrano un modello on-device, le nuove foto di gatti sul Web differiscono in modo significativo.

Con l'avvelenamento dei dati, gli aggressori acquistano domini e caricano contenuti che influenzano una parte dei dati di training. Anche piccole modifiche ai dati hanno un impatto significativo sul comportamento appreso dal modello. Di conseguenza, l'avvelenamento può instillare pregiudizi razzisti, sessisti o altri pregiudizi dannosi se non controllato.

[Microsoft Tay](https://en.wikipedia.org/wiki/Tay_(chatbot)) è un chatbot lanciato da Microsoft nel 2016. È stato progettato per imparare dalle sue interazioni con gli utenti su piattaforme di social media come Twitter. Sfortunatamente, Microsoft Tay è diventato un esempio lampante di avvelenamento dei dati nei modelli di ML. Entro 24 ore dal suo lancio, Microsoft ha dovuto mettere Tay offline perché aveva iniziato a produrre messaggi offensivi e inappropriati, tra cui incitamento all'odio e commenti razzisti. Ciò è accaduto perché alcuni utenti sui social media hanno intenzionalmente fornito a Tay input dannosi e offensivi, da cui il chatbot ha poi imparato e incorporato nelle sue risposte.

Questo incidente è un chiaro esempio di avvelenamento dei dati, poiché i malintenzionati hanno intenzionalmente manipolato i dati utilizzati per addestrare il chatbot e modellarne le risposte. L'avvelenamento dei dati ha portato il chatbot ad adottare pregiudizi dannosi e a produrre output che i suoi sviluppatori non avevano previsto. Dimostra come anche piccole quantità di dati creati in modo dannoso possano avere un impatto significativo sul comportamento dei modelli ML e sottolinea l'importanza di implementare solidi meccanismi di filtraggio e convalida dei dati per impedire che tali incidenti si verifichino.

Tali pregiudizi potrebbero avere pericolosi impatti nel mondo reale. La convalida rigorosa dei dati, il rilevamento delle anomalie e il monitoraggio della provenienza dei dati sono misure difensive fondamentali. L'adozione di framework come Five Safes garantisce che i modelli siano addestrati su dati rappresentativi di alta qualità [@desai2016five].

L'avvelenamento dei dati è una preoccupazione urgente per l'apprendimento sicuro sul dispositivo poiché i dati dell'endpoint non possono essere facilmente monitorati in tempo reale. Se ai modelli viene consentito di adattarsi da soli, corriamo il rischio che il dispositivo agisca in modo dannoso. Tuttavia, è necessaria una ricerca continua sull'apprendimento automatico avversario per sviluppare soluzioni efficaci per rilevare e mitigare tali attacchi ai dati.

### Attacchi Avversari

Durante la fase di addestramento, gli aggressori potrebbero iniettare dati dannosi nel dataset di training, il che può alterare sottilmente il comportamento del modello. Ad esempio, un aggressore potrebbe aggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini. Se fatto in modo intelligente, l'accuratezza del modello potrebbe non diminuire in modo significativo e l'attacco potrebbe essere notato. Il modello classificherebbe quindi erroneamente alcuni gatti come cani, il che potrebbe avere conseguenze a seconda dell'applicazione.

In un sistema di telecamere di sicurezza embedded, ad esempio, ciò potrebbe consentire a un intruso di evitare il rilevamento indossando uno specifico pattern che il modello è stato ingannato a classificare come non minaccioso.

Durante la fase di inferenza, gli aggressori possono utilizzare esempi avversari per ingannare il modello. Gli esempi avversari sono input che sono stati leggermente alterati in un modo da far sì che il modello faccia previsioni errate. Ad esempio, un aggressore potrebbe aggiungere una piccola quantità di rumore a un'immagine in un modo che un sistema di riconoscimento facciale identifichi erroneamente una persona. Questi attacchi possono essere particolarmente preoccupanti nelle applicazioni in cui è in gioco la sicurezza, come i veicoli autonomi. Un esempio concreto di ciò è quando i ricercatori sono riusciti a far sì che un sistema di riconoscimento della segnaletica stradale classificasse erroneamente un segnale di stop come un segnale di limite di velocità. Questo tipo di classificazione errata potrebbe causare incidenti se si verificasse in un sistema di guida autonoma nel mondo reale.

Per mitigare questi rischi, possono essere impiegate diverse difese:

* **Validazione e Sanificazione dei Dati:** Prima di incorporare nuovi dati nel dataset di addestramento, questi devono essere convalidati e sanificati a fondo per garantire che non siano dannosi.
* **Addestramento Avversario:** Il modello può essere addestrato su esempi avversari per renderlo più robusto a questi tipi di attacchi.
* **Validazione degli Input:** Durante l'inferenza, gli input devono essere convalidati per garantire che non siano stati manipolati per creare esempi avversari.
* **Audit e Monitoraggio Regolari:** L'audit e il monitoraggio regolari del comportamento del modello possono aiutare a rilevare e mitigare gli attacchi avversari. Tuttavia, è più facile a dirsi che a farsi nel contesto di piccoli sistemi ML. Spesso è difficile monitorare i sistemi ML embedded all'endpoint a causa delle limitazioni della larghezza di banda della comunicazione, di cui parleremo nel capitolo MLOps.

Comprendendo i potenziali rischi e implementando queste difese, possiamo contribuire a proteggere il training on-device all'endpoint/edge e mitigare l'impatto degli attacchi avversari. La maggior parte delle persone confonde facilmente l'avvelenamento dei dati e gli attacchi avversari. Quindi @tbl-attacks confronta l'avvelenamento dei dati e gli attacchi avversari:

+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Aspetto                        | Avvelenamento dei dati                                                                                                                        | Attacchi avversari                                                                                                                                |
+:===============================+:==============================================================================================================================================+:==================================================================================================================================================+
| Tempistica                     | Fase di addestramento                                                                                                                         | Fase di inferenza                                                                                                                                 |
+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Target                         | Dati di addestramento                                                                                                                         | Dati di input                                                                                                                                     |
+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Obiettivo                      | Influenza negativamente le prestazioni del modello                                                                                            | Causa previsioni errate                                                                                                                           |
+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Metodo                         | Inserire esempi dannosi nei dati di training, spesso con etichette errate                                                                     | Aggiungere rumore attentamente elaborato ai dati di input                                                                                         |
+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Esempio                        | Aggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini    | Aggiungere una piccola quantità di rumore a un'immagine in modo che un sistema di riconoscimento facciale identifichi erroneamente una persona    |
+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Effetti Potenziali             | Il modello apprende pattern errati e fa previsioni errate                                                                                     | Previsioni errate immediate e potenzialmente pericolose                                                                                           |
+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Applicazioni Interessate       | Qualsiasi modello ML                                                                                                                          | Veicoli autonomi, sistemi di sicurezza, ecc.                                                                                                      |
+--------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+

: Confronto tra avvelenamento dei dati e attacchi avversari. {#tbl-attacks .striped .hover}

### Inversione del Modello

Gli attacchi di inversione del modello rappresentano una minaccia per la privacy dei modelli di machine learning su dispositivo addestrati su dati utente sensibili [@nguyen2023re]. Comprendere questo vettore di attacco e le strategie di mitigazione saranno importanti per creare un'intelligenza artificiale su dispositivo sicura ed etica. Ad esempio, si immagini un'app per iPhone che utilizza l'apprendimento su dispositivo per categorizzare le foto in gruppi come "spiaggia", "cibo" o "selfie" per una ricerca più semplice.

Il modello su dispositivo potrebbe essere addestrato da Apple su un set di dati di foto iCloud di utenti consenzienti. Un aggressore malintenzionato potrebbe tentare di estrarre parti di quelle foto di addestramento iCloud originali utilizzando l'inversione del modello. In particolare, l'aggressore inserisce input sintetici creati ad arte nel classificatore di foto su dispositivo. Modificando gli input sintetici e osservando come il modello li categorizza, possono perfezionare gli input fino a ricostruire copie dei dati di training originali, come una foto di una spiaggia dall'iCloud di un utente. Ora, l'aggressore ha violato la privacy di quell'utente ottenendo una delle sue foto senza consenso. Questo dimostra perché l'inversione del modello è pericolosa: può potenzialmente far trapelare dati di training altamente sensibili.

Le foto sono un tipo di dati particolarmente rischioso perché spesso contengono persone identificabili, informazioni sulla posizione e momenti privati. Tuttavia, la stessa metodologia di attacco potrebbe essere applicata ad altri dati personali, come registrazioni audio, messaggi di testo o dati sanitari degli utenti.

Per difendersi dall'inversione del modello, sarebbe necessario prendere precauzioni come l'aggiunta di rumore agli output del modello o l'utilizzo di tecniche di apprendimento automatico che preservano la privacy come l'[apprendimento federato](@sec-fl) per addestrare il modello sul dispositivo. L'obiettivo è impedire agli aggressori di ricostruire i dati di training originali.

### Problemi di Sicurezza dell'Apprendimento On-Device

Sebbene l'avvelenamento dei dati e gli attacchi avversari siano preoccupazioni comuni per i modelli ML in generale, l'apprendimento su dispositivo introduce rischi di sicurezza unici. Quando vengono pubblicate varianti su dispositivo di modelli su larga scala, gli avversari possono sfruttare questi modelli più piccoli per attaccare le loro controparti più grandi. La ricerca ha dimostrato che man mano che i modelli su dispositivo e i modelli su scala reale diventano più simili, la vulnerabilità dei modelli originali su larga scala aumenta in modo significativo. Ad esempio, le valutazioni su 19 reti neurali profonde (DNN) hanno rivelato che lo sfruttamento dei modelli su dispositivo potrebbe aumentare la vulnerabilità dei modelli originali su larga scala di [fino a 100 volte](http://arxiv.org/abs/2212.13700).

Esistono tre tipi principali di rischi per la sicurezza specifici dell'apprendimento on-device:

* **Attacchi Basati sul Trasferimento:** Questi attacchi sfruttano la proprietà di trasferibilità tra un modello surrogato (un'approssimazione del modello di destinazione, simile a un modello su dispositivo) e un modello di destinazione remoto (il modello originale su scala reale). Gli aggressori generano esempi avversari utilizzando il modello surrogato, che può quindi essere utilizzato per ingannare il modello di destinazione. Ad esempio, si immagini un modello on-device progettato per identificare le e-mail di spam. Un aggressore potrebbe usare questo modello per generare un'e-mail di spam che non viene rilevata dal sistema di filtraggio più grande e completo.

* **Attacchi Basati sull'Ottimizzazione:** Questi attacchi generano esempi avversari per attacchi basati sul trasferimento usando una qualche forma di funzione obiettivo e modificano iterativamente gli input per ottenere il risultato desiderato. Gli attacchi di stima del gradiente, ad esempio, approssimano il gradiente del modello usando output di query (come punteggi di confidenza softmax), mentre gli attacchi senza gradiente usano la decisione finale del modello (la classe prevista) per approssimare il gradiente, sebbene richiedano molte più query.

* **Attacchi di Query con Priorità di Trasferimento:** Questi attacchi combinano elementi di attacchi basati sul trasferimento e basati sull'ottimizzazione. Eseguono il reverse engineering dei modelli sul dispositivo per fungere da surrogati del modello completo di destinazione. In altre parole, gli aggressori usano il modello sul dispositivo più piccolo per capire come funziona il modello più grande e quindi usano questa conoscenza per attaccare il modello completo.

Grazie alla comprensione di questi rischi specifici associati all'apprendimento on-device, possiamo sviluppare protocolli di sicurezza più solidi per proteggere sia i modelli on-device che quelli su scala reale da potenziali attacchi.

### Attenuazione dei Rischi dell'Apprendimento On-Device

Si possono impiegare vari metodi per mitigare i numerosi rischi per la sicurezza associati all'apprendimento on-device. Questi metodi possono essere specifici per il tipo di attacco o fungere da strumento generale per rafforzare la sicurezza.

Una strategia per ridurre i rischi per la sicurezza è quella di ridurre la somiglianza tra modelli on-device e modelli su scala reale, riducendo così la trasferibilità fino al 90%. Questo metodo, noto come similarity-unpairing, affronta il problema che si verifica quando gli avversari sfruttano la somiglianza del gradiente di input tra i due modelli. Ottimizzando il modello su scala reale per creare una nuova versione con accuratezza simile ma gradienti di input diversi, possiamo costruire il modello on-device quantizzando questo modello su scala reale aggiornato. Questa disassociazione riduce la vulnerabilità dei modelli su dispositivo limitando l'esposizione del modello su scala reale originale. È importante notare che l'ordine di ottimizzazione e quantizzazione può essere variato pur ottenendo la mitigazione del rischio [@hong2023publishing].

Per contrastare l'avvelenamento dei dati, è fondamentale reperire set di dati da [fornitori](https://www.eetimes.com/cybersecurity-threats-loom-over-endpoint-ai-systems/?_gl=1%2A17zgs0d%2A_ga%2AMTY0MzA1MTAyNS4xNjk4MDgyNzc1%2A_ga_ZLV02RYCZ8%2AMTY5ODA4Mjc3NS4xLjAuMTY5ODA4Mjc3NS42MC4wLjA) affidabili e fidati.

Per combattere gli attacchi avversari, si possono impiegare diverse strategie. Un approccio proattivo prevede la generazione di esempi avversari e la loro incorporazione nel set di dati di training del modello, rafforzando così il modello contro tali attacchi. Strumenti come [CleverHans](http://github.com/cleverhans-lab/cleverhans), una libreria di training open source, sono fondamentali per creare esempi avversari. La "Defense distillation" [distillazione della difesa] è un'altra strategia efficace, in cui il modello sul dispositivo genera probabilità di classificazioni diverse anziché decisioni definitive [@hong2023publishing], rendendo più difficile per gli esempi avversari sfruttare il modello.

Il furto di proprietà intellettuale è un altro problema significativo quando si distribuiscono modelli on-device. Il furto di proprietà intellettuale è un problema quando si distribuiscono modelli on-device, poiché gli avversari potrebbero tentare di sottoporre a reverse engineering il modello per rubare la tecnologia sottostante. Per proteggersi dal furto di proprietà intellettuale, l'eseguibile binario del modello addestrato dovrebbe essere archiviato su un'unità microcontrollore con software crittografato e interfacce fisiche protette del chip. Inoltre, il set di dati finale utilizzato per l'addestramento del modello dovrebbe essere mantenuto [privato](https://www.eetimes.com/cybersecurity-threats-loom-over-endpoint-ai-systems/?_gl=1%2A17zgs0d%2A_ga%2AMTY0MzA1MTAyNS4xNjk4MDgyNzc1%2A_ga_ZLV02RYCZ8%2AMTY5ODA4Mjc3NS4xLjAuMTY5ODA4Mjc3NS42MC4wLjA).

Inoltre, i modelli on-device utilizzano spesso set di dati noti o open source, come Visual Wake Words di MobileNet. Pertanto, è importante mantenere la [privacy del set di dati finale](http://arxiv.org/abs/2212.13700) utilizzato per l'addestramento del modello. Inoltre, proteggere il processo di "data augmentation" e incorporare casi d'uso specifici può ridurre al minimo il rischio di reverse engineering di un modello on-device.

Infine, l'Adversarial Threat Landscape for Artificial Intelligence Systems ([ATLAS](https://atlas.mitre.org/)) funge da prezioso strumento matriciale che aiuta a valutare il profilo di rischio dei modelli su dispositivo, consentendo agli sviluppatori di identificare e [mitigare](https://www.eetimes.com/cybersecurity-threats-loom-over-endpoint-ai-systems/?_gl=1%2A17zgs0d%2A_ga%2AMTY0MzA1MTAyNS4xNjk4MDgyNzc1%2A_ga_ZLV02RYCZ8%2AMTY5ODA4Mjc3NS4xLjAuMTY5ODA4Mjc3NS42MC4wLjA) i potenziali rischi in modo proattivo.

### Protezione dei Dati di Training

Esistono vari modi per proteggere i dati di training sul dispositivo. Ogni concetto è molto profondo e potrebbe valere una lezione a sé stante. Quindi, qui, faremo un breve accenno a quei concetti in modo che si sappia cosa approfondire.

#### Crittografia

La crittografia funge da prima linea di difesa per i dati di training. Ciò comporta l'implementazione della crittografia end-to-end per l'archiviazione locale su dispositivi e canali di comunicazione per impedire l'accesso non autorizzato ai dati di training grezzi. Ambienti di esecuzione affidabili, come [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html) e [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-a#:~:text=Arm%20TrustZone%20technology%20offers%20an,trust%20based%20on%20PSA%20guidelines.), sono essenziali per facilitare il training sicuro su dati crittografati.

Inoltre, quando si aggregano aggiornamenti da più dispositivi, è possibile impiegare protocolli di elaborazione "multi-party" sicuri per migliorare la sicurezza [@kairouz2015secure]; un'applicazione pratica di ciò è nell'apprendimento collaborativo on-device, in cui è possibile implementare l'aggregazione crittografica che preserva la privacy degli aggiornamenti del modello utente. Questa tecnica nasconde efficacemente i dati dei singoli utenti anche durante la fase di aggregazione.

#### Privacy Differenziale

La privacy differenziale è un'altra strategia cruciale per proteggere i dati di training. Iniettando rumore statistico calibrato nei dati, possiamo mascherare i singoli record estraendo comunque preziosi pattern di popolazione [@dwork2014algorithmic]. Anche la gestione del budget per la privacy su più iterazioni di training e la riduzione del rumore man mano che il modello converge sono essenziali [@abadi2016deep]. Possono essere impiegati metodi come la privacy differenziale formalmente dimostrabile, che può includere l'aggiunta di rumore di Laplace o gaussiano scalato alla sensibilità del set di dati.

#### Rilevamento delle Anomalie

Il rilevamento delle anomalie svolge un ruolo importante nell'identificazione e nell'attenuazione di potenziali attacchi di avvelenamento dei dati. Ciò può essere ottenuto tramite analisi statistiche come la "Principal Component Analysis (PCA)" [analisi delle componenti principali] e il clustering, che aiutano a rilevare deviazioni nei dati di training aggregati. I metodi di serie temporali come i grafici [Cumulative Sum (CUSUM)](https://en.wikipedia.org/wiki/CUSUM) sono utili per identificare spostamenti indicativi di potenziale avvelenamento. Anche il confronto delle distribuzioni dei dati correnti con distribuzioni di dati pulite precedentemente visualizzate può aiutare a segnalare anomalie. Inoltre, i batch sospetti di essere avvelenati dovrebbero essere rimossi dal processo di aggregazione degli aggiornamenti di training. Ad esempio, è possibile condurre controlli a campione su sottoinsiemi di immagini di training sui dispositivi utilizzando hash [photoDNA](https://www.microsoft.com/en-us/photodna) per identificare input avvelenati.

#### Validazione dei Dati di Input

Infine, la convalida dei dati di input è essenziale per garantire l'integrità e la validità dei dati di input prima che vengano immessi nel modello di training, proteggendo così dai payload avversari. Misure di similarità, come la distanza del coseno, possono essere impiegate per catturare input che si discostano in modo significativo dalla distribuzione prevista. Gli input sospetti che potrebbero contenere payload avversari devono essere messi in quarantena e sanificati. Inoltre, l'accesso del parser ai dati di training deve essere limitato solo ai percorsi di codice convalidati. Sfruttare le funzionalità di sicurezza hardware, come ARM Pointer Authentication, può impedire la corruzione della memoria (ARM Limited, 2023). Un esempio di ciò è l'implementazione di controlli di integrità degli input sui dati di training audio utilizzati dagli smart speaker prima dell'elaborazione da parte del modello di riconoscimento vocale [@chen2023learning].

## Framework di Training On-Device

Framework di inferenza embedded come TF-Lite Micro [@david2021tensorflow], TVM [@chen2018tvm] e MCUNet [@lin2020mcunet] forniscono un runtime snello per l'esecuzione di modelli di reti neurali su microcontrollori e altri dispositivi con risorse limitate. Tuttavia, non supportano l'addestramento on-device. L'addestramento richiede un proprio set di strumenti specializzati a causa dell'impatto della quantizzazione sul calcolo del gradiente e dell'ingombro di memoria della backpropagation [@lin2022device].

Negli ultimi anni, hanno iniziato a emergere una manciata di strumenti e framework che consentono l'addestramento sul dispositivo. Tra questi Tiny Training Engine [@lin2022device], TinyTL [@cai2020tinytl] e TinyTrain [@kwon2023tinytrain].

### Tiny Training Engine

Tiny Training Engine (TTE) utilizza diverse tecniche per ottimizzare l'utilizzo della memoria e velocizzare il processo di training. Una panoramica del flusso di lavoro TTE è mostrata in @fig-tte-workflow. Innanzitutto, TTE scarica la differenziazione automatica in fase di compilazione anziché in fase di runtime, riducendo significativamente il sovraccarico durante il training. In secondo luogo, TTE esegue l'ottimizzazione del grafo come la potatura e gli aggiornamenti sparsi per ridurre i requisiti di memoria e accelerare i calcoli.

![Flusso di lavoro di TTE.](images/png/ondevice_training_flow.png){#fig-tte-workflow}

In particolare, TTE segue quattro passaggi principali:

* Durante la fase di compilazione, TTE traccia il grafo di propagazione "forward" e deriva il grafo "backward" corrispondente per la backpropagation. Ciò consente alla [differenziazione](https://harvard-edge.github.io/cs249r_book/frameworks.html#differentiable-programming) di avvenire in fase di compilazione anziché in fase di esecuzione.
* TTE elimina tutti i nodi che rappresentano pesi congelati dal grafo backward. I pesi congelati sono pesi che non vengono aggiornati durante l'addestramento per ridurre l'impatto di determinati neuroni. La potatura dei loro nodi consente di risparmiare memoria.
* TTE riordina gli operatori di discesa del gradiente per intercalarli con i calcoli del passaggio del backward. Questa pianificazione riduce al minimo le "impronte" [occupazione] di memoria.
* TTE utilizza la generazione di codice per compilare i grafi "forward" e "backward" ottimizzati, che vengono poi distribuiti per l'addestramento on-device.

### Tiny Transfer Learning

Tiny Transfer Learning (TinyTL) consente un training efficiente in termini di memoria sul dispositivo tramite una tecnica chiamata congelamento dei pesi. Durante il training, gran parte del collo di bottiglia della memoria deriva dall'archiviazione delle attivazioni intermedie e dall'aggiornamento dei pesi nella rete neurale.

Per ridurre questo sovraccarico di memoria, TinyTL congela la maggior parte dei pesi in modo che non debbano essere aggiornati durante il training. Ciò elimina la necessità di archiviare le attivazioni intermedie per le parti congelate della rete. TinyTL ottimizza solo i termini di bias, che sono molto più piccoli dei pesi. Una panoramica del flusso di lavoro TinyTL è mostrata in @fig-tinytl-workflow.

![Flusso di lavoro di TinyTL. Fonte: @cai2020tinytl.)](images/png/ondevice_transfer_tinytl.png){#fig-tinytl-workflow}

I pesi di congelamento si applicano a layer completamente connessi, nonché a layer di normalizzazione e convoluzionali. Tuttavia, solo l'adattamento dei bias limita la capacità del modello di apprendere e adattarsi a nuovi dati.

Per aumentare l'adattabilità senza molta memoria aggiuntiva, TinyTL utilizza un piccolo modello di apprendimento residuo. Questo affina le mappe delle feature intermedie per produrre output migliori, anche con pesi fissi. Il modello residuo introduce un overhead minimo, inferiore al 3,8% in più rispetto al modello di base.

Congelando la maggior parte dei pesi, TinyTL riduce significativamente l'utilizzo della memoria durante l'addestramento on-device. Il modello residuo consente quindi di adattarsi e apprendere in modo efficace per l'attività. L'approccio combinato fornisce un addestramento on-device efficiente in termini di memoria con un impatto minimo sulla precisione del modello.

### Tiny Train

TinyTrain riduce significativamente il tempo necessario per l'addestramento sul dispositivo aggiornando selettivamente solo determinate parti del modello. Ciò avviene utilizzando una tecnica chiamata aggiornamento sparso adattivo all'attività, come mostrato in @fig-tiny-train.

![Flusso di lavoro di TinyTrain. Fonte: @kwon2023tinytrain.](images/png/ondevice_pretraining.png){#fig-tiny-train}

In base ai dati utente, alla memoria e al calcolo disponibili sul dispositivo, TinyTrain sceglie dinamicamente quali layer della rete neurale aggiornare durante l'addestramento. Questa selezione di layer è ottimizzata per ridurre l'utilizzo di calcolo e memoria mantenendo un'elevata accuratezza.

Più specificamente, TinyTrain esegue prima il pre-addestramento offline del modello. Durante il pre-addestramento, non solo addestra il modello sui dati dell'attività, ma anche il meta-addestramento del modello. Meta-addestramento significa addestrare il modello sui metadati relativi al processo di addestramento stesso. Questo meta-addestramento migliora la capacità del modello di adattarsi in modo accurato anche quando sono disponibili dati limitati per l'attività target.

Poi, durante la fase di adattamento online, quando il modello viene personalizzato sul dispositivo, TinyTrain esegue aggiornamenti adattivi sparsi all'attività. Utilizzando i criteri relativi alle capacità del dispositivo, seleziona solo determinati layer da aggiornare tramite backpropagation. I layer vengono scelti per bilanciare accuratezza, utilizzo della memoria e tempo di elaborazione.

Aggiornando in modo sparso i layer su misura per il dispositivo e l'attività, TinyTrain riduce significativamente il tempo di addestramento sul dispositivo e l'utilizzo delle risorse. Il meta-training offline migliora anche l'accuratezza quando si adatta a dati limitati. Insieme, questi metodi consentono un training on-device rapido, efficiente e accurato.

### Confronto

@tbl-framework-comparison riassume le principali somiglianze e differenze tra i diversi framework.

+------------------------+----------------------------------------+-----------------------------------------------+
| Framework              | Somiglianze                            | Differenze                                    |
+:=======================+:=======================================+:==============================================+
| Tiny Training Engine   | - Addestramento sul dispositivo        | - Traccia grafi forward & backward            |
|                        | - Ottimizza memoria e calcolo          | - Elimina i pesi congelati                    |
|                        | - Sfrutta potatura, sparsità, ecc.     | - Interlaccia backprop e gradienti            |
|                        |                                        | - Generazione di codice                       |
+------------------------+----------------------------------------+-----------------------------------------------+
| TinyTL                 | - Addestramento sul dispositivo        | - Congela la maggior parte dei pesi           |
|                        | - Ottimizza memoria e calcolo          | - Adatta solo i bias                          |
|                        | - Sfrutta congelamento, sparsità, ecc. | - Utilizza il modello residuo                 |
+------------------------+----------------------------------------+-----------------------------------------------+
| TinyTrain              | - Addestramento sul dispositivo        | - Meta-addestramento nel pre-addestramento    |
|                        | - Ottimizza memoria e calcolo          | - Aggiornamento sparse adattivo alle attività |
|                        | - Sfrutta sparsità, ecc.               | - Aggiornamento selettivo dei layer           |
+------------------------+----------------------------------------+-----------------------------------------------+

: Confronto di framework per l'ottimizzazione del training on-device. {#tbl-framework-comparison .striped .hover}

## Conclusione

Il concetto di apprendimento on-device [su dispositivo] è sempre più importante per aumentare l'usabilità e la scalabilità di TinyML. Questo capitolo ha esplorato le complessità dell'apprendimento on-device, esplorandone vantaggi e limiti, strategie di adattamento, algoritmi e tecniche chiave correlate, implicazioni di sicurezza e framework di training on-device esistenti ed emergenti.

L'apprendimento su dispositivo è, senza dubbio, un paradigma rivoluzionario che porta con sé numerosi vantaggi per le distribuzioni ML embedded ed edge. Eseguendo il training direttamente sui dispositivi endpoint, si elimina la necessità di una connettività cloud continua, rendendolo particolarmente adatto per applicazioni IoT ed edge computing. Presenta vantaggi quali maggiore privacy, facilità di conformità ed efficienza delle risorse. Allo stesso tempo, l'apprendimento su on-device deve affrontare limitazioni legate a vincoli hardware, dimensioni dei dati limitate e ridotta accuratezza e generalizzazione del modello.

Meccanismi quali la ridotta complessità del modello, tecniche di ottimizzazione e compressione dei dati e metodi di apprendimento correlati quali apprendimento tramite trasferimento e apprendimento federato consentono ai modelli di adattarsi per apprendere ed evolversi in base a vincoli di risorse, fungendo così da fondamento per un efficace ML sui dispositivi edge.

Le problematiche critiche di sicurezza nell'apprendimento su dispositivo evidenziate in questo capitolo, che vanno dall'avvelenamento dei dati e dagli attacchi avversari ai rischi specifici introdotti dall'apprendimento on-device, devono essere affrontate in carichi di lavoro reali affinché l'apprendimento su dispositivo sia un paradigma praticabile. Strategie di mitigazione efficaci, quali convalida dei dati, crittografia, privacy differenziale, rilevamento delle anomalie e convalida dei dati di input, sono fondamentali per salvaguardare i sistemi di apprendimento on-device da queste minacce.

L'emergere di framework di training specializzati on-device, come Tiny Training Engine, Tiny Transfer Learning e Tiny Train, offre strumenti pratici che consentono un training efficiente sui dispositivi. Questi framework impiegano varie tecniche per ottimizzare l'utilizzo della memoria, ridurre il sovraccarico computazionale e semplificare il processo di training on-device.

In conclusione, l'apprendimento on-device è in prima linea in TinyML, promettendo un futuro in cui i modelli possono acquisire autonomamente conoscenze e adattarsi ad ambienti mutevoli su dispositivi edge. L'applicazione dell'apprendimento on-device ha il potenziale per rivoluzionare vari ambiti, tra cui sanità, IoT industriale e città intelligenti. Tuttavia, il potenziale trasformativo dell'apprendimento on-device deve essere bilanciato con misure di sicurezza robuste per proteggere da violazioni dei dati e minacce avversarie. L'adozione di framework di training on-device innovativi e l'implementazione di protocolli di sicurezza rigorosi sono passaggi chiave per sbloccare il pieno potenziale dell'apprendimento su dispositivo. Man mano che questa tecnologia continua a evolversi, promette di rendere i nostri dispositivi più intelligenti, più reattivi e meglio integrati nella nostra vita quotidiana.

## Risorse {#sec-on-device-learning-resource}

Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.

:::{.callout-note collapse="false"}

# Slide

Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.

* [Intro to TensorFlow Lite (TFLite).](https://docs.google.com/presentation/d/19nF6CATRBqQWGBBv4uO4RzWpAwwuhmBAv8AQdBkkAVY/edit#slide=id.g94db9f9f78_0_2)

* [TFLite Optimization and Quantization.](https://docs.google.com/presentation/d/1JwP46J6eLFUebNy2vKDvPzExe20DuTL95Nw8ubCxNPg/edit#slide=id.g94db9f9f78_0_2)

* [TFLite Quantization-Aware Training.](https://docs.google.com/presentation/d/1eSOyAOu8Vg_VfIHZ9gWRVjWnmFTOcZ4FavaNMc4reHQ/edit#slide=id.p1)

* Trasferimento dell'Apprendimento:
   * [Transfer Learning: with Visual Wake Words example.](https://docs.google.com/presentation/d/1kVev1WwXG2MrpEMmRbiPjTBwQ6CSCE_K84SUlSbuUPM/edit#slide=id.ga654406365_0_127)

   * [On-device Training and Transfer Learning.](https://docs.google.com/presentation/d/1wou3qW4kXttufz6hR5lXAcZ3kXlwkl1O/edit?usp=sharing&ouid=102419556060649178683&rtpof=true&sd=true)

* Addestramento Distribuito:
   * [Distributed Training.](https://docs.google.com/presentation/d/19YyoXqFzLaywEGOb5ccLK4MeNqxvr-qo/edit?usp=drive_link&ouid=102419556060649178683&rtpof=true&sd=true)

   * [Distributed Training.](https://docs.google.com/presentation/d/16xSDyhiHoSgmnUNMzvcYSFMCg2LGF8Gu/edit?usp=drive_link&ouid=102419556060649178683&rtpof=true&sd=true)

* Monitoraggio Continuo:
   * [Continuous Evaluation Challenges for TinyML.](https://docs.google.com/presentation/d/1OuhwH5feIwPivEU6pTDyR3QMs7AFstHLiF_LB8T5qYQ/edit?usp=drive_link&resourcekey=0-DZxIuVBUbJawuFh0AO-Pvw)

   * [Federated Learning Challenges.](https://docs.google.com/presentation/d/1Q8M76smakrt5kTqggoPW8WFTrP0zIrV-cWj_BEfPxIA/edit?resourcekey=0-mPx0WwZOEVkHndVhr_MzMQ#slide=id.g94db9f9f78_0_2)

   * [Continuous Monitoring with Federated ML.](https://docs.google.com/presentation/d/1dHqWjKflisdLhX43jjOUmZCyM0tNhXTVgcch-Bcp-uo/edit?usp=drive_link&resourcekey=0-AuuCxz6QKc-t3lXMPeX1Sg)

   * [Continuous Monitoring Impact on MLOps.](https://docs.google.com/presentation/d/1D7qI7aLGnoUV7x3s5Dqa44CsJTQdDO5xtID5MBM0GxI/edit?usp=drive_link&resourcekey=0-g7SB2RDsdGt01tPCI7VeUQ)

:::

:::{.callout-important collapse="false"}

#### Video

* @vid-tl

* @vid-fl

:::

:::{.callout-caution collapse="false"}

#### Esercizi

Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.

* @exr-tlb

* @exr-flg

* @exr-fli
:::

:::{.callout-warning collapse="false"}

#### Laboratori

Oltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l'esperienza di apprendimento.

* _Prossimamente._
:::
