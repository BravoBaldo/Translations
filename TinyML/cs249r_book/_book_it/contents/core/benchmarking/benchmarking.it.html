<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Benchmarking dell’IA – Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" rel="next">
<link href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalità oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/benchmarking/benchmarking.it.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell’IA</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABORATORI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/part_LABS.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">LABORATORI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/part_nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">part_nicla_vision.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/part_xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">part_xiao_esp32s3.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/part_raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">part_raspi.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/part_shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">part_shared.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">11.1</span> Introduzione</a></li>
  <li><a href="#contesto-storico" id="toc-contesto-storico" class="nav-link" data-scroll-target="#contesto-storico"><span class="header-section-number">11.2</span> Contesto Storico</a>
  <ul>
  <li><a href="#benchmark-delle-prestazioni" id="toc-benchmark-delle-prestazioni" class="nav-link" data-scroll-target="#benchmark-delle-prestazioni"><span class="header-section-number">11.2.1</span> Benchmark delle Prestazioni</a></li>
  <li><a href="#benchmark-energetici" id="toc-benchmark-energetici" class="nav-link" data-scroll-target="#benchmark-energetici"><span class="header-section-number">11.2.2</span> Benchmark Energetici</a></li>
  <li><a href="#benchmark-personalizzati" id="toc-benchmark-personalizzati" class="nav-link" data-scroll-target="#benchmark-personalizzati"><span class="header-section-number">11.2.3</span> Benchmark Personalizzati</a></li>
  <li><a href="#consenso-della-comunità" id="toc-consenso-della-comunità" class="nav-link" data-scroll-target="#consenso-della-comunità"><span class="header-section-number">11.2.4</span> Consenso della Comunità</a></li>
  </ul></li>
  <li><a href="#benchmark-ai-sistema-modello-e-dati" id="toc-benchmark-ai-sistema-modello-e-dati" class="nav-link" data-scroll-target="#benchmark-ai-sistema-modello-e-dati"><span class="header-section-number">11.3</span> Benchmark AI: Sistema, Modello e Dati</a>
  <ul>
  <li><a href="#benchmark-di-sistema" id="toc-benchmark-di-sistema" class="nav-link" data-scroll-target="#benchmark-di-sistema"><span class="header-section-number">11.3.1</span> Benchmark di Sistema</a></li>
  <li><a href="#benchmark-del-modello" id="toc-benchmark-del-modello" class="nav-link" data-scroll-target="#benchmark-del-modello"><span class="header-section-number">11.3.2</span> Benchmark del Modello</a></li>
  <li><a href="#benchmark-dei-dati" id="toc-benchmark-dei-dati" class="nav-link" data-scroll-target="#benchmark-dei-dati"><span class="header-section-number">11.3.3</span> Benchmark dei Dati</a></li>
  </ul></li>
  <li><a href="#benchmarking-di-sistema" id="toc-benchmarking-di-sistema" class="nav-link" data-scroll-target="#benchmarking-di-sistema"><span class="header-section-number">11.4</span> Benchmarking di Sistema</a>
  <ul>
  <li><a href="#granularità" id="toc-granularità" class="nav-link" data-scroll-target="#granularità"><span class="header-section-number">11.4.1</span> Granularità</a>
  <ul class="collapse">
  <li><a href="#micro-benchmark" id="toc-micro-benchmark" class="nav-link" data-scroll-target="#micro-benchmark">Micro Benchmark</a></li>
  <li><a href="#macro-benchmark" id="toc-macro-benchmark" class="nav-link" data-scroll-target="#macro-benchmark">Macro Benchmark</a></li>
  <li><a href="#benchmark-end-to-end" id="toc-benchmark-end-to-end" class="nav-link" data-scroll-target="#benchmark-end-to-end">Benchmark end-to-end</a></li>
  <li><a href="#comprendere-i-compromessi" id="toc-comprendere-i-compromessi" class="nav-link" data-scroll-target="#comprendere-i-compromessi">Comprendere i Compromessi</a></li>
  </ul></li>
  <li><a href="#componenti-dei-benchmark" id="toc-componenti-dei-benchmark" class="nav-link" data-scroll-target="#componenti-dei-benchmark"><span class="header-section-number">11.4.2</span> Componenti dei Benchmark</a>
  <ul class="collapse">
  <li><a href="#dataset-standardizzati" id="toc-dataset-standardizzati" class="nav-link" data-scroll-target="#dataset-standardizzati">Dataset Standardizzati</a></li>
  <li><a href="#attività-predefinite" id="toc-attività-predefinite" class="nav-link" data-scroll-target="#attività-predefinite">Attività Predefinite</a></li>
  <li><a href="#metriche-di-valutazione" id="toc-metriche-di-valutazione" class="nav-link" data-scroll-target="#metriche-di-valutazione">Metriche di Valutazione</a></li>
  <li><a href="#baseline-e-modelli-baseline" id="toc-baseline-e-modelli-baseline" class="nav-link" data-scroll-target="#baseline-e-modelli-baseline">Baseline e Modelli Baseline</a></li>
  <li><a href="#specifiche-hardware-e-software" id="toc-specifiche-hardware-e-software" class="nav-link" data-scroll-target="#specifiche-hardware-e-software">Specifiche Hardware e Software</a></li>
  <li><a href="#condizioni-ambientali" id="toc-condizioni-ambientali" class="nav-link" data-scroll-target="#condizioni-ambientali">Condizioni Ambientali</a></li>
  <li><a href="#regole-di-riproducibilità" id="toc-regole-di-riproducibilità" class="nav-link" data-scroll-target="#regole-di-riproducibilità">Regole di Riproducibilità</a></li>
  <li><a href="#linee-guida-per-linterpretazione-dei-risultati" id="toc-linee-guida-per-linterpretazione-dei-risultati" class="nav-link" data-scroll-target="#linee-guida-per-linterpretazione-dei-risultati">Linee Guida per l’Interpretazione dei Risultati</a></li>
  </ul></li>
  <li><a href="#training-vs.-inferenza" id="toc-training-vs.-inferenza" class="nav-link" data-scroll-target="#training-vs.-inferenza"><span class="header-section-number">11.4.3</span> Training vs.&nbsp;inferenza</a></li>
  <li><a href="#i-benchmark-del-training" id="toc-i-benchmark-del-training" class="nav-link" data-scroll-target="#i-benchmark-del-training"><span class="header-section-number">11.4.4</span> I Benchmark del Training</a>
  <ul class="collapse">
  <li><a href="#scopo" id="toc-scopo" class="nav-link" data-scroll-target="#scopo">Scopo</a></li>
  <li><a href="#metriche" id="toc-metriche" class="nav-link" data-scroll-target="#metriche">Metriche</a></li>
  <li><a href="#i-task" id="toc-i-task" class="nav-link" data-scroll-target="#i-task">I Task</a></li>
  <li><a href="#i-benchmark" id="toc-i-benchmark" class="nav-link" data-scroll-target="#i-benchmark">I Benchmark</a></li>
  <li><a href="#caso-duso-di-esempio" id="toc-caso-duso-di-esempio" class="nav-link" data-scroll-target="#caso-duso-di-esempio">Caso d’Uso di Esempio</a></li>
  </ul></li>
  <li><a href="#benchmark-di-inferenza" id="toc-benchmark-di-inferenza" class="nav-link" data-scroll-target="#benchmark-di-inferenza"><span class="header-section-number">11.4.5</span> Benchmark di Inferenza</a>
  <ul class="collapse">
  <li><a href="#scopo-1" id="toc-scopo-1" class="nav-link" data-scroll-target="#scopo-1">Scopo</a></li>
  <li><a href="#metriche-1" id="toc-metriche-1" class="nav-link" data-scroll-target="#metriche-1">Metriche</a></li>
  <li><a href="#i-task-1" id="toc-i-task-1" class="nav-link" data-scroll-target="#i-task-1">I Task</a></li>
  <li><a href="#i-benchmark-1" id="toc-i-benchmark-1" class="nav-link" data-scroll-target="#i-benchmark-1">I Benchmark</a></li>
  <li><a href="#caso-duso-di-esempio-1" id="toc-caso-duso-di-esempio-1" class="nav-link" data-scroll-target="#caso-duso-di-esempio-1">Caso d’Uso di Esempio</a></li>
  </ul></li>
  <li><a href="#misura-dellefficienza-energetica" id="toc-misura-dellefficienza-energetica" class="nav-link" data-scroll-target="#misura-dellefficienza-energetica"><span class="header-section-number">11.4.6</span> Misura dell’Efficienza Energetica</a></li>
  <li><a href="#esempio-di-benchmark" id="toc-esempio-di-benchmark" class="nav-link" data-scroll-target="#esempio-di-benchmark"><span class="header-section-number">11.4.7</span> Esempio di Benchmark</a>
  <ul class="collapse">
  <li><a href="#task" id="toc-task" class="nav-link" data-scroll-target="#task">Task</a></li>
  <li><a href="#il-dataset" id="toc-il-dataset" class="nav-link" data-scroll-target="#il-dataset">Il Dataset</a></li>
  <li><a href="#modello" id="toc-modello" class="nav-link" data-scroll-target="#modello">Modello</a></li>
  <li><a href="#metriche-2" id="toc-metriche-2" class="nav-link" data-scroll-target="#metriche-2">Metriche</a></li>
  <li><a href="#benchmark-harness" id="toc-benchmark-harness" class="nav-link" data-scroll-target="#benchmark-harness">Benchmark Harness</a></li>
  <li><a href="#la-baseline" id="toc-la-baseline" class="nav-link" data-scroll-target="#la-baseline">La Baseline</a></li>
  </ul></li>
  <li><a href="#sfide-e-limitazioni" id="toc-sfide-e-limitazioni" class="nav-link" data-scroll-target="#sfide-e-limitazioni"><span class="header-section-number">11.4.8</span> Sfide e Limitazioni</a>
  <ul class="collapse">
  <li><a href="#lotteria-hardware" id="toc-lotteria-hardware" class="nav-link" data-scroll-target="#lotteria-hardware">Lotteria Hardware</a></li>
  <li><a href="#benchmark-engineering" id="toc-benchmark-engineering" class="nav-link" data-scroll-target="#benchmark-engineering">Benchmark Engineering</a></li>
  <li><a href="#problema" id="toc-problema" class="nav-link" data-scroll-target="#problema">Problema</a></li>
  <li><a href="#problemi" id="toc-problemi" class="nav-link" data-scroll-target="#problemi">Problemi</a></li>
  <li><a href="#attenuazione" id="toc-attenuazione" class="nav-link" data-scroll-target="#attenuazione">Attenuazione</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#benchmarking-del-modello" id="toc-benchmarking-del-modello" class="nav-link" data-scroll-target="#benchmarking-del-modello"><span class="header-section-number">11.5</span> Benchmarking del Modello</a>
  <ul>
  <li><a href="#contesto-storico-1" id="toc-contesto-storico-1" class="nav-link" data-scroll-target="#contesto-storico-1"><span class="header-section-number">11.5.1</span> Contesto Storico</a>
  <ul class="collapse">
  <li><a href="#mnist-1998" id="toc-mnist-1998" class="nav-link" data-scroll-target="#mnist-1998">MNIST (1998)</a></li>
  <li><a href="#imagenet-2009" id="toc-imagenet-2009" class="nav-link" data-scroll-target="#imagenet-2009">ImageNet (2009)</a></li>
  <li><a href="#coco-2014" id="toc-coco-2014" class="nav-link" data-scroll-target="#coco-2014">COCO (2014)</a></li>
  <li><a href="#gpt-3-2020" id="toc-gpt-3-2020" class="nav-link" data-scroll-target="#gpt-3-2020">GPT-3 (2020)</a></li>
  <li><a href="#presente-e-futuro" id="toc-presente-e-futuro" class="nav-link" data-scroll-target="#presente-e-futuro">Presente e Futuro</a></li>
  </ul></li>
  <li><a href="#metriche-del-modello" id="toc-metriche-del-modello" class="nav-link" data-scroll-target="#metriche-del-modello"><span class="header-section-number">11.5.2</span> Metriche del Modello</a>
  <ul class="collapse">
  <li><a href="#precisione" id="toc-precisione" class="nav-link" data-scroll-target="#precisione">Precisione</a></li>
  <li><a href="#equità" id="toc-equità" class="nav-link" data-scroll-target="#equità">Equità</a></li>
  <li><a href="#complessità" id="toc-complessità" class="nav-link" data-scroll-target="#complessità">Complessità</a></li>
  </ul></li>
  <li><a href="#lezioni-apprese" id="toc-lezioni-apprese" class="nav-link" data-scroll-target="#lezioni-apprese"><span class="header-section-number">11.5.3</span> Lezioni Apprese</a>
  <ul class="collapse">
  <li><a href="#tendenze-emergenti" id="toc-tendenze-emergenti" class="nav-link" data-scroll-target="#tendenze-emergenti">Tendenze Emergenti</a></li>
  </ul></li>
  <li><a href="#limitazioni-e-sfide" id="toc-limitazioni-e-sfide" class="nav-link" data-scroll-target="#limitazioni-e-sfide"><span class="header-section-number">11.5.4</span> Limitazioni e Sfide</a></li>
  </ul></li>
  <li><a href="#benchmarking-dei-dati" id="toc-benchmarking-dei-dati" class="nav-link" data-scroll-target="#benchmarking-dei-dati"><span class="header-section-number">11.6</span> Benchmarking dei Dati</a>
  <ul>
  <li><a href="#limitazioni-dellia-incentrata-sul-modello" id="toc-limitazioni-dellia-incentrata-sul-modello" class="nav-link" data-scroll-target="#limitazioni-dellia-incentrata-sul-modello"><span class="header-section-number">11.6.1</span> Limitazioni dell’IA Incentrata sul Modello</a></li>
  <li><a href="#verso-unintelligenza-artificiale-incentrata-sui-dati" id="toc-verso-unintelligenza-artificiale-incentrata-sui-dati" class="nav-link" data-scroll-target="#verso-unintelligenza-artificiale-incentrata-sui-dati"><span class="header-section-number">11.6.2</span> Verso un’Intelligenza Artificiale Incentrata sui Dati</a></li>
  <li><a href="#benchmarking-dei-dati-1" id="toc-benchmarking-dei-dati-1" class="nav-link" data-scroll-target="#benchmarking-dei-dati-1"><span class="header-section-number">11.6.3</span> Benchmarking dei Dati</a></li>
  <li><a href="#efficienza-dei-dati" id="toc-efficienza-dei-dati" class="nav-link" data-scroll-target="#efficienza-dei-dati"><span class="header-section-number">11.6.4</span> Efficienza dei Dati</a></li>
  </ul></li>
  <li><a href="#la-tripletta" id="toc-la-tripletta" class="nav-link" data-scroll-target="#la-tripletta"><span class="header-section-number">11.7</span> La Tripletta</a></li>
  <li><a href="#benchmark-per-tecnologie-emergenti" id="toc-benchmark-per-tecnologie-emergenti" class="nav-link" data-scroll-target="#benchmark-per-tecnologie-emergenti"><span class="header-section-number">11.8</span> Benchmark per Tecnologie Emergenti</a></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">11.9</span> Conclusione</a></li>
  <li><a href="#sec-benchmarking-ai-resource" id="toc-sec-benchmarking-ai-resource" class="nav-link" data-scroll-target="#sec-benchmarking-ai-resource"><span class="header-section-number">11.10</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-benchmarking_ai" class="quarto-section-identifier"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell’IA</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-benchmarking-ai-resource">Slide</a>, <a href="#sec-benchmarking-ai-resource">Video</a>, <a href="#sec-benchmarking-ai-resource">Esercizi</a>, <a href="#sec-benchmarking-ai-resource">Laboratori</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ai_benchmarking.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Foto di un podio su uno sfondo a tema tecnologico. Su ogni piattaforma del podio ci sono chip AI con design intricati. Il chip in alto ha una medaglia d’oro appesa, il secondo una medaglia d’argento e il terzo una medaglia di bronzo. Sullo sfondo sono ben visibili striscioni con la scritta “AI Olympics”.</em></figcaption>
</figure>
</div>
<p>Il benchmarking è fondamentale per lo sviluppo e la distribuzione di sistemi di machine learning, in particolare applicazioni TinyML. I benchmark consentono agli sviluppatori di misurare e confrontare le prestazioni di diverse architetture di modelli, procedure di training e strategie di distribuzione. Ciò fornisce informazioni chiave su quali approcci funzionano meglio per il problema in questione e sui vincoli dell’ambiente di distribuzione.</p>
<p>Questo capitolo fornirà una panoramica dei benchmark ML più diffusi, le best practice e come utilizzarli per migliorare lo sviluppo del modello e le prestazioni del sistema. Fornire agli sviluppatori gli strumenti e le conoscenze adeguati per effettuare test di benchmark e ottimizzare in modo efficace i propri sistemi, in particolare quelli TinyML.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell’Apprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere lo scopo e gli obiettivi del benchmarking dei sistemi di intelligenza artificiale, tra cui valutazione delle prestazioni, valutazione delle risorse, validazione e altro ancora.</p></li>
<li><p>Scoprire i principali parametri di riferimento, le metriche e le tendenze dei modelli, tra cui accuratezza, equità, complessità, prestazioni ed efficienza energetica.</p></li>
<li><p>Acquisire familiarità con i componenti chiave di un benchmark di intelligenza artificiale, tra cui set di dati, attività, metriche, linee di base, regole di riproducibilità e altro ancora.</p></li>
<li><p>Comprendere la distinzione tra training e inferenza e come ogni fase giustifichi il benchmarking specializzato dei sistemi ML.</p></li>
<li><p>Scoprire i concetti di benchmarking del sistema come produttività, latenza, potenza ed efficienza computazionale.</p></li>
<li><p>Apprezzare l’evoluzione del benchmarking del modello dall’accuratezza a metriche più olistiche come correttezza, robustezza e applicabilità nel mondo reale.</p></li>
<li><p>Riconoscere il ruolo crescente del benchmarking dei dati nella valutazione di problemi come bias, rumore, equilibrio e diversità.</p></li>
<li><p>Comprendere i limiti della valutazione di modelli, dati e sistemi in isolamento e l’esigenza emergente di benchmarking integrato.</p></li>
</ul>
</div>
</div>
<section id="introduzione" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="introduzione"><span class="header-section-number">11.1</span> Introduzione</h2>
<p>Il benchmarking fornisce le misure essenziali necessarie per guidare il progresso dell’apprendimento automatico e comprendere veramente le prestazioni del sistema. Come ha affermato il fisico Lord Kelvin, “Misurare è conoscere”. I benchmark ci consentono di conoscere quantitativamente le capacità di diversi modelli, software e hardware. Consentono agli sviluppatori di ML di misurare il tempo di inferenza, l’utilizzo della memoria, il consumo energetico e altre metriche che caratterizzano un sistema. Inoltre, i benchmark creano processi standardizzati per la misurazione, consentendo confronti equi tra diverse soluzioni.</p>
<p>Quando i benchmark vengono mantenuti nel tempo, diventano fondamentali per catturare i progressi attraverso generazioni di algoritmi, set di dati e hardware. I modelli e le tecniche che stabiliscono nuovi record sui benchmark di ML da un anno all’altro dimostrano miglioramenti tangibili in ciò che è possibile per l’apprendimento automatico “on-device”. Utilizzando i benchmark per misurare, i professionisti di ML possono conoscere le capacità reali dei loro sistemi e avere la certezza che ogni passaggio rifletta un progresso autentico verso lo stato dell’arte.</p>
<p>Il benchmarking ha diversi obiettivi e scopi importanti che guidano la sua implementazione per i sistemi di apprendimento automatico.</p>
<ul>
<li><p><strong>Valutazione delle prestazioni.</strong> Ciò comporta la valutazione di parametri chiave come la velocità, l’accuratezza e l’efficienza di un dato modello. Ad esempio, in un contesto TinyML, è fondamentale confrontare la rapidità con cui un assistente vocale può riconoscere i comandi, poiché ciò valuta le prestazioni in tempo reale.</p></li>
<li><p><strong>Valutazione della potenza.</strong> Valutare la potenza assorbita da un carico di lavoro insieme alle sue prestazioni equivale alla sua efficienza energetica. Poiché l’impatto ambientale dell’elaborazione ML continua a crescere, il benchmarking dell’energia può consentirci di ottimizzare meglio i sistemi per la sostenibilità.</p></li>
<li><p><strong>Valutazione delle risorse.</strong> Ciò significa valutare l’impatto del modello sulle risorse critiche del sistema, tra cui durata della batteria, utilizzo della memoria e sovraccarico computazionale. Un esempio rilevante è il confronto del consumo della batteria di due diversi algoritmi di riconoscimento delle immagini in esecuzione su un dispositivo indossabile.</p></li>
<li><p><strong>Validazione e verifica.</strong> Il benchmarking aiuta a garantire che il sistema funzioni correttamente e soddisfi i requisiti specificati. Un modo è quello di controllare l’accuratezza di un algoritmo, come un cardiofrequenzimetro su uno smartwatch, rispetto alle letture di apparecchiature di livello medico come forma di validazione clinica.</p></li>
<li><p><strong>Analisi competitiva.</strong> Ciò consente di confrontare le soluzioni con le offerte concorrenti sul mercato. Ad esempio, il benchmarking di un modello personalizzato di rilevamento di oggetti rispetto ai benchmark TinyML comuni come MobileNet e Tiny-YOLO.</p></li>
<li><p><strong>Credibilità.</strong> I benchmark accurati sostengono la credibilità delle soluzioni AI e delle organizzazioni che le sviluppano. Dimostrano un impegno verso trasparenza, onestà e qualità, essenziali per creare fiducia con utenti e stakeholder.</p></li>
<li><p><strong>Regolamentazione e Standardizzazione</strong>. Man mano che il settore dell’AI continua a crescere, cresce anche la necessità di regolamentazione e standardizzazione per garantire che le soluzioni AI siano sicure, etiche ed efficaci. I benchmark accurati e affidabili sono essenziali per questo quadro normativo, poiché forniscono i dati e le prove necessari per valutare la conformità con gli standard del settore e i requisiti legali.</p></li>
</ul>
<p>Questo capitolo tratterà i 3 tipi di benchmark AI, le metriche standard, gli strumenti e le tecniche che i progettisti utilizzano per ottimizzare i loro sistemi e le sfide e le tendenze nel benchmarking.</p>
</section>
<section id="contesto-storico" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="contesto-storico"><span class="header-section-number">11.2</span> Contesto Storico</h2>
<section id="benchmark-delle-prestazioni" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="benchmark-delle-prestazioni"><span class="header-section-number">11.2.1</span> Benchmark delle Prestazioni</h3>
<p>L’evoluzione dei benchmark nell’informatica illustra vividamente l’incessante ricerca dell’eccellenza e dell’innovazione da parte del settore. Nei primi giorni dell’informatica, negli anni ’60 e ’70, i benchmark erano rudimentali e progettati per i mainframe. Ad esempio, il <a href="https://en.wikipedia.org/wiki/Whetstone_(benchmark)">benchmark Whetstone</a>, che prende il nome dal compilatore Whetstone ALGOL, è stato uno dei primi test standardizzati per misurare le prestazioni aritmetiche in virgola mobile di una CPU. Questi benchmark pionieristici hanno spinto i produttori a perfezionare le loro architetture e algoritmi per ottenere punteggi di benchmark migliori.</p>
<p>Gli anni ’80 hanno segnato un cambiamento significativo con l’ascesa dei personal computer. Mentre aziende come IBM, Apple e Commodore gareggiavano per quote di mercato, i benchmark sono diventati strumenti essenziali per consentire una concorrenza leale. I <a href="https://www.spec.org/cpu/">benchmark CPU SPEC</a>, introdotti dalla <a href="https://www.spec.org/">System Performance Evaluation Cooperative (SPEC)</a>, hanno stabilito test standardizzati che consentono confronti oggettivi tra diverse macchine. Questa standardizzazione ha creato un ambiente competitivo, spingendo i produttori di chip e i creatori di sistemi a migliorare continuamente le loro offerte hardware e software.</p>
<p>Gli anni ’90 hanno portato l’era delle applicazioni e dei videogiochi “graphics-intensive”. La necessità di benchmark per valutare le prestazioni delle schede grafiche ha portato alla creazione di <a href="https://www.3dmark.com/">3DMark</a> da parte di Futuremark. Mentre i giocatori e i professionisti cercavano schede grafiche ad alte prestazioni, aziende come NVIDIA e AMD sono state spinte a una rapida innovazione, portando a importanti progressi nella tecnologia GPU come gli shader programmabili.</p>
<p>Gli anni 2000 hanno visto un’impennata di telefoni cellulari e dispositivi portatili come i tablet. Con la portabilità è arrivata la sfida di bilanciare prestazioni e consumo energetico. Benchmark come <a href="https://bapco.com/products/mobilemark-2014/">MobileMark</a> di BAPCo hanno valutato velocità e durata della batteria. Ciò ha spinto le aziende a sviluppare System-on-Chip (SOC) più efficienti dal punto di vista energetico, portando all’emergere di architetture come ARM che hanno dato priorità all’efficienza energetica.</p>
<p>L’attenzione dell’ultimo decennio si è spostata verso il cloud computing, i big data e l’intelligenza artificiale. I provider di servizi cloud come Amazon Web Services e Google Cloud competono su prestazioni, scalabilità e convenienza. I benchmark specifici del cloud come <a href="http://cloudsuite.ch/">CloudSuite</a> sono diventati essenziali, spingendo i provider a ottimizzare la propria infrastruttura per servizi migliori.</p>
</section>
<section id="benchmark-energetici" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="benchmark-energetici"><span class="header-section-number">11.2.2</span> Benchmark Energetici</h3>
<p>Il consumo energetico e le preoccupazioni ambientali hanno acquisito importanza negli ultimi anni, rendendo il benchmarking energetico sempre più importante nel settore. Questo cambiamento è iniziato a metà degli anni 2000, quando i processori e i sistemi hanno iniziato a raggiungere i limiti di raffreddamento e la scalabilità è diventata un aspetto cruciale della costruzione di sistemi su larga scala grazie ai progressi di Internet. Da allora, le considerazioni energetiche si sono espanse fino a comprendere tutte le aree dell’informatica, dai dispositivi personali ai data center su larga scala.</p>
<p>Il benchmarking energetico mira a misurare l’efficienza energetica dei sistemi informatici, valutando le prestazioni in relazione al consumo energetico. Ciò è fondamentale per diversi motivi:</p>
<ul>
<li><strong>Impatto ambientale:</strong> Con la crescente impronta di carbonio del settore tecnologico, c’è un’urgente necessità di ridurre il consumo energetico.</li>
<li><strong>Costi operativi:</strong> Le spese energetiche costituiscono una parte significativa dei costi operativi del data center.</li>
<li><strong>Longevità del dispositivo:</strong> Per i dispositivi mobili, l’efficienza energetica ha un impatto diretto sulla durata della batteria e sull’esperienza utente.</li>
</ul>
<p>In questo ambito sono emersi diversi benchmark chiave:</p>
<ul>
<li><strong>SPEC Power:</strong> Introdotto nel 2007, <a href="https://www.spec.org/power/">SPEC Power</a> è stato uno dei primi benchmark standard del settore per la valutazione delle caratteristiche di potenza e prestazioni dei server.</li>
<li><strong>Green500:</strong> L’elenco <a href="https://top500.org/lists/green500/">Green500</a> classifica i supercomputer in base all’efficienza energetica, integrando l’elenco TOP500 incentrato sulle prestazioni.</li>
<li><strong>Energy Star:</strong> Pur non essendo un benchmark in sé, il programma di certificazione <a href="https://www.energystar.gov/products/computers">ENERGY STAR for Computers</a> ha spinto i produttori a migliorare l’efficienza energetica dell’elettronica di consumo.</li>
</ul>
<p>Il benchmarking energetico affronta sfide uniche, come la contabilizzazione di diversi carichi di lavoro e configurazioni di sistema e la misura accurata del consumo energetico su una gamma di hardware che varia da microWatt a megawatt nel consumo energetico. Man mano che l’IA e l’edge computing continuano a crescere, è probabile che il benchmarking energetico diventi ancora più critico, guidando lo sviluppo di ottimizzazioni hardware e software AI specializzate ed efficienti dal punto di vista energetico.</p>
</section>
<section id="benchmark-personalizzati" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="benchmark-personalizzati"><span class="header-section-number">11.2.3</span> Benchmark Personalizzati</h3>
<p>Oltre ai benchmark standard del settore, ci sono benchmark personalizzati specificamente progettati per soddisfare i requisiti unici di una particolare applicazione o attività. Sono personalizzati in base alle esigenze specifiche dell’utente o dello sviluppatore, assicurando che le metriche delle prestazioni siano direttamente pertinenti all’uso previsto del modello o del sistema di intelligenza artificiale. I benchmark personalizzati possono essere creati da singole organizzazioni, ricercatori o sviluppatori e sono spesso utilizzati insieme ai benchmark standard del settore per fornire una valutazione completa delle prestazioni dell’intelligenza artificiale.</p>
<p>Ad esempio, un ospedale potrebbe sviluppare un benchmark per valutare un modello di intelligenza artificiale per prevedere la riammissione dei pazienti. Questo benchmark incorporerebbe metriche pertinenti alla popolazione di pazienti dell’ospedale, come dati demografici, anamnesi e fattori sociali. Allo stesso modo, il benchmark di rilevamento delle frodi di un istituto finanziario potrebbe concentrarsi sull’identificazione accurata delle transazioni fraudolente riducendo al minimo i falsi positivi. Nel settore automobilistico, un benchmark di veicoli autonomi potrebbe dare priorità alle prestazioni in diverse condizioni, alla risposta agli ostacoli e alla sicurezza. I rivenditori potrebbero confrontare i sistemi di raccomandazione utilizzando il tasso di clic, il tasso di conversione e la soddisfazione del cliente. Le aziende manifatturiere potrebbero confrontare i sistemi di controllo qualità in base all’identificazione dei difetti, all’efficienza e alla riduzione degli sprechi. In ogni settore, i benchmark personalizzati forniscono alle organizzazioni criteri di valutazione su misura per le loro esigenze e il loro contesto unici. Ciò consente una valutazione più significativa di quanto i sistemi di intelligenza artificiale soddisfino i requisiti.</p>
<p>Il vantaggio dei benchmark personalizzati risiede nella loro flessibilità e pertinenza. Possono essere progettati per testare aspetti specifici delle prestazioni critici per il successo della soluzione di intelligenza artificiale nella sua applicazione prevista. Ciò consente una valutazione più mirata e accurata delle capacità del modello o del sistema di intelligenza artificiale. I benchmark personalizzati forniscono anche informazioni preziose sulle prestazioni delle soluzioni di intelligenza artificiale in scenari reali, il che può essere cruciale per identificare potenziali problemi e aree di miglioramento.</p>
<p>Nell’intelligenza artificiale, i benchmark svolgono un ruolo cruciale nel guidare il progresso e l’innovazione. Sebbene i benchmark siano stati a lungo utilizzati nell’informatica, la loro applicazione all’apprendimento automatico è relativamente recente. I benchmark incentrati sull’intelligenza artificiale forniscono metriche standardizzate per valutare e confrontare le prestazioni di diversi algoritmi, architetture di modelli e piattaforme hardware.</p>
</section>
<section id="consenso-della-comunità" class="level3" data-number="11.2.4">
<h3 data-number="11.2.4" class="anchored" data-anchor-id="consenso-della-comunità"><span class="header-section-number">11.2.4</span> Consenso della Comunità</h3>
<p>Una prerogativa fondamentale affinché un benchmark abbia un impatto è che deve riflettere le priorità e i valori condivisi della più ampia comunità di ricerca. I benchmark progettati in modo isolato rischiano di non ottenere accettazione se trascurano metriche chiave considerate importanti dai gruppi leader. Attraverso uno sviluppo collaborativo con la partecipazione aperta di laboratori accademici, aziende e altri stakeholder, i benchmark possono incorporare un contributo collettivo su capacità critiche che vale la pena misurare. Ciò aiuta a garantire che i benchmark valutino aspetti che la comunità concorda siano essenziali per far progredire il campo. Il processo di raggiungimento dell’allineamento su attività e metriche supporta di per sé la convergenza su ciò che conta di più.</p>
<p>Inoltre, i benchmark pubblicati con ampia co-paternità da istituzioni rispettate hanno autorità e validità che convincono la comunità ad adottarli come standard affidabili. I benchmark percepiti come distorti da particolari interessi aziendali o istituzionali generano scetticismo. Anche il coinvolgimento continuo della comunità attraverso workshop e sfide è fondamentale dopo la versione iniziale, ed è ciò che, ad esempio, ha portato al successo di ImageNet. Col progredire della ricerca, la partecipazione collettiva consente un continuo perfezionamento ed espansione dei benchmark nel tempo.</p>
<p>Infine, i benchmark sviluppati dalla comunità rilasciati con accesso aperto accelerano l’adozione e l’implementazione coerente. Abbiamo condiviso codice open source, documentazione, modelli e infrastrutture per ridurre le barriere che impediscono ai gruppi di confrontare le soluzioni su un piano di parità utilizzando implementazioni standardizzate. Questa coerenza è fondamentale per confronti equi. Senza coordinamento, laboratori e aziende potrebbero implementare i benchmark in modo diverso, riducendo la riproducibilità dei risultati.</p>
<p>Il consenso della comunità conferisce ai benchmark una rilevanza duratura, mentre la frammentazione confonde. Attraverso lo sviluppo collaborativo e un funzionamento trasparente, i benchmark possono diventare standard autorevoli per monitorare i progressi. Molti dei benchmark di cui parliamo in questo capitolo sono stati sviluppati e creati dalla comunità, per la comunità, ed è questo che alla fine ha portato al loro successo.</p>
</section>
</section>
<section id="benchmark-ai-sistema-modello-e-dati" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="benchmark-ai-sistema-modello-e-dati"><span class="header-section-number">11.3</span> Benchmark AI: Sistema, Modello e Dati</h2>
<p>La necessità di un benchmarking completo diventa fondamentale man mano che i sistemi AI diventano più complessi e onnipresenti. In questo contesto, i benchmark sono spesso classificati in tre categorie principali: Hardware, Modello e Dati. Analizziamo perché ognuno di questi gruppi è essenziale e il significato della valutazione dell’AI da queste tre dimensioni distinte:</p>
<section id="benchmark-di-sistema" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="benchmark-di-sistema"><span class="header-section-number">11.3.1</span> Benchmark di Sistema</h3>
<p>I calcoli AI, in particolare quelli nel deep learning, richiedono molte risorse. L’hardware su cui vengono eseguiti questi calcoli svolge un ruolo importante nel determinare la velocità, l’efficienza e la scalabilità delle soluzioni AI. Di conseguenza, i benchmark hardware aiutano a valutare le prestazioni di CPU, GPU, TPU e altri acceleratori nelle attività AI. Comprendendone le prestazioni, gli sviluppatori possono scegliere quali piattaforme hardware si adattano meglio a specifiche applicazioni AI. Inoltre, i produttori di hardware utilizzano questi benchmark per identificare aree di miglioramento, guidando l’innovazione nei progetti di chip specifici per AI.</p>
</section>
<section id="benchmark-del-modello" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="benchmark-del-modello"><span class="header-section-number">11.3.2</span> Benchmark del Modello</h3>
<p>L’architettura, le dimensioni e la complessità dei modelli AI variano notevolmente. Modelli diversi hanno diverse esigenze di calcolo e offrono diversi livelli di accuratezza ed efficienza. I benchmark dei modelli aiutano a valutare le prestazioni di varie architetture AI su attività standardizzate. Forniscono informazioni sulla velocità, l’accuratezza e le richieste di risorse di diversi modelli. Eseguendo il benchmarking dei modelli, i ricercatori possono identificare le architetture più performanti per attività specifiche, guidando la comunità AI verso soluzioni più efficienti ed efficaci. Inoltre, questi benchmark aiutano a monitorare i progressi della ricerca sull’intelligenza artificiale, mostrando i progressi nella progettazione e nell’ottimizzazione dei modelli.</p>
</section>
<section id="benchmark-dei-dati" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="benchmark-dei-dati"><span class="header-section-number">11.3.3</span> Benchmark dei Dati</h3>
<p>L’intelligenza artificiale, in particolare l’apprendimento automatico, è intrinsecamente basata sui dati. La qualità, le dimensioni e la diversità dei dati influenzano l’efficacia dell’addestramento e la capacità di generalizzazione dei modelli di intelligenza artificiale. I benchmark dei dati si concentrano sui set di dati utilizzati nell’addestramento e nella valutazione dell’intelligenza artificiale. Forniscono set di dati standardizzati che la comunità può utilizzare per addestrare e testare i modelli, garantendo parità di condizioni per i confronti. Inoltre, questi benchmark evidenziano le sfide relative alla qualità, alla diversità e alla rappresentazione dei dati, spingendo la comunità ad affrontare bias e lacune nei dati di addestramento dell’intelligenza artificiale. Comprendendo i benchmark dei dati, i ricercatori possono anche valutare come i modelli potrebbero comportarsi in scenari reali, garantendo robustezza e affidabilità.</p>
<p>Nelle restanti sezioni, discuteremo ciascuno di questi tipi di benchmark. L’attenzione sarà rivolta a un’esplorazione approfondita dei benchmark di sistema, poiché sono fondamentali per comprendere e migliorare le prestazioni del sistema di apprendimento automatico. Parleremo brevemente dei benchmark dei modelli e dei dati per una prospettiva completa, ma l’enfasi e la maggior parte del contenuto saranno dedicati ai benchmark di sistema.</p>
</section>
</section>
<section id="benchmarking-di-sistema" class="level2 page-columns page-full" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="benchmarking-di-sistema"><span class="header-section-number">11.4</span> Benchmarking di Sistema</h2>
<section id="granularità" class="level3 page-columns page-full" data-number="11.4.1">
<h3 data-number="11.4.1" class="anchored" data-anchor-id="granularità"><span class="header-section-number">11.4.1</span> Granularità</h3>
<p>Il benchmarking del sistema di apprendimento automatico fornisce un approccio strutturato e sistematico per valutare le prestazioni di un sistema in diverse dimensioni. Data la complessità dei sistemi ML, possiamo analizzare le loro prestazioni attraverso diversi livelli di granularità e ottenere una visione completa dell’efficienza del sistema, identificare potenziali colli di bottiglia e individuare le aree di miglioramento. A tal fine, nel corso degli anni si sono evoluti vari tipi di benchmark che continuano a persistere.</p>
<p><a href="#fig-granularity" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-granularity</span></a> illustra i diversi livelli di granularità di un sistema ML. A livello di applicazione, i benchmark end-to-end valutano le prestazioni complessive del sistema, considerando fattori come la pre-elaborazione dei dati, l’addestramento del modello e l’inferenza. Mentre a livello di modello, i benchmark si concentrano sulla valutazione dell’efficienza e dell’accuratezza di modelli specifici. Ciò include la valutazione di quanto bene i modelli si generalizzano a nuovi dati e della loro efficienza computazionale durante l’addestramento e l’inferenza. Inoltre, il benchmarking può estendersi all’infrastruttura hardware e software, esaminando le prestazioni di singoli componenti come GPU o TPU.</p>
<div id="fig-granularity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/end2end.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.1: Granularità del sistema ML.
</figcaption>
</figure>
</div>
<section id="micro-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="micro-benchmark">Micro Benchmark</h4>
<p>I micro-benchmark nell’IA sono specializzati e valutano componenti distinti o operazioni specifiche all’interno di un processo di apprendimento automatico più ampio. Questi benchmark si concentrano su singole attività, offrendo approfondimenti sulle richieste computazionali di un particolare layer di rete neurale, l’efficienza di un’unica tecnica di ottimizzazione o la produttività di una specifica funzione di attivazione. Ad esempio, i professionisti potrebbero utilizzare i micro-benchmark per misurare il tempo di calcolo richiesto da un layer convoluzionale in un modello di deep learning o per valutare la velocità di preelaborazione che alimenta i dati nel modello. Tali valutazioni granulari sono fondamentali per la messa a punto e l’ottimizzazione di aspetti discreti dei modelli di IA, assicurando che ogni componente funzioni al massimo del suo potenziale.</p>
<p>Questi tipi di microbenchmark includono lo zoom su operazioni o componenti molto specifiche della pipeline AI, come le seguenti:</p>
<ul>
<li><p><strong>Operazioni Tensoriali:</strong> Librerie come <a href="https://developer.nvidia.com/cudnn">cuDNN</a> (di NVIDIA) spesso hanno benchmark per misurare le prestazioni di singole operazioni tensoriali, come convoluzioni o moltiplicazioni di matrici, che sono fondamentali per i calcoli del deep learning.</p></li>
<li><p><strong>Funzioni di Attivazione:</strong> Benchmark che misurano la velocità e l’efficienza di varie funzioni di attivazione come ReLU, Sigmoid o Tanh in isolamento.</p></li>
<li><p><strong>Benchmark di Layer:</strong> Valutazioni dell’efficienza computazionale di distinti layer di rete neurale, come blocchi LSTM o Transformer, quando si opera su dimensioni di input standardizzate.</p></li>
</ul>
<p>Esempio: <a href="https://github.com/baidu-research/DeepBench">DeepBench</a>, introdotto da Baidu, è un buon esempio di qualcosa che valuta quanto sopra. DeepBench valuta le prestazioni delle operazioni di base nei modelli di deep learning, fornendo informazioni su come diverse piattaforme hardware gestiscono l’addestramento e l’inferenza delle reti neurali.</p>
<div id="exr-cuda" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;11.1: Benchmarking di Sistema - Operazioni Tensoriali
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Ci si è mai chiesto come mai i filtri immagine diventano così veloci? Librerie speciali come cuDNN potenziano quei calcoli su determinati hardware. In questo Colab, useremo cuDNN con PyTorch per velocizzare il filtraggio delle immagini. Lo si consideri un piccolo benchmark, che mostra come il software giusto può sbloccare la potenza della GPU!</p>
<p><a href="https://colab.research.google.com/github/RyanHartzell/cudnn-image-filtering/blob/master/notebooks/CuDNN%20Image%20Filtering%20Tutorial%20Using%20PyTorch.ipynb#scrollTo=1sWeXdYsATrr"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="macro-benchmark" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="macro-benchmark">Macro Benchmark</h4>
<p>I macro benchmark forniscono una visione olistica, valutando le prestazioni end-to-end di interi modelli di apprendimento automatico o sistemi di intelligenza artificiale completi. Invece di concentrarsi sulle singole operazioni, i macro benchmark valutano l’efficacia collettiva dei modelli in scenari o attività del mondo reale. Ad esempio, un macro benchmark potrebbe valutare le prestazioni complete di un modello di apprendimento profondo che esegue la classificazione delle immagini su un set di dati come <a href="https://www.image-net.org/">ImageNet</a>. Ciò include la misura dell’accuratezza, della velocità di calcolo e del consumo di risorse. Allo stesso modo, si potrebbero misurare il tempo e le risorse cumulativi necessari per addestrare un modello di elaborazione del linguaggio naturale su corpora di testo estesi o valutare le prestazioni di un intero sistema di raccomandazione, dall’inserimento dei dati agli output finali specifici dell’utente.</p>
<p>Esempi: Questi benchmark valutano il modello di intelligenza artificiale:</p>
<ul>
<li><p><a href="https://github.com/mlcommons/inference">MLPerf Inference</a> <span class="citation" data-cites="reddi2020mlperf">(<a href="#ref-reddi2020mlperf" role="doc-biblioref">Reddi et al. 2020</a>)</span>: Un set di benchmark standard per misurare le prestazioni di software e hardware di apprendimento automatico. MLPerf ha una suite di benchmark dedicati per scale specifiche, come <a href="https://github.com/mlcommons/mobile_app_open">MLPerf Mobile</a> per dispositivi di classe mobile e <a href="https://github.com/mlcommons/tiny">MLPerf Tiny</a>, che si concentra su microcontrollori e altri dispositivi con risorse limitate.</p></li>
<li><p><a href="https://github.com/eembc/mlmark">MLMark di EEMBC</a>: Una suite di benchmarking per valutare le prestazioni e l’efficienza energetica dei dispositivi embedded che eseguono carichi di lavoro di apprendimento automatico. Questo benchmark fornisce informazioni su come diverse piattaforme hardware gestiscono attività come il riconoscimento delle immagini o l’elaborazione audio.</p></li>
<li><p><a href="https://ai-benchmark.com/">AI-Benchmark</a> <span class="citation" data-cites="ignatov2018ai">(<a href="#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2019</a>)</span>: Uno strumento di benchmarking progettato per dispositivi Android, valuta le prestazioni delle attività di intelligenza artificiale sui dispositivi mobili, comprendendo vari scenari del mondo reale come il riconoscimento delle immagini, l’analisi dei volti e il riconoscimento ottico dei caratteri.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-reddi2020mlperf" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. <span>«<span>MLPerf</span> Inference Benchmark»</span>. In <em>2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>, 446–59. IEEE; IEEE. <a href="https://doi.org/10.1109/isca45697.2020.00045">https://doi.org/10.1109/isca45697.2020.00045</a>.
</div><div id="ref-ignatov2018ai" class="csl-entry" role="listitem">
Ignatov, Andrey, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, e Luc Van Gool. 2019. <span>«<span>AI</span> Benchmark: <span>All</span> About Deep Learning on Smartphones in 2019»</span>. In <em>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em>, 0–0. IEEE. <a href="https://doi.org/10.1109/iccvw.2019.00447">https://doi.org/10.1109/iccvw.2019.00447</a>.
</div></div></section>
<section id="benchmark-end-to-end" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-end-to-end">Benchmark end-to-end</h4>
<p>I benchmark end-to-end forniscono una valutazione completa che si estende oltre i confini del modello di IA stesso. Invece di concentrarsi esclusivamente sull’efficienza o l’accuratezza computazionale di un modello di apprendimento automatico, questi benchmark comprendono l’intera pipeline di un sistema di IA. Ciò include la pre-elaborazione iniziale dei dati, le prestazioni del modello principale, la post-elaborazione degli output del modello e altri componenti integrali come l’archiviazione e le interazioni di rete.</p>
<p>La pre-elaborazione dei dati è la prima fase in molti sistemi di IA, trasformando i dati grezzi in un formato adatto per l’addestramento o l’inferenza del modello. L’efficienza, la scalabilità e l’accuratezza di queste fasi di pre-elaborazione sono vitali per le prestazioni complessive del sistema. I benchmark end-to-end valutano questa fase, assicurando che la pulizia dei dati, la normalizzazione, l’aumento o qualsiasi altro processo di trasformazione non diventi un collo di bottiglia.</p>
<p>Anche la fase di post-elaborazione è al centro dell’attenzione. Ciò comporta l’interpretazione degli output grezzi del modello, eventualmente la conversione dei punteggi in categorie significative, il filtraggio dei risultati o persino l’integrazione con altri sistemi. Nelle applicazioni del mondo reale, questa fase è fondamentale per fornire informazioni fruibili e i benchmark end-to-end ne garantiscono l’efficienza e l’efficacia.</p>
<p>Oltre alle operazioni di base dell’IA, altri componenti del sistema sono importanti per le prestazioni complessive e l’esperienza utente. Le soluzioni di archiviazione, basate su cloud, on-premise o ibride, possono avere un impatto significativo sui tempi di recupero e archiviazione dei dati, in particolare con vasti set di dati di IA. Allo stesso modo, le interazioni di rete, vitali per le soluzioni di IA basate su cloud o per i sistemi distribuiti, possono diventare colli di bottiglia delle prestazioni se non ottimizzate. I benchmark end-to-end valutano in modo olistico questi componenti, assicurando che l’intero sistema funzioni senza problemi, dal recupero dei dati alla consegna dell’output finale.</p>
<p>Ad oggi, non esistono benchmark end-to-end pubblici che tengano conto del ruolo dell’archiviazione dei dati, della rete e delle prestazioni di elaborazione. Si può sostenere che MLPerf Training and Inference si avvicini all’idea di un benchmark end-to-end, ma si concentrano esclusivamente sulle prestazioni del modello ML e non rappresentano scenari di distribuzione nel mondo reale di come i modelli vengono utilizzati sul campo. Tuttavia, forniscono un segnale molto utile che aiuta a valutare le prestazioni del sistema AI.</p>
<p>Data la specificità intrinseca del benchmarking end-to-end, viene in genere eseguito internamente in un’azienda “strumentando” [inserendo punti di controllo] distribuzioni di produzione reali di AI. Ciò consente agli ingegneri di avere una comprensione e una ripartizione realistiche delle prestazioni, ma data la sensibilità e la specificità delle informazioni, raramente vengono segnalate all’esterno dell’azienda.</p>
</section>
<section id="comprendere-i-compromessi" class="level4">
<h4 class="anchored" data-anchor-id="comprendere-i-compromessi">Comprendere i Compromessi</h4>
<p>Diversi problemi sorgono nelle diverse fasi di un sistema di intelligenza artificiale. I micro-benchmark aiutano a mettere a punto i singoli componenti, i macro-benchmark aiutano a perfezionare le architetture o gli algoritmi del modello e i benchmark end-to-end guidano l’ottimizzazione dell’intero flusso di lavoro. Comprendendo dove si trova un problema, gli sviluppatori possono applicare ottimizzazioni mirate.</p>
<p>Inoltre, mentre i singoli componenti di un sistema di intelligenza artificiale potrebbero funzionare in modo ottimale in isolamento, possono emergere colli di bottiglia quando interagiscono. I benchmark end-to-end, in particolare, sono fondamentali per garantire che l’intero sistema, quando funziona collettivamente, soddisfi gli standard di prestazioni ed efficienza desiderati.</p>
<p>Infine, le organizzazioni possono prendere decisioni informate su dove allocare le risorse individuando colli di bottiglia o inefficienze nelle prestazioni. Ad esempio, se i micro-benchmark rivelano inefficienze in specifiche operazioni tensoriali, gli investimenti possono essere indirizzati verso acceleratori hardware specializzati. Al contrario, se i benchmark end-to-end indicano problemi di recupero dei dati, gli investimenti potrebbero essere incanalati verso soluzioni di archiviazione migliori.</p>
</section>
</section>
<section id="componenti-dei-benchmark" class="level3" data-number="11.4.2">
<h3 data-number="11.4.2" class="anchored" data-anchor-id="componenti-dei-benchmark"><span class="header-section-number">11.4.2</span> Componenti dei Benchmark</h3>
<p>In sostanza, un benchmark AI è più di un semplice test o punteggio; è un framework di valutazione completo. Per comprenderlo in modo approfondito, analizziamo i componenti tipici che compongono un benchmark AI.</p>
<section id="dataset-standardizzati" class="level4">
<h4 class="anchored" data-anchor-id="dataset-standardizzati">Dataset Standardizzati</h4>
<p>I set di dati fungono da base per la maggior parte dei benchmark AI. Forniscono un set di dati coerente su cui i modelli vengono addestrati e valutati, garantendo parità di condizioni per i confronti.</p>
<p>Esempio: ImageNet, un set di dati su larga scala contenente milioni di immagini etichettate che abbracciano migliaia di categorie, è uno standard di benchmarking popolare per le attività di classificazione delle immagini.</p>
</section>
<section id="attività-predefinite" class="level4">
<h4 class="anchored" data-anchor-id="attività-predefinite">Attività Predefinite</h4>
<p>Un benchmark dovrebbe avere un obiettivo o un compito chiaro che i modelli mirano a raggiungere. Questo compito definisce il problema che il sistema AI sta cercando di risolvere.</p>
<p>Esempio: I compiti per i benchmark di elaborazione del linguaggio naturale potrebbero includere analisi del “sentiment”, riconoscimento di entità denominate o traduzione automatica.</p>
</section>
<section id="metriche-di-valutazione" class="level4">
<h4 class="anchored" data-anchor-id="metriche-di-valutazione">Metriche di Valutazione</h4>
<p>Una volta definito un task, i benchmark richiedono parametri per quantificare le prestazioni. Questi parametri offrono misure oggettive per confrontare diversi modelli o sistemi. Nei task di classificazione, parametri come accuratezza, precisione, richiamo e <a href="https://en.wikipedia.org/wiki/F-score">punteggio F1</a> sono comunemente utilizzati. Errori quadratici medi o assoluti potrebbero essere utilizzati per i task di regressione. Possiamo anche misurare la potenza consumata dall’esecuzione del benchmark per calcolare l’efficienza energetica.</p>
</section>
<section id="baseline-e-modelli-baseline" class="level4">
<h4 class="anchored" data-anchor-id="baseline-e-modelli-baseline">Baseline e Modelli Baseline</h4>
<p>I benchmark spesso includono modelli “baseline” o implementazioni di riferimento. Di solito servono come punti di partenza o standard minimi di prestazione per confrontare nuovi modelli o nuove tecniche. I modelli “baseline” aiutano i ricercatori a misurare l’efficacia di nuovi algoritmi.</p>
<p>Nelle suite di benchmark, modelli semplici come la regressione lineare o le reti neurali di base sono spesso le baseline comuni. Queste forniscono un contesto quando si valutano modelli più complessi. Confrontando questi modelli più semplici, i ricercatori possono quantificare i miglioramenti derivanti da approcci avanzati.</p>
<p>Le metriche delle prestazioni variano in base all’attività, ma ecco alcuni esempi:</p>
<ul>
<li>Le attività di classificazione utilizzano metriche come accuratezza, precisione, richiamo e punteggio F1.</li>
<li>Le attività di regressione utilizzano spesso l’errore quadratico medio o l’errore assoluto medio.</li>
</ul>
</section>
<section id="specifiche-hardware-e-software" class="level4">
<h4 class="anchored" data-anchor-id="specifiche-hardware-e-software">Specifiche Hardware e Software</h4>
<p>Data la variabilità introdotta da diverse configurazioni hardware e software, i benchmark spesso specificano o documentano gli ambienti hardware e software in cui vengono condotti i test.</p>
<p>Esempio: Un benchmark AI potrebbe indicare che le valutazioni sono state condotte su una GPU NVIDIA Tesla V100 utilizzando TensorFlow v2.4.</p>
</section>
<section id="condizioni-ambientali" class="level4">
<h4 class="anchored" data-anchor-id="condizioni-ambientali">Condizioni Ambientali</h4>
<p>Poiché fattori esterni possono influenzare i risultati del benchmark, è essenziale controllare o documentare condizioni come temperatura, fonte di alimentazione o processi di background del sistema.</p>
<p>Esempio: I benchmark AI mobili potrebbero specificare che i test sono stati condotti a temperatura ambiente con dispositivi collegati a una fonte di alimentazione per eliminare le variazioni del livello della batteria.</p>
</section>
<section id="regole-di-riproducibilità" class="level4">
<h4 class="anchored" data-anchor-id="regole-di-riproducibilità">Regole di Riproducibilità</h4>
<p>Per garantire che i benchmark siano credibili e possano essere replicati da altri nella comunità, spesso includono protocolli dettagliati che coprono tutto, dai “random seed” utilizzati agli iperparametri esatti.</p>
<p>Esempio: Un benchmark per un’attività di learning di rinforzo potrebbe specificare gli episodi esatti dell’addestramento, i rapporti di esplorazione-sfruttamento e le strutture di ricompensa utilizzate.</p>
</section>
<section id="linee-guida-per-linterpretazione-dei-risultati" class="level4">
<h4 class="anchored" data-anchor-id="linee-guida-per-linterpretazione-dei-risultati">Linee Guida per l’Interpretazione dei Risultati</h4>
<p>Oltre ai punteggi o alle metriche pure, i benchmark spesso forniscono linee guida o contesto per interpretare i risultati, aiutando i professionisti a comprendere le implicazioni più ampie.</p>
<p>Esempio: Un benchmark potrebbe evidenziare che, sebbene il Modello A abbia ottenuto un punteggio più alto del Modello B in termini di accuratezza, offre migliori prestazioni in tempo reale, rendendolo più adatto per applicazioni sensibili al fattore tempo.</p>
</section>
</section>
<section id="training-vs.-inferenza" class="level3" data-number="11.4.3">
<h3 data-number="11.4.3" class="anchored" data-anchor-id="training-vs.-inferenza"><span class="header-section-number">11.4.3</span> Training vs.&nbsp;inferenza</h3>
<p>Il ciclo di vita dello sviluppo di un modello di apprendimento automatico prevede due fasi critiche: addestramento e inferenza. <a href="../../../contents/core/training/training.it.html">Training</a>, come forse ricorderete, è il processo di apprendimento di pattern dai dati per creare il modello. L’inferenza si riferisce al modello che fa previsioni su nuovi dati non etichettati. Entrambe le fasi svolgono ruoli indispensabili ma distinti. Di conseguenza, ogni fase garantisce un rigoroso benchmarking per valutare metriche delle prestazioni come velocità, accuratezza ed efficienza computazionale.</p>
<p>Il benchmarking della fase di training fornisce informazioni su come diverse architetture di modelli, valori di iperparametri e algoritmi di ottimizzazione influiscono sul tempo e sulle risorse necessarie per il training del modello. Ad esempio, il benchmarking mostra come la profondità della rete neurale influisce sul tempo di training su un dato set di dati. Il benchmarking rivela anche come acceleratori hardware come GPU e TPU possono accelerare il training.</p>
<p>D’altro canto, il benchmark dell’inferenza valuta le prestazioni del modello in condizioni reali dopo la distribuzione. Le metriche chiave includono latenza, throughput, footprint di memoria e consumo energetico. Questo tipo di benchmarking determina se un modello soddisfa i requisiti della sua applicazione target in termini di tempo di risposta e vincoli del dispositivo. Tuttavia, ne discuteremo ampiamente per garantire una comprensione generale.</p>
</section>
<section id="i-benchmark-del-training" class="level3 page-columns page-full" data-number="11.4.4">
<h3 data-number="11.4.4" class="anchored" data-anchor-id="i-benchmark-del-training"><span class="header-section-number">11.4.4</span> I Benchmark del Training</h3>
<p>Il training [addestramento] rappresenta la fase in cui il sistema elabora e assimila dati grezzi per adattare e perfezionare i propri parametri. Pertanto, è un’attività algoritmica e comporta considerazioni a livello di sistema, tra cui pipeline di dati, archiviazione, risorse di elaborazione e meccanismi di orchestrazione. L’obiettivo è garantire che il sistema ML possa apprendere in modo efficiente dai dati, ottimizzando sia le prestazioni del modello sia l’utilizzo delle risorse del sistema.</p>
<section id="scopo" class="level4">
<h4 class="anchored" data-anchor-id="scopo">Scopo</h4>
<p>Dal punto di vista dei sistemi ML, i benchmark del training valutano quanto bene il sistema si adatta all’aumento dei volumi di dati e delle richieste di elaborazione. Si tratta di comprendere l’interazione tra hardware, software e pipeline di dati nel processo di training.</p>
<p>Si consideri un sistema ML distribuito progettato per il training su vasti set di dati, come quelli utilizzati nelle raccomandazioni di prodotti di e-commerce su larga scala. Un benchmark di training valuterebbe l’efficienza con cui il sistema si adatta a più nodi, gestisce lo sharding [partizionamento] dei dati e gestisce guasti o abbandoni dei nodi durante il training.</p>
<p>I benchmark di training valutano l’utilizzo di CPU, GPU, memoria e rete durante la fase di training, guidando le ottimizzazioni del sistema. Quando si addestra un modello in un sistema ML basato su cloud, è fondamentale capire come vengono utilizzate le risorse. Le GPU vengono sfruttate appieno? C’è un sovraccarico di memoria non necessario? I benchmark possono evidenziare colli di bottiglia o inefficienze nell’utilizzo delle risorse, con conseguenti risparmi sui costi e miglioramenti delle prestazioni.</p>
<p>Il training di un modello ML è subordinato alla consegna tempestiva ed efficiente dei dati. I benchmark in questo contesto valuterebbero anche l’efficienza delle pipeline di dati, la velocità di preelaborazione dei dati e i tempi di recupero dell’archiviazione. Per i sistemi di analisi in tempo reale, come quelli utilizzati nel rilevamento delle frodi, la velocità con cui i dati di training vengono ingeriti, preelaborati e immessi nel modello può essere critica. I benchmark valuterebbero la latenza delle pipeline di dati, l’efficienza dei sistemi di archiviazione (come SSD rispetto a HDD) e la velocità delle attività di aumento o trasformazione dei dati.</p>
</section>
<section id="metriche" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="metriche">Metriche</h4>
<p>Se viste da una prospettiva di sistema, le metriche di training offrono informazioni che trascendono gli indicatori di prestazioni algoritmiche convenzionali. Queste metriche misurano l’efficacia di apprendimento del modello e misurano l’efficienza, la scalabilità e la robustezza dell’intero sistema ML durante la fase di training. Analizziamo più a fondo queste metriche e il loro significato.</p>
<p>Le seguenti metriche sono spesso considerate importanti:</p>
<ol type="1">
<li><p><strong>Tempo di training:</strong> Il tempo necessario per addestrare un modello da zero fino a raggiungere un livello di prestazioni soddisfacente. Misura direttamente le risorse di elaborazione necessarie per addestrare un modello. Ad esempio, <a href="https://arxiv.org/abs/1810.04805">il BERT di Google</a> <span class="citation" data-cites="devlin2018bert">(<a href="#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span> è un modello di elaborazione del linguaggio naturale che richiede diversi giorni per l’addestramento su un corpus enorme di dati di testo utilizzando più GPU. Il lungo tempo di training è una sfida significativa in termini di consumo di risorse e costi. In alcuni casi, i benchmark possono invece misurare la produttività del training (campioni di training per unità di tempo). La produttività può essere calcolata molto più velocemente e facilmente del tempo di addestramento, ma potrebbe oscurare le metriche che ci interessano davvero (ad esempio, il tempo di addestramento).</p></li>
<li><p><strong>Scalabilità:</strong> Quanto bene il processo di addestramento può gestire gli aumenti delle dimensioni dei dati o della complessità del modello. La scalabilità può essere valutata misurando il tempo di addestramento, l’utilizzo della memoria e altri consumi di risorse all’aumentare delle dimensioni dei dati o della complessità del modello. Il modello GPT-3 di <a href="https://arxiv.org/abs/2005.14165">OpenAI</a> <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> ha 175 miliardi di parametri, il che lo rende uno dei più grandi modelli linguistici esistenti. L’addestramento di GPT-3 ha richiesto notevoli sforzi ingegneristici per scalare il processo di addestramento in modo da gestire le enormi dimensioni del modello. Ciò ha comportato l’utilizzo di hardware specializzato, addestramento distribuito e altre tecniche per garantire che il modello potesse essere addestrato in modo efficiente.</p></li>
<li><p><strong>Utilizzo delle Risorse:</strong> La misura in cui il processo di addestramento utilizza le risorse di calcolo disponibili come CPU, GPU, memoria e I/O del disco. Un elevato utilizzo delle risorse può indicare un processo di training efficiente, mentre un basso utilizzo può suggerire colli di bottiglia o inefficienze. Ad esempio, il training di una rete neurale convoluzionale (CNN) per la classificazione delle immagini richiede notevoli risorse GPU. L’utilizzo di configurazioni multi-GPU e l’ottimizzazione del codice di training per l’accelerazione GPU possono migliorare notevolmente l’utilizzo delle risorse e l’efficienza del training.</p></li>
<li><p><strong>Consumo di Memoria:</strong> La quantità di memoria utilizzata dal processo di training. Il consumo di memoria può essere un fattore limitante per il training di modelli o set di dati di grandi dimensioni. Ad esempio, i ricercatori di Google hanno dovuto affrontare notevoli sfide di consumo di memoria durante il training di BERT. Il modello ha centinaia di milioni di parametri, che richiedono grandi quantità di memoria. I ricercatori hanno dovuto sviluppare tecniche per ridurre il consumo di memoria, come il checkpointing del gradiente e il parallelismo del modello.</p></li>
<li><p><strong>Consumo Energetico:</strong> L’energia consumata durante il training. Man mano che i modelli di apprendimento automatico diventano più complessi, il consumo energetico è diventato un fattore importante da considerare. Il training di grandi modelli di apprendimento automatico può consumare molta energia, e quindi molto carbonio. Ad esempio, si è stimato che l’addestramento di GPT-3 di OpenAI abbia un’impronta di carbonio equivalente a un viaggio in auto di 700.000 chilometri.</p></li>
<li><p><strong>Throughput:</strong> l numero di campioni di addestramento elaborati per unità di tempo. Un throughput [produttività] più elevato indica generalmente un processo di addestramento più efficiente. La produttività è una metrica importante da considerare quando si addestra un sistema di raccomandazione per una piattaforma di e-commerce. Una produttività elevata assicura che il modello possa elaborare rapidamente grandi volumi di dati di interazione dell’utente, il che è fondamentale per mantenere la pertinenza e l’accuratezza delle raccomandazioni. Ma è anche importante capire come bilanciare la produttività con i limiti di latenza. Pertanto, un vincolo di produttività limitato dalla latenza viene spesso imposto agli accordi sul livello di servizio per le distribuzioni di applicazioni del data center.</p></li>
<li><p><strong>Costo:</strong> Il costo della training di un modello può includere sia risorse computazionali che umane. Il costo è importante quando si considera la praticità e la fattibilità del training di modelli grandi o complessi. Si stima che l’addestramento di modelli di linguaggio grandi come GPT-3 costi milioni di dollari. Questo costo include risorse computazionali, elettriche e umane necessarie per lo sviluppo e l’addestramento del modello.</p></li>
<li><p><strong>Tolleranza agli Errori e Robustezza:</strong> La capacità del processo di training di gestire guasti o errori senza bloccarsi o produrre risultati errati. Questo è importante per garantire l’affidabilità del processo di addestramento. Errori di rete o malfunzionamenti hardware possono verificarsi in uno scenario reale in cui un modello di apprendimento automatico viene addestrato su un sistema distribuito. Negli ultimi anni, è diventato abbondantemente chiaro che gli errori derivanti dalla corruzione “silenziosa” dei dati sono emersi come un problema importante. Un processo di addestramento affidabile e tollerante agli errori può recuperare da tali errori senza compromettere l’integrità del modello.</p></li>
<li><p><strong>Facilità d’Uso e Flessibilità:</strong> La facilità con cui il processo di addestramento può essere impostato e utilizzato e la sua flessibilità nella gestione di diversi tipi di dati e modelli. In aziende come Google, l’efficienza può talvolta essere misurata dal numero di anni di “Software Engineer (SWE)” risparmiati poiché ciò si traduce direttamente in impatto. La facilità d’uso e la flessibilità possono ridurre il tempo e lo sforzo necessari per addestrare un modello. TensorFlow e PyTorch sono popolari framework di apprendimento automatico che forniscono interfacce intuitive e API flessibili per la creazione e l’addestramento di modelli di machine-learning. Questi framework supportano molte architetture di modelli e sono dotati di strumenti che semplificano il processo di addestramento.</p></li>
<li><p><strong>Riproducibilità:</strong> La capacità di riprodurre i risultati del processo di training. La riproducibilità è importante per verificare la correttezza e la validità di un modello. Tuttavia, le variazioni dovute alle caratteristiche stocastiche della rete spesso rendono difficile riprodurre il comportamento preciso delle applicazioni in fase di addestramento, il che può rappresentare una sfida per il benchmarking.</p></li>
</ol>
<div class="no-row-height column-margin column-container"></div><p>Eseguendo il benchmarking per questi tipi di metriche, possiamo ottenere una visione completa delle prestazioni e dell’efficienza del processo di training da una prospettiva di sistema. Ciò può aiutare a identificare le aree di miglioramento e garantire che le risorse siano utilizzate in modo efficace.</p>
</section>
<section id="i-task" class="level4">
<h4 class="anchored" data-anchor-id="i-task">I Task</h4>
<p>Selezionare una manciata di task [attività] rappresentative per il benchmarking dei sistemi di machine learning è una sfida, perché l’apprendimento automatico viene applicato a vari domini con caratteristiche e requisiti unici. Ecco alcune delle sfide affrontate nella selezione di attività rappresentative:</p>
<ol type="1">
<li><strong>Diversità di Applicazioni:</strong> L’apprendimento automatico viene utilizzato in numerosi campi, come sanità, finanza, elaborazione del linguaggio naturale, visione artificiale e molti altri. Ogni campo ha attività specifiche che potrebbero non essere rappresentative di altri campi. Ad esempio, le attività di classificazione delle immagini nella visione artificiale potrebbero non essere importanti per il rilevamento delle frodi finanziarie.</li>
<li><strong>Variabilità nei Tipi di Dati e nella Qualità:</strong> Diverse attività richiedono tipi di dati diversi, come testo, immagini, video o dati numerici. La qualità e la disponibilità dei dati possono variare notevolmente tra le attività, rendendo difficile selezionare attività rappresentative delle sfide generali affrontate nell’apprendimento automatico.</li>
<li><strong>Complessità e Difficoltà delle Attività:</strong> La complessità delle attività varia notevolmente. Alcune sono relativamente semplici, mentre altre sono molto complesse e richiedono modelli e tecniche sofisticate. Selezionare attività rappresentative che coprano le complessità riscontrate nell’apprendimento automatico è difficile.</li>
<li><strong>Problemi Etici e di Privacy:</strong> Alcune attività possono riguardare dati sensibili o privati, come cartelle cliniche o informazioni personali. Queste attività possono presentare problemi etici e di privacy che devono essere affrontati, rendendole meno adatte come attività rappresentative per il benchmarking.</li>
<li><strong>Requisiti di Scalabilità e Risorse:</strong> Attività diverse possono avere requisiti di scalabilità e risorse diverse. Alcune attività possono richiedere ampie risorse di calcolo, mentre altre possono essere eseguite con risorse minime. Selezionare attività che rappresentino i requisiti di risorse generali nell’apprendimento automatico è difficile.</li>
<li><strong>Metriche di Valutazione:</strong> Le metriche utilizzate per valutare le prestazioni dei modelli di apprendimento automatico variano tra le attività. Alcune attività possono avere metriche di valutazione consolidate, mentre altre non hanno metriche chiare o standardizzate. Ciò può rendere difficile confrontare le prestazioni tra diverse attività.</li>
<li><strong>Generalizzabilità dei Risultati:</strong> I risultati ottenuti dal benchmarking su un’attività specifica potrebbero non essere generalizzabili ad altre attività. Ciò significa che le prestazioni di un sistema di apprendimento automatico su un’attività selezionata potrebbero non essere indicative delle sue prestazioni su altre attività.</li>
</ol>
<p>È importante considerare attentamente questi fattori quando si progettano benchmark per garantire che siano significativi e pertinenti per la vasta gamma di attività incontrate nel machine learning.</p>
</section>
<section id="i-benchmark" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="i-benchmark">I Benchmark</h4>
<p>Ecco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per l’addestramento di sistemi di apprendimento automatico.</p>
<p><em><a href="https://github.com/mlcommons/training">MLPerf Training Benchmark</a></em></p>
<p>MLPerf è una suite di benchmark progettata per misurare le prestazioni di hardware, software e servizi di apprendimento automatico. Il benchmark di MLPerf Training <span class="citation" data-cites="mattson2020mlperf">(<a href="#ref-mattson2020mlperf" role="doc-biblioref">Mattson et al. 2020a</a>)</span> si concentra sul tempo necessario per addestrare i modelli a una metrica di qualità target. Include carichi di lavoro diversi, come classificazione delle immagini, rilevamento di oggetti, traduzione e apprendimento per rinforzo. <a href="#fig-perf-trend" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-perf-trend</span></a> evidenzia i miglioramenti delle prestazioni nelle versioni progressive dei benchmark di MLPerf Training, che hanno tutti superato la legge di Moore. L’utilizzo di trend di benchmarking standardizzati ci consente di mostrare rigorosamente la rapida evoluzione del ML computing.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-perf-trend" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-perf-trend-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlperf_perf_trend.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-perf-trend-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.2: Tendenze delle prestazioni di MLPerf Training. Fonte: <span class="citation" data-cites="mattson2020mlperf">Mattson et al. (<a href="#ref-mattson2020mlperf" role="doc-biblioref">2020a</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>Metriche:</p>
<ul>
<li>Tempo di training per la qualità target</li>
<li>Throughput (esempi al secondo)</li>
<li>Utilizzo delle risorse (CPU, GPU, memoria, I/O del disco)</li>
</ul>
<p><em><a href="https://dawn.cs.stanford.edu/benchmark/">DAWNBench</a></em></p>
<p>DAWNBench <span class="citation" data-cites="coleman2017dawnbench">(<a href="#ref-coleman2017dawnbench" role="doc-biblioref">Coleman et al. 2019</a>)</span> è una suite di benchmark incentrata sul tempo di training end-to-end del deep learning e sulle prestazioni dell’inferenza. Include attività comuni come la classificazione delle immagini e la risposta alle domande.</p>
<div class="no-row-height column-margin column-container"><div id="ref-coleman2017dawnbench" class="csl-entry" role="listitem">
Coleman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, e Matei Zaharia. 2019. <span>«Analysis of <span>DAWNBench,</span> a Time-to-Accuracy Machine Learning Performance Benchmark»</span>. <em>ACM SIGOPS Operating Systems Review</em> 53 (1): 14–25. <a href="https://doi.org/10.1145/3352020.3352024">https://doi.org/10.1145/3352020.3352024</a>.
</div></div><p>Metriche:</p>
<ul>
<li>Tempo di training per la precisione target</li>
<li>Latenza dell’inferenza</li>
<li>Costo (in termini di risorse di cloud computing e storage)</li>
</ul>
<p><em><a href="https://github.com/rdadolf/fathom">Fathom</a></em></p>
<p>Fathom <span class="citation" data-cites="adolf2016fathom">(<a href="#ref-adolf2016fathom" role="doc-biblioref">Adolf et al. 2016</a>)</span> è un benchmark dell’Università di Harvard che valuta le prestazioni dei modelli di deep learning utilizzando un set diversificato di carichi di lavoro. Questi includono attività comuni come la classificazione delle immagini, il riconoscimento vocale e la modellazione del linguaggio.</p>
<div class="no-row-height column-margin column-container"><div id="ref-adolf2016fathom" class="csl-entry" role="listitem">
Adolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. <span>«Fathom: <span>Reference</span> workloads for modern deep learning methods»</span>. In <em>2016 IEEE International Symposium on Workload Characterization (IISWC)</em>, 1–10. IEEE; IEEE. <a href="https://doi.org/10.1109/iiswc.2016.7581275">https://doi.org/10.1109/iiswc.2016.7581275</a>.
</div></div><p>Metriche:</p>
<ul>
<li>Operazioni al secondo (per misurare l’efficienza computazionale)</li>
<li>Tempo di completamento per ogni carico di lavoro</li>
<li>Larghezza di banda della memoria</li>
</ul>
</section>
<section id="caso-duso-di-esempio" class="level4">
<h4 class="anchored" data-anchor-id="caso-duso-di-esempio">Caso d’Uso di Esempio</h4>
<p>Consideriamo uno scenario in cui vogliamo eseguire il benchmark dell’addestramento di un modello di classificazione delle immagini su una piattaforma hardware specifica.</p>
<ol type="1">
<li><strong>Task:</strong> L’attività consiste nell’addestrare una rete neurale convoluzionale (CNN) per la classificazione delle immagini sul set di dati CIFAR-10.</li>
<li><strong>Benchmark:</strong> Possiamo usare il benchmark di addestramento MLPerf per questa attività. Include un carico di lavoro di classificazione delle immagini pertinente alla nostra attività.</li>
<li><strong>Metriche:</strong> Misureremo le seguenti metriche:</li>
</ol>
<ul>
<li>Tempo di addestramento per raggiungere una precisione target del 90%.</li>
<li>Throughput in termini di immagini elaborate al secondo.</li>
<li>Utilizzo di GPU e CPU durante l’addestramento.</li>
</ul>
<p>Misurando queste metriche, possiamo valutare le prestazioni e l’efficienza del processo di addestramento sulla piattaforma hardware selezionata. Queste informazioni possono quindi essere utilizzate per identificare potenziali colli di bottiglia o aree di miglioramento.</p>
</section>
</section>
<section id="benchmark-di-inferenza" class="level3" data-number="11.4.5">
<h3 data-number="11.4.5" class="anchored" data-anchor-id="benchmark-di-inferenza"><span class="header-section-number">11.4.5</span> Benchmark di Inferenza</h3>
<p>L’inferenza nell’apprendimento automatico si riferisce all’uso di un modello addestrato per fare previsioni su dati nuovi e mai visti prima. È la fase in cui il modello applica le conoscenze apprese per risolvere il problema per cui è stato progettato, come la classificazione di immagini, il riconoscimento vocale o la traduzione di testo.</p>
<section id="scopo-1" class="level4">
<h4 class="anchored" data-anchor-id="scopo-1">Scopo</h4>
<p>Quando creiamo modelli di machine learning, il nostro obiettivo finale è di distribuirli in applicazioni del mondo reale in cui possano fornire previsioni accurate e affidabili su dati nuovi e mai visti. Questo processo di utilizzo di un modello addestrato per fare previsioni è noto come inferenza. Le prestazioni reali di un modello di apprendimento automatico possono differire in modo significativo dalle sue prestazioni su set di dati di addestramento o validazione, il che rende l’inferenza di benchmarking un passaggio cruciale nello sviluppo e nell’implementazione di modelli di machine learning.</p>
<p>Il benchmarking dell’inferenza ci consente di valutare quanto bene un modello di apprendimento automatico funziona in scenari del mondo reale. Questa valutazione garantisce che il modello sia pratico e affidabile quando distribuito in applicazioni, fornendo una comprensione più completa del comportamento del modello con dati reali. Inoltre, il benchmarking può aiutare a identificare potenziali colli di bottiglia o limitazioni nelle prestazioni del modello. Ad esempio, se un modello impiega troppo tempo per dedurre, potrebbe non essere pratico per applicazioni in tempo reale come la guida autonoma o gli assistenti vocali.</p>
<p>L’efficienza delle risorse è un altro aspetto critico dell’inferenza, poiché può essere computazionalmente intensiva e richiedere memoria e potenza di elaborazione significative. Il benchmarking aiuta a garantire che il modello sia efficiente per quanto riguarda l’utilizzo delle risorse, il che è particolarmente importante per i dispositivi edge con capacità computazionali limitate, come smartphone o dispositivi IoT. Inoltre, il benchmarking ci consente di confrontare le prestazioni del nostro modello con quelli concorrenti o versioni precedenti dello stesso modello. Questo confronto è essenziale per prendere decisioni informate su quale modello implementare in un’applicazione specifica.</p>
<p>Infine, è fondamentale garantire che le previsioni del modello non siano solo accurate, ma anche coerenti tra diversi dati. Il benchmarking aiuta a verificare l’accuratezza e la coerenza del modello, assicurando che soddisfi i requisiti dell’applicazione. Valuta inoltre la robustezza del modello, assicurando che possa gestire la variabilità dei dati del mondo reale e comunque fare previsioni accurate.</p>
</section>
<section id="metriche-1" class="level4">
<h4 class="anchored" data-anchor-id="metriche-1">Metriche</h4>
<ol type="1">
<li><p><strong>Precisione:</strong> La precisione è una delle metriche più importanti quando si confrontano i modelli di machine learning. Quantifica la percentuale di previsioni corrette effettuate dal modello rispetto ai valori o alle etichette reali. Ad esempio, se un modello di rilevamento dello spam riesce a classificare correttamente 95 messaggi e-mail su 100, la sua precisione verrebbe calcolata al 95%.</p></li>
<li><p><strong>Latenza:</strong> La latenza è una metrica delle prestazioni che calcola il ritardo o l’intervallo di tempo tra la ricezione dell’input e la produzione dell’output corrispondente da parte del sistema di apprendimento automatico. Un esempio che descrive chiaramente la latenza è un’applicazione di traduzione in tempo reale; se esiste un ritardo di mezzo secondo dal momento in cui un utente inserisce una frase al momento in cui l’app visualizza il testo tradotto, la latenza del sistema è di 0.5 secondi.</p></li>
<li><p><strong>Latency-Bounded Throughput:</strong> Il throughput limitato dalla latenza è una metrica preziosa che combina gli aspetti di latenza e throughput, misurando il throughput massimo di un sistema pur rispettando un vincolo di latenza specificato. Ad esempio, in un’applicazione di streaming video che utilizza un modello di apprendimento automatico per generare e visualizzare automaticamente i sottotitoli, il throughput limitato dalla latenza misurerebbe quanti frame video il sistema può elaborare al secondo (throughput) garantendo al contempo che i sottotitoli vengano visualizzati con un ritardo non superiore a 1 secondo (latenza). Questa metrica è particolarmente importante nelle applicazioni in tempo reale in cui soddisfare i requisiti di latenza è fondamentale per l’esperienza utente.</p></li>
<li><p><strong>Throughput:</strong> Il throughput valuta la capacità del sistema misurando il numero di inferenze o previsioni che un modello di apprendimento automatico può gestire entro un’unità di tempo specifica. Si consideri un sistema di riconoscimento vocale che utilizza una Recurrent Neural Network (RNN) come modello sottostante; se questo sistema riesce a elaborare e comprendere 50 diverse clip audio in un minuto, allora la sua velocità di elaborazione è di 50 clip al minuto.</p></li>
<li><p><strong>Efficienza energetica:</strong> L’efficienza energetica è una metrica che determina la quantità di energia consumata dal modello di apprendimento automatico per eseguire una singola inferenza. Un esempio lampante di ciò sarebbe un modello di elaborazione del linguaggio naturale basato su un’architettura di rete Transformer; se utilizza 0,1 Joule di energia per tradurre una frase dall’inglese al francese, la sua efficienza energetica è misurata a 0,1 Joule per inferenza.</p></li>
<li><p><strong>Utilizzo della memoria:</strong> L’utilizzo della memoria quantifica il volume di RAM necessario a un modello di apprendimento automatico per svolgere attività di inferenza. Un esempio rilevante per illustrare questo sarebbe un sistema di riconoscimento facciale basato su una CNN; se un tale sistema richiede 150 MB di RAM per elaborare e riconoscere i volti all’interno di un’immagine, il suo utilizzo della memoria è di 150 MB.</p></li>
</ol>
</section>
<section id="i-task-1" class="level4">
<h4 class="anchored" data-anchor-id="i-task-1">I Task</h4>
<p>Le sfide nella scelta di attività rappresentative per il benchmarking dei sistemi di apprendimento automatico inferenziale sono, in generale, piuttosto simili alla tassonomia che abbiamo fornito per la il training. Tuttavia, per essere pignoli, discutiamone nel contesto dei sistemi di machine learning inferenziale.</p>
<ol type="1">
<li><p><strong>Diversità di Applicazioni:</strong> L’apprendimento automatico inferenziale è impiegato in numerosi domini come sanità, finanza, intrattenimento, sicurezza e altro. Ogni dominio ha attività uniche e ciò che è rappresentativo in un dominio potrebbe non esserlo in un altro. Ad esempio, un’attività di inferenza per prevedere i prezzi delle azioni nel dominio finanziario potrebbe differire dalle attività di riconoscimento delle immagini nel dominio medico.</p></li>
<li><p><strong>Variabilità nei Tipi di Dati:</strong> Diverse attività di inferenza richiedono diversi tipi di dati: testo, immagini, video, dati numerici, ecc. Assicurarsi che i benchmark affrontino l’ampia varietà di tipi di dati utilizzati nelle applicazioni del mondo reale è una sfida. Ad esempio, i sistemi di riconoscimento vocale elaborano dati audio, che sono molto diversi dai dati visivi elaborati dai sistemi di riconoscimento facciale.</p></li>
<li><p><strong>Complessità delle Attività:</strong> La complessità delle attività di inferenza può variare enormemente, da attività di classificazione di base ad attività complesse che richiedono modelli all’avanguardia. Ad esempio, distinguere tra due categorie (classificazione binaria) è in genere più semplice che rilevare centinaia di tipi di oggetti in una scena affollata.</p></li>
<li><p><strong>Requisiti in Tempo Reale:</strong> Alcune applicazioni richiedono risposte immediate o in tempo reale, mentre altre possono consentire un certo ritardo. Nella guida autonoma, il rilevamento degli oggetti in tempo reale e il processo decisionale sono fondamentali, mentre un motore di raccomandazione per un sito Web di shopping potrebbe tollerare lievi ritardi.</p></li>
<li><p><strong>Problemi di Scalabilità:</strong> Data la variabilità della scala delle applicazioni, dai dispositivi edge ai server basati su cloud, le attività devono rappresentare i diversi ambienti di elaborazione in cui si verifica l’inferenza. Ad esempio, un’attività di inferenza in esecuzione sulle risorse limitate di uno smartphone è diversa da un potente server cloud.</p></li>
<li><p><strong>Diversità delle Metriche di Valutazione:</strong> Le metriche utilizzate per valutare le prestazioni possono variare in modo significativo a seconda dell’attività. Trovare un terreno comune o una metrica universalmente accettata per attività diverse è una sfida. Ad esempio, la precisione e il “recall” [richiamo] potrebbero essere vitali per un’attività di diagnosi medica, mentre la produttività (inferenze al secondo) potrebbero essere più cruciali per le attività di elaborazione video.</p></li>
<li><p><strong>Problemi Etici e di Privacy:</strong> Esistono problemi relativi all’etica e alla privacy, soprattutto in aree sensibili come il riconoscimento facciale o l’elaborazione dei dati personali. Questi problemi possono influire sulla selezione e sulla natura delle attività utilizzate per il benchmarking. Ad esempio, l’utilizzo di dati facciali reali per il benchmarking può sollevare problemi di privacy, mentre i dati sintetici potrebbero non replicare le sfide del mondo reale.</p></li>
<li><p><strong>Diversità Hardware:</strong> Con un’ampia gamma di dispositivi, da GPU, CPU e TPU ad ASIC personalizzati utilizzati per l’inferenza, garantire che le attività siano rappresentative su hardware diversi è una sfida. Ad esempio, un’attività ottimizzata per l’inferenza su una GPU potrebbe avere prestazioni non ottimali su un dispositivo edge.</p></li>
</ol>
</section>
<section id="i-benchmark-1" class="level4">
<h4 class="anchored" data-anchor-id="i-benchmark-1">I Benchmark</h4>
<p>Ecco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per sistemi di apprendimento automatico inferenziale.</p>
<p><strong><a href="https://github.com/mlcommons/inference">MLPerf Inference Benchmark</a>:</strong> MLPerf Inference è una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una varietà di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l’obiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza. Le sue metriche includono:</p>
<p>MLPerf Inference è una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una varietà di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l’obiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza.</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Latenza</li>
<li>Throughput [Produttività]</li>
<li>Precisione</li>
<li>Consumo energetico</li>
</ul>
<p><strong><a href="https://ai-benchmark.com/">AI Benchmark</a>:</strong> AI Benchmark è uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attività di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un’analisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware. Le sue metriche includono:</p>
<p>AI Benchmark è uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e di apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attività di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un’analisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware.</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Latenza</li>
<li>Consumo energetico</li>
<li>Utilizzo della memoria</li>
<li>Throughput [Produttività]</li>
</ul>
<p><strong><a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">Toolkit OpenVINO</a>:</strong> Il toolkit OpenVINO fornisce uno strumento di benchmark per misurare le prestazioni dei modelli di apprendimento profondo per varie attività, come la classificazione delle immagini, il rilevamento degli oggetti e il riconoscimento facciale, su hardware Intel. Offre approfondimenti dettagliati sulle prestazioni di inferenza dei modelli su diverse configurazioni hardware. Le sue metriche includono:</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Throughput [Produttività]</li>
<li>Latenza</li>
<li>Utilizzo di CPU e GPU</li>
</ul>
</section>
<section id="caso-duso-di-esempio-1" class="level4">
<h4 class="anchored" data-anchor-id="caso-duso-di-esempio-1">Caso d’Uso di Esempio</h4>
<p>Consideriamo uno scenario in cui vogliamo valutare le prestazioni di inferenza di un modello di rilevamento di oggetti su uno specifico dispositivo edge.</p>
<p>Task: L’attività consiste nell’eseguire il rilevamento di oggetti in tempo reale su flussi video, rilevando e identificando oggetti quali veicoli, pedoni e segnali stradali.</p>
<p>Benchmark: Possiamo utilizzare AI Benchmark per questa attività in quanto valuta le prestazioni di inferenza sui dispositivi edge, il che si adatta al nostro scenario.</p>
<p>Metriche: Misureremo le seguenti metriche:</p>
<ul>
<li>Tempo di inferenza per elaborare ogni fotogramma video</li>
<li>Latenza per generare i “bounding box” per gli oggetti rilevati</li>
<li>Consumo energetico durante il processo di inferenza</li>
<li>Produttività in termini di fotogrammi video elaborati al secondo</li>
</ul>
<p>Misurando queste metriche, possiamo valutare le prestazioni del modello di rilevamento di oggetti sul dispositivo edge e identificare eventuali colli di bottiglia o aree di ottimizzazione per migliorare le capacità di elaborazione in tempo reale.</p>
<div id="exr-perf" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;11.2: Benchmark di Inferenza - MLPerf
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Prepararsi a mettere alla prova i propri modelli di intelligenza artificiale! MLPerf è come le Olimpiadi per le prestazioni del machine learning. In questo Colab, utilizzeremo un toolkit chiamato CK per eseguire benchmark MLPerf ufficiali, misurare la velocità e l’accuratezza di un proprio modello e persino utilizzare TVM per dargli una spinta super veloce. Pronti a vedere il modello vincere la sua medaglia?</p>
<p><a href="https://colab.research.google.com/drive/1aywGlyD1ZRDtQTrQARVgL1882JcvmFK-?usp=sharing#scrollTo=tnyHAdErL72u"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="misura-dellefficienza-energetica" class="level3 page-columns page-full" data-number="11.4.6">
<h3 data-number="11.4.6" class="anchored" data-anchor-id="misura-dellefficienza-energetica"><span class="header-section-number">11.4.6</span> Misura dell’Efficienza Energetica</h3>
<p>Con l’espansione delle capacità di apprendimento automatico, sia nel training che nell’inferenza, le preoccupazioni relative all’aumento del consumo energetico e al suo impatto ecologico si sono intensificate. Affrontare la sostenibilità dei sistemi ML, un argomento esplorato più approfonditamente nel capitolo <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html">IA Sostenibile</a>, è quindi diventata una priorità fondamentale. Questa attenzione alla sostenibilità ha portato allo sviluppo di benchmark standardizzati progettati per misurare con precisione l’efficienza energetica. Tuttavia, la standardizzazione di queste metodologie pone delle sfide dovute alla necessità di adattarsi a scale molto diverse, dal consumo di microwatt dei dispositivi TinyML alle richieste di megawatt dei sistemi di training dei data center. Inoltre, per garantire che il benchmarking sia equo e riproducibile è necessario adattarsi alla vasta gamma di configurazioni hardware e architetture in uso oggi.</p>
<p>Un esempio è la metodologia di benchmarking MLPerf Power <span class="citation" data-cites="tschand2024mlperf">(<a href="#ref-tschand2024mlperf" role="doc-biblioref">Tschand et al. 2024</a>)</span>, che affronta queste sfide adattando le metodologie per data center, edge inference e tiny inference systems, misurando al contempo il consumo energetico nel modo più completo possibile per ogni scala. Questa metodologia si adatta a una varietà di hardware, dalle CPU generiche agli acceleratori AI specializzati, mantenendo principi di misurazione uniformi per garantire che i confronti siano equi e accurati su diverse piattaforme.</p>
<div class="no-row-height column-margin column-container"></div><p><a href="#fig-power-diagram" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-power-diagram</span></a> illustra i limiti di misurazione dell’alimentazione per diverse scale di sistema, dai dispositivi TinyML ai nodi di inferenza e ai rack di training. Ciascun esempio evidenzia i componenti all’interno del limite di misurazione e quelli al di fuori di esso. Questa configurazione consente una riflessione accurata dei veri costi energetici associati all’esecuzione di carichi di lavoro ML in vari scenari del mondo reale e garantisce che il benchmark catturi l’intero spettro di consumo energetico.</p>
<div id="fig-power-diagram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-power-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/power_component_diagram.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-power-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.3: Diagramma di misurazione del sistema MLPerf Power. Fonte: <span class="citation" data-cites="tschand2024mlperf">Tschand et al. (<a href="#ref-tschand2024mlperf" role="doc-biblioref">2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-tschand2024mlperf" class="csl-entry" role="listitem">
Tschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024. <span>«MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from <span class="math inline">\(\{\)</span><span class="math inline">\(\backslash\)</span>mu<span class="math inline">\(\}\)</span> Watts to MWatts for Sustainable AI»</span>. <em>arXiv preprint arXiv:2410.12032</em>.
</div></div></figure>
</div>
<p>È importante notare che l’ottimizzazione di un sistema per le prestazioni potrebbe non portare all’esecuzione più efficiente dal punto di vista energetico. Spesso, sacrificare una piccola quantità di prestazioni o accuratezza può portare a guadagni significativi nell’efficienza energetica, evidenziando l’importanza di un benchmarking accurato delle metriche della potenza. Le future intuizioni dal benchmarking dell’efficienza energetica e della sostenibilità ci consentiranno di ottimizzare per sistemi ML più sostenibili.</p>
</section>
<section id="esempio-di-benchmark" class="level3 page-columns page-full" data-number="11.4.7">
<h3 data-number="11.4.7" class="anchored" data-anchor-id="esempio-di-benchmark"><span class="header-section-number">11.4.7</span> Esempio di Benchmark</h3>
<p>Per illustrare correttamente i componenti di un benchmark di sistema, possiamo esaminare il benchmark di individuazione delle parole chiave in MLPerf Tiny e spiegare la motivazione alla base di ogni decisione.</p>
<section id="task" class="level4">
<h4 class="anchored" data-anchor-id="task">Task</h4>
<p>L’individuazione delle parole chiave è stata selezionata come attività perché è un caso d’uso comune in TinyML che è stato ben consolidato per anni. Inoltre, l’hardware tipico utilizzato per l’individuazione delle parole chiave differisce sostanzialmente dalle offerte di altri benchmark, come l’attività di riconoscimento vocale di MLPerf Inference.</p>
</section>
<section id="il-dataset" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="il-dataset">Il Dataset</h4>
<p><a href="https://www.tensorflow.org/datasets/catalog/speech_commands">Google Speech Commands</a> <span class="citation" data-cites="warden2018speech">(<a href="#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span> è stato selezionato come il miglior dataset per rappresentare l’attività. Il dataset è ben consolidato nella comunità di ricerca e ha una licenza permissiva, che consente di utilizzarlo facilmente in un benchmark.</p>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>«Speech commands: <span>A</span> dataset for limited-vocabulary speech recognition»</span>. <em>ArXiv preprint</em> abs/1804.03209. <a href="https://arxiv.org/abs/1804.03209">https://arxiv.org/abs/1804.03209</a>.
</div></div></section>
<section id="modello" class="level4">
<h4 class="anchored" data-anchor-id="modello">Modello</h4>
<p>Il componente principale successivo è il modello, che fungerà da carico di lavoro primario per il benchmark. Il modello dovrebbe essere ben consolidato come soluzione per l’attività selezionata piuttosto che una soluzione all’avanguardia. Il modello selezionato è un semplice modello di convoluzione separabile in profondità. Questa architettura non è la soluzione all’avanguardia per l’attività, ma è ben consolidata e non progettata per una piattaforma hardware specifica come molte soluzioni all’avanguardia. Nonostante sia un benchmark di inferenza, stabilisce anche una ricetta di training di riferimento per essere completamente riproducibile e trasparente.</p>
</section>
<section id="metriche-2" class="level4">
<h4 class="anchored" data-anchor-id="metriche-2">Metriche</h4>
<p>La latenza è stata selezionata come metrica primaria per il benchmark, poiché i sistemi di individuazione delle parole chiave devono reagire rapidamente per mantenere la soddisfazione dell’utente. Inoltre, dato che i sistemi TinyML sono spesso alimentati a batteria, il consumo energetico viene misurato per garantire l’efficienza della piattaforma hardware. L’accuratezza del modello viene misurata anche per garantire che le ottimizzazioni applicate da un submitter, come la quantizzazione, non degradino l’accuratezza oltre una soglia.</p>
</section>
<section id="benchmark-harness" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-harness">Benchmark Harness</h4>
<p>MLPerf Tiny utilizza <a href="https://github.com/eembc/energyrunner">EEMBCs EnergyRunner benchmark harness</a> per caricare gli input nel modello e isolare e misurare il consumo energetico del dispositivo. Quando si misura il consumo energetico, è fondamentale selezionare un “harness” [imbracatura] che sia accurato ai livelli di potenza previsti dei dispositivi sottoposti a test e sufficientemente semplice da non diventare un peso per i partecipanti al benchmark.</p>
</section>
<section id="la-baseline" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="la-baseline">La Baseline</h4>
<p>Gli invii di baseline sono fondamentali per contestualizzare i risultati e come punto di riferimento per aiutare i partecipanti a iniziare. L’invio di base dovrebbe dare priorità alla semplicità e alla leggibilità rispetto alle prestazioni avanzate. L’individuazione della parola chiave della baseline utilizza un <a href="https://www.st.com/en/microcontrollers-microprocessors.html">microcontrollore STM</a> standard come hardware e <a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite come microcontrollore</a> <span class="citation" data-cites="david2021tensorflow">(<a href="#ref-david2021tensorflow" role="doc-biblioref">David et al. 2021</a>)</span> come framework di inferenza.</p>
<div class="no-row-height column-margin column-container"><div id="ref-david2021tensorflow" class="csl-entry" role="listitem">
David, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. <span>«Tensorflow lite micro: <span>Embedded</span> machine learning for tinyml systems»</span>. <em>Proceedings of Machine Learning and Systems</em> 3: 800–811.
</div></div></section>
</section>
<section id="sfide-e-limitazioni" class="level3 page-columns page-full" data-number="11.4.8">
<h3 data-number="11.4.8" class="anchored" data-anchor-id="sfide-e-limitazioni"><span class="header-section-number">11.4.8</span> Sfide e Limitazioni</h3>
<p>Sebbene il benchmarking fornisca una metodologia strutturata per la valutazione delle prestazioni in domini complessi come l’intelligenza artificiale e l’informatica, il processo pone anche diverse sfide. Se non affrontati correttamente, questi ostacoli possono minare la credibilità e l’accuratezza dei risultati del benchmarking. Alcune delle difficoltà predominanti affrontate nel benchmarking includono quanto segue:</p>
<ul>
<li><p><strong>Copertura incompleta del problema:</strong> Le attività di benchmarking potrebbero non rappresentare completamente lo spazio del problema. Ad esempio, i set di dati di classificazione delle immagini comuni come <a href="https://www.cs.toronto.edu/kriz/cifar.html">CIFAR-10</a> hanno una diversità limitata nei tipi di immagini. Gli algoritmi ottimizzati per tali benchmark potrebbero non riuscire a generalizzare bene con i set di dati del mondo reale.</p></li>
<li><p><strong>Insignificanza statistica:</strong> I benchmark devono avere prove e campioni di dati sufficienti per produrre risultati statisticamente significativi. Ad esempio, il benchmarking di un modello OCR su solo poche scansioni di testo potrebbe non catturare adeguatamente i suoi veri tassi di errore.</p></li>
<li><p><strong>Riproducibilità limitata:</strong> Variazioni di hardware, versioni software, basi di codice e altri fattori possono ridurre la riproducibilità dei risultati di benchmark. MLPerf affronta questo problema fornendo implementazioni di riferimento e specifiche ambientali.</p></li>
<li><p><strong>Disallineamento con gli obiettivi finali:</strong> I benchmark che si concentrano solo su metriche di velocità o accuratezza possono disallineare gli obiettivi reali come costi ed efficienza energetica. I benchmark devono riflettere tutti gli assi prestazionali critici.</p></li>
<li><p><strong>Rapida obsolescenza:</strong> A causa del rapido ritmo dei progressi nell’intelligenza artificiale e nell’informatica, i benchmark e i loro set di dati possono rapidamente diventare obsoleti. Mantenere benchmark aggiornati è quindi una sfida persistente.</p></li>
</ul>
<p>Ma di tutte queste, la sfida più importante è l’ingegneria dei benchmark.</p>
<section id="lotteria-hardware" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="lotteria-hardware">Lotteria Hardware</h4>
<p>La lotteria hardware, descritta per la prima volta da <span class="citation" data-cites="10.1145/3467017">Hooker (<a href="#ref-10.1145/3467017" role="doc-biblioref">2021</a>)</span>, si riferisce alla situazione in cui il successo o l’efficienza di un modello di apprendimento automatico sono significativamente influenzati dalla sua compatibilità con l’hardware sottostante <span class="citation" data-cites="chu2021discovering">(<a href="#ref-chu2021discovering" role="doc-biblioref">Chu et al. 2021</a>)</span>. Alcuni modelli hanno prestazioni eccezionali non perché sono intrinsecamente superiori, ma perché sono ottimizzati per caratteristiche hardware specifiche, come le capacità di elaborazione parallela delle unità di elaborazione grafica (GPU) o delle unità di elaborazione tensoriale (TPU).</p>
<div class="no-row-height column-margin column-container"><div id="ref-10.1145/3467017" class="csl-entry" role="listitem">
Hooker, Sara. 2021. <span>«The hardware lottery»</span>. <em>Commun. ACM</em> 64 (12): 58–65. <a href="https://doi.org/10.1145/3467017">https://doi.org/10.1145/3467017</a>.
</div></div><p>Ad esempio, <a href="#fig-hardware-lottery" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-hardware-lottery</span></a> confronta le prestazioni dei modelli su diverse piattaforme hardware. I modelli multi-hardware mostrano risultati comparabili a “MobileNetV3 Large min” sia sulle configurazioni CPU uint8 che GPU. Tuttavia, questi modelli multi-hardware dimostrano miglioramenti significativi delle prestazioni rispetto alla baseline MobileNetV3 Large quando eseguiti su hardware EdgeTPU e DSP. Ciò sottolinea l’efficienza variabile dei modelli multi-hardware in ambienti di elaborazione specializzati.</p>
<div id="fig-hardware-lottery" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hardware-lottery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hardware_lottery.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-lottery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.4: Compromessi tra precisione e latenza di più modelli ML e modalità di funzionamento su vari hardware. Fonte: <span class="citation" data-cites="chu2021discovering">Chu et al. (<a href="#ref-chu2021discovering" role="doc-biblioref">2021</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-chu2021discovering" class="csl-entry" role="listitem">
Chu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. <span>«Discovering Multi-Hardware Mobile Models via Architecture Search»</span>. In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 3022–31. IEEE. <a href="https://doi.org/10.1109/cvprw53098.2021.00337">https://doi.org/10.1109/cvprw53098.2021.00337</a>.
</div></div></figure>
</div>
<p>La lotteria hardware può introdurre sfide e pregiudizi nel benchmarking dei sistemi di apprendimento automatico, poiché le prestazioni del modello non dipendono esclusivamente dall’architettura o dall’algoritmo del modello ma anche dalla compatibilità e dalle sinergie con l’hardware sottostante. Ciò può rendere difficile confrontare equamente diversi modelli e identificare il modello migliore in base ai suoi meriti intrinseci. Può anche portare a una situazione in cui la comunità converge su modelli che sono adatti all’hardware più diffuso del momento, trascurando potenzialmente altri modelli che potrebbero essere superiori ma incompatibili con le attuali tendenze hardware.</p>
</section>
<section id="benchmark-engineering" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-engineering">Benchmark Engineering</h4>
<p>La lotteria hardware si verifica quando un modello di apprendimento automatico funziona in modo eccezionalmente bene o male su una configurazione hardware specifica a causa di compatibilità o incompatibilità impreviste. Il modello non è esplicitamente progettato o ottimizzato per quell’hardware specifico dagli sviluppatori o dagli ingegneri; piuttosto, capita che si allinei o (non si allinei) con le capacità o le limitazioni dell’hardware. In questo caso, le prestazioni del modello sull’hardware sono un prodotto della coincidenza piuttosto che della progettazione.</p>
<p>Contrariamente alla lotteria hardware accidentale, il benchmark engineering implica l’ottimizzazione o la progettazione deliberata di un modello di apprendimento automatico per funzionare eccezionalmente bene su hardware specifico, spesso per vincere benchmark o competizioni. Questa ottimizzazione intenzionale potrebbe includere la modifica dell’architettura, degli algoritmi o dei parametri del modello per sfruttare appieno le funzionalità e le capacità dell’hardware.</p>
</section>
<section id="problema" class="level4">
<h4 class="anchored" data-anchor-id="problema">Problema</h4>
<p>Il benchmark engineering si riferisce alla modifica o all’ottimizzazione di un sistema di intelligenza artificiale per ottimizzare le prestazioni su test di benchmark specifici, spesso a scapito della generalizzabilità o delle prestazioni nel mondo reale. Ciò può includere la regolazione di iperparametri, dati di training o altri aspetti del sistema specificamente per ottenere punteggi elevati sulle metriche di benchmark senza necessariamente migliorare la funzionalità o l’utilità complessiva del sistema.</p>
<p>La motivazione alla base dell’ingegneria dei benchmark spesso deriva dal desiderio di ottenere punteggi di prestazioni elevate per scopi di marketing o competitivi. Punteggi di benchmark elevati possono dimostrare la superiorità di un sistema di intelligenza artificiale rispetto ai concorrenti e possono essere un argomento chiave per la vendita per potenziali utenti o investitori. Questa pressione per ottenere buoni risultati nei benchmark a volte porta a dare priorità alle ottimizzazioni specifiche del benchmark rispetto a miglioramenti più olistici del sistema.</p>
<p>Può comportare diversi rischi e sfide. Uno dei rischi principali è che il sistema di intelligenza artificiale possa funzionare meglio nelle applicazioni del mondo reale rispetto a quanto suggeriscono i punteggi di benchmark. Ciò può portare a insoddisfazione dell’utente, danni alla reputazione e potenziali problemi di sicurezza o etici. Inoltre, l’ingegneria dei benchmark può contribuire a una mancanza di trasparenza e responsabilità nella comunità dell’intelligenza artificiale, poiché può essere difficile discernere quanta parte delle prestazioni di un sistema di intelligenza artificiale sia dovuta a miglioramenti genuini rispetto a ottimizzazioni specifiche del benchmark.</p>
<p>La comunità AI deve dare priorità alla trasparenza e alla responsabilità per mitigare i rischi associati all’ingegneria dei benchmark. Ciò può includere la divulgazione di eventuali ottimizzazioni o modifiche apportate specificamente per i test di benchmark e la fornitura di valutazioni più complete dei sistemi AI che includono metriche delle prestazioni del mondo reale e punteggi di benchmark. I ricercatori e gli sviluppatori devono dare priorità a miglioramenti olistici dei sistemi AI che ne migliorino la generalizzabilità e la funzionalità in varie applicazioni anziché concentrarsi esclusivamente su ottimizzazioni specifiche del benchmark.</p>
</section>
<section id="problemi" class="level4">
<h4 class="anchored" data-anchor-id="problemi">Problemi</h4>
<p>Uno dei problemi principali dell’ingegneria del benchmark è che può compromettere le prestazioni reali dei sistemi di intelligenza artificiale. Quando gli sviluppatori si concentrano sull’ottimizzazione dei loro sistemi per ottenere punteggi elevati in specifici test di benchmark, potrebbero trascurare altri importanti aspetti delle prestazioni del sistema, cruciali nelle applicazioni del mondo reale. Ad esempio, un sistema di intelligenza artificiale progettato per il riconoscimento delle immagini potrebbe essere progettato per funzionare eccezionalmente bene in un test di benchmark che include un set specifico di immagini, ma necessita di aiuto per riconoscere accuratamente immagini leggermente diverse da quelle nel set di test.</p>
<p>Un’altra area di miglioramento con l’ingegneria di benchmark è che può comportare sistemi di intelligenza artificiale privi di generalizzabilità. In altre parole, mentre il sistema può funzionare bene nel test di benchmark, potrebbe aver bisogno di aiuto per gestire una vasta gamma di input o scenari. Ad esempio, un modello di intelligenza artificiale sviluppato per l’elaborazione del linguaggio naturale potrebbe essere progettato per ottenere punteggi elevati in un test di benchmark che include un tipo specifico di testo, ma non riesce a elaborare accuratamente il testo che non rientra in quel tipo specifico.</p>
<p>Può anche portare a risultati fuorvianti. Quando i sistemi di intelligenza artificiale sono progettati per funzionare bene nei test di benchmark, i risultati potrebbero non riflettere accuratamente le reali capacità del sistema. Questo può essere problematico per gli utenti o gli investitori che si affidano ai punteggi di benchmark per prendere decisioni informate su quali sistemi di intelligenza artificiale utilizzare o in cui investire. Ad esempio, un sistema di intelligenza artificiale progettato per ottenere punteggi elevati in un test di benchmark per il riconoscimento vocale potrebbe dover essere più in grado di riconoscere accuratamente il parlato in situazioni reali, portando gli utenti o gli investitori a prendere decisioni basate su informazioni imprecise.</p>
</section>
<section id="attenuazione" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="attenuazione">Attenuazione</h4>
<p>Esistono diversi modi per mitigare l’ingegneria dei benchmark. La trasparenza nel processo di benchmarking è fondamentale per mantenere l’accuratezza e l’affidabilità dei benchmark. Ciò implica la divulgazione chiara delle metodologie, dei set di dati e dei criteri di valutazione utilizzati nei test di benchmark, nonché di eventuali ottimizzazioni o modifiche apportate al sistema di intelligenza artificiale ai fini del benchmark.</p>
<p>Un modo per ottenere trasparenza è attraverso l’uso di benchmark open source. I benchmark open source vengono resi disponibili al pubblico, consentendo a ricercatori, sviluppatori e altre parti interessate di esaminarli, criticarli e contribuire, garantendone così l’accuratezza e l’affidabilità. Questo approccio collaborativo facilita anche la condivisione delle “best practice” e lo sviluppo di benchmark più solidi e completi.</p>
<p>Un esempio è MLPerf Tiny. È un framework open source progettato per semplificare il confronto di diverse soluzioni nel mondo di TinyML. Il suo design modulare consente di sostituire i componenti per il confronto o il miglioramento. Le implementazioni di riferimento, mostrate in verde e arancione in <a href="#fig-ml-perf" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-ml-perf</span></a>, fungono da base per i risultati. TinyML spesso necessita di ottimizzazione nell’intero sistema e gli utenti possono contribuire concentrandosi su parti specifiche, come la quantizzazione. Il design modulare del benchmark consente agli utenti di mostrare i propri contributi e il vantaggio competitivo modificando un’implementazione di riferimento. In breve, MLPerf Tiny offre un modo flessibile e modulare per valutare e migliorare le applicazioni TinyML, semplificando il confronto e il miglioramento di diversi aspetti della tecnologia.</p>
<div id="fig-ml-perf" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-ml-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlperf_tiny.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.5: Design modulare di MLPerf Tiny. Fonte: <span class="citation" data-cites="mattson2020mlperf">Mattson et al. (<a href="#ref-mattson2020mlperf" role="doc-biblioref">2020a</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-mattson2020mlperf" class="csl-entry" role="listitem">
———, et al. 2020a. <span>«<span>MLPerf:</span> <span>An</span> Industry Standard Benchmark Suite for Machine Learning Performance»</span>. <em>IEEE Micro</em> 40 (2): 8–16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div></div></figure>
</div>
<p>Un altro metodo per ottenere trasparenza è attraverso la revisione paritaria dei benchmark. Ciò comporta che esperti indipendenti esaminino e convalidino la metodologia, i set di dati e i risultati del benchmark per garantirne la credibilità e l’affidabilità. La revisione paritaria può fornire un mezzo prezioso per verificare l’accuratezza dei test di benchmark e contribuire a creare fiducia nei risultati.</p>
<p>La standardizzazione dei benchmark è un’altra importante soluzione per mitigare l’ingegneria dei benchmark. I benchmark standardizzati forniscono un quadro comune per la valutazione dei sistemi di intelligenza artificiale, garantendo coerenza e comparabilità tra diversi sistemi e applicazioni. Ciò può essere ottenuto sviluppando standard e “best practice” per l’intero settore per il benchmarking e tramite metriche e criteri di valutazione comuni.</p>
<p>Anche la verifica da parte di terze parti dei risultati può essere preziosa per mitigare l’ingegneria dei benchmark. Ciò comporta che una terza parte indipendente verifichi i risultati di un test di benchmark per garantirne la credibilità e l’affidabilità. La verifica di terze parti può creare fiducia nei risultati e fornire un mezzo prezioso per convalidare le prestazioni e le capacità dei sistemi di intelligenza artificiale.</p>
</section>
</section>
</section>
<section id="benchmarking-del-modello" class="level2 page-columns page-full" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="benchmarking-del-modello"><span class="header-section-number">11.5</span> Benchmarking del Modello</h2>
<p>Il benchmarking dei modelli di machine learning è importante per determinare l’efficacia e l’efficienza di vari algoritmi di apprendimento automatico nella risoluzione di compiti o problemi specifici. Analizzando i risultati ottenuti dal benchmarking, sviluppatori e ricercatori possono identificare i punti di forza e di debolezza dei loro modelli, portando a decisioni più informate sulla selezione del modello e su un’ulteriore ottimizzazione.</p>
<p>L’evoluzione e il progresso dei modelli di apprendimento automatico sono intrinsecamente collegati alla disponibilità e alla qualità dei set di dati. Nell’apprendimento automatico, i dati fungono da materia prima che alimenta gli algoritmi, consentendo loro di apprendere, adattarsi e, in definitiva, eseguire compiti che erano tradizionalmente di dominio degli esseri umani. Pertanto, è importante comprendere questa storia.</p>
<section id="contesto-storico-1" class="level3 page-columns page-full" data-number="11.5.1">
<h3 data-number="11.5.1" class="anchored" data-anchor-id="contesto-storico-1"><span class="header-section-number">11.5.1</span> Contesto Storico</h3>
<p>I dataset di apprendimento automatico hanno una storia ricca e si sono evoluti in modo significativo nel corso degli anni, crescendo in dimensioni, complessità e diversità per soddisfare le richieste sempre crescenti del settore. Diamo un’occhiata più da vicino a questa evoluzione, partendo da uno dei primi e più iconici set di dati: MNIST.</p>
<section id="mnist-1998" class="level4">
<h4 class="anchored" data-anchor-id="mnist-1998">MNIST (1998)</h4>
<p>Il <a href="https://www.tensorflow.org/datasets/catalog/mnist">dataset MNIST</a>, creato da Yann LeCun, Corinna Cortes e Christopher J.C. Burges nel 1998, può essere considerato una pietra miliare nella storia dei dataset di machine learning. Comprende 70.000 immagini in scala di grigi da 28x28 pixel etichettate di cifre scritte a mano (0-9). MNIST è stato ampiamente utilizzato per il benchmarking degli algoritmi nell’elaborazione delle immagini e nell’apprendimento automatico come punto di partenza per molti ricercatori e professionisti. <a href="#fig-mnist" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-mnist</span></a> mostra alcuni esempi di cifre scritte a mano.</p>
<div id="fig-mnist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mnist.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.6: Cifre scritte a mano in MNIST. Fonte: <a href="https://en.wikipedia.org/wiki/File:MnistExamplesModified.png">Suvanjanprasai</a>
</figcaption>
</figure>
</div>
</section>
<section id="imagenet-2009" class="level4">
<h4 class="anchored" data-anchor-id="imagenet-2009">ImageNet (2009)</h4>
<p>Facciamo un salto al 2009 e vediamo l’introduzione di <a href="https://www.tensorflow.org/datasets/catalog/imagenet2012">ImageNet</a>, che ha segnato un balzo significativo nella scala e nella complessità dei dataset. ImageNet è composto da oltre 14 milioni di immagini etichettate che abbracciano più di 20.000 categorie. Fei-Fei Li e il suo team lo hanno sviluppato per far progredire il riconoscimento degli oggetti e la ricerca sulla visione artificiale. Il dataset è diventato sinonimo della ImageNet <a href="https://www.image-net.org/challenges/LSVRC/">Large Scale Visual Recognition Challenge (LSVRC)</a>, una competizione annuale cruciale nello sviluppo di modelli di deep learning, tra cui il famoso AlexNet nel 2012.</p>
</section>
<section id="coco-2014" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="coco-2014">COCO (2014)</h4>
<p>Il <a href="https://cocodataset.org/">Common Objects in Context (COCO) dataset</a> <span class="citation" data-cites="lin2014microsoft">(<a href="#ref-lin2014microsoft" role="doc-biblioref">Lin et al. 2014</a>)</span>, rilasciato nel 2014, ha ulteriormente ampliato il panorama dei set di dati di apprendimento automatico introducendo un set più ricco di annotazioni. COCO è costituito da immagini contenenti scene complesse con più oggetti e ogni immagine è annotata con riquadri di delimitazione degli oggetti, maschere di segmentazione e didascalie, come mostrato in <a href="#fig-coco" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-coco</span></a>. Questo set di dati è stato determinante nel far progredire la ricerca nel rilevamento degli oggetti, nella segmentazione e nella didascalia delle immagini.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2014microsoft" class="csl-entry" role="listitem">
Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, e C Lawrence Zitnick. 2014. <span>«Microsoft coco: <span>Common</span> objects in context»</span>. In <em>Computer Vision<span></span>ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, 740–55. Springer.
</div></div><div id="fig-coco" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coco-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/coco.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coco-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.7: Immagini di esempio dal set di dati COCO. Fonte: <a href="https://cocodataset.org/">Coco</a>
</figcaption>
</figure>
</div>
</section>
<section id="gpt-3-2020" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="gpt-3-2020">GPT-3 (2020)</h4>
<p>Sebbene gli esempi sopra riportati si concentrino principalmente sui dataset di immagini, si sono verificati anche sviluppi significativi nei dataset di testo. Un esempio degno di nota è GPT-3 <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>, sviluppato da OpenAI. GPT-3 è un modello linguistico addestrato su testo Internet eterogeneo. Sebbene il dataset utilizzato per addestrare GPT-3 non sia disponibile al pubblico, il modello stesso, costituito da 175 miliardi di parametri, è una testimonianza della scala e della complessità dei moderni dataset e modelli di apprendimento automatico.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>«Language Models are Few-Shot Learners»</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div></div></section>
<section id="presente-e-futuro" class="level4">
<h4 class="anchored" data-anchor-id="presente-e-futuro">Presente e Futuro</h4>
<p>Oggi disponiamo di una pletora di dataset che abbracciano vari domini, tra cui sanità, finanza, scienze sociali e altro ancora. Le seguenti caratteristiche ci aiutano a classificare lo spazio e la crescita dei dataset di apprendimento automatico che alimentano lo sviluppo del modello.</p>
<ol type="1">
<li><p><strong>Diversità dei Set di Dati:</strong> La varietà di set di dati disponibili per ricercatori e ingegneri si è ampliata notevolmente, coprendo molti campi, tra cui l’elaborazione del linguaggio naturale, il riconoscimento delle immagini e altro ancora. Questa diversità ha alimentato lo sviluppo di modelli di apprendimento automatico specializzati, su misura per attività specifiche, come la traduzione, il riconoscimento vocale e il riconoscimento facciale.</p></li>
<li><p><strong>Volume di Dati:</strong> L’enorme volume di dati che è diventato disponibile nell’era digitale ha anche svolto un ruolo cruciale nel progresso dei modelli di apprendimento automatico. I grandi set di dati consentono ai modelli di catturare la complessità e le sfumature dei fenomeni del mondo reale, portando a previsioni più accurate e affidabili.</p></li>
<li><p><strong>Qualità e Pulizia dei Dati:</strong> La qualità dei dati è un altro fattore critico che influenza le prestazioni dei modelli di apprendimento automatico. Set di dati puliti, ben etichettati e imparziali sono essenziali per modelli di addestramento solidi ed equi.</p></li>
<li><p><strong>Accesso Aperto ai Dati:</strong> La disponibilità di set di dati “open-access” ha contribuito in modo significativo anche al progresso dell’apprendimento automatico. I dati “aperti” consentono ai ricercatori di tutto il mondo di collaborare, condividere approfondimenti e basarsi sul lavoro degli altri, portando a un’innovazione più rapida e allo sviluppo di modelli più avanzati.</p></li>
<li><p><strong>Problemi di Etica e Privacy:</strong> Man mano che i set di dati crescono in dimensioni e complessità, le considerazioni etiche e i problemi di privacy diventano sempre più importanti. È in corso un dibattito sull’equilibrio tra lo sfruttamento dei dati per i progressi dell’apprendimento automatico e la protezione dei diritti alla privacy degli individui.</p></li>
</ol>
<p>Lo sviluppo di modelli di apprendimento automatico si basa in larga misura sulla disponibilità di set di dati diversificati, grandi, di alta qualità e ad accesso libero. Mentre andiamo avanti, affrontare le considerazioni etiche e le preoccupazioni sulla privacy associate all’uso di grandi set di dati è fondamentale per garantire che le tecnologie di apprendimento automatico siano vantaggiose per la società. C’è una crescente consapevolezza che i dati agiscono come carburante per l’apprendimento automatico, guidando e alimentando lo sviluppo di modelli di apprendimento automatico. Di conseguenza, si sta ponendo maggiore attenzione sullo sviluppo dei set di dati stessi. Esploreremo questo aspetto in modo più dettagliato nella sezione del benchmarking dei dati.</p>
</section>
</section>
<section id="metriche-del-modello" class="level3 page-columns page-full" data-number="11.5.2">
<h3 data-number="11.5.2" class="anchored" data-anchor-id="metriche-del-modello"><span class="header-section-number">11.5.2</span> Metriche del Modello</h3>
<p>La valutazione del modello di machine learning si è evoluta da un focus ristretto sulla precisione a un approccio più completo che considera una serie di fattori, da considerazioni etiche e applicabilità nel mondo reale a vincoli pratici come dimensioni ed efficienza del modello. Questo cambiamento riflette la maturazione del campo poiché i modelli di apprendimento automatico vengono sempre più applicati in scenari reali diversi e complessi.</p>
<section id="precisione" class="level4">
<h4 class="anchored" data-anchor-id="precisione">Precisione</h4>
<p>La precisione è una delle metriche più intuitive e comunemente utilizzate per valutare i modelli di apprendimento automatico. In sostanza, la precisione misura la percentuale di previsioni corrette effettuate dal modello rispetto a tutte le previsioni. Ad esempio, immaginiamo di aver sviluppato un modello di apprendimento automatico per classificare le immagini come contenenti o meno un gatto. Se testiamo questo modello su un set di dati di 100 immagini e ne identifica correttamente 90, calcoleremmo la sua precisione al 90%.</p>
<p>Nelle fasi iniziali dell’apprendimento automatico, la precisione era spesso la metrica principale, se non l’unica, considerata quando si valutavano le prestazioni del modello. Ciò è comprensibile, data la sua natura semplice e la facilità di interpretazione. Tuttavia, con il progredire del settore, i limiti del fare affidamento esclusivamente sulla precisione sono diventati più evidenti.</p>
<p>Si consideri l’esempio di un modello di diagnosi medica con una precisione del 95%. Sebbene a prima vista possa sembrare impressionante, dobbiamo guardare più a fondo per valutare appieno le prestazioni del modello. Supponiamo che il modello non riesca a diagnosticare accuratamente condizioni gravi che, sebbene rare, possono avere gravi conseguenze; la sua elevata precisione potrebbe non essere così significativa. Un esempio pertinente di ciò è il <a href="https://about.google/intl/ALL_us/stories/seeingpotential/">modello di apprendimento automatico della retinopatia di Google</a>, progettato per diagnosticare la retinopatia diabetica e l’edema maculare diabetico da fotografie della retina.</p>
<p>Il modello di Google ha dimostrato livelli di precisione impressionanti in contesti di laboratorio. Tuttavia, quando è stato distribuito in ambienti clinici reali in Thailandia, <a href="https://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-clinic-covid-diabetes-retina-disease/">ha dovuto affrontare sfide significative</a>. Nel contesto reale, il modello ha incontrato popolazioni di pazienti diverse, qualità delle immagini variabili e una gamma di diverse condizioni mediche a cui non era stato esposto durante il suo training. Di conseguenza, le sue prestazioni avrebbero potuto essere migliori e ha fatto fatica a mantenere gli stessi livelli di accuratezza osservati in laboratorio. Questo esempio serve come un chiaro promemoria del fatto che, sebbene un’elevata accuratezza sia un attributo importante e desiderabile per un modello di diagnosi medica, deve essere valutata insieme ad altri fattori, come la capacità del modello di generalizzare a diverse popolazioni e gestire condizioni reali diverse e imprevedibili, per comprenderne veramente il valore e il potenziale impatto sull’assistenza ai pazienti.</p>
<p>Allo stesso modo, se il modello funziona bene in media ma mostra significative disparità nelle prestazioni tra diversi gruppi demografici, anche questo sarebbe motivo di preoccupazione.</p>
<p>L’evoluzione dell’apprendimento automatico ha quindi visto uno spostamento verso un approccio più olistico alla valutazione del modello, tenendo conto non solo dell’accuratezza, ma anche di altri fattori cruciali come la correttezza, trasparenza e applicabilità nel mondo reale. Un esempio lampante è il progetto <a href="http://gendershades.org/">Gender Shades</a> del MIT Media Lab, guidato da Joy Buolamwini, che evidenzia significativi pregiudizi razziali e di genere nei sistemi commerciali di riconoscimento facciale. Il progetto ha valutato le prestazioni di tre tecnologie di riconoscimento facciale sviluppate da IBM, Microsoft e Face++. Ha scoperto che tutte presentavano dei pregiudizi, con prestazioni migliori su volti maschili e dalla pelle più chiara rispetto a volti femminili e dalla pelle più scura.</p>
<p>Sebbene l’accuratezza rimanga una metrica fondamentale e preziosa per la valutazione dei modelli di apprendimento automatico, è necessario un approccio più completo per valutare appieno le prestazioni di un modello. Ciò significa considerare metriche aggiuntive che tengano conto di correttezza, trasparenza e applicabilità nel mondo reale, nonché condurre test rigorosi su diversi set di dati per scoprire e mitigare eventuali potenziali pregiudizi. Il passaggio a un approccio più olistico alla valutazione del modello riflette la maturazione del campo e il suo crescente riconoscimento delle implicazioni nel mondo reale e delle considerazioni etiche associate all’implementazione di modelli di apprendimento automatico.</p>
</section>
<section id="equità" class="level4">
<h4 class="anchored" data-anchor-id="equità">Equità</h4>
<p>La correttezza nei modelli di apprendimento automatico è un aspetto multiforme e critico che richiede un’attenzione particolare, in particolare nelle applicazioni ad alto rischio che influenzano significativamente la vita delle persone, come nei processi di approvazione dei prestiti, nelle assunzioni e nella giustizia penale. Si riferisce al trattamento equo di tutti gli individui, indipendentemente dai loro attributi demografici o sociali come razza, genere, età o stato socioeconomico.</p>
<p>Affidarsi semplicemente all’accuratezza può essere insufficiente e potenzialmente fuorviante quando si valutano i modelli. Ad esempio, si consideri un modello di approvazione dei prestiti con un tasso di accuratezza del 95%. Sebbene questa cifra possa sembrare impressionante a prima vista, non rivela come il modello si comporta nei diversi gruppi demografici. Se questo modello discrimina costantemente un gruppo particolare, la sua accuratezza è meno encomiabile e la sua correttezza viene messa in discussione.</p>
<p>La discriminazione può manifestarsi in varie forme, come la discriminazione diretta, in cui un modello utilizza esplicitamente attributi sensibili come razza o genere nel suo processo decisionale, o discriminazione indiretta, in cui variabili apparentemente neutre sono correlate ad attributi sensibili, influenzando indirettamente i risultati del modello. Un esempio infame di quest’ultimo è lo strumento COMPAS utilizzato nel sistema di giustizia penale degli Stati Uniti, che ha mostrato pregiudizi razziali nel prevedere i tassi di recidiva nonostante non utilizzasse esplicitamente la razza come variabile.</p>
<p>Affrontare l’equità implica un attento esame delle prestazioni del modello tra gruppi diversi, identificando potenziali pregiudizi e rettificando le disparità attraverso misure correttive come il ribilanciamento dei set di dati, l’adeguamento dei parametri del modello e l’implementazione di algoritmi consapevoli dell’equità e della correttezza. Ricercatori e professionisti sviluppano continuamente metriche e metodologie su misura per casi d’uso specifici per valutare la correttezza e l’equità in scenari del mondo reale. Ad esempio, l’analisi di impatto disparato, la parità demografica e le pari opportunità sono alcune delle metriche impiegate per valutare l’equità/correttezza.</p>
<p>Inoltre, la trasparenza e l’interpretabilità dei modelli sono fondamentali per raggiungere la correttezza. Comprendere come un modello prende decisioni può rivelare potenziali pregiudizi e consentire alle parti interessate di ritenere responsabili gli sviluppatori. Strumenti open source come <a href="https://ai-fairness-360.org/">AI Fairness 360</a> di IBM e <a href="https://www.tensorflow.org/tfx/guide/fairness_indicators">Fairness Indicators</a> di TensorFlow sono in fase di sviluppo per facilitare le valutazioni dell’equità/correttezza e l’attenuazione dei pregiudizi nei modelli di apprendimento automatico.</p>
<p>Garantire l’equità/correttezza nei modelli di apprendimento automatico, in particolare nelle applicazioni che hanno un impatto significativo sulla vita delle persone, richiede una rigorosa valutazione delle prestazioni del modello in gruppi diversi, un’attenta identificazione e attenuazione dei pregiudizi e l’implementazione di misure di trasparenza e interpretabilità. Affrontando la correttezza in modo completo, possiamo lavorare per sviluppare modelli di apprendimento automatico equi, giusti e vantaggiosi per la società.</p>
</section>
<section id="complessità" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="complessità">Complessità</h4>
<section id="parametri" class="level5">
<h5 class="anchored" data-anchor-id="parametri">Parametri</h5>
<p>Nelle fasi iniziali del machine learning, il benchmarking dei modelli si basava spesso sui conteggi dei parametri come proxy [sostituto] per la complessità del modello. La logica era che più parametri in genere portano a un modello più complesso, che dovrebbe, a sua volta, fornire prestazioni migliori. Tuttavia, questo approccio si è dimostrato inadeguato in quanto deve tenere conto del costo computazionale associato all’elaborazione di molti parametri.</p>
<p>Ad esempio, GPT-3, sviluppato da OpenAI, è un modello linguistico che vanta ben 175 miliardi di parametri. Sebbene raggiunga prestazioni all’avanguardia in varie attività di elaborazione del linguaggio naturale, le sue dimensioni e le risorse computazionali necessarie per eseguirlo lo rendono poco pratico per l’implementazione in molti scenari del mondo reale, in particolare quelli con capacità computazionali limitate.</p>
<p>Affidarsi ai conteggi dei parametri come proxy per la complessità del modello non riesce a considerare anche l’efficienza del modello. Se ottimizzato per l’efficienza, un modello con meno parametri potrebbe essere altrettanto efficace, se non di più, di un modello con un conteggio di parametri più elevato. Ad esempio, MobileNets, sviluppato da Google, è una famiglia di modelli progettati specificamente per dispositivi mobili ed edge. Utilizzano convoluzioni separabili in base alla profondità per ridurre il numero di parametri e i costi computazionali, pur mantenendo prestazioni competitive.</p>
<p>Alla luce di queste limitazioni, il settore si è spostato verso un approccio più olistico al benchmarking dei modelli che considera i conteggi dei parametri e altri fattori cruciali come le operazioni in virgola mobile al secondo (FLOP), il consumo di memoria e la latenza. I FLOP, in particolare, sono emersi come una metrica importante in quanto forniscono una rappresentazione più accurata del carico computazionale imposto da un modello. Questo passaggio a un approccio più completo al benchmarking dei modelli riflette il riconoscimento della necessità di bilanciare prestazioni e praticità, assicurando che i modelli siano efficaci, efficienti e implementabili in scenari reali.</p>
</section>
<section id="flop" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="flop">FLOP</h5>
<p>La dimensione di un modello di apprendimento automatico è un aspetto essenziale che influisce direttamente sulla sua usabilità in scenari pratici, soprattutto quando le risorse computazionali sono limitate. Tradizionalmente, il numero di parametri in un modello veniva spesso utilizzato come proxy per le sue dimensioni, con l’ipotesi di base che più parametri si sarebbero tradotti in prestazioni migliori. Tuttavia, questa visione semplicistica non considera il costo computazionale dell’elaborazione di questi parametri. È qui che entra in gioco il concetto di “floating-point operations per second (FLOP)” [operazioni in virgola mobile al secondo], che fornisce una rappresentazione più accurata del carico computazionale imposto da un modello.</p>
<p>I FLOP misurano il numero di operazioni in virgola mobile eseguite da un modello per generare una previsione. Un modello con molti FLOP richiede risorse computazionali sostanziali per elaborare il vasto numero di operazioni, il che potrebbe renderlo poco pratico per alcune applicazioni. Al contrario, un modello con un conteggio di FLOP inferiore è più leggero e può essere facilmente distribuito in scenari in cui le risorse computazionali sono limitate. <a href="#fig-flops" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-flops</span></a>, da <span class="citation" data-cites="bianco2018benchmark">(<a href="#ref-bianco2018benchmark" role="doc-biblioref">Bianco et al. 2018</a>)</span>, mostra la relazione tra la Top-1 Accuracy su ImageNet (asse <em>y</em>), i G-FLOP del modello (asse <em>x</em>) e il conteggio dei parametri del modello (dimensione del cerchio).</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-flops" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/model_FLOPS_VS_TOP_1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.8: Un grafico che raffigura la top-1 Imagenet Accuracy rispetto al conteggio FLOP di un modello insieme al conteggio dei parametri del modello. La figura mostra un compromesso complessivo tra complessità e accuratezza del modello, sebbene alcune architetture del modello siano più efficienti di altre. Fonte: <span class="citation" data-cites="bianco2018benchmark">Bianco et al. (<a href="#ref-bianco2018benchmark" role="doc-biblioref">2018</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-bianco2018benchmark" class="csl-entry" role="listitem">
Bianco, Simone, Remi Cadene, Luigi Celona, e Paolo Napoletano. 2018. <span>«Benchmark analysis of representative deep neural network architectures»</span>. <em>IEEE access</em> 6: 64270–77.
</div></div></figure>
</div>
<p>Consideriamo un esempio. BERT—Bidirectional Encoder Representations from Transformers <span class="citation" data-cites="devlin2018bert">(<a href="#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span>—è un modello di elaborazione del linguaggio naturale molto diffuso, con oltre 340 milioni di parametri, il che lo rende un modello di grandi dimensioni con elevata accuratezza e prestazioni impressionanti in diverse attività. Tuttavia, le dimensioni di BERT, unite al suo elevato numero di FLOP, lo rendono un modello computazionalmente intensivo che potrebbe non essere adatto per applicazioni in tempo reale o per l’implementazione su dispositivi edge con capacità computazionali limitate.</p>
<div class="no-row-height column-margin column-container"><div id="ref-devlin2018bert" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, e Kristina Toutanova. 2019. <span>«<span>BERT:</span> <span>Pre-training</span> of Deep Bidirectional Transformers for Language Understanding»</span>. In <em>Proceedings of the 2019 Conference of the North</em>, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/n19-1423">https://doi.org/10.18653/v1/n19-1423</a>.
</div></div><p>Alla luce di ciò, c’è stato un crescente interesse nello sviluppo di modelli più piccoli in grado di raggiungere livelli di prestazioni simili alle loro controparti più grandi, pur essendo più efficienti nel carico computazionale. DistilBERT, ad esempio, è una versione più piccola di BERT che mantiene il 97% delle sue prestazioni, pur essendo il 40% più piccola in termini di numero di parametri. La riduzione delle dimensioni si traduce anche in un numero di FLOP inferiore, rendendo DistilBERT una scelta più pratica per scenari con risorse limitate.</p>
<p>In sintesi, mentre il conteggio dei parametri fornisce un’indicazione utile della dimensione del modello, non è una metrica completa in quanto deve considerare il costo computazionale associato all’elaborazione di questi parametri. I FLOP, d’altro canto, offrono una rappresentazione più accurata del carico computazionale di un modello e sono quindi una considerazione essenziale quando si distribuiscono modelli di apprendimento automatico in scenari reali, in particolare quando le risorse computazionali sono limitate. L’evoluzione dal basarsi esclusivamente sul conteggio dei parametri alla considerazione dei FLOP indica una maturazione nel campo, che riflette una maggiore consapevolezza dei vincoli pratici e delle sfide dell’implementazione di modelli di apprendimento automatico in contesti diversi.</p>
</section>
<section id="efficienza" class="level5">
<h5 class="anchored" data-anchor-id="efficienza">Efficienza</h5>
<p>Anche le metriche di efficienza, come il consumo di memoria e la latenza/capacità di elaborazione, hanno acquisito importanza. Queste metriche sono particolarmente cruciali quando si distribuiscono modelli su dispositivi edge o in applicazioni in tempo reale, poiché misurano la velocità con cui un modello può elaborare i dati e la quantità di memoria richiesta. In questo contesto, le curve di Pareto vengono spesso utilizzate per visualizzare il compromesso tra diverse metriche, aiutando le parti interessate a decidere quale modello si adatta meglio alle loro esigenze.</p>
</section>
</section>
</section>
<section id="lezioni-apprese" class="level3 page-columns page-full" data-number="11.5.3">
<h3 data-number="11.5.3" class="anchored" data-anchor-id="lezioni-apprese"><span class="header-section-number">11.5.3</span> Lezioni Apprese</h3>
<p>Il benchmarking dei modelli ci ha offerto diverse preziose intuizioni che possono essere sfruttate per guidare l’innovazione nei benchmark di sistema. La progressione dei modelli di apprendimento automatico è stata profondamente influenzata dall’avvento delle classifiche e dalla disponibilità open source di modelli e set di dati. Questi elementi hanno svolto il ruolo di catalizzatori significativi, spingendo l’innovazione e accelerando l’integrazione di modelli all’avanguardia negli ambienti di produzione. Tuttavia, come approfondiremo ulteriormente, questi non sono gli unici fattori che contribuiscono allo sviluppo dei benchmark di apprendimento automatico.</p>
<p>Le classifiche svolgono un ruolo fondamentale nel fornire un metodo oggettivo e trasparente per ricercatori e professionisti per valutare l’efficacia di diversi modelli, classificandoli in base alle loro prestazioni nei benchmark. Questo sistema promuove un ambiente competitivo, incoraggiando lo sviluppo di modelli che non siano solo accurati ma anche efficienti. L’ImageNet Large Scale Visual Recognition Challenge (ILSVRC) ne è un ottimo esempio, con la sua classifica annuale che contribuisce in modo significativo allo sviluppo di modelli innovativi come AlexNet.</p>
<p>L’accesso open source a modelli e set di dati all’avanguardia diffonde ulteriormente l’apprendimento automatico, facilitando la collaborazione tra ricercatori e professionisti in tutto il mondo. Questo accesso aperto accelera il processo di test, convalida e distribuzione di nuovi modelli in ambienti di produzione, come dimostrato dall’adozione diffusa di modelli come BERT e GPT-3 in varie applicazioni, dall’elaborazione del linguaggio naturale a compiti multimodali più complessi.</p>
<p>Piattaforme di collaborazione della comunità come Kaggle hanno rivoluzionato il settore ospitando competizioni che uniscono data scientist da tutto il mondo per risolvere problemi intricati. Benchmark specifici fungono da paletti per l’innovazione e lo sviluppo di modelli.</p>
<p>Inoltre, la disponibilità di set di dati diversi e di alta qualità è fondamentale per l’addestramento e il test dei modelli di apprendimento automatico. Set di dati come ImageNet hanno svolto un ruolo fondamentale nell’evoluzione dei modelli di riconoscimento delle immagini, mentre ampi set di dati di testo hanno facilitato i progressi nei modelli di elaborazione del linguaggio naturale.</p>
<p>Infine, è necessario supportare i contributi di istituti accademici e di ricerca. Il loro ruolo nella pubblicazione di articoli di ricerca, nella condivisione di risultati in conferenze e nella promozione della collaborazione tra varie istituzioni ha contribuito in modo significativo al progresso dei modelli e dei benchmark di apprendimento automatico.</p>
<section id="tendenze-emergenti" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="tendenze-emergenti">Tendenze Emergenti</h4>
<p>Man mano che i modelli di apprendimento automatico diventano più sofisticati, lo diventano anche i benchmark necessari per valutarli in modo accurato. Ci sono diversi benchmark e dataset emergenti che stanno guadagnando popolarità grazie alla loro capacità di valutare i modelli in scenari più complessi e realistici:</p>
<p><strong>Dataset Multimodali:</strong> Questi set di dati contengono più tipi di dati, come testo, immagini e audio, per rappresentare meglio le situazioni del mondo reale. Un esempio è VQA (Visual Question Answering) <span class="citation" data-cites="antol2015vqa">(<a href="#ref-antol2015vqa" role="doc-biblioref">Antol et al. 2015</a>)</span>, in cui viene testata la capacità dei modelli di rispondere a domande basate su testo sulle immagini.</p>
<div class="no-row-height column-margin column-container"><div id="ref-antol2015vqa" class="csl-entry" role="listitem">
Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, e Devi Parikh. 2015. <span>«<span>VQA:</span> <span>Visual</span> Question Answering»</span>. In <em>2015 IEEE International Conference on Computer Vision (ICCV)</em>, 2425–33. IEEE. <a href="https://doi.org/10.1109/iccv.2015.279">https://doi.org/10.1109/iccv.2015.279</a>.
</div></div><p><strong>Valutazione di Correttezza e Bias:</strong> C’è una crescente attenzione alla creazione di benchmark che valutino l’equità/Correttezza e i bias [pregiudizi] dei modelli di apprendimento automatico. Esempi includono il toolkit <a href="https://ai-fairness-360.org/">AI Fairness 360</a>, che offre un set completo di metriche e set di dati per valutare il bias nei modelli.</p>
<p><strong>Generalizzazione Out-of-Distribution:</strong> Test di quanto bene i modelli funzionano su dati diversi dalla distribuzione di training originale. Questo valuta la capacità del modello di generalizzare a dati nuovi e inediti. Esempi di benchmark sono Wilds <span class="citation" data-cites="koh2021wilds">(<a href="#ref-koh2021wilds" role="doc-biblioref">Koh et al. 2021</a>)</span>, RxRx e ANC-Bench.</p>
<div class="no-row-height column-margin column-container"><div id="ref-koh2021wilds" class="csl-entry" role="listitem">
Koh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. <span>«<span>WILDS:</span> <span>A</span> Benchmark of in-the-Wild Distribution Shifts»</span>. In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, a cura di Marina Meila e Tong Zhang, 139:5637–64. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/koh21a.html">http://proceedings.mlr.press/v139/koh21a.html</a>.
</div><div id="ref-hendrycks2021natural" class="csl-entry" role="listitem">
Hendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, e Dawn Song. 2021. <span>«Natural Adversarial Examples»</span>. In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 15262–71. IEEE. <a href="https://doi.org/10.1109/cvpr46437.2021.01501">https://doi.org/10.1109/cvpr46437.2021.01501</a>.
</div><div id="ref-xie2020adversarial" class="csl-entry" role="listitem">
Xie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, e Quoc V. Le. 2020. <span>«Adversarial Examples Improve Image Recognition»</span>. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 816–25. IEEE. <a href="https://doi.org/10.1109/cvpr42600.2020.00090">https://doi.org/10.1109/cvpr42600.2020.00090</a>.
</div></div><p><strong>Robustezza Avversaria:</strong> Valutazione delle prestazioni del modello in caso di attacchi avversari o perturbazioni ai dati di input. Questo testa la robustezza del modello. Esempi di benchmark sono ImageNet-A <span class="citation" data-cites="hendrycks2021natural">(<a href="#ref-hendrycks2021natural" role="doc-biblioref">Hendrycks et al. 2021</a>)</span>, ImageNet-C <span class="citation" data-cites="xie2020adversarial">(<a href="#ref-xie2020adversarial" role="doc-biblioref">Xie et al. 2020</a>)</span> e CIFAR-10.1.</p>
<p><strong>Prestazioni nel Mondo Reale:</strong> Test di modelli su set di dati del mondo reale che corrispondono da vicino alle attività finali anziché solo su set di dati di benchmark predefiniti. Esempi sono set di dati di imaging medico per attività sanitarie o log di chat di assistenza clienti per sistemi di dialogo.</p>
<p><strong>Efficienza Energetica e di Calcolo:</strong> Benchmark che misurano le risorse di calcolo necessarie per ottenere una particolare accuratezza. Questo valuta l’efficienza del modello. Esempi sono MLPerf e Greenbench, già discussi nella sezione Benchmarking dei sistemi.</p>
<p><strong>Interpretabilità e Spiegabilità:</strong> Benchmark che valutano quanto sia facile comprendere e spiegare la logica interna e le previsioni di un modello. Esempi di parametri sono la fedeltà ai gradienti di input e la coerenza delle spiegazioni.</p>
</section>
</section>
<section id="limitazioni-e-sfide" class="level3" data-number="11.5.4">
<h3 data-number="11.5.4" class="anchored" data-anchor-id="limitazioni-e-sfide"><span class="header-section-number">11.5.4</span> Limitazioni e Sfide</h3>
<p>Sebbene i benchmark dei modelli siano uno strumento essenziale per valutare i modelli di machine learning, è necessario affrontare diverse limitazioni e sfide per garantire che riflettano accuratamente le prestazioni in scenari reali.</p>
<p><strong>Il dataset non corrisponde a scenari reali:</strong> Spesso, i dati utilizzati nei benchmark dei modelli vengono puliti e preelaborati a tal punto che potrebbe essere necessario rappresentare accuratamente i dati che un modello incontrerebbe in applicazioni reali. Questa versione idealizzata dei dati può portare a una sovrastima delle prestazioni di un modello. Nel caso del set di dati ImageNet, le immagini sono ben etichettate e categorizzate. Tuttavia, in uno scenario reale, un modello potrebbe dover gestire immagini sfocate che potrebbero essere meglio illuminate o scattate da angolazioni scomode. Questa discrepanza può influire in modo significativo sulle prestazioni del modello.</p>
<p><strong>Sim2Real Gap:</strong> Il Sim2Real Gap si riferisce alla differenza nelle prestazioni di un modello quando si passa da un ambiente simulato a un ambiente reale. Questo gap è spesso osservato nella robotica, dove un robot addestrato in un ambiente simulato ha difficoltà a svolgere compiti nel mondo reale a causa della complessità e dell’imprevedibilità degli ambienti reali. Un robot addestrato a raccogliere oggetti in un ambiente simulato potrebbe aver bisogno di aiuto per svolgere lo stesso compito nel mondo reale perché l’ambiente simulato non rappresenta accuratamente le complessità della fisica, dell’illuminazione e della variabilità degli oggetti del mondo reale.</p>
<p><strong>Sfide nella Creazione di Dataset:</strong> La creazione di un set di dati per il benchmarking del modello è un’attività impegnativa che richiede un’attenta considerazione di vari fattori come qualità dei dati, diversità e rappresentazione. Come discusso nella sezione di ingegneria dei dati, garantire che i dati siano puliti, imparziali e rappresentativi dello scenario del mondo reale è fondamentale per l’accuratezza e l’affidabilità del benchmark. Ad esempio, quando si crea un set di dati per un’attività correlata all’assistenza sanitaria, è importante assicurarsi che i dati siano rappresentativi dell’intera popolazione e non distorti verso un particolare gruppo demografico. Ciò garantisce che il modello funzioni bene in diverse popolazioni di pazienti.</p>
<p>I benchmark del modello sono essenziali per misurare la capacità di un’architettura di modello di risolvere un’attività fissa, ma è importante affrontare le limitazioni e le sfide ad essi associate. Ciò include il garantire che il set di dati rappresenti accuratamente scenari del mondo reale, affrontare il divario Sim2Real e superare le sfide della creazione di set di dati imparziali e rappresentativi. Affrontando queste sfide e molte altre, possiamo garantire che i benchmark del modello forniscano una valutazione più accurata e affidabile delle prestazioni di un modello in applicazioni del mondo reale.</p>
<p>Lo <a href="https://arxiv.org/pdf/1804.03209.pdf">Speech Commands dataset</a> e il suo successore <a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/fe131d7f5a6b38b23cc967316c13dae2-Paper-round2.pdf">MSWC</a> sono benchmark comuni per una delle applicazioni TinyML per eccellenza, l’individuazione delle parole chiave. I comandi vocali stabiliscono metriche di errore di streaming oltre la precisione di classificazione standard top-1 più pertinenti al caso d’uso di individuazione delle parole chiave. L’utilizzo di metriche pertinenti ai casi è ciò che eleva un dataset a un benchmark del modello.</p>
</section>
</section>
<section id="benchmarking-dei-dati" class="level2 page-columns page-full" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="benchmarking-dei-dati"><span class="header-section-number">11.6</span> Benchmarking dei Dati</h2>
<p>Negli ultimi anni, l’intelligenza artificiale si è concentrata sullo sviluppo di modelli di apprendimento automatico sempre più sofisticati, come i grandi modelli linguistici. L’obiettivo è stato quello di creare modelli in grado di prestazioni di livello umano o sovrumane su un’ampia gamma di attività, addestrandoli su enormi set di dati. Questo approccio incentrato sul modello ha prodotto rapidi progressi, con modelli che hanno ottenuto risultati all’avanguardia su molti benchmark consolidati. <a href="#fig-superhuman-perf" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-superhuman-perf</span></a> mostra le prestazioni dei sistemi di intelligenza artificiale rispetto alle prestazioni umane (contrassegnate dalla linea orizzontale a 0) in cinque applicazioni: riconoscimento della scrittura a mano, riconoscimento vocale, riconoscimento delle immagini, comprensione della lettura e comprensione del linguaggio. Negli ultimi dieci anni, le prestazioni dell’intelligenza artificiale hanno superato quelle degli esseri umani.</p>
<div id="fig-superhuman-perf" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-superhuman-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/dynabench.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-superhuman-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.9: IA e prestazioni umane. Fonte: <span class="citation" data-cites="kiela2021dynabench">Kiela et al. (<a href="#ref-kiela2021dynabench" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>Tuttavia, le crescenti preoccupazioni su questioni come pregiudizi, sicurezza e robustezza persistono anche nei modelli che raggiungono un’elevata accuratezza sui benchmark standard. Inoltre, alcuni set di dati popolari utilizzati per la valutazione dei modelli stanno iniziando a saturarsi, con modelli che raggiungono prestazioni quasi perfette su divisioni di test esistenti <span class="citation" data-cites="kiela2021dynabench">(<a href="#ref-kiela2021dynabench" role="doc-biblioref">Kiela et al. 2021</a>)</span>. Come semplice esempio, ci sono immagini di test nel classico dataset di cifre scritte a mano MNIST che potrebbero sembrare indecifrabili per la maggior parte dei valutatori umani, ma a cui è stata assegnata un’etichetta quando è stato creato il set di dati: i modelli che concordano con quelle etichette potrebbero sembrare esibire prestazioni sovrumane, ma potrebbero invece catturare solo idiosincrasie del processo di etichettatura e acquisizione dalla creazione del set di dati nel 1994. Con lo stesso spirito, i ricercatori di visione artificiale ora chiedono: “Abbiamo finito con ImageNet?” <span class="citation" data-cites="beyer2020we">(<a href="#ref-beyer2020we" role="doc-biblioref">Beyer et al. 2020</a>)</span>. Ciò evidenzia i limiti nell’approccio convenzionale incentrato sul modello di ottimizzazione dell’accuratezza su set di dati fissi tramite innovazioni architettoniche.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kiela2021dynabench" class="csl-entry" role="listitem">
Kiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. <span>«Dynabench: <span>Rethinking</span> Benchmarking in <span>NLP</span>»</span>. In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 4110–24. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.naacl-main.324">https://doi.org/10.18653/v1/2021.naacl-main.324</a>.
</div><div id="ref-beyer2020we" class="csl-entry" role="listitem">
Beyer, Lucas, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, e Aäron van den Oord. 2020. <span>«Are we done with imagenet?»</span> <em>ArXiv preprint</em> abs/2006.07159. <a href="https://arxiv.org/abs/2006.07159">https://arxiv.org/abs/2006.07159</a>.
</div></div><p>Sta emergendo un paradigma alternativo chiamato IA incentrata sui dati. Invece di trattare i dati come statici e concentrarsi strettamente sulle prestazioni del modello, questo approccio riconosce che i modelli sono validi solo quanto i loro dati di training. Quindi, l’enfasi si sposta sulla cura di dataset di alta qualità che riflettano meglio la complessità del mondo reale, sviluppando benchmark di valutazione più informativi e considerando attentamente come i dati vengono campionati, preelaborati e aumentati. L’obiettivo è ottimizzare il comportamento del modello migliorando i dati anziché semplicemente ottimizzando le metriche su set di dati imperfetti. L’intelligenza artificiale incentrata sui dati esamina e migliora criticamente i dati stessi per produrre un’intelligenza artificiale utile. Ciò riflette un’importante evoluzione nella mentalità, poiché il campo affronta le carenze di un benchmarking ristretto.</p>
<p>Questa sezione esplorerà le principali differenze tra gli approcci all’intelligenza artificiale incentrati sui modelli e sui dati. Questa distinzione ha importanti implicazioni sul modo in cui eseguiamo il benchmarking dei sistemi di intelligenza artificiale. In particolare, vedremo come concentrarsi sulla qualità dei dati e sull’efficienza può migliorare direttamente le prestazioni dell’apprendimento automatico come alternativa all’ottimizzazione delle sole architetture dei modelli. L’approccio incentrato sui dati riconosce che i modelli sono validi solo quanto i loro dati di addestramento. Quindi, migliorare la cura dei dati, i benchmark di valutazione e i processi di gestione dei dati può produrre sistemi di intelligenza artificiale più sicuri, più equi e più robusti. Ripensare il benchmarking per dare priorità ai dati insieme ai modelli rappresenta un’importante evoluzione, poiché il settore si sforza di fornire un impatto affidabile nel mondo reale.</p>
<section id="limitazioni-dellia-incentrata-sul-modello" class="level3" data-number="11.6.1">
<h3 data-number="11.6.1" class="anchored" data-anchor-id="limitazioni-dellia-incentrata-sul-modello"><span class="header-section-number">11.6.1</span> Limitazioni dell’IA Incentrata sul Modello</h3>
<p>Nell’era dell’IA incentrata sul modello, una caratteristica importante era lo sviluppo di architetture di modelli complesse. Ricercatori e professionisti hanno dedicato notevoli sforzi alla progettazione di modelli sofisticati e intricati nella ricerca di prestazioni superiori. Ciò ha spesso comportato l’incorporazione di livelli aggiuntivi e la messa a punto di una moltitudine di iperparametri per ottenere miglioramenti nell’accuratezza. Contemporaneamente, c’era una notevole enfasi sullo sfruttamento di algoritmi avanzati. Questi algoritmi, spesso in prima linea nelle ultime ricerche, sono stati impiegati per migliorare le prestazioni dei modelli di IA. L’obiettivo principale di questi algoritmi era ottimizzare il processo di apprendimento dei modelli, estraendo così il massimo delle informazioni dai dati di addestramento.</p>
<p>Sebbene l’approccio incentrato sul modello sia stato centrale per molti progressi nell’IA, ha diverse aree di miglioramento. Innanzitutto, lo sviluppo di architetture di modelli complesse può spesso portare a un overfitting. Questo è quando il modello funziona bene sui dati di addestramento ma deve generalizzare a nuovi dati mai visti. I layer aggiuntivi e la complessità possono catturare il rumore nei dati di training come se fosse un pattern reale, danneggiando le prestazioni del modello su nuovi dati.</p>
<p>In secondo luogo, affidarsi ad algoritmi avanzati può a volte oscurare la reale comprensione del funzionamento di un modello. Questi algoritmi spesso agiscono come una scatola nera, rendendo difficile interpretare il modo in cui il modello prende decisioni. Questa mancanza di trasparenza può essere un ostacolo significativo, specialmente in applicazioni critiche come sanità e finanza, dove la comprensione del processo decisionale del modello è fondamentale.</p>
<p>In terzo luogo, l’enfasi sul raggiungimento di risultati all’avanguardia su set di dati di riferimento può a volte essere fuorviante. Questi dataset devono rappresentare in modo più completo le complessità e la variabilità dei dati del mondo reale. Un modello che funziona bene su un set di dati di riferimento potrebbe non essere necessariamente generalizzato bene a dati nuovi e mai visti in un’applicazione del mondo reale. Questa discrepanza può portare a una falsa fiducia nelle capacità del modello e ostacolarne l’applicabilità pratica.</p>
<p>Infine, l’approccio incentrato sul modello spesso si basa su grandi set di dati etichettati per l’addestramento. Tuttavia, ottenere tali set di dati richiede tempo e impegno in molti scenari del mondo reale. Questa dipendenza da grandi dataset limita anche l’applicabilità dell’IA in domini in cui i dati sono scarsi o costosi da etichettare.</p>
<p>Come risultato delle ragioni di cui sopra, e di molte altre, la comunità dell’IA sta passando a un approccio più incentrato sui dati. Invece di concentrarsi solo sull’architettura del modello, i ricercatori stanno ora dando priorità alla cura di set di dati di alta qualità, allo sviluppo di migliori benchmark di valutazione e alla considerazione di come i dati vengono campionati e preelaborati. L’idea chiave è che i modelli sono validi solo quanto i loro dati di training. Quindi, concentrandoci sull’ottenimento dei dati giusti, potremo sviluppare sistemi di intelligenza artificiale più equi, sicuri e allineati con i valori umani. Questo cambiamento incentrato sui dati rappresenta un importante cambiamento di mentalità man mano che l’intelligenza artificiale progredisce.</p>
</section>
<section id="verso-unintelligenza-artificiale-incentrata-sui-dati" class="level3 page-columns page-full" data-number="11.6.2">
<h3 data-number="11.6.2" class="anchored" data-anchor-id="verso-unintelligenza-artificiale-incentrata-sui-dati"><span class="header-section-number">11.6.2</span> Verso un’Intelligenza Artificiale Incentrata sui Dati</h3>
<p>L’intelligenza artificiale incentrata sui dati è un paradigma che sottolinea l’importanza di dataset di alta qualità, ben etichettati e diversificati nello sviluppo di modelli di intelligenza artificiale. Contrariamente all’approccio incentrato sul modello, che si concentra sulla rifinitura e l’iterazione dell’architettura e dell’algoritmo del modello per migliorare le prestazioni, l’intelligenza artificiale incentrata sui dati dà priorità alla qualità dei dati di input come motore principale per migliorare le prestazioni del modello. I dati di alta qualità sono <a href="https://landing.ai/blog/tips-for-a-data-centric-ai-approach/">puliti, ben etichettati</a> e rappresentativi degli scenari del mondo reale che il modello incontrerà. Al contrario, i dati di bassa qualità possono portare a scarse prestazioni del modello, indipendentemente dalla complessità o dalla sofisticatezza dell’architettura del modello.</p>
<p>L’intelligenza artificiale incentrata sui dati pone una forte enfasi sulla pulizia e l’etichettatura dei dati. La pulizia comporta la rimozione di valori anomali, la gestione dei valori mancanti e la risoluzione di altre incongruenze nei dati. L’etichettatura, d’altro canto, comporta l’assegnazione di etichette significative e accurate ai dati. Entrambi questi processi sono fondamentali per garantire che il modello di intelligenza artificiale venga addestrato su dati accurati e pertinenti. Un altro aspetto importante dell’approccio incentrato sui dati è il “data augmentation” [l’aumento dei dati]. Ciò comporta l’aumento artificiale delle dimensioni e della diversità del set di dati applicando varie trasformazioni ai dati, come rotazione, ridimensionamento e capovolgimento delle immagini di addestramento. L’aumento dei dati aiuta a migliorare la robustezza del modello e le capacità di generalizzazione.</p>
<p>Ci sono diversi vantaggi nell’adottare un approccio incentrato sui dati per lo sviluppo dell’intelligenza artificiale. Innanzitutto, porta a prestazioni del modello migliorate e capacità di generalizzazione. Assicurandosi che il modello venga addestrato su dati diversi e di alta qualità, il modello può generalizzare meglio a dati nuovi e mai visti <span class="citation" data-cites="gaviria2022dollar">(<a href="#ref-gaviria2022dollar" role="doc-biblioref">Mattson et al. 2020b</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>Inoltre, un approccio incentrato sui dati può spesso portare a modelli più semplici che sono più facili da interpretare e gestire. Questo perché l’enfasi è sui dati piuttosto che sull’architettura del modello, il che significa che i modelli più semplici possono raggiungere prestazioni elevate quando addestrati su dati di alta qualità.</p>
<p>Il passaggio all’IA incentrata sui dati rappresenta un significativo cambiamento di paradigma. Dando priorità alla qualità dei dati di input, questo approccio cerca di modellare le prestazioni e le capacità di generalizzazione, portando in ultima analisi a sistemi di intelligenza artificiale più solidi e affidabili. <a href="#fig-data-vs-model" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-data-vs-model</span></a> illustra questa differenza. Mentre continuiamo ad avanzare nella nostra comprensione e applicazione dell’IA, è probabile che l’approccio incentrato sui dati svolga un ruolo importante nel plasmare il futuro di questo campo.</p>
<div id="fig-data-vs-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-vs-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/datavsmodelai.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-vs-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.10: Sviluppo di machine learning incentrato sul modello e incentrato sui dati. Fonte: <a href="https://blogs.nvidia.com/blog/difference-deep-learning-training-inference-ai/">NVIDIA</a>
</figcaption>
</figure>
</div>
</section>
<section id="benchmarking-dei-dati-1" class="level3 page-columns page-full" data-number="11.6.3">
<h3 data-number="11.6.3" class="anchored" data-anchor-id="benchmarking-dei-dati-1"><span class="header-section-number">11.6.3</span> Benchmarking dei Dati</h3>
<p>Il benchmarking dei dati mira a valutare problemi comuni nei set di dati, come l’identificazione di errori di etichetta, caratteristiche rumorose, squilibrio di rappresentazione (ad esempio, su 1000 classi in Imagenet-1K, ci sono oltre 100 categorie che sono solo tipi di cani), squilibrio di classe (dove alcune classi hanno molti più campioni di altre), se i modelli addestrati su un dato set di dati possono generalizzare a caratteristiche fuori distribuzione o quali tipi di bias potrebbero esistere in un dato set di dati <span class="citation" data-cites="gaviria2022dollar">(<a href="#ref-gaviria2022dollar" role="doc-biblioref">Mattson et al. 2020b</a>)</span>. Nella sua forma più semplice, il benchmarking dei dati mira a migliorare l’accuratezza su un set di test rimuovendo campioni di addestramento rumorosi o etichettati in modo errato mantenendo fissa l’architettura del modello. Recenti competizioni nel benchmarking dei dati hanno invitato i partecipanti a presentare nuove strategie di “augmentation” e tecniche di apprendimento attivo.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gaviria2022dollar" class="csl-entry" role="listitem">
Mattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020b. <span>«<span>MLPerf:</span> <span>An</span> Industry Standard Benchmark Suite for Machine Learning Performance»</span>. <em>IEEE Micro</em> 40 (2): 8–16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div></div><p>Le tecniche incentrate sui dati continuano a guadagnare attenzione nel benchmarking, soprattutto perché i modelli di base sono sempre più addestrati su obiettivi auto-supervisionati. Rispetto ai set di dati più piccoli come Imagenet-1K, i set di dati più grandi comunemente usati nell’apprendimento auto-supervisionato, come Common Crawl, OpenImages e LAION-5B, contengono quantità maggiori di rumore, duplicati, bias e dati potenzialmente offensivi.</p>
<p><a href="https://www.datacomp.ai/">DataComp</a> è una competizione di dataset lanciata di recente che ha come obiettivo la valutazione di grandi corpora. DataComp si concentra sulle coppie linguaggio-immagine usate per addestrare i modelli CLIP. Il documento introduttivo rileva che quando il budget di elaborazione totale per l’addestramento è costante, i modelli CLIP più performanti nelle attività downstream, come la classificazione ImageNet, vengono addestrati solo sul 30% del pool di campioni disponibile. Ciò suggerisce che un corretto filtraggio di grandi corpora è fondamentale per migliorare l’accuratezza dei modelli di base. Analogamente, Demystifying CLIP Data <span class="citation" data-cites="xu2023demystifying">(<a href="#ref-xu2023demystifying" role="doc-biblioref">Xu et al. 2023</a>)</span> chiede se il successo di CLIP sia attribuibile all’architettura o al set di dati.</p>
<div class="no-row-height column-margin column-container"><div id="ref-xu2023demystifying" class="csl-entry" role="listitem">
Xu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, e Christoph Feichtenhofer. 2023. <span>«Demystifying <span>CLIP</span> Data»</span>. <em>ArXiv preprint</em> abs/2309.16671. <a href="https://arxiv.org/abs/2309.16671">https://arxiv.org/abs/2309.16671</a>.
</div></div><p><a href="https://www.dataperf.org/">DataPerf</a> è un altro recente lavoro incentrato sul benchmarking dei dati in varie modalità. DataPerf offre round di competizione online per stimolare il miglioramento dei dataset. L’offerta inaugurale è stata lanciata con sfide in termini di visione, parlato, acquisizione, debug e prompt di testo per la generazione di immagini.</p>
</section>
<section id="efficienza-dei-dati" class="level3 page-columns page-full" data-number="11.6.4">
<h3 data-number="11.6.4" class="anchored" data-anchor-id="efficienza-dei-dati"><span class="header-section-number">11.6.4</span> Efficienza dei Dati</h3>
<p>Man mano che i modelli di apprendimento automatico diventano più grandi e complessi e le risorse di elaborazione diventano più scarse di fronte alla crescente domanda, diventa difficile soddisfare i requisiti di elaborazione anche con le flotte di machine learning più grandi. Per superare queste sfide e garantire la scalabilità del sistema di apprendimento automatico, è necessario esplorare nuove opportunità che aumentino gli approcci convenzionali alla scalabilità delle risorse.</p>
<p>Migliorare la qualità dei dati può essere un metodo utile per avere un impatto significativo sulle prestazioni del sistema di apprendimento automatico. Uno dei principali vantaggi del miglioramento della qualità dei dati è il potenziale di poter ridurre le dimensioni del set di dati di addestramento mantenendo o addirittura migliorando le prestazioni del modello. Questa riduzione delle dimensioni dei dati è direttamente correlata alla quantità di tempo di addestramento richiesto, consentendo così ai modelli di convergere in modo più rapido ed efficiente. Raggiungere questo equilibrio tra qualità dei dati e dimensioni del set di dati è un compito impegnativo che richiede lo sviluppo di metodi, algoritmi e tecniche sofisticati.</p>
<p>Possono essere adottati diversi approcci per migliorare la qualità dei dati. Questi metodi includono e non sono limitati a quanto segue:</p>
<ul>
<li><strong>Pulizia dei Dati:</strong> Ciò comporta la gestione dei valori mancanti, la correzione degli errori e la rimozione dei valori anomali. I dati puliti assicurano che il modello non stia imparando da rumore o imprecisioni.</li>
<li><strong>Interpretabilità e Spiegabilità dei Dati:</strong> Le tecniche comuni includono LIME <span class="citation" data-cites="ribeiro2016should">(<a href="#ref-ribeiro2016should" role="doc-biblioref">Ribeiro, Singh, e Guestrin 2016</a>)</span>, che fornisce informazioni sui limiti decisionali dei classificatori, e valori Shapley <span class="citation" data-cites="lundberg2017unified">(<a href="#ref-lundberg2017unified" role="doc-biblioref">Lundberg e Lee 2017</a>)</span>, che stimano l’importanza dei singoli campioni nel contribuire alle previsioni di un modello.</li>
<li><strong>Feature Engineering:</strong> Trasformare o creare nuove funzionalità può migliorare significativamente le prestazioni del modello fornendo informazioni più pertinenti per l’apprendimento.</li>
<li><strong>Data Augmentation:</strong> Aumentare i dati creando nuovi campioni tramite varie trasformazioni può aiutare a migliorare la robustezza e la generalizzazione del modello.</li>
<li><strong>Active Learning:</strong> Questo è un approccio di apprendimento semi-supervisionato in cui il modello interroga attivamente un “oracolo” umano per etichettare i campioni più informativi <span class="citation" data-cites="coleman2022similarity">(<a href="#ref-coleman2022similarity" role="doc-biblioref">Coleman et al. 2022</a>)</span>. Ciò garantisce che il modello venga addestrato sui dati più rilevanti.</li>
<li><strong>Riduzione della Dimensionalità:</strong> Tecniche come PCA possono ridurre il numero di feature in un set di dati, riducendo così la complessità e il tempo di training.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-ribeiro2016should" class="csl-entry" role="listitem">
Ribeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. <span>«<span>”</span> Why should i trust you?<span>”</span> Explaining the predictions of any classifier»</span>. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, 1135–44.
</div><div id="ref-lundberg2017unified" class="csl-entry" role="listitem">
Lundberg, Scott M., e Su-In Lee. 2017. <span>«A Unified Approach to Interpreting Model Predictions»</span>. In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765–74. <a href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</a>.
</div><div id="ref-coleman2022similarity" class="csl-entry" role="listitem">
Coleman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert D. Nowak, Roshan Sumbaly, Matei Zaharia, e I. Zeki Yalniz. 2022. <span>«Similarity Search for Efficient Active Learning and Search of Rare Concepts»</span>. In <em>Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022</em>, 6402–10. AAAI Press. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20591">https://ojs.aaai.org/index.php/AAAI/article/view/20591</a>.
</div></div><p>Esistono molti altri metodi in circolazione. Ma l’obiettivo è lo stesso. Affinare il set di dati e garantire che sia della massima qualità può ridurre il tempo di addestramento necessario per la convergenza dei modelli. Tuttavia, per raggiungere questo obiettivo è necessario sviluppare e implementare metodi, algoritmi e tecniche sofisticati in grado di pulire, preelaborare e aumentare i dati, mantenendo al contempo i campioni più informativi. Questa è una sfida continua che richiederà una continua ricerca e innovazione nel campo dell’apprendimento automatico.</p>
</section>
</section>
<section id="la-tripletta" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="la-tripletta"><span class="header-section-number">11.7</span> La Tripletta</h2>
<p>Mentre i benchmark di sistema, modello e dati sono stati tradizionalmente studiati in modo isolato, si sta diffondendo la consapevolezza che per comprendere e far progredire completamente l’IA, dobbiamo adottare una visione più olistica. Iterando tra sistemi di benchmarking, modelli e dataset insieme, potrebbero emergere nuove intuizioni che non sono evidenti quando questi componenti vengono analizzati separatamente. Le prestazioni del sistema influiscono sulla precisione del modello, le capacità del modello determinano le esigenze dei dati e le caratteristiche dei dati determinano i requisiti del sistema.</p>
<p>Il benchmarking della triade di sistema, modello e dati in modo integrato porterà probabilmente a scoperte sulla progettazione congiunta dei sistemi di IA, sulle proprietà di generalizzazione dei modelli e sul ruolo della cura e della qualità dei dati nel consentire le prestazioni. Piuttosto che benchmark ristretti di singoli componenti, il futuro dell’IA richiede benchmark che valutino la relazione simbiotica tra piattaforme di elaborazione, algoritmi e dati di training. Questa prospettiva a livello di sistema sarà fondamentale per superare le attuali limitazioni e sbloccare il prossimo livello di capacità dell’IA.</p>
<p><a href="#fig-benchmarking-trifecta" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-benchmarking-trifecta</span></a> illustra i molti modi potenziali per far interagire tra loro il benchmarking dei dati, quello dei modelli e quello dell’infrastruttura di sistema. L’esplorazione di queste complesse interazioni probabilmente porterà alla scoperta di nuove opportunità di ottimizzazione e capacità di miglioramento. La tripletta di benchmark di dati, modelli e sistemi offre un ricco spazio per la progettazione congiunta e la co-ottimizzazione.</p>
<div id="fig-benchmarking-trifecta" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-benchmarking-trifecta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/trifecta.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-benchmarking-trifecta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.11: La tripletta del Benchmarking.
</figcaption>
</figure>
</div>
<p>Sebbene questa prospettiva integrata rappresenti una tendenza emergente, il settore ha ancora molto da scoprire sulle sinergie e i compromessi tra questi componenti. Mentre eseguiamo il benchmarking iterativo di combinazioni di dati, modelli e sistemi, emergeranno nuove intuizioni che rimangono nascoste quando questi elementi vengono studiati separatamente. Questo approccio di benchmarking multiforme che traccia le intersezioni di dati, algoritmi e hardware promette di essere una strada fruttuosa per importanti progressi nell’intelligenza artificiale, anche se è ancora nelle sue fasi iniziali.</p>
</section>
<section id="benchmark-per-tecnologie-emergenti" class="level2 page-columns page-full" data-number="11.8">
<h2 data-number="11.8" class="anchored" data-anchor-id="benchmark-per-tecnologie-emergenti"><span class="header-section-number">11.8</span> Benchmark per Tecnologie Emergenti</h2>
<p>Date le loro significative differenze rispetto alle tecniche esistenti, le tecnologie emergenti possono essere particolarmente difficili da progettare per i benchmark. I benchmark standard utilizzati per le tecnologie esistenti potrebbero non evidenziare le caratteristiche chiave del nuovo approccio. Al contrario, i nuovi benchmark potrebbero essere visti come artificiosi per favorire la tecnologia emergente rispetto ad altre. Potrebbero essere così diversi dai benchmark esistenti da non poter essere compresi e perdere significato. Pertanto, i benchmark per le tecnologie emergenti devono bilanciare equità, applicabilità e facilità di confronto con quelli esistenti.</p>
<p>Un esempio di tecnologia emergente in cui il benchmarking si è dimostrato particolarmente difficile è nel <a href="@sec-neuromorphic">Neuromorphic Computing</a>. Utilizzando il cervello come fonte di ispirazione per un’intelligenza generale scalabile, robusta ed efficiente dal punto di vista energetico, il calcolo neuromorfico <span class="citation" data-cites="schuman2022opportunities">(<a href="#ref-schuman2022opportunities" role="doc-biblioref">Schuman et al. 2022</a>)</span> incorpora direttamente meccanismi biologicamente realistici sia negli algoritmi di calcolo che nell’hardware, come le reti neurali spiking <span class="citation" data-cites="maass1997networks">(<a href="#ref-maass1997networks" role="doc-biblioref">Maass 1997</a>)</span> e le architetture non-von Neumann architectures per eseguirle <span class="citation" data-cites="davies2018loihi modha2023neural">(<a href="#ref-davies2018loihi" role="doc-biblioref">Davies et al. 2018</a>; <a href="#ref-modha2023neural" role="doc-biblioref">Modha et al. 2023</a>)</span>. Da una prospettiva full-stack di modelli, tecniche di training e sistemi hardware, il calcolo neuromorfico differisce dall’hardware e dall’intelligenza artificiale convenzionali. Pertanto, esiste una sfida fondamentale nello sviluppo di benchmark equi e utili per guidare la tecnologia.</p>
<div class="no-row-height column-margin column-container"><div id="ref-schuman2022opportunities" class="csl-entry" role="listitem">
Schuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. <span>«Opportunities for neuromorphic computing algorithms and applications»</span>. <em>Nature Computational Science</em> 2 (1): 10–19. <a href="https://doi.org/10.1038/s43588-021-00184-y">https://doi.org/10.1038/s43588-021-00184-y</a>.
</div><div id="ref-maass1997networks" class="csl-entry" role="listitem">
Maass, Wolfgang. 1997. <span>«Networks of spiking neurons: <span>The</span> third generation of neural network models»</span>. <em>Neural Networks</em> 10 (9): 1659–71. <a href="https://doi.org/10.1016/s0893-6080(97)00011-7">https://doi.org/10.1016/s0893-6080(97)00011-7</a>.
</div><div id="ref-davies2018loihi" class="csl-entry" role="listitem">
Davies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. <span>«Loihi: <span>A</span> Neuromorphic Manycore Processor with On-Chip Learning»</span>. <em>IEEE Micro</em> 38 (1): 82–99. <a href="https://doi.org/10.1109/mm.2018.112130359">https://doi.org/10.1109/mm.2018.112130359</a>.
</div><div id="ref-modha2023neural" class="csl-entry" role="listitem">
Modha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. <span>«Neural inference at the frontier of energy, space, and time»</span>. <em>Science</em> 382 (6668): 329–35. <a href="https://doi.org/10.1126/science.adh1174">https://doi.org/10.1126/science.adh1174</a>.
</div><div id="ref-yik2023neurobench" class="csl-entry" role="listitem">
Yik, Jason, Soikat Hasan Ahmed, Zergham Ahmed, Brian Anderson, Andreas G. Andreou, Chiara Bartolozzi, Arindam Basu, et al. 2023. <span>«<span>NeuroBench:</span> <span>Advancing</span> Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking»</span>. <a href="https://arxiv.org/abs/2304.04640">https://arxiv.org/abs/2304.04640</a>.
</div></div><p>Un’iniziativa in corso per sviluppare benchmark neuromorfici standard è NeuroBench <span class="citation" data-cites="yik2023neurobench">(<a href="#ref-yik2023neurobench" role="doc-biblioref">Yik et al. 2023</a>)</span>. Per un benchmarking adeguato del neuromorfico, NeuroBench segue principi di alto livello di <em>inclusività</em> attraverso l’applicabilità di attività e metriche sia alle soluzioni neuromorfiche che non neuromorfiche, <em>attuabilità</em> dell’implementazione utilizzando strumenti comuni e aggiornamenti <em>iterativi</em> per continuare a garantire la pertinenza man mano che il campo cresce rapidamente. NeuroBench e altri benchmark per le tecnologie emergenti forniscono una guida critica per le tecniche future, che potrebbero essere necessarie man mano che i limiti di scalabilità degli approcci esistenti si avvicinano.</p>
</section>
<section id="conclusione" class="level2" data-number="11.9">
<h2 data-number="11.9" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">11.9</span> Conclusione</h2>
<p>Ciò che viene misurato viene migliorato. Questo capitolo ha esplorato la natura multiforme del benchmarking che abbraccia sistemi, modelli e dati. Il benchmarking è importante per far progredire l’IA in quanto fornisce le misurazioni essenziali per monitorare i progressi.</p>
<p>I benchmark del sistema ML consentono l’ottimizzazione attraverso metriche di velocità, efficienza e scalabilità. I benchmark del modello guidano l’innovazione attraverso attività e metriche standardizzate oltre l’accuratezza. I benchmark dei dati evidenziano problemi di qualità, equilibrio e rappresentazione.</p>
<p>È importante notare che la valutazione di questi componenti in modo isolato presenta dei limiti. In futuro, sarà probabilmente utilizzato un benchmarking più integrato per esplorare l’interazione tra benchmark di sistema, modello e dati. Questa visione promette nuove intuizioni sulla progettazione congiunta di dati, algoritmi e infrastrutture.</p>
<p>Man mano che l’IA diventa più complessa, il benchmarking completo diventa ancora più critico. Gli standard devono evolversi continuamente per misurare nuove capacità e rivelare limitazioni. Una stretta collaborazione tra settore, mondo accademico, etichette nazionali, ecc. è essenziale per sviluppare benchmark rigorosi, trasparenti e socialmente utili.</p>
<p>Il benchmarking fornisce la bussola per guidare il progresso nell’IA. Misurando costantemente e condividendo apertamente i risultati, possiamo orientarci verso sistemi performanti, robusti e affidabili. Se l’IA deve soddisfare adeguatamente le esigenze sociali e umane, deve essere sottoposta a benchmarking tenendo a mente gli interessi dell’umanità. A tal fine, ci sono aree emergenti, come il benchmarking della sicurezza dei sistemi di IA, ma questo è per un altro giorno e qualcosa di cui possiamo discutere ulteriormente in “Generative AI”!</p>
<p>Il benchmarking è un argomento in continua evoluzione. L’articolo <a href="https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b">The Olympics of AI: Benchmarking Machine Learning Systems</a> copre diversi sottocampi emergenti nel benchmarking dell’IA, tra cui robotica, realtà estesa e calcolo neuromorfico che incoraggiamo il lettore ad approfondire.</p>
</section>
<section id="sec-benchmarking-ai-resource" class="level2" data-number="11.10">
<h2 data-number="11.10" class="anchored" data-anchor-id="sec-benchmarking-ai-resource"><span class="header-section-number">11.10</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/17udz3gxeYF3r3X1r4ePwu1I9H8ljb53W3ktFSmuDlGs/edit?usp=drive_link&amp;resourcekey=0-Espn0a0x81kl2txL_jIWjw">Perché il benchmarking è importante?</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/18PI_0xmcW1xwwfcjmj25PikqBM_92vQfOXFV4hah-6I/edit?resourcekey=0-KO3HQcDAsR--jgbKd5cp4w#slide=id.g94db9f9f78_0_2">Benchmarking di inferenza embedded.</a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-cuda" class="quarto-xref">Esercizio&nbsp;<span class="quarto-unresolved-ref">exr-cuda</span></a></p></li>
<li><p><a href="#exr-perf" class="quarto-xref">Esercizio&nbsp;<span class="quarto-unresolved-ref">exr-perf</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laboratori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Oltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="pagination-link" aria-label="Accelerazione IA">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="pagination-link" aria-label="Apprendimento On-Device">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro è stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>