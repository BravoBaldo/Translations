<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Benchmarking dell‚ÄôIA ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" rel="next">
<link href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>


</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/benchmarking/benchmarking.it.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2b8d2ba3a08f8b4ab16660e3d0aa1206" class="alert alert-primary hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>‚≠ê [18 Ott] <b>Abbiamo raggiunto 1.000 stelle GitHub</b> üéâ Grazie a voi, Arduino e SEEED hanno donato kit hardware di IA per <a href="https://tinyml.seas.harvard.edu/4D/pastEvents">per i workshop TinyML</a> nei paesi in via di sviluppo <br> üéì [15 Nov] La <a href="https://www.edgeaifoundation.org/">EDGE AI Foundation</a> <strong>equipara i fondi per borse di studio accademiche</strong> per ogni nuovo GitHub ‚≠ê (fino a 10.000 stelle). <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per supportare!</a> üôè <br> üöÄ <b>La nostra missione. 1 ‚≠ê = 1 üë©‚Äçüéì Studente</b>. Ogni stella racconta una storia: studenti che acquisiscono conoscenze e sostenitori che guidano la missione. Insieme, stiamo facendo la differenza.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/about/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/ai/socratiq.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABORATORI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/part_LABS.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">LABORATORI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/part_nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">part_nicla_vision.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/part_xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">part_xiao_esp32s3.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/part_raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">part_raspi.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/part_shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">part_shared.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#panoramica" id="toc-panoramica" class="nav-link active" data-scroll-target="#panoramica"><span class="header-section-number">11.1</span> Panoramica</a></li>
  <li><a href="#contesto-storico" id="toc-contesto-storico" class="nav-link" data-scroll-target="#contesto-storico"><span class="header-section-number">11.2</span> Contesto Storico</a>
  <ul>
  <li><a href="#benchmark-delle-prestazioni" id="toc-benchmark-delle-prestazioni" class="nav-link" data-scroll-target="#benchmark-delle-prestazioni"><span class="header-section-number">11.2.1</span> Benchmark delle Prestazioni</a></li>
  <li><a href="#benchmark-energetici" id="toc-benchmark-energetici" class="nav-link" data-scroll-target="#benchmark-energetici"><span class="header-section-number">11.2.2</span> Benchmark Energetici</a></li>
  <li><a href="#benchmark-personalizzati" id="toc-benchmark-personalizzati" class="nav-link" data-scroll-target="#benchmark-personalizzati"><span class="header-section-number">11.2.3</span> Benchmark Personalizzati</a></li>
  <li><a href="#consenso-della-comunit√†" id="toc-consenso-della-comunit√†" class="nav-link" data-scroll-target="#consenso-della-comunit√†"><span class="header-section-number">11.2.4</span> Consenso della Comunit√†</a></li>
  </ul></li>
  <li><a href="#benchmark-ai-sistema-modello-e-dati" id="toc-benchmark-ai-sistema-modello-e-dati" class="nav-link" data-scroll-target="#benchmark-ai-sistema-modello-e-dati"><span class="header-section-number">11.3</span> Benchmark AI: Sistema, Modello e Dati</a>
  <ul>
  <li><a href="#benchmark-di-sistema" id="toc-benchmark-di-sistema" class="nav-link" data-scroll-target="#benchmark-di-sistema"><span class="header-section-number">11.3.1</span> Benchmark di Sistema</a></li>
  <li><a href="#benchmark-del-modello" id="toc-benchmark-del-modello" class="nav-link" data-scroll-target="#benchmark-del-modello"><span class="header-section-number">11.3.2</span> Benchmark del Modello</a></li>
  <li><a href="#benchmark-dei-dati" id="toc-benchmark-dei-dati" class="nav-link" data-scroll-target="#benchmark-dei-dati"><span class="header-section-number">11.3.3</span> Benchmark dei Dati</a></li>
  </ul></li>
  <li><a href="#benchmarking-di-sistema" id="toc-benchmarking-di-sistema" class="nav-link" data-scroll-target="#benchmarking-di-sistema"><span class="header-section-number">11.4</span> Benchmarking di Sistema</a>
  <ul>
  <li><a href="#granularit√†" id="toc-granularit√†" class="nav-link" data-scroll-target="#granularit√†"><span class="header-section-number">11.4.1</span> Granularit√†</a>
  <ul class="collapse">
  <li><a href="#micro-benchmark" id="toc-micro-benchmark" class="nav-link" data-scroll-target="#micro-benchmark">Micro Benchmark</a></li>
  <li><a href="#macro-benchmark" id="toc-macro-benchmark" class="nav-link" data-scroll-target="#macro-benchmark">Macro Benchmark</a></li>
  <li><a href="#benchmark-end-to-end" id="toc-benchmark-end-to-end" class="nav-link" data-scroll-target="#benchmark-end-to-end">Benchmark end-to-end</a></li>
  <li><a href="#comprendere-i-compromessi" id="toc-comprendere-i-compromessi" class="nav-link" data-scroll-target="#comprendere-i-compromessi">Comprendere i Compromessi</a></li>
  </ul></li>
  <li><a href="#componenti-dei-benchmark" id="toc-componenti-dei-benchmark" class="nav-link" data-scroll-target="#componenti-dei-benchmark"><span class="header-section-number">11.4.2</span> Componenti dei Benchmark</a>
  <ul class="collapse">
  <li><a href="#dataset-standardizzati" id="toc-dataset-standardizzati" class="nav-link" data-scroll-target="#dataset-standardizzati">Dataset Standardizzati</a></li>
  <li><a href="#attivit√†-predefinite" id="toc-attivit√†-predefinite" class="nav-link" data-scroll-target="#attivit√†-predefinite">Attivit√† Predefinite</a></li>
  <li><a href="#metriche-di-valutazione" id="toc-metriche-di-valutazione" class="nav-link" data-scroll-target="#metriche-di-valutazione">Metriche di Valutazione</a></li>
  <li><a href="#baseline-e-modelli-baseline" id="toc-baseline-e-modelli-baseline" class="nav-link" data-scroll-target="#baseline-e-modelli-baseline">Baseline e Modelli Baseline</a></li>
  <li><a href="#specifiche-hardware-e-software" id="toc-specifiche-hardware-e-software" class="nav-link" data-scroll-target="#specifiche-hardware-e-software">Specifiche Hardware e Software</a></li>
  <li><a href="#condizioni-ambientali" id="toc-condizioni-ambientali" class="nav-link" data-scroll-target="#condizioni-ambientali">Condizioni Ambientali</a></li>
  <li><a href="#regole-di-riproducibilit√†" id="toc-regole-di-riproducibilit√†" class="nav-link" data-scroll-target="#regole-di-riproducibilit√†">Regole di Riproducibilit√†</a></li>
  <li><a href="#linee-guida-per-linterpretazione-dei-risultati" id="toc-linee-guida-per-linterpretazione-dei-risultati" class="nav-link" data-scroll-target="#linee-guida-per-linterpretazione-dei-risultati">Linee Guida per l‚ÄôInterpretazione dei Risultati</a></li>
  </ul></li>
  <li><a href="#i-benchmark-del-training" id="toc-i-benchmark-del-training" class="nav-link" data-scroll-target="#i-benchmark-del-training"><span class="header-section-number">11.4.3</span> I Benchmark del Training</a>
  <ul class="collapse">
  <li><a href="#scopo" id="toc-scopo" class="nav-link" data-scroll-target="#scopo">Scopo</a></li>
  <li><a href="#metriche" id="toc-metriche" class="nav-link" data-scroll-target="#metriche">Metriche</a></li>
  <li><a href="#i-benchmark" id="toc-i-benchmark" class="nav-link" data-scroll-target="#i-benchmark">I Benchmark</a></li>
  <li><a href="#caso-duso-di-esempio" id="toc-caso-duso-di-esempio" class="nav-link" data-scroll-target="#caso-duso-di-esempio">Caso d‚ÄôUso di Esempio</a></li>
  </ul></li>
  <li><a href="#benchmark-di-inferenza" id="toc-benchmark-di-inferenza" class="nav-link" data-scroll-target="#benchmark-di-inferenza"><span class="header-section-number">11.4.4</span> Benchmark di Inferenza</a>
  <ul class="collapse">
  <li><a href="#scopo-1" id="toc-scopo-1" class="nav-link" data-scroll-target="#scopo-1">Scopo</a></li>
  <li><a href="#metriche-1" id="toc-metriche-1" class="nav-link" data-scroll-target="#metriche-1">Metriche</a></li>
  <li><a href="#i-benchmark-1" id="toc-i-benchmark-1" class="nav-link" data-scroll-target="#i-benchmark-1">I Benchmark</a></li>
  <li><a href="#caso-duso-di-esempio-1" id="toc-caso-duso-di-esempio-1" class="nav-link" data-scroll-target="#caso-duso-di-esempio-1">Caso d‚ÄôUso di Esempio</a></li>
  </ul></li>
  <li><a href="#selezione-delle-attivit√†-di-benchmark" id="toc-selezione-delle-attivit√†-di-benchmark" class="nav-link" data-scroll-target="#selezione-delle-attivit√†-di-benchmark"><span class="header-section-number">11.4.5</span> Selezione delle Attivit√† di Benchmark</a></li>
  <li><a href="#misura-dellefficienza-energetica" id="toc-misura-dellefficienza-energetica" class="nav-link" data-scroll-target="#misura-dellefficienza-energetica"><span class="header-section-number">11.4.6</span> Misura dell‚ÄôEfficienza Energetica</a></li>
  <li><a href="#esempio-di-benchmark" id="toc-esempio-di-benchmark" class="nav-link" data-scroll-target="#esempio-di-benchmark"><span class="header-section-number">11.4.7</span> Esempio di Benchmark</a>
  <ul class="collapse">
  <li><a href="#task" id="toc-task" class="nav-link" data-scroll-target="#task">Task</a></li>
  <li><a href="#il-dataset" id="toc-il-dataset" class="nav-link" data-scroll-target="#il-dataset">Il Dataset</a></li>
  <li><a href="#modello" id="toc-modello" class="nav-link" data-scroll-target="#modello">Modello</a></li>
  <li><a href="#metriche-2" id="toc-metriche-2" class="nav-link" data-scroll-target="#metriche-2">Metriche</a></li>
  <li><a href="#benchmark-harness" id="toc-benchmark-harness" class="nav-link" data-scroll-target="#benchmark-harness">Benchmark Harness</a></li>
  <li><a href="#la-baseline" id="toc-la-baseline" class="nav-link" data-scroll-target="#la-baseline">La Baseline</a></li>
  </ul></li>
  <li><a href="#sfide-e-limitazioni" id="toc-sfide-e-limitazioni" class="nav-link" data-scroll-target="#sfide-e-limitazioni"><span class="header-section-number">11.4.8</span> Sfide e Limitazioni</a>
  <ul class="collapse">
  <li><a href="#lotteria-hardware" id="toc-lotteria-hardware" class="nav-link" data-scroll-target="#lotteria-hardware">Lotteria Hardware</a></li>
  <li><a href="#benchmark-engineering" id="toc-benchmark-engineering" class="nav-link" data-scroll-target="#benchmark-engineering">Benchmark Engineering</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#benchmarking-del-modello" id="toc-benchmarking-del-modello" class="nav-link" data-scroll-target="#benchmarking-del-modello"><span class="header-section-number">11.5</span> Benchmarking del Modello</a>
  <ul>
  <li><a href="#contesto-storico-1" id="toc-contesto-storico-1" class="nav-link" data-scroll-target="#contesto-storico-1"><span class="header-section-number">11.5.1</span> Contesto Storico</a>
  <ul class="collapse">
  <li><a href="#mnist-1998" id="toc-mnist-1998" class="nav-link" data-scroll-target="#mnist-1998">MNIST (1998)</a></li>
  <li><a href="#imagenet-2009" id="toc-imagenet-2009" class="nav-link" data-scroll-target="#imagenet-2009">ImageNet (2009)</a></li>
  <li><a href="#coco-2014" id="toc-coco-2014" class="nav-link" data-scroll-target="#coco-2014">COCO (2014)</a></li>
  <li><a href="#gpt-3-2020" id="toc-gpt-3-2020" class="nav-link" data-scroll-target="#gpt-3-2020">GPT-3 (2020)</a></li>
  <li><a href="#presente-e-futuro" id="toc-presente-e-futuro" class="nav-link" data-scroll-target="#presente-e-futuro">Presente e Futuro</a></li>
  </ul></li>
  <li><a href="#metriche-del-modello" id="toc-metriche-del-modello" class="nav-link" data-scroll-target="#metriche-del-modello"><span class="header-section-number">11.5.2</span> Metriche del Modello</a>
  <ul class="collapse">
  <li><a href="#precisione" id="toc-precisione" class="nav-link" data-scroll-target="#precisione">Precisione</a></li>
  <li><a href="#equit√†" id="toc-equit√†" class="nav-link" data-scroll-target="#equit√†">Equit√†</a></li>
  <li><a href="#complessit√†" id="toc-complessit√†" class="nav-link" data-scroll-target="#complessit√†">Complessit√†</a></li>
  </ul></li>
  <li><a href="#lezioni-apprese" id="toc-lezioni-apprese" class="nav-link" data-scroll-target="#lezioni-apprese"><span class="header-section-number">11.5.3</span> Lezioni Apprese</a>
  <ul class="collapse">
  <li><a href="#tendenze-emergenti" id="toc-tendenze-emergenti" class="nav-link" data-scroll-target="#tendenze-emergenti">Tendenze Emergenti</a></li>
  </ul></li>
  <li><a href="#limitazioni-e-sfide" id="toc-limitazioni-e-sfide" class="nav-link" data-scroll-target="#limitazioni-e-sfide"><span class="header-section-number">11.5.4</span> Limitazioni e Sfide</a></li>
  </ul></li>
  <li><a href="#benchmarking-dei-dati" id="toc-benchmarking-dei-dati" class="nav-link" data-scroll-target="#benchmarking-dei-dati"><span class="header-section-number">11.6</span> Benchmarking dei Dati</a>
  <ul>
  <li><a href="#limitazioni-dellia-incentrata-sul-modello" id="toc-limitazioni-dellia-incentrata-sul-modello" class="nav-link" data-scroll-target="#limitazioni-dellia-incentrata-sul-modello"><span class="header-section-number">11.6.1</span> Limitazioni dell‚ÄôIA Incentrata sul Modello</a></li>
  <li><a href="#verso-unintelligenza-artificiale-incentrata-sui-dati" id="toc-verso-unintelligenza-artificiale-incentrata-sui-dati" class="nav-link" data-scroll-target="#verso-unintelligenza-artificiale-incentrata-sui-dati"><span class="header-section-number">11.6.2</span> Verso un‚ÄôIntelligenza Artificiale Incentrata sui Dati</a></li>
  <li><a href="#benchmarking-dei-dati-1" id="toc-benchmarking-dei-dati-1" class="nav-link" data-scroll-target="#benchmarking-dei-dati-1"><span class="header-section-number">11.6.3</span> Benchmarking dei Dati</a></li>
  <li><a href="#efficienza-dei-dati" id="toc-efficienza-dei-dati" class="nav-link" data-scroll-target="#efficienza-dei-dati"><span class="header-section-number">11.6.4</span> Efficienza dei Dati</a></li>
  </ul></li>
  <li><a href="#la-tripletta" id="toc-la-tripletta" class="nav-link" data-scroll-target="#la-tripletta"><span class="header-section-number">11.7</span> La Tripletta</a></li>
  <li><a href="#benchmark-per-tecnologie-emergenti" id="toc-benchmark-per-tecnologie-emergenti" class="nav-link" data-scroll-target="#benchmark-per-tecnologie-emergenti"><span class="header-section-number">11.8</span> Benchmark per Tecnologie Emergenti</a></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">11.9</span> Conclusione</a></li>
  <li><a href="#sec-benchmarking-ai-resource" id="toc-sec-benchmarking-ai-resource" class="nav-link" data-scroll-target="#sec-benchmarking-ai-resource"><span class="header-section-number">11.10</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-benchmarking_ai" class="quarto-section-identifier"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-benchmarking-ai-resource">Slide</a>, <a href="#sec-benchmarking-ai-resource">Video</a>, <a href="#sec-benchmarking-ai-resource">Esercizi</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ai_benchmarking.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Foto di un podio su uno sfondo a tema tecnologico. Su ogni piattaforma del podio ci sono chip AI con design intricati. Il chip in alto ha una medaglia d‚Äôoro appesa, il secondo una medaglia d‚Äôargento e il terzo una medaglia di bronzo. Sullo sfondo sono ben visibili striscioni con la scritta ‚ÄúAI Olympics‚Äù.</em></figcaption>
</figure>
</div>
<p>Il benchmarking √® fondamentale per lo sviluppo e la distribuzione di sistemi di machine learning, in particolare applicazioni TinyML. I benchmark consentono agli sviluppatori di misurare e confrontare le prestazioni di diverse architetture di modelli, procedure di training e strategie di distribuzione. Ci√≤ fornisce informazioni chiave su quali approcci funzionano meglio per il problema in questione e sui vincoli dell‚Äôambiente di distribuzione.</p>
<p>Questo capitolo fornir√† una panoramica dei benchmark ML pi√π diffusi, le best practice e come utilizzarli per migliorare lo sviluppo del modello e le prestazioni del sistema. Fornire agli sviluppatori gli strumenti e le conoscenze adeguati per effettuare test di benchmark e ottimizzare in modo efficace i propri sistemi, in particolare quelli TinyML.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere lo scopo e gli obiettivi del benchmarking dei sistemi di intelligenza artificiale, tra cui valutazione delle prestazioni, valutazione delle risorse, validazione e altro ancora.</p></li>
<li><p>Scoprire i principali parametri di riferimento, le metriche e le tendenze dei modelli, tra cui accuratezza, equit√†, complessit√†, prestazioni ed efficienza energetica.</p></li>
<li><p>Acquisire familiarit√† con i componenti chiave di un benchmark di intelligenza artificiale, tra cui set di dati, attivit√†, metriche, linee di base, regole di riproducibilit√† e altro ancora.</p></li>
<li><p>Comprendere la distinzione tra training e inferenza e come ogni fase giustifichi il benchmarking specializzato dei sistemi ML.</p></li>
<li><p>Scoprire i concetti di benchmarking del sistema come produttivit√†, latenza, potenza ed efficienza computazionale.</p></li>
<li><p>Apprezzare l‚Äôevoluzione del benchmarking del modello dall‚Äôaccuratezza a metriche pi√π olistiche come correttezza, robustezza e applicabilit√† nel mondo reale.</p></li>
<li><p>Riconoscere il ruolo crescente del benchmarking dei dati nella valutazione di problemi come bias, rumore, equilibrio e diversit√†.</p></li>
<li><p>Comprendere i limiti della valutazione di modelli, dati e sistemi in isolamento e l‚Äôesigenza emergente di benchmarking integrato.</p></li>
</ul>
</div>
</div>
<section id="panoramica" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="panoramica"><span class="header-section-number">11.1</span> Panoramica</h2>
<p>Il benchmarking fornisce le misure essenziali necessarie per guidare il progresso dell‚Äôapprendimento automatico e comprendere veramente le prestazioni del sistema. Come ha affermato il fisico Lord Kelvin, ‚ÄúMisurare √® conoscere‚Äù. I benchmark ci consentono di conoscere quantitativamente le capacit√† di diversi modelli, software e hardware. Consentono agli sviluppatori di ML di misurare il tempo di inferenza, l‚Äôutilizzo della memoria, il consumo energetico e altre metriche che caratterizzano un sistema. Inoltre, i benchmark creano processi standardizzati per la misurazione, consentendo confronti equi tra diverse soluzioni.</p>
<p>Quando i benchmark vengono mantenuti nel tempo, diventano fondamentali per catturare i progressi attraverso generazioni di algoritmi, set di dati e hardware. I modelli e le tecniche che stabiliscono nuovi record sui benchmark di ML da un anno all‚Äôaltro dimostrano miglioramenti tangibili in ci√≤ che √® possibile per l‚Äôapprendimento automatico ‚Äúon-device‚Äù. Utilizzando i benchmark per misurare, i professionisti di ML possono conoscere le capacit√† reali dei loro sistemi e avere la certezza che ogni passaggio rifletta un progresso autentico verso lo stato dell‚Äôarte.</p>
<p>Il benchmarking ha diversi obiettivi e scopi importanti che guidano la sua implementazione per i sistemi di apprendimento automatico.</p>
<ul>
<li><p><strong>Valutazione delle prestazioni.</strong> Ci√≤ comporta la valutazione di parametri chiave come la velocit√†, l‚Äôaccuratezza e l‚Äôefficienza di un dato modello. Ad esempio, in un contesto TinyML, √® fondamentale confrontare la rapidit√† con cui un assistente vocale pu√≤ riconoscere i comandi, poich√© ci√≤ valuta le prestazioni in tempo reale.</p></li>
<li><p><strong>Valutazione della potenza.</strong> Valutare la potenza assorbita da un carico di lavoro insieme alle sue prestazioni equivale alla sua efficienza energetica. Poich√© l‚Äôimpatto ambientale dell‚Äôelaborazione ML continua a crescere, il benchmarking dell‚Äôenergia pu√≤ consentirci di ottimizzare meglio i sistemi per la sostenibilit√†.</p></li>
<li><p><strong>Valutazione delle risorse.</strong> Ci√≤ significa valutare l‚Äôimpatto del modello sulle risorse critiche del sistema, tra cui durata della batteria, utilizzo della memoria e sovraccarico computazionale. Un esempio rilevante √® il confronto del consumo della batteria di due diversi algoritmi di riconoscimento delle immagini in esecuzione su un dispositivo indossabile.</p></li>
<li><p><strong>Validazione e verifica.</strong> Il benchmarking aiuta a garantire che il sistema funzioni correttamente e soddisfi i requisiti specificati. Un modo √® quello di controllare l‚Äôaccuratezza di un algoritmo, come un cardiofrequenzimetro su uno smartwatch, rispetto alle letture di apparecchiature di livello medico come forma di validazione clinica.</p></li>
<li><p><strong>Analisi competitiva.</strong> Ci√≤ consente di confrontare le soluzioni con le offerte concorrenti sul mercato. Ad esempio, il benchmarking di un modello personalizzato di rilevamento di oggetti rispetto ai benchmark TinyML comuni come MobileNet e Tiny-YOLO.</p></li>
<li><p><strong>Credibilit√†.</strong> I benchmark accurati sostengono la credibilit√† delle soluzioni AI e delle organizzazioni che le sviluppano. Dimostrano un impegno verso trasparenza, onest√† e qualit√†, essenziali per creare fiducia con utenti e stakeholder.</p></li>
<li><p><strong>Regolamentazione e Standardizzazione</strong>. Man mano che il settore dell‚ÄôAI continua a crescere, cresce anche la necessit√† di regolamentazione e standardizzazione per garantire che le soluzioni AI siano sicure, etiche ed efficaci. I benchmark accurati e affidabili sono essenziali per questo quadro normativo, poich√© forniscono i dati e le prove necessari per valutare la conformit√† con gli standard del settore e i requisiti legali.</p></li>
</ul>
<p>Questo capitolo tratter√† i 3 tipi di benchmark AI, le metriche standard, gli strumenti e le tecniche che i progettisti utilizzano per ottimizzare i loro sistemi e le sfide e le tendenze nel benchmarking.</p>
</section>
<section id="contesto-storico" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="contesto-storico"><span class="header-section-number">11.2</span> Contesto Storico</h2>
<section id="benchmark-delle-prestazioni" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="benchmark-delle-prestazioni"><span class="header-section-number">11.2.1</span> Benchmark delle Prestazioni</h3>
<p>L‚Äôevoluzione dei benchmark nell‚Äôinformatica illustra vividamente l‚Äôincessante ricerca dell‚Äôeccellenza e dell‚Äôinnovazione da parte del settore. Nei primi giorni dell‚Äôinformatica, negli anni ‚Äô60 e ‚Äô70, i benchmark erano rudimentali e progettati per i mainframe. Ad esempio, il <a href="https://en.wikipedia.org/wiki/Whetstone_(benchmark)">benchmark Whetstone</a>, che prende il nome dal compilatore Whetstone ALGOL, √® stato uno dei primi test standardizzati per misurare le prestazioni aritmetiche in virgola mobile di una CPU. Questi benchmark pionieristici hanno spinto i produttori a perfezionare le loro architetture e algoritmi per ottenere punteggi di benchmark migliori.</p>
<p>Gli anni ‚Äô80 hanno segnato un cambiamento significativo con l‚Äôascesa dei personal computer. Mentre aziende come IBM, Apple e Commodore gareggiavano per quote di mercato, i benchmark sono diventati strumenti essenziali per consentire una concorrenza leale. I <a href="https://www.spec.org/cpu/">benchmark CPU SPEC</a>, introdotti dalla <a href="https://www.spec.org/">System Performance Evaluation Cooperative (SPEC)</a>, hanno stabilito test standardizzati che consentono confronti oggettivi tra diverse macchine. Questa standardizzazione ha creato un ambiente competitivo, spingendo i produttori di chip e i creatori di sistemi a migliorare continuamente le loro offerte hardware e software.</p>
<p>Gli anni ‚Äô90 hanno portato l‚Äôera delle applicazioni e dei videogiochi ‚Äúgraphics-intensive‚Äù. La necessit√† di benchmark per valutare le prestazioni delle schede grafiche ha portato alla creazione di <a href="https://www.3dmark.com/">3DMark</a> da parte di Futuremark. Mentre i giocatori e i professionisti cercavano schede grafiche ad alte prestazioni, aziende come NVIDIA e AMD sono state spinte a una rapida innovazione, portando a importanti progressi nella tecnologia GPU come gli shader programmabili.</p>
<p>Gli anni 2000 hanno visto un‚Äôimpennata di telefoni cellulari e dispositivi portatili come i tablet. Con la portabilit√† √® arrivata la sfida di bilanciare prestazioni e consumo energetico. Benchmark come <a href="https://bapco.com/products/mobilemark-2014/">MobileMark</a> di BAPCo hanno valutato velocit√† e durata della batteria. Ci√≤ ha spinto le aziende a sviluppare System-on-Chip (SOC) pi√π efficienti dal punto di vista energetico, portando all‚Äôemergere di architetture come ARM che hanno dato priorit√† all‚Äôefficienza energetica.</p>
<p>L‚Äôattenzione dell‚Äôultimo decennio si √® spostata verso il cloud computing, i big data e l‚Äôintelligenza artificiale. I provider di servizi cloud come Amazon Web Services e Google Cloud competono su prestazioni, scalabilit√† e convenienza. I benchmark specifici del cloud come <a href="http://cloudsuite.ch/">CloudSuite</a> sono diventati essenziali, spingendo i provider a ottimizzare la propria infrastruttura per servizi migliori.</p>
</section>
<section id="benchmark-energetici" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="benchmark-energetici"><span class="header-section-number">11.2.2</span> Benchmark Energetici</h3>
<p>Il consumo energetico e le preoccupazioni ambientali hanno acquisito importanza negli ultimi anni, rendendo il benchmarking energetico sempre pi√π importante nel settore. Questo cambiamento √® iniziato a met√† degli anni 2000, quando i processori e i sistemi hanno iniziato a raggiungere i limiti di raffreddamento e la scalabilit√† √® diventata un aspetto cruciale della costruzione di sistemi su larga scala grazie ai progressi di Internet. Da allora, le considerazioni energetiche si sono espanse fino a comprendere tutte le aree dell‚Äôinformatica, dai dispositivi personali ai data center su larga scala.</p>
<p>Il benchmarking energetico mira a misurare l‚Äôefficienza energetica dei sistemi informatici, valutando le prestazioni in relazione al consumo energetico. Ci√≤ √® fondamentale per diversi motivi:</p>
<ul>
<li><strong>Impatto ambientale:</strong> Con la crescente impronta di carbonio del settore tecnologico, c‚Äô√® un‚Äôurgente necessit√† di ridurre il consumo energetico.</li>
<li><strong>Costi operativi:</strong> Le spese energetiche costituiscono una parte significativa dei costi operativi del data center.</li>
<li><strong>Longevit√† del dispositivo:</strong> Per i dispositivi mobili, l‚Äôefficienza energetica ha un impatto diretto sulla durata della batteria e sull‚Äôesperienza utente.</li>
</ul>
<p>In questo ambito sono emersi diversi benchmark chiave:</p>
<ul>
<li><strong>SPEC Power:</strong> Introdotto nel 2007, <a href="https://www.spec.org/power/">SPEC Power</a> √® stato uno dei primi benchmark standard del settore per la valutazione delle caratteristiche di potenza e prestazioni dei server.</li>
<li><strong>Green500:</strong> L‚Äôelenco <a href="https://top500.org/lists/green500/">Green500</a> classifica i supercomputer in base all‚Äôefficienza energetica, integrando l‚Äôelenco TOP500 incentrato sulle prestazioni.</li>
<li><strong>Energy Star:</strong> Pur non essendo un benchmark in s√©, il programma di certificazione <a href="https://www.energystar.gov/products/computers">ENERGY STAR for Computers</a> ha spinto i produttori a migliorare l‚Äôefficienza energetica dell‚Äôelettronica di consumo.</li>
</ul>
<p>Il benchmarking energetico affronta sfide uniche, come la contabilizzazione di diversi carichi di lavoro e configurazioni di sistema e la misura accurata del consumo energetico su una gamma di hardware che varia da microWatt a megawatt nel consumo energetico. Man mano che l‚ÄôIA e l‚Äôedge computing continuano a crescere, √® probabile che il benchmarking energetico diventi ancora pi√π critico, guidando lo sviluppo di ottimizzazioni hardware e software AI specializzate ed efficienti dal punto di vista energetico.</p>
</section>
<section id="benchmark-personalizzati" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="benchmark-personalizzati"><span class="header-section-number">11.2.3</span> Benchmark Personalizzati</h3>
<p>Oltre ai benchmark standard del settore, ci sono benchmark personalizzati specificamente progettati per soddisfare i requisiti unici di una particolare applicazione o attivit√†. Sono personalizzati in base alle esigenze specifiche dell‚Äôutente o dello sviluppatore, assicurando che le metriche delle prestazioni siano direttamente pertinenti all‚Äôuso previsto del modello o del sistema di intelligenza artificiale. I benchmark personalizzati possono essere creati da singole organizzazioni, ricercatori o sviluppatori e sono spesso utilizzati insieme ai benchmark standard del settore per fornire una valutazione completa delle prestazioni dell‚Äôintelligenza artificiale.</p>
<p>Ad esempio, un ospedale potrebbe sviluppare un benchmark per valutare un modello di intelligenza artificiale per prevedere la riammissione dei pazienti. Questo benchmark incorporerebbe metriche pertinenti alla popolazione di pazienti dell‚Äôospedale, come dati demografici, anamnesi e fattori sociali. Allo stesso modo, il benchmark di rilevamento delle frodi di un istituto finanziario potrebbe concentrarsi sull‚Äôidentificazione accurata delle transazioni fraudolente riducendo al minimo i falsi positivi. Nel settore automobilistico, un benchmark di veicoli autonomi potrebbe dare priorit√† alle prestazioni in diverse condizioni, alla risposta agli ostacoli e alla sicurezza. I rivenditori potrebbero confrontare i sistemi di raccomandazione utilizzando il tasso di clic, il tasso di conversione e la soddisfazione del cliente. Le aziende manifatturiere potrebbero confrontare i sistemi di controllo qualit√† in base all‚Äôidentificazione dei difetti, all‚Äôefficienza e alla riduzione degli sprechi. In ogni settore, i benchmark personalizzati forniscono alle organizzazioni criteri di valutazione su misura per le loro esigenze e il loro contesto unici. Ci√≤ consente una valutazione pi√π significativa di quanto i sistemi di intelligenza artificiale soddisfino i requisiti.</p>
<p>Il vantaggio dei benchmark personalizzati risiede nella loro flessibilit√† e pertinenza. Possono essere progettati per testare aspetti specifici delle prestazioni critici per il successo della soluzione di intelligenza artificiale nella sua applicazione prevista. Ci√≤ consente una valutazione pi√π mirata e accurata delle capacit√† del modello o del sistema di intelligenza artificiale. I benchmark personalizzati forniscono anche informazioni preziose sulle prestazioni delle soluzioni di intelligenza artificiale in scenari reali, il che pu√≤ essere cruciale per identificare potenziali problemi e aree di miglioramento.</p>
<p>Nell‚Äôintelligenza artificiale, i benchmark svolgono un ruolo cruciale nel guidare il progresso e l‚Äôinnovazione. Sebbene i benchmark siano stati a lungo utilizzati nell‚Äôinformatica, la loro applicazione all‚Äôapprendimento automatico √® relativamente recente. I benchmark incentrati sull‚Äôintelligenza artificiale forniscono metriche standardizzate per valutare e confrontare le prestazioni di diversi algoritmi, architetture di modelli e piattaforme hardware.</p>
</section>
<section id="consenso-della-comunit√†" class="level3" data-number="11.2.4">
<h3 data-number="11.2.4" class="anchored" data-anchor-id="consenso-della-comunit√†"><span class="header-section-number">11.2.4</span> Consenso della Comunit√†</h3>
<p>Una prerogativa fondamentale affinch√© un benchmark abbia un impatto √® che deve riflettere le priorit√† e i valori condivisi della pi√π ampia comunit√† di ricerca. I benchmark progettati in modo isolato rischiano di non ottenere accettazione se trascurano metriche chiave considerate importanti dai gruppi leader. Attraverso uno sviluppo collaborativo con la partecipazione aperta di laboratori accademici, aziende e altri stakeholder, i benchmark possono incorporare un contributo collettivo su capacit√† critiche che vale la pena misurare. Ci√≤ aiuta a garantire che i benchmark valutino aspetti che la comunit√† concorda siano essenziali per far progredire il campo. Il processo di raggiungimento dell‚Äôallineamento su attivit√† e metriche supporta di per s√© la convergenza su ci√≤ che conta di pi√π.</p>
<p>Inoltre, i benchmark pubblicati con ampia co-paternit√† da istituzioni rispettate hanno autorit√† e validit√† che convincono la comunit√† ad adottarli come standard affidabili. I benchmark percepiti come distorti da particolari interessi aziendali o istituzionali generano scetticismo. Anche il coinvolgimento continuo della comunit√† attraverso workshop e sfide √® fondamentale dopo la versione iniziale, ed √® ci√≤ che, ad esempio, ha portato al successo di ImageNet. Col progredire della ricerca, la partecipazione collettiva consente un continuo perfezionamento ed espansione dei benchmark nel tempo.</p>
<p>Infine, rilasciare benchmark sviluppati dalla comunit√† con accesso aperto ne promuove l‚Äôadozione e l‚Äôuso coerente. Fornendo codice open source, documentazione, modelli e infrastrutture, riduciamo le barriere all‚Äôingresso, consentendo ai gruppi di confrontare le soluzioni su un piano di parit√† con le implementazioni standardizzate. Questa coerenza √® essenziale per confronti equi. Senza coordinamento, laboratori e aziende potrebbero implementare i benchmark in modo diverso, il che pu√≤ compromettere la riproducibilit√† e la comparabilit√† dei risultati.</p>
<p>Il consenso della comunit√† conferisce ai benchmark una rilevanza duratura, mentre la frammentazione confonde. Attraverso lo sviluppo collaborativo e un funzionamento trasparente, i benchmark possono diventare standard autorevoli per monitorare i progressi. Molti dei benchmark di cui parliamo in questo capitolo sono stati sviluppati e creati dalla comunit√†, per la comunit√†, ed √® questo che alla fine ha portato al loro successo.</p>
</section>
</section>
<section id="benchmark-ai-sistema-modello-e-dati" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="benchmark-ai-sistema-modello-e-dati"><span class="header-section-number">11.3</span> Benchmark AI: Sistema, Modello e Dati</h2>
<p>La necessit√† di un benchmarking completo diventa fondamentale man mano che i sistemi AI diventano pi√π complessi e onnipresenti. In questo contesto, i benchmark sono spesso classificati in tre categorie principali: Hardware, Modello e Dati. Analizziamo perch√© ognuno di questi gruppi √® essenziale e il significato della valutazione dell‚ÄôAI da queste tre dimensioni distinte:</p>
<section id="benchmark-di-sistema" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="benchmark-di-sistema"><span class="header-section-number">11.3.1</span> Benchmark di Sistema</h3>
<p>I calcoli AI, in particolare quelli nel deep learning, richiedono molte risorse. L‚Äôhardware su cui vengono eseguiti questi calcoli svolge un ruolo importante nel determinare la velocit√†, l‚Äôefficienza e la scalabilit√† delle soluzioni AI. Di conseguenza, i benchmark hardware aiutano a valutare le prestazioni di CPU, GPU, TPU e altri acceleratori nelle attivit√† AI. Comprendendone le prestazioni, gli sviluppatori possono scegliere quali piattaforme hardware si adattano meglio a specifiche applicazioni AI. Inoltre, i produttori di hardware utilizzano questi benchmark per identificare aree di miglioramento, guidando l‚Äôinnovazione nei progetti di chip specifici per AI.</p>
</section>
<section id="benchmark-del-modello" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="benchmark-del-modello"><span class="header-section-number">11.3.2</span> Benchmark del Modello</h3>
<p>L‚Äôarchitettura, le dimensioni e la complessit√† dei modelli AI variano notevolmente. Modelli diversi hanno diverse esigenze di calcolo e offrono diversi livelli di accuratezza ed efficienza. I benchmark dei modelli aiutano a valutare le prestazioni di varie architetture AI su attivit√† standardizzate. Forniscono informazioni sulla velocit√†, l‚Äôaccuratezza e le richieste di risorse di diversi modelli. Eseguendo il benchmarking dei modelli, i ricercatori possono identificare le architetture pi√π performanti per attivit√† specifiche, guidando la comunit√† AI verso soluzioni pi√π efficienti ed efficaci. Inoltre, questi benchmark aiutano a monitorare i progressi della ricerca sull‚Äôintelligenza artificiale, mostrando i progressi nella progettazione e nell‚Äôottimizzazione dei modelli.</p>
</section>
<section id="benchmark-dei-dati" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="benchmark-dei-dati"><span class="header-section-number">11.3.3</span> Benchmark dei Dati</h3>
<p>Nell‚Äôapprendimento automatico, i dati sono fondamentali perch√© la qualit√†, la scala e la diversit√† dei set di dati influiscono direttamente sull‚Äôefficacia e sulla generalizzazione del modello. I benchmark dei dati si concentrano sui set di dati utilizzati nel training e nella valutazione. Forniscono set di dati standardizzati che la comunit√† pu√≤ utilizzare per addestrare e testare i modelli, garantendo parit√† di condizioni per i confronti. Inoltre, questi parametri di riferimento evidenziano le sfide relative alla qualit√† dei dati, alla diversit√† e alla rappresentazione, spingendo la comunit√† ad affrontare i ‚Äúbias‚Äù [pregiudizi] e i ‚Äúgap‚Äù [lacune] nei dati di addestramento. Comprendendo i benchmark dei dati, i ricercatori possono anche valutare come i modelli potrebbero comportarsi in scenari reali, garantendo robustezza e affidabilit√†.</p>
<p>Nelle restanti sezioni, discuteremo ciascuno di questi tipi di benchmark. L‚Äôattenzione sar√† rivolta a un‚Äôesplorazione approfondita dei benchmark di sistema, poich√© sono fondamentali per comprendere e migliorare le prestazioni del sistema di apprendimento automatico. Parleremo brevemente dei benchmark dei modelli e dei dati per una prospettiva completa, ma l‚Äôenfasi e la maggior parte del contenuto saranno dedicati ai benchmark di sistema.</p>
</section>
</section>
<section id="benchmarking-di-sistema" class="level2 page-columns page-full" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="benchmarking-di-sistema"><span class="header-section-number">11.4</span> Benchmarking di Sistema</h2>
<section id="granularit√†" class="level3 page-columns page-full" data-number="11.4.1">
<h3 data-number="11.4.1" class="anchored" data-anchor-id="granularit√†"><span class="header-section-number">11.4.1</span> Granularit√†</h3>
<p>Il benchmarking del sistema di apprendimento automatico fornisce un approccio strutturato e sistematico per valutare le prestazioni di un sistema in diverse dimensioni. Data la complessit√† dei sistemi ML, possiamo analizzare le loro prestazioni attraverso diversi livelli di granularit√† e ottenere una visione completa dell‚Äôefficienza del sistema, identificare potenziali colli di bottiglia e individuare le aree di miglioramento. A tal fine, nel corso degli anni si sono evoluti vari tipi di benchmark che continuano a persistere.</p>
<p><a href="#fig-granularity" class="quarto-xref">Figura&nbsp;<span>11.1</span></a> illustra i diversi livelli di granularit√† di un sistema ML. A livello di applicazione, i benchmark end-to-end valutano le prestazioni complessive del sistema, considerando fattori come la pre-elaborazione dei dati, l‚Äôaddestramento del modello e l‚Äôinferenza. Mentre a livello di modello, i benchmark si concentrano sulla valutazione dell‚Äôefficienza e dell‚Äôaccuratezza di modelli specifici. Ci√≤ include la valutazione di quanto bene i modelli si generalizzano a nuovi dati e della loro efficienza computazionale durante l‚Äôaddestramento e l‚Äôinferenza. Inoltre, il benchmarking pu√≤ estendersi all‚Äôinfrastruttura hardware e software, esaminando le prestazioni di singoli componenti come GPU o TPU.</p>
<div id="fig-granularity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/end2end.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.1: Granularit√† del sistema ML.
</figcaption>
</figure>
</div>
<section id="micro-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="micro-benchmark">Micro Benchmark</h4>
<p>I micro-benchmark sono specializzati e valutano componenti distinti o operazioni specifiche all‚Äôinterno di un processo di apprendimento automatico pi√π ampio. Questi benchmark si concentrano su singole attivit√†, offrendo approfondimenti sulle richieste computazionali di un particolare layer di rete neurale, l‚Äôefficienza di un‚Äôunica tecnica di ottimizzazione o la produttivit√† di una specifica funzione di attivazione. Ad esempio, i professionisti potrebbero utilizzare i micro-benchmark per misurare il tempo di calcolo richiesto da un layer convoluzionale in un modello di deep learning o per valutare la velocit√† di preelaborazione che alimenta i dati nel modello. Tali valutazioni granulari sono fondamentali per la messa a punto e l‚Äôottimizzazione di aspetti discreti dei modelli, assicurando che ogni componente funzioni al massimo del suo potenziale.</p>
<p>Questi tipi di microbenchmark includono lo zoom su operazioni o componenti molto specifiche della pipeline AI, come le seguenti:</p>
<ul>
<li><p><strong>Operazioni Tensoriali:</strong> Librerie come <a href="https://developer.nvidia.com/cudnn">cuDNN</a> (di NVIDIA) spesso hanno benchmark per misurare le prestazioni di singole operazioni tensoriali, come convoluzioni o moltiplicazioni di matrici, che sono fondamentali per i calcoli del deep learning.</p></li>
<li><p><strong>Funzioni di Attivazione:</strong> Benchmark che misurano la velocit√† e l‚Äôefficienza di varie funzioni di attivazione come ReLU, Sigmoid o Tanh in isolamento.</p></li>
<li><p><strong>Benchmark di Layer:</strong> Valutazioni dell‚Äôefficienza computazionale di distinti layer di rete neurale, come blocchi LSTM o Transformer, quando si opera su dimensioni di input standardizzate.</p></li>
</ul>
<p>Esempio: <a href="https://github.com/baidu-research/DeepBench">DeepBench</a>, introdotto da Baidu, √® un buon benchmark che valuta le operazioni fondamentali di deep learning, come quelle menzionate sopra. DeepBench valuta le prestazioni delle operazioni di base nei modelli di deep learning, fornendo informazioni su come diverse piattaforme hardware gestiscono l‚Äôaddestramento e l‚Äôinferenza delle reti neurali.</p>
<div id="exr-cuda" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;11.1: Benchmarking di Sistema - Operazioni Tensoriali
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Ci si √® mai chiesto come mai i filtri immagine diventano cos√¨ veloci? Librerie speciali come cuDNN potenziano quei calcoli su determinati hardware. In questo Colab, useremo cuDNN con PyTorch per velocizzare il filtraggio delle immagini. Lo si consideri un piccolo benchmark, che mostra come il software giusto pu√≤ sbloccare la potenza della GPU!</p>
<p><a href="https://colab.research.google.com/github/RyanHartzell/cudnn-image-filtering/blob/master/notebooks/CuDNN%20Image%20Filtering%20Tutorial%20Using%20PyTorch.ipynb#scrollTo=1sWeXdYsATrr"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="macro-benchmark" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="macro-benchmark">Macro Benchmark</h4>
<p>I macro benchmark forniscono una visione olistica, valutando le prestazioni end-to-end di interi modelli di apprendimento automatico o sistemi di ML completi. Invece di concentrarsi sulle singole operazioni, i macro benchmark valutano l‚Äôefficacia collettiva dei modelli in scenari o attivit√† del mondo reale. Ad esempio, un macro benchmark potrebbe valutare le prestazioni complete di un modello di apprendimento profondo che esegue la classificazione delle immagini su un set di dati come <a href="https://www.image-net.org/">ImageNet</a>. Ci√≤ include la misura dell‚Äôaccuratezza, della velocit√† di calcolo e del consumo di risorse. Allo stesso modo, si potrebbero misurare il tempo e le risorse cumulativi necessari per addestrare un modello di elaborazione del linguaggio naturale su corpora di testo estesi o valutare le prestazioni di un intero sistema di raccomandazione, dall‚Äôinserimento dei dati agli output finali specifici dell‚Äôutente.</p>
<p>Esempi: Questi benchmark valutano il modello di intelligenza artificiale:</p>
<ul>
<li><p><a href="https://github.com/mlcommons/inference">MLPerf Inference</a> <span class="citation" data-cites="reddi2020mlperf">(<a href="../../../references.it.html#ref-reddi2020mlperf" role="doc-biblioref">Reddi et al. 2020</a>)</span>: Un set di benchmark standard per misurare le prestazioni di software e hardware di apprendimento automatico. MLPerf ha una suite di benchmark dedicati per scale specifiche, come <a href="https://github.com/mlcommons/mobile_app_open">MLPerf Mobile</a> per dispositivi di classe mobile e <a href="https://github.com/mlcommons/tiny">MLPerf Tiny</a>, che si concentra su microcontrollori e altri dispositivi con risorse limitate.</p></li>
<li><p><a href="https://github.com/eembc/mlmark">MLMark di EEMBC</a>: Una suite di benchmarking per valutare le prestazioni e l‚Äôefficienza energetica dei dispositivi embedded che eseguono carichi di lavoro di apprendimento automatico. Questo benchmark fornisce informazioni su come diverse piattaforme hardware gestiscono attivit√† come il riconoscimento delle immagini o l‚Äôelaborazione audio.</p></li>
<li><p><a href="https://ai-benchmark.com/">AI-Benchmark</a> <span class="citation" data-cites="ignatov2018ai">(<a href="../../../references.it.html#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2019</a>)</span>: Uno strumento di benchmarking progettato per dispositivi Android, valuta le prestazioni delle attivit√† di intelligenza artificiale sui dispositivi mobili, comprendendo vari scenari del mondo reale come il riconoscimento delle immagini, l‚Äôanalisi dei volti e il riconoscimento ottico dei caratteri.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-reddi2020mlperf" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. <span>¬´MLPerf Inference Benchmark¬ª</span>. In <em>2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>, 446‚Äì59. IEEE; IEEE. <a href="https://doi.org/10.1109/isca45697.2020.00045">https://doi.org/10.1109/isca45697.2020.00045</a>.
</div><div id="ref-ignatov2018ai" class="csl-entry" role="listitem">
Ignatov, Andrey, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, e Luc Van Gool. 2019. <span>¬´AI Benchmark: All About Deep Learning on Smartphones in 2019¬ª</span>. In <em>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em>, 3617‚Äì35. IEEE. <a href="https://doi.org/10.1109/iccvw.2019.00447">https://doi.org/10.1109/iccvw.2019.00447</a>.
</div></div></section>
<section id="benchmark-end-to-end" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-end-to-end">Benchmark end-to-end</h4>
<p>I benchmark end-to-end forniscono una valutazione completa che si estende oltre i confini del modello di ML stesso. Invece di concentrarsi esclusivamente sull‚Äôefficienza o l‚Äôaccuratezza computazionale di un modello di apprendimento automatico, questi benchmark comprendono l‚Äôintera pipeline di un sistema di IA. Ci√≤ include la pre-elaborazione iniziale dei dati, le prestazioni del modello principale, la post-elaborazione degli output del modello e altri componenti integrali come l‚Äôarchiviazione e le interazioni di rete.</p>
<p>La pre-elaborazione dei dati √® la prima fase in molti sistemi di IA, trasformando i dati grezzi in un formato adatto per l‚Äôaddestramento o l‚Äôinferenza del modello. L‚Äôefficienza, la scalabilit√† e l‚Äôaccuratezza di queste fasi di pre-elaborazione sono vitali per le prestazioni complessive del sistema. I benchmark end-to-end valutano questa fase, assicurando che la pulizia dei dati, la normalizzazione, l‚Äôaumento o qualsiasi altro processo di trasformazione non diventi un collo di bottiglia.</p>
<p>Anche la fase di post-elaborazione √® al centro dell‚Äôattenzione. Ci√≤ comporta l‚Äôinterpretazione degli output grezzi del modello, eventualmente la conversione dei punteggi in categorie significative, il filtraggio dei risultati o persino l‚Äôintegrazione con altri sistemi. Nelle applicazioni del mondo reale, questa fase √® fondamentale per fornire informazioni fruibili e i benchmark end-to-end ne garantiscono l‚Äôefficienza e l‚Äôefficacia.</p>
<p>Oltre alle operazioni di base dell‚ÄôIA, altri componenti del sistema sono importanti per le prestazioni complessive e l‚Äôesperienza utente. Le soluzioni di archiviazione, basate su cloud, on-premise o ibride, possono avere un impatto significativo sui tempi di recupero e archiviazione dei dati, in particolare con vasti set di dati di IA. Allo stesso modo, le interazioni di rete, vitali per le soluzioni di IA basate su cloud o per i sistemi distribuiti, possono diventare colli di bottiglia delle prestazioni se non ottimizzate. I benchmark end-to-end valutano in modo olistico questi componenti, assicurando che l‚Äôintero sistema funzioni senza problemi, dal recupero dei dati alla consegna dell‚Äôoutput finale.</p>
<p>Ad oggi, non esistono benchmark end-to-end pubblici che tengano conto del ruolo dell‚Äôarchiviazione dei dati, della rete e delle prestazioni di elaborazione. Si pu√≤ sostenere che MLPerf Training and Inference si avvicini all‚Äôidea di un benchmark end-to-end, ma si concentrano esclusivamente sulle prestazioni del modello ML e non rappresentano scenari di distribuzione nel mondo reale di come i modelli vengono utilizzati sul campo. Tuttavia, forniscono un segnale molto utile che aiuta a valutare le prestazioni del sistema AI.</p>
<p>Data la specificit√† intrinseca del benchmarking end-to-end, viene in genere eseguito internamente in un‚Äôazienda ‚Äústrumentando‚Äù [inserendo punti di controllo] distribuzioni di produzione reali di AI. Ci√≤ consente agli ingegneri di avere una comprensione e una ripartizione realistiche delle prestazioni, ma data la sensibilit√† e la specificit√† delle informazioni, raramente vengono segnalate all‚Äôesterno dell‚Äôazienda.</p>
</section>
<section id="comprendere-i-compromessi" class="level4">
<h4 class="anchored" data-anchor-id="comprendere-i-compromessi">Comprendere i Compromessi</h4>
<p>Diversi problemi sorgono nelle diverse fasi di un sistema di intelligenza artificiale. I micro-benchmark aiutano a mettere a punto i singoli componenti, i macro-benchmark aiutano a perfezionare le architetture o gli algoritmi del modello e i benchmark end-to-end guidano l‚Äôottimizzazione dell‚Äôintero flusso di lavoro. Comprendendo dove si trova un problema, gli sviluppatori possono applicare ottimizzazioni mirate.</p>
<p>Inoltre, mentre i singoli componenti di un sistema di intelligenza artificiale potrebbero funzionare in modo ottimale in isolamento, possono emergere colli di bottiglia quando interagiscono. I benchmark end-to-end, in particolare, sono fondamentali per garantire che l‚Äôintero sistema, quando funziona collettivamente, soddisfi gli standard di prestazioni ed efficienza desiderati.</p>
<p>Infine, le organizzazioni possono prendere decisioni informate su dove allocare le risorse individuando colli di bottiglia o inefficienze nelle prestazioni. Ad esempio, se i micro-benchmark rivelano inefficienze in specifiche operazioni tensoriali, gli investimenti possono essere indirizzati verso acceleratori hardware specializzati. Al contrario, se i benchmark end-to-end indicano problemi di recupero dei dati, gli investimenti potrebbero essere incanalati verso soluzioni di archiviazione migliori.</p>
</section>
</section>
<section id="componenti-dei-benchmark" class="level3" data-number="11.4.2">
<h3 data-number="11.4.2" class="anchored" data-anchor-id="componenti-dei-benchmark"><span class="header-section-number">11.4.2</span> Componenti dei Benchmark</h3>
<p>In sostanza, un benchmark AI √® pi√π di un semplice test o punteggio; √® un framework di valutazione completo. Per comprenderlo in modo approfondito, analizziamo i componenti tipici che compongono un benchmark AI.</p>
<section id="dataset-standardizzati" class="level4">
<h4 class="anchored" data-anchor-id="dataset-standardizzati">Dataset Standardizzati</h4>
<p>I set di dati fungono da base per la maggior parte dei benchmark AI. Forniscono un set di dati coerente su cui i modelli vengono addestrati e valutati, garantendo parit√† di condizioni per i confronti.</p>
<p>Esempio: ImageNet, un set di dati su larga scala contenente milioni di immagini etichettate che abbracciano migliaia di categorie, √® uno standard di benchmarking popolare per le attivit√† di classificazione delle immagini.</p>
</section>
<section id="attivit√†-predefinite" class="level4">
<h4 class="anchored" data-anchor-id="attivit√†-predefinite">Attivit√† Predefinite</h4>
<p>Un benchmark dovrebbe avere un obiettivo o un compito chiaro che i modelli mirano a raggiungere. Questo compito definisce il problema che il sistema AI sta cercando di risolvere.</p>
<p>Esempio: I compiti per i benchmark di elaborazione del linguaggio naturale potrebbero includere analisi del ‚Äúsentiment‚Äù, riconoscimento di entit√† denominate o traduzione automatica.</p>
</section>
<section id="metriche-di-valutazione" class="level4">
<h4 class="anchored" data-anchor-id="metriche-di-valutazione">Metriche di Valutazione</h4>
<p>Una volta definito un task, i benchmark richiedono parametri per quantificare le prestazioni. Questi parametri offrono misure oggettive per confrontare diversi modelli o sistemi. Nei task di classificazione, parametri come accuratezza, precisione, richiamo e <a href="https://en.wikipedia.org/wiki/F-score">punteggio F1</a> sono comunemente utilizzati. Errori quadratici medi o assoluti potrebbero essere utilizzati per i task di regressione. Possiamo anche misurare la potenza consumata dall‚Äôesecuzione del benchmark per calcolare l‚Äôefficienza energetica.</p>
</section>
<section id="baseline-e-modelli-baseline" class="level4">
<h4 class="anchored" data-anchor-id="baseline-e-modelli-baseline">Baseline e Modelli Baseline</h4>
<p>I benchmark spesso includono modelli ‚Äúbaseline‚Äù o implementazioni di riferimento. Di solito servono come punti di partenza o standard minimi di prestazione per confrontare nuovi modelli o nuove tecniche. I modelli ‚Äúbaseline‚Äù aiutano i ricercatori a misurare l‚Äôefficacia di nuovi algoritmi.</p>
<p>Nelle suite di benchmark, modelli semplici come la regressione lineare o le reti neurali di base sono spesso le baseline comuni. Queste forniscono un contesto quando si valutano modelli pi√π complessi. Confrontando questi modelli pi√π semplici, i ricercatori possono quantificare i miglioramenti derivanti da approcci avanzati.</p>
<p>Le metriche delle prestazioni variano in base all‚Äôattivit√†, ma ecco alcuni esempi:</p>
<ul>
<li>Le attivit√† di classificazione utilizzano metriche come accuratezza, precisione, richiamo e punteggio F1.</li>
<li>Le attivit√† di regressione utilizzano spesso l‚Äôerrore quadratico medio o l‚Äôerrore assoluto medio.</li>
</ul>
</section>
<section id="specifiche-hardware-e-software" class="level4">
<h4 class="anchored" data-anchor-id="specifiche-hardware-e-software">Specifiche Hardware e Software</h4>
<p>Data la variabilit√† introdotta da diverse configurazioni hardware e software, i benchmark spesso specificano o documentano gli ambienti hardware e software in cui vengono condotti i test.</p>
<p>Esempio: Un benchmark AI potrebbe indicare che le valutazioni sono state condotte su una GPU NVIDIA Tesla V100 utilizzando TensorFlow v2.4.</p>
</section>
<section id="condizioni-ambientali" class="level4">
<h4 class="anchored" data-anchor-id="condizioni-ambientali">Condizioni Ambientali</h4>
<p>Poich√© fattori esterni possono influenzare i risultati del benchmark, √® essenziale controllare o documentare condizioni come temperatura, fonte di alimentazione o processi di background del sistema.</p>
<p>Esempio: I benchmark AI mobili potrebbero specificare che i test sono stati condotti a temperatura ambiente con dispositivi collegati a una fonte di alimentazione per eliminare le variazioni del livello della batteria.</p>
</section>
<section id="regole-di-riproducibilit√†" class="level4">
<h4 class="anchored" data-anchor-id="regole-di-riproducibilit√†">Regole di Riproducibilit√†</h4>
<p>Per garantire che i benchmark siano credibili e possano essere replicati da altri nella comunit√†, spesso includono protocolli dettagliati che coprono tutto, dai ‚Äúrandom seed‚Äù utilizzati agli iperparametri esatti.</p>
<p>Esempio: Un benchmark per un‚Äôattivit√† di learning di rinforzo potrebbe specificare gli episodi esatti dell‚Äôaddestramento, i rapporti di esplorazione-sfruttamento e le strutture di ricompensa utilizzate.</p>
</section>
<section id="linee-guida-per-linterpretazione-dei-risultati" class="level4">
<h4 class="anchored" data-anchor-id="linee-guida-per-linterpretazione-dei-risultati">Linee Guida per l‚ÄôInterpretazione dei Risultati</h4>
<p>Oltre ai punteggi o alle metriche pure, i benchmark spesso forniscono linee guida o contesto per interpretare i risultati, aiutando i professionisti a comprendere le implicazioni pi√π ampie.</p>
<p>Esempio: Un benchmark potrebbe evidenziare che, sebbene il Modello A abbia ottenuto un punteggio pi√π alto del Modello B in termini di accuratezza, offre migliori prestazioni in tempo reale, rendendolo pi√π adatto per applicazioni sensibili al fattore tempo.</p>
</section>
</section>
<section id="i-benchmark-del-training" class="level3 page-columns page-full" data-number="11.4.3">
<h3 data-number="11.4.3" class="anchored" data-anchor-id="i-benchmark-del-training"><span class="header-section-number">11.4.3</span> I Benchmark del Training</h3>
<p>Il ciclo di vita dello sviluppo di un modello di apprendimento automatico prevede due fasi critiche: addestramento e inferenza. Il training [addestramento] rappresenta la fase in cui il sistema elabora e assimila dati grezzi per adattare e perfezionare i propri parametri. Il benchmarking della fase di training rivela come le scelte nella pipeline di dati, soluzioni di storage, architetture di modelli, risorse di elaborazione, impostazioni di iperparametri e algoritmi di ottimizzazione influiscono sull‚Äôefficienza e sulle richieste di risorse del training del modello. L‚Äôobiettivo √® garantire che il sistema ML possa apprendere in modo efficiente dai dati, ottimizzando sia le prestazioni del modello sia l‚Äôutilizzo delle risorse del sistema.</p>
<section id="scopo" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="scopo">Scopo</h4>
<p>Dal punto di vista dei sistemi, l‚Äôaddestramento dei modelli di apprendimento automatico richiede molte risorse, soprattutto quando si lavora con modelli di grandi dimensioni. Questi modelli spesso contengono miliardi o addirittura trilioni di parametri addestrabili e richiedono enormi quantit√† di dati, spesso su una scala di molti terabyte. Ad esempio, <a href="https://arxiv.org/abs/2005.14165">GPT-3 di OpenAI</a> <span class="citation" data-cites="brown2020language">(<a href="../../../references.it.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> ha 175 miliardi di parametri, √® stato addestrato su 45 TB di dati compressi in testo normale e ha richiesto 3.640 petaflop-giorni di elaborazione per il pre-addestramento. I benchmark di training ML valutano i sistemi e le risorse necessari per gestire il carico computazionale dell‚Äôaddestramento di tali modelli.</p>
<div class="no-row-height column-margin column-container"></div><p>Anche l‚Äôarchiviazione e la distribuzione efficienti dei dati durante l‚Äôaddestramento svolgono un ruolo importante nel processo di addestramento. Ad esempio, in un modello di apprendimento automatico che prevede riquadri di delimitazione attorno agli oggetti in un‚Äôimmagine, potrebbero essere necessarie migliaia di immagini. Tuttavia, caricare un intero set di dati di immagini nella memoria √® in genere irrealizzabile, quindi i professionisti si affidano ai caricatori di dati (come discusso in <a href="../frameworks/frameworks.it.html#sec-frameworks-data-loaders" class="quarto-xref"><span>Sezione 6.4.3.1</span></a>) dai framework ML. Il training di successo del modello dipende dalla consegna tempestiva ed efficiente dei dati, rendendo essenziale il benchmarking di strumenti come caricatori di dati, pipeline di dati, velocit√† di pre-elaborazione e tempi di recupero dell‚Äôarchiviazione per comprenderne l‚Äôimpatto sulle prestazioni del training.</p>
<p>La selezione dell‚Äôhardware √® un altro fattore chiave nel training dei sistemi di machine learning, in quanto pu√≤ avere un impatto significativo sui tempi. I benchmark di training valutano l‚Äôutilizzo di CPU, GPU, memoria e rete durante la fase di training per guidare le ottimizzazioni del sistema. √à essenziale comprendere come vengono utilizzate le risorse: le GPU vengono sfruttate appieno? C‚Äô√® un sovraccarico di memoria non necessario? I benchmark possono scoprire colli di bottiglia o inefficienze nell‚Äôutilizzo delle risorse, con conseguenti risparmi sui costi e miglioramenti delle prestazioni.</p>
<p>In molti casi, l‚Äôutilizzo di un singolo acceleratore hardware, come una singola GPU, non √® sufficiente per soddisfare le esigenze computazionali del training di modelli su larga scala. I modelli di apprendimento automatico vengono spesso addestrati in data center con pi√π GPU o TPU, dove il calcolo distribuito consente l‚Äôelaborazione parallela tra i nodi. I benchmark di addestramento valutano l‚Äôefficienza con cui il sistema si ridimensiona su pi√π nodi, gestisce lo sharding dei dati e gestisce sfide come guasti o taglio dei nodi durante l‚Äôaddestramento.</p>
</section>
<section id="metriche" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="metriche">Metriche</h4>
<p>Se viste da una prospettiva di sistema, le metriche di training offrono informazioni che trascendono gli indicatori di prestazioni algoritmiche convenzionali. Queste metriche misurano l‚Äôefficacia di apprendimento del modello e misurano l‚Äôefficienza, la scalabilit√† e la robustezza dell‚Äôintero sistema ML durante la fase di training. Analizziamo pi√π a fondo queste metriche e il loro significato.</p>
<p>Le seguenti metriche sono spesso considerate importanti:</p>
<ol type="1">
<li><p><strong>Tempo di training:</strong> Il tempo necessario per addestrare un modello da zero fino a raggiungere un livello di prestazioni soddisfacente. Misura direttamente le risorse di elaborazione necessarie per addestrare un modello. Ad esempio, <a href="https://arxiv.org/abs/1810.04805">il BERT di Google</a> <span class="citation" data-cites="devlin2018bert">(<a href="../../../references.it.html#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span> √® un modello di elaborazione del linguaggio naturale che richiede diversi giorni per l‚Äôaddestramento su un corpus enorme di dati di testo utilizzando pi√π GPU. Il lungo tempo di training √® una sfida significativa in termini di consumo di risorse e costi. In alcuni casi, i benchmark possono invece misurare la produttivit√† del training (campioni di training per unit√† di tempo). La produttivit√† pu√≤ essere calcolata molto pi√π velocemente e facilmente del tempo di addestramento, ma potrebbe oscurare le metriche che ci interessano davvero (ad esempio, il tempo di addestramento).</p></li>
<li><p><strong>Scalabilit√†:</strong> Quanto bene il processo di addestramento pu√≤ gestire gli aumenti delle dimensioni dei dati o della complessit√† del modello. La scalabilit√† pu√≤ essere valutata misurando il tempo di addestramento, l‚Äôutilizzo della memoria e altri consumi di risorse all‚Äôaumentare delle dimensioni dei dati o della complessit√† del modello. Ad esempio, l‚Äôaddestramento del GPT-3 di OpenAI ha richiesto notevoli sforzi ingegneristici per adattare il processo di training a numerosi nodi GPU, in modo da gestire le enormi dimensioni del modello. Ci√≤ ha comportato l‚Äôutilizzo di hardware specializzato, addestramento distribuito e altre tecniche per garantire che il modello potesse essere addestrato in modo efficiente.</p></li>
<li><p><strong>Utilizzo delle Risorse:</strong> La misura in cui il processo di addestramento utilizza le risorse di calcolo disponibili come CPU, GPU, memoria e I/O del disco. Un elevato utilizzo delle risorse pu√≤ indicare un processo di training efficiente, mentre un basso utilizzo pu√≤ suggerire colli di bottiglia o inefficienze. Ad esempio, il training di una rete neurale convoluzionale (CNN) per la classificazione delle immagini richiede notevoli risorse GPU. L‚Äôutilizzo di configurazioni multi-GPU e l‚Äôottimizzazione del codice di training per l‚Äôaccelerazione GPU possono migliorare notevolmente l‚Äôutilizzo delle risorse e l‚Äôefficienza del training.</p></li>
<li><p><strong>Consumo di Memoria:</strong> La quantit√† di memoria utilizzata dal processo di training. Il consumo di memoria pu√≤ essere un fattore limitante per il training di modelli o set di dati di grandi dimensioni. Ad esempio, i ricercatori di Google hanno dovuto affrontare notevoli sfide di consumo di memoria durante il training di BERT. Il modello ha centinaia di milioni di parametri, che richiedono grandi quantit√† di memoria. I ricercatori hanno dovuto sviluppare tecniche per ridurre il consumo di memoria, come il checkpointing del gradiente e il parallelismo del modello.</p></li>
<li><p><strong>Consumo Energetico:</strong> L‚Äôenergia consumata durante il training. Man mano che i modelli di apprendimento automatico diventano pi√π complessi, il consumo energetico √® diventato un fattore importante da considerare. Il training di grandi modelli di apprendimento automatico pu√≤ consumare molta energia, e quindi molto carbonio. Ad esempio, si √® stimato che l‚Äôaddestramento di GPT-3 di OpenAI abbia un‚Äôimpronta di carbonio equivalente a un viaggio in auto di 700.000 chilometri (~435,000 miglia).</p></li>
<li><p><strong>Throughput:</strong> l numero di campioni di addestramento elaborati per unit√† di tempo. Un throughput [produttivit√†] pi√π elevato indica generalmente un processo di addestramento pi√π efficiente. La produttivit√† √® una metrica importante da considerare quando si addestra un sistema di raccomandazione per una piattaforma di e-commerce. Una produttivit√† elevata assicura che il modello possa elaborare rapidamente grandi volumi di dati di interazione dell‚Äôutente, il che √® fondamentale per mantenere la pertinenza e l‚Äôaccuratezza delle raccomandazioni. Ma √® anche importante capire come bilanciare la produttivit√† con i limiti di latenza. Pertanto, un vincolo di produttivit√† limitato dalla latenza viene spesso imposto agli accordi sul livello di servizio per le distribuzioni di applicazioni del data center.</p></li>
<li><p><strong>Costo:</strong> Il costo della training di un modello pu√≤ includere sia risorse computazionali che umane. Il costo √® importante quando si considera la praticit√† e la fattibilit√† del training di modelli grandi o complessi. Si stima che l‚Äôaddestramento di modelli di linguaggio grandi come GPT-3 costi milioni di dollari. Questo costo include risorse computazionali, elettriche e umane necessarie per lo sviluppo e l‚Äôaddestramento del modello.</p></li>
<li><p><strong>Tolleranza agli Errori e Robustezza:</strong> La capacit√† del processo di training di gestire guasti o errori senza bloccarsi o produrre risultati errati. Questo √® importante per garantire l‚Äôaffidabilit√† del processo di addestramento. Errori di rete o malfunzionamenti hardware possono verificarsi in uno scenario reale in cui un modello di apprendimento automatico viene addestrato su un sistema distribuito. Negli ultimi anni, √® diventato abbondantemente chiaro che gli errori derivanti dalla corruzione ‚Äúsilenziosa‚Äù dei dati sono emersi come un problema importante. Un processo di addestramento affidabile e tollerante agli errori pu√≤ recuperare da tali errori senza compromettere l‚Äôintegrit√† del modello.</p></li>
<li><p><strong>Facilit√† d‚ÄôUso e Flessibilit√†:</strong> La facilit√† con cui il processo di addestramento pu√≤ essere impostato e utilizzato e la sua flessibilit√† nella gestione di diversi tipi di dati e modelli. In aziende come Google, l‚Äôefficienza pu√≤ talvolta essere misurata dal numero di anni di ‚ÄúSoftware Engineer (SWE)‚Äù risparmiati poich√© ci√≤ si traduce direttamente in impatto. La facilit√† d‚Äôuso e la flessibilit√† possono ridurre il tempo e lo sforzo necessari per addestrare un modello. TensorFlow e PyTorch sono popolari framework di apprendimento automatico che forniscono interfacce intuitive e API flessibili per la creazione e l‚Äôaddestramento di modelli di machine-learning. Questi framework supportano molte architetture di modelli e sono dotati di strumenti che semplificano il processo di addestramento.</p></li>
<li><p><strong>Riproducibilit√†:</strong> La capacit√† di riprodurre i risultati del processo di training. La riproducibilit√† √® importante per verificare la correttezza e la validit√† di un modello. Tuttavia, le variazioni dovute alle caratteristiche stocastiche della rete spesso rendono difficile riprodurre il comportamento preciso delle applicazioni in fase di addestramento, il che pu√≤ rappresentare una sfida per il benchmarking.</p></li>
</ol>
<div class="no-row-height column-margin column-container"></div><p>Eseguendo il benchmarking per questi tipi di metriche, possiamo ottenere una visione completa delle prestazioni e dell‚Äôefficienza del processo di training da una prospettiva di sistema. Ci√≤ pu√≤ aiutare a identificare le aree di miglioramento e garantire che le risorse siano utilizzate in modo efficace.</p>
</section>
<section id="i-benchmark" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="i-benchmark">I Benchmark</h4>
<p>Ecco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per l‚Äôaddestramento di sistemi di apprendimento automatico.</p>
<p><strong><a href="https://github.com/mlcommons/training">MLPerf Training Benchmark</a></strong>: MLPerf √® una suite di benchmark progettata per misurare le prestazioni di hardware, software e servizi di apprendimento automatico. Il benchmark di MLPerf Training <span class="citation" data-cites="mattson2020mlperf">(<a href="../../../references.it.html#ref-mattson2020mlperf" role="doc-biblioref">Mattson et al. 2020a</a>)</span> si concentra sul tempo necessario per addestrare i modelli a una metrica di qualit√† target. Include carichi di lavoro diversi, come classificazione delle immagini, rilevamento di oggetti, traduzione e apprendimento per rinforzo. <a href="#fig-perf-trend" class="quarto-xref">Figura&nbsp;<span>11.2</span></a> evidenzia i miglioramenti delle prestazioni nelle versioni progressive dei benchmark di MLPerf Training, che hanno tutti superato la legge di Moore. L‚Äôutilizzo di trend di benchmarking standardizzati ci consente di mostrare rigorosamente la rapida evoluzione del ML computing.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-perf-trend" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-perf-trend-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlperf_perf_trend.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-perf-trend-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.2: Tendenze delle prestazioni di MLPerf Training. Fonte: <span class="citation" data-cites="mattson2020mlperf">Mattson et al. (<a href="../../../references.it.html#ref-mattson2020mlperf" role="doc-biblioref">2020a</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-mattson2020mlperf" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî, et al. 2020a. <span>¬´MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance¬ª</span>. <em>IEEE Micro</em> 40 (2): 8‚Äì16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div></div></figure>
</div>
<p>Metriche:</p>
<ul>
<li>Tempo di training per la qualit√† target</li>
<li>Throughput (esempi al secondo)</li>
<li>Utilizzo delle risorse (CPU, GPU, memoria, I/O del disco)</li>
</ul>
<p><strong><a href="https://dawn.cs.stanford.edu/benchmark/">DAWNBench</a></strong>: DAWNBench <span class="citation" data-cites="coleman2017dawnbench">(<a href="../../../references.it.html#ref-coleman2017dawnbench" role="doc-biblioref">Coleman et al. 2019</a>)</span> √® una suite di benchmark incentrata sui tempi di training end-to-end del deep learning e sulle prestazioni di inferenza. Include attivit√† comuni come la classificazione delle immagini e la risposta alle domande.</p>
<div class="no-row-height column-margin column-container"><div id="ref-coleman2017dawnbench" class="csl-entry" role="listitem">
Coleman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris R√©, e Matei Zaharia. 2019. <span>¬´Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark¬ª</span>. <em>ACM SIGOPS Operating Systems Review</em> 53 (1): 14‚Äì25. <a href="https://doi.org/10.1145/3352020.3352024">https://doi.org/10.1145/3352020.3352024</a>.
</div></div><p>Metriche:</p>
<ul>
<li>Tempo di training per la precisione target</li>
<li>Latenza dell‚Äôinferenza</li>
<li>Costo (in termini di risorse di cloud computing e storage)</li>
</ul>
<p><strong><a href="https://github.com/rdadolf/fathom">Fathom</a></strong>: Fathom <span class="citation" data-cites="adolf2016fathom">(<a href="../../../references.it.html#ref-adolf2016fathom" role="doc-biblioref">Adolf et al. 2016</a>)</span> √® un benchmark dell‚ÄôUniversit√† di Harvard che valuta le prestazioni dei modelli di deep learning utilizzando un set diversificato di carichi di lavoro. Questi includono attivit√† comuni come la classificazione delle immagini, il riconoscimento vocale e la modellazione del linguaggio.</p>
<div class="no-row-height column-margin column-container"><div id="ref-adolf2016fathom" class="csl-entry" role="listitem">
Adolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. <span>¬´Fathom: reference workloads for modern deep learning methods¬ª</span>. In <em>2016 IEEE International Symposium on Workload Characterization (IISWC)</em>, 1‚Äì10. IEEE; IEEE. <a href="https://doi.org/10.1109/iiswc.2016.7581275">https://doi.org/10.1109/iiswc.2016.7581275</a>.
</div></div><p>Metriche:</p>
<ul>
<li>Operazioni al secondo (per misurare l‚Äôefficienza computazionale)</li>
<li>Tempo di completamento per ogni carico di lavoro</li>
<li>Larghezza di banda della memoria</li>
</ul>
</section>
<section id="caso-duso-di-esempio" class="level4">
<h4 class="anchored" data-anchor-id="caso-duso-di-esempio">Caso d‚ÄôUso di Esempio</h4>
<p>Si immagini di essere stati incaricati di effettuare il benchmarking delle prestazioni di training di un modello di classificazione delle immagini su una piattaforma hardware specifica. Analizziamo come si potrebbe affrontare questa situazione:</p>
<ol type="1">
<li><p><strong>Definire l‚ÄôAttivit√†</strong>: Per prima cosa, si sceglie un modello e un set di dati. In questo caso, si allener√† una CNN per classificare le immagini nel set di dati <a href="https://www.cs.toronto.edu/kriz/cifar.html">CIFAR-10</a>, un benchmark ampiamente utilizzato nella visione artificiale.</p></li>
<li><p><strong>Selezionare il benchmark</strong>: La scelta di un benchmark ampiamente accettato aiuta a garantire che la configurazione sia confrontabile con altre valutazioni del mondo reale. Si potrebbe scegliere di utilizzare il benchmark di training MLPerf perch√© fornisce un carico di lavoro di classificazione delle immagini strutturato, rendendolo un‚Äôopzione pertinente e standardizzata per valutare le prestazioni di training su CIFAR-10. L‚Äôutilizzo di MLPerf consente di valutare il sistema rispetto a metriche standard del settore, contribuendo a garantire che i risultati siano significativi e confrontabili con quelli ottenuti su altre piattaforme hardware.</p></li>
<li><p><strong>Identificare le Metriche Chiave</strong>: Ora, si decidono le metriche che aiuteranno a valutare le prestazioni di training del sistema. Per questo esempio, si potrebbero tracciare:</p>
<ul>
<li><strong>Tempo di Training</strong>: Quanto tempo ci vuole per raggiungere il 90% di accuratezza?</li>
<li><strong>Produttivit√†</strong>: Quante immagini vengono elaborate al secondo?</li>
<li><strong>Utilizzo delle Risorse</strong>: Qual √® l‚Äôutilizzo di GPU e CPU durante il training?</li>
</ul></li>
</ol>
<p>Analizzando queste metriche, si otterranno informazioni sulle prestazioni di training del modello sulla piattaforma hardware scelta. Valutare se il tempo di training soddisfa le aspettative, se ci sono colli di bottiglia, come GPU sottoutilizzate o caricamento lento dei dati. Questo processo aiuta a identificare aree per una potenziale ottimizzazione, come il miglioramento della gestione dei dati o la regolazione dell‚Äôallocazione delle risorse, e pu√≤ guidare le future decisioni di benchmarking.</p>
</section>
</section>
<section id="benchmark-di-inferenza" class="level3" data-number="11.4.4">
<h3 data-number="11.4.4" class="anchored" data-anchor-id="benchmark-di-inferenza"><span class="header-section-number">11.4.4</span> Benchmark di Inferenza</h3>
<p>L‚Äôinferenza nell‚Äôapprendimento automatico si riferisce all‚Äôuso di un modello addestrato per fare previsioni su dati nuovi e mai visti prima. √à la fase in cui il modello applica le conoscenze apprese per risolvere il problema per cui √® stato progettato, come la classificazione di immagini, il riconoscimento vocale o la traduzione di testo.</p>
<section id="scopo-1" class="level4">
<h4 class="anchored" data-anchor-id="scopo-1">Scopo</h4>
<p>Quando creiamo modelli di machine learning, il nostro obiettivo finale √® di distribuirli in applicazioni del mondo reale in cui possano fornire previsioni accurate e affidabili su dati nuovi e mai visti. Questo processo di utilizzo di un modello addestrato per fare previsioni √® noto come inferenza. Le prestazioni reali di un modello di apprendimento automatico possono differire in modo significativo dalle sue prestazioni su set di dati di addestramento o validazione, il che rende l‚Äôinferenza di benchmarking un passaggio cruciale nello sviluppo e nell‚Äôimplementazione di modelli di machine learning.</p>
<p>Il benchmarking dell‚Äôinferenza ci consente di valutare quanto bene un modello di apprendimento automatico funziona in scenari del mondo reale. Questa valutazione garantisce che il modello sia pratico e affidabile quando distribuito in applicazioni, fornendo una comprensione pi√π completa del comportamento del modello con dati reali. Inoltre, il benchmarking pu√≤ aiutare a identificare potenziali colli di bottiglia o limitazioni nelle prestazioni del modello. Ad esempio, se un modello impiega troppo tempo per dedurre, potrebbe non essere pratico per applicazioni in tempo reale come la guida autonoma o gli assistenti vocali.</p>
<p>L‚Äôefficienza delle risorse √® un altro aspetto critico dell‚Äôinferenza, poich√© pu√≤ essere computazionalmente intensiva e richiedere memoria e potenza di elaborazione significative. Il benchmarking aiuta a garantire che il modello sia efficiente per quanto riguarda l‚Äôutilizzo delle risorse, il che √® particolarmente importante per i dispositivi edge con capacit√† computazionali limitate, come smartphone o dispositivi IoT. Inoltre, il benchmarking ci consente di confrontare le prestazioni del nostro modello con quelli concorrenti o versioni precedenti dello stesso modello. Questo confronto √® essenziale per prendere decisioni informate su quale modello implementare in un‚Äôapplicazione specifica.</p>
<p>Infine, √® fondamentale garantire che le previsioni del modello non siano solo accurate, ma anche coerenti tra diversi dati. Il benchmarking aiuta a verificare l‚Äôaccuratezza e la coerenza del modello, assicurando che soddisfi i requisiti dell‚Äôapplicazione. Valuta inoltre la robustezza del modello, assicurando che possa gestire la variabilit√† dei dati del mondo reale e comunque fare previsioni accurate.</p>
</section>
<section id="metriche-1" class="level4">
<h4 class="anchored" data-anchor-id="metriche-1">Metriche</h4>
<ol type="1">
<li><p><strong>Precisione:</strong> La precisione √® una delle metriche pi√π importanti quando si confrontano i modelli di machine learning. Quantifica la percentuale di previsioni corrette effettuate dal modello rispetto ai valori o alle etichette reali. Ad esempio, se un modello di rilevamento dello spam riesce a classificare correttamente 95 messaggi e-mail su 100, la sua precisione verrebbe calcolata al 95%.</p></li>
<li><p><strong>Latenza:</strong> La latenza √® una metrica delle prestazioni che calcola il ritardo o l‚Äôintervallo di tempo tra la ricezione dell‚Äôinput e la produzione dell‚Äôoutput corrispondente da parte del sistema di apprendimento automatico. Un esempio che descrive chiaramente la latenza √® un‚Äôapplicazione di traduzione in tempo reale; se esiste un ritardo di mezzo secondo dal momento in cui un utente inserisce una frase al momento in cui l‚Äôapp visualizza il testo tradotto, la latenza del sistema √® di 0.5 secondi.</p></li>
<li><p><strong>Latency-Bounded Throughput:</strong> Il throughput limitato dalla latenza √® una metrica preziosa che combina gli aspetti di latenza e throughput, misurando il throughput massimo di un sistema pur rispettando un vincolo di latenza specificato. Ad esempio, in un‚Äôapplicazione di streaming video che utilizza un modello di apprendimento automatico per generare e visualizzare automaticamente i sottotitoli, il throughput limitato dalla latenza misurerebbe quanti frame video il sistema pu√≤ elaborare al secondo (throughput) garantendo al contempo che i sottotitoli vengano visualizzati con un ritardo non superiore a 1 secondo (latenza). Questa metrica √® particolarmente importante nelle applicazioni in tempo reale in cui soddisfare i requisiti di latenza √® fondamentale per l‚Äôesperienza utente.</p></li>
<li><p><strong>Throughput:</strong> Il throughput valuta la capacit√† del sistema misurando il numero di inferenze o previsioni che un modello di apprendimento automatico pu√≤ gestire entro un‚Äôunit√† di tempo specifica. Si consideri un sistema di riconoscimento vocale che utilizza una Recurrent Neural Network (RNN) come modello sottostante; se questo sistema riesce a elaborare e comprendere 50 diverse clip audio in un minuto, allora la sua velocit√† di elaborazione √® di 50 clip al minuto.</p></li>
<li><p><strong>Efficienza energetica:</strong> L‚Äôefficienza energetica √® una metrica che determina la quantit√† di energia consumata dal modello di apprendimento automatico per eseguire una singola inferenza. Un esempio lampante di ci√≤ sarebbe un modello di elaborazione del linguaggio naturale basato su un‚Äôarchitettura di rete Transformer; se utilizza 0,1 Joule di energia per tradurre una frase dall‚Äôinglese al francese, la sua efficienza energetica √® misurata a 0,1 Joule per inferenza.</p></li>
<li><p><strong>Utilizzo della memoria:</strong> L‚Äôutilizzo della memoria quantifica il volume di RAM necessario a un modello di apprendimento automatico per svolgere attivit√† di inferenza. Un esempio rilevante per illustrare questo sarebbe un sistema di riconoscimento facciale basato su una CNN; se un tale sistema richiede 150 MB di RAM per elaborare e riconoscere i volti all‚Äôinterno di un‚Äôimmagine, il suo utilizzo della memoria √® di 150 MB.</p></li>
</ol>
</section>
<section id="i-benchmark-1" class="level4">
<h4 class="anchored" data-anchor-id="i-benchmark-1">I Benchmark</h4>
<p>Ecco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per sistemi di apprendimento automatico inferenziale.</p>
<p><strong><a href="https://github.com/mlcommons/inference">MLPerf Inference Benchmark</a>:</strong> MLPerf Inference √® una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una variet√† di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l‚Äôobiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza. Le sue metriche includono:</p>
<p>MLPerf Inference √® una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una variet√† di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l‚Äôobiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza.</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Latenza</li>
<li>Throughput [Produttivit√†]</li>
<li>Precisione</li>
<li>Consumo energetico</li>
</ul>
<p><strong><a href="https://ai-benchmark.com/">AI Benchmark</a>:</strong> AI Benchmark √® uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attivit√† di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un‚Äôanalisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware. Le sue metriche includono:</p>
<p>AI Benchmark √® uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e di apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attivit√† di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un‚Äôanalisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware.</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Latenza</li>
<li>Consumo energetico</li>
<li>Utilizzo della memoria</li>
<li>Throughput [Produttivit√†]</li>
</ul>
<p><strong><a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">Toolkit OpenVINO</a>:</strong> Il toolkit OpenVINO fornisce uno strumento di benchmark per misurare le prestazioni dei modelli di apprendimento profondo per varie attivit√†, come la classificazione delle immagini, il rilevamento degli oggetti e il riconoscimento facciale, su hardware Intel. Offre approfondimenti dettagliati sulle prestazioni di inferenza dei modelli su diverse configurazioni hardware. Le sue metriche includono:</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Throughput [Produttivit√†]</li>
<li>Latenza</li>
<li>Utilizzo di CPU e GPU</li>
</ul>
</section>
<section id="caso-duso-di-esempio-1" class="level4">
<h4 class="anchored" data-anchor-id="caso-duso-di-esempio-1">Caso d‚ÄôUso di Esempio</h4>
<p>Supponiamo che sia stato assegnato il compito di valutare le prestazioni di inferenza di un modello di rilevamento di oggetti su uno specifico dispositivo edge. Ecco come ci si potrebbe approcciare alla strutturazione di questo benchmark:</p>
<ol type="1">
<li><p><strong>Definire l‚ÄôAttivit√†</strong>: In questo caso, l‚Äôattivit√† √® il rilevamento di oggetti in tempo reale su flussi video, identificando oggetti come veicoli, pedoni e segnali stradali.</p></li>
<li><p><strong>Selezionare il Benchmark</strong>: Per allinearsi all‚Äôobiettivo di valutare l‚Äôinferenza su un dispositivo edge, l‚ÄôAI Benchmark √® una scelta adatta. Fornisce un framework standardizzato specificamente per valutare le prestazioni di inferenza su hardware edge, rendendolo rilevante per questo scenario.</p></li>
<li><p><strong>Identificare le Metriche Chiave</strong>: Ora, si determinano le metriche che aiuteranno a valutare le prestazioni di inferenza del modello. Per questo esempio, si potrebbero tracciare:</p>
<ul>
<li><strong>Tempo di Inferenza</strong>: Quanto tempo ci vuole per elaborare ogni fotogramma video?</li>
<li><strong>Latenza</strong>: Qual √® il ritardo nella generazione di bounding box per gli oggetti rilevati?</li>
<li><strong>Consumo Energetico</strong>: Quanta energia viene utilizzata durante l‚Äôinferenza?</li>
<li><strong>Produttivit√†</strong>: Quanti frame video vengono elaborati al secondo?</li>
</ul></li>
</ol>
<p>Misurando queste metriche, si otterranno informazioni su quanto bene funziona il modello di rilevamento degli oggetti sul dispositivo edge. Ci√≤ pu√≤ aiutare a identificare eventuali colli di bottiglia, come l‚Äôelaborazione lenta dei frame o l‚Äôelevato consumo energetico, e a evidenziare aree per una potenziale ottimizzazione per migliorare le prestazioni in tempo reale.</p>
<div id="exr-perf" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;11.2: Benchmark di Inferenza - MLPerf
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Prepararsi a mettere alla prova i propri modelli di intelligenza artificiale! MLPerf √® come le Olimpiadi per le prestazioni del machine learning. In questo Colab, utilizzeremo un toolkit chiamato CK per eseguire benchmark MLPerf ufficiali, misurare la velocit√† e l‚Äôaccuratezza di un proprio modello e persino utilizzare TVM per dargli una spinta super veloce. Pronti a vedere il modello vincere la sua medaglia?</p>
<p><a href="https://colab.research.google.com/drive/1aywGlyD1ZRDtQTrQARVgL1882JcvmFK-?usp=sharing#scrollTo=tnyHAdErL72u"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="selezione-delle-attivit√†-di-benchmark" class="level3" data-number="11.4.5">
<h3 data-number="11.4.5" class="anchored" data-anchor-id="selezione-delle-attivit√†-di-benchmark"><span class="header-section-number">11.4.5</span> Selezione delle Attivit√† di Benchmark</h3>
<p>La selezione di attivit√† rappresentative per il benchmarking dei sistemi di machine learning √® complessa a causa delle diverse applicazioni, tipi di dati e requisiti nei diversi domini. L‚Äôapprendimento automatico viene applicato in settori quali sanit√†, finanza, elaborazione del linguaggio naturale e visione artificiale, ognuno con attivit√† uniche che potrebbero non essere pertinenti o paragonabili ad altre. Le principali sfide nella selezione delle attivit√† includono:</p>
<ol type="1">
<li><strong>Diversit√† di Applicazioni e Tipi di Dati:</strong> Le attivit√† nei vari domini coinvolgono diversi tipi di dati (ad esempio testo, immagini, video) e qualit√†, rendendo difficile trovare benchmark che rappresentino universalmente le sfide dell‚Äôapprendimento automatico.</li>
<li><strong>Complessit√† delle Attivit√† e Necessit√† di Risorse:</strong> Le attivit√† variano in complessit√† e richieste di risorse, con alcune che richiedono una notevole potenza di calcolo e modelli sofisticati, mentre altre possono essere affrontate con risorse e metodi pi√π semplici.</li>
<li><strong>Problemi di Privacy:</strong> Le attivit√† che coinvolgono dati sensibili, come cartelle cliniche o informazioni personali, introducono problemi etici e di privacy, rendendole inadatte per benchmark generali.</li>
<li><strong>Metriche di Valutazione:</strong> Le metriche delle prestazioni variano notevolmente tra le attivit√† e i risultati di un‚Äôattivit√† spesso non si generalizzano ad altre, complicando i confronti e limitando le informazioni da un‚Äôattivit√† di benchmarking a un‚Äôaltra.</li>
</ol>
<p>Affrontare queste sfide √® essenziale per progettare benchmark significativi che siano pertinenti tra le diverse attivit√† incontrate nell‚Äôapprendimento automatico, assicurando che i benchmark forniscano informazioni utili e generalizzabili sia per la formazione che per l‚Äôinferenza.</p>
</section>
<section id="misura-dellefficienza-energetica" class="level3 page-columns page-full" data-number="11.4.6">
<h3 data-number="11.4.6" class="anchored" data-anchor-id="misura-dellefficienza-energetica"><span class="header-section-number">11.4.6</span> Misura dell‚ÄôEfficienza Energetica</h3>
<p>Con l‚Äôespansione delle capacit√† di apprendimento automatico, sia nel training che nell‚Äôinferenza, le preoccupazioni relative all‚Äôaumento del consumo energetico e al suo impatto ecologico si sono intensificate. Affrontare la sostenibilit√† dei sistemi ML, un argomento esplorato pi√π approfonditamente nel capitolo <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html">IA Sostenibile</a>, √® quindi diventata una priorit√† fondamentale. Questa attenzione alla sostenibilit√† ha portato allo sviluppo di benchmark standardizzati progettati per misurare con precisione l‚Äôefficienza energetica. Tuttavia, la standardizzazione di queste metodologie pone delle sfide dovute alla necessit√† di adattarsi a scale molto diverse, dal consumo di microwatt dei dispositivi TinyML alle richieste di megawatt dei sistemi di training dei data center. Inoltre, per garantire che il benchmarking sia equo e riproducibile √® necessario adattarsi alla vasta gamma di configurazioni hardware e architetture in uso oggi.</p>
<p>Un esempio √® la metodologia di benchmarking MLPerf Power <span class="citation" data-cites="tschand2024mlperf">(<a href="../../../references.it.html#ref-tschand2024mlperf" role="doc-biblioref">Tschand et al. 2024</a>)</span>, che affronta queste sfide adattando le metodologie per data center, edge inference e tiny inference systems, misurando al contempo il consumo energetico nel modo pi√π completo possibile per ogni scala. Questa metodologia si adatta a una variet√† di hardware, dalle CPU generiche agli acceleratori AI specializzati, mantenendo principi di misurazione uniformi per garantire che i confronti siano equi e accurati su diverse piattaforme.</p>
<div class="no-row-height column-margin column-container"></div><p><a href="#fig-power-diagram" class="quarto-xref">Figura&nbsp;<span>11.3</span></a> illustra i limiti di misurazione dell‚Äôalimentazione per diverse scale di sistema, dai dispositivi TinyML ai nodi di inferenza e ai rack di training. Ciascun esempio evidenzia i componenti all‚Äôinterno del limite di misurazione e quelli al di fuori di esso. Questa configurazione consente una riflessione accurata dei veri costi energetici associati all‚Äôesecuzione di carichi di lavoro ML in vari scenari del mondo reale e garantisce che il benchmark catturi l‚Äôintero spettro di consumo energetico.</p>
<div id="fig-power-diagram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-power-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/power_component_diagram.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-power-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.3: Diagramma di misurazione del sistema MLPerf Power. Fonte: <span class="citation" data-cites="tschand2024mlperf">Tschand et al. (<a href="../../../references.it.html#ref-tschand2024mlperf" role="doc-biblioref">2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-tschand2024mlperf" class="csl-entry" role="listitem">
Tschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024. <span>¬´MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI¬ª</span>. <em>arXiv preprint arXiv:2410.12032</em>, ottobre. <a href="http://arxiv.org/abs/2410.12032v1">http://arxiv.org/abs/2410.12032v1</a>.
</div></div></figure>
</div>
<p>√à importante notare che l‚Äôottimizzazione di un sistema per le prestazioni potrebbe non portare all‚Äôesecuzione pi√π efficiente dal punto di vista energetico. Spesso, sacrificare una piccola quantit√† di prestazioni o accuratezza pu√≤ portare a guadagni significativi nell‚Äôefficienza energetica, evidenziando l‚Äôimportanza di un benchmarking accurato delle metriche della potenza. Le future intuizioni dal benchmarking dell‚Äôefficienza energetica e della sostenibilit√† ci consentiranno di ottimizzare per sistemi ML pi√π sostenibili.</p>
</section>
<section id="esempio-di-benchmark" class="level3 page-columns page-full" data-number="11.4.7">
<h3 data-number="11.4.7" class="anchored" data-anchor-id="esempio-di-benchmark"><span class="header-section-number">11.4.7</span> Esempio di Benchmark</h3>
<p>Per illustrare correttamente i componenti di un benchmark di sistema, possiamo esaminare il benchmark di individuazione delle parole chiave in MLPerf Tiny e spiegare la motivazione alla base di ogni decisione.</p>
<section id="task" class="level4">
<h4 class="anchored" data-anchor-id="task">Task</h4>
<p>L‚Äôindividuazione delle parole chiave √® stata selezionata come attivit√† perch√© √® un caso d‚Äôuso comune in TinyML che √® stato ben consolidato per anni. Inoltre, l‚Äôhardware tipico utilizzato per l‚Äôindividuazione delle parole chiave differisce sostanzialmente dalle offerte di altri benchmark, come l‚Äôattivit√† di riconoscimento vocale di MLPerf Inference.</p>
</section>
<section id="il-dataset" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="il-dataset">Il Dataset</h4>
<p><a href="https://www.tensorflow.org/datasets/catalog/speech_commands">Google Speech Commands</a> <span class="citation" data-cites="warden2018speech">(<a href="../../../references.it.html#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span> √® stato selezionato come il miglior dataset per rappresentare l‚Äôattivit√†. Il dataset √® ben consolidato nella comunit√† di ricerca e ha una licenza permissiva, che consente di utilizzarlo facilmente in un benchmark.</p>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>¬´Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition¬ª</span>. <em>ArXiv preprint</em> abs/1804.03209 (aprile). <a href="http://arxiv.org/abs/1804.03209v1">http://arxiv.org/abs/1804.03209v1</a>.
</div></div></section>
<section id="modello" class="level4">
<h4 class="anchored" data-anchor-id="modello">Modello</h4>
<p>Il componente principale successivo √® il modello, che funger√† da carico di lavoro primario per il benchmark. Il modello dovrebbe essere ben consolidato come soluzione per l‚Äôattivit√† selezionata piuttosto che una soluzione all‚Äôavanguardia. Il modello selezionato √® un semplice modello di convoluzione separabile in profondit√†. Questa architettura non √® la soluzione all‚Äôavanguardia per l‚Äôattivit√†, ma √® ben consolidata e non progettata per una piattaforma hardware specifica come molte soluzioni all‚Äôavanguardia. Nonostante sia un benchmark di inferenza, stabilisce anche una ricetta di training di riferimento per essere completamente riproducibile e trasparente.</p>
</section>
<section id="metriche-2" class="level4">
<h4 class="anchored" data-anchor-id="metriche-2">Metriche</h4>
<p>La latenza √® stata selezionata come metrica primaria per il benchmark, poich√© i sistemi di individuazione delle parole chiave devono reagire rapidamente per mantenere la soddisfazione dell‚Äôutente. Inoltre, dato che i sistemi TinyML sono spesso alimentati a batteria, il consumo energetico viene misurato per garantire l‚Äôefficienza della piattaforma hardware. L‚Äôaccuratezza del modello viene misurata anche per garantire che le ottimizzazioni applicate da un submitter, come la quantizzazione, non degradino l‚Äôaccuratezza oltre una soglia.</p>
</section>
<section id="benchmark-harness" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-harness">Benchmark Harness</h4>
<p>MLPerf Tiny utilizza <a href="https://github.com/eembc/energyrunner">EEMBCs EnergyRunner benchmark harness</a> per caricare gli input nel modello e isolare e misurare il consumo energetico del dispositivo. Quando si misura il consumo energetico, √® fondamentale selezionare un ‚Äúharness‚Äù [imbracatura] che sia accurato ai livelli di potenza previsti dei dispositivi sottoposti a test e sufficientemente semplice da non diventare un peso per i partecipanti al benchmark.</p>
</section>
<section id="la-baseline" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="la-baseline">La Baseline</h4>
<p>Gli invii di baseline sono fondamentali per contestualizzare i risultati e come punto di riferimento per aiutare i partecipanti a iniziare. L‚Äôinvio di base dovrebbe dare priorit√† alla semplicit√† e alla leggibilit√† rispetto alle prestazioni avanzate. L‚Äôindividuazione della parola chiave della baseline utilizza un <a href="https://www.st.com/en/microcontrollers-microprocessors.html">microcontrollore STM</a> standard come hardware e <a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite come microcontrollore</a> <span class="citation" data-cites="david2021tensorflow">(<a href="../../../references.it.html#ref-david2021tensorflow" role="doc-biblioref">David et al. 2021</a>)</span> come framework di inferenza.</p>
<div class="no-row-height column-margin column-container"><div id="ref-david2021tensorflow" class="csl-entry" role="listitem">
David, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. <span>¬´Tensorflow lite micro: Embedded machine learning for tinyml systems¬ª</span>. <em>Proceedings of Machine Learning and Systems</em> 3: 800‚Äì811.
</div></div></section>
</section>
<section id="sfide-e-limitazioni" class="level3 page-columns page-full" data-number="11.4.8">
<h3 data-number="11.4.8" class="anchored" data-anchor-id="sfide-e-limitazioni"><span class="header-section-number">11.4.8</span> Sfide e Limitazioni</h3>
<p>Sebbene il benchmarking fornisca una metodologia strutturata per la valutazione delle prestazioni in domini complessi come l‚Äôintelligenza artificiale e l‚Äôinformatica, il processo pone anche diverse sfide. Se non affrontati correttamente, questi ostacoli possono minare la credibilit√† e l‚Äôaccuratezza dei risultati del benchmarking. Alcune delle difficolt√† predominanti affrontate nel benchmarking includono quanto segue:</p>
<ul>
<li><p><strong>Copertura incompleta del problema:</strong> Le attivit√† di benchmarking potrebbero non rappresentare completamente lo spazio del problema. Ad esempio, i set di dati di classificazione delle immagini comuni come <a href="https://www.cs.toronto.edu/kriz/cifar.html">CIFAR-10</a> hanno una diversit√† limitata nei tipi di immagini. Gli algoritmi ottimizzati per tali benchmark potrebbero non riuscire a generalizzare bene con i set di dati del mondo reale.</p></li>
<li><p><strong>Insignificanza statistica:</strong> I benchmark devono avere prove e campioni di dati sufficienti per produrre risultati statisticamente significativi. Ad esempio, il benchmarking di un modello OCR su solo poche scansioni di testo potrebbe non catturare adeguatamente i suoi veri tassi di errore.</p></li>
<li><p><strong>Riproducibilit√† limitata:</strong> Variazioni di hardware, versioni software, basi di codice e altri fattori possono ridurre la riproducibilit√† dei risultati di benchmark. MLPerf affronta questo problema fornendo implementazioni di riferimento e specifiche ambientali.</p></li>
<li><p><strong>Disallineamento con gli obiettivi finali:</strong> I benchmark che si concentrano solo su metriche di velocit√† o accuratezza possono disallineare gli obiettivi reali come costi ed efficienza energetica. I benchmark devono riflettere tutti gli assi prestazionali critici.</p></li>
<li><p><strong>Rapida obsolescenza:</strong> A causa del rapido ritmo dei progressi nell‚Äôintelligenza artificiale e nell‚Äôinformatica, i benchmark e i loro set di dati possono rapidamente diventare obsoleti. Mantenere benchmark aggiornati √® quindi una sfida persistente.</p></li>
</ul>
<p>Ma di tutte queste, la sfida pi√π importante √® l‚Äôingegneria dei benchmark.</p>
<section id="lotteria-hardware" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="lotteria-hardware">Lotteria Hardware</h4>
<p>La lotteria hardware, descritta per la prima volta da <span class="citation" data-cites="10.1145/3467017">Hooker (<a href="../../../references.it.html#ref-10.1145/3467017" role="doc-biblioref">2021</a>)</span>, si riferisce alla situazione in cui il successo o l‚Äôefficienza di un modello di apprendimento automatico sono significativamente influenzati dalla sua compatibilit√† con l‚Äôhardware sottostante <span class="citation" data-cites="chu2021discovering">(<a href="../../../references.it.html#ref-chu2021discovering" role="doc-biblioref">Chu et al. 2021</a>)</span>. Alcuni modelli hanno prestazioni eccezionali non perch√© sono intrinsecamente superiori, ma perch√© sono ottimizzati per caratteristiche hardware specifiche, come le capacit√† di elaborazione parallela delle unit√† di elaborazione grafica (GPU) o delle unit√† di elaborazione tensoriale (TPU).</p>
<div class="no-row-height column-margin column-container"><div id="ref-10.1145/3467017" class="csl-entry" role="listitem">
Hooker, Sara. 2021. <span>¬´The hardware lottery¬ª</span>. <em>Communications of the ACM</em> 64 (12): 58‚Äì65. <a href="https://doi.org/10.1145/3467017">https://doi.org/10.1145/3467017</a>.
</div></div><p>Ad esempio, <a href="#fig-hardware-lottery" class="quarto-xref">Figura&nbsp;<span>11.4</span></a> confronta le prestazioni dei modelli su diverse piattaforme hardware. I modelli multi-hardware mostrano risultati comparabili a ‚ÄúMobileNetV3 Large min‚Äù sia sulle configurazioni CPU uint8 che GPU. Tuttavia, questi modelli multi-hardware dimostrano miglioramenti significativi delle prestazioni rispetto alla baseline MobileNetV3 Large quando eseguiti su hardware EdgeTPU e DSP. Ci√≤ sottolinea l‚Äôefficienza variabile dei modelli multi-hardware in ambienti di elaborazione specializzati.</p>
<div id="fig-hardware-lottery" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hardware-lottery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hardware_lottery.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-lottery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.4: Compromessi tra precisione e latenza di pi√π modelli ML e modalit√† di funzionamento su vari hardware. Fonte: <span class="citation" data-cites="chu2021discovering">Chu et al. (<a href="../../../references.it.html#ref-chu2021discovering" role="doc-biblioref">2021</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-chu2021discovering" class="csl-entry" role="listitem">
Chu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. <span>¬´Discovering Multi-Hardware Mobile Models via Architecture Search¬ª</span>. In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 3016‚Äì25. IEEE. <a href="https://doi.org/10.1109/cvprw53098.2021.00337">https://doi.org/10.1109/cvprw53098.2021.00337</a>.
</div></div></figure>
</div>
<p>La lotteria hardware pu√≤ introdurre sfide e pregiudizi nel benchmarking dei sistemi di apprendimento automatico, poich√© le prestazioni del modello non dipendono esclusivamente dall‚Äôarchitettura o dall‚Äôalgoritmo del modello ma anche dalla compatibilit√† e dalle sinergie con l‚Äôhardware sottostante. Ci√≤ pu√≤ rendere difficile confrontare equamente diversi modelli e identificare il modello migliore in base ai suoi meriti intrinseci. Pu√≤ anche portare a una situazione in cui la comunit√† converge su modelli che sono adatti all‚Äôhardware pi√π diffuso del momento, trascurando potenzialmente altri modelli che potrebbero essere superiori ma incompatibili con le attuali tendenze hardware.</p>
</section>
<section id="benchmark-engineering" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="benchmark-engineering">Benchmark Engineering</h4>
<p>La lotteria hardware si verifica quando un modello di apprendimento automatico funziona in modo eccezionalmente bene o male su una configurazione hardware specifica a causa di compatibilit√† o incompatibilit√† impreviste. Il modello non √® esplicitamente progettato o ottimizzato per quell‚Äôhardware specifico dagli sviluppatori o dagli ingegneri; piuttosto, capita che si allinei o (non si allinei) con le capacit√† o le limitazioni dell‚Äôhardware. In questo caso, le prestazioni del modello sull‚Äôhardware sono un prodotto della coincidenza piuttosto che della progettazione.</p>
<p>Contrariamente alla lotteria hardware accidentale, il benchmark engineering implica l‚Äôottimizzazione o la progettazione deliberata di un modello di apprendimento automatico per funzionare eccezionalmente bene su hardware specifico, spesso per vincere benchmark o competizioni. Questa ottimizzazione intenzionale potrebbe includere la modifica dell‚Äôarchitettura, degli algoritmi o dei parametri del modello per sfruttare appieno le funzionalit√† e le capacit√† dell‚Äôhardware.</p>
<section id="problema" class="level5">
<h5 class="anchored" data-anchor-id="problema">Problema</h5>
<p>Il benchmark engineering si riferisce alla modifica o all‚Äôottimizzazione di un sistema di intelligenza artificiale per ottimizzare le prestazioni su test di benchmark specifici, spesso a scapito della generalizzabilit√† o delle prestazioni nel mondo reale. Ci√≤ pu√≤ includere la regolazione di iperparametri, dati di training o altri aspetti del sistema specificamente per ottenere punteggi elevati sulle metriche di benchmark senza necessariamente migliorare la funzionalit√† o l‚Äôutilit√† complessiva del sistema.</p>
<p>La motivazione alla base dell‚Äôingegneria dei benchmark spesso deriva dal desiderio di ottenere punteggi di prestazioni elevate per scopi di marketing o competitivi. Punteggi di benchmark elevati possono dimostrare la superiorit√† di un sistema di intelligenza artificiale rispetto ai concorrenti e possono essere un argomento chiave per la vendita per potenziali utenti o investitori. Questa pressione per ottenere buoni risultati nei benchmark a volte porta a dare priorit√† alle ottimizzazioni specifiche del benchmark rispetto a miglioramenti pi√π olistici del sistema.</p>
<p>Pu√≤ comportare diversi rischi e sfide. Uno dei rischi principali √® che il sistema di intelligenza artificiale possa funzionare meglio nelle applicazioni del mondo reale rispetto a quanto suggeriscono i punteggi di benchmark. Ci√≤ pu√≤ portare a insoddisfazione dell‚Äôutente, danni alla reputazione e potenziali problemi di sicurezza o etici. Inoltre, l‚Äôingegneria dei benchmark pu√≤ contribuire a una mancanza di trasparenza e responsabilit√† nella comunit√† dell‚Äôintelligenza artificiale, poich√© pu√≤ essere difficile discernere quanta parte delle prestazioni di un sistema di intelligenza artificiale sia dovuta a miglioramenti genuini rispetto a ottimizzazioni specifiche del benchmark.</p>
<p>La comunit√† AI deve dare priorit√† alla trasparenza e alla responsabilit√† per mitigare i rischi associati all‚Äôingegneria dei benchmark. Ci√≤ pu√≤ includere la divulgazione di eventuali ottimizzazioni o modifiche apportate specificamente per i test di benchmark e la fornitura di valutazioni pi√π complete dei sistemi AI che includono metriche delle prestazioni del mondo reale e punteggi di benchmark. I ricercatori e gli sviluppatori devono dare priorit√† a miglioramenti olistici dei sistemi AI che ne migliorino la generalizzabilit√† e la funzionalit√† in varie applicazioni anzich√© concentrarsi esclusivamente su ottimizzazioni specifiche del benchmark.</p>
</section>
<section id="problemi" class="level5">
<h5 class="anchored" data-anchor-id="problemi">Problemi</h5>
<p>Uno dei problemi principali dell‚Äôingegneria del benchmark √® che pu√≤ compromettere le prestazioni reali dei sistemi di intelligenza artificiale. Quando gli sviluppatori si concentrano sull‚Äôottimizzazione dei loro sistemi per ottenere punteggi elevati in specifici test di benchmark, potrebbero trascurare altri importanti aspetti delle prestazioni del sistema, cruciali nelle applicazioni del mondo reale. Ad esempio, un sistema di intelligenza artificiale progettato per il riconoscimento delle immagini potrebbe essere progettato per funzionare eccezionalmente bene in un test di benchmark che include un set specifico di immagini, ma necessita di aiuto per riconoscere accuratamente immagini leggermente diverse da quelle nel set di test.</p>
<p>Un‚Äôaltra area di miglioramento con l‚Äôingegneria di benchmark √® che pu√≤ comportare sistemi di intelligenza artificiale privi di generalizzabilit√†. In altre parole, mentre il sistema pu√≤ funzionare bene nel test di benchmark, potrebbe aver bisogno di aiuto per gestire una vasta gamma di input o scenari. Ad esempio, un modello di intelligenza artificiale sviluppato per l‚Äôelaborazione del linguaggio naturale potrebbe essere progettato per ottenere punteggi elevati in un test di benchmark che include un tipo specifico di testo, ma non riesce a elaborare accuratamente il testo che non rientra in quel tipo specifico.</p>
<p>Pu√≤ anche portare a risultati fuorvianti. Quando i sistemi di intelligenza artificiale sono progettati per funzionare bene nei test di benchmark, i risultati potrebbero non riflettere accuratamente le reali capacit√† del sistema. Questo pu√≤ essere problematico per gli utenti o gli investitori che si affidano ai punteggi di benchmark per prendere decisioni informate su quali sistemi di intelligenza artificiale utilizzare o in cui investire. Ad esempio, un sistema di intelligenza artificiale progettato per ottenere punteggi elevati in un test di benchmark per il riconoscimento vocale potrebbe dover essere pi√π in grado di riconoscere accuratamente il parlato in situazioni reali, portando gli utenti o gli investitori a prendere decisioni basate su informazioni imprecise.</p>
</section>
<section id="attenuazione" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="attenuazione">Attenuazione</h5>
<p>Esistono diversi modi per mitigare l‚Äôingegneria dei benchmark. La trasparenza nel processo di benchmarking √® fondamentale per mantenere l‚Äôaccuratezza e l‚Äôaffidabilit√† dei benchmark. Ci√≤ implica la divulgazione chiara delle metodologie, dei set di dati e dei criteri di valutazione utilizzati nei test di benchmark, nonch√© di eventuali ottimizzazioni o modifiche apportate al sistema di intelligenza artificiale ai fini del benchmark.</p>
<p>Un modo per ottenere trasparenza √® attraverso l‚Äôuso di benchmark open source. I benchmark open source vengono resi disponibili al pubblico, consentendo a ricercatori, sviluppatori e altre parti interessate di esaminarli, criticarli e contribuire, garantendone cos√¨ l‚Äôaccuratezza e l‚Äôaffidabilit√†. Questo approccio collaborativo facilita anche la condivisione delle ‚Äúbest practice‚Äù e lo sviluppo di benchmark pi√π solidi e completi.</p>
<p>Il design modulare di MLPerf Tiny si collega al problema dell‚Äôingegneria dei benchmark fornendo un approccio strutturato ma flessibile che incoraggia una valutazione equilibrata di TinyML. Nell‚Äôingegneria dei benchmark, i sistemi possono essere eccessivamente ottimizzati per benchmark specifici, portando a punteggi di prestazioni gonfiati che non si traducono necessariamente in efficacia nel mondo reale. Il design modulare di MLPerf Tiny mira ad affrontare questo problema consentendo ai collaboratori di scambiare e testare componenti specifici all‚Äôinterno di un framework standardizzato, come hardware, tecniche di quantizzazione o modelli di inferenza. Le implementazioni di riferimento, evidenziate in verde e arancione in <a href="#fig-ml-perf" class="quarto-xref">Figura&nbsp;<span>11.5</span></a>, forniscono una base di riferimento per i risultati, consentendo test flessibili ma controllati specificando quali componenti possono essere modificati. Questa struttura supporta trasparenza e flessibilit√†, consentendo di concentrarsi su miglioramenti genuini piuttosto che su ottimizzazioni specifiche del benchmark.</p>
<div id="fig-ml-perf" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-ml-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlperf_tiny.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.5: Design modulare del benchmark MLPerf Tiny, che mostra l‚Äôimplementazione di riferimento con componenti modificabili. Questo approccio modulare consente test flessibili e mirati mantenendo una base di riferimento standardizzata. Fonte: <span class="citation" data-cites="banbury2021mlperf">Banbury et al. (<a href="../../../references.it.html#ref-banbury2021mlperf" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-banbury2021mlperf" class="csl-entry" role="listitem">
Banbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. <span>¬´MLPerf Tiny Benchmark¬ª</span>. <em>arXiv preprint arXiv:2106.07597</em>, giugno. <a href="http://arxiv.org/abs/2106.07597v4">http://arxiv.org/abs/2106.07597v4</a>.
</div></div></figure>
</div>
<p>Un altro metodo per ottenere trasparenza √® attraverso la revisione paritaria dei benchmark. Ci√≤ comporta che esperti indipendenti esaminino e convalidino la metodologia, i set di dati e i risultati del benchmark per garantirne la credibilit√† e l‚Äôaffidabilit√†. La revisione paritaria pu√≤ fornire un mezzo prezioso per verificare l‚Äôaccuratezza dei test di benchmark e contribuire a creare fiducia nei risultati.</p>
<p>La standardizzazione dei benchmark √® un‚Äôaltra importante soluzione per mitigare l‚Äôingegneria dei benchmark. I benchmark standardizzati forniscono un quadro comune per la valutazione dei sistemi di intelligenza artificiale, garantendo coerenza e comparabilit√† tra diversi sistemi e applicazioni. Ci√≤ pu√≤ essere ottenuto sviluppando standard e ‚Äúbest practice‚Äù per l‚Äôintero settore per il benchmarking e tramite metriche e criteri di valutazione comuni.</p>
<p>Anche la verifica da parte di terze parti dei risultati pu√≤ essere preziosa per mitigare l‚Äôingegneria dei benchmark. Ci√≤ comporta che una terza parte indipendente verifichi i risultati di un test di benchmark per garantirne la credibilit√† e l‚Äôaffidabilit√†. La verifica di terze parti pu√≤ creare fiducia nei risultati e fornire un mezzo prezioso per convalidare le prestazioni e le capacit√† dei sistemi di intelligenza artificiale.</p>
</section>
</section>
</section>
</section>
<section id="benchmarking-del-modello" class="level2 page-columns page-full" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="benchmarking-del-modello"><span class="header-section-number">11.5</span> Benchmarking del Modello</h2>
<p>Il benchmarking dei modelli di machine learning √® importante per determinare l‚Äôefficacia e l‚Äôefficienza di vari algoritmi di apprendimento automatico nella risoluzione di compiti o problemi specifici. Analizzando i risultati ottenuti dal benchmarking, sviluppatori e ricercatori possono identificare i punti di forza e di debolezza dei loro modelli, portando a decisioni pi√π informate sulla selezione del modello e su un‚Äôulteriore ottimizzazione.</p>
<p>L‚Äôevoluzione e il progresso dei modelli di apprendimento automatico sono intrinsecamente collegati alla disponibilit√† e alla qualit√† dei set di dati. Nell‚Äôapprendimento automatico, i dati fungono da materia prima che alimenta gli algoritmi, consentendo loro di apprendere, adattarsi e, in definitiva, eseguire compiti che erano tradizionalmente di dominio degli esseri umani. Pertanto, √® importante comprendere questa storia.</p>
<section id="contesto-storico-1" class="level3 page-columns page-full" data-number="11.5.1">
<h3 data-number="11.5.1" class="anchored" data-anchor-id="contesto-storico-1"><span class="header-section-number">11.5.1</span> Contesto Storico</h3>
<p>I dataset di apprendimento automatico hanno una storia ricca e si sono evoluti in modo significativo nel corso degli anni, crescendo in dimensioni, complessit√† e diversit√† per soddisfare le richieste sempre crescenti del settore. Diamo un‚Äôocchiata pi√π da vicino a questa evoluzione, partendo da uno dei primi e pi√π iconici set di dati: MNIST.</p>
<section id="mnist-1998" class="level4">
<h4 class="anchored" data-anchor-id="mnist-1998">MNIST (1998)</h4>
<p>Il <a href="https://www.tensorflow.org/datasets/catalog/mnist">dataset MNIST</a>, creato da Yann LeCun, Corinna Cortes e Christopher J.C. Burges nel 1998, pu√≤ essere considerato una pietra miliare nella storia dei dataset di machine learning. Comprende 70.000 immagini in scala di grigi da 28x28 pixel etichettate di cifre scritte a mano (0-9). MNIST √® stato ampiamente utilizzato per il benchmarking degli algoritmi nell‚Äôelaborazione delle immagini e nell‚Äôapprendimento automatico come punto di partenza per molti ricercatori e professionisti. <a href="#fig-mnist" class="quarto-xref">Figura&nbsp;<span>11.6</span></a> mostra alcuni esempi di cifre scritte a mano.</p>
<div id="fig-mnist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mnist.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.6: Cifre scritte a mano in MNIST. Fonte: <a href="https://en.wikipedia.org/wiki/File:MnistExamplesModified.png">Suvanjanprasai</a>
</figcaption>
</figure>
</div>
</section>
<section id="imagenet-2009" class="level4">
<h4 class="anchored" data-anchor-id="imagenet-2009">ImageNet (2009)</h4>
<p>Facciamo un salto al 2009 e vediamo l‚Äôintroduzione di <a href="https://www.tensorflow.org/datasets/catalog/imagenet2012">ImageNet</a>, che ha segnato un balzo significativo nella scala e nella complessit√† dei dataset. ImageNet √® composto da oltre 14 milioni di immagini etichettate che abbracciano pi√π di 20.000 categorie. Fei-Fei Li e il suo team lo hanno sviluppato per far progredire il riconoscimento degli oggetti e la ricerca sulla visione artificiale. Il dataset √® diventato sinonimo della ImageNet <a href="https://www.image-net.org/challenges/LSVRC/">Large Scale Visual Recognition Challenge (LSVRC)</a>, una competizione annuale cruciale nello sviluppo di modelli di deep learning, tra cui il famoso AlexNet nel 2012.</p>
</section>
<section id="coco-2014" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="coco-2014">COCO (2014)</h4>
<p>Il <a href="https://cocodataset.org/">Common Objects in Context (COCO) dataset</a> <span class="citation" data-cites="lin2014microsoft">(<a href="../../../references.it.html#ref-lin2014microsoft" role="doc-biblioref">Lin et al. 2014</a>)</span>, rilasciato nel 2014, ha ulteriormente ampliato il panorama dei set di dati di apprendimento automatico introducendo un set pi√π ricco di annotazioni. COCO √® costituito da immagini contenenti scene complesse con pi√π oggetti e ogni immagine √® annotata con riquadri di delimitazione degli oggetti, maschere di segmentazione e didascalie, come mostrato in <a href="#fig-coco" class="quarto-xref">Figura&nbsp;<span>11.7</span></a>. Questo set di dati √® stato determinante nel far progredire la ricerca nel rilevamento degli oggetti, nella segmentazione e nella didascalia delle immagini.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2014microsoft" class="csl-entry" role="listitem">
Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, e C. Lawrence Zitnick. 2014. <span>¬´Microsoft COCO: Common Objects in Context¬ª</span>. In <em>Computer Vision ‚Äì ECCV 2014</em>, 740‚Äì55. Springer; Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-10602-1\_48">https://doi.org/10.1007/978-3-319-10602-1\_48</a>.
</div></div><div id="fig-coco" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coco-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/coco.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coco-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.7: Immagini di esempio dal set di dati COCO. Fonte: <a href="https://cocodataset.org/">Coco</a>
</figcaption>
</figure>
</div>
</section>
<section id="gpt-3-2020" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="gpt-3-2020">GPT-3 (2020)</h4>
<p>Sebbene gli esempi sopra riportati si concentrino principalmente sui dataset di immagini, si sono verificati anche sviluppi significativi nei dataset di testo. Un esempio degno di nota √® GPT-3 <span class="citation" data-cites="brown2020language">(<a href="../../../references.it.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>, sviluppato da OpenAI. GPT-3 √® un modello linguistico addestrato su testo Internet eterogeneo. Sebbene il dataset utilizzato per addestrare GPT-3 non sia disponibile al pubblico, il modello stesso, costituito da 175 miliardi di parametri, √® una testimonianza della scala e della complessit√† dei moderni dataset e modelli di apprendimento automatico.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>¬´Language Models are Few-Shot Learners¬ª</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div></div></section>
<section id="presente-e-futuro" class="level4">
<h4 class="anchored" data-anchor-id="presente-e-futuro">Presente e Futuro</h4>
<p>Oggi disponiamo di una pletora di dataset che abbracciano vari domini, tra cui sanit√†, finanza, scienze sociali e altro ancora. Le seguenti caratteristiche ci aiutano a classificare lo spazio e la crescita dei dataset di apprendimento automatico che alimentano lo sviluppo del modello.</p>
<ol type="1">
<li><p><strong>Diversit√† dei Set di Dati:</strong> La variet√† di set di dati disponibili per ricercatori e ingegneri si √® ampliata notevolmente, coprendo molti campi, tra cui l‚Äôelaborazione del linguaggio naturale, il riconoscimento delle immagini e altro ancora. Questa diversit√† ha alimentato lo sviluppo di modelli di apprendimento automatico specializzati, su misura per attivit√† specifiche, come la traduzione, il riconoscimento vocale e il riconoscimento facciale.</p></li>
<li><p><strong>Volume di Dati:</strong> L‚Äôenorme volume di dati che √® diventato disponibile nell‚Äôera digitale ha anche svolto un ruolo cruciale nel progresso dei modelli di apprendimento automatico. I grandi set di dati consentono ai modelli di catturare la complessit√† e le sfumature dei fenomeni del mondo reale, portando a previsioni pi√π accurate e affidabili.</p></li>
<li><p><strong>Qualit√† e Pulizia dei Dati:</strong> La qualit√† dei dati √® un altro fattore critico che influenza le prestazioni dei modelli di apprendimento automatico. Set di dati puliti, ben etichettati e imparziali sono essenziali per modelli di addestramento solidi ed equi.</p></li>
<li><p><strong>Accesso Aperto ai Dati:</strong> La disponibilit√† di set di dati ‚Äúopen-access‚Äù ha contribuito in modo significativo anche al progresso dell‚Äôapprendimento automatico. I dati ‚Äúaperti‚Äù consentono ai ricercatori di tutto il mondo di collaborare, condividere approfondimenti e basarsi sul lavoro degli altri, portando a un‚Äôinnovazione pi√π rapida e allo sviluppo di modelli pi√π avanzati.</p></li>
<li><p><strong>Problemi di Etica e Privacy:</strong> Man mano che i set di dati crescono in dimensioni e complessit√†, le considerazioni etiche e i problemi di privacy diventano sempre pi√π importanti. √à in corso un dibattito sull‚Äôequilibrio tra lo sfruttamento dei dati per i progressi dell‚Äôapprendimento automatico e la protezione dei diritti alla privacy degli individui.</p></li>
</ol>
<p>Lo sviluppo di modelli di apprendimento automatico si basa in larga misura sulla disponibilit√† di set di dati diversificati, grandi, di alta qualit√† e ad accesso libero. Mentre andiamo avanti, affrontare le considerazioni etiche e le preoccupazioni sulla privacy associate all‚Äôuso di grandi set di dati √® fondamentale per garantire che le tecnologie di apprendimento automatico siano vantaggiose per la societ√†. C‚Äô√® una crescente consapevolezza che i dati agiscono come carburante per l‚Äôapprendimento automatico, guidando e alimentando lo sviluppo di modelli di apprendimento automatico. Di conseguenza, si sta ponendo maggiore attenzione sullo sviluppo dei set di dati stessi. Esploreremo questo aspetto in modo pi√π dettagliato nella sezione del benchmarking dei dati.</p>
</section>
</section>
<section id="metriche-del-modello" class="level3 page-columns page-full" data-number="11.5.2">
<h3 data-number="11.5.2" class="anchored" data-anchor-id="metriche-del-modello"><span class="header-section-number">11.5.2</span> Metriche del Modello</h3>
<p>La valutazione del modello di machine learning si √® evoluta da un focus ristretto sulla precisione a un approccio pi√π completo che considera una serie di fattori, da considerazioni etiche e applicabilit√† nel mondo reale a vincoli pratici come dimensioni ed efficienza del modello. Questo cambiamento riflette la maturazione del campo poich√© i modelli di apprendimento automatico vengono sempre pi√π applicati in scenari reali diversi e complessi.</p>
<section id="precisione" class="level4">
<h4 class="anchored" data-anchor-id="precisione">Precisione</h4>
<p>La precisione √® una delle metriche pi√π intuitive e comunemente utilizzate per valutare i modelli di apprendimento automatico. Nelle prime fasi dell‚Äôapprendimento automatico, la precisione era spesso la metrica principale, se non l‚Äôunica, considerata quando si valutavano le prestazioni del modello. Tuttavia, con l‚Äôevoluzione del campo, √® diventato chiaro che fare affidamento esclusivamente sull‚Äôaccuratezza pu√≤ essere fuorviante, soprattutto in applicazioni in cui determinati tipi di errori comportano conseguenze significative.</p>
<p>Si consideri l‚Äôesempio di un modello di diagnosi medica con una precisione del 95%. Sebbene a prima vista possa sembrare impressionante, dobbiamo guardare pi√π a fondo per valutare appieno le prestazioni del modello. Supponiamo che il modello non riesca a diagnosticare accuratamente condizioni gravi che, sebbene rare, possono avere gravi conseguenze; la sua elevata precisione potrebbe non essere cos√¨ significativa. Un esempio ben noto di questa limitazione √® il <a href="https://about.google/intl/ALL_us/stories/seeingpotential/">modello di retinopatia diabetica di Google</a>. Sebbene abbia raggiunto un‚Äôelevata accuratezza in laboratorio, ha incontrato delle difficolt√† quando √® stato implementato in cliniche reali in Thailandia, dove le variazioni nelle popolazioni di pazienti, la qualit√† delle immagini e i fattori ambientali ne hanno ridotto l‚Äôefficacia. Questo esempio illustra che anche i modelli con elevata accuratezza devono essere testati per la loro capacit√† di generalizzare in condizioni diverse e imprevedibili per garantire affidabilit√† e impatto in contesti reali.</p>
<p>Allo stesso modo, se il modello funziona bene in media ma mostra significative disparit√† nelle prestazioni tra diversi gruppi demografici, anche questo sarebbe motivo di preoccupazione. L‚Äôevoluzione dell‚Äôapprendimento automatico ha quindi visto uno spostamento verso un approccio pi√π olistico alla valutazione del modello, tenendo conto non solo dell‚Äôaccuratezza, ma anche di altri fattori cruciali come la correttezza, trasparenza e applicabilit√† nel mondo reale. Un esempio lampante √® il progetto <a href="http://gendershades.org/">Gender Shades</a> del MIT Media Lab, guidato da Joy Buolamwini, che evidenzia i pregiudizi ottenendo risultati migliori sui volti maschili e dalla pelle pi√π chiara rispetto ai volti femminili e dalla pelle pi√π scura.</p>
<p>Sebbene l‚Äôaccuratezza resti essenziale per valutare i modelli di apprendimento automatico, √® necessario un approccio completo per valutare appieno le prestazioni. Ci√≤ include metriche aggiuntive per l‚Äôequit√†, la trasparenza e l‚Äôapplicabilit√† nel mondo reale, insieme a test rigorosi su diversi set di dati per identificare e affrontare i pregiudizi. Questo approccio di valutazione olistico riflette la crescente consapevolezza del settore delle implicazioni nel mondo reale nell‚Äôimplementazione dei modelli.</p>
</section>
<section id="equit√†" class="level4">
<h4 class="anchored" data-anchor-id="equit√†">Equit√†</h4>
<p>Il ‚Äúfairness‚Äù <a href="#equit√†">equit√†</a> nell‚Äôapprendimento automatico implica la garanzia che i modelli funzionino in modo coerente su diversi gruppi, in particolare in applicazioni ad alto impatto come approvazioni di prestiti, assunzioni e giustizia penale. Affidarsi esclusivamente all‚Äôaccuratezza pu√≤ essere fuorviante se il modello mostra risultati distorti su gruppi demografici. Ad esempio, un modello di approvazione dei prestiti con elevata accuratezza potrebbe comunque negare sistematicamente i prestiti a determinati gruppi, sollevando dubbi sulla sua equit√†.</p>
<p>Il ‚Äúbias‚Äù [distorsione] nei modelli pu√≤ sorgere direttamente, quando attributi sensibili come razza o genere influenzano le decisioni, o indirettamente, quando caratteristiche neutre sono correlate a questi attributi, influenzando i risultati. Affidarsi semplicemente all‚Äôaccuratezza pu√≤ essere insufficiente quando si valutano i modelli. Ad esempio, si consideri un modello di approvazione dei prestiti con un tasso di accuratezza del 95%. Sebbene questa cifra possa sembrare impressionante a prima vista, non rivela come il modello si comporta nei diversi gruppi demografici. Un esempio ben noto √® lo strumento COMPAS utilizzato nel sistema di giustizia penale degli Stati Uniti, che ha mostrato pregiudizi razziali nel prevedere la recidiva nonostante non utilizzasse esplicitamente la razza come variabile.</p>
<p>Per affrontare l‚Äôequit√† √® necessario analizzare le prestazioni di un modello tra i gruppi, identificare i pregiudizi e applicare misure correttive come il ribilanciamento dei set di dati o l‚Äôutilizzo di algoritmi consapevoli dell‚Äôequit√†. Ricercatori e professionisti sviluppano continuamente metriche e metodologie su misura per casi d‚Äôuso specifici per valutare la correttezza e l‚Äôequit√† in scenari del mondo reale. Ad esempio, l‚Äôanalisi di impatto disparato, la parit√† demografica e le pari opportunit√† sono alcune delle metriche impiegate per valutare l‚Äôequit√†/correttezza. Inoltre, la trasparenza e l‚Äôinterpretabilit√† dei modelli sono fondamentali per raggiungere la correttezza. Strumenti come <a href="https://ai-fairness-360.org/">AI Fairness 360</a> e <a href="https://www.tensorflow.org/tfx/guide/fairness_indicators">Fairness Indicators</a> aiutano a spiegare come un modello prende decisioni, consentendo agli sviluppatori di rilevare e correggere problemi di equit√† nei modelli di apprendimento automatico.</p>
<p>Sebbene l‚Äôaccuratezza sia una metrica preziosa, non sempre fornisce il quadro completo; la valutazione dell‚Äôequit√† garantisce che i modelli siano efficaci in scenari del mondo reale. Garantire l‚Äôequit√†/correttezza nei modelli di apprendimento automatico, in particolare nelle applicazioni che hanno un impatto significativo sulla vita delle persone, richiede una rigorosa valutazione delle prestazioni del modello in gruppi diversi, un‚Äôattenta identificazione e attenuazione dei pregiudizi e l‚Äôimplementazione di misure di trasparenza e interpretabilit√†.</p>
</section>
<section id="complessit√†" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="complessit√†">Complessit√†</h4>
<section id="parametri" class="level5">
<h5 class="anchored" data-anchor-id="parametri">Parametri</h5>
<p>Nelle fasi iniziali del machine learning, il benchmarking dei modelli si basava spesso sui conteggi dei parametri come proxy [sostituto] per la complessit√† del modello. La logica era che pi√π parametri in genere portano a un modello pi√π complesso, che dovrebbe, a sua volta, fornire prestazioni migliori. Tuttavia, questo approccio trascura i costi pratici associati all‚Äôelaborazione di modelli di grandi dimensioni. Man mano che aumenta il numero dei parametri, aumentano anche le risorse computazionali richieste, rendendo tali modelli poco pratici per l‚Äôimplementazione in scenari reali, in particolare su dispositivi con potenza di elaborazione limitata.</p>
<p>Affidarsi ai conteggi dei parametri come proxy per la complessit√† del modello non riesce a considerare anche l‚Äôefficienza del modello. Un modello ben ottimizzato con meno parametri pu√≤ spesso ottenere prestazioni paragonabili o addirittura superiori a un modello pi√π grande. Ad esempio, MobileNets, sviluppato da Google, √® una famiglia di modelli progettati specificamente per dispositivi mobili ed edge. Hanno utilizzato convoluzioni separabili in profondit√† per ridurre il numero dei parametri e le richieste computazionali mantenendo comunque prestazioni elevate.</p>
<p>Alla luce di queste limitazioni, il settore si √® spostato verso un approccio pi√π olistico al benchmarking dei modelli che considera i conteggi dei parametri e altri fattori cruciali come le operazioni in virgola mobile al secondo (FLOP), il consumo di memoria e la latenza. Questo approccio completo bilancia le prestazioni con la distribuibilit√†, assicurando che i modelli non siano solo accurati, ma anche efficienti e adatti alle applicazioni del mondo reale.</p>
</section>
<section id="flop" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="flop">FLOP</h5>
<p>I ‚Äúfloating-point operation‚Äù (FLOP), o operazioni in virgola mobile al secondo, sono diventati una metrica critica per rappresentare il carico computazionale di un modello. Tradizionalmente, il numero dei parametri veniva utilizzato come indicatore per la complessit√† del modello, basandosi sul presupposto che pi√π parametri avrebbero prodotto prestazioni migliori. Tuttavia, questo approccio trascura il costo computazionale dell‚Äôelaborazione di questi parametri, che pu√≤ influire sull‚Äôusabilit√† di un modello in scenari del mondo reale con risorse limitate.</p>
<p>I FLOP misurano il numero di operazioni in virgola mobile eseguite da un modello per generare una previsione. Un modello con molti FLOP richiede risorse computazionali sostanziali per elaborare il vasto numero di operazioni, il che potrebbe renderlo poco pratico per alcune applicazioni. Al contrario, un modello con un numero di FLOP inferiore √® pi√π leggero e pu√≤ essere facilmente distribuito in scenari in cui le risorse computazionali sono limitate. <a href="#fig-flops" class="quarto-xref">Figura&nbsp;<span>11.8</span></a>, da <span class="citation" data-cites="bianco2018benchmark">(<a href="../../../references.it.html#ref-bianco2018benchmark" role="doc-biblioref">Bianco et al. 2018</a>)</span>, illustra il compromesso tra accuratezza di ImageNet, FLOP e numero dei parametri, dimostrando che alcune architetture raggiungono un‚Äôefficienza maggiore di altre.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-flops" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/model_FLOPS_VS_TOP_1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.8: Un grafico che raffigura la top-1 Imagenet Accuracy rispetto al numero di FLOP di un modello insieme al conteggio dei parametri del modello. La figura mostra un compromesso complessivo tra complessit√† e accuratezza del modello, sebbene alcune architetture del modello siano pi√π efficienti di altre. Fonte: <span class="citation" data-cites="bianco2018benchmark">Bianco et al. (<a href="../../../references.it.html#ref-bianco2018benchmark" role="doc-biblioref">2018</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-bianco2018benchmark" class="csl-entry" role="listitem">
Bianco, Simone, Remi Cadene, Luigi Celona, e Paolo Napoletano. 2018. <span>¬´Benchmark Analysis of Representative Deep Neural Network Architectures¬ª</span>. <em>IEEE Access</em> 6: 64270‚Äì77. <a href="https://doi.org/10.1109/access.2018.2877890">https://doi.org/10.1109/access.2018.2877890</a>.
</div></div></figure>
</div>
<p>Consideriamo un esempio. BERT‚ÄîBidirectional Encoder Representations from Transformers <span class="citation" data-cites="devlin2018bert">(<a href="../../../references.it.html#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span>‚Äî√® un modello di elaborazione del linguaggio naturale molto diffuso, con oltre 340 milioni di parametri, il che lo rende un modello di grandi dimensioni con elevata accuratezza e prestazioni impressionanti in diverse attivit√†. Tuttavia, le dimensioni di BERT, unite al suo elevato numero di FLOP, lo rendono un modello computazionalmente intensivo che potrebbe non essere adatto per applicazioni in tempo reale o per l‚Äôimplementazione su dispositivi edge con capacit√† computazionali limitate. Alla luce di ci√≤, c‚Äô√® stato un crescente interesse nello sviluppo di modelli pi√π piccoli in grado di raggiungere livelli di prestazioni simili alle loro controparti pi√π grandi, pur essendo pi√π efficienti nel carico computazionale. DistilBERT, ad esempio, √® una versione pi√π piccola di BERT che mantiene il 97% delle sue prestazioni, pur essendo il 40% pi√π piccola in termini di numero di parametri. La riduzione delle dimensioni si traduce anche in un numero di FLOP inferiore, rendendo DistilBERT una scelta pi√π pratica per scenari con risorse limitate.</p>
<div class="no-row-height column-margin column-container"><div id="ref-devlin2018bert" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, e Kristina Toutanova. 2019. <span>¬´None¬ª</span>. In <em>Proceedings of the 2019 Conference of the North</em>, 4171‚Äì86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/n19-1423">https://doi.org/10.18653/v1/n19-1423</a>.
</div></div><p>Sebbene il numero dei parametri indichi la dimensione del modello, non cattura completamente il costo computazionale. I FLOP forniscono una misura pi√π accurata del carico computazionale, evidenziando i compromessi pratici nell‚Äôimplementazione del modello. Questo passaggio dal conteggio dei parametri ai FLOP riflette la crescente consapevolezza del settore delle sfide di implementazione in contesti diversi.</p>
</section>
<section id="efficienza" class="level5">
<h5 class="anchored" data-anchor-id="efficienza">Efficienza</h5>
<p>Anche le metriche di efficienza, come il consumo di memoria e la latenza/capacit√† di elaborazione, hanno acquisito importanza. Queste metriche sono particolarmente cruciali quando si distribuiscono modelli su dispositivi edge o in applicazioni in tempo reale, poich√© misurano la velocit√† con cui un modello pu√≤ elaborare i dati e la quantit√† di memoria richiesta. In questo contesto, le curve di Pareto vengono spesso utilizzate per visualizzare il compromesso tra diverse metriche, aiutando le parti interessate a decidere quale modello si adatta meglio alle loro esigenze.</p>
</section>
</section>
</section>
<section id="lezioni-apprese" class="level3 page-columns page-full" data-number="11.5.3">
<h3 data-number="11.5.3" class="anchored" data-anchor-id="lezioni-apprese"><span class="header-section-number">11.5.3</span> Lezioni Apprese</h3>
<p>Il benchmarking dei modelli ci ha offerto diverse preziose intuizioni che possono essere sfruttate per guidare l‚Äôinnovazione nei benchmark di sistema. La progressione dei modelli di apprendimento automatico √® stata profondamente influenzata dall‚Äôavvento delle classifiche e dalla disponibilit√† open source di modelli e set di dati. Questi elementi hanno svolto il ruolo di catalizzatori significativi, spingendo l‚Äôinnovazione e accelerando l‚Äôintegrazione di modelli all‚Äôavanguardia negli ambienti di produzione. Tuttavia, come approfondiremo ulteriormente, questi non sono gli unici fattori che contribuiscono allo sviluppo dei benchmark di apprendimento automatico.</p>
<p>Le classifiche svolgono un ruolo fondamentale nel fornire un metodo oggettivo e trasparente per ricercatori e professionisti per valutare l‚Äôefficacia di diversi modelli, classificandoli in base alle loro prestazioni nei benchmark. Questo sistema promuove un ambiente competitivo, incoraggiando lo sviluppo di modelli che non siano solo accurati ma anche efficienti. L‚ÄôImageNet Large Scale Visual Recognition Challenge (ILSVRC) ne √® un ottimo esempio, con la sua classifica annuale che contribuisce in modo significativo allo sviluppo di modelli innovativi come AlexNet.</p>
<p>L‚Äôaccesso open source a modelli e set di dati all‚Äôavanguardia diffonde ulteriormente l‚Äôapprendimento automatico, facilitando la collaborazione tra ricercatori e professionisti in tutto il mondo. Questo accesso aperto accelera il processo di test, convalida e distribuzione di nuovi modelli in ambienti di produzione, come dimostrato dall‚Äôadozione diffusa di modelli come BERT e GPT-3 in varie applicazioni, dall‚Äôelaborazione del linguaggio naturale a compiti multimodali pi√π complessi.</p>
<p>Piattaforme di collaborazione della comunit√† come Kaggle hanno rivoluzionato il settore ospitando competizioni che uniscono data scientist da tutto il mondo per risolvere problemi intricati. Benchmark specifici fungono da paletti per l‚Äôinnovazione e lo sviluppo di modelli.</p>
<p>Inoltre, la disponibilit√† di set di dati diversi e di alta qualit√† √® fondamentale per l‚Äôaddestramento e il test dei modelli di apprendimento automatico. Set di dati come ImageNet hanno svolto un ruolo fondamentale nell‚Äôevoluzione dei modelli di riconoscimento delle immagini, mentre ampi set di dati di testo hanno facilitato i progressi nei modelli di elaborazione del linguaggio naturale.</p>
<p>Infine, √® necessario supportare i contributi di istituti accademici e di ricerca. Il loro ruolo nella pubblicazione di articoli di ricerca, nella condivisione di risultati in conferenze e nella promozione della collaborazione tra varie istituzioni ha contribuito in modo significativo al progresso dei modelli e dei benchmark di apprendimento automatico.</p>
<section id="tendenze-emergenti" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="tendenze-emergenti">Tendenze Emergenti</h4>
<p>Man mano che i modelli di apprendimento automatico diventano pi√π sofisticati, lo diventano anche i benchmark necessari per valutarli in modo accurato. Ci sono diversi benchmark e dataset emergenti che stanno guadagnando popolarit√† grazie alla loro capacit√† di valutare i modelli in scenari pi√π complessi e realistici:</p>
<p><strong>Dataset Multimodali:</strong> Questi set di dati contengono pi√π tipi di dati, come testo, immagini e audio, per rappresentare meglio le situazioni del mondo reale. Un esempio √® VQA (Visual Question Answering) <span class="citation" data-cites="antol2015vqa">(<a href="../../../references.it.html#ref-antol2015vqa" role="doc-biblioref">Antol et al. 2015</a>)</span>, in cui viene testata la capacit√† dei modelli di rispondere a domande basate su testo sulle immagini.</p>
<div class="no-row-height column-margin column-container"><div id="ref-antol2015vqa" class="csl-entry" role="listitem">
Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, e Devi Parikh. 2015. <span>¬´VQA: Visual Question Answering¬ª</span>. In <em>2015 IEEE International Conference on Computer Vision (ICCV)</em>, 2425‚Äì33. IEEE. <a href="https://doi.org/10.1109/iccv.2015.279">https://doi.org/10.1109/iccv.2015.279</a>.
</div></div><p><strong>Valutazione di Correttezza e Bias:</strong> C‚Äô√® una crescente attenzione alla creazione di benchmark che valutino l‚Äôequit√†/Correttezza e i bias [pregiudizi] dei modelli di apprendimento automatico. Esempi includono il toolkit <a href="https://ai-fairness-360.org/">AI Fairness 360</a>, che offre un set completo di metriche e set di dati per valutare il bias nei modelli.</p>
<p><strong>Generalizzazione Out-of-Distribution:</strong> Test di quanto bene i modelli funzionano su dati diversi dalla distribuzione di training originale. Questo valuta la capacit√† del modello di generalizzare a dati nuovi e inediti. Esempi di benchmark sono Wilds <span class="citation" data-cites="koh2021wilds">(<a href="../../../references.it.html#ref-koh2021wilds" role="doc-biblioref">Koh et al. 2021</a>)</span>, RxRx e ANC-Bench.</p>
<div class="no-row-height column-margin column-container"><div id="ref-koh2021wilds" class="csl-entry" role="listitem">
Koh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. <span>¬´WILDS: A Benchmark of in-the-Wild Distribution Shifts.¬ª</span> In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, a cura di Marina Meila e Tong Zhang, 139:5637‚Äì64. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/koh21a.html">http://proceedings.mlr.press/v139/koh21a.html</a>.
</div><div id="ref-hendrycks2021natural" class="csl-entry" role="listitem">
Hendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, e Dawn Song. 2021. <span>¬´Natural Adversarial Examples¬ª</span>. In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 15257‚Äì66. IEEE. <a href="https://doi.org/10.1109/cvpr46437.2021.01501">https://doi.org/10.1109/cvpr46437.2021.01501</a>.
</div><div id="ref-xie2020adversarial" class="csl-entry" role="listitem">
Xie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, e Quoc V. Le. 2020. <span>¬´Adversarial Examples Improve Image Recognition¬ª</span>. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 816‚Äì25. IEEE. <a href="https://doi.org/10.1109/cvpr42600.2020.00090">https://doi.org/10.1109/cvpr42600.2020.00090</a>.
</div></div><p><strong>Robustezza Avversaria:</strong> Valutazione delle prestazioni del modello in caso di attacchi avversari o perturbazioni ai dati di input. Questo testa la robustezza del modello. Esempi di benchmark sono ImageNet-A <span class="citation" data-cites="hendrycks2021natural">(<a href="../../../references.it.html#ref-hendrycks2021natural" role="doc-biblioref">Hendrycks et al. 2021</a>)</span>, ImageNet-C <span class="citation" data-cites="xie2020adversarial">(<a href="../../../references.it.html#ref-xie2020adversarial" role="doc-biblioref">Xie et al. 2020</a>)</span> e CIFAR-10.1.</p>
<p><strong>Prestazioni nel Mondo Reale:</strong> Test di modelli su set di dati del mondo reale che corrispondono da vicino alle attivit√† finali anzich√© solo su set di dati di benchmark predefiniti. Esempi sono set di dati di imaging medico per attivit√† sanitarie o log di chat di assistenza clienti per sistemi di dialogo.</p>
<p><strong>Efficienza Energetica e di Calcolo:</strong> Benchmark che misurano le risorse di calcolo necessarie per ottenere una particolare accuratezza. Questo valuta l‚Äôefficienza del modello. Esempi sono MLPerf e Greenbench, gi√† discussi nella sezione Benchmarking dei sistemi.</p>
<p><strong>Interpretabilit√† e Spiegabilit√†:</strong> Benchmark che valutano quanto sia facile comprendere e spiegare la logica interna e le previsioni di un modello. Esempi di parametri sono la fedelt√† ai gradienti di input e la coerenza delle spiegazioni.</p>
</section>
</section>
<section id="limitazioni-e-sfide" class="level3" data-number="11.5.4">
<h3 data-number="11.5.4" class="anchored" data-anchor-id="limitazioni-e-sfide"><span class="header-section-number">11.5.4</span> Limitazioni e Sfide</h3>
<p>Sebbene i benchmark dei modelli siano uno strumento essenziale per valutare i modelli di machine learning, √® necessario affrontare diverse limitazioni e sfide per garantire che riflettano accuratamente le prestazioni in scenari reali.</p>
<p><strong>Il dataset non corrisponde a scenari reali:</strong> Spesso, i dati utilizzati nei benchmark dei modelli vengono puliti e preelaborati a tal punto che potrebbe essere necessario rappresentare accuratamente i dati che un modello incontrerebbe in applicazioni reali. Questa versione idealizzata dei dati pu√≤ portare a una sovrastima delle prestazioni di un modello. Nel caso del set di dati ImageNet, le immagini sono ben etichettate e categorizzate. Tuttavia, in uno scenario reale, un modello potrebbe dover gestire immagini sfocate che potrebbero essere meglio illuminate o scattate da angolazioni scomode. Questa discrepanza pu√≤ influire in modo significativo sulle prestazioni del modello.</p>
<p><strong>Sim2Real Gap:</strong> Il Sim2Real Gap si riferisce alla differenza nelle prestazioni di un modello quando si passa da un ambiente simulato a un ambiente reale. Questo gap √® spesso osservato nella robotica, dove un robot addestrato in un ambiente simulato ha difficolt√† a svolgere compiti nel mondo reale a causa della complessit√† e dell‚Äôimprevedibilit√† degli ambienti reali. Un robot addestrato a raccogliere oggetti in un ambiente simulato potrebbe aver bisogno di aiuto per svolgere lo stesso compito nel mondo reale perch√© l‚Äôambiente simulato non rappresenta accuratamente le complessit√† della fisica, dell‚Äôilluminazione e della variabilit√† degli oggetti del mondo reale.</p>
<p><strong>Sfide nella Creazione di Dataset:</strong> La creazione di un set di dati per il benchmarking del modello √® un‚Äôattivit√† impegnativa che richiede un‚Äôattenta considerazione di vari fattori come qualit√† dei dati, diversit√† e rappresentazione. Come discusso nella sezione di ingegneria dei dati, garantire che i dati siano puliti, imparziali e rappresentativi dello scenario del mondo reale √® fondamentale per l‚Äôaccuratezza e l‚Äôaffidabilit√† del benchmark. Ad esempio, quando si crea un set di dati per un‚Äôattivit√† correlata all‚Äôassistenza sanitaria, √® importante assicurarsi che i dati siano rappresentativi dell‚Äôintera popolazione e non distorti verso un particolare gruppo demografico. Ci√≤ garantisce che il modello funzioni bene in diverse popolazioni di pazienti.</p>
<p>I benchmark del modello sono essenziali per misurare la capacit√† di un‚Äôarchitettura di modello di risolvere un‚Äôattivit√† fissa, ma √® importante affrontare le limitazioni e le sfide ad essi associate. Ci√≤ include il garantire che il set di dati rappresenti accuratamente scenari del mondo reale, affrontare il divario Sim2Real e superare le sfide della creazione di set di dati imparziali e rappresentativi. Affrontando queste sfide e molte altre, possiamo garantire che i benchmark del modello forniscano una valutazione pi√π accurata e affidabile delle prestazioni di un modello in applicazioni del mondo reale.</p>
<p>Lo <a href="https://arxiv.org/pdf/1804.03209.pdf">Speech Commands dataset</a> e il suo successore <a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/fe131d7f5a6b38b23cc967316c13dae2-Paper-round2.pdf">MSWC</a> sono benchmark comuni per una delle applicazioni TinyML per eccellenza, l‚Äôindividuazione delle parole chiave. I comandi vocali stabiliscono metriche di errore di streaming oltre la precisione di classificazione standard top-1 pi√π pertinenti al caso d‚Äôuso di individuazione delle parole chiave. L‚Äôutilizzo di metriche pertinenti ai casi √® ci√≤ che eleva un dataset a un benchmark del modello.</p>
</section>
</section>
<section id="benchmarking-dei-dati" class="level2 page-columns page-full" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="benchmarking-dei-dati"><span class="header-section-number">11.6</span> Benchmarking dei Dati</h2>
<p>Negli ultimi anni, l‚Äôintelligenza artificiale si √® concentrata sullo sviluppo di modelli di apprendimento automatico sempre pi√π sofisticati, come i grandi modelli linguistici. L‚Äôobiettivo √® stato quello di creare modelli in grado di prestazioni di livello umano o sovrumane su un‚Äôampia gamma di attivit√†, addestrandoli su enormi set di dati. Questo approccio incentrato sul modello ha prodotto rapidi progressi, con modelli che hanno ottenuto risultati all‚Äôavanguardia su molti benchmark consolidati. <a href="#fig-superhuman-perf" class="quarto-xref">Figura&nbsp;<span>11.9</span></a> mostra le prestazioni dei sistemi di intelligenza artificiale rispetto alle prestazioni umane (contrassegnate dalla linea orizzontale a 0) in cinque applicazioni: riconoscimento della scrittura a mano, riconoscimento vocale, riconoscimento delle immagini, comprensione della lettura e comprensione del linguaggio. Negli ultimi dieci anni, le prestazioni dell‚Äôintelligenza artificiale hanno superato quelle degli esseri umani.</p>
<div id="fig-superhuman-perf" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-superhuman-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/dynabench.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-superhuman-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.9: IA e prestazioni umane. Fonte: <span class="citation" data-cites="kiela2021dynabench">Kiela et al. (<a href="../../../references.it.html#ref-kiela2021dynabench" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>Tuttavia, le crescenti preoccupazioni su questioni come pregiudizi, sicurezza e robustezza persistono anche nei modelli che raggiungono un‚Äôelevata accuratezza sui benchmark standard. Inoltre, alcuni set di dati popolari utilizzati per la valutazione dei modelli stanno iniziando a saturarsi, con modelli che raggiungono prestazioni quasi perfette su divisioni di test esistenti <span class="citation" data-cites="kiela2021dynabench">(<a href="../../../references.it.html#ref-kiela2021dynabench" role="doc-biblioref">Kiela et al. 2021</a>)</span>. Come semplice esempio, ci sono immagini di test nel classico dataset di cifre scritte a mano MNIST che potrebbero sembrare indecifrabili per la maggior parte dei valutatori umani, ma a cui √® stata assegnata un‚Äôetichetta quando √® stato creato il set di dati: i modelli che concordano con quelle etichette potrebbero sembrare esibire prestazioni sovrumane, ma potrebbero invece catturare solo idiosincrasie del processo di etichettatura e acquisizione dalla creazione del set di dati nel 1994. Con lo stesso spirito, i ricercatori di visione artificiale ora chiedono: ‚ÄúAbbiamo finito con ImageNet?‚Äù <span class="citation" data-cites="beyer2020we">(<a href="../../../references.it.html#ref-beyer2020we" role="doc-biblioref">Beyer et al. 2020</a>)</span>. Ci√≤ evidenzia i limiti nell‚Äôapproccio convenzionale incentrato sul modello di ottimizzazione dell‚Äôaccuratezza su set di dati fissi tramite innovazioni architettoniche.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kiela2021dynabench" class="csl-entry" role="listitem">
Kiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. <span>¬´Dynabench: Rethinking Benchmarking in NLP¬ª</span>. In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 4110‚Äì24. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.naacl-main.324">https://doi.org/10.18653/v1/2021.naacl-main.324</a>.
</div><div id="ref-beyer2020we" class="csl-entry" role="listitem">
Beyer, Lucas, Olivier J. H√©naff, Alexander Kolesnikov, Xiaohua Zhai, e A√§ron van den Oord. 2020. <span>¬´Are we done with ImageNet?¬ª</span> <em>ArXiv preprint</em> abs/2006.07159 (giugno). <a href="http://arxiv.org/abs/2006.07159v1">http://arxiv.org/abs/2006.07159v1</a>.
</div></div><p>Sta emergendo un paradigma alternativo chiamato IA incentrata sui dati. Invece di trattare i dati come statici e concentrarsi strettamente sulle prestazioni del modello, questo approccio riconosce che i modelli sono validi solo quanto i loro dati di training. Quindi, l‚Äôenfasi si sposta sulla cura di dataset di alta qualit√† che riflettano meglio la complessit√† del mondo reale, sviluppando benchmark di valutazione pi√π informativi e considerando attentamente come i dati vengono campionati, preelaborati e aumentati. L‚Äôobiettivo √® ottimizzare il comportamento del modello migliorando i dati anzich√© semplicemente ottimizzando le metriche su set di dati imperfetti. L‚Äôintelligenza artificiale incentrata sui dati esamina e migliora criticamente i dati stessi per produrre un‚Äôintelligenza artificiale utile. Ci√≤ riflette un‚Äôimportante evoluzione nella mentalit√†, poich√© il campo affronta le carenze di un benchmarking ristretto.</p>
<p>Questa sezione esplorer√† le principali differenze tra gli approcci all‚Äôintelligenza artificiale incentrati sui modelli e sui dati. Questa distinzione ha importanti implicazioni sul modo in cui eseguiamo il benchmarking dei sistemi di intelligenza artificiale. In particolare, vedremo come concentrarsi sulla qualit√† dei dati e sull‚Äôefficienza pu√≤ migliorare direttamente le prestazioni dell‚Äôapprendimento automatico come alternativa all‚Äôottimizzazione delle sole architetture dei modelli. L‚Äôapproccio incentrato sui dati riconosce che i modelli sono validi solo quanto i loro dati di addestramento. Quindi, migliorare la cura dei dati, i benchmark di valutazione e i processi di gestione dei dati pu√≤ produrre sistemi di intelligenza artificiale pi√π sicuri, pi√π equi e pi√π robusti. Ripensare il benchmarking per dare priorit√† ai dati insieme ai modelli rappresenta un‚Äôimportante evoluzione, poich√© il settore si sforza di fornire un impatto affidabile nel mondo reale.</p>
<section id="limitazioni-dellia-incentrata-sul-modello" class="level3" data-number="11.6.1">
<h3 data-number="11.6.1" class="anchored" data-anchor-id="limitazioni-dellia-incentrata-sul-modello"><span class="header-section-number">11.6.1</span> Limitazioni dell‚ÄôIA Incentrata sul Modello</h3>
<p>Nell‚Äôera dell‚ÄôIA incentrata sul modello, una caratteristica importante era lo sviluppo di architetture di modelli complesse. Ricercatori e professionisti hanno dedicato notevoli sforzi alla progettazione di modelli sofisticati e intricati nella ricerca di prestazioni superiori. Ci√≤ ha spesso comportato l‚Äôincorporazione di livelli aggiuntivi e la messa a punto di una moltitudine di iperparametri per ottenere miglioramenti nell‚Äôaccuratezza. Contemporaneamente, c‚Äôera una notevole enfasi sullo sfruttamento di algoritmi avanzati. Questi algoritmi, spesso in prima linea nelle ultime ricerche, sono stati impiegati per migliorare le prestazioni dei modelli di IA. L‚Äôobiettivo principale di questi algoritmi era ottimizzare il processo di apprendimento dei modelli, estraendo cos√¨ il massimo delle informazioni dai dati di addestramento.</p>
<p>Sebbene l‚Äôapproccio incentrato sul modello sia stato centrale per molti progressi nell‚ÄôIA, ha diverse aree di miglioramento. Innanzitutto, lo sviluppo di architetture di modelli complesse pu√≤ spesso portare a un overfitting. Questo √® quando il modello funziona bene sui dati di addestramento ma deve generalizzare a nuovi dati mai visti. I layer aggiuntivi e la complessit√† possono catturare il rumore nei dati di training come se fosse un pattern reale, danneggiando le prestazioni del modello su nuovi dati.</p>
<p>In secondo luogo, affidarsi ad algoritmi avanzati pu√≤ a volte oscurare la reale comprensione del funzionamento di un modello. Questi algoritmi spesso agiscono come una scatola nera, rendendo difficile interpretare il modo in cui il modello prende decisioni. Questa mancanza di trasparenza pu√≤ essere un ostacolo significativo, specialmente in applicazioni critiche come sanit√† e finanza, dove la comprensione del processo decisionale del modello √® fondamentale.</p>
<p>In terzo luogo, l‚Äôenfasi sul raggiungimento di risultati all‚Äôavanguardia su set di dati di riferimento pu√≤ a volte essere fuorviante. Questi dataset devono rappresentare in modo pi√π completo le complessit√† e la variabilit√† dei dati del mondo reale. Un modello che funziona bene su un set di dati di riferimento potrebbe non essere necessariamente generalizzato bene a dati nuovi e mai visti in un‚Äôapplicazione del mondo reale. Questa discrepanza pu√≤ portare a una falsa fiducia nelle capacit√† del modello e ostacolarne l‚Äôapplicabilit√† pratica.</p>
<p>Infine, l‚Äôapproccio incentrato sul modello spesso si basa su grandi set di dati etichettati per l‚Äôaddestramento. Tuttavia, ottenere tali set di dati richiede tempo e impegno in molti scenari del mondo reale. Questa dipendenza da grandi dataset limita anche l‚Äôapplicabilit√† dell‚ÄôIA in domini in cui i dati sono scarsi o costosi da etichettare.</p>
<p>Come risultato delle ragioni di cui sopra, e di molte altre, la comunit√† dell‚ÄôIA sta passando a un approccio pi√π incentrato sui dati. Invece di concentrarsi solo sull‚Äôarchitettura del modello, i ricercatori stanno ora dando priorit√† alla cura di set di dati di alta qualit√†, allo sviluppo di migliori benchmark di valutazione e alla considerazione di come i dati vengono campionati e preelaborati. L‚Äôidea chiave √® che i modelli sono validi solo quanto i loro dati di training. Quindi, concentrandoci sull‚Äôottenimento dei dati giusti, potremo sviluppare sistemi di intelligenza artificiale pi√π equi, sicuri e allineati con i valori umani. Questo cambiamento incentrato sui dati rappresenta un importante cambiamento di mentalit√† man mano che l‚Äôintelligenza artificiale progredisce.</p>
</section>
<section id="verso-unintelligenza-artificiale-incentrata-sui-dati" class="level3 page-columns page-full" data-number="11.6.2">
<h3 data-number="11.6.2" class="anchored" data-anchor-id="verso-unintelligenza-artificiale-incentrata-sui-dati"><span class="header-section-number">11.6.2</span> Verso un‚ÄôIntelligenza Artificiale Incentrata sui Dati</h3>
<p>L‚Äôintelligenza artificiale incentrata sui dati √® un paradigma che sottolinea l‚Äôimportanza di dataset di alta qualit√†, ben etichettati e diversificati nello sviluppo di modelli di intelligenza artificiale. Contrariamente all‚Äôapproccio incentrato sul modello, che si concentra sulla rifinitura e l‚Äôiterazione dell‚Äôarchitettura e dell‚Äôalgoritmo del modello per migliorare le prestazioni, l‚Äôintelligenza artificiale incentrata sui dati d√† priorit√† alla qualit√† dei dati di input come motore principale per migliorare le prestazioni del modello. I dati di alta qualit√† sono <a href="https://landing.ai/blog/tips-for-a-data-centric-ai-approach/">puliti, ben etichettati</a> e rappresentativi degli scenari del mondo reale che il modello incontrer√†. Al contrario, i dati di bassa qualit√† possono portare a scarse prestazioni del modello, indipendentemente dalla complessit√† o dalla sofisticatezza dell‚Äôarchitettura del modello.</p>
<p>L‚Äôintelligenza artificiale incentrata sui dati pone una forte enfasi sulla pulizia e l‚Äôetichettatura dei dati. La pulizia comporta la rimozione di valori anomali, la gestione dei valori mancanti e la risoluzione di altre incongruenze nei dati. L‚Äôetichettatura, d‚Äôaltro canto, comporta l‚Äôassegnazione di etichette significative e accurate ai dati. Entrambi questi processi sono fondamentali per garantire che il modello di intelligenza artificiale venga addestrato su dati accurati e pertinenti. Un altro aspetto importante dell‚Äôapproccio incentrato sui dati √® il ‚Äúdata augmentation‚Äù [l‚Äôaumento dei dati]. Ci√≤ comporta l‚Äôaumento artificiale delle dimensioni e della diversit√† del set di dati applicando varie trasformazioni ai dati, come rotazione, ridimensionamento e capovolgimento delle immagini di addestramento. L‚Äôaumento dei dati aiuta a migliorare la robustezza del modello e le capacit√† di generalizzazione.</p>
<p>Ci sono diversi vantaggi nell‚Äôadottare un approccio incentrato sui dati per lo sviluppo dell‚Äôintelligenza artificiale. Innanzitutto, porta a prestazioni del modello migliorate e capacit√† di generalizzazione. Assicurandosi che il modello venga addestrato su dati diversi e di alta qualit√†, il modello pu√≤ generalizzare meglio a dati nuovi e mai visti <span class="citation" data-cites="gaviria2022dollar">(<a href="../../../references.it.html#ref-gaviria2022dollar" role="doc-biblioref">Mattson et al. 2020b</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>Inoltre, un approccio incentrato sui dati pu√≤ spesso portare a modelli pi√π semplici che sono pi√π facili da interpretare e gestire. Questo perch√© l‚Äôenfasi √® sui dati piuttosto che sull‚Äôarchitettura del modello, il che significa che i modelli pi√π semplici possono raggiungere prestazioni elevate quando addestrati su dati di alta qualit√†.</p>
<p>Il passaggio all‚ÄôIA incentrata sui dati rappresenta un significativo cambiamento di paradigma. Dando priorit√† alla qualit√† dei dati di input, questo approccio cerca di modellare le prestazioni e le capacit√† di generalizzazione, portando in ultima analisi a sistemi di intelligenza artificiale pi√π solidi e affidabili. <a href="#fig-data-vs-model" class="quarto-xref">Figura&nbsp;<span>11.10</span></a> illustra questa differenza. Mentre continuiamo ad avanzare nella nostra comprensione e applicazione dell‚ÄôIA, √® probabile che l‚Äôapproccio incentrato sui dati svolga un ruolo importante nel plasmare il futuro di questo campo.</p>
<div id="fig-data-vs-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-vs-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/datavsmodelai.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-vs-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.10: Sviluppo di machine learning incentrato sul modello e incentrato sui dati. Fonte: <a href="https://blogs.nvidia.com/blog/difference-deep-learning-training-inference-ai/">NVIDIA</a>
</figcaption>
</figure>
</div>
</section>
<section id="benchmarking-dei-dati-1" class="level3 page-columns page-full" data-number="11.6.3">
<h3 data-number="11.6.3" class="anchored" data-anchor-id="benchmarking-dei-dati-1"><span class="header-section-number">11.6.3</span> Benchmarking dei Dati</h3>
<p>Il benchmarking dei dati mira a valutare problemi comuni nei set di dati, come l‚Äôidentificazione di errori di etichetta, caratteristiche rumorose, squilibrio di rappresentazione (ad esempio, su 1000 classi in Imagenet-1K, ci sono oltre 100 categorie che sono solo tipi di cani), squilibrio di classe (dove alcune classi hanno molti pi√π campioni di altre), se i modelli addestrati su un dato set di dati possono generalizzare a caratteristiche fuori distribuzione o quali tipi di bias potrebbero esistere in un dato set di dati <span class="citation" data-cites="gaviria2022dollar">(<a href="../../../references.it.html#ref-gaviria2022dollar" role="doc-biblioref">Mattson et al. 2020b</a>)</span>. Nella sua forma pi√π semplice, il benchmarking dei dati mira a migliorare l‚Äôaccuratezza su un set di test rimuovendo campioni di addestramento rumorosi o etichettati in modo errato mantenendo fissa l‚Äôarchitettura del modello. Recenti competizioni nel benchmarking dei dati hanno invitato i partecipanti a presentare nuove strategie di ‚Äúaugmentation‚Äù e tecniche di apprendimento attivo.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gaviria2022dollar" class="csl-entry" role="listitem">
Mattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020b. <span>¬´MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance¬ª</span>. <em>IEEE Micro</em> 40 (2): 8‚Äì16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div></div><p>Le tecniche incentrate sui dati continuano a guadagnare attenzione nel benchmarking, soprattutto perch√© i modelli di base sono sempre pi√π addestrati su obiettivi auto-supervisionati. Rispetto ai set di dati pi√π piccoli come Imagenet-1K, i set di dati pi√π grandi comunemente usati nell‚Äôapprendimento auto-supervisionato, come Common Crawl, OpenImages e LAION-5B, contengono quantit√† maggiori di rumore, duplicati, bias e dati potenzialmente offensivi.</p>
<p><a href="https://www.datacomp.ai/">DataComp</a> √® una competizione di dataset lanciata di recente che ha come obiettivo la valutazione di grandi corpora. DataComp si concentra sulle coppie linguaggio-immagine usate per addestrare i modelli CLIP. Il documento introduttivo rileva che quando il budget di elaborazione totale per l‚Äôaddestramento √® costante, i modelli CLIP pi√π performanti nelle attivit√† downstream, come la classificazione ImageNet, vengono addestrati solo sul 30% del pool di campioni disponibile. Ci√≤ suggerisce che un corretto filtraggio di grandi corpora √® fondamentale per migliorare l‚Äôaccuratezza dei modelli di base. Analogamente, Demystifying CLIP Data <span class="citation" data-cites="xu2023demystifying">(<a href="../../../references.it.html#ref-xu2023demystifying" role="doc-biblioref">Xu et al. 2023</a>)</span> chiede se il successo di CLIP sia attribuibile all‚Äôarchitettura o al set di dati.</p>
<div class="no-row-height column-margin column-container"><div id="ref-xu2023demystifying" class="csl-entry" role="listitem">
Xu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, e Christoph Feichtenhofer. 2023. <span>¬´Demystifying CLIP Data¬ª</span>. <em>ArXiv preprint</em> abs/2309.16671 (settembre). <a href="http://arxiv.org/abs/2309.16671v4">http://arxiv.org/abs/2309.16671v4</a>.
</div></div><p><a href="https://www.dataperf.org/">DataPerf</a> √® un altro recente lavoro incentrato sul benchmarking dei dati in varie modalit√†. DataPerf offre round di competizione online per stimolare il miglioramento dei dataset. L‚Äôofferta inaugurale √® stata lanciata con sfide in termini di visione, parlato, acquisizione, debug e prompt di testo per la generazione di immagini.</p>
</section>
<section id="efficienza-dei-dati" class="level3 page-columns page-full" data-number="11.6.4">
<h3 data-number="11.6.4" class="anchored" data-anchor-id="efficienza-dei-dati"><span class="header-section-number">11.6.4</span> Efficienza dei Dati</h3>
<p>Man mano che i modelli di apprendimento automatico diventano pi√π grandi e complessi e le risorse di elaborazione diventano pi√π scarse di fronte alla crescente domanda, diventa difficile soddisfare i requisiti di elaborazione anche con le flotte di machine learning pi√π grandi. Per superare queste sfide e garantire la scalabilit√† del sistema di apprendimento automatico, √® necessario esplorare nuove opportunit√† che aumentino gli approcci convenzionali alla scalabilit√† delle risorse.</p>
<p>Migliorare la qualit√† dei dati pu√≤ essere un metodo utile per avere un impatto significativo sulle prestazioni del sistema di apprendimento automatico. Uno dei principali vantaggi del miglioramento della qualit√† dei dati √® il potenziale di poter ridurre le dimensioni del set di dati di addestramento mantenendo o addirittura migliorando le prestazioni del modello. Questa riduzione delle dimensioni dei dati √® direttamente correlata alla quantit√† di tempo di addestramento richiesto, consentendo cos√¨ ai modelli di convergere in modo pi√π rapido ed efficiente. Raggiungere questo equilibrio tra qualit√† dei dati e dimensioni del set di dati √® un compito impegnativo che richiede lo sviluppo di metodi, algoritmi e tecniche sofisticati.</p>
<p>Possono essere adottati diversi approcci per migliorare la qualit√† dei dati. Questi metodi includono e non sono limitati a quanto segue:</p>
<ul>
<li><strong>Pulizia dei Dati:</strong> Ci√≤ comporta la gestione dei valori mancanti, la correzione degli errori e la rimozione dei valori anomali. I dati puliti assicurano che il modello non stia imparando da rumore o imprecisioni.</li>
<li><strong>Interpretabilit√† e Spiegabilit√† dei Dati:</strong> Le tecniche comuni includono LIME <span class="citation" data-cites="ribeiro2016should">(<a href="../../../references.it.html#ref-ribeiro2016should" role="doc-biblioref">Ribeiro, Singh, e Guestrin 2016</a>)</span>, che fornisce informazioni sui limiti decisionali dei classificatori, e valori Shapley <span class="citation" data-cites="lundberg2017unified">(<a href="../../../references.it.html#ref-lundberg2017unified" role="doc-biblioref">Lundberg e Lee 2017</a>)</span>, che stimano l‚Äôimportanza dei singoli campioni nel contribuire alle previsioni di un modello.</li>
<li><strong>Feature Engineering:</strong> Trasformare o creare nuove funzionalit√† pu√≤ migliorare significativamente le prestazioni del modello fornendo informazioni pi√π pertinenti per l‚Äôapprendimento.</li>
<li><strong>Data Augmentation:</strong> Aumentare i dati creando nuovi campioni tramite varie trasformazioni pu√≤ aiutare a migliorare la robustezza e la generalizzazione del modello.</li>
<li><strong>Active Learning:</strong> Questo √® un approccio di apprendimento semi-supervisionato in cui il modello interroga attivamente un ‚Äúoracolo‚Äù umano per etichettare i campioni pi√π informativi <span class="citation" data-cites="coleman2022similarity">(<a href="../../../references.it.html#ref-coleman2022similarity" role="doc-biblioref">Coleman et al. 2022</a>)</span>. Ci√≤ garantisce che il modello venga addestrato sui dati pi√π rilevanti.</li>
<li><strong>Riduzione della Dimensionalit√†:</strong> Tecniche come PCA possono ridurre il numero di feature in un set di dati, riducendo cos√¨ la complessit√† e il tempo di training.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-ribeiro2016should" class="csl-entry" role="listitem">
Ribeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. <span>¬´‚Äù Why should i trust you?‚Äù Explaining the predictions of any classifier¬ª</span>. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, 1135‚Äì44.
</div><div id="ref-lundberg2017unified" class="csl-entry" role="listitem">
Lundberg, Scott M., e Su-In Lee. 2017. <span>¬´A Unified Approach to Interpreting Model Predictions¬ª</span>. In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765‚Äì74. <a href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</a>.
</div><div id="ref-coleman2022similarity" class="csl-entry" role="listitem">
Coleman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, e I. Zeki Yalniz. 2022. <span>¬´Similarity Search for Efficient Active Learning and Search of Rare Concepts¬ª</span>. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 36 (6): 6402‚Äì10. <a href="https://doi.org/10.1609/aaai.v36i6.20591">https://doi.org/10.1609/aaai.v36i6.20591</a>.
</div></div><p>Esistono molti altri metodi in circolazione. Ma l‚Äôobiettivo √® lo stesso. Affinare il set di dati e garantire che sia della massima qualit√† pu√≤ ridurre il tempo di addestramento necessario per la convergenza dei modelli. Tuttavia, per raggiungere questo obiettivo √® necessario sviluppare e implementare metodi, algoritmi e tecniche sofisticati in grado di pulire, preelaborare e aumentare i dati, mantenendo al contempo i campioni pi√π informativi. Questa √® una sfida continua che richieder√† una continua ricerca e innovazione nel campo dell‚Äôapprendimento automatico.</p>
</section>
</section>
<section id="la-tripletta" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="la-tripletta"><span class="header-section-number">11.7</span> La Tripletta</h2>
<p>Mentre i benchmark di sistema, modello e dati sono stati tradizionalmente studiati in modo isolato, si sta diffondendo la consapevolezza che per comprendere e far progredire completamente l‚ÄôIA, dobbiamo adottare una visione pi√π olistica. Iterando tra sistemi di benchmarking, modelli e dataset insieme, potrebbero emergere nuove intuizioni che non sono evidenti quando questi componenti vengono analizzati separatamente. Le prestazioni del sistema influiscono sulla precisione del modello, le capacit√† del modello determinano le esigenze dei dati e le caratteristiche dei dati determinano i requisiti del sistema.</p>
<p>Il benchmarking della triade di sistema, modello e dati in modo integrato porter√† probabilmente a scoperte sulla progettazione congiunta dei sistemi di IA, sulle propriet√† di generalizzazione dei modelli e sul ruolo della cura e della qualit√† dei dati nel consentire le prestazioni. Piuttosto che benchmark ristretti di singoli componenti, il futuro dell‚ÄôIA richiede benchmark che valutino la relazione simbiotica tra piattaforme di elaborazione, algoritmi e dati di training. Questa prospettiva a livello di sistema sar√† fondamentale per superare le attuali limitazioni e sbloccare il prossimo livello di capacit√† dell‚ÄôIA.</p>
<p><a href="#fig-benchmarking-trifecta" class="quarto-xref">Figura&nbsp;<span>11.11</span></a> illustra i molti modi potenziali per far interagire tra loro il benchmarking dei dati, quello dei modelli e quello dell‚Äôinfrastruttura di sistema. L‚Äôesplorazione di queste complesse interazioni probabilmente porter√† alla scoperta di nuove opportunit√† di ottimizzazione e capacit√† di miglioramento. La tripletta di benchmark di dati, modelli e sistemi offre un ricco spazio per la progettazione congiunta e la co-ottimizzazione.</p>
<div id="fig-benchmarking-trifecta" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-benchmarking-trifecta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/benchmarking_trifecta.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-benchmarking-trifecta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.11: La tripletta del Benchmarking.
</figcaption>
</figure>
</div>
<p>Sebbene questa prospettiva integrata rappresenti una tendenza emergente, il settore ha ancora molto da scoprire sulle sinergie e i compromessi tra questi componenti. Mentre eseguiamo il benchmarking iterativo di combinazioni di dati, modelli e sistemi, emergeranno nuove intuizioni che rimangono nascoste quando questi elementi vengono studiati separatamente. Questo approccio di benchmarking multiforme che traccia le intersezioni di dati, algoritmi e hardware promette di essere una strada fruttuosa per importanti progressi nell‚Äôintelligenza artificiale, anche se √® ancora nelle sue fasi iniziali.</p>
</section>
<section id="benchmark-per-tecnologie-emergenti" class="level2 page-columns page-full" data-number="11.8">
<h2 data-number="11.8" class="anchored" data-anchor-id="benchmark-per-tecnologie-emergenti"><span class="header-section-number">11.8</span> Benchmark per Tecnologie Emergenti</h2>
<p>Date le loro significative differenze rispetto alle tecniche esistenti, le tecnologie emergenti possono essere particolarmente difficili da progettare per i benchmark. I benchmark standard utilizzati per le tecnologie esistenti potrebbero non evidenziare le feature chiave del nuovo approccio. Al contrario, i nuovi benchmark potrebbero essere visti come artificiosi per favorire la tecnologia emergente rispetto ad altre. Potrebbero essere cos√¨ diversi dai benchmark esistenti da non poter essere compresi e perdere significato. Pertanto, i benchmark per le tecnologie emergenti devono bilanciare equit√†, applicabilit√† e facilit√† di confronto con quelli esistenti.</p>
<p>Un esempio di tecnologia emergente in cui il benchmarking si √® dimostrato particolarmente difficile √® nel <a href="@sec-neuromorphic">Neuromorphic Computing</a>. Utilizzando il cervello come fonte di ispirazione per un‚Äôintelligenza generale scalabile, robusta ed efficiente dal punto di vista energetico, il calcolo neuromorfico <span class="citation" data-cites="schuman2022opportunities">(<a href="../../../references.it.html#ref-schuman2022opportunities" role="doc-biblioref">Schuman et al. 2022</a>)</span> incorpora direttamente meccanismi biologicamente realistici sia negli algoritmi di calcolo che nell‚Äôhardware, come le reti neurali spiking <span class="citation" data-cites="maass1997networks">(<a href="../../../references.it.html#ref-maass1997networks" role="doc-biblioref">Maass 1997</a>)</span> e le architetture non-von Neumann architectures per eseguirle <span class="citation" data-cites="davies2018loihi modha2023neural">(<a href="../../../references.it.html#ref-davies2018loihi" role="doc-biblioref">Davies et al. 2018</a>; <a href="../../../references.it.html#ref-modha2023neural" role="doc-biblioref">Modha et al. 2023</a>)</span>. Da una prospettiva full-stack di modelli, tecniche di training e sistemi hardware, il calcolo neuromorfico differisce dall‚Äôhardware e dall‚Äôintelligenza artificiale convenzionali. Pertanto, esiste una sfida fondamentale nello sviluppo di benchmark equi e utili per guidare la tecnologia.</p>
<div class="no-row-height column-margin column-container"><div id="ref-schuman2022opportunities" class="csl-entry" role="listitem">
Schuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. <span>¬´Opportunities for neuromorphic computing algorithms and applications¬ª</span>. <em>Nature Computational Science</em> 2 (1): 10‚Äì19. <a href="https://doi.org/10.1038/s43588-021-00184-y">https://doi.org/10.1038/s43588-021-00184-y</a>.
</div><div id="ref-maass1997networks" class="csl-entry" role="listitem">
Maass, Wolfgang. 1997. <span>¬´Networks of spiking neurons: The third generation of neural network models¬ª</span>. <em>Neural Networks</em> 10 (9): 1659‚Äì71. <a href="https://doi.org/10.1016/s0893-6080(97)00011-7">https://doi.org/10.1016/s0893-6080(97)00011-7</a>.
</div><div id="ref-davies2018loihi" class="csl-entry" role="listitem">
Davies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. <span>¬´Loihi: A Neuromorphic Manycore Processor with On-Chip Learning¬ª</span>. <em>IEEE Micro</em> 38 (1): 82‚Äì99. <a href="https://doi.org/10.1109/mm.2018.112130359">https://doi.org/10.1109/mm.2018.112130359</a>.
</div><div id="ref-modha2023neural" class="csl-entry" role="listitem">
Modha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. <span>¬´Neural inference at the frontier of energy, space, and time¬ª</span>. <em>Science</em> 382 (6668): 329‚Äì35. <a href="https://doi.org/10.1126/science.adh1174">https://doi.org/10.1126/science.adh1174</a>.
</div><div id="ref-yik2023neurobench" class="csl-entry" role="listitem">
Yik, Jason, Korneel Van den Berghe, Douwe den Blanken, Younes Bouhadjar, Maxime Fabre, Paul Hueber, Denis Kleyko, et al. 2023. <span>¬´NeuroBench: A Framework for Benchmarking Neuromorphic Computing Algorithms and Systems¬ª</span>, aprile. <a href="http://arxiv.org/abs/2304.04640v3">http://arxiv.org/abs/2304.04640v3</a>.
</div></div><p>Un‚Äôiniziativa in corso per sviluppare benchmark neuromorfici standard √® NeuroBench <span class="citation" data-cites="yik2023neurobench">(<a href="../../../references.it.html#ref-yik2023neurobench" role="doc-biblioref">Yik et al. 2023</a>)</span>. Per un benchmarking adeguato del neuromorfico, NeuroBench segue principi di alto livello di <em>inclusivit√†</em> attraverso l‚Äôapplicabilit√† di attivit√† e metriche sia alle soluzioni neuromorfiche che non neuromorfiche, <em>attuabilit√†</em> dell‚Äôimplementazione utilizzando strumenti comuni e aggiornamenti <em>iterativi</em> per continuare a garantire la pertinenza man mano che il campo cresce rapidamente. NeuroBench e altri benchmark per le tecnologie emergenti forniscono una guida critica per le tecniche future, che potrebbero essere necessarie man mano che i limiti di scalabilit√† degli approcci esistenti si avvicinano.</p>
</section>
<section id="conclusione" class="level2" data-number="11.9">
<h2 data-number="11.9" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">11.9</span> Conclusione</h2>
<p>Ci√≤ che viene misurato viene migliorato. Questo capitolo ha esplorato la natura multiforme del benchmarking che abbraccia sistemi, modelli e dati. Il benchmarking √® importante per far progredire l‚ÄôIA in quanto fornisce le misurazioni essenziali per monitorare i progressi.</p>
<p>I benchmark del sistema ML consentono l‚Äôottimizzazione attraverso metriche di velocit√†, efficienza e scalabilit√†. I benchmark del modello guidano l‚Äôinnovazione attraverso attivit√† e metriche standardizzate oltre l‚Äôaccuratezza. I benchmark dei dati evidenziano problemi di qualit√†, equilibrio e rappresentazione.</p>
<p>√à importante notare che la valutazione di questi componenti in modo isolato presenta dei limiti. In futuro, sar√† probabilmente utilizzato un benchmarking pi√π integrato per esplorare l‚Äôinterazione tra benchmark di sistema, modello e dati. Questa visione promette nuove intuizioni sulla progettazione congiunta di dati, algoritmi e infrastrutture.</p>
<p>Man mano che l‚ÄôIA diventa pi√π complessa, il benchmarking completo diventa ancora pi√π critico. Gli standard devono evolversi continuamente per misurare nuove capacit√† e rivelare limitazioni. Una stretta collaborazione tra settore, mondo accademico, etichette nazionali, ecc. √® essenziale per sviluppare benchmark rigorosi, trasparenti e socialmente utili.</p>
<p>Il benchmarking fornisce la bussola per guidare il progresso nell‚ÄôIA. Misurando costantemente e condividendo apertamente i risultati, possiamo orientarci verso sistemi performanti, robusti e affidabili. Se l‚ÄôIA deve soddisfare adeguatamente le esigenze sociali e umane, deve essere sottoposta a benchmarking tenendo a mente gli interessi dell‚Äôumanit√†. A tal fine, ci sono aree emergenti, come il benchmarking della sicurezza dei sistemi di IA, ma questo √® per un altro giorno e qualcosa di cui possiamo discutere ulteriormente in ‚ÄúGenerative AI‚Äù!</p>
<p>Il benchmarking √® un argomento in continua evoluzione. L‚Äôarticolo <a href="https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b">The Olympics of AI: Benchmarking Machine Learning Systems</a> copre diversi sottocampi emergenti nel benchmarking dell‚ÄôIA, tra cui robotica, realt√† estesa e calcolo neuromorfico che incoraggiamo il lettore ad approfondire.</p>
</section>
<section id="sec-benchmarking-ai-resource" class="level2" data-number="11.10">
<h2 data-number="11.10" class="anchored" data-anchor-id="sec-benchmarking-ai-resource"><span class="header-section-number">11.10</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/17udz3gxeYF3r3X1r4ePwu1I9H8ljb53W3ktFSmuDlGs/edit?usp=drive_link&amp;resourcekey=0-Espn0a0x81kl2txL_jIWjw">Perch√© il benchmarking √® importante?</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/18PI_0xmcW1xwwfcjmj25PikqBM_92vQfOXFV4hah-6I/edit?resourcekey=0-KO3HQcDAsR--jgbKd5cp4w#slide=id.g94db9f9f78_0_2">Benchmarking di inferenza embedded.</a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-cuda" class="quarto-xref">Esercizio&nbsp;<span>11.1</span></a></p></li>
<li><p><a href="#exr-perf" class="quarto-xref">Esercizio&nbsp;<span>11.2</span></a></p></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="pagination-link" aria-label="Accelerazione IA">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="pagination-link" aria-label="Apprendimento On-Device">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University). Traduzione di <a href="https://github.com/BravoBaldo">Baldassarre Cesarano</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>