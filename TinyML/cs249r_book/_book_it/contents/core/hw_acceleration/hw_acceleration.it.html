<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Accelerazione IA ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/benchmarking/benchmarking.it.html" rel="next">
<link href="../../../contents/core/optimizations/optimizations.it.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>


</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2b8d2ba3a08f8b4ab16660e3d0aa1206" class="alert alert-primary hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>‚≠ê [18 Ott] <b>Abbiamo raggiunto 1.000 stelle GitHub</b> üéâ Grazie a voi, Arduino e SEEED hanno donato kit hardware di IA per <a href="https://tinyml.seas.harvard.edu/4D/pastEvents">per i workshop TinyML</a> nei paesi in via di sviluppo <br> üéì [15 Nov] La <a href="https://www.edgeaifoundation.org/">EDGE AI Foundation</a> <strong>equipara i fondi per borse di studio accademiche</strong> per ogni nuovo GitHub ‚≠ê (fino a 10.000 stelle). <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per supportare!</a> üôè <br> üöÄ <b>La nostra missione. 1 ‚≠ê = 1 üë©‚Äçüéì Studente</b>. Ogni stella racconta una storia: studenti che acquisiscono conoscenze e sostenitori che guidano la missione. Insieme, stiamo facendo la differenza.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/about/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/ai/socratiq.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABORATORI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/part_LABS.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">LABORATORI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/part_nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">part_nicla_vision.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/part_xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">part_xiao_esp32s3.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/part_raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">part_raspi.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/part_shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">part_shared.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#panoramica" id="toc-panoramica" class="nav-link active" data-scroll-target="#panoramica"><span class="header-section-number">10.1</span> Panoramica</a></li>
  <li><a href="#background-e-basi" id="toc-background-e-basi" class="nav-link" data-scroll-target="#background-e-basi"><span class="header-section-number">10.2</span> Background e Basi</a>
  <ul>
  <li><a href="#background-storico" id="toc-background-storico" class="nav-link" data-scroll-target="#background-storico"><span class="header-section-number">10.2.1</span> Background Storico</a></li>
  <li><a href="#la-necessit√†-di-accelerazione" id="toc-la-necessit√†-di-accelerazione" class="nav-link" data-scroll-target="#la-necessit√†-di-accelerazione"><span class="header-section-number">10.2.2</span> La Necessit√† di Accelerazione</a></li>
  <li><a href="#principi-generali" id="toc-principi-generali" class="nav-link" data-scroll-target="#principi-generali"><span class="header-section-number">10.2.3</span> Principi Generali</a>
  <ul class="collapse">
  <li><a href="#prestazioni-entro-i-budget-di-potenza" id="toc-prestazioni-entro-i-budget-di-potenza" class="nav-link" data-scroll-target="#prestazioni-entro-i-budget-di-potenza">Prestazioni entro i Budget di Potenza</a></li>
  <li><a href="#gestione-dellarea-e-dei-costi-del-silicio" id="toc-gestione-dellarea-e-dei-costi-del-silicio" class="nav-link" data-scroll-target="#gestione-dellarea-e-dei-costi-del-silicio">Gestione dell‚ÄôArea e dei Costi del Silicio</a></li>
  <li><a href="#ottimizzazioni-specifiche-del-carico-di-lavoro" id="toc-ottimizzazioni-specifiche-del-carico-di-lavoro" class="nav-link" data-scroll-target="#ottimizzazioni-specifiche-del-carico-di-lavoro">Ottimizzazioni Specifiche del Carico di Lavoro</a></li>
  <li><a href="#progettazione-hardware-sostenibile" id="toc-progettazione-hardware-sostenibile" class="nav-link" data-scroll-target="#progettazione-hardware-sostenibile">Progettazione Hardware Sostenibile</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-aihw" id="toc-sec-aihw" class="nav-link" data-scroll-target="#sec-aihw"><span class="header-section-number">10.3</span> Tipi di acceleratori</a>
  <ul>
  <li><a href="#application-specific-integrated-circuits-asic" id="toc-application-specific-integrated-circuits-asic" class="nav-link" data-scroll-target="#application-specific-integrated-circuits-asic"><span class="header-section-number">10.3.1</span> Application-Specific Integrated Circuits (ASIC)</a>
  <ul class="collapse">
  <li><a href="#vantaggi" id="toc-vantaggi" class="nav-link" data-scroll-target="#vantaggi">Vantaggi</a></li>
  <li><a href="#svantaggi" id="toc-svantaggi" class="nav-link" data-scroll-target="#svantaggi">Svantaggi</a></li>
  </ul></li>
  <li><a href="#field-programmable-gate-array-fpga" id="toc-field-programmable-gate-array-fpga" class="nav-link" data-scroll-target="#field-programmable-gate-array-fpga"><span class="header-section-number">10.3.2</span> Field-Programmable Gate Array (FPGA)</a>
  <ul class="collapse">
  <li><a href="#vantaggi-1" id="toc-vantaggi-1" class="nav-link" data-scroll-target="#vantaggi-1">Vantaggi</a></li>
  <li><a href="#svantaggi-1" id="toc-svantaggi-1" class="nav-link" data-scroll-target="#svantaggi-1">Svantaggi</a></li>
  </ul></li>
  <li><a href="#digital-signal-processor-dsp" id="toc-digital-signal-processor-dsp" class="nav-link" data-scroll-target="#digital-signal-processor-dsp"><span class="header-section-number">10.3.3</span> Digital Signal Processor (DSP)</a>
  <ul class="collapse">
  <li><a href="#vantaggi-2" id="toc-vantaggi-2" class="nav-link" data-scroll-target="#vantaggi-2">Vantaggi</a></li>
  <li><a href="#svantaggi-2" id="toc-svantaggi-2" class="nav-link" data-scroll-target="#svantaggi-2">Svantaggi</a></li>
  </ul></li>
  <li><a href="#graphics-processing-unit-gpu" id="toc-graphics-processing-unit-gpu" class="nav-link" data-scroll-target="#graphics-processing-unit-gpu"><span class="header-section-number">10.3.4</span> Graphics Processing Unit (GPU)</a>
  <ul class="collapse">
  <li><a href="#vantaggi-3" id="toc-vantaggi-3" class="nav-link" data-scroll-target="#vantaggi-3">Vantaggi</a></li>
  <li><a href="#svantaggi-3" id="toc-svantaggi-3" class="nav-link" data-scroll-target="#svantaggi-3">Svantaggi</a></li>
  </ul></li>
  <li><a href="#central-processing-unit-cpu" id="toc-central-processing-unit-cpu" class="nav-link" data-scroll-target="#central-processing-unit-cpu"><span class="header-section-number">10.3.5</span> Central Processing Unit (CPU)</a>
  <ul class="collapse">
  <li><a href="#vantaggi-4" id="toc-vantaggi-4" class="nav-link" data-scroll-target="#vantaggi-4">Vantaggi</a></li>
  <li><a href="#svantaggi-4" id="toc-svantaggi-4" class="nav-link" data-scroll-target="#svantaggi-4">Svantaggi</a></li>
  </ul></li>
  <li><a href="#confronto" id="toc-confronto" class="nav-link" data-scroll-target="#confronto"><span class="header-section-number">10.3.6</span> Confronto</a></li>
  </ul></li>
  <li><a href="#co-progettazione-hardware-software" id="toc-co-progettazione-hardware-software" class="nav-link" data-scroll-target="#co-progettazione-hardware-software"><span class="header-section-number">10.4</span> Co-Progettazione Hardware-Software</a>
  <ul>
  <li><a href="#la-necessit√†-della-progettazione-congiunta" id="toc-la-necessit√†-della-progettazione-congiunta" class="nav-link" data-scroll-target="#la-necessit√†-della-progettazione-congiunta"><span class="header-section-number">10.4.1</span> La Necessit√† della Progettazione Congiunta</a>
  <ul class="collapse">
  <li><a href="#aumento-delle-dimensioni-e-della-complessit√†-del-modello" id="toc-aumento-delle-dimensioni-e-della-complessit√†-del-modello" class="nav-link" data-scroll-target="#aumento-delle-dimensioni-e-della-complessit√†-del-modello">Aumento delle Dimensioni e della Complessit√† del Modello</a></li>
  <li><a href="#vincoli-della-distribuzione-embedded" id="toc-vincoli-della-distribuzione-embedded" class="nav-link" data-scroll-target="#vincoli-della-distribuzione-embedded">Vincoli della Distribuzione Embedded</a></li>
  <li><a href="#rapida-evoluzione-degli-algoritmi-ai" id="toc-rapida-evoluzione-degli-algoritmi-ai" class="nav-link" data-scroll-target="#rapida-evoluzione-degli-algoritmi-ai">Rapida Evoluzione degli Algoritmi AI</a></li>
  <li><a href="#interazioni-complesse-hardware-software" id="toc-interazioni-complesse-hardware-software" class="nav-link" data-scroll-target="#interazioni-complesse-hardware-software">Interazioni Complesse Hardware-Software</a></li>
  <li><a href="#necessit√†-di-specializzazione" id="toc-necessit√†-di-specializzazione" class="nav-link" data-scroll-target="#necessit√†-di-specializzazione">Necessit√† di Specializzazione</a></li>
  <li><a href="#richiesta-di-maggiore-efficienza" id="toc-richiesta-di-maggiore-efficienza" class="nav-link" data-scroll-target="#richiesta-di-maggiore-efficienza">Richiesta di Maggiore Efficienza</a></li>
  </ul></li>
  <li><a href="#principi-di-progettazione-congiunta-hardware-software" id="toc-principi-di-progettazione-congiunta-hardware-software" class="nav-link" data-scroll-target="#principi-di-progettazione-congiunta-hardware-software"><span class="header-section-number">10.4.2</span> Principi di Progettazione Congiunta Hardware-Software</a>
  <ul class="collapse">
  <li><a href="#ottimizzazione-software-consapevole-dellhardware" id="toc-ottimizzazione-software-consapevole-dellhardware" class="nav-link" data-scroll-target="#ottimizzazione-software-consapevole-dellhardware">Ottimizzazione Software Consapevole dell‚ÄôHardware</a></li>
  <li><a href="#specializzazione-hardware-algorithm-driven" id="toc-specializzazione-hardware-algorithm-driven" class="nav-link" data-scroll-target="#specializzazione-hardware-algorithm-driven">Specializzazione Hardware Algorithm-Driven</a></li>
  <li><a href="#co-esplorazione-algoritmo-hardware" id="toc-co-esplorazione-algoritmo-hardware" class="nav-link" data-scroll-target="#co-esplorazione-algoritmo-hardware">Co-esplorazione Algoritmo-Hardware</a></li>
  </ul></li>
  <li><a href="#sfide" id="toc-sfide" class="nav-link" data-scroll-target="#sfide"><span class="header-section-number">10.4.3</span> Sfide</a>
  <ul class="collapse">
  <li><a href="#aumento-dei-costi-di-prototipazione" id="toc-aumento-dei-costi-di-prototipazione" class="nav-link" data-scroll-target="#aumento-dei-costi-di-prototipazione">Aumento dei Costi di Prototipazione</a></li>
  <li><a href="#ostacoli-organizzativi-e-di-team" id="toc-ostacoli-organizzativi-e-di-team" class="nav-link" data-scroll-target="#ostacoli-organizzativi-e-di-team">Ostacoli Organizzativi e di Team</a></li>
  <li><a href="#complessit√†-di-simulazione-e-modellazione" id="toc-complessit√†-di-simulazione-e-modellazione" class="nav-link" data-scroll-target="#complessit√†-di-simulazione-e-modellazione">Complessit√† di Simulazione e Modellazione</a></li>
  <li><a href="#rischi-di-eccessiva-specializzazione" id="toc-rischi-di-eccessiva-specializzazione" class="nav-link" data-scroll-target="#rischi-di-eccessiva-specializzazione">Rischi di Eccessiva Specializzazione</a></li>
  <li><a href="#problemi-sui-cambiamenti" id="toc-problemi-sui-cambiamenti" class="nav-link" data-scroll-target="#problemi-sui-cambiamenti">Problemi sui Cambiamenti</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#software-per-hardware-ai" id="toc-software-per-hardware-ai" class="nav-link" data-scroll-target="#software-per-hardware-ai"><span class="header-section-number">10.5</span> Software per Hardware AI</a>
  <ul>
  <li><a href="#sec-programming-models" id="toc-sec-programming-models" class="nav-link" data-scroll-target="#sec-programming-models"><span class="header-section-number">10.5.1</span> Modelli di Programmazione</a></li>
  <li><a href="#librerie-e-runtime" id="toc-librerie-e-runtime" class="nav-link" data-scroll-target="#librerie-e-runtime"><span class="header-section-number">10.5.2</span> Librerie e Runtime</a></li>
  <li><a href="#ottimizzazione-dei-compilatori" id="toc-ottimizzazione-dei-compilatori" class="nav-link" data-scroll-target="#ottimizzazione-dei-compilatori"><span class="header-section-number">10.5.3</span> Ottimizzazione dei Compilatori</a></li>
  <li><a href="#simulazione-e-modellazione" id="toc-simulazione-e-modellazione" class="nav-link" data-scroll-target="#simulazione-e-modellazione"><span class="header-section-number">10.5.4</span> Simulazione e Modellazione</a></li>
  </ul></li>
  <li><a href="#benchmarking-dellhardware-ai" id="toc-benchmarking-dellhardware-ai" class="nav-link" data-scroll-target="#benchmarking-dellhardware-ai"><span class="header-section-number">10.6</span> Benchmarking dell‚ÄôHardware AI</a></li>
  <li><a href="#sfide-e-soluzioni" id="toc-sfide-e-soluzioni" class="nav-link" data-scroll-target="#sfide-e-soluzioni"><span class="header-section-number">10.7</span> Sfide e Soluzioni</a>
  <ul>
  <li><a href="#problemi-di-portabilit√†compatibilit√†" id="toc-problemi-di-portabilit√†compatibilit√†" class="nav-link" data-scroll-target="#problemi-di-portabilit√†compatibilit√†"><span class="header-section-number">10.7.1</span> Problemi di Portabilit√†/Compatibilit√†</a>
  <ul class="collapse">
  <li><a href="#soluzioni-e-strategie" id="toc-soluzioni-e-strategie" class="nav-link" data-scroll-target="#soluzioni-e-strategie">Soluzioni e Strategie</a></li>
  </ul></li>
  <li><a href="#problemi-di-consumo-energetico" id="toc-problemi-di-consumo-energetico" class="nav-link" data-scroll-target="#problemi-di-consumo-energetico"><span class="header-section-number">10.7.2</span> Problemi di Consumo Energetico</a></li>
  <li><a href="#superare-i-vincoli-delle-risorse" id="toc-superare-i-vincoli-delle-risorse" class="nav-link" data-scroll-target="#superare-i-vincoli-delle-risorse"><span class="header-section-number">10.7.3</span> Superare i Vincoli delle Risorse</a></li>
  </ul></li>
  <li><a href="#tecnologie-emergenti" id="toc-tecnologie-emergenti" class="nav-link" data-scroll-target="#tecnologie-emergenti"><span class="header-section-number">10.8</span> Tecnologie Emergenti</a>
  <ul>
  <li><a href="#metodi-di-integrazione" id="toc-metodi-di-integrazione" class="nav-link" data-scroll-target="#metodi-di-integrazione"><span class="header-section-number">10.8.1</span> Metodi di Integrazione</a>
  <ul class="collapse">
  <li><a href="#ai-su-scala-wafer" id="toc-ai-su-scala-wafer" class="nav-link" data-scroll-target="#ai-su-scala-wafer">AI su Scala Wafer</a></li>
  <li><a href="#chiplet-per-ai" id="toc-chiplet-per-ai" class="nav-link" data-scroll-target="#chiplet-per-ai">Chiplet per AI</a></li>
  </ul></li>
  <li><a href="#sec-neuromorphic" id="toc-sec-neuromorphic" class="nav-link" data-scroll-target="#sec-neuromorphic"><span class="header-section-number">10.8.2</span> Elaborazione N√πeuromorfica</a></li>
  <li><a href="#calcolo-analogico" id="toc-calcolo-analogico" class="nav-link" data-scroll-target="#calcolo-analogico"><span class="header-section-number">10.8.3</span> Calcolo Analogico</a></li>
  <li><a href="#elettronica-flessibile" id="toc-elettronica-flessibile" class="nav-link" data-scroll-target="#elettronica-flessibile"><span class="header-section-number">10.8.4</span> Elettronica Flessibile</a></li>
  <li><a href="#tecnologie-delle-memorie" id="toc-tecnologie-delle-memorie" class="nav-link" data-scroll-target="#tecnologie-delle-memorie"><span class="header-section-number">10.8.5</span> Tecnologie delle Memorie</a></li>
  <li><a href="#calcolo-ottico" id="toc-calcolo-ottico" class="nav-link" data-scroll-target="#calcolo-ottico"><span class="header-section-number">10.8.6</span> Calcolo Ottico</a></li>
  <li><a href="#quantum-computing" id="toc-quantum-computing" class="nav-link" data-scroll-target="#quantum-computing"><span class="header-section-number">10.8.7</span> Quantum Computing</a></li>
  </ul></li>
  <li><a href="#tendenze-future" id="toc-tendenze-future" class="nav-link" data-scroll-target="#tendenze-future"><span class="header-section-number">10.9</span> Tendenze Future</a>
  <ul>
  <li><a href="#ml-per-lautomazione-della-progettazione-hardware" id="toc-ml-per-lautomazione-della-progettazione-hardware" class="nav-link" data-scroll-target="#ml-per-lautomazione-della-progettazione-hardware"><span class="header-section-number">10.9.1</span> ML per l‚Äôautomazione della progettazione hardware</a></li>
  <li><a href="#simulazione-e-verifica-hardware-basate-su-ml" id="toc-simulazione-e-verifica-hardware-basate-su-ml" class="nav-link" data-scroll-target="#simulazione-e-verifica-hardware-basate-su-ml"><span class="header-section-number">10.9.2</span> Simulazione e Verifica Hardware Basate su ML</a></li>
  <li><a href="#ml-per-architetture-hardware-efficienti" id="toc-ml-per-architetture-hardware-efficienti" class="nav-link" data-scroll-target="#ml-per-architetture-hardware-efficienti"><span class="header-section-number">10.9.3</span> ML per Architetture Hardware Efficienti</a></li>
  <li><a href="#ml-per-ottimizzare-la-produzione-e-ridurre-i-difetti" id="toc-ml-per-ottimizzare-la-produzione-e-ridurre-i-difetti" class="nav-link" data-scroll-target="#ml-per-ottimizzare-la-produzione-e-ridurre-i-difetti"><span class="header-section-number">10.9.4</span> ML per Ottimizzare la Produzione e Ridurre i Difetti</a></li>
  <li><a href="#verso-modelli-di-base-per-la-progettazione-hardware" id="toc-verso-modelli-di-base-per-la-progettazione-hardware" class="nav-link" data-scroll-target="#verso-modelli-di-base-per-la-progettazione-hardware"><span class="header-section-number">10.9.5</span> Verso Modelli di Base per la Progettazione Hardware</a></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">10.10</span> Conclusione</a></li>
  <li><a href="#sec-ai-acceleration-resource" id="toc-sec-ai-acceleration-resource" class="nav-link" data-scroll-target="#sec-ai-acceleration-resource"><span class="header-section-number">10.11</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/hw_acceleration/hw_acceleration.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/hw_acceleration/hw_acceleration.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-ai_acceleration" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-ai-acceleration-resource">Slide</a>, <a href="#sec-ai-acceleration-resource">Video</a>, <a href="#sec-ai-acceleration-resource">Esercizi</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ai_hardware.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Creare una rappresentazione intricata e colorata di un progetto di System on Chip (SoC) in un formato rettangolare. Mostrare una variet√† di acceleratori e piccoli chip di machine learning specializzati, tutti integrati nel processore. Fornire una vista dettagliata all‚Äôinterno del chip, evidenziando il rapido movimento degli elettroni. Ogni acceleratore e piccolo chip dovrebbe essere progettato per interagire con i neuroni, gli strati e le attivazioni della rete neurale, enfatizzandone la velocit√† di elaborazione. Rappresentare le reti neurali come una rete di nodi interconnessi, con flussi di dati vibranti che scorrono tra i pezzi dell‚Äôacceleratore, mostrando la migliorata velocit√† di elaborazione.</em></figcaption>
</figure>
</div>
<p>L‚Äôimplementazione di ML su dispositivi edge presenta sfide quali velocit√† di elaborazione limitata, vincoli di memoria e rigorosi requisiti di efficienza energetica. Per superare queste sfide, l‚Äôaccelerazione hardware specializzata √® fondamentale. Gli acceleratori hardware sono progettati per ottimizzare attivit√† ad alta intensit√† di calcolo come l‚Äôinferenza utilizzando chip di silicio personalizzati su misura per moltiplicazioni di matrici, fornendo accelerazioni significative rispetto alle CPU per uso generico. Ci√≤ consente l‚Äôesecuzione in tempo reale di modelli avanzati su dispositivi con rigorosi vincoli di dimensioni, peso e potenza.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere perch√© l‚Äôaccelerazione hardware √® necessaria per i carichi di lavoro AI</p></li>
<li><p>Esaminare le opzioni chiave di accelerazione come GPU, TPU, FPGA e ASIC e i loro compromessi</p></li>
<li><p>Scoprire modelli di programmazione, framework e compilatori per acceleratori AI</p></li>
<li><p>Apprezzare l‚Äôimportanza del benchmarking e delle metriche per la valutazione hardware</p></li>
<li><p>Riconoscere il ruolo della progettazione congiunta hardware-software nella creazione di sistemi efficienti</p></li>
<li><p>Ottenere visibilit√† su direzioni di ricerca all‚Äôavanguardia come il calcolo neuromorfico e quantistico</p></li>
<li><p>Comprendere come il ML sta iniziando ad aumentare e migliorare la progettazione hardware</p></li>
</ul>
</div>
</div>
<section id="panoramica" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="panoramica"><span class="header-section-number">10.1</span> Panoramica</h2>
<p>Probabilmente avrete notato la crescente domanda di integrazione dell‚Äôapprendimento automatico nei dispositivi di uso quotidiano, come gli smartphone nelle nostre tasche, gli elettrodomestici intelligenti e persino i veicoli autonomi. Portare le funzionalit√† di ML in questi ambienti del mondo reale √® entusiasmante, ma comporta una serie di sfide. A differenza dei potenti server dei data center, questi dispositivi edge hanno risorse di elaborazione limitate, il che rende difficile eseguire modelli complessi in modo efficace.</p>
<p>L‚Äôaccelerazione hardware specializzata √® la chiave per rendere possibile l‚Äôapprendimento automatico ad alte prestazioni su dispositivi edge con risorse limitate. Quando parliamo di accelerazione hardware, ci riferiamo all‚Äôuso di chip e architetture personalizzati progettati per gestire il pesante lavoro delle operazioni di ML, alleggerendo il carico del processore principale. Nelle reti neurali, alcune delle attivit√† pi√π impegnative riguardano le moltiplicazioni di matrici durante l‚Äôinferenza. Gli acceleratori hardware sono progettati per ottimizzare queste operazioni, spesso offrendo accelerazioni da 10 a 100 volte superiori rispetto alle CPU per uso generico. Questo tipo di accelerazione √® ci√≤ che rende fattibile l‚Äôesecuzione di modelli di reti neurali avanzate su dispositivi limitati da dimensioni, peso e potenza, e di fare tutto in tempo reale.</p>
<p>In questo capitolo, esamineremo pi√π da vicino le diverse tecniche di accelerazione hardware disponibili per l‚Äôapprendimento automatico embedded e i compromessi che derivano da ciascuna opzione. L‚Äôobiettivo √® fornire una solida comprensione di come funzionano queste tecniche, in modo che si possano prendere decisioni informate quando si tratta di scegliere l‚Äôhardware giusto e ottimizzare il software. Alla fine, sarete ben equipaggiati per sviluppare capacit√† di apprendimento automatico ad alte prestazioni su dispositivi edge, anche con i loro vincoli.</p>
</section>
<section id="background-e-basi" class="level2 page-columns page-full" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="background-e-basi"><span class="header-section-number">10.2</span> Background e Basi</h2>
<section id="background-storico" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="background-storico"><span class="header-section-number">10.2.1</span> Background Storico</h3>
<p>Le origini dell‚Äôaccelerazione hardware risalgono agli anni ‚Äô60, con l‚Äôavvento dei coprocessori matematici in virgola mobile per eliminare i calcoli dalla CPU principale. Un primo esempio √® stato il chip <a href="https://en.wikipedia.org/wiki/Intel_8087">Intel 8087</a> rilasciato nel 1980 per accelerare le operazioni in virgola mobile per il processore 8086. Ci√≤ ha stabilito la pratica di utilizzare processori specializzati per gestire in modo efficiente carichi di lavoro ad alta intensit√† di calcolo.</p>
<p>Negli anni ‚Äô90, sono emerse le prime <a href="https://en.wikipedia.org/wiki/History_of_the_graphics_processor">Graphics Processing Units (GPU)</a> [Unit√† di elaborazione grafica] per elaborare rapidamente pipeline grafiche per rendering e giochi. La <a href="https://en.wikipedia.org/wiki/GeForce_256">GeForce 256</a> di Nvidia nel 1999 √® stata una delle prime GPU programmabili in grado di eseguire algoritmi software personalizzati. Le GPU esemplificano acceleratori a funzione fissa specifici per dominio e si sono evolute in acceleratori programmabili paralleli.</p>
<p>Negli anni 2000, le GPU sono state applicate all‚Äôelaborazione generica in <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GPGPU</a>. La loro elevata larghezza di banda di memoria e la produttivit√† computazionale le hanno rese adatte a carichi di lavoro ad alta intensit√† di calcolo. Ci√≤ ha incluso innovazioni nell‚Äôuso di GPU per accelerare il training di modelli di deep learning come <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">AlexNet</a> nel 2012.</p>
<p>Negli ultimi anni, le <a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">Tensor Processing Unit (TPU)</a> di Google rappresentano ASIC personalizzati specificamente progettati per la moltiplicazione di matrici nel deep learning. Durante l‚Äôinferenza, i loro core tensoriali ottimizzati raggiungono TeraOPS/watt pi√π elevati rispetto a CPU o GPU. L‚Äôinnovazione continua include tecniche di compressione del modello come <a href="https://arxiv.org/abs/1506.02626">pruning</a> e <a href="https://arxiv.org/abs/1609.07061">quantizzazione</a> per adattare reti neurali pi√π grandi su dispositivi edge.</p>
<p>Questa evoluzione dimostra come l‚Äôaccelerazione hardware si sia concentrata sulla risoluzione di colli di bottiglia ad alta intensit√† di calcolo, dalla matematica in virgola mobile alla grafica alla moltiplicazione di matrici per ML. Comprendere questa storia fornisce un contesto cruciale per gli acceleratori AI specializzati odierni.</p>
</section>
<section id="la-necessit√†-di-accelerazione" class="level3 page-columns page-full" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="la-necessit√†-di-accelerazione"><span class="header-section-number">10.2.2</span> La Necessit√† di Accelerazione</h3>
<p>L‚Äôevoluzione dell‚Äôaccelerazione hardware √® strettamente legata alla storia pi√π ampia dell‚Äôinformatica. Centrale in questa storia √® il ruolo dei transistor, i mattoni fondamentali dell‚Äôelettronica moderna. I transistor agiscono come piccoli interruttori che possono accendersi o spegnersi, consentendo i calcoli complessi che guidano tutto, dalle semplici calcolatrici ai modelli avanzati di apprendimento automatico. Nei primi decenni, la progettazione dei chip era governata dalla legge di Moore, che prevedeva che il numero di transistor su un circuito integrato sarebbe raddoppiato approssimativamente ogni due anni, e dal Dennard Scaling, che osservava che man mano che i transistor diventavano pi√π piccoli, le loro prestazioni (velocit√†) aumentavano, mentre la densit√† di potenza (potenza per unit√† di area) rimaneva costante. Queste due leggi sono state mantenute durante l‚Äôera single-core. <a href="#fig-moore-dennard" class="quarto-xref">Figura&nbsp;<span>10.1</span></a> mostra le tendenze di diverse metriche dei microprocessori. Come indica la figura, il Dennard Scaling fallisce intorno alla met√† degli anni 2000; si noti come la velocit√† di clock (frequenza) rimanga pressoch√© costante anche se il numero di transistor continua ad aumentare.</p>
<div id="fig-moore-dennard" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-moore-dennard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hwai_40yearsmicrotrenddata.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-moore-dennard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.1: Tendenze dei Microprocessori. Fonte: <a href="https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/">Karl Rupp</a>.
</figcaption>
</figure>
</div>
<p>Tuttavia, come descrive <span class="citation" data-cites="patterson2016computer">Patterson e Hennessy (<a href="../../../references.it.html#ref-patterson2016computer" role="doc-biblioref">2016</a>)</span>, i vincoli tecnologici alla fine hanno imposto una transizione all‚Äôera multicore, con chip contenenti pi√π core di elaborazione per offrire guadagni in termini di prestazioni. Le limitazioni di potenza hanno impedito un ulteriore ridimensionamento, il che ha portato al ‚Äúsilicio scuro‚Äù (<a href="https://en.wikipedia.org/wiki/Dark_silicon">Dark Silicon</a>), in cui non tutte le aree del chip potevano essere attive simultaneamente <span class="citation" data-cites="xiu2019time">(<a href="../../../references.it.html#ref-xiu2019time" role="doc-biblioref">Xiu 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-patterson2016computer" class="csl-entry" role="listitem">
Patterson, David A, e John L Hennessy. 2016. <em>Computer organization and design <span>ARM</span> edition: <span>The</span> hardware software interface</em>. Morgan kaufmann.
</div><div id="ref-xiu2019time" class="csl-entry" role="listitem">
Xiu, Liming. 2019. <span>¬´Time Moore: <span>Exploiting</span> <span>Moore‚Äôs</span> Law From The Perspective of Time¬ª</span>. <em>IEEE Solid-State Circuits Mag.</em> 11 (1): 39‚Äì55. <a href="https://doi.org/10.1109/mssc.2018.2882285">https://doi.org/10.1109/mssc.2018.2882285</a>.
</div></div><p>‚ÄúDark silicon‚Äù si riferisce a parti del chip che non possono essere alimentate simultaneamente a causa di limitazioni termiche e di potenza. In sostanza, con l‚Äôaumento della densit√† dei transistor, la quota del chip che poteva essere utilizzata attivamente senza surriscaldarsi o superare i budget di potenza si √® ridotta.</p>
<p>Questo fenomeno ha comportato che, sebbene i chip avessero pi√π transistor, non tutti potevano essere operativi simultaneamente, limitando i potenziali guadagni in termini di prestazioni. Questa crisi energetica ha reso necessario un passaggio all‚Äôera degli acceleratori, con unit√† hardware specializzate su misura per attivit√† specifiche per massimizzare l‚Äôefficienza. L‚Äôesplosione dei carichi di lavoro dell‚Äôintelligenza artificiale ha ulteriormente spinto la domanda di acceleratori personalizzati. I fattori abilitanti includevano nuovi linguaggi di programmazione, strumenti software e progressi nella produzione.</p>
<p>Fondamentalmente, gli acceleratori hardware vengono valutati in base a Prestazioni, Potenza e Area di silicio (PPA); la natura dell‚Äôapplicazione target, sia essa legata alla memoria o al calcolo, influenza notevolmente la progettazione. Ad esempio, i carichi di lavoro legati alla memoria richiedono un‚Äôelevata larghezza di banda e un accesso a bassa latenza, mentre le applicazioni legate al calcolo richiedono la massima produttivit√† di elaborazione.</p>
</section>
<section id="principi-generali" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="principi-generali"><span class="header-section-number">10.2.3</span> Principi Generali</h3>
<p>La progettazione di acceleratori hardware specializzati comporta la gestione di compromessi complessi tra prestazioni, efficienza energetica, area di silicio e ottimizzazioni specifiche del carico di lavoro. Questa sezione delinea considerazioni e metodologie fondamentali per raggiungere un equilibrio ottimale in base ai requisiti dell‚Äôapplicazione e ai vincoli hardware.</p>
<section id="prestazioni-entro-i-budget-di-potenza" class="level4">
<h4 class="anchored" data-anchor-id="prestazioni-entro-i-budget-di-potenza">Prestazioni entro i Budget di Potenza</h4>
<p>Per capire come raggiungere il giusto equilibrio tra prestazioni e budget di potenza, √® importante definire prima alcuni concetti chiave che svolgono un ruolo cruciale in questo processo. Le prestazioni si riferiscono in generale alla capacit√† complessiva di un sistema di completare efficacemente le attivit√† di calcolo entro determinati vincoli. Uno dei componenti chiave delle prestazioni √® il throughput, ovvero la velocit√† con cui vengono elaborate queste attivit√†, comunemente misurata in ‚Äúfloating point operations per second (FLOPS)‚Äù [operazioni in virgola mobile al secondo ] o frame al secondo (FPS). Il throughput dipende fortemente dal parallelismo, ovvero la capacit√† dell‚Äôhardware di eseguire pi√π operazioni contemporaneamente, e dalla frequenza di clock, ovvero la velocit√† con cui il processore esegue ciclicamente queste operazioni. Un throughput pi√π elevato in genere comporta prestazioni migliori, ma aumenta anche il consumo di energia all‚Äôaumentare dell‚Äôattivit√†.</p>
<p>La semplice massimizzazione del throughput non √® sufficiente; anche l‚Äôefficienza dell‚Äôhardware √® importante. L‚Äôefficienza √® la misura di quante operazioni vengono eseguite per watt di potenza consumata, riflettendo la relazione tra lavoro di calcolo e consumo di energia. In scenari in cui la potenza √® un fattore limitante, come nei dispositivi edge, ottenere un‚Äôelevata efficienza √® fondamentale. Per aiutare a ricordare come questi concetti si interconnettono, considerare le seguenti relazioni:</p>
<ul>
<li><strong>Prestazioni</strong> = Throughput * Efficienza</li>
<li><strong>Throughput</strong> ~= Parallelismo * Frequenza di Clock</li>
<li><strong>Efficienza</strong> = Operazioni / Watt</li>
</ul>
<p>Gli acceleratori hardware mirano a massimizzare le prestazioni entro budget di potenza stabiliti. Ci√≤ richiede un attento bilanciamento del parallelismo, della frequenza di clock del chip, della tensione di esercizio, dell‚Äôottimizzazione del carico di lavoro e di altre tecniche per massimizzare le operazioni per watt.</p>
<p>Ad esempio, le GPU raggiungono un throughput elevato tramite architetture massivamente parallele. Tuttavia, la loro efficienza √® inferiore a quella dei circuiti integrati specifici per applicazione (ASIC) personalizzati come il TPU di Google, che ottimizzano per un carico di lavoro specifico.</p>
</section>
<section id="gestione-dellarea-e-dei-costi-del-silicio" class="level4">
<h4 class="anchored" data-anchor-id="gestione-dellarea-e-dei-costi-del-silicio">Gestione dell‚ÄôArea e dei Costi del Silicio</h4>
<p>La dimensione dell‚Äôarea di un chip ha un impatto diretto sul suo costo di produzione. Per capirne il motivo, √® utile conoscere un po‚Äô il processo di produzione.</p>
<p>I chip vengono creati da grandi e sottili fette di materiale semiconduttore note come wafer. Durante la produzione, ogni wafer viene suddiviso in blocchi pi√π piccoli chiamati ‚Äúdie‚Äù, e ogni die contenente i circuiti per un singolo chip. Dopo che il wafer √® stato elaborato, viene tagliato in questi singoli die, che vengono poi confezionati per formare i chip finali utilizzati nei dispositivi elettronici.</p>
<p>I die pi√π grandi richiedono pi√π materiale e sono pi√π inclini a difetti, il che pu√≤ ridurre la resa, il che significa che vengono prodotti meno chip utilizzabili da ogni wafer. Mentre i produttori possono scalare i progetti combinando pi√π die pi√π piccoli in un singolo pacchetto (pacchetti multi-die), ci√≤ aggiunge complessit√† e costi al processo di confezionamento e produzione.</p>
<p>La quantit√† di area di silicio necessaria su un die dipende da diversi fattori:</p>
<ul>
<li><strong>Risorse di Calcolo</strong>, ad esempio numero di core, memoria, cache</li>
<li><strong>Nodo del Processo di Produzione</strong>, transistor pi√π piccoli consentono una maggiore densit√†</li>
<li><strong>Modello di Programmazione</strong>, acceleratori programmati richiedono maggiore flessibilit√†</li>
</ul>
<p>La progettazione dell‚Äôacceleratore implica la compressione delle massime prestazioni entro questi vincoli di area del silicio. Tecniche come la potatura e la compressione aiutano ad adattare modelli pi√π grandi al chip senza superare lo spazio disponibile.</p>
</section>
<section id="ottimizzazioni-specifiche-del-carico-di-lavoro" class="level4">
<h4 class="anchored" data-anchor-id="ottimizzazioni-specifiche-del-carico-di-lavoro">Ottimizzazioni Specifiche del Carico di Lavoro</h4>
<p>La progettazione di acceleratori hardware efficaci richiede di adattare l‚Äôarchitettura alle esigenze specifiche del carico di lavoro target. Diversi tipi di carichi di lavoro, che siano in AI, grafica o robotica, hanno caratteristiche uniche che stabiliscono come l‚Äôacceleratore dovrebbe essere ottimizzato.</p>
<p>Alcune delle considerazioni chiave quando si ottimizza l‚Äôhardware per carichi di lavoro specifici includono:</p>
<ul>
<li><strong>Memoria vs Limiti di Calcolo:</strong> I carichi di lavoro vincolati alla memoria richiedono una maggiore larghezza di banda di memoria, mentre le app vincolate al calcolo necessitano di un throughput [produttivit√†] aritmetico.</li>
<li><strong>Localit√† dei Dati:</strong> Lo spostamento dei dati dovrebbe essere ridotto al minimo per l‚Äôefficienza. La memoria vicina al calcolo aiuta.</li>
<li><strong>Operazioni a Livello di Bit:</strong> I tipi di dati a bassa precisione come INT8/INT4 ottimizzano la densit√† di calcolo.</li>
<li><strong>Parallelismo dei Dati:</strong> Pi√π unit√† di calcolo replicate consentono l‚Äôesecuzione parallela.</li>
<li><strong>Pipelining:</strong> L‚Äôesecuzione sovrapposta delle operazioni aumenta la produttivit√†.</li>
</ul>
<p>La comprensione delle caratteristiche del carico di lavoro consente un‚Äôaccelerazione personalizzata. Ad esempio, le reti neurali convoluzionali utilizzano operazioni di ‚Äúfinestra scorrevole‚Äù mappate in modo ottimale su array spaziali di elementi di elaborazione.</p>
<p>Grazie alla comprensione di questi compromessi architettonici, i progettisti possono prendere decisioni informate sull‚Äôarchitettura dell‚Äôacceleratore hardware, assicurandosi che fornisca le migliori prestazioni possibili per l‚Äôuso previsto.</p>
</section>
<section id="progettazione-hardware-sostenibile" class="level4">
<h4 class="anchored" data-anchor-id="progettazione-hardware-sostenibile">Progettazione Hardware Sostenibile</h4>
<p>Negli ultimi anni, la sostenibilit√† dell‚ÄôIA √® diventata una preoccupazione urgente, guidata da due fattori chiave: la scala crescente dei carichi di lavoro dell‚ÄôIA e il consumo energetico associato.</p>
<p>Innanzitutto, le dimensioni dei modelli e dei set di dati dell‚ÄôIA sono cresciute rapidamente. Ad esempio, in base alle tendenze di elaborazione dell‚ÄôIA di OpenAI, la quantit√† di elaborazione utilizzata per addestrare modelli all‚Äôavanguardia raddoppia ogni 3,5 mesi. Questa crescita esponenziale richiede enormi risorse di elaborazione nei data center.</p>
<p>In secondo luogo, l‚Äôuso di energia per l‚Äôaddestramento e l‚Äôinferenza dell‚ÄôIA presenta problemi di sostenibilit√†. I data center che eseguono applicazioni di IA consumano molta energia, contribuendo a elevate emissioni di carbonio. Si stima che l‚Äôaddestramento di un grande modello di IA possa avere un‚Äôimpronta di carbonio di 626.000 libbre di CO<sub>2</sub> equivalente, quasi 5 volte le emissioni di un‚Äôauto media nel corso della sua vita.</p>
<p>Per affrontare queste sfide, la progettazione hardware sostenibile si concentra sull‚Äôottimizzazione dell‚Äôefficienza energetica senza compromettere le prestazioni. Ci√≤ comporta lo sviluppo di acceleratori specializzati che riducono al minimo il consumo di energia massimizzando al contempo la produttivit√† computazionale.</p>
<p>Parleremo di <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html">IA sostenibile</a> in un capitolo successivo, dove ne discuteremo pi√π in dettaglio.</p>
</section>
</section>
</section>
<section id="sec-aihw" class="level2 page-columns page-full" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="sec-aihw"><span class="header-section-number">10.3</span> Tipi di acceleratori</h2>
<p>Gli acceleratori hardware possono assumere molte forme. Possono esistere come widget (come il <a href="https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/">Neural Engine nel chip Apple M1</a>) o come interi chip appositamente progettati per svolgere molto bene determinate attivit√†. Questa sezione esaminer√† i processori per carichi di lavoro di apprendimento automatico lungo lo spettro che va dagli ASIC altamente specializzati alle CPU pi√π generiche.</p>
<p>Ci concentriamo prima sull‚Äôhardware personalizzato appositamente progettato per l‚Äôintelligenza artificiale per comprendere le ottimizzazioni pi√π estreme possibili quando vengono rimossi i vincoli di progettazione. Questo stabilisce un limite massimo per prestazioni ed efficienza. Poi prendiamo in considerazione progressivamente architetture pi√π programmabili e adattabili, discutendo di GPU e FPGA. Queste fanno compromessi nella personalizzazione per mantenere la flessibilit√†. Infine, trattiamo le CPU generiche che sacrificano le ottimizzazioni per un carico di lavoro particolare in cambio di una programmabilit√† versatile tra le applicazioni.</p>
<p>Strutturando l‚Äôanalisi lungo questo spettro, miriamo a illustrare i compromessi fondamentali tra utilizzo, efficienza, programmabilit√† e flessibilit√† nella progettazione dell‚Äôacceleratore. Il punto di equilibrio ottimale dipende dai vincoli e dai requisiti dell‚Äôapplicazione target. Questa prospettiva dello spettro fornisce un quadro per ragionare sulle scelte hardware per l‚Äôapprendimento automatico e sulle capacit√† richieste a ciascun livello di specializzazione.</p>
<p><a href="#fig-design-tradeoffs" class="quarto-xref">Figura&nbsp;<span>10.2</span></a> illustra la complessa interazione tra flessibilit√†, prestazioni, diversit√† funzionale e area di progettazione dell‚Äôarchitettura. Notare come l‚ÄôASIC si trovi nell‚Äôangolo in basso a destra, con area minima, flessibilit√† e consumo energetico e prestazioni massime, a causa della sua natura altamente specializzata per l‚Äôapplicazione. Un compromesso chiave √® la diversit√† funzionale rispetto alle prestazioni: le architetture per uso generico possono servire applicazioni diverse, ma le loro prestazioni applicative sono degradate rispetto alle architetture pi√π personalizzate.</p>
<div id="fig-design-tradeoffs" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-design-tradeoffs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tradeoffs.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-design-tradeoffs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.2: Compromessi di Progettazione. Fonte: <span class="citation" data-cites="rayis2014">El-Rayis (<a href="../../../references.it.html#ref-rayis2014" role="doc-biblioref">2014</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-rayis2014" class="csl-entry" role="listitem">
El-Rayis, A. O. 2014. <span>¬´Reconfigurable architectures for the next generation of mobile device telecommunications systems¬ª</span>. <a href=": https://www.researchgate.net/publication/292608967">: https://www.researchgate.net/publication/292608967</a>.
</div></div></figure>
</div>
<p>La progressione inizia con l‚Äôopzione pi√π specializzata, gli ASIC appositamente progettati per l‚Äôintelligenza artificiale, per basare la nostra comprensione sulle massime ottimizzazioni possibili prima di espanderci ad architetture pi√π generalizzabili. Questo approccio strutturato chiarisce lo spazio di progettazione dell‚Äôacceleratore.</p>
<section id="application-specific-integrated-circuits-asic" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="application-specific-integrated-circuits-asic"><span class="header-section-number">10.3.1</span> Application-Specific Integrated Circuits (ASIC)</h3>
<p>Un ‚Äúcircuito integrato specifico per applicazione‚Äù (ASIC) √® un tipo di <a href="https://en.wikipedia.org/wiki/Integrated_circuit">circuito integrato</a> (IC) progettato su misura per un‚Äôapplicazione o un carico di lavoro specifico, anzich√© per un uso generico. A differenza di CPU e GPU, gli ASIC non supportano pi√π applicazioni o carichi di lavoro. Piuttosto, sono ottimizzati per eseguire un singolo compito in modo estremamente efficiente. Google TPU √® un esempio di ASIC.</p>
<p>Gli ASIC raggiungono questa efficienza adattando ogni aspetto del design del chip, ovvero le porte logiche sottostanti, i componenti elettronici, l‚Äôarchitettura, la memoria, l‚ÄôI/O e il processo di produzione, specificamente per l‚Äôapplicazione target. Questo livello di personalizzazione consente di rimuovere qualsiasi logica o funzionalit√† non necessaria richiesta per il calcolo generale. Il risultato √® un IC che massimizza le prestazioni e l‚Äôefficienza energetica sul carico di lavoro desiderato. I guadagni di efficienza derivanti dall‚Äôhardware specifico per applicazione sono cos√¨ sostanziali che queste aziende incentrate sul software dedicano enormi risorse ingegneristiche alla progettazione di ASIC personalizzati.</p>
<p>L‚Äôascesa di algoritmi di apprendimento automatico pi√π complessi ha reso i vantaggi prestazionali abilitati dall‚Äôaccelerazione hardware personalizzata un fattore di differenziazione competitiva chiave, anche per le aziende tradizionalmente concentrate sull‚Äôingegneria del software. Gli ASIC sono diventati un investimento ad alta priorit√† per i principali provider cloud che mirano a offrire un calcolo AI pi√π veloce.</p>
<section id="vantaggi" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi">Vantaggi</h4>
<p>Grazie alla loro natura personalizzata, gli ASIC offrono vantaggi significativi rispetto ai processori generici come CPU e GPU. I principali vantaggi includono quanto segue.</p>
<section id="prestazioni-ed-efficienza-massimizzate" class="level5">
<h5 class="anchored" data-anchor-id="prestazioni-ed-efficienza-massimizzate">Prestazioni ed efficienza massimizzate</h5>
<p>Il vantaggio pi√π fondamentale degli ASIC √® la massimizzazione delle prestazioni e dell‚Äôefficienza energetica personalizzando l‚Äôarchitettura hardware specificamente per l‚Äôapplicazione target. Ogni transistor e aspetto della progettazione √® ottimizzato per il carico di lavoro desiderato: non √® necessaria alcuna logica o sovraccarico non necessario per supportare il calcolo generico.</p>
<p>Ad esempio, <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">le Tensor Processing Units (TPU) di Google</a> contengono architetture su misura esattamente per le operazioni di moltiplicazione di matrici utilizzate nelle reti neurali. Per progettare gli ASIC TPU, i team di ingegneria di Google devono definire chiaramente le specifiche del chip, scrivere la descrizione dell‚Äôarchitettura utilizzando linguaggi di descrizione hardware come <a href="https://www.verilog.com/">Verilog</a>, sintetizzare il design per mapparlo sui componenti hardware e posizionare e instradare con cura transistor e collegamenti in base alle regole di progettazione del processo di fabbricazione. Questo complesso processo di progettazione, noto come ‚Äúvery-large-scale integration‚Äù (VLSI) [integrazione su larga scala ], consente loro di creare un IC ottimizzato per carichi di lavoro di apprendimento automatico.</p>
<p>Di conseguenza, gli ASIC TPU raggiungono un‚Äôefficienza di oltre un ordine di grandezza superiore nelle operazioni per watt rispetto alle GPU per uso generico sui carichi di lavoro di apprendimento automatico massimizzando le prestazioni e riducendo al minimo il consumo energetico tramite un design hardware full-stack personalizzato.</p>
</section>
<section id="memoria-on-chip-specializzata" class="level5">
<h5 class="anchored" data-anchor-id="memoria-on-chip-specializzata">Memoria On-Chip Specializzata</h5>
<p>Gli ASIC incorporano memoria on-chip, come SRAM (Static Random Access Memory) e cache specificamente ottimizzate per fornire dati alle unit√† di elaborazione. La SRAM √® un tipo di memoria pi√π veloce e affidabile della DRAM (Dynamic Random Access Memory) perch√© non deve essere aggiornata periodicamente. Tuttavia, richiede pi√π transistor per bit di dati, il che la rende pi√π ingombrante e costosa da produrre rispetto alla DRAM.</p>
<p>La SRAM √® ideale per la memoria on-chip, dove la velocit√† √® fondamentale. Il vantaggio di avere grandi quantit√† di SRAM on-chip ad alta larghezza di banda √® che i dati possono essere archiviati vicino agli elementi di elaborazione, consentendo un rapido accesso. Ci√≤ fornisce enormi vantaggi in termini di velocit√† rispetto all‚Äôaccesso alla DRAM off-chip, che, sebbene di capacit√† maggiore, pu√≤ essere fino a 100 volte pi√π lenta. Ad esempio, il system-on-a-chip M1 di Apple contiene una speciale SRAM a bassa latenza per accelerare le prestazioni del suo hardware di machine learning Neural Engine.</p>
<p>La localit√† dei dati e l‚Äôottimizzazione della gerarchia di memoria sono fondamentali per un throughput elevato e un basso consumo energetico. <a href="#tbl-latency-comparison" class="quarto-xref">Tabella&nbsp;<span>10.1</span></a> mostra ‚ÄúNumeri che Tutti Dovrebbero Conoscere‚Äù, di <a href="https://research.google/people/jeff/">Jeff Dean</a>.</p>
<div id="tbl-latency-comparison" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-latency-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;10.1: Confronto della latenza delle operazioni di elaborazione e di rete.
</figcaption>
<div aria-describedby="tbl-latency-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 67%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Operazione</th>
<th style="text-align: left;">Latenza</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Riferimento alla cache L1</td>
<td style="text-align: left;">0,5 ns</td>
</tr>
<tr class="even">
<td style="text-align: left;">Branch mispredict</td>
<td style="text-align: left;">5 ns</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Riferimento alla cache L2</td>
<td style="text-align: left;">7 ns</td>
</tr>
<tr class="even">
<td style="text-align: left;">Blocco/sblocco mutex</td>
<td style="text-align: left;">25 ns</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Riferimento alla memoria principale</td>
<td style="text-align: left;">100 ns</td>
</tr>
<tr class="even">
<td style="text-align: left;">Comprimere 1K byte con Zippy</td>
<td style="text-align: left;">3.000 ns (3 us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Inviare 1 KB byte su una rete da 1 Gbps</td>
<td style="text-align: left;">10.000 ns (10 us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Leggere 4 KB casualmente da SSD</td>
<td style="text-align: left;">150.000 ns (150 us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Leggere 1 MB in sequenza dalla memoria</td>
<td style="text-align: left;">250.000 ns (250 us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Andata e ritorno all‚Äôinterno dello stesso data center</td>
<td style="text-align: left;">500.000 ns (0,5 ms)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Leggere 1 MB in sequenza da SSD</td>
<td style="text-align: left;">1.000.000 ns (1 ms)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ricerca su disco</td>
<td style="text-align: left;">10.000.000 ns (10 ms)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Leggere 1 MB in sequenza da disco</td>
<td style="text-align: left;">20.000.000 ns (20 ms)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Inviare un pacchetto CA ‚Üí Paesi Bassi ‚Üí CA</td>
<td style="text-align: left;">150.000.000 ns (150 ms)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="tipi-di-dati-e-operazioni-personalizzati" class="level5">
<h5 class="anchored" data-anchor-id="tipi-di-dati-e-operazioni-personalizzati">Tipi di Dati e Operazioni Personalizzati</h5>
<p>A differenza dei processori generici, gli ASIC possono essere progettati per supportare in modo nativo tipi di dati personalizzati come INT4 o bfloat16, ampiamente utilizzati nei modelli di ML. Ad esempio, l‚Äôarchitettura GPU Ampere di Nvidia ha un bfloat16 dedicato ai Tensor Core per accelerare i carichi di lavoro AI. I tipi di dati a bassa precisione consentono una maggiore densit√† aritmetica e prestazioni. Per ulteriori dettagli fare riferimento a <a href="../efficient_ai/efficient_ai.it.html#sec-efficient-numerics" class="quarto-xref"><span>Sezione 8.6</span></a>. Gli ASIC possono anche incorporare direttamente operazioni non standard negli algoritmi ML come operazioni primitive, ad esempio, il supporto nativo di funzioni di attivazione come ReLU rende l‚Äôesecuzione pi√π efficiente.</p>
</section>
<section id="parallelismo-elevato" class="level5">
<h5 class="anchored" data-anchor-id="parallelismo-elevato">Parallelismo Elevato</h5>
<p>Le architetture ASIC possono sfruttare un parallelismo pi√π elevato ottimizzato per il carico di lavoro del target rispetto alle CPU o GPU generiche. Un maggior numero di unit√† di calcolo personalizzate per l‚Äôapplicazione significa pi√π operazioni eseguite simultaneamente. Gli ASIC altamente paralleli raggiungono un throughput enorme per carichi di lavoro paralleli di dati come l‚Äôinferenza di reti neurali.</p>
</section>
<section id="nodi-di-processo-avanzati" class="level5">
<h5 class="anchored" data-anchor-id="nodi-di-processo-avanzati">Nodi di Processo Avanzati</h5>
<p>I processi di produzione all‚Äôavanguardia consentono di impacchettare pi√π transistor in aree di die pi√π piccole, aumentando la densit√†. Gli ASIC progettati specificamente per applicazioni ad alto volume possono ammortizzare meglio i costi dei nodi.</p>
</section>
</section>
<section id="svantaggi" class="level4">
<h4 class="anchored" data-anchor-id="svantaggi">Svantaggi</h4>
<section id="tempistiche-di-progettazione-lunghe" class="level5">
<h5 class="anchored" data-anchor-id="tempistiche-di-progettazione-lunghe">Tempistiche di Progettazione Lunghe</h5>
<p>Il processo di progettazione e validazione di un ASIC pu√≤ richiedere 2-3 anni. La sintesi dell‚Äôarchitettura utilizzando linguaggi di descrizione hardware, la definizione del layout del chip e la fabbricazione del chip su nodi di processo avanzati comportano lunghi cicli di sviluppo. Ad esempio, per realizzare un chip da 7 nm, i team devono definire attentamente le specifiche, scrivere l‚Äôarchitettura in HDL, sintetizzare le porte logiche, posizionare i componenti, instradare tutte le interconnessioni e finalizzare il layout da inviare per la fabbricazione. Questa ‚ÄúVery Large-Scale Integration (VLSI)‚Äù significa che la progettazione e la produzione di ASIC possono tradizionalmente richiedere 2-5 anni.</p>
<p>Ci sono alcuni motivi chiave per cui le lunghe tempistiche di progettazione degli ASIC, spesso 2-3 anni, possono essere difficili per i carichi di lavoro di apprendimento automatico:</p>
<ul>
<li><strong>Gli algoritmi ML si evolvono rapidamente:</strong> Nuove architetture di modelli, tecniche di training e ottimizzazioni di rete emergono continuamente. Ad esempio, i Transformers sono diventati estremamente popolari nell‚ÄôNLP negli ultimi anni. Quando un ASIC termina il tapeout, l‚Äôarchitettura ottimale per un carico di lavoro potrebbe essere cambiata.</li>
<li><strong>I dataset crescono rapidamente:</strong> Gli ASIC progettati per determinate dimensioni di modello o tipi di dati possono diventare sottodimensionati rispetto alla domanda. Ad esempio, i modelli di linguaggio naturale stanno aumentando esponenzialmente con pi√π dati e parametri. Un chip progettato per BERT potrebbe non supportare GPT-3.</li>
<li><strong>Le applicazioni ML cambiano frequentemente:</strong> L‚Äôattenzione del settore cambia tra visione artificiale, parlato, NLP, sistemi di raccomandazione, ecc. Un ASIC ottimizzato per la classificazione delle immagini potrebbe avere meno rilevanza in pochi anni.</li>
<li><strong>Cicli di progettazione pi√π rapidi con GPU/FPGA:</strong> Gli acceleratori programmabili come le GPU possono adattarsi molto pi√π rapidamente aggiornando le librerie software e i framework. I nuovi algoritmi possono essere implementati senza modifiche hardware.</li>
<li><strong>Esigenze di time-to-market:</strong> Ottenere un vantaggio competitivo in ML richiede di sperimentare e implementare rapidamente nuove idee. Attendere diversi anni per un ASIC √® diverso da un‚Äôiterazione rapida.</li>
</ul>
<p>Il ritmo dell‚Äôinnovazione in ML deve essere adattato meglio alla scala temporale pluriennale per lo sviluppo di ASIC. Sono necessari notevoli sforzi ingegneristici per estendere la durata di vita di ASIC tramite architetture modulari, ridimensionamento dei processi, compressione dei modelli e altre tecniche. Tuttavia, la rapida evoluzione di ML rende l‚Äôhardware a funzione fissa una sfida.</p>
</section>
<section id="elevati-costi-di-progettazione-non-ricorrenti" class="level5">
<h5 class="anchored" data-anchor-id="elevati-costi-di-progettazione-non-ricorrenti">Elevati Costi di Progettazione Non Ricorrenti</h5>
<p>I costi fissi per portare un ASIC dalla progettazione alla produzione ad alto volume possono essere molto dispendiosi in termini di capitale, spesso decine di milioni di dollari. La fabbricazione di fotomaschere per il tape-out dei chip in nodi di processo avanzati, il packaging e il lavoro di progettazione una tantum sono costosi. Ad esempio, un solo tape-out del chip da 7 nm potrebbe costare milioni. L‚Äôelevato ‚Äúnon-recurring engineering (NRE)‚Äù [investimento di progettazione non ricorrente] riduce la fattibilit√† dell‚ÄôASIC ai casi di utilizzo della produzione ad alto volume in cui il costo iniziale pu√≤ essere ammortizzato.</p>
</section>
<section id="integrazione-e-programmazione-complesse" class="level5">
<h5 class="anchored" data-anchor-id="integrazione-e-programmazione-complesse">Integrazione e Programmazione Complesse</h5>
<p>Gli ASIC richiedono un ampio lavoro di integrazione software, inclusi driver, compilatori, supporto del sistema operativo e strumenti di debug. Hanno anche bisogno di competenza nel packaging elettrico e termico. Inoltre, programmare in modo efficiente le architetture ASIC pu√≤ comportare sfide come il partizionamento del carico di lavoro e la pianificazione su molte unit√† parallele. La natura personalizzata richiede notevoli sforzi di integrazione per trasformare l‚Äôhardware grezzo in acceleratori completamente operativi.</p>
<p>Mentre gli ASIC forniscono enormi guadagni di efficienza nelle applicazioni target adattando ogni aspetto della progettazione hardware a un‚Äôattivit√† specifica, la loro natura fissa comporta compromessi in termini di flessibilit√† e costi di sviluppo rispetto agli acceleratori programmabili, che devono essere soppesati in base all‚Äôapplicazione.</p>
</section>
</section>
</section>
<section id="field-programmable-gate-array-fpga" class="level3 page-columns page-full" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="field-programmable-gate-array-fpga"><span class="header-section-number">10.3.2</span> Field-Programmable Gate Array (FPGA)</h3>
<p>Gli FPGA sono circuiti integrati programmabili che possono essere riconfigurati per diverse applicazioni. La loro natura personalizzabile offre vantaggi per accelerare gli algoritmi AI rispetto agli ASIC fissi o alle GPU inflessibili. Mentre Google, Meta e NVIDIA stanno valutando di installare gli ASIC nei data center, Microsoft ha distribuito gli FPGA nei suoi data center <span class="citation" data-cites="putnam2014reconfigurable">(<a href="../../../references.it.html#ref-putnam2014reconfigurable" role="doc-biblioref">Putnam et al. 2014</a>)</span> nel 2011 per servire in modo efficiente diversi carichi di lavoro.</p>
<div class="no-row-height column-margin column-container"><div id="ref-xiong2021mribased" class="csl-entry" role="listitem">
Xiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei Cao, Xuegong Zhou, et al. 2021. <span>¬´<span>MRI</span>-based brain tumor segmentation using <span>FPGA</span>-accelerated neural network¬ª</span>. <em>BMC Bioinf.</em> 22 (1): 421. <a href="https://doi.org/10.1186/s12859-021-04347-6">https://doi.org/10.1186/s12859-021-04347-6</a>.
</div></div><p>Gli FPGA hanno trovato ampia applicazione in vari campi, tra cui l‚Äôimaging medico, la robotica e la finanza, dove eccellono nella gestione di attivit√† di machine learning ad alta intensit√† di calcolo. Nell‚Äôimaging medico, un esempio illustrativo √® l‚Äôapplicazione degli FPGA per la segmentazione dei tumori cerebrali, un processo tradizionalmente dispendioso in termini di tempo e soggetto a errori. Rispetto alle implementazioni tradizionali di GPU e CPU, gli FPGA hanno dimostrato rispettivamente miglioramenti delle prestazioni di oltre 5 e 44 volte e guadagni di 11 e 82 volte in termini di efficienza energetica, evidenziando il loro potenziale per applicazioni esigenti <span class="citation" data-cites="xiong2021mribased">(<a href="../../../references.it.html#ref-xiong2021mribased" role="doc-biblioref">Xiong et al. 2021</a>)</span>.</p>
<section id="vantaggi-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="vantaggi-1">Vantaggi</h4>
<p>Gli FPGA offrono diversi vantaggi rispetto alle GPU e agli ASIC per accelerare i carichi di lavoro di apprendimento automatico.</p>
<section id="flessibilit√†-tramite-reconfigurable-fabric" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="flessibilit√†-tramite-reconfigurable-fabric">Flessibilit√† Tramite ‚ÄúReconfigurable Fabric‚Äù</h5>
<p>Il vantaggio principale degli FPGA √® la capacit√† di riconfigurare il ‚Äúfabric‚Äù [tessuto] sottostante per implementare architetture personalizzate ottimizzate per diversi modelli, a differenza degli ASIC a funzione fissa. Ad esempio, le societ√† di trading quantitativo utilizzano gli FPGA per accelerare i loro algoritmi perch√© cambiano frequentemente e il basso costo NRE degli FPGA √® pi√π fattibile rispetto acquistare i nuovi ASIC. <a href="#fig-different-fpgas" class="quarto-xref">Figura&nbsp;<span>10.3</span></a> contiene una tabella che confronta tre diversi FPGA.</p>
<div id="fig-different-fpgas" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-different-fpgas-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/fpga.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-different-fpgas-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.3: Confronto di FPGA. Fonte: <span class="citation" data-cites="gwennap_certus-nx_nodate">Gwennap (<a href="../../../references.it.html#ref-gwennap_certus-nx_nodate" role="doc-biblioref">s.d.</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-gwennap_certus-nx_nodate" class="csl-entry" role="listitem">
Gwennap, Linley. s.d. <span>¬´Certus-<span>NX</span> Innovates General-Purpose <span>FPGAs</span>¬ª</span>.
</div></div></figure>
</div>
<p>Gli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono fornite una quantit√† base di queste risorse e gli ingegneri programmano i chip compilando il codice HDL in flussi di bit che riorganizzano la struttura in diverse configurazioni. Questo rende gli FPGA adattabili man mano che gli algoritmi evolvono.</p>
<p>Sebbene gli FPGA possano non raggiungere le massime prestazioni ed efficienza degli ASIC specifici per il carico di lavoro, la loro programmabilit√† offre maggiore flessibilit√† man mano che gli algoritmi cambiano. Questa adattabilit√† rende gli FPGA una scelta interessante per accelerare le applicazioni di machine learning in evoluzione.</p>
</section>
<section id="parallelismo-e-pipeline-personalizzati" class="level5">
<h5 class="anchored" data-anchor-id="parallelismo-e-pipeline-personalizzati">Parallelismo e Pipeline Personalizzati</h5>
<p>Le architetture FPGA possono sfruttare il parallelismo spaziale e il pipelining adattando la progettazione hardware per rispecchiare il parallelismo nei modelli ML. Ad esempio, su una piattaforma FPGA HARPv2 di Intel √® possibile suddividere i layer di una rete convoluzionale su elementi di elaborazione separati per massimizzare la produttivit√†. Sugli FPGA sono possibili anche pattern paralleli unici come le valutazioni di ‚Äúensemble‚Äù ad albero. Pipeline profonde con buffering e flusso di dati ottimizzati possono essere personalizzate in base alla struttura e ai tipi di dati di ogni modello. Questo livello di parallelismo e pipeline su misura non √® fattibile sulle GPU.</p>
</section>
<section id="memoria-on-chip-a-bassa-latenza" class="level5">
<h5 class="anchored" data-anchor-id="memoria-on-chip-a-bassa-latenza">Memoria On-Chip a Bassa Latenza</h5>
<p>Grandi quantit√† di memoria on-chip ad alta larghezza di banda consentono l‚Äôarchiviazione localizzata per pesi e attivazioni. Ad esempio, gli FPGA Xilinx Versal contengono 32 MB di blocchi RAM a bassa latenza e interfacce DDR4 a doppio canale per la memoria esterna. Avvicinare fisicamente la memoria alle unit√† di elaborazione riduce la latenza di accesso. Ci√≤ fornisce significativi vantaggi di velocit√† rispetto alle GPU che attraversano PCIe (Peripheral Component Interconnect Express) o altri bus di sistema per raggiungere la memoria GDDR6 off-chip.</p>
</section>
<section id="supporto-nativo-per-bassa-precisione" class="level5">
<h5 class="anchored" data-anchor-id="supporto-nativo-per-bassa-precisione">Supporto Nativo per Bassa Precisione</h5>
<p>Un vantaggio fondamentale degli FPGA √® la capacit√† di implementare in modo nativo qualsiasi larghezza di bit per unit√† aritmetiche, come INT4 o bfloat16, utilizzate nei modelli ML quantizzati. Ad esempio, <a href="https://www.intel.com/content/www/us/en/products/details/fpga/stratix/10/nx.html">Intel Stratix 10 NX FPGA</a> ha core INT8 dedicati che possono raggiungere fino a 143 INT8 TOPS (Tera Operations Per Second) a ~1 TOPS/W (Tera Operations Per Second per Watt). TOPS √® una misura di prestazioni simile a FLOPS, ma mentre FLOPS misura calcoli in virgola mobile, TOPS misura il numero di operazioni intere che un sistema pu√≤ eseguire al secondo. Le larghezze di bit inferiori, come INT8 o INT4, aumentano la densit√† aritmetica e le prestazioni. Gli FPGA possono persino supportare la sintonizzazione a precisione mista o dinamica in fase di esecuzione.</p>
</section>
</section>
<section id="svantaggi-1" class="level4">
<h4 class="anchored" data-anchor-id="svantaggi-1">Svantaggi</h4>
<section id="throughput-di-picco-inferiore-rispetto-agli-asic" class="level5">
<h5 class="anchored" data-anchor-id="throughput-di-picco-inferiore-rispetto-agli-asic">Throughput di Picco Inferiore Rispetto agli ASIC</h5>
<p>Gli FPGA non possono eguagliare i numeri di throughput grezzi degli ASIC, personalizzati per un modello e una precisione specifici. I sovraccarichi del ‚Äúfabric‚Äù riconfigurabile rispetto all‚Äôhardware a funzione fissa comportano prestazioni di picco inferiori. Ad esempio, i pod TPU v5e consentono di connettere fino a 256 chip con oltre 100 PetaOps (Peta Operations Per Second) di prestazioni INT8, mentre gli FPGA possono offrire fino a 143 INT8 TOPS o 286 INT4 TOPS come sull‚ÄôFPGA Intel Stratix 10 NX; PetaOps rappresenta quadrilioni di operazioni al secondo, mentre TOPS misura trilioni, evidenziando la capacit√† di elaborazione molto maggiore dei pod TPU rispetto agli FPGA.</p>
<p>Questo perch√© gli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono forniti con una quantit√† stabilita di queste risorse. Per programmare gli FPGA, gli ingegneri scrivono codice HDL e lo compilano in flussi di bit che riorganizzano il ‚Äúfabric‚Äù, che ha sovraccarichi intrinseci rispetto a un ASIC appositamente progettato per un calcolo.</p>
</section>
<section id="complessit√†-di-programmazione" class="level5">
<h5 class="anchored" data-anchor-id="complessit√†-di-programmazione">Complessit√† di Programmazione</h5>
<p>Per ottimizzare le prestazioni FPGA, gli ingegneri devono programmare le architetture in linguaggi di descrizione hardware di basso livello come Verilog o VHDL. Ci√≤ richiede competenza nella progettazione hardware e cicli di sviluppo pi√π lunghi rispetto a framework software di livello superiore come TensorFlow. Massimizzare l‚Äôutilizzo pu√≤ essere difficile nonostante i progressi nella sintesi di alto livello da C/C++.</p>
</section>
<section id="sovraccarichi-di-riconfigurazione" class="level5">
<h5 class="anchored" data-anchor-id="sovraccarichi-di-riconfigurazione">Sovraccarichi di Riconfigurazione</h5>
<p>La modifica delle configurazioni FPGA richiede il ricaricamento di un nuovo flusso di bit, che ha costi di latenza e dimensioni di archiviazione considerevoli. Ad esempio, la riconfigurazione parziale su FPGA Xilinx pu√≤ richiedere centinaia di millisecondi. Questo rende impossibile lo scambio dinamico di architetture in tempo reale. L‚Äôarchiviazione del flusso di bit consuma anche memoria on-chip.</p>
</section>
<section id="guadagni-in-diminuzione-sui-nodi-avanzati" class="level5">
<h5 class="anchored" data-anchor-id="guadagni-in-diminuzione-sui-nodi-avanzati">Guadagni in diminuzione sui nodi avanzati</h5>
<p>Sebbene i nodi di processo pi√π piccoli siano molto vantaggiosi per gli ASIC, offrono meno vantaggi per gli FPGA. A 7 nm e al di sotto, effetti come variazione di processo, vincoli termici e invecchiamento hanno un impatto sproporzionato sulle prestazioni degli FPGA. Anche le spese generali della struttura configurabile riducono i guadagni rispetto agli ASIC a funzione fissa.</p>
</section>
</section>
</section>
<section id="digital-signal-processor-dsp" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="digital-signal-processor-dsp"><span class="header-section-number">10.3.3</span> Digital Signal Processor (DSP)</h3>
<p>Il primo core di elaborazione del segnale digitale √® stato costruito nel 1948 da Texas Instruments (<a href="https://audioxpress.com/article/the-evolution-of-audio-dsps">The Evolution of Audio DSPs</a>). Tradizionalmente, i DSP avrebbero avuto una logica per accedere direttamente ai dati digitali/audio nella memoria, eseguire un‚Äôoperazione aritmetica (moltiplica-addiziona-accumula-MAC era una delle operazioni pi√π comuni) e quindi scrivere il risultato nella memoria. Il DSP avrebbe incluso componenti analogici specializzati per recuperare i dati digitali/audio.</p>
<p>Una volta entrati nell‚Äôera degli smartphone, i DSP hanno iniziato a comprendere attivit√† pi√π sofisticate. Richiedevano Bluetooth, Wi-Fi e connettivit√† cellulare. Anche i media sono diventati molto pi√π complessi. Oggi, √® raro avere chip interi dedicati solo al DSP, ma un System on Chip includerebbe DSP e CPU per uso generico. Ad esempio, l‚Äô<a href="https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor">Hexagon Digital Signal Processor</a> di Qualcomm afferma di essere un ‚Äúprocessore di livello mondiale con funzionalit√† sia CPU che DSP per supportare le esigenze di elaborazione profondamente integrate della piattaforma mobile per funzioni sia multimediali che modem‚Äù. <a href="https://blog.google/products/pixel/google-tensor-g3-pixel-8/">Google Tensors</a>, il chip nei telefoni Google Pixel, include anche CPU e motori DSP specializzati.</p>
<section id="vantaggi-2" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi-2">Vantaggi</h4>
<p>I DSP offrono vantaggi architettonici in termini di throughput della matematica vettoriale, accesso alla memoria a bassa latenza, efficienza energetica e supporto per diversi tipi di dati, rendendoli adatti all‚Äôaccelerazione ML embedded.</p>
<section id="architettura-ottimizzata-per-la-matematica-vettoriale" class="level5">
<h5 class="anchored" data-anchor-id="architettura-ottimizzata-per-la-matematica-vettoriale">Architettura Ottimizzata per la Matematica Vettoriale</h5>
<p>I DSP contengono percorsi dati specializzati, file di registro e istruzioni ottimizzati specificamente per le operazioni di matematica vettoriale comunemente utilizzate nei modelli di apprendimento automatico. Ci√≤ include motori di prodotto scalare, unit√† MAC e funzionalit√† SIMD su misura per calcoli vettoriali/matriciali. Ad esempio, il DSP CEVA-XM6 (<a href="https://www.ceva-dsp.com/wp-content/uploads/2020/04/Ceva-SensPro-Fuses-AI-and-Vector-DSP.pdf">‚ÄúCeva SensPro fonde AI e Vector DSP‚Äù</a>) ha unit√† vettoriali a 512 bit per accelerare le convoluzioni. Questa efficienza sui carichi di lavoro di matematica vettoriale va ben oltre le CPU generiche.</p>
</section>
<section id="memoria-on-chip-a-bassa-latenza-1" class="level5">
<h5 class="anchored" data-anchor-id="memoria-on-chip-a-bassa-latenza-1">Memoria On-Chip a Bassa Latenza</h5>
<p>I DSP integrano grandi quantit√† di memoria SRAM veloce su chip per conservare i dati localmente per l‚Äôelaborazione. Avvicinare fisicamente la memoria alle unit√† di calcolo riduce la latenza di accesso. Ad esempio, il DSP SHARC+ di Analog contiene 10 MB di SRAM su chip. Questa memoria locale ad alta larghezza di banda offre vantaggi di velocit√† per le applicazioni in tempo reale.</p>
</section>
<section id="efficienza-energetica" class="level5">
<h5 class="anchored" data-anchor-id="efficienza-energetica">Efficienza Energetica</h5>
<p>I DSP sono progettati per fornire elevate prestazioni per watt su carichi di lavoro di segnali digitali. Percorsi dati efficienti, parallelismo e architetture di memoria consentono trilioni di operazioni matematiche al secondo entro budget di potenza mobili ridotti. Ad esempio, <a href="https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor">l‚ÄôHexagon DSP di Qualcomm</a> pu√≤ fornire 4 TOPS consumando un numero minimo di watt.</p>
</section>
<section id="supporto-per-matematica-a-virgola-mobile-e-intera" class="level5">
<h5 class="anchored" data-anchor-id="supporto-per-matematica-a-virgola-mobile-e-intera">Supporto per Matematica a Virgola Mobile e Intera</h5>
<p>A differenza delle GPU che eccellono in precisione singola o dimezzata, i DSP possono supportare nativamente tipi di dati a virgola mobile e intera a 8/16 bit utilizzati nei modelli ML. Alcuni DSP supportano l‚Äôaccelerazione del prodotto scalare a precisione INT8 per reti neurali quantizzate.</p>
</section>
</section>
<section id="svantaggi-2" class="level4">
<h4 class="anchored" data-anchor-id="svantaggi-2">Svantaggi</h4>
<p>I DSP fanno compromessi architettonici che limitano il throughput di picco, la precisione e la capacit√† del modello rispetto ad altri acceleratori AI. Tuttavia, i loro vantaggi in termini di efficienza energetica e matematica intera li rendono una valida opzione di edge computing. Quindi, mentre i DSP offrono alcuni vantaggi rispetto alle CPU, presentano anche delle limitazioni per i carichi di lavoro di apprendimento automatico:</p>
<section id="throughput-di-picco-inferiore-rispetto-ad-asicgpu" class="level5">
<h5 class="anchored" data-anchor-id="throughput-di-picco-inferiore-rispetto-ad-asicgpu">Throughput di Picco Inferiore Rispetto ad ASIC/GPU</h5>
<p>I DSP non possono eguagliare il throughput computazionale grezzo delle GPU o degli ASIC personalizzati progettati specificamente per l‚Äôapprendimento automatico. Ad esempio, l‚ÄôASIC Cloud AI 100 di Qualcomm fornisce 480 TOPS su INT8, mentre il loro DSP Hexagon fornisce 10 TOPS. I DSP non hanno il massiccio parallelismo delle unit√† GPU SM.</p>
</section>
<section id="prestazioni-a-doppia-precisione-pi√π-lente" class="level5">
<h5 class="anchored" data-anchor-id="prestazioni-a-doppia-precisione-pi√π-lente">Prestazioni a Doppia Precisione pi√π Lente</h5>
<p>La maggior parte dei DSP deve essere ottimizzata per la virgola mobile di precisione pi√π elevata necessaria in alcuni modelli ML. I loro motori di prodotto scalare si concentrano su INT8/16 e FP32, che forniscono una migliore efficienza energetica. Tuttavia, la produttivit√† in virgola mobile a 64 bit √® molto pi√π bassa, il che pu√≤ limitare l‚Äôutilizzo nei modelli che richiedono un‚Äôelevata precisione.</p>
</section>
<section id="capacit√†-del-modello-limitata" class="level5">
<h5 class="anchored" data-anchor-id="capacit√†-del-modello-limitata">Capacit√† del Modello Limitata</h5>
<p>La limitata memoria on-chip dei DSP limita le dimensioni del modello che possono eseguire. Grandi modelli di deep learning con centinaia di megabyte di parametri supererebbero la capacit√† delle SRAM on-chip. I DSP sono pi√π adatti per modelli di piccole e medie dimensioni destinati a dispositivi edge.</p>
</section>
<section id="complessit√†-di-programmazione-1" class="level5">
<h5 class="anchored" data-anchor-id="complessit√†-di-programmazione-1">Complessit√† di Programmazione</h5>
<p>La programmazione efficiente delle architetture DSP richiede competenza nella programmazione parallela e nell‚Äôottimizzazione dei pattern di accesso ai dati. Le loro microarchitetture specializzate hanno una curva di apprendimento pi√π ripida rispetto ai framework software di alto livello, rendendo lo sviluppo pi√π complesso.</p>
</section>
</section>
</section>
<section id="graphics-processing-unit-gpu" class="level3 page-columns page-full" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="graphics-processing-unit-gpu"><span class="header-section-number">10.3.4</span> Graphics Processing Unit (GPU)</h3>
<p>Il termine ‚Äúgraphics processing unit‚Äù [unit√† di elaborazione grafica] esiste almeno dagli anni ‚Äô80. C‚Äô√® sempre stata una richiesta di hardware grafico nelle console per videogiochi (elevata richiesta, doveva avere un costo relativamente basso) e nelle simulazioni scientifiche (richiesta inferiore, ma risoluzione pi√π alta, poteva avere un prezzo elevato).</p>
<p>Il termine √® stato reso popolare, tuttavia, nel 1999 quando NVIDIA ha lanciato la GeForce 256, mirando principalmente al settore di mercato dei giochi per PC <span class="citation" data-cites="lindholm2008nvidia">(<a href="../../../references.it.html#ref-lindholm2008nvidia" role="doc-biblioref">Lindholm et al. 2008</a>)</span>. Man mano che i giochi per PC diventavano pi√π sofisticati, le GPU NVIDIA diventavano pi√π programmabili. Presto, gli utenti si resero conto che potevano sfruttare questa programmabilit√†, eseguire vari carichi di lavoro non correlati alla grafica sulle GPU e trarre vantaggio dall‚Äôarchitettura sottostante. E cos√¨, alla fine degli anni 2000, le GPU divennero unit√† di elaborazione grafica per uso generale o GP-GPU.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lindholm2008nvidia" class="csl-entry" role="listitem">
Lindholm, Erik, John Nickolls, Stuart Oberman, e John Montrym. 2008. <span>¬´<span>NVIDIA</span> Tesla: <span>A</span> Unified Graphics and Computing Architecture¬ª</span>. <em>IEEE Micro</em> 28 (2): 39‚Äì55. <a href="https://doi.org/10.1109/mm.2008.31">https://doi.org/10.1109/mm.2008.31</a>.
</div></div><p>In seguito a questo cambiamento, altri importanti attori come Intel con la sua <a href="https://www.intel.com/content/www/us/en/products/details/fpga/stratix/10/nx.html">Arc Graphics</a> e AMD con la sua serie <a href="https://www.amd.com/en/graphics/radeon-rx-graphics">Radeon RX</a> hanno anche evoluto le loro GPU per supportare una gamma pi√π ampia di applicazioni oltre al rendering grafico tradizionale. Questa espansione delle capacit√† delle GPU ha aperto nuove possibilit√†, in particolare nei campi che richiedono un‚Äôenorme potenza di calcolo.</p>
<p>Un esempio lampante di questo potenziale √® la recente ricerca rivoluzionaria condotta da OpenAI <span class="citation" data-cites="brown2020language">(<a href="../../../references.it.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> con GPT-3, un modello di linguaggio con 175 miliardi di parametri. L‚Äôaddestramento di un modello cos√¨ massiccio, che avrebbe richiesto mesi su CPU convenzionali, √® stato completato in pochi giorni utilizzando potenti GPU, dimostrando l‚Äôimpatto trasformativo delle GPU nell‚Äôaccelerazione di complesse attivit√† di apprendimento automatico.</p>
<div class="no-row-height column-margin column-container"></div><section id="vantaggi-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="vantaggi-3">Vantaggi</h4>
<section id="elevata-capacit√†-di-elaborazione" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="elevata-capacit√†-di-elaborazione">Elevata Capacit√† di Elaborazione</h5>
<p>Il vantaggio principale delle GPU √® la loro capacit√† di eseguire calcoli in virgola mobile paralleli massivi ottimizzati per la computer grafica e l‚Äôalgebra lineare <span class="citation" data-cites="rajat2009largescale">(<a href="../../../references.it.html#ref-rajat2009largescale" role="doc-biblioref">Raina, Madhavan, e Ng 2009</a>)</span>. Le GPU moderne come la A100 di Nvidia offrono fino a 19,5 teraflop di prestazioni FP32 con 6912 core CUDA e 40 GB di memoria grafica strettamente accoppiati a 1,6 TB/s di larghezza di banda della memoria grafica.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rajat2009largescale" class="csl-entry" role="listitem">
Raina, Rajat, Anand Madhavan, e Andrew Y. Ng. 2009. <span>¬´Large-scale deep unsupervised learning using graphics processors¬ª</span>. In <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>, a cura di Andrea Pohoreckyj Danyluk, L√©on Bottou, e Michael L. Littman, 382:873‚Äì80. ACM International Conference Proceeding Series. ACM. <a href="https://doi.org/10.1145/1553374.1553486">https://doi.org/10.1145/1553374.1553486</a>.
</div></div><p>Questa capacit√† di elaborazione grezza deriva dall‚Äôarchitettura ‚ÄúStreaming Multiprocessor‚Äù (SM) altamente parallela, pensata per carichi di lavoro paralleli ai dati <span class="citation" data-cites="jia2019beyond">(<a href="../../../references.it.html#ref-jia2019beyond" role="doc-biblioref">Zhihao Jia, Zaharia, e Aiken 2019</a>)</span>. Ogni SM contiene centinaia di core scalari ottimizzati per la matematica float32/64. Con migliaia di SM su un chip, le GPU sono appositamente progettate per la moltiplicazione di matrici e le operazioni vettoriali utilizzate in tutte le reti neurali.</p>
<p>Ad esempio, l‚Äôultima GPU <a href="https://www.nvidia.com/en-us/data-center/h100/">H100</a> di Nvidia fornisce 4000 TFLOP di FP8, 2000 TFLOP di FP16, 1000 TFLOP di TF32, 67 TFLOP di FP32 e 34 TFLOP di prestazioni di elaborazione FP64, che possono accelerare notevolmente l‚Äôaddestramento di grandi batch su modelli come BERT, GPT-3 e altre architetture di trasformatori. Il parallelismo scalabile delle GPU √® fondamentale per accelerare il deep learning computazionalmente intensivo.</p>
</section>
<section id="ecosistema-software-maturo" class="level5">
<h5 class="anchored" data-anchor-id="ecosistema-software-maturo">Ecosistema Software Maturo</h5>
<p>Nvidia fornisce ampie librerie di runtime come <a href="https://developer.nvidia.com/cudnn">cuDNN</a> e <a href="https://developer.nvidia.com/cublas">cuBLAS</a> che sono altamente ottimizzate per primitive di deep learning. Framework come TensorFlow e PyTorch si integrano con queste librerie per abilitare l‚Äôaccelerazione GPU senza programmazione diretta. Queste librerie sono basate su CUDA, la piattaforma di elaborazione parallela e il modello di programmazione di Nvidia.</p>
<p>CUDA (Compute Unified Device Architecture) √® il framework sottostante che consente a queste librerie di alto livello di interagire con l‚Äôhardware della GPU. Fornisce agli sviluppatori un accesso di basso livello alle risorse della GPU, consentendo calcoli e ottimizzazioni personalizzate che sfruttano appieno le capacit√† di elaborazione parallela della GPU. Utilizzando CUDA, gli sviluppatori possono scrivere software che sfruttano l‚Äôarchitettura della GPU per attivit√† di elaborazione ad alte prestazioni.</p>
<p>Questo ecosistema consente di sfruttare rapidamente le GPU ad alto livello tramite Python senza competenze di programmazione GPU. Flussi di lavoro e astrazioni noti forniscono una comoda rampa di accesso per scalare gli esperimenti di deep learning. La maturit√† del software integra i vantaggi della produttivit√†.</p>
</section>
<section id="ampia-disponibilit√†" class="level5">
<h5 class="anchored" data-anchor-id="ampia-disponibilit√†">Ampia Disponibilit√†</h5>
<p>Le economie di scala dell‚Äôelaborazione grafica rendono le GPU ampiamente accessibili nei data center, nelle piattaforme cloud come AWS e GCP e nelle workstation desktop. La loro disponibilit√† negli ambienti di ricerca ha fornito una comoda piattaforma di sperimentazione e innovazione nel ML. Ad esempio, quasi tutti i risultati di deep learning all‚Äôavanguardia hanno coinvolto l‚Äôaccelerazione GPU per merito di questa ubiquit√†. L‚Äôampio accesso integra la maturit√† del software per rendere le GPU l‚Äôacceleratore ML standard.</p>
</section>
<section id="architettura-programmabile" class="level5">
<h5 class="anchored" data-anchor-id="architettura-programmabile">Architettura Programmabile</h5>
<p>Sebbene non siano flessibili come gli FPGA, le GPU offrono programmabilit√† tramite linguaggi CUDA e shader per personalizzare i calcoli. Gli sviluppatori possono ottimizzare i pattern di accesso ai dati, creare nuove operazioni e regolare le precisioni per modelli e algoritmi in evoluzione.</p>
</section>
</section>
<section id="svantaggi-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="svantaggi-3">Svantaggi</h4>
<p>Sebbene le GPU siano diventate l‚Äôacceleratore standard per il deep learning, la loro architettura presenta alcuni svantaggi importanti.</p>
<section id="meno-efficienti-degli-asic-custom" class="level5">
<h5 class="anchored" data-anchor-id="meno-efficienti-degli-asic-custom">Meno Efficienti degli ASIC Custom</h5>
<p>L‚Äôaffermazione ‚ÄúLe GPU sono meno efficienti degli ASIC‚Äù potrebbe scatenare un acceso dibattito nel campo ML/AI e far esplodere questo libro.</p>
<p>In genere, le GPU sono percepite come meno efficienti degli ASIC perch√© questi ultimi sono realizzati su misura per attivit√† specifiche e quindi possono funzionare in modo pi√π efficiente nativamente. Con la loro architettura generica, le GPU sono intrinsecamente pi√π versatili e programmabili, soddisfacendo un ampio spettro di attivit√† computazionali oltre a ML/AI.</p>
<p>Tuttavia, le GPU moderne si sono evolute per includere un supporto hardware specializzato per operazioni AI essenziali, come la moltiplicazione di matrici generalizzata (GEMM) e altre operazioni di matrice, supporto nativo per la quantizzazione e supporto nativo per la potatura, che sono fondamentali per l‚Äôesecuzione efficace dei modelli ML. Questi miglioramenti hanno notevolmente migliorato l‚Äôefficienza delle GPU per le attivit√† AI al punto che possono rivaleggiare con le prestazioni degli ASIC per determinate applicazioni.</p>
<p>Di conseguenza, le GPU contemporanee sono convergenti, incorporando capacit√† specializzate simili ad ASIC all‚Äôinterno di un framework di elaborazione flessibile e di uso generale. Questa adattabilit√† ha offuscato i confini tra i due tipi di hardware. Le GPU offrono un forte equilibrio tra specializzazione e programmabilit√† che si adatta bene alle esigenze dinamiche della ricerca e sviluppo ML/AI.</p>
</section>
<section id="elevate-esigenze-di-larghezza-di-banda-di-memoria" class="level5">
<h5 class="anchored" data-anchor-id="elevate-esigenze-di-larghezza-di-banda-di-memoria">Elevate Esigenze di Larghezza di Banda di Memoria</h5>
<p>L‚Äôarchitettura massicciamente parallela richiede un‚Äôenorme larghezza di banda di memoria per alimentare migliaia di core. Ad esempio, la GPU Nvidia A100 richiede 1.6 TB/sec per saturare completamente il suo computer. Le GPU si affidano ad ampi bus di memoria a 384 bit per RAM GDDR6 ad alta larghezza di banda, ma anche la GDDR6 pi√π veloce raggiunge il massimo a circa 1 TB/sec.&nbsp;Questa dipendenza dalla DRAM esterna comporta latenza e sovraccarico di potenza.</p>
</section>
<section id="complessit√†-di-programmazione-2" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="complessit√†-di-programmazione-2">Complessit√† di Programmazione</h5>
<p>Sebbene strumenti come CUDA siano utili, la mappatura e il partizionamento ottimali dei carichi di lavoro ML nell‚Äôarchitettura GPU massivamente parallela rimangono una sfida, il raggiungimento di un utilizzo elevato e della localit√† della memoria richiede una messa a punto di basso livello <span class="citation" data-cites="jia2018dissecting">(<a href="../../../references.it.html#ref-jia2018dissecting" role="doc-biblioref">Zhe Jia et al. 2018</a>)</span>. Astrazioni come TensorFlow possono tralasciare le prestazioni.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jia2018dissecting" class="csl-entry" role="listitem">
Jia, Zhe, Marco Maggioni, Benjamin Staiger, e Daniele P. Scarpazza. 2018. <span>¬´Dissecting the <span>NVIDIA</span> <span>Volta</span> <span>GPU</span> Architecture via Microbenchmarking¬ª</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/1804.06826">https://arxiv.org/abs/1804.06826</a>.
</div></div></section>
<section id="memoria-on-chip-limitata" class="level5">
<h5 class="anchored" data-anchor-id="memoria-on-chip-limitata">Memoria On-Chip Limitata</h5>
<p>Le GPU hanno cache di memoria on-chip relativamente piccole rispetto ai grandi requisiti di working set dei modelli ML durante l‚Äôaddestramento. Si basano su un accesso ad alta larghezza di banda alla DRAM esterna, che gli ASIC riducono al minimo con una grande SRAM on-chip.</p>
</section>
<section id="architettura-fissa" class="level5">
<h5 class="anchored" data-anchor-id="architettura-fissa">Architettura Fissa</h5>
<p>A differenza degli FPGA, l‚Äôarchitettura fondamentale della GPU non pu√≤ essere modificata dopo la produzione. Questo vincolo limita l‚Äôadattamento a nuovi carichi di lavoro o layer ML. Il confine CPU-GPU crea anche overhead di spostamento dei dati.</p>
</section>
</section>
</section>
<section id="central-processing-unit-cpu" class="level3 page-columns page-full" data-number="10.3.5">
<h3 data-number="10.3.5" class="anchored" data-anchor-id="central-processing-unit-cpu"><span class="header-section-number">10.3.5</span> Central Processing Unit (CPU)</h3>
<p>Il termine CPU ha una lunga storia che risale al 1955 <span class="citation" data-cites="weik1955survey">(<a href="../../../references.it.html#ref-weik1955survey" role="doc-biblioref">Weik 1955</a>)</span> mentre la prima CPU a microprocessore, l‚ÄôIntel 4004, √® stata inventata nel 1971 (<a href="https://computerhistory.org/blog/who-invented-the-microprocessor/">Chi ha inventato il microprocessore?</a>). I compilatori traducono linguaggi di programmazione di alto livello come Python, Java o C per assemblare istruzioni (x86, ARM, RISC-V, ecc.) che le CPU devono elaborare. Il set di istruzioni che una CPU comprende √® chiamato ‚Äúinstruction set architecture‚Äù (ISA), che definisce i comandi che il processore pu√≤ eseguire direttamente. Deve essere concordato sia dall‚Äôhardware che dal software ci gira sopra.</p>
<div class="no-row-height column-margin column-container"><div id="ref-weik1955survey" class="csl-entry" role="listitem">
Weik, Martin H. 1955. <em>A Survey of Domestic Electronic Digital Computing Systems</em>. Ballistic Research Laboratories.
</div></div><p>Una panoramica degli sviluppi significativi nelle CPU:</p>
<ul>
<li><strong>Era del Single-core (anni ‚Äô50-2000):</strong> Questa era √® nota per i miglioramenti microarchitettonici aggressivi. Tecniche come l‚Äôesecuzione speculativa (esecuzione di un‚Äôistruzione prima che quella precedente fosse finita), ‚Äúout-of-order execution‚Äù [esecuzione fuori ordine] (riordinamento delle istruzioni per renderle pi√π efficaci) e ‚Äúwider issue widths‚Äù [larghezze di emissione pi√π ampie] (esecuzione di pi√π istruzioni contemporaneamente) sono state implementate per aumentare la produttivit√† delle istruzioni. Anche il termine ‚ÄúSystem on Chip‚Äù ha avuto origine in questa era, poich√© diversi componenti analogici (componenti progettati con transistor) e componenti digitali (componenti progettati con linguaggi di descrizione hardware mappati su transistor) sono stati inseriti sulla stessa piattaforma per realizzare un‚Äôattivit√†.</li>
<li><strong>Era Multicore (anni 2000):</strong> Guidata dalla diminuzione della legge di Moore, questa √® caratterizzata dall‚Äôaumento del numero di core all‚Äôinterno di una CPU. Ora, le attivit√† possono essere suddivise su pi√π core diversi, ognuno con il proprio percorso dati e unit√† di controllo. Molti dei problemi di quest‚Äôepoca riguardavano come condividere determinate risorse, quali risorse condividere e come mantenere coerenza e consistenza in tutti i core.</li>
<li><strong>Un Mare di acceleratori (anni 2010):</strong> Ancora una volta, spinta dalla diminuzione della legge di Moore, quest‚Äôepoca √® caratterizzata dal delegare le attivit√† pi√π complicate su acceleratori (widget) collegati al datapath principale nelle CPU. √à comune vedere acceleratori dedicati a vari carichi di lavoro di intelligenza artificiale, nonch√© elaborazione di immagini/digitali e crittografia. In queste progettazioni, le CPU sono spesso descritte pi√π come giudici, che decidono quali attivit√† devono essere elaborate piuttosto che eseguire l‚Äôelaborazione stessa. Qualsiasi attivit√† potrebbe comunque essere eseguita sulla CPU anzich√© sugli acceleratori, ma la CPU sarebbe generalmente pi√π lenta. Tuttavia, il costo di progettazione e programmazione dell‚Äôacceleratore √® diventato un ostacolo non banale che ha suscitato interesse per le librerie specifiche per la progettazione (DSL).</li>
<li><strong>Presenza nei data center:</strong> Sebbene sentiamo spesso dire che le GPU dominano il mercato dei data center, le CPU sono comunque adatte per attivit√† che non possiedono intrinsecamente un elevato grado di parallelismo. Le CPU spesso gestiscono attivit√† seriali e di piccole dimensioni e coordinano il data center.</li>
<li><strong>Sull‚Äôedge:</strong> Dati i vincoli pi√π rigidi sulle risorse sull‚Äôedge, le CPU edge spesso implementano solo un sottoinsieme delle tecniche sviluppate nell‚Äôera single-core perch√© queste ottimizzazioni tendono a essere pesanti in termini di consumo di energia e area. Le CPU edge mantengono comunque un datapath relativamente semplice con capacit√† di memoria limitate.</li>
</ul>
<p>Tradizionalmente, le CPU sono state sinonimo di elaborazione generica, un termine che √® cambiato anche perch√© il carico di lavoro ‚Äúmedio‚Äù che un consumatore esegue cambia nel tempo. Ad esempio, i componenti in virgola mobile erano un tempo considerati riservati alla ‚Äúelaborazione scientifica‚Äù, di solito venivano implementati come un coprocessore (un componente modulare che funzionava con il datapath) e raramente distribuiti ai consumatori medi. Confrontate questo atteggiamento con quello odierno, in cui le FPU sono integrate in ogni datapath.</p>
<section id="vantaggi-4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="vantaggi-4">Vantaggi</h4>
<p>Sebbene la produttivit√† in s√© sia limitata, le CPU per uso generico offrono vantaggi pratici di accelerazione AI.</p>
<section id="programmabilit√†-generale" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="programmabilit√†-generale">Programmabilit√† Generale</h5>
<p>Le CPU supportano carichi di lavoro diversi oltre al ML, offrendo una programmabilit√† flessibile per uso generico. Questa versatilit√† deriva dai loro set di istruzioni standardizzati e dagli ecosistemi di compilatori maturi, che consentono di eseguire qualsiasi applicazione, dai database e server Web alle pipeline analitiche <span class="citation" data-cites="hennessy2019golden">(<a href="../../../references.it.html#ref-hennessy2019golden" role="doc-biblioref">Hennessy e Patterson 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hennessy2019golden" class="csl-entry" role="listitem">
Hennessy, John L., e David A. Patterson. 2019. <span>¬´A new golden age for computer architecture¬ª</span>. <em>Commun. ACM</em> 62 (2): 48‚Äì60. <a href="https://doi.org/10.1145/3282307">https://doi.org/10.1145/3282307</a>.
</div></div><p>Questo evita la necessit√† di acceleratori ML dedicati e consente di sfruttare l‚Äôinfrastruttura basata su CPU esistenti per la distribuzione ML di base. Ad esempio, i server X86 di fornitori come Intel e AMD possono eseguire framework ML comuni utilizzando pacchetti Python e TensorFlow insieme ad altri carichi di lavoro aziendali.</p>
</section>
<section id="ecosistema-software-maturo-1" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="ecosistema-software-maturo-1">Ecosistema Software Maturo</h5>
<p>Per decenni, librerie matematiche altamente ottimizzate come <a href="https://www.netlib.org/blas/">BLAS</a>, <a href="https://hpc.llnl.gov/software/mathematical-software/lapack#:~:text=The%20Linear%20Algebra%20PACKage%20(LAPACK,problems%2C%20and%20singular%20value%20decomposition.)">LAPACK</a> e <a href="https://www.fftw.org/">FFTW</a> hanno sfruttato istruzioni vettorializzate e multithreading su CPU <span class="citation" data-cites="dongarra2009evolution">(<a href="../../../references.it.html#ref-dongarra2009evolution" role="doc-biblioref">Dongarra 2009</a>)</span>. I principali framework ML come PyTorch, TensorFlow e SciKit-Learn sono progettati per integrarsi perfettamente con questi kernel matematici di CPU.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dongarra2009evolution" class="csl-entry" role="listitem">
Dongarra, Jack J. 2009. <span>¬´The evolution of high performance computing on system z¬ª</span>. <em>IBM J. Res. Dev.</em> 53: 3‚Äì4.
</div></div><p>I fornitori di hardware come Intel e AMD forniscono anche librerie di basso livello per ottimizzare completamente le prestazioni per primitive di deep learning (<a href="https://www.intel.com/content/www/us/en/developer/articles/technical/ai-inference-acceleration-on-intel-cpus.html#gs.0w9qn2">accelerazione dell‚Äôinferenza AI su CPU</a>). Questo ecosistema software robusto e maturo consente di distribuire rapidamente ML su infrastrutture di CPU esistenti.</p>
</section>
<section id="ampia-disponibilit√†-1" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="ampia-disponibilit√†-1">Ampia Disponibilit√†</h5>
<p>Le economie di scala della produzione di CPU, guidate dalla domanda in molti mercati come PC, server e dispositivi mobili, le rendono disponibili ovunque. Le CPU Intel, ad esempio, hanno alimentato la maggior parte dei server per decenni <span class="citation" data-cites="ranganathan2011from">(<a href="../../../references.it.html#ref-ranganathan2011from" role="doc-biblioref">Ranganathan 2011</a>)</span>. Questa ampia disponibilit√† nei data center riduce i costi hardware per l‚Äôimplementazione di ML di base.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ranganathan2011from" class="csl-entry" role="listitem">
Ranganathan, Parthasarathy. 2011. <span>¬´From Microprocessors to Nanostores: <span>Rethinking</span> Data-Centric Systems¬ª</span>. <em>Computer</em> 44 (1): 39‚Äì48. <a href="https://doi.org/10.1109/mc.2011.18">https://doi.org/10.1109/mc.2011.18</a>.
</div></div><p>Anche i piccoli dispositivi embedded in genere integrano una certa CPU, consentendo l‚Äôinferenza edge. L‚Äôubiquit√† riduce la necessit√† di acquistare acceleratori ML specializzati in molte situazioni.</p>
</section>
<section id="basso-consumo-per-linferenza" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="basso-consumo-per-linferenza">Basso Consumo per L‚Äôinferenza</h5>
<p>Ottimizzazioni come ARM Neon e le estensioni vettoriali Intel AVX forniscono un throughput di numeri interi e in virgola mobile a basso consumo ottimizzato per carichi di lavoro ‚Äúa raffica‚Äù come l‚Äôinferenza <span class="citation" data-cites="ignatov2018ai">(<a href="../../../references.it.html#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2018</a>)</span>. Sebbene pi√π lenta delle GPU, l‚Äôinferenza CPU pu√≤ essere implementata in ambienti con vincoli energetici. Ad esempio, le CPU Cortex-M di ARM ora offrono oltre 1 TOPS di prestazioni INT8 sotto 1 W, consentendo l‚Äôindividuazione di parole chiave e applicazioni di visione su dispositivi edge (<a href="https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/armv8_2d00_m-based-processor-software-development-hints-and-tips">ARM</a>).</p>
<div class="no-row-height column-margin column-container"></div></section>
</section>
<section id="svantaggi-4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="svantaggi-4">Svantaggi</h4>
<p>Pur offrendo alcuni vantaggi, le CPU per uso generico presentano anche delle limitazioni per i carichi di lavoro AI.</p>
<section id="throughput-inferiore-rispetto-agli-acceleratori" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="throughput-inferiore-rispetto-agli-acceleratori">Throughput Inferiore Rispetto agli Acceleratori</h5>
<p>Le CPU non dispongono delle architetture specializzate per l‚Äôelaborazione parallela massiva che GPU e altri acceleratori forniscono. Il loro design per uso generico riduce il throughput computazionale per le operazioni matematiche altamente parallelizzabili comuni nei modelli ML <span class="citation" data-cites="jouppi2017datacenter">(<a href="../../../references.it.html#ref-jouppi2017datacenter" role="doc-biblioref">N. P. Jouppi et al. 2017a</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jouppi2017datacenter" class="csl-entry" role="listitem">
Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. <span>¬´In-Datacenter Performance Analysis of a Tensor Processing Unit¬ª</span>. In <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, 1‚Äì12. ISCA ‚Äô17. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3079856.3080246">https://doi.org/10.1145/3079856.3080246</a>.
</div></div></section>
<section id="non-ottimizzato-per-il-parallelismo-dei-dati" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="non-ottimizzato-per-il-parallelismo-dei-dati">Non Ottimizzato per il Parallelismo dei Dati</h5>
<p>Le architetture delle CPU non sono specificamente ottimizzate per i carichi di lavoro paralleli dei dati inerenti all‚ÄôAI <span class="citation" data-cites="sze2017efficient">(<a href="../../../references.it.html#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>. Assegnano un‚Äôarea di silicio sostanziale alla decodifica delle istruzioni, all‚Äôesecuzione speculativa, alla memorizzazione nella cache e al controllo del flusso che fornisce pochi vantaggi per le operazioni su array utilizzate nelle reti neurali (<a href="https://www.intel.com/content/www/us/en/developer/articles/technical/ai-inference-acceleration-on-intel-cpus.html#gs.0w9qn2">accelerazione dell‚Äôinferenza AI sulle CPU</a>). Tuttavia, le CPU moderne sono dotate di istruzioni vettoriali come <a href="https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/what-is-intel-avx-512.html">AVX-512</a> specificamente per accelerare determinate operazioni chiave come la moltiplicazione matriciale.</p>
<div class="no-row-height column-margin column-container"></div><p>I multiprocessori di streaming GPU, ad esempio, dedicano la maggior parte dei transistor alle unit√† a virgola mobile anzich√© alla logica di predizione di diramazione complessa. Questa specializzazione consente un utilizzo molto pi√π elevato per la matematica ML.</p>
</section>
<section id="maggiore-latenza-della-memoria" class="level5">
<h5 class="anchored" data-anchor-id="maggiore-latenza-della-memoria">Maggiore Latenza della Memoria</h5>
<p>Le CPU soffrono di una latenza maggiore nell‚Äôaccesso alla memoria principale rispetto alle GPU e ad altri acceleratori (<a href="https://www.integralmemory.com/articles/the-evolution-of-ddr-sdram/">DDR</a>). Tecniche come il tiling e il caching possono aiutare, ma la separazione fisica dalla RAM off-chip crea colli di bottiglia nei carichi di lavoro ML ad alta intensit√† di dati. Ci√≤ sottolinea la necessit√† di architetture di memoria specializzate nell‚Äôhardware ML.</p>
</section>
<section id="inefficienza-energetica-in-caso-di-carichi-di-lavoro-pesanti" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="inefficienza-energetica-in-caso-di-carichi-di-lavoro-pesanti">Inefficienza Energetica in Caso di Carichi di Lavoro Pesanti</h5>
<p>Sebbene sia adatto per l‚Äôinferenza intermittente, il mantenimento di una produttivit√† quasi di picco per l‚Äôaddestramento comporta un consumo energetico inefficiente sulle CPU, in particolare sulle CPU mobili <span class="citation" data-cites="ignatov2018ai">(<a href="../../../references.it.html#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2018</a>)</span>. Gli acceleratori ottimizzano esplicitamente il flusso di dati, la memoria e il calcolo per carichi di lavoro ML sostenuti. Le CPU sono inefficienti dal punto di vista energetico per l‚Äôaddestramento di modelli di grandi dimensioni.</p>
<div class="no-row-height column-margin column-container"></div></section>
</section>
</section>
<section id="confronto" class="level3" data-number="10.3.6">
<h3 data-number="10.3.6" class="anchored" data-anchor-id="confronto"><span class="header-section-number">10.3.6</span> Confronto</h3>
<p><a href="#tbl-accelerator-comparison" class="quarto-xref">Tabella&nbsp;<span>10.2</span></a> confronta i diversi tipi di funzionalit√† hardware.</p>
<div id="tbl-accelerator-comparison" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-accelerator-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;10.2: Confronto di diversi acceleratori hardware per carichi di lavoro AI.
</figcaption>
<div aria-describedby="tbl-accelerator-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 30%">
<col style="width: 30%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Acceleratore</th>
<th style="text-align: left;">Descrizione</th>
<th style="text-align: left;">Principali vantaggi</th>
<th style="text-align: left;">Principali svantaggi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">ASIC</td>
<td style="text-align: left;">IC personalizzati progettati per carichi di lavoro target come l‚Äôinferenza AI</td>
<td style="text-align: left;"><ul>
<li>Massimizza le prestazioni/watt.</li>
<li>Ottimizzato per le operazioni tensoriali</li>
<li>Memoria on-chip a bassa latenza</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>L‚Äôarchitettura fissa manca di flessibilit√†</li>
<li>Elevato costo NRE</li>
<li>Lunghi cicli di progettazione</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;">FPGA</td>
<td style="text-align: left;">Fabric riconfigurabile con logica programmabile e routing</td>
<td style="text-align: left;"><ul>
<li>Architettura flessibile</li>
<li>Accesso alla memoria a bassa latenza</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Prestazioni/watt inferiori rispetto agli ASIC</li>
<li>Programmazione complessa</li>
</ul></td>
</tr>
<tr class="odd">
<td style="text-align: left;">GPU</td>
<td style="text-align: left;">Originariamente per la grafica, ora utilizzate per l‚Äôaccelerazione della rete neurale</td>
<td style="text-align: left;"><ul>
<li>Elevata produttivit√†</li>
<li>Scalabilit√† parallela</li>
<li>Ecosistema software con CUDA</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Non efficienti dal punto di vista energetico come gli ASIC</li>
<li>Richiede un‚Äôelevata larghezza di banda della memoria</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;">CPU</td>
<td style="text-align: left;">Processori per uso generico</td>
<td style="text-align: left;"><ul>
<li>Programmabilit√†</li>
<li>Disponibilit√† ubiqua</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Prestazioni inferiori per carichi di lavoro AI</li>
</ul></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In generale, le CPU forniscono una baseline prontamente disponibile, le GPU offrono un‚Äôaccelerazione ampiamente accessibile, gli FPGA offrono programmabilit√† e gli ASIC massimizzano l‚Äôefficienza per funzioni fisse. La scelta ottimale dipende dalla scala, dal costo, dalla flessibilit√† e da altri requisiti dell‚Äôapplicazione target.</p>
<p>Sebbene inizialmente sviluppati per l‚Äôimplementazione del data center, Google ha anche profuso notevoli sforzi nello sviluppo di <a href="https://cloud.google.com/edge-tpu">TPU Edge</a>. Questi TPU Edge mantengono l‚Äôispirazione degli array sistolici [https://it.wikipedia.org/wiki/Array_sistolico], ma sono adattati alle risorse limitate accessibili all‚Äôedge.</p>
</section>
</section>
<section id="co-progettazione-hardware-software" class="level2 page-columns page-full" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="co-progettazione-hardware-software"><span class="header-section-number">10.4</span> Co-Progettazione Hardware-Software</h2>
<p>La co-progettazione hardware-software si basa sul principio secondo cui i sistemi AI raggiungono prestazioni ed efficienza ottimali quando i componenti hardware e software sono progettati in stretta integrazione. Ci√≤ comporta un ciclo di progettazione iterativo e collaborativo in cui l‚Äôarchitettura hardware e gli algoritmi software vengono sviluppati e perfezionati contemporaneamente con un feedback continuo tra i team.</p>
<p>Ad esempio, un nuovo modello di rete neurale pu√≤ essere prototipato su una piattaforma di accelerazione basata su FPGA per ottenere dati sulle prestazioni reali all‚Äôinizio del processo di progettazione. Questi risultati forniscono un feedback ai progettisti hardware su potenziali ottimizzazioni e agli sviluppatori software su perfezionamenti del modello o framework per sfruttare meglio le capacit√† hardware. Questo livello di sinergia √® difficile da raggiungere con la pratica comune di software sviluppato in modo indipendente per essere distribuito su hardware fisso.</p>
<p>La progettazione congiunta √® fondamentale per i sistemi di intelligenza artificiale embedded che affrontano notevoli vincoli di risorse come budget di potenza ridotti, memoria e capacit√† di elaborazione limitate e requisiti di latenza in tempo reale. La stretta integrazione tra sviluppatori di algoritmi e architetti hardware aiuta a sbloccare le ottimizzazioni in tutto lo stack per soddisfare queste restrizioni. Le tecniche di abilitazione includono miglioramenti algoritmici come la ricerca e il pruning [potatura] dell‚Äôarchitettura neurale e progressi hardware come flussi di dati specializzati e gerarchie di memoria.</p>
<p>Riunendo la progettazione hardware e software, anzich√© svilupparli separatamente, √® possibile realizzare ottimizzazioni olistiche che massimizzano prestazioni ed efficienza. Le sezioni successive forniscono maggiori dettagli su specifici approcci di progettazione congiunta.</p>
<section id="la-necessit√†-della-progettazione-congiunta" class="level3 page-columns page-full" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="la-necessit√†-della-progettazione-congiunta"><span class="header-section-number">10.4.1</span> La Necessit√† della Progettazione Congiunta</h3>
<p>Diversi fattori chiave rendono essenziale un approccio di progettazione congiunta hardware-software collaborativo per la creazione di sistemi di intelligenza artificiale efficienti.</p>
<section id="aumento-delle-dimensioni-e-della-complessit√†-del-modello" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="aumento-delle-dimensioni-e-della-complessit√†-del-modello">Aumento delle Dimensioni e della Complessit√† del Modello</h4>
<p>I modelli di intelligenza artificiale all‚Äôavanguardia sono cresciuti rapidamente in termini di dimensioni, abilitati dai progressi nella progettazione dell‚Äôarchitettura neurale e dalla disponibilit√† di grandi set di dati. Ad esempio, il modello linguistico GPT-3 contiene 175 miliardi di parametri <span class="citation" data-cites="brown2020language">(<a href="../../../references.it.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>, che richiedono enormi risorse di calcolo per l‚Äôaddestramento. Questa esplosione nella complessit√† del modello richiede una progettazione congiunta per sviluppare hardware e algoritmi efficienti in tandem. Tecniche come la compressione del modello <span class="citation" data-cites="cheng2017survey">(<a href="../../../references.it.html#ref-cheng2017survey" role="doc-biblioref">Cheng et al. 2018</a>)</span> e la quantizzazione devono essere co-ottimizzate con l‚Äôarchitettura hardware.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>¬´Language Models are Few-Shot Learners¬ª</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div><div id="ref-cheng2017survey" class="csl-entry" role="listitem">
Cheng, Yu, Duo Wang, Pan Zhou, e Tao Zhang. 2018. <span>¬´Model Compression and Acceleration for Deep Neural Networks: <span>The</span> Principles, Progress, and Challenges¬ª</span>. <em>IEEE Signal Process Mag.</em> 35 (1): 126‚Äì36. <a href="https://doi.org/10.1109/msp.2017.2765695">https://doi.org/10.1109/msp.2017.2765695</a>.
</div></div></section>
<section id="vincoli-della-distribuzione-embedded" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="vincoli-della-distribuzione-embedded">Vincoli della Distribuzione Embedded</h4>
<p>L‚Äôimplementazione di applicazioni AI su dispositivi edge come telefoni cellulari o elettrodomestici intelligenti introduce vincoli significativi su energia, memoria e area di silicio <span class="citation" data-cites="sze2017efficient">(<a href="../../../references.it.html#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>. Abilitare l‚Äôinferenza in tempo reale con queste restrizioni richiede la co-esplorazione di ottimizzazioni hardware come flussi di dati specializzati e compressione con progettazione efficiente di reti neurali e tecniche di potatura. La co-progettazione massimizza le prestazioni entro rigidi vincoli di distribuzione.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="rapida-evoluzione-degli-algoritmi-ai" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="rapida-evoluzione-degli-algoritmi-ai">Rapida Evoluzione degli Algoritmi AI</h4>
<p>L‚Äôintelligenza artificiale si sta evolvendo rapidamente, con nuove architetture di modelli, metodologie di training e framework software che emergono costantemente. Ad esempio, i Transformers sono diventati di recente molto popolari per l‚ÄôNLP <span class="citation" data-cites="young2018recent">(<a href="../../../references.it.html#ref-young2018recent" role="doc-biblioref">Young et al. 2018</a>)</span>. Per tenere il passo con queste innovazioni algoritmiche √® necessaria una progettazione congiunta hardware-software per adattare le piattaforme ed evitare rapidamente il debito tecnico accumulato.</p>
<div class="no-row-height column-margin column-container"><div id="ref-young2018recent" class="csl-entry" role="listitem">
Young, Tom, Devamanyu Hazarika, Soujanya Poria, e Erik Cambria. 2018. <span>¬´Recent Trends in Deep Learning Based Natural Language Processing <span>[Review</span> Article]¬ª</span>. <em>IEEE Comput. Intell. Mag.</em> 13 (3): 55‚Äì75. <a href="https://doi.org/10.1109/mci.2018.2840738">https://doi.org/10.1109/mci.2018.2840738</a>.
</div></div></section>
<section id="interazioni-complesse-hardware-software" class="level4">
<h4 class="anchored" data-anchor-id="interazioni-complesse-hardware-software">Interazioni Complesse Hardware-Software</h4>
<p>Molte interazioni e compromessi sottili tra scelte architettoniche hardware e ottimizzazioni software hanno un impatto significativo sull‚Äôefficienza complessiva. Ad esempio, tecniche come il partizionamento tensoriale e il batching influenzano il parallelismo e i pattern di accesso ai dati influenzano l‚Äôutilizzo della memoria. La progettazione congiunta fornisce una prospettiva multilivello per svelare queste dipendenze.</p>
</section>
<section id="necessit√†-di-specializzazione" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="necessit√†-di-specializzazione">Necessit√† di Specializzazione</h4>
<p>I carichi di lavoro dell‚Äôintelligenza artificiale traggono vantaggio da operazioni specializzate come matematica a bassa precisione e gerarchie di memoria personalizzate. Ci√≤ motiva l‚Äôincorporazione di hardware personalizzato su misura per algoritmi di reti neurali piuttosto che affidarsi esclusivamente a software flessibile in esecuzione su hardware generico <span class="citation" data-cites="sze2017efficient">(<a href="../../../references.it.html#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>. Tuttavia, lo stack software deve mirare esplicitamente alle operazioni hardware personalizzate per realizzare i vantaggi.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="richiesta-di-maggiore-efficienza" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="richiesta-di-maggiore-efficienza">Richiesta di Maggiore Efficienza</h4>
<p>Con la crescente complessit√† del modello, si verificano rendimenti decrescenti e spese generali derivanti dall‚Äôottimizzazione del solo hardware o software in isolamento <span class="citation" data-cites="putnam2014reconfigurable">(<a href="../../../references.it.html#ref-putnam2014reconfigurable" role="doc-biblioref">Putnam et al. 2014</a>)</span>. Si presentano inevitabili compromessi che richiedono un‚Äôottimizzazione globale su pi√π livelli. La progettazione congiunta di hardware e software fornisce grandi guadagni di efficienza composti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-putnam2014reconfigurable" class="csl-entry" role="listitem">
Putnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. <span>¬´A reconfigurable fabric for accelerating large-scale datacenter services¬ª</span>. <em>ACM SIGARCH Computer Architecture News</em> 42 (3): 13‚Äì24. <a href="https://doi.org/10.1145/2678373.2665678">https://doi.org/10.1145/2678373.2665678</a>.
</div></div></section>
</section>
<section id="principi-di-progettazione-congiunta-hardware-software" class="level3 page-columns page-full" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="principi-di-progettazione-congiunta-hardware-software"><span class="header-section-number">10.4.2</span> Principi di Progettazione Congiunta Hardware-Software</h3>
<p>L‚Äôarchitettura hardware e lo stack software devono essere strettamente integrati e co-ottimizzati per creare sistemi di intelligenza artificiale efficienti e ad alte prestazioni. Nessuno dei due pu√≤ essere progettato in isolamento; massimizzare le loro sinergie richiede un approccio olistico noto come progettazione congiunta hardware-software.</p>
<p>L‚Äôobiettivo principale √® adattare le capacit√† hardware in modo che corrispondano agli algoritmi e ai carichi di lavoro eseguiti dal software. Ci√≤ richiede un ciclo di feedback tra architetti hardware e sviluppatori software per convergere su soluzioni ottimizzate. Diverse tecniche consentono un‚Äôefficace co-progettazione:</p>
<section id="ottimizzazione-software-consapevole-dellhardware" class="level4">
<h4 class="anchored" data-anchor-id="ottimizzazione-software-consapevole-dellhardware">Ottimizzazione Software Consapevole dell‚ÄôHardware</h4>
<p>Lo stack software pu√≤ essere ottimizzato per sfruttare meglio le capacit√† hardware:</p>
<ul>
<li><strong>Parallelismo:</strong> Parallelizzare i calcoli matriciali come convoluzione o layer di attenzione per massimizzare la produttivit√† sui motori vettoriali.</li>
<li><strong>Ottimizzazione della Memoria:</strong> Ottimizzare i layout dei dati per migliorare la localit√† della cache in base alla profilazione hardware. Ci√≤ massimizza il riutilizzo e riduce al minimo l‚Äôaccesso DRAM costoso.</li>
<li><strong>Compressione:</strong> Utilizzare la sparsity [diradazione] nei modelli per ridurre lo spazio di archiviazione e risparmiare sui calcoli tramite operazioni di zero-skipping.</li>
<li><strong>Operazioni Personalizzate:</strong> Incorporare operazioni specializzate come INT4 a bassa precisione o bfloat16 nei modelli per sfruttare al meglio il supporto hardware dedicato.</li>
<li><strong>Mappatura del Flusso di Dati:</strong> Mappare esplicitamente le fasi del modello alle unit√† di calcolo per ottimizzare lo spostamento dei dati sull‚Äôhardware.</li>
</ul>
</section>
<section id="specializzazione-hardware-algorithm-driven" class="level4">
<h4 class="anchored" data-anchor-id="specializzazione-hardware-algorithm-driven">Specializzazione Hardware Algorithm-Driven</h4>
<p>L‚Äôhardware pu√≤ essere adattato alle caratteristiche degli algoritmi ML:</p>
<ul>
<li><strong>Tipi di Dati Personalizzati:</strong> Supportare INT8/4 o bfloat16 a bassa precisione nell‚Äôhardware per una maggiore densit√† aritmetica.</li>
<li><strong>Memoria su Chip:</strong> Aumentare la larghezza di banda SRAM e ridurre la latenza di accesso per adattarla ai pattern di accesso alla memoria del modello.</li>
<li><strong>Operazioni Specifiche del Dominio:</strong> Aggiungere unit√† hardware per funzioni ML chiave come FFT o moltiplicazione di matrici per ridurre latenza ed energia.</li>
<li><strong>Profilazione del Modello:</strong> Utilizzare la simulazione e la profilazione del modello per identificare hotspot computazionali e ottimizzare l‚Äôhardware.</li>
</ul>
<p>La chiave √® il feedback collaborativo: le informazioni dalla profilazione dell‚Äôhardware guidano le ottimizzazioni del software, mentre i progressi algoritmici informano la specializzazione dell‚Äôhardware. Questo miglioramento reciproco fornisce guadagni di efficienza moltiplicativa rispetto agli sforzi isolati.</p>
</section>
<section id="co-esplorazione-algoritmo-hardware" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="co-esplorazione-algoritmo-hardware">Co-esplorazione Algoritmo-Hardware</h4>
<p>Una potente tecnica di co-progettazione prevede l‚Äôesplorazione congiunta di innovazioni nelle architetture di reti neurali e nella progettazione custom dell‚Äôhardware. A powerful co-design technique involves jointly exploring innovations in neural network architectures and custom hardware design. Ci√≤ consente di trovare abbinamenti ideali su misura per i rispettivi punti di forza <span class="citation" data-cites="sze2017efficient">(<a href="../../../references.it.html#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sze2017efficient" class="csl-entry" role="listitem">
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, e Joel S. Emer. 2017. <span>¬´Efficient Processing of Deep Neural Networks: <span>A</span> Tutorial and Survey¬ª</span>. <em>Proc. IEEE</em> 105 (12): 2295‚Äì2329. <a href="https://doi.org/10.1109/jproc.2017.2761740">https://doi.org/10.1109/jproc.2017.2761740</a>.
</div><div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. <span>¬´<span>MobileNets:</span> <span>Efficient</span> Convolutional Neural Networks for Mobile Vision Applications¬ª</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a>.
</div><div id="ref-jacob2018quantization" class="csl-entry" role="listitem">
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. <span>¬´Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference¬ª</span>. In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2704‚Äì13. IEEE. <a href="https://doi.org/10.1109/cvpr.2018.00286">https://doi.org/10.1109/cvpr.2018.00286</a>.
</div><div id="ref-gale2019state" class="csl-entry" role="listitem">
Gale, Trevor, Erich Elsen, e Sara Hooker. 2019. <span>¬´The state of sparsity in deep neural networks¬ª</span>. <em>ArXiv preprint</em> abs/1902.09574. <a href="https://arxiv.org/abs/1902.09574">https://arxiv.org/abs/1902.09574</a>.
</div><div id="ref-asit2021accelerating" class="csl-entry" role="listitem">
Mishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, e Paulius Micikevicius. 2021. <span>¬´Accelerating Sparse Deep Neural Networks¬ª</span>. <em>CoRR</em> abs/2104.08378. <a href="https://arxiv.org/abs/2104.08378">https://arxiv.org/abs/2104.08378</a>.
</div></div><p>Ad esempio, il passaggio ad architetture mobili come MobileNets <span class="citation" data-cites="howard2017mobilenets">(<a href="../../../references.it.html#ref-howard2017mobilenets" role="doc-biblioref">Howard et al. 2017</a>)</span> √® stato guidato dai vincoli dei dispositivi edge come dimensioni del modello e latenza. La quantizzazione <span class="citation" data-cites="jacob2018quantization">(<a href="../../../references.it.html#ref-jacob2018quantization" role="doc-biblioref">Jacob et al. 2018</a>)</span> e le tecniche di pruning [potatura] <span class="citation" data-cites="gale2019state">(<a href="../../../references.it.html#ref-gale2019state" role="doc-biblioref">Gale, Elsen, e Hooker 2019</a>)</span> che hanno reso questi modelli efficienti sono diventate possibili grazie ad acceleratori hardware con supporto nativo per interi a bassa precisione e supporto per potatura <span class="citation" data-cites="asit2021accelerating">(<a href="../../../references.it.html#ref-asit2021accelerating" role="doc-biblioref">Mishra et al. 2021</a>)</span>.</p>
<p>I modelli basati sull‚Äôattenzione hanno prosperato su GPU e ASIC massivamente paralleli, dove il loro calcolo si mappa bene nello spazialmente, al contrario delle architetture RNN, che si basano sull‚Äôelaborazione sequenziale. La co-evoluzione di algoritmi e hardware ha evidenziato nuove capacit√†.</p>
<p>Una co-esplorazione efficace richiede una stretta collaborazione tra ricercatori di algoritmi e architetti hardware. La prototipazione rapida su FPGA <span class="citation" data-cites="zhang2015fpga">(<a href="../../../references.it.html#ref-zhang2015fpga" role="doc-biblioref">C. Zhang et al. 2015</a>)</span> o simulatori di intelligenza artificiale specializzati consente una rapida valutazione di diverse coppie di architetture di modelli e progetti hardware pre-silicio.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2015fpga" class="csl-entry" role="listitem">
Zhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, e Jason Optimizing Cong. 2015. <span>¬´<span>FPGA</span>-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 <span>ACM</span>¬ª</span>. In <em>SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA</em>, 15:161‚Äì70.
</div></div><p>Ad esempio, l‚Äôarchitettura TPU di Google si √® evoluta con ottimizzazioni verso i modelli TensorFlow per massimizzare le prestazioni sulla classificazione delle immagini. Questo stretto ciclo di feedback ha prodotto modelli su misura per la TPU che sarebbero stati improbabili in isolamento.</p>
<p>Gli studi hanno mostrato guadagni di prestazioni ed efficienza da 2 a 5 volte superiori con la co-esplorazione algoritmo-hardware rispetto agli sforzi isolati di ottimizzazione di algoritmi o hardware <span class="citation" data-cites="suda2016throughput">(<a href="../../../references.it.html#ref-suda2016throughput" role="doc-biblioref">Suda et al. 2016</a>)</span>. Parallelizzare lo sviluppo congiunto riduce anche i ‚Äútime-to-deployment‚Äù [tempi di distribuzione].</p>
<div class="no-row-height column-margin column-container"><div id="ref-suda2016throughput" class="csl-entry" role="listitem">
Suda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma, Sarma Vrudhula, Jae-sun Seo, e Yu Cao. 2016. <span>¬´Throughput-Optimized <span>OpenCL</span>-based <span>FPGA</span> Accelerator for Large-Scale Convolutional Neural Networks¬ª</span>. In <em>Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</em>, 16‚Äì25. ACM. <a href="https://doi.org/10.1145/2847263.2847276">https://doi.org/10.1145/2847263.2847276</a>.
</div></div><p>Nel complesso, esplorare le strette interdipendenze tra innovazione del modello e progressi hardware crea opportunit√† che devono essere visibili quando vengono affrontate in sequenza. Questa progettazione sinergica congiunta produce soluzioni maggiori della somma delle loro parti.</p>
</section>
</section>
<section id="sfide" class="level3 page-columns page-full" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="sfide"><span class="header-section-number">10.4.3</span> Sfide</h3>
<p>Sebbene la progettazione collaborativa possa migliorare l‚Äôefficienza, l‚Äôadattabilit√† e il time-to-market, presenta anche sfide ingegneristiche e organizzative.</p>
<section id="aumento-dei-costi-di-prototipazione" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="aumento-dei-costi-di-prototipazione">Aumento dei Costi di Prototipazione</h4>
<p>√à richiesta una prototipazione pi√π estesa per valutare diverse accoppiate hardware-software. La necessit√† di prototipi rapidi e iterativi su FPGA o emulatori aumenta il sovraccarico della validazione. Ad esempio, Microsoft ha scoperto che erano necessari pi√π prototipi per la progettazione collaborativa di un acceleratore AI rispetto alla progettazione sequenziale <span class="citation" data-cites="fowers2018configurable">(<a href="../../../references.it.html#ref-fowers2018configurable" role="doc-biblioref">Fowers et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-fowers2018configurable" class="csl-entry" role="listitem">
Fowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. <span>¬´A Configurable Cloud-Scale <span>DNN</span> Processor for Real-Time <span>AI</span>¬ª</span>. In <em>2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</em>, 1‚Äì14. IEEE; IEEE. <a href="https://doi.org/10.1109/isca.2018.00012">https://doi.org/10.1109/isca.2018.00012</a>.
</div></div></section>
<section id="ostacoli-organizzativi-e-di-team" class="level4">
<h4 class="anchored" data-anchor-id="ostacoli-organizzativi-e-di-team">Ostacoli Organizzativi e di Team</h4>
<p>La progettazione collaborativa richiede uno stretto coordinamento tra gruppi hardware e software tradizionalmente scollegati. Ci√≤ potrebbe causare problemi di comunicazione o priorit√† e pianificazioni non allineate. Anche la navigazione di diversi flussi di lavoro di progettazione √® impegnativa. Potrebbe esistere una certa inerzia organizzativa nell‚Äôadottare pratiche integrate.</p>
</section>
<section id="complessit√†-di-simulazione-e-modellazione" class="level4">
<h4 class="anchored" data-anchor-id="complessit√†-di-simulazione-e-modellazione">Complessit√† di Simulazione e Modellazione</h4>
<p>Catturare interazioni sottili tra layer hardware e software per la simulazione e la modellazione congiunte aggiunge una complessit√† significativa. Le astrazioni complete ‚Äúcross-layer‚Äù sono difficili da costruire quantitativamente prima dell‚Äôimplementazione, rendendo pi√π difficile quantificare in anticipo le ottimizzazioni olistiche.</p>
</section>
<section id="rischi-di-eccessiva-specializzazione" class="level4">
<h4 class="anchored" data-anchor-id="rischi-di-eccessiva-specializzazione">Rischi di Eccessiva Specializzazione</h4>
<p>Una progettazione congiunta rigorosa comporta il rischio di adattare eccessivamente le ottimizzazioni agli algoritmi correnti, sacrificando la generalit√†. Ad esempio, l‚Äôhardware ottimizzato esclusivamente per i modelli Transformer potrebbe avere prestazioni inferiori con le tecniche future. Mantenere la flessibilit√† richiede lungimiranza.</p>
</section>
<section id="problemi-sui-cambiamenti" class="level4">
<h4 class="anchored" data-anchor-id="problemi-sui-cambiamenti">Problemi sui Cambiamenti</h4>
<p>Gli ingegneri che hanno familiarit√† con le consolidate pratiche di progettazione hardware o software discrete potrebbero accettare solo flussi di lavoro collaborativi familiari. Nonostante i vantaggi a lungo termine, i progetti potrebbero incontrare attriti nella transizione alla progettazione congiunta.</p>
</section>
</section>
</section>
<section id="software-per-hardware-ai" class="level2 page-columns page-full" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="software-per-hardware-ai"><span class="header-section-number">10.5</span> Software per Hardware AI</h2>
<p>Acceleratori hardware specializzati come GPU, TPU e FPGA sono essenziali per fornire applicazioni di intelligenza artificiale ad alte prestazioni. Tuttavia, √® necessario un ampio stack software per sfruttare efficacemente queste piattaforme hardware, che coprano l‚Äôintero ciclo di vita di sviluppo e distribuzione. Framework e librerie costituiscono la spina dorsale dell‚Äôhardware AI, offrendo set di codice, algoritmi e funzioni pre-costruiti e robusti, specificamente ottimizzati per eseguire varie attivit√† AI su hardware diversi. Sono progettati per semplificare le complessit√† dell‚Äôutilizzo dell‚Äôhardware da zero, che pu√≤ richiedere molto tempo ed essere soggetto a errori. Il software svolge un ruolo importante:</p>
<ul>
<li>Fornendo astrazioni di programmazione e modelli come CUDA e OpenCL per mappare i calcoli sugli acceleratori.</li>
<li>Integrando gli acceleratori in framework di deep learning popolari come TensorFlow e PyTorch.</li>
<li>Ottimizzando l‚Äôintero stack hardware-software con compilatori e tool.</li>
<li>Con piattaforme di simulazione per modellare insieme hardware e software.</li>
<li>Con l‚Äôinfrastruttura per gestire la distribuzione sugli acceleratori.</li>
</ul>
<p>Questo vasto ecosistema software √® importante quanto l‚Äôhardware nel fornire applicazioni AI performanti ed efficienti. Questa sezione fornisce una panoramica degli strumenti disponibili a ogni livello dello stack per consentire agli sviluppatori di creare ed eseguire sistemi AI basati sull‚Äôaccelerazione hardware.</p>
<section id="sec-programming-models" class="level3 page-columns page-full" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="sec-programming-models"><span class="header-section-number">10.5.1</span> Modelli di Programmazione</h3>
<p>I modelli di programmazione forniscono astrazioni per mappare calcoli e dati su acceleratori hardware eterogenei:</p>
<ul>
<li><strong><a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a>:</strong> Modello di programmazione parallela di Nvidia per sfruttare le GPU utilizzando estensioni a linguaggi come C/C++. Consente di avviare kernel su core GPU <span class="citation" data-cites="luebke2008cuda">(<a href="../../../references.it.html#ref-luebke2008cuda" role="doc-biblioref">Luebke 2008</a>)</span>.</li>
<li><strong><a href="https://www.khronos.org/opencl/">OpenCL</a>:</strong> Standard aperto per scrivere programmi che spaziano tra CPU, GPU, FPGA e altri acceleratori. Specifica un framework di elaborazione eterogeneo <span class="citation" data-cites="munshi2009opencl">(<a href="../../../references.it.html#ref-munshi2009opencl" role="doc-biblioref">Munshi 2009</a>)</span>.</li>
<li><strong><a href="https://www.opengl.org">OpenGL/WebGL</a>:</strong> Interfacce di programmazione grafica 3D in grado di mappare codice generico su core GPU <span class="citation" data-cites="segal1999opengl">(<a href="../../../references.it.html#ref-segal1999opengl" role="doc-biblioref">Segal e Akeley 1999</a>)</span>.</li>
<li><strong><a href="https://www.verilog.com">Verilog</a>/VHDL:</strong> ‚ÄúHardware description languages (HDL)‚Äù [Linguaggi di descrizione hardware] utilizzati per configurare FPGA come acceleratori AI specificando circuiti digitali <span class="citation" data-cites="gannot1994verilog">(<a href="../../../references.it.html#ref-gannot1994verilog" role="doc-biblioref">Gannot e Ligthart 1994</a>)</span>.</li>
<li><strong><a href="https://tvm.apache.org">TVM</a>:</strong> Un framework di compilazione che fornisce un frontend Python per ottimizzare e mappare modelli di deep learning su diversi backend hardware <span class="citation" data-cites="chen2018tvm">(<a href="../../../references.it.html#ref-chen2018tvm" role="doc-biblioref">Chen et al. 2018</a>)</span>.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-luebke2008cuda" class="csl-entry" role="listitem">
Luebke, David. 2008. <span>¬´<span>CUDA:</span> <span>Scalable</span> parallel programming for high-performance scientific computing¬ª</span>. In <em>2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro</em>, 836‚Äì38. IEEE. <a href="https://doi.org/10.1109/isbi.2008.4541126">https://doi.org/10.1109/isbi.2008.4541126</a>.
</div><div id="ref-munshi2009opencl" class="csl-entry" role="listitem">
Munshi, Aaftab. 2009. <span>¬´The <span>OpenCL</span> specification¬ª</span>. In <em>2009 IEEE Hot Chips 21 Symposium (HCS)</em>, 1‚Äì314. IEEE. <a href="https://doi.org/10.1109/hotchips.2009.7478342">https://doi.org/10.1109/hotchips.2009.7478342</a>.
</div><div id="ref-segal1999opengl" class="csl-entry" role="listitem">
Segal, Mark, e Kurt Akeley. 1999. <span>¬´The <span>OpenGL</span> graphics system: <span>A</span> specification (version 1.1)¬ª</span>.
</div><div id="ref-gannot1994verilog" class="csl-entry" role="listitem">
Gannot, G., e M. Ligthart. 1994. <span>¬´Verilog <span>HDL</span> based <span>FPGA</span> design¬ª</span>. In <em>International Verilog HDL Conference</em>, 86‚Äì92. IEEE. <a href="https://doi.org/10.1109/ivc.1994.323743">https://doi.org/10.1109/ivc.1994.323743</a>.
</div><div id="ref-chen2018tvm" class="csl-entry" role="listitem">
Chen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. <span>¬´<span>TVM:</span> <span>An</span> automated End-to-End optimizing compiler for deep learning¬ª</span>. In <em>13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</em>, 578‚Äì94.
</div></div><p>Le sfide principali includono l‚Äôespressione del parallelismo, la gestione della memoria tra dispositivi e l‚Äôabbinamento di algoritmi alle capacit√† hardware. Le astrazioni devono bilanciare la portabilit√† con la possibilit√† di personalizzazione hardware. I modelli di programmazione consentono agli sviluppatori di sfruttare gli acceleratori senza competenze hardware. Questi dettagli sono discussi nella sezione <a href="../../../contents/core/frameworks/frameworks.it.html">AI frameworks</a> section.</p>
<div id="exr-tvm" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;10.1: Software per hardware AI - TVM
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Abbiamo imparato che l‚Äôhardware AI sofisticato ha bisogno di un software speciale per fare magie. TVM √® come un traduttore super intelligente, che trasforma il codice in istruzioni che gli acceleratori capiscono. In questo Colab, useremo TVM per creare un acceleratore finto chiamato VTA che esegue la moltiplicazione di matrici super velocemente. Pronti a vedere come il software alimenta l‚Äôhardware?</p>
<p><a href="https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/04a_TVM_Tutorial_VTA_Mat_Mult.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="librerie-e-runtime" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="librerie-e-runtime"><span class="header-section-number">10.5.2</span> Librerie e Runtime</h3>
<p>Librerie e runtime specializzati forniscono astrazioni software per accedere e massimizzare l‚Äôutilizzo degli acceleratori AI:</p>
<ul>
<li><strong>Librerie Matematiche:</strong> Implementazioni altamente ottimizzate di primitive di algebra lineare come GEMM, FFT, convoluzioni, ecc., su misura per l‚Äôhardware target. <a href="https://developer.nvidia.com/cublas">Nvidia cuBLAS</a>, <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html">Intel MKL</a> e <a href="https://www.arm.com/technologies/compute-library">librerie di elaborazione Arm</a> sono esempi.</li>
<li><strong>Integrazioni di Framework:</strong> Librerie per accelerare framework di deep learning come TensorFlow, PyTorch e MXNet su hardware supportato. Ad esempio, <a href="https://developer.nvidia.com/cudnn">cuDNN</a> accelera le CNN sulle GPU Nvidia.</li>
<li><strong>Runtime:</strong> Software per gestire l‚Äôesecuzione dell‚Äôacceleratore, tra cui pianificazione, sincronizzazione, gestione della memoria e altre attivit√†. <a href="https://developer.nvidia.com/tensorrt">Nvidia TensorRT</a> √® un ottimizzatore di inferenza e runtime.</li>
<li><strong>Driver e firmware:</strong> Software di basso livello per interfacciarsi con l‚Äôhardware, inizializzare i dispositivi e gestire l‚Äôesecuzione. Fornitori come Xilinx forniscono driver per le loro schede acceleratrici.</li>
</ul>
<p>Ad esempio, gli integratori PyTorch utilizzano librerie cuDNN e cuBLAS per accelerare l‚Äôaddestramento sulle GPU Nvidia. Il runtime TensorFlow XLA ottimizza e compila modelli per acceleratori come le TPU. I driver inizializzano i dispositivi e delegano le operazioni.</p>
<p>Le sfide includono il partizionamento e la pianificazione efficienti dei carichi di lavoro su dispositivi eterogenei come nodi multi-GPU. I runtime devono anche ridurre al minimo il sovraccarico dei trasferimenti di dati e della sincronizzazione.</p>
<p>Librerie, runtime e driver forniscono i mattoni ottimizzati che gli sviluppatori di deep learning possono sfruttare per le prestazioni dell‚Äôacceleratore senza competenze di programmazione hardware. La loro ottimizzazione √® essenziale per le distribuzioni.</p>
</section>
<section id="ottimizzazione-dei-compilatori" class="level3" data-number="10.5.3">
<h3 data-number="10.5.3" class="anchored" data-anchor-id="ottimizzazione-dei-compilatori"><span class="header-section-number">10.5.3</span> Ottimizzazione dei Compilatori</h3>
<p>L‚Äôottimizzazione dei compilatori √® fondamentale per estrarre le massime prestazioni ed efficienza dagli acceleratori hardware per i carichi di lavoro AI. Applicano ottimizzazioni che spaziano tra modifiche algoritmiche, trasformazioni a livello di grafico e generazione di codice di basso livello.</p>
<ul>
<li><strong>Ottimizzazione degli Algoritmi:</strong> Tecniche come quantizzazione, potatura e ricerca di architettura neurale per migliorare l‚Äôefficienza del modello e abbinare le capacit√† hardware.</li>
<li><strong>Ottimizzazioni dei Grafi:</strong> Ottimizzazioni a livello di grafo come fusione degli operatori, riscrittura e trasformazioni di layout per ottimizzare le prestazioni sull‚Äôhardware target.</li>
<li><strong>Generazione di Codice:</strong> Generazione di codice di basso livello ottimizzato per acceleratori da modelli e framework di alto livello.</li>
</ul>
<p>Ad esempio, lo stack di compilatori ‚Äúopen‚Äù TVM applica la quantizzazione per un modello BERT che ha come target le GPU Arm. Fonde le operazioni di convoluzione puntuale e trasforma il layout dei pesi per ottimizzare l‚Äôaccesso alla memoria. Infine, emette codice OpenGL ottimizzato per eseguire il carico di lavoro GPU.</p>
<p>Le ottimizzazioni chiave del compilatore includono la massimizzazione del parallelismo, il miglioramento della localit√† e del riutilizzo dei dati, la riduzione al minimo dell‚Äôingombro della memoria e lo sfruttamento delle operazioni hardware personalizzate. I compilatori creano e ottimizzano i carichi di lavoro di machine learning in modo olistico su componenti hardware come CPU, GPU e altri acceleratori.</p>
<p>Tuttavia, la mappatura efficiente di modelli complessi introduce sfide come il partizionamento efficiente dei carichi di lavoro su dispositivi eterogenei. I compilatori a livello di produzione richiedono anche molto tempo per la messa a punto su carichi di lavoro rappresentativi. Tuttavia, l‚Äôottimizzazione dei compilatori √® per sfruttare tutte le capacit√† degli acceleratori AI.</p>
</section>
<section id="simulazione-e-modellazione" class="level3 page-columns page-full" data-number="10.5.4">
<h3 data-number="10.5.4" class="anchored" data-anchor-id="simulazione-e-modellazione"><span class="header-section-number">10.5.4</span> Simulazione e Modellazione</h3>
<p>Il software di simulazione √® importante nella progettazione congiunta hardware-software. Consente accoppiare la modellazione di architetture hardware e stack software proposti:</p>
<ul>
<li><strong>Simulazione Hardware:</strong> Piattaforme come <a href="https://www.gem5.org">Gem5</a> consentono la simulazione dettagliata di componenti hardware come pipeline, cache, interconnessioni e gerarchie di memoria. Gli ingegneri possono modellare le modifiche hardware senza prototipazione fisica <span class="citation" data-cites="binkert2011gem5">(<a href="../../../references.it.html#ref-binkert2011gem5" role="doc-biblioref">Binkert et al. 2011</a>)</span>.</li>
<li><strong>Simulazione Software:</strong> Stack di compilatori come <a href="https://tvm.apache.org">TVM</a> supportano la simulazione di carichi di lavoro di machine learning per stimare le prestazioni sulle architetture hardware target. Questo aiuta con le ottimizzazioni software.</li>
<li><strong>Co-simulazione:</strong> Piattaforme unificate come SCALE-Sim <span class="citation" data-cites="samajdar2018scale">(<a href="../../../references.it.html#ref-samajdar2018scale" role="doc-biblioref">Samajdar et al. 2018</a>)</span> integrano la simulazione hardware e software in un unico strumento. Ci√≤ consente un‚Äôanalisi ‚Äúwhat-if‚Äù per quantificare gli impatti a livello di sistema delle ottimizzazioni cross-layer all‚Äôinizio del ciclo di progettazione.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-binkert2011gem5" class="csl-entry" role="listitem">
Binkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. <span>¬´The gem5 simulator¬ª</span>. <em>ACM SIGARCH Computer Architecture News</em> 39 (2): 1‚Äì7. <a href="https://doi.org/10.1145/2024716.2024718">https://doi.org/10.1145/2024716.2024718</a>.
</div><div id="ref-samajdar2018scale" class="csl-entry" role="listitem">
Samajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, e Tushar Krishna. 2018. <span>¬´Scale-sim: <span>Systolic</span> cnn accelerator simulator¬ª</span>. <em>ArXiv preprint</em> abs/1811.02883. <a href="https://arxiv.org/abs/1811.02883">https://arxiv.org/abs/1811.02883</a>.
</div></div><p>Ad esempio, un progetto di acceleratore AI basato su FPGA potrebbe essere simulato utilizzando il linguaggio di descrizione hardware Verilog e sintetizzato in un modello Gem5. Verilog √® adatto per descrivere la logica digitale e le interconnessioni dell‚Äôarchitettura dell‚Äôacceleratore. Verilog consente al progettista di specificare i datapath [percorsi dati], la logica di controllo, le memorie on-chip e altri componenti implementati nella struttura FPGA. Una volta completato il progetto Verilog, pu√≤ essere sintetizzato in un modello che simula il comportamento dell‚Äôhardware, ad esempio utilizzando il simulatore Gem5. Gem5 √® utile per questa attivit√† perch√© consente la modellazione di sistemi completi, inclusi processori, cache, bus e acceleratori personalizzati. Gem5 supporta l‚Äôinterfacciamento dei modelli Verilog dell‚Äôhardware alla simulazione, consentendo la modellazione unificata del sistema.</p>
<p>Il modello di acceleratore FPGA sintetizzato potrebbe quindi avere carichi di lavoro ML simulati utilizzando TVM compilato su di esso all‚Äôinterno dell‚Äôambiente Gem5 per una modellazione unificata. TVM consente la compilazione ottimizzata di modelli di ML su hardware eterogeneo come FPGA. L‚Äôesecuzione di carichi di lavoro compilati con TVM sull‚Äôacceleratore all‚Äôinterno della simulazione Gem5 fornisce un modo integrato per convalidare e perfezionare la progettazione hardware, lo stack software e l‚Äôintegrazione di sistema prima di realizzare fisicamente l‚Äôacceleratore su un FPGA reale.</p>
<p>Questo tipo di co-simulazione fornisce stime di metriche complessive come throughput, latenza e potenza per guidare la progettazione congiunta prima della costosa prototipazione fisica. Aiutano anche con le ottimizzazioni di partizionamento tra hardware e software per guidare i compromessi di progettazione.</p>
<p>Tuttavia, la precisione nella modellazione di interazioni sottili di basso livello tra componenti √® limitata. Le simulazioni quantificate sono stime ma non possono sostituire completamente i prototipi fisici e i test. Tuttavia, la simulazione e la modellazione unificate forniscono preziose informazioni iniziali sulle opportunit√† di ottimizzazione a livello di sistema durante il processo di co-progettazione.</p>
</section>
</section>
<section id="benchmarking-dellhardware-ai" class="level2 page-columns page-full" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="benchmarking-dellhardware-ai"><span class="header-section-number">10.6</span> Benchmarking dell‚ÄôHardware AI</h2>
<p>Il benchmarking √® un processo critico che quantifica e confronta le prestazioni di varie piattaforme hardware progettate per accelerare le applicazioni di intelligenza artificiale. Guida le decisioni di acquisto, l‚Äôattenzione allo sviluppo e gli sforzi di ottimizzazione delle prestazioni per i produttori di hardware e gli sviluppatori di software.</p>
<p>Il <a href="../../../contents/core/benchmarking/benchmarking.it.html">capitolo sul benchmarking</a> esplora questo argomento in modo molto dettagliato, spiegando perch√© √® diventato una parte indispensabile del ciclo di sviluppo dell‚Äôhardware AI e come influisce sul pi√π ampio panorama tecnologico. Qui, esamineremo brevemente i concetti principali, ma consigliamo di fare riferimento al capitolo per maggiori dettagli.</p>
<p>Suite di benchmarking come MLPerf, Fathom e AI Benchmark offrono una serie di test standardizzati utilizzabili su diverse piattaforme hardware. Queste suite misurano le prestazioni dell‚Äôacceleratore AI su varie reti neurali e attivit√† di apprendimento automatico, dalla classificazione di immagini di base all‚Äôelaborazione complessa del linguaggio. Fornendo un terreno comune per il confronto, aiutano a garantire che le dichiarazioni sulle prestazioni siano coerenti e verificabili. Questi ‚Äútool‚Äù vengono applicati non solo per guidare lo sviluppo dell‚Äôhardware, ma anche per garantire che lo stack software sfrutti appieno il potenziale dell‚Äôarchitettura sottostante.</p>
<ul>
<li><strong>MLPerf:</strong> Include un ampio set di benchmark che coprono sia l‚Äôaddestramento <span class="citation" data-cites="mattson2020mlperf">(<a href="../../../references.it.html#ref-mattson2020mlperf" role="doc-biblioref">Mattson et al. 2020</a>)</span> che l‚Äôinferenza <span class="citation" data-cites="reddi2020mlperf">(<a href="../../../references.it.html#ref-reddi2020mlperf" role="doc-biblioref">Reddi et al. 2020</a>)</span> per una gamma di attivit√† di machine learning. <a href="../benchmarking/benchmarking.it.html#fig-ml-perf" class="quarto-xref">Figura&nbsp;<span>11.5</span></a> illustra la diversit√† dei casi d‚Äôuso dell‚ÄôIA trattati da MLPerf.</li>
<li><strong>Fathom:</strong> Si concentra sulle operazioni principali nei modelli di deep learning, enfatizzandone l‚Äôesecuzione su diverse architetture <span class="citation" data-cites="adolf2016fathom">(<a href="../../../references.it.html#ref-adolf2016fathom" role="doc-biblioref">Adolf et al. 2016</a>)</span>.</li>
<li><strong>AI Benchmark:</strong> Mira a dispositivi mobili e consumer, valutando le prestazioni dell‚ÄôIA nelle applicazioni per utenti finali <span class="citation" data-cites="ignatov2018ai">(<a href="../../../references.it.html#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2018</a>)</span>.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-mattson2020mlperf" class="csl-entry" role="listitem">
Mattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020. <span>¬´<span>MLPerf:</span> <span>An</span> Industry Standard Benchmark Suite for Machine Learning Performance¬ª</span>. <em>IEEE Micro</em> 40 (2): 8‚Äì16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div><div id="ref-reddi2020mlperf" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. <span>¬´<span>MLPerf</span> Inference Benchmark¬ª</span>. In <em>2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>, 446‚Äì59. IEEE; IEEE. <a href="https://doi.org/10.1109/isca45697.2020.00045">https://doi.org/10.1109/isca45697.2020.00045</a>.
</div><div id="ref-adolf2016fathom" class="csl-entry" role="listitem">
Adolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. <span>¬´Fathom: <span>Reference</span> workloads for modern deep learning methods¬ª</span>. In <em>2016 IEEE International Symposium on Workload Characterization (IISWC)</em>, 1‚Äì10. IEEE; IEEE. <a href="https://doi.org/10.1109/iiswc.2016.7581275">https://doi.org/10.1109/iiswc.2016.7581275</a>.
</div><div id="ref-ignatov2018ai" class="csl-entry" role="listitem">
Ignatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, e Luc Van Gool. 2018. <span>¬´<span>AI</span> Benchmark: <span>Running</span> deep neural networks on Android smartphones¬ª</span>, 0‚Äì0.
</div></div><div id="fig-ml-perf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlperf.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.4: MLPerf Training v3.0 e suoi utilizzi. Fonte: <a href="https://www.forbes.com/sites/stevemcdowell/2023/06/27/nvidia-h100-dominates-new-mlperf-v30-benchmark-results/">Forbes</a>
</figcaption>
</figure>
</div>
<p>I benchmark hanno anche metriche delle prestazioni che sono misure quantificabili utilizzate per valutare l‚Äôefficacia degli acceleratori di IA. Queste metriche forniscono una visione completa delle capacit√† di un acceleratore e vengono utilizzate per guidare il processo di progettazione e selezione per i sistemi di IA. Le metriche comuni comprendono:</p>
<ul>
<li><strong>Throughput:</strong> Solitamente misurato in operazioni al secondo, questo parametro indica il volume di calcoli che un acceleratore pu√≤ gestire.</li>
<li><strong>Latenza:</strong> Il ritardo temporale tra input e output in un sistema √® fondamentale per le attivit√† di elaborazione in tempo reale.</li>
<li><strong>Efficienza Energetica:</strong> Calcolato come elaborazione per watt, che rappresenta il compromesso tra prestazioni e consumo energetico.</li>
<li><strong>Efficienza dei Costi:</strong> Valuta il costo operativo in relazione alle prestazioni, un parametro essenziale per le distribuzioni attente al budget.</li>
<li><strong>Precisione:</strong> Nelle attivit√† di inferenza, la precisione dei calcoli √® fondamentale e talvolta bilanciata rispetto alla velocit√†.</li>
<li><strong>Scalabilit√†:</strong> La capacit√† del sistema di mantenere i guadagni in termini di prestazioni man mano che il carico computazionale aumenta.</li>
</ul>
<p>I risultati del benchmark forniscono informazioni che vanno oltre i semplici numeri: possono rivelare colli di bottiglia nello stack software e nell‚Äôhardware. Ad esempio, i benchmark possono mostrare come l‚Äôaumento delle dimensioni del batch migliori l‚Äôutilizzo della GPU fornendo pi√π parallelismo o come le ottimizzazioni del compilatore aumentino le prestazioni della TPU. Questi insegnamenti consentono un‚Äôottimizzazione continua <span class="citation" data-cites="jia2019beyond">(<a href="../../../references.it.html#ref-jia2019beyond" role="doc-biblioref">Zhihao Jia, Zaharia, e Aiken 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jia2019beyond" class="csl-entry" role="listitem">
Jia, Zhihao, Matei Zaharia, e Alex Aiken. 2019. <span>¬´Beyond Data and Model Parallelism for Deep Neural Networks¬ª</span>. In <em>Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019</em>, a cura di Ameet Talwalkar, Virginia Smith, e Matei Zaharia. mlsys.org. <a href="https://proceedings.mlsys.org/book/265.pdf">https://proceedings.mlsys.org/book/265.pdf</a>.
</div><div id="ref-zhu2018benchmarking" class="csl-entry" role="listitem">
Zhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand Jayarajan, Amar Phanishayee, Bianca Schroeder, e Gennady Pekhimenko. 2018. <span>¬´Benchmarking and Analyzing Deep Neural Network Training¬ª</span>. In <em>2018 IEEE International Symposium on Workload Characterization (IISWC)</em>, 88‚Äì100. IEEE; IEEE. <a href="https://doi.org/10.1109/iiswc.2018.8573476">https://doi.org/10.1109/iiswc.2018.8573476</a>.
</div></div><p>Il benchmarking standardizzato fornisce una valutazione quantificata e comparabile degli acceleratori AI per informare la progettazione, l‚Äôacquisto e l‚Äôottimizzazione. Tuttavia, anche la convalida delle prestazioni nel mondo reale rimane essenziale <span class="citation" data-cites="zhu2018benchmarking">(<a href="../../../references.it.html#ref-zhu2018benchmarking" role="doc-biblioref">H. Zhu et al. 2018</a>)</span>.</p>
</section>
<section id="sfide-e-soluzioni" class="level2 page-columns page-full" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="sfide-e-soluzioni"><span class="header-section-number">10.7</span> Sfide e Soluzioni</h2>
<p>Gli acceleratori AI offrono notevoli miglioramenti delle prestazioni, ma spesso √® necessario migliorare i significativi problemi di portabilit√† e compatibilit√† nella loro integrazione nel pi√π ampio panorama AI. Il nocciolo della questione risiede nella diversit√† dell‚Äôecosistema AI: esiste una vasta gamma di acceleratori, framework e linguaggi di programmazione per l‚Äôapprendimento automatico, ognuno con le sue caratteristiche e requisiti unici.</p>
<section id="problemi-di-portabilit√†compatibilit√†" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="problemi-di-portabilit√†compatibilit√†"><span class="header-section-number">10.7.1</span> Problemi di Portabilit√†/Compatibilit√†</h3>
<p>Gli sviluppatori incontrano spesso difficolt√† nel trasferire i loro modelli AI da un ambiente hardware a un altro. Ad esempio, un modello di machine learning sviluppato per un ambiente desktop in Python utilizzando il framework PyTorch, ottimizzato per una GPU Nvidia, potrebbe non essere facilmente trasferito a un dispositivo pi√π vincolato come Arduino Nano 33 BLE. Questa complessit√† deriva da nette differenze nei requisiti di programmazione: Python e PyTorch sul desktop rispetto a un ambiente C++ su un Arduino, per non parlare del passaggio dall‚Äôarchitettura x86 ad ARM ISA.</p>
<p>Queste divergenze evidenziano la complessit√† della portabilit√† all‚Äôinterno dei sistemi AI. Inoltre, il rapido progresso negli algoritmi e nei modelli di intelligenza artificiale implica che gli acceleratori hardware debbano adattarsi continuamente, creando un obiettivo mobile per la compatibilit√†. L‚Äôassenza di standard e interfacce universali aggrava il problema, rendendo difficile l‚Äôimplementazione di soluzioni di intelligenza artificiale in modo coerente su vari dispositivi e piattaforme.</p>
<section id="soluzioni-e-strategie" class="level4">
<h4 class="anchored" data-anchor-id="soluzioni-e-strategie">Soluzioni e Strategie</h4>
<p>Per affrontare questi ostacoli, il settore dell‚Äôintelligenza artificiale si sta muovendo verso diverse soluzioni:</p>
<section id="iniziative-di-standardizzazione" class="level5">
<h5 class="anchored" data-anchor-id="iniziative-di-standardizzazione">Iniziative di Standardizzazione</h5>
<p><a href="https://onnx.ai/">Open Neural Network Exchange (ONNX)</a> √® in prima linea in questa ricerca, proponendo un ecosistema aperto e condiviso che promuove l‚Äôintercambiabilit√† dei modelli. ONNX facilita l‚Äôuso di modelli di intelligenza artificiale su vari framework, consentendo ai modelli addestrati in un ambiente di essere distribuiti in modo efficiente in un altro, riducendo significativamente la necessit√† di riscritture o modifiche che richiedono molto tempo.</p>
</section>
<section id="framework-multipiattaforma" class="level5">
<h5 class="anchored" data-anchor-id="framework-multipiattaforma">Framework Multipiattaforma</h5>
<p>A complemento degli sforzi di standardizzazione, framework multipiattaforma come TensorFlow Lite e PyTorch Mobile sono stati sviluppati specificamente per creare coesione tra diversi ambienti di calcolo che vanno dai desktop ai dispositivi mobili ed embedded. Questi framework offrono versioni semplificate e leggere delle loro versioni principali, garantendo compatibilit√† e integrit√† funzionale su diversi tipi di hardware senza sacrificare le prestazioni. Ci√≤ garantisce che gli sviluppatori possano creare applicazioni con la certezza che funzioneranno su molti dispositivi, colmando un divario che tradizionalmente ha rappresentato una sfida considerevole nello sviluppo dell‚Äôintelligenza artificiale.</p>
</section>
<section id="piattaforme-indipendenti-dallhardware" class="level5">
<h5 class="anchored" data-anchor-id="piattaforme-indipendenti-dallhardware">Piattaforme Indipendenti dall‚ÄôHardware</h5>
<p>L‚Äôascesa delle piattaforme indipendenti dall‚Äôhardware ha anche svolto un ruolo importante nella democratizzazione dell‚Äôuso dell‚ÄôIA. Creando ambienti in cui le applicazioni di IA possono essere eseguite su vari acceleratori, queste piattaforme eliminano l‚Äôonere della codifica specifica per l‚Äôhardware dagli sviluppatori. Questa astrazione semplifica il processo di sviluppo e apre nuove possibilit√† per l‚Äôinnovazione e l‚Äôimplementazione delle applicazioni, libere dai vincoli delle specifiche hardware.</p>
</section>
<section id="strumenti-di-compilazione-avanzati" class="level5">
<h5 class="anchored" data-anchor-id="strumenti-di-compilazione-avanzati">Strumenti di Compilazione Avanzati</h5>
<p>Inoltre, l‚Äôavvento di strumenti di compilazione avanzati come TVM, un compilatore di tensori end-to-end, offre un percorso ottimizzato attraverso la giungla delle diverse architetture hardware. TVM fornisce agli sviluppatori i mezzi per mettere a punto modelli di machine learning per un ampio spettro di substrati computazionali, garantendo prestazioni ottimali ed evitando la regolazione manuale del modello ogni volta che si verifica uno spostamento nell‚Äôhardware sottostante.</p>
</section>
<section id="collaborazione-tra-comunit√†-e-settore" class="level5">
<h5 class="anchored" data-anchor-id="collaborazione-tra-comunit√†-e-settore">Collaborazione tra Comunit√† e Settore</h5>
<p>La collaborazione tra comunit√† open source e consorzi di settore non pu√≤ essere sottovalutata. Questi organismi collettivi sono fondamentali per la formazione di standard condivisi e best practice a cui tutti gli sviluppatori e i produttori possono aderire. Tale collaborazione promuove un ecosistema AI pi√π unificato e sinergico, riducendo significativamente la prevalenza di problemi di portabilit√† e spianando la strada verso l‚Äôintegrazione e l‚Äôavanzamento dell‚ÄôAI globale. Attraverso questi lavori combinati, l‚ÄôAI si sta muovendo costantemente verso un futuro in cui la distribuzione di modelli senza soluzione di continuit√† su varie piattaforme diventa uno standard piuttosto che un‚Äôeccezione.</p>
<p>Risolvere le sfide della portabilit√† √® fondamentale per il campo dell‚ÄôIA per realizzare il pieno potenziale degli acceleratori hardware in un panorama tecnologico dinamico e diversificato. Richiede uno sforzo concertato da parte dei produttori di hardware, degli sviluppatori di software e degli enti normativi per creare un ambiente pi√π interoperabile e flessibile. Con innovazione e collaborazione continue, la comunit√† dell‚ÄôIA pu√≤ aprire la strada a un‚Äôintegrazione e a un‚Äôimplementazione senza soluzione di continuit√† dei modelli di IA su molte piattaforme.</p>
</section>
</section>
</section>
<section id="problemi-di-consumo-energetico" class="level3 page-columns page-full" data-number="10.7.2">
<h3 data-number="10.7.2" class="anchored" data-anchor-id="problemi-di-consumo-energetico"><span class="header-section-number">10.7.2</span> Problemi di Consumo Energetico</h3>
<p>Il consumo energetico √® un problema cruciale nello sviluppo e nel funzionamento degli acceleratori AI dei data center, come le unit√† di elaborazione grafica (GPU) e le unit√† di elaborazione tensoriale (TPU) <span class="citation" data-cites="jouppi2017indatacenter">(<a href="../../../references.it.html#ref-jouppi2017indatacenter" role="doc-biblioref">N. P. Jouppi et al. 2017b</a>)</span> <span class="citation" data-cites="norrie2021design">(<a href="../../../references.it.html#ref-norrie2021design" role="doc-biblioref">Norrie et al. 2021</a>)</span> <span class="citation" data-cites="jouppi2023tpu">(<a href="../../../references.it.html#ref-jouppi2023tpu" role="doc-biblioref">N. Jouppi et al. 2023</a>)</span>. Questi potenti componenti sono la spina dorsale dell‚Äôinfrastruttura AI contemporanea, ma le loro elevate richieste di energia contribuiscono all‚Äôimpatto ambientale della tecnologia e aumentano significativamente i costi operativi. Man mano che le esigenze di elaborazione dei dati diventano pi√π complesse, con la crescente popolarit√† dell‚ÄôAI e del deep learning, c‚Äô√® una richiesta pressante di GPU e TPU in grado di fornire la potenza di calcolo necessaria in modo pi√π efficiente. L‚Äôimpatto di tali progressi √® duplice: possono ridurre l‚Äôimpatto ambientale di queste tecnologie e ridurre i costi di esecuzione delle applicazioni AI.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jouppi2017indatacenter" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî, et al. 2017b. <span>¬´In-Datacenter Performance Analysis of a Tensor Processing Unit¬ª</span>. In <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, 1‚Äì12. ISCA ‚Äô17. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3079856.3080246">https://doi.org/10.1145/3079856.3080246</a>.
</div><div id="ref-norrie2021design" class="csl-entry" role="listitem">
Norrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, e David Patterson. 2021. <span>¬´The Design Process for Google‚Äôs Training Chips: <span>Tpuv2</span> and <span>TPUv3</span>¬ª</span>. <em>IEEE Micro</em> 41 (2): 56‚Äì63. <a href="https://doi.org/10.1109/mm.2021.3058217">https://doi.org/10.1109/mm.2021.3058217</a>.
</div><div id="ref-jouppi2023tpu" class="csl-entry" role="listitem">
Jouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, et al. 2023. <span>¬´<span>TPU</span> v4: <span>An</span> Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings¬ª</span>. In <em>Proceedings of the 50th Annual International Symposium on Computer Architecture</em>. ISCA ‚Äô23. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3579371.3589350">https://doi.org/10.1145/3579371.3589350</a>.
</div></div><p>Le tecnologie hardware emergenti sono sul punto di rivoluzionare l‚Äôefficienza energetica in questo settore. L‚Äôinformatica fotonica, ad esempio, utilizza la luce anzich√© l‚Äôelettricit√† per trasportare informazioni, offrendo la promessa di un‚Äôelaborazione ad alta velocit√† con una frazione del consumo energetico. Analizziamo pi√π approfonditamente questa e altre tecnologie innovative nella sezione ‚ÄúTecnologie Hardware Emergenti‚Äù, esplorando il loro potenziale per affrontare le attuali sfide del consumo energetico.</p>
<p>Ai margini della rete, gli acceleratori AI sono progettati per elaborare dati su dispositivi come smartphone, sensori IoT e dispositivi indossabili intelligenti. Questi dispositivi spesso funzionano con gravi limitazioni di potenza, rendendo necessario un attento bilanciamento tra prestazioni e consumo energetico. Un modello AI ad alte prestazioni pu√≤ fornire risultati rapidi, ma a costo di esaurire rapidamente la durata della batteria e aumentare la produzione termica, il che pu√≤ influire sulla funzionalit√† e sulla durata del dispositivo. La posta in gioco √® pi√π alta per i dispositivi distribuiti in aree remote o difficili da raggiungere, dove non √® possibile garantire un‚Äôalimentazione costante, il che sottolinea la necessit√† di soluzioni a basso consumo energetico.</p>
<p>I problemi di latenza aggravano ulteriormente la sfida dell‚Äôefficienza energetica ai margini. Le applicazioni AI Edge in settori quali la guida autonoma e il monitoraggio sanitario richiedono velocit√†, precisione e affidabilit√†, poich√© i ritardi nell‚Äôelaborazione possono comportare gravi rischi per la sicurezza. Per queste applicazioni, gli sviluppatori devono ottimizzare sia gli algoritmi AI sia la progettazione hardware per raggiungere un equilibrio ottimale tra consumo energetico e latenza.</p>
<p>Questo sforzo di ottimizzazione non riguarda solo l‚Äôapporto di miglioramenti incrementali alle tecnologie esistenti; riguarda il ripensamento di come e dove elaboriamo le attivit√† AI. Progettando acceleratori AI che siano sia efficienti dal punto di vista energetico sia in grado di elaborare rapidamente, possiamo garantire che questi dispositivi svolgano i loro scopi previsti senza un consumo energetico non necessario o prestazioni compromesse. Tali sviluppi potrebbero promuovere l‚Äôadozione diffusa dell‚ÄôAI in vari settori, consentendo un uso pi√π intelligente, sicuro e sostenibile della tecnologia.</p>
</section>
<section id="superare-i-vincoli-delle-risorse" class="level3 page-columns page-full" data-number="10.7.3">
<h3 data-number="10.7.3" class="anchored" data-anchor-id="superare-i-vincoli-delle-risorse"><span class="header-section-number">10.7.3</span> Superare i Vincoli delle Risorse</h3>
<p>Anche i vincoli di risorse rappresentano una sfida significativa per gli acceleratori Edge AI, poich√© queste soluzioni hardware e software specializzate devono fornire prestazioni robuste entro i limiti dei dispositivi edge. A causa dei limiti di potenza e dimensioni, gli acceleratori Edge AI hanno spesso capacit√† di calcolo, memoria e archiviazione limitate <span class="citation" data-cites="lin2022ondevice">(<a href="../../../references.it.html#ref-lin2022ondevice" role="doc-biblioref">L. Zhu et al. 2023</a>)</span>. Questa scarsit√† di risorse richiede un‚Äôattenta allocazione delle capacit√† di elaborazione per eseguire modelli di apprendimento automatico in modo efficiente.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2022ondevice" class="csl-entry" role="listitem">
Zhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2023. <span>¬´<span>PockEngine:</span> <span>Sparse</span> and Efficient Fine-tuning in a Pocket¬ª</span>. In <em>56th Annual IEEE/ACM International Symposium on Microarchitecture</em>. ACM. <a href="https://doi.org/10.1145/3613424.3614307">https://doi.org/10.1145/3613424.3614307</a>.
</div><div id="ref-lin2023awq" class="csl-entry" role="listitem">
Lin, Ji, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, e Song Han. 2023. <span>¬´<span>AWQ:</span> <span>Activation-aware</span> Weight Quantization for <span>LLM</span> Compression and Acceleration¬ª</span>. <em>arXiv</em>.
</div><div id="ref-Li2020Additive" class="csl-entry" role="listitem">
Li, Yuhang, Xin Dong, e Wei Wang. 2020. <span>¬´Additive Powers-of-Two Quantization: <span>An</span> Efficient Non-uniform Discretization for Neural Networks¬ª</span>. In <em>8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net. <a href="https://openreview.net/forum?id=BkgXT24tDS">https://openreview.net/forum?id=BkgXT24tDS</a>.
</div><div id="ref-wang2020apq" class="csl-entry" role="listitem">
Wang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, e Song Han. 2020. <span>¬´<span>APQ:</span> <span>Joint</span> Search for Network Architecture, Pruning and Quantization Policy¬ª</span>. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2075‚Äì84. IEEE. <a href="https://doi.org/10.1109/cvpr42600.2020.00215">https://doi.org/10.1109/cvpr42600.2020.00215</a>.
</div></div><p>Inoltre, la gestione di risorse limitate richiede approcci innovativi, tra cui la quantizzazione del modello <span class="citation" data-cites="lin2023awq">(<a href="../../../references.it.html#ref-lin2023awq" role="doc-biblioref">Lin et al. 2023</a>)</span> <span class="citation" data-cites="Li2020Additive">(<a href="../../../references.it.html#ref-Li2020Additive" role="doc-biblioref">Li, Dong, e Wang 2020</a>)</span>, pruning <span class="citation" data-cites="wang2020apq">(<a href="../../../references.it.html#ref-wang2020apq" role="doc-biblioref">Wang et al. 2020</a>)</span> e l‚Äôottimizzazione delle pipeline di inferenza. Gli acceleratori Edge AI devono trovare un delicato equilibrio tra la fornitura di funzionalit√† AI significative e il non esaurire le risorse disponibili, mantenendo al contempo un basso consumo energetico. Superare questi vincoli di risorse √® fondamentale per garantire l‚Äôimplementazione di successo dell‚Äôintelligenza artificiale ai margini, dove molte applicazioni, dall‚ÄôIoT ai dispositivi mobili, si basano sull‚Äôuso efficiente di risorse hardware limitate per fornire un processo decisionale intelligente e in tempo reale.</p>
</section>
</section>
<section id="tecnologie-emergenti" class="level2 page-columns page-full" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="tecnologie-emergenti"><span class="header-section-number">10.8</span> Tecnologie Emergenti</h2>
<p>Finora abbiamo discusso la tecnologia hardware AI nel contesto della progettazione dell‚Äôarchitettura von Neumann convenzionale e dell‚Äôimplementazione basata su CMOS. Questi chip AI specializzati offrono vantaggi come una maggiore produttivit√† ed efficienza energetica, ma si basano sui principi di elaborazione tradizionali. La crescita inarrestabile della domanda di potenza di elaborazione AI sta guidando le innovazioni nei metodi di integrazione per l‚Äôhardware AI.</p>
<p>Sono emersi due approcci principali per massimizzare la densit√† di elaborazione, l‚Äôintegrazione su ‚Äúscala wafer‚Äù e le architetture basate su ‚Äúchiplet‚Äù, di cui parleremo in questa sezione. Guardando molto pi√π avanti, esamineremo le tecnologie emergenti che divergono dalle architetture convenzionali e adottano approcci fondamentalmente diversi per l‚Äôelaborazione specializzata AI.</p>
<p>Alcuni di questi paradigmi non convenzionali includono l‚Äôelaborazione neuromorfica, che imita le reti neurali biologiche; l‚Äôelaborazione quantistica, che sfrutta gli effetti della meccanica quantistica; e l‚Äôelaborazione ottica, che utilizza fotoni anzich√© elettroni. Oltre ai nuovi substrati di elaborazione, le nuove tecnologie dei dispositivi stanno consentendo ulteriori guadagni attraverso una migliore memoria e interconnessione.</p>
<p>Esempi includono i ‚Äúmemristor‚Äù [https://it.wikipedia.org/wiki/Memristore] per l‚Äôelaborazione in memoria e la nanofotonica per la comunicazione fotonica integrata. Insieme, queste tecnologie offrono il potenziale per miglioramenti di ordini di grandezza in termini di velocit√†, efficienza e scalabilit√† rispetto all‚Äôattuale hardware AI. Esamineremo questi aspetti in questa sezione.</p>
<section id="metodi-di-integrazione" class="level3 page-columns page-full" data-number="10.8.1">
<h3 data-number="10.8.1" class="anchored" data-anchor-id="metodi-di-integrazione"><span class="header-section-number">10.8.1</span> Metodi di Integrazione</h3>
<p>I metodi di integrazione si riferiscono agli approcci utilizzati per combinare e interconnettere i vari componenti di elaborazione e memoria di un chip o sistema AI. Collegando strettamente gli elementi di elaborazione chiave, l‚Äôintegrazione cerca di massimizzare le prestazioni, l‚Äôefficienza energetica e la densit√†.</p>
<p>In passato, l‚Äôelaborazione AI veniva eseguita principalmente su CPU e GPU costruite utilizzando metodi di integrazione convenzionali. Questi componenti discreti venivano fabbricati separatamente e collegati insieme su una scheda. Tuttavia, questa integrazione poco stretta crea colli di bottiglia, come i sovraccarichi dei trasferimento di dati.</p>
<p>Con l‚Äôaumento dei carichi di lavoro AI, aumenta la domanda di una pi√π stretta integrazione tra elementi di elaborazione, memoria e comunicazione. Alcuni fattori chiave dell‚Äôintegrazione includono:</p>
<ul>
<li><strong>Riduzione al minimo dello spostamento dei dati:</strong> Una stretta integrazione riduce la latenza e l‚Äôenergia per lo spostamento dei dati tra i componenti. Ci√≤ migliora l‚Äôefficienza.</li>
<li><strong>Personalizzazione:</strong> Adattare tutti i componenti del sistema ai carichi di lavoro AI consente ottimizzazioni in tutto lo stack hardware.</li>
<li><strong>Parallelismo:</strong> L‚Äôintegrazione di molti elementi di elaborazione consente un calcolo parallelo massiccio.</li>
<li><strong>Densit√†:</strong> Una pi√π stretta integrazione consente di impacchettare pi√π transistor e memoria in una determinata area.</li>
<li><strong>Costo:</strong> Le economie di scala derivanti da grandi sistemi integrati possono ridurre i costi.</li>
</ul>
<p>In risposta, nuove tecniche di produzione come la fabbricazione su scala di wafer e il confezionamento avanzato consentono ora livelli di integrazione molto pi√π elevati. L‚Äôobiettivo √® creare complessi di elaborazione AI unificati e specializzati, su misura per il deep learning e altri algoritmi AI. Un‚Äôintegrazione pi√π stretta √® fondamentale per fornire le prestazioni e l‚Äôefficienza necessarie per la prossima generazione di AI.</p>
<section id="ai-su-scala-wafer" class="level4">
<h4 class="anchored" data-anchor-id="ai-su-scala-wafer">AI su Scala Wafer</h4>
<p>L‚Äôintelligenza artificiale su ‚Äúwafer-scale‚Äù adotta un approccio estremamente integrato, producendo un intero wafer di silicio come un gigantesco chip. Ci√≤ differisce drasticamente dalle CPU e GPU convenzionali, che tagliano ogni wafer in molti chip singoli pi√π piccoli. <a href="#fig-wafer-scale" class="quarto-xref">Figura&nbsp;<span>10.5</span></a> mostra un confronto tra Cerebras Wafer Scale Engine 2, che √® il chip pi√π grande mai costruito, e la GPU pi√π grande. Mentre alcune GPU possono contenere miliardi di transistor, impallidiscono comunque rispetto alla scala di un chip delle dimensioni di un wafer con oltre un trilione di transistor.</p>
<div id="fig-wafer-scale" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wafer-scale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/aimage1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wafer-scale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.5: Wafer-scale vs.&nbsp;GPU. Fonte: <a href="https://www.cerebras.net/product-chip/">Cerebras</a>.
</figcaption>
</figure>
</div>
<p>L‚Äôapproccio su scala di wafer diverge anche dai progetti system-on-chip pi√π modulari che hanno ancora componenti discreti che comunicano tramite bus. Invece, l‚Äôintelligenza artificiale su scala di wafer consente la personalizzazione completa e la stretta integrazione di elaborazione, memoria e interconnessioni nell‚Äôintero di die.</p>
<p>Progettando il wafer come un‚Äôunit√† logica integrata, il trasferimento dati tra gli elementi √® ridotto al minimo. Ci√≤ fornisce una latenza e un consumo energetico inferiori rispetto ai design discreti system-on-chip o chiplet. Mentre i chiplet possono offrire flessibilit√† mescolando e abbinando i componenti, la comunicazione tra chiplet √® impegnativa. La natura monolitica dell‚Äôintegrazione su scala wafer elimina questi colli di bottiglia nella comunicazione tra chip.</p>
<p>Tuttavia, la scala ultra-large pone anche difficolt√† per la producibilit√† e la resa con i design su scala wafer. Difetti in qualsiasi regione del wafer possono rendere (alcune parti del) chip inutilizzabile. Sono necessarie tecniche di litografia specializzate per produrre tali matrici di grandi dimensioni. Quindi, l‚Äôintegrazione su scala wafer persegue i massimi guadagni in termini di prestazioni dall‚Äôintegrazione ma richiede il superamento di sostanziali sfide di fabbricazione.</p>
<p><a href="#vid-wfscale" class="quarto-xref">Video&nbsp;<span>10.1</span></a> fornisce ulteriore contesto sui chip AI su scala wafer.</p>
<div id="vid-wfscale" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;10.1: Wafer-scale AI Chips
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Fcob512SJz0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
<section id="chiplet-per-ai" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="chiplet-per-ai">Chiplet per AI</h4>
<p>Il design chiplet si riferisce a un‚Äôarchitettura semiconduttrice in cui un singolo circuito integrato (IC) √® costruito da pi√π componenti pi√π piccoli e individuali noti come chiplet. Ogni chiplet √® un blocco funzionale autonomo, in genere specializzato per un‚Äôattivit√† o funzionalit√† specifica. Questi chiplet sono quindi interconnessi su un substrato o un package pi√π grande per creare un sistema coeso.</p>
<p><a href="#fig-chiplet" class="quarto-xref">Figura&nbsp;<span>10.6</span></a> illustra questo concetto. Per l‚Äôhardware AI, i chiplet consentono di combinare diversi tipi di chip ottimizzati per attivit√† come moltiplicazione di matrici, spostamento di dati, I/O analogico e memorie specializzate. Questa integrazione eterogenea differisce notevolmente dall‚Äôintegrazione wafer-scale, in cui tutta la logica √® prodotta come un unico chip monolitico. Aziende come Intel e AMD hanno adottato design chiplet per le loro CPU.</p>
<p>I chiplet sono interconnessi utilizzando tecniche di packaging avanzate come interposer di substrato ad alta densit√†, impilamento 2.5D/3D e packaging a livello di wafer. Ci√≤ consente di combinare chiplet realizzati con diversi nodi di processo, memorie specializzate e vari motori AI ottimizzati.</p>
<div id="fig-chiplet" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-chiplet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/aimage2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chiplet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.6: Partizionamento chiplet. Fonte: <span class="citation" data-cites="vivet2021intact">Vivet et al. (<a href="../../../references.it.html#ref-vivet2021intact" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-vivet2021intact" class="csl-entry" role="listitem">
Vivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar Fuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021. <span>¬´<span>IntAct:</span> <span>A</span> 96-Core Processor With Six Chiplets <span>3D</span>-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management¬ª</span>. <em>IEEE J. Solid-State Circuits</em> 56 (1): 79‚Äì97. <a href="https://doi.org/10.1109/jssc.2020.3036341">https://doi.org/10.1109/jssc.2020.3036341</a>.
</div></div></figure>
</div>
<p>Ecco alcuni vantaggi chiave dell‚Äôuso di chiplet per l‚Äôintelligenza artificiale:</p>
<ul>
<li><strong>Flessibilit√†:</strong> I chiplet consentono la combinazione di diversi tipi di chip, nodi di processo e memorie su misura per ogni funzione. Questo √® pi√π modulare rispetto a un design fisso su scala wafer.</li>
<li><strong>Resa:</strong> I chiplet pi√π piccoli hanno una resa maggiore rispetto a un gigantesco chip su scala wafer. I difetti sono contenuti nei singoli chiplet.</li>
<li><strong>Costo:</strong> Sfrutta le capacit√† di produzione esistenti anzich√© richiedere nuovi processi specializzati. Riduce i costi riutilizzando la fabbricazione assestata.</li>
<li><strong>Compatibilit√†:</strong> Pu√≤ integrarsi con architetture di sistema pi√π convenzionali come PCIe e interfacce di memoria DDR standard.</li>
</ul>
<p>Tuttavia, i chiplet devono anche affrontare sfide di integrazione e prestazioni:</p>
<ul>
<li>Densit√† inferiore rispetto alla scala wafer, poich√© i chiplet sono limitati in termini di dimensioni.</li>
<li>Latenza aggiuntiva durante la comunicazione tra chiplet rispetto all‚Äôintegrazione monolitica. Richiede ottimizzazione per interconnessioni a bassa latenza.</li>
<li>Il packaging avanzato aggiunge complessit√† rispetto all‚Äôintegrazione su scala wafer, sebbene ci√≤ sia discutibile.</li>
</ul>
<p>L‚Äôobiettivo principale dei chiplet √® trovare il giusto equilibrio tra flessibilit√† modulare e densit√† di integrazione per prestazioni AI ottimali. I chiplet mirano a un‚Äôaccelerazione AI efficiente pur lavorando entro i vincoli delle tecniche di produzione convenzionali. I chiplet prendono una via di mezzo tra gli estremi dell‚Äôintegrazione su scala wafer e dei componenti completamente discreti. Ci√≤ fornisce vantaggi pratici ma pu√≤ sacrificare una certa densit√† computazionale ed efficienza rispetto a un sistema teorico a livello di wafer.</p>
</section>
</section>
<section id="sec-neuromorphic" class="level3 page-columns page-full" data-number="10.8.2">
<h3 data-number="10.8.2" class="anchored" data-anchor-id="sec-neuromorphic"><span class="header-section-number">10.8.2</span> Elaborazione N√πeuromorfica</h3>
<p>L‚Äôelaborazione neuromorfica √® un campo emergente che mira a emulare l‚Äôefficienza e la robustezza dei sistemi neurali biologici per applicazioni di machine learning. Una differenza fondamentale rispetto alle classiche architetture di Von Neumann √® la fusione di memoria ed elaborazione nello stesso circuito <span class="citation" data-cites="schuman2022opportunities markovic2020physics furber2016large">(<a href="../../../references.it.html#ref-schuman2022opportunities" role="doc-biblioref">Schuman et al. 2022</a>; <a href="../../../references.it.html#ref-markovic2020physics" role="doc-biblioref">Markoviƒá et al. 2020</a>; <a href="../../../references.it.html#ref-furber2016large" role="doc-biblioref">Furber 2016</a>)</span>, come illustrato in <a href="#fig-neuromorphic" class="quarto-xref">Figura&nbsp;<span>10.7</span></a>. La struttura del cervello ispira questo approccio integrato. Un vantaggio fondamentale √® il potenziale per un miglioramento di ordini di grandezza nel calcolo efficiente dal punto di vista energetico rispetto all‚Äôhardware AI convenzionale. Ad esempio, le stime prevedono guadagni di 100x-1000x nell‚Äôefficienza energetica rispetto agli attuali sistemi basati su GPU per carichi di lavoro equivalenti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-markovic2020physics" class="csl-entry" role="listitem">
Markoviƒá, Danijela, Alice Mizrahi, Damien Querlioz, e Julie Grollier. 2020. <span>¬´Physics for neuromorphic computing¬ª</span>. <em>Nature Reviews Physics</em> 2 (9): 499‚Äì510. <a href="https://doi.org/10.1038/s42254-020-0208-2">https://doi.org/10.1038/s42254-020-0208-2</a>.
</div><div id="ref-furber2016large" class="csl-entry" role="listitem">
Furber, Steve. 2016. <span>¬´Large-scale neuromorphic computing systems¬ª</span>. <em>J. Neural Eng.</em> 13 (5): 051001. <a href="https://doi.org/10.1088/1741-2560/13/5/051001">https://doi.org/10.1088/1741-2560/13/5/051001</a>.
</div></div><div id="fig-neuromorphic" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-neuromorphic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/aimage3.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neuromorphic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.7: Confronto tra l‚Äôarchitettura di von Neumann e l‚Äôarchitettura neuromorfica. Fonte: <span class="citation" data-cites="schuman2022opportunities">Schuman et al. (<a href="../../../references.it.html#ref-schuman2022opportunities" role="doc-biblioref">2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-schuman2022opportunities" class="csl-entry" role="listitem">
Schuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. <span>¬´Opportunities for neuromorphic computing algorithms and applications¬ª</span>. <em>Nature Computational Science</em> 2 (1): 10‚Äì19. <a href="https://doi.org/10.1038/s43588-021-00184-y">https://doi.org/10.1038/s43588-021-00184-y</a>.
</div></div></figure>
</div>
<p>Intel e IBM stanno guidando gli sforzi commerciali nell‚Äôhardware neuromorfico. I chip Loihi e Loihi 2 di Intel <span class="citation" data-cites="davies2018loihi davies2021advancing">(<a href="../../../references.it.html#ref-davies2018loihi" role="doc-biblioref">Davies et al. 2018</a>, <a href="../../../references.it.html#ref-davies2021advancing" role="doc-biblioref">2021</a>)</span> offrono core neuromorfici programmabili con apprendimento on-chip. Il dispositivo Northpole <span class="citation" data-cites="modha2023neural">(<a href="../../../references.it.html#ref-modha2023neural" role="doc-biblioref">Modha et al. 2023</a>)</span> di IBM comprende oltre 100 milioni di sinapsi a giunzione a tunnel magnetico e 68 miliardi di transistor. Questi chip specializzati offrono vantaggi come un basso consumo energetico per l‚Äôinferenza edge.</p>
<div class="no-row-height column-margin column-container"><div id="ref-davies2018loihi" class="csl-entry" role="listitem">
Davies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. <span>¬´Loihi: <span>A</span> Neuromorphic Manycore Processor with On-Chip Learning¬ª</span>. <em>IEEE Micro</em> 38 (1): 82‚Äì99. <a href="https://doi.org/10.1109/mm.2018.112130359">https://doi.org/10.1109/mm.2018.112130359</a>.
</div><div id="ref-davies2021advancing" class="csl-entry" role="listitem">
Davies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya, Gabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, e Sumedh R. Risbud. 2021. <span>¬´Advancing Neuromorphic Computing With Loihi: <span>A</span> Survey of Results and Outlook¬ª</span>. <em>Proc. IEEE</em> 109 (5): 911‚Äì34. <a href="https://doi.org/10.1109/jproc.2021.3067593">https://doi.org/10.1109/jproc.2021.3067593</a>.
</div><div id="ref-modha2023neural" class="csl-entry" role="listitem">
Modha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. <span>¬´Neural inference at the frontier of energy, space, and time¬ª</span>. <em>Science</em> 382 (6668): 329‚Äì35. <a href="https://doi.org/10.1126/science.adh1174">https://doi.org/10.1126/science.adh1174</a>.
</div><div id="ref-maass1997networks" class="csl-entry" role="listitem">
Maass, Wolfgang. 1997. <span>¬´Networks of spiking neurons: <span>The</span> third generation of neural network models¬ª</span>. <em>Neural Networks</em> 10 (9): 1659‚Äì71. <a href="https://doi.org/10.1016/s0893-6080(97)00011-7">https://doi.org/10.1016/s0893-6080(97)00011-7</a>.
</div></div><p>Le ‚ÄúSpiking neural network (SNN)‚Äù <span class="citation" data-cites="maass1997networks">(<a href="../../../references.it.html#ref-maass1997networks" role="doc-biblioref">Maass 1997</a>)</span> sono modelli computazionali per hardware neuromorfico. A differenza delle reti neurali profonde che comunicano tramite valori continui, le SNN utilizzano picchi discreti che sono pi√π simili ai neuroni biologici. Questo consente un calcolo efficiente basato sugli eventi anzich√© un‚Äôelaborazione costante. Inoltre, le SNN considerano le caratteristiche temporali e spaziali dei dati di input. Ci√≤ imita meglio le reti neurali biologiche, in cui la tempistica dei picchi neuronali svolge un ruolo importante.</p>
<p>Tuttavia, l‚Äôaddestramento delle SNN rimane impegnativo a causa della complessit√† temporale aggiunta. <a href="#fig-spiking" class="quarto-xref">Figura&nbsp;<span>10.8</span></a> fornisce una panoramica della metodologia spiking: (a) illustrazione di un neurone; (b) Misura di un potenziale d‚Äôazione propagato lungo l‚Äôassone di un neurone. Solo il potenziale d‚Äôazione √® rilevabile lungo l‚Äôassone; (c) Il picco del neurone √® approssimato con una rappresentazione binaria; (d) Elaborazione guidata dagli eventi; (e) Active Pixel Sensor e Dynamic Vision Sensor.</p>
<div id="fig-spiking" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-spiking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/aimage4.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spiking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.8: Spiking neuromorfico. Fonte: <span class="citation" data-cites="eshraghian2023training">Eshraghian et al. (<a href="../../../references.it.html#ref-eshraghian2023training" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-eshraghian2023training" class="csl-entry" role="listitem">
Eshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, e Wei D. Lu. 2023. <span>¬´Training Spiking Neural Networks Using Lessons From Deep Learning¬ª</span>. <em>Proc. IEEE</em> 111 (9): 1016‚Äì54. <a href="https://doi.org/10.1109/jproc.2023.3308088">https://doi.org/10.1109/jproc.2023.3308088</a>.
</div></div></figure>
</div>
<p>Si pu√≤ anche guardare <a href="#vid-snn" class="quarto-xref">Video&nbsp;<span>10.2</span></a> linkato di seguito per una spiegazione pi√π dettagliata.</p>
<div id="vid-snn" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;10.2: Neuromorphic Computing
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/yihk_8XnCzg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
<p>Dispositivi nanoelettronici specializzati chiamati memristor <span class="citation" data-cites="chua1971memristor">(<a href="../../../references.it.html#ref-chua1971memristor" role="doc-biblioref">Chua 1971</a>)</span> sono componenti sinaptici nei sistemi neuromorfici. I memristor agiscono come memoria non volatile con conduttanza regolabile, emulando la plasticit√† delle sinapsi reali. I memristor consentono l‚Äôapprendimento in situ senza trasferimenti di dati separati combinando funzioni di memoria ed elaborazione. Tuttavia, la tecnologia dei memristor deve ancora raggiungere la maturit√† e la scalabilit√† per l‚Äôhardware commerciale.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chua1971memristor" class="csl-entry" role="listitem">
Chua, L. 1971. <span>¬´Memristor-The missing circuit element¬ª</span>. <em>#IEEE_J_CT#</em> 18 (5): 507‚Äì19. <a href="https://doi.org/10.1109/tct.1971.1083337">https://doi.org/10.1109/tct.1971.1083337</a>.
</div></div><p>L‚Äôintegrazione della fotonica con il calcolo neuromorfico <span class="citation" data-cites="shastri2021photonics">(<a href="../../../references.it.html#ref-shastri2021photonics" role="doc-biblioref">Shastri et al. 2021</a>)</span> √® emersa di recente come un‚Äôarea di ricerca attiva. L‚Äôuso della luce per il calcolo e la comunicazione consente alte velocit√† e un consumo energetico ridotto. Tuttavia, la piena realizzazione di sistemi neuromorfici fotonici richiede il superamento di problemi di progettazione e integrazione.</p>
<p>Il calcolo neuromorfico offre promettenti capacit√† per un‚Äôefficace inferenza edge, ma incontra ostacoli in merito ad algoritmi di addestramento, integrazione dei nanodispositivi e progettazione del sistema. La ricerca multidisciplinare in corso in informatica, ingegneria, scienza dei materiali e fisica sar√† fondamentale per sbloccare il pieno potenziale di questa tecnologia per i casi d‚Äôuso dell‚Äôintelligenza artificiale.</p>
</section>
<section id="calcolo-analogico" class="level3 page-columns page-full" data-number="10.8.3">
<h3 data-number="10.8.3" class="anchored" data-anchor-id="calcolo-analogico"><span class="header-section-number">10.8.3</span> Calcolo Analogico</h3>
<p>Il computing analogico √® un approccio emergente che utilizza segnali e componenti analogici come condensatori, induttori e amplificatori anzich√© la logica digitale per il calcolo. Rappresenta le informazioni come segnali elettrici continui anzich√© 0 e 1 discreti. Ci√≤ consente al calcolo di riflettere direttamente la natura analogica dei dati del mondo reale, evitando errori di digitalizzazione e overhead.</p>
<p>Il computing analogico ha generato un rinnovato interesse per l‚Äôhardware AI efficiente, in particolare per l‚Äôinferenza direttamente su dispositivi edge a basso consumo. I circuiti analogici, come la moltiplicazione e la sommatoria al centro delle reti neurali, possono essere utilizzati con un consumo energetico molto basso. Ci√≤ rende l‚Äôanalogico adatto per l‚Äôimplementazione di modelli ML su nodi finali con vincoli energetici. Startup come Mythic stanno sviluppando acceleratori AI analogici.</p>
<p>Mentre il computing analogico era popolare nei primi computer, il boom della logica digitale ha portato al suo declino. Tuttavia, l‚Äôanalogico √® convincente per applicazioni di nicchia che richiedono estrema efficienza <span class="citation" data-cites="haensch2018next">(<a href="../../../references.it.html#ref-haensch2018next" role="doc-biblioref">Haensch, Gokmen, e Puri 2019</a>)</span>. Contrasta con gli approcci neuromorfici digitali che utilizzano ancora picchi digitali per il calcolo. L‚Äôanalogico pu√≤ consentire un calcolo di precisione inferiore, ma richiede competenza nella progettazione di circuiti analogici. I compromessi su precisione, complessit√† di programmazione e costi di fabbricazione rimangono aree di ricerca attive.</p>
<div class="no-row-height column-margin column-container"><div id="ref-haensch2018next" class="csl-entry" role="listitem">
Haensch, Wilfried, Tayfun Gokmen, e Ruchir Puri. 2019. <span>¬´The Next Generation of Deep Learning Hardware: <span>Analog</span> Computing¬ª</span>. <em>Proc. IEEE</em> 107 (1): 108‚Äì22. <a href="https://doi.org/10.1109/jproc.2018.2871057">https://doi.org/10.1109/jproc.2018.2871057</a>.
</div><div id="ref-hazan2021neuromorphic" class="csl-entry" role="listitem">
Hazan, Avi, e Elishai Ezra Tsur. 2021. <span>¬´Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation¬ª</span>. <em>Front. Neurosci.</em> 15 (febbraio): 627221. <a href="https://doi.org/10.3389/fnins.2021.627221">https://doi.org/10.3389/fnins.2021.627221</a>.
</div></div><p>Il calcolo neuromorfico, che emula i sistemi neurali biologici per un‚Äôinferenza ML efficiente, pu√≤ utilizzare circuiti analogici per implementare i componenti e i comportamenti chiave del cervello. Ad esempio, i ricercatori hanno progettato circuiti analogici per modellare neuroni e sinapsi utilizzando condensatori, transistor e amplificatori operazionali <span class="citation" data-cites="hazan2021neuromorphic">(<a href="../../../references.it.html#ref-hazan2021neuromorphic" role="doc-biblioref">Hazan e Ezra Tsur 2021</a>)</span>. I condensatori possono esibire le dinamiche di picco dei neuroni biologici, mentre gli amplificatori e i transistor forniscono una somma ponderata di input per imitare i dendriti. Le tecnologie a resistore variabile come i memristor possono realizzare sinapsi analogiche con plasticit√† dipendente dal tempo di picco, che pu√≤ rafforzare o indebolire le connessioni in base all‚Äôattivit√† di picco.</p>
<p>Startup come SynSense hanno sviluppato chip neuromorfici analogici contenenti questi componenti biomimetici <span class="citation" data-cites="bains2020business">(<a href="../../../references.it.html#ref-bains2020business" role="doc-biblioref">Bains 2020</a>)</span>. Questo approccio analogico si traduce in un basso consumo energetico e un‚Äôelevata scalabilit√† per i dispositivi edge rispetto alle complesse implementazioni SNN digitali.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bains2020business" class="csl-entry" role="listitem">
Bains, Sunny. 2020. <span>¬´The business of building brains¬ª</span>. <em>Nature Electronics</em> 3 (7): 348‚Äì51. <a href="https://doi.org/10.1038/s41928-020-0449-1">https://doi.org/10.1038/s41928-020-0449-1</a>.
</div></div><p>Tuttavia, l‚Äôaddestramento di SNN analogiche sui chip rimane una sfida aperta. Nel complesso, la realizzazione analogica √® una tecnica promettente per fornire l‚Äôefficienza, la scalabilit√† e la plausibilit√† biologica previste con il calcolo neuromorfico. La fisica dei componenti analogici combinata con la progettazione dell‚Äôarchitettura neurale potrebbe migliorare l‚Äôefficienza dell‚Äôinferenza rispetto alle reti neurali digitali convenzionali.</p>
</section>
<section id="elettronica-flessibile" class="level3 page-columns page-full" data-number="10.8.4">
<h3 data-number="10.8.4" class="anchored" data-anchor-id="elettronica-flessibile"><span class="header-section-number">10.8.4</span> Elettronica Flessibile</h3>
<p>Mentre gran parte della nuova tecnologia hardware nell‚Äôarea di lavoro ML si √® concentrata sull‚Äôottimizzazione e sulla creazione di sistemi pi√π efficienti, c‚Äô√® una traiettoria parallela che mira ad adattare l‚Äôhardware per applicazioni specifiche <span class="citation" data-cites="gates2009flexible musk2019integrated tang2023flexible tang2022soft kwon2022flexible">(<a href="../../../references.it.html#ref-gates2009flexible" role="doc-biblioref">Gates 2009</a>; <a href="../../../references.it.html#ref-musk2019integrated" role="doc-biblioref">Musk et al. 2019</a>; <a href="../../../references.it.html#ref-tang2023flexible" role="doc-biblioref">Tang et al. 2023</a>; <a href="../../../references.it.html#ref-tang2022soft" role="doc-biblioref">Tang, He, e Liu 2022</a>; <a href="../../../references.it.html#ref-kwon2022flexible" role="doc-biblioref">Kwon e Dong 2022</a>)</span>. Una di queste strade √® lo sviluppo di elettronica flessibile per casi d‚Äôuso AI.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gates2009flexible" class="csl-entry" role="listitem">
Gates, Byron D. 2009. <span>¬´Flexible Electronics¬ª</span>. <em>Science</em> 323 (5921): 1566‚Äì67. <a href="https://doi.org/10.1126/science.1171230">https://doi.org/10.1126/science.1171230</a>.
</div><div id="ref-tang2023flexible" class="csl-entry" role="listitem">
Tang, Xin, Hao Shen, Siyuan Zhao, Na Li, e Jia Liu. 2023. <span>¬´Flexible brain<span></span>computer interfaces¬ª</span>. <em>Nature Electronics</em> 6 (2): 109‚Äì18. <a href="https://doi.org/10.1038/s41928-022-00913-9">https://doi.org/10.1038/s41928-022-00913-9</a>.
</div><div id="ref-tang2022soft" class="csl-entry" role="listitem">
Tang, Xin, Yichun He, e Jia Liu. 2022. <span>¬´Soft bioelectronics for cardiac interfaces¬ª</span>. <em>Biophysics Reviews</em> 3 (1). <a href="https://doi.org/10.1063/5.0069516">https://doi.org/10.1063/5.0069516</a>.
</div></div><p>L‚Äôelettronica flessibile si riferisce a circuiti elettronici e dispositivi fabbricati su substrati flessibili in plastica o polimeri anzich√© in silicio rigido. A differenza delle schede e dei chip rigidi convenzionali, ci√≤ consente all‚Äôelettronica di piegarsi, torcersi e adattarsi a forme irregolari. <a href="#fig-flexible-device" class="quarto-xref">Figura&nbsp;<span>10.9</span></a> mostra un esempio di un prototipo di dispositivo flessibile che misura in modalit√† wireless la temperatura corporea, che pu√≤ essere integrato senza soluzione di continuit√† in indumenti o cerotti cutanei. La flessibilit√† e la piegabilit√† dei materiali elettronici emergenti consentono di integrarli in fattori di forma sottili e leggeri, adatti per applicazioni AI e TinyML embedded.</p>
<p>L‚Äôhardware AI flessibile pu√≤ adattarsi a superfici curve e funzionare in modo efficiente con budget di potenza in microwatt. La flessibilit√† consente inoltre fattori di forma arrotolabili o pieghevoli per ridurre al minimo l‚Äôingombro e il peso del dispositivo, ideali per piccoli dispositivi intelligenti portatili e dispositivi indossabili che incorporano TinyML. Un altro vantaggio fondamentale dell‚Äôelettronica flessibile rispetto alle tecnologie convenzionali sono i costi di produzione inferiori e i processi di fabbricazione pi√π semplici, che potrebbero democratizzare l‚Äôaccesso a queste tecnologie. Mentre le maschere in silicio e i costi di fabbricazione in genere costano milioni di dollari, l‚Äôhardware flessibile in genere costa solo decine di centesimi per la produzione <span class="citation" data-cites="huang2010pseudo biggs2021natively">(<a href="../../../references.it.html#ref-huang2010pseudo" role="doc-biblioref">Huang et al. 2011</a>; <a href="../../../references.it.html#ref-biggs2021natively" role="doc-biblioref">Biggs et al. 2021</a>)</span>. Il potenziale di fabbricare elettronica flessibile direttamente su pellicole di plastica utilizzando processi di stampa e rivestimento ad alta produttivit√† pu√≤ ridurre i costi e migliorare la producibilit√† su larga scala rispetto ai chip AI rigidi <span class="citation" data-cites="musk2019integrated">(<a href="../../../references.it.html#ref-musk2019integrated" role="doc-biblioref">Musk et al. 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-huang2010pseudo" class="csl-entry" role="listitem">
Huang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi Sekitani, Takao Someya, e Kwang-Ting Cheng. 2011. <span>¬´Pseudo-<span>CMOS:</span> <span>A</span> Design Style for Low-Cost and Robust Flexible Electronics¬ª</span>. <em>IEEE Trans. Electron Devices</em> 58 (1): 141‚Äì50. <a href="https://doi.org/10.1109/ted.2010.2088127">https://doi.org/10.1109/ted.2010.2088127</a>.
</div><div id="ref-biggs2021natively" class="csl-entry" role="listitem">
Biggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony Sou, Catherine Ramsdale, Ken Williamson, Richard Price, e Scott White. 2021. <span>¬´A natively flexible 32-bit Arm microprocessor¬ª</span>. <em>Nature</em> 595 (7868): 532‚Äì36. <a href="https://doi.org/10.1038/s41586-021-03625-w">https://doi.org/10.1038/s41586-021-03625-w</a>.
</div></div><div id="fig-flexible-device" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flexible-device-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/flexible-circuit.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flexible-device-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.9: Prototipo di dispositivo flessibile. Fonte: Jabil Circuit.
</figcaption>
</figure>
</div>
<p>Il campo √® abilitato dai progressi nei semiconduttori organici e nei nanomateriali che possono essere depositati su pellicole sottili e flessibili. Tuttavia, la fabbricazione rimane impegnativa rispetto ai processi maturi del silicio. I circuiti flessibili attualmente presentano in genere prestazioni inferiori rispetto agli equivalenti rigidi. Tuttavia, promettono di trasformare l‚Äôelettronica in materiali leggeri e pieghevoli.</p>
<p>I casi d‚Äôuso dell‚Äôelettronica flessibile sono adatti per l‚Äôintegrazione intima con il corpo umano. Le potenziali applicazioni dell‚Äôintelligenza artificiale medica includono sensori biointegrati, ‚Äúsoft robot‚Äù e impianti che monitorano o stimolano il sistema nervoso in modo intelligente. In particolare, gli array di elettrodi flessibili potrebbero consentire interfacce neurali a densit√† pi√π elevata e meno invasive rispetto agli equivalenti rigidi.</p>
<p>Pertanto, l‚Äôelettronica flessibile sta inaugurando una nuova era di dispositivi indossabili e sensori corporei, in gran parte grazie alle innovazioni nei transistor organici. Questi componenti consentono un‚Äôelettronica pi√π leggera e pieghevole, ideale per dispositivi indossabili, pelle elettronica e dispositivi medici che si adattano al corpo.</p>
<p>Sono adatti per dispositivi bioelettronici in termini di biocompatibilit√†, aprendo la strada ad applicazioni in interfacce cerebrali e cardiache. Ad esempio, la ricerca sulle interfacce flessibili cervello-computer e sulla bioelettronica morbida per applicazioni cardiache dimostra il potenziale per applicazioni mediche di vasta portata.</p>
<p>Aziende e istituti di ricerca non stanno solo sviluppando e investendo grandi quantit√† di risorse in elettrodi flessibili, come mostrato nel lavoro di Neuralink <span class="citation" data-cites="musk2019integrated">(<a href="../../../references.it.html#ref-musk2019integrated" role="doc-biblioref">Musk et al. 2019</a>)</span>. Tuttavia, stanno anche spingendo i confini per integrare modelli di apprendimento automatico nei sistemi <span class="citation" data-cites="kwon2022flexible">(<a href="../../../references.it.html#ref-kwon2022flexible" role="doc-biblioref">Kwon e Dong 2022</a>)</span>. Questi sensori intelligenti mirano a una simbiosi fluida e duratura con il corpo umano.</p>
<div class="no-row-height column-margin column-container"><div id="ref-musk2019integrated" class="csl-entry" role="listitem">
Musk, Elon et al. 2019. <span>¬´An Integrated Brain-Machine Interface Platform With Thousands of Channels¬ª</span>. <em>J. Med. Internet Res.</em> 21 (10): e16194. <a href="https://doi.org/10.2196/16194">https://doi.org/10.2196/16194</a>.
</div><div id="ref-kwon2022flexible" class="csl-entry" role="listitem">
Kwon, Sun Hwa, e Lin Dong. 2022. <span>¬´Flexible sensors and machine learning for heart monitoring¬ª</span>. <em>Nano Energy</em> 102 (novembre): 107632. <a href="https://doi.org/10.1016/j.nanoen.2022.107632">https://doi.org/10.1016/j.nanoen.2022.107632</a>.
</div><div id="ref-segura2018ethical" class="csl-entry" role="listitem">
Segura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, e P. W. C. Prasad. 2017. <span>¬´Ethical Implications of User Perceptions of Wearable Devices¬ª</span>. <em>Sci. Eng. Ethics</em> 24 (1): 1‚Äì28. <a href="https://doi.org/10.1007/s11948-017-9872-8">https://doi.org/10.1007/s11948-017-9872-8</a>.
</div><div id="ref-goodyear2017social" class="csl-entry" role="listitem">
Goodyear, Victoria A. 2017. <span>¬´Social media, apps and wearable technologies: <span>Navigating</span> ethical dilemmas and procedures¬ª</span>. <em>Qualitative Research in Sport, Exercise and Health</em> 9 (3): 285‚Äì302. <a href="https://doi.org/10.1080/2159676x.2017.1303790">https://doi.org/10.1080/2159676x.2017.1303790</a>.
</div><div id="ref-farah2005neuroethics" class="csl-entry" role="listitem">
Farah, Martha J. 2005. <span>¬´Neuroethics: <span>The</span> practical and the philosophical¬ª</span>. <em>Trends Cogn. Sci.</em> 9 (1): 34‚Äì40. <a href="https://doi.org/10.1016/j.tics.2004.12.001">https://doi.org/10.1016/j.tics.2004.12.001</a>.
</div><div id="ref-roskies2002neuroethics" class="csl-entry" role="listitem">
Roskies, Adina. 2002. <span>¬´Neuroethics for the New Millenium¬ª</span>. <em>Neuron</em> 35 (1): 21‚Äì23. <a href="https://doi.org/10.1016/s0896-6273(02)00763-8">https://doi.org/10.1016/s0896-6273(02)00763-8</a>.
</div></div><p>Eticamente, l‚Äôincorporazione di sensori intelligenti basati sull‚Äôapprendimento automatico nel corpo solleva importanti questioni. Le problematiche relative alla privacy dei dati, al consenso informato e alle implicazioni sociali a lungo termine di tali tecnologie sono al centro del lavoro in corso in neuroetica e bioetica <span class="citation" data-cites="segura2018ethical goodyear2017social farah2005neuroethics roskies2002neuroethics">(<a href="../../../references.it.html#ref-segura2018ethical" role="doc-biblioref">Segura Anaya et al. 2017</a>; <a href="../../../references.it.html#ref-goodyear2017social" role="doc-biblioref">Goodyear 2017</a>; <a href="../../../references.it.html#ref-farah2005neuroethics" role="doc-biblioref">Farah 2005</a>; <a href="../../../references.it.html#ref-roskies2002neuroethics" role="doc-biblioref">Roskies 2002</a>)</span>. Il campo sta progredendo a un ritmo che richiede progressi paralleli nei parametri etici per guidare lo sviluppo e l‚Äôimplementazione responsabili di queste tecnologie. Sebbene vi siano limitazioni e ostacoli etici da superare, le prospettive per l‚Äôelettronica flessibile sono ampie e promettono molto per la ricerca e le applicazioni future.</p>
</section>
<section id="tecnologie-delle-memorie" class="level3 page-columns page-full" data-number="10.8.5">
<h3 data-number="10.8.5" class="anchored" data-anchor-id="tecnologie-delle-memorie"><span class="header-section-number">10.8.5</span> Tecnologie delle Memorie</h3>
<p>Le tecnologie delle memorie sono fondamentali per l‚Äôhardware AI, ma la DDR DRAM e la SRAM convenzionali creano colli di bottiglia. I carichi di lavoro AI richiedono un‚Äôelevata larghezza di banda (&gt;1 TB/s). Le applicazioni scientifiche estreme dell‚ÄôAI richiedono una latenza estremamente bassa (&lt;50 ns) per alimentare i dati alle unit√† di calcolo <span class="citation" data-cites="duarte2022fastml">(<a href="../../../references.it.html#ref-duarte2022fastml" role="doc-biblioref">Duarte et al. 2022</a>)</span>, un‚Äôelevata densit√† (&gt;128 Gb) per archiviare grandi parametri di modelli e set di dati e un‚Äôeccellente efficienza energetica (&lt;100 fJ/b) per uso embedded <span class="citation" data-cites="verma2019memory">(<a href="../../../references.it.html#ref-verma2019memory" role="doc-biblioref">Verma et al. 2019</a>)</span>. Sono necessarie nuove memorie per soddisfare queste esigenze. Le opzioni emergenti includono diverse nuove tecnologie:</p>
<div class="no-row-height column-margin column-container"><div id="ref-duarte2022fastml" class="csl-entry" role="listitem">
Duarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, e Vijay Janapa Reddi. 2022. <span>¬´<span>FastML</span> Science Benchmarks: <span>Accelerating</span> Real-Time Scientific Edge Machine Learning¬ª</span>. <em>ArXiv preprint</em> abs/2207.07958. <a href="https://arxiv.org/abs/2207.07958">https://arxiv.org/abs/2207.07958</a>.
</div><div id="ref-verma2019memory" class="csl-entry" role="listitem">
Verma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay, Lung-Yen Chen, Bonan Zhang, e Peter Deaville. 2019. <span>¬´In-Memory Computing: <span>Advances</span> and Prospects¬ª</span>. <em>IEEE Solid-State Circuits Mag.</em> 11 (3): 43‚Äì55. <a href="https://doi.org/10.1109/mssc.2019.2922889">https://doi.org/10.1109/mssc.2019.2922889</a>.
</div></div><ul>
<li>La RAM resistiva (ReRAM) pu√≤ migliorare la densit√† con semplici array passivi. Tuttavia, permangono dei problemi legati alla variabilit√† <span class="citation" data-cites="chi2016prime">(<a href="../../../references.it.html#ref-chi2016prime" role="doc-biblioref">Chi et al. 2016</a>)</span>.</li>
<li>La ‚ÄúPhase change memory (PCM)‚Äù [memoria a cambiamento di fase ] sfrutta le propriet√† uniche del vetro calcogenuro. Le fasi cristalline e amorfe hanno resistenze diverse. L‚ÄôOptane DCPMM di Intel fornisce PCM veloci (100 ns) e ad alta resistenza. Tuttavia, le sfide includono cicli di scrittura limitati e corrente di reset elevata <span class="citation" data-cites="burr2016recent">(<a href="../../../references.it.html#ref-burr2016recent" role="doc-biblioref">Burr et al. 2016</a>)</span>.</li>
<li>Lo stacking 3D pu√≤ anche aumentare la densit√† di memoria e la larghezza di banda integrando verticalmente strati di memoria con interconnessioni TSV <span class="citation" data-cites="loh20083dstacked">(<a href="../../../references.it.html#ref-loh20083dstacked" role="doc-biblioref">Loh 2008</a>)</span>. Ad esempio, HBM fornisce interfacce larghe 1024 bit.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-burr2016recent" class="csl-entry" role="listitem">
Burr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng, Jau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. <span>¬´Recent Progress in Phase-<span>Change?Pub</span> _newline <span>?Memory</span> Technology¬ª</span>. <em>IEEE Journal on Emerging and Selected Topics in Circuits and Systems</em> 6 (2): 146‚Äì62. <a href="https://doi.org/10.1109/jetcas.2016.2547718">https://doi.org/10.1109/jetcas.2016.2547718</a>.
</div><div id="ref-loh20083dstacked" class="csl-entry" role="listitem">
Loh, Gabriel H. 2008. <span>¬´<span>3D</span>-Stacked Memory Architectures for Multi-core Processors¬ª</span>. <em>ACM SIGARCH Computer Architecture News</em> 36 (3): 453‚Äì64. <a href="https://doi.org/10.1145/1394608.1382159">https://doi.org/10.1145/1394608.1382159</a>.
</div></div><p>Le nuove tecnologie di memoria, con le loro innovative architetture e materiali cellulari, sono fondamentali per sbloccare il prossimo livello di prestazioni ed efficienza hardware AI. Realizzare i loro vantaggi nei sistemi commerciali rimane una sfida continua.</p>
<p>L‚Äôelaborazione in-memory sta guadagnando terreno come promettente strada per ottimizzare l‚Äôapprendimento automatico e i carichi di lavoro di elaborazione ad alte prestazioni. Al centro, la tecnologia colloca l‚Äôarchiviazione e l‚Äôelaborazione dei dati per migliorare l‚Äôefficienza energetica e ridurre la latenza <span class="citation" data-cites="verma2019memory mittal2021survey">Wong et al. (<a href="../../../references.it.html#ref-wong2012metal" role="doc-biblioref">2012</a>)</span>. Due tecnologie chiave sotto questo ombrello sono la ‚ÄúResistive RAM (ReRAM)‚Äù e il ‚ÄúProcessing-In-Memory (PIM)‚Äù.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wong2012metal" class="csl-entry" role="listitem">
Wong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu, Pang-Shiu Chen, Byoungil Lee, Frederick T. Chen, e Ming-Jinn Tsai. 2012. <span>¬´<span>Metal<span></span>Oxide</span> <span>RRAM</span>¬ª</span>. <em>Proc. IEEE</em> 100 (6): 1951‚Äì70. <a href="https://doi.org/10.1109/jproc.2012.2190369">https://doi.org/10.1109/jproc.2012.2190369</a>.
</div><div id="ref-chi2016prime" class="csl-entry" role="listitem">
Chi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, e Yuan Xie. 2016. <span>¬´Prime: a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory¬ª</span>. <em>ACM SIGARCH Computer Architecture News</em> 44 (3): 27‚Äì39. <a href="https://doi.org/10.1145/3007787.3001140">https://doi.org/10.1145/3007787.3001140</a>.
</div></div><p>ReRAM <span class="citation" data-cites="wong2012metal">(<a href="../../../references.it.html#ref-wong2012metal" role="doc-biblioref">Wong et al. 2012</a>)</span> e PIM <span class="citation" data-cites="chi2016prime">(<a href="../../../references.it.html#ref-chi2016prime" role="doc-biblioref">Chi et al. 2016</a>)</span> sono le colonne portanti per l‚Äôelaborazione in memoria, l‚Äôarchiviazione e l‚Äôelaborazione dei dati nella stessa posizione. ReRAM si concentra su questioni di uniformit√†, resistenza, conservazione, funzionamento multi-bit e scalabilit√†. D‚Äôaltro canto, PIM coinvolge unit√† CPU integrate direttamente in array di memoria, specializzate per attivit√† come la moltiplicazione di matrici, che sono centrali nei calcoli AI.</p>
<p>Queste tecnologie trovano applicazioni nei carichi di lavoro AI e nell‚Äôelaborazione ad alte prestazioni, dove la sinergia di storage e calcolo pu√≤ portare a significativi guadagni in termini di prestazioni. L‚Äôarchitettura √® particolarmente utile per le attivit√† di elaborazione intensiva comuni nei modelli di apprendimento automatico.</p>
<p>Mentre le tecnologie di elaborazione in memoria come ReRAM e PIM offrono interessanti prospettive di efficienza e prestazioni, presentano le loro sfide, come l‚Äôuniformit√† dei dati e i problemi di scalabilit√† in ReRAM <span class="citation" data-cites="imani2016resistive">(<a href="../../../references.it.html#ref-imani2016resistive" role="doc-biblioref">Imani, Rahimi, e S. Rosing 2016</a>)</span>. Tuttavia, il campo √® maturo per l‚Äôinnovazione e affrontare queste limitazioni pu√≤ aprire nuove frontiere nell‚ÄôAI e nell‚Äôelaborazione ad alte prestazioni.</p>
<div class="no-row-height column-margin column-container"><div id="ref-imani2016resistive" class="csl-entry" role="listitem">
Imani, Mohsen, Abbas Rahimi, e Tajana S. Rosing. 2016. <span>¬´Resistive Configurable Associative Memory for Approximate Computing¬ª</span>. In <em>Proceedings of the 2016 Design, Automation &amp;amp; Test in Europe Conference &amp;amp; Exhibition (DATE)</em>, 1327‚Äì32. IEEE; Research Publishing Services. <a href="https://doi.org/10.3850/9783981537079_0454">https://doi.org/10.3850/9783981537079_0454</a>.
</div></div></section>
<section id="calcolo-ottico" class="level3 page-columns page-full" data-number="10.8.6">
<h3 data-number="10.8.6" class="anchored" data-anchor-id="calcolo-ottico"><span class="header-section-number">10.8.6</span> Calcolo Ottico</h3>
<p>Nell‚Äôaccelerazione dell‚Äôintelligenza artificiale, un‚Äôarea di interesse in rapida crescita risiede nelle nuove tecnologie che si discostano dai paradigmi tradizionali. Alcune tecnologie emergenti menzionate sopra, come l‚Äôelettronica flessibile, il calcolo in memoria o persino il calcolo neuromorfico, stanno per diventare realt√†, date le loro innovazioni e applicazioni rivoluzionarie. Una delle frontiere promettenti e all‚Äôavanguardia della prossima generazione √® la tecnologia del calcolo ottico <span class="citation" data-cites="miller2000optical">H. Zhou et al. (<a href="../../../references.it.html#ref-zhou2022photonic" role="doc-biblioref">2022</a>)</span>. Aziende come <a href="https://lightmatter.co/">LightMatter</a> sono pioniere nell‚Äôuso della fotonica luminosa per i calcoli, utilizzando quindi fotoni al posto degli elettroni per la trasmissione dei dati e l‚Äôelaborazione.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhou2022photonic" class="csl-entry" role="listitem">
Zhou, Hailong, Jianji Dong, Junwei Cheng, Wenchan Dong, Chaoran Huang, Yichen Shen, Qiming Zhang, et al. 2022. <span>¬´Photonic matrix multiplication lights up photonic accelerator and beyond¬ª</span>. <em>Light: Science &amp;amp; Applications</em> 11 (1): 30. <a href="https://doi.org/10.1038/s41377-022-00717-8">https://doi.org/10.1038/s41377-022-00717-8</a>.
</div><div id="ref-shastri2021photonics" class="csl-entry" role="listitem">
Shastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H. P. Pernice, Harish Bhaskaran, C. D. Wright, e Paul R. Prucnal. 2021. <span>¬´Photonics for artificial intelligence and neuromorphic computing¬ª</span>. <em>Nat. Photonics</em> 15 (2): 102‚Äì14. <a href="https://doi.org/10.1038/s41566-020-00754-y">https://doi.org/10.1038/s41566-020-00754-y</a>.
</div></div><p>Il calcolo ottico utilizza fotoni e dispositivi fotonici anzich√© i tradizionali circuiti elettronici per il calcolo e l‚Äôelaborazione dei dati. Trae ispirazione dai collegamenti di comunicazione in fibra ottica che si basano sulla luce per un trasferimento dati rapido ed efficiente <span class="citation" data-cites="shastri2021photonics">(<a href="../../../references.it.html#ref-shastri2021photonics" role="doc-biblioref">Shastri et al. 2021</a>)</span>. La luce pu√≤ propagarsi con una perdita molto inferiore rispetto agli elettroni dei semiconduttori, consentendo vantaggi intrinseci in termini di velocit√† ed efficienza.</p>
<p>Alcuni vantaggi specifici dell‚Äôelaborazione ottica includono:</p>
<ul>
<li><strong>Alta produttivit√†:</strong> I fotoni possono trasmettere con larghezze di banda &gt;100 Tb/s utilizzando il multiplexing a divisione di lunghezza d‚Äôonda.</li>
<li><strong>Bassa latenza:</strong> I fotoni interagiscono su scale temporali di femtosecondi, milioni di volte pi√π velocemente dei transistor al silicio.</li>
<li><strong>Parallelismo:</strong> Pi√π segnali di dati possono propagarsi simultaneamente attraverso lo stesso mezzo ottico.</li>
<li><strong>Bassa potenza:</strong> I circuiti fotonici che utilizzano guide d‚Äôonda e risonatori possono ottenere una logica e una memoria complesse con solo microwatt di potenza.</li>
</ul>
<p>Tuttavia, l‚Äôelaborazione ottica deve attualmente affrontare sfide significative:</p>
<ul>
<li>Mancanza di memoria ottica equivalente alla RAM elettronica</li>
<li>Richiede la conversione tra domini ottici ed elettrici.</li>
<li>Set limitato di componenti ottici disponibili rispetto al ricco ecosistema elettronico.</li>
<li>Metodi di integrazione immaturi per combinare la fotonica con i tradizionali chip CMOS.</li>
<li>Modelli di programmazione complessi richiesti per gestire il parallelismo.</li>
</ul>
<p>Di conseguenza, l‚Äôelaborazione ottica √® ancora in una fase di ricerca molto precoce nonostante il suo potenziale promettente. Tuttavia, le innovazioni tecniche potrebbero consentirgli di integrare l‚Äôelettronica e sbloccare guadagni di prestazioni per i carichi di lavoro AI. Aziende come Lightmatter sono pioniere nei primi acceleratori ottici AI. A lungo termine, se le sfide chiave saranno superate, potrebbe rappresentare un substrato di elaborazione rivoluzionario.</p>
</section>
<section id="quantum-computing" class="level3" data-number="10.8.7">
<h3 data-number="10.8.7" class="anchored" data-anchor-id="quantum-computing"><span class="header-section-number">10.8.7</span> Quantum Computing</h3>
<p>I computer quantistici sfruttano fenomeni unici della fisica quantistica, come la sovrapposizione e l‚Äôentanglement, per rappresentare ed elaborare informazioni in modi non possibili in modo classico. Invece dei bit binari, l‚Äôunit√† fondamentale √® il bit quantistico o qubit. A differenza dei bit classici, che sono limitati a 0 o 1, i qubit possono esistere simultaneamente in una sovrapposizione di entrambi gli stati a causa degli effetti quantistici.</p>
<p>Anche pi√π qubit possono essere entangled, portando a una densit√† di informazioni esponenziale ma introducendo risultati probabilistici. La sovrapposizione consente il calcolo parallelo su tutti gli stati possibili, mentre l‚Äôentanglement consente correlazioni non locali tra qubit. <a href="#fig-qubit" class="quarto-xref">Figura&nbsp;<span>10.10</span></a> trasmette visivamente le differenze tra i bit classici nell‚Äôinformatica e i bit quantistici (qbit).</p>
<div id="fig-qubit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qubit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/qubit.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qubit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.10: Qubit, i mattoni del calcolo quantistico. Fonte: <a href="https://azure.microsoft.com/en-gb/resources/cloud-computing-dictionary/what-is-a-qubit">Microsoft</a>
</figcaption>
</figure>
</div>
<p>Gli algoritmi quantistici manipolano attentamente questi effetti meccanici quantistici intrinseci per risolvere problemi come l‚Äôottimizzazione o la ricerca in modo pi√π efficiente rispetto alle loro controparti classiche in teoria.</p>
<ul>
<li>Training pi√π rapido di reti neurali profonde sfruttando il parallelismo quantistico per operazioni di algebra lineare.</li>
<li>Gli algoritmi ML quantistici efficienti sfruttano le capacit√† uniche dei qubit.</li>
<li>Reti neurali quantistiche con effetti quantistici intrinseci integrati nell‚Äôarchitettura del modello.</li>
<li>Ottimizzatori quantistici che sfruttano algoritmi di ‚Äúannealing‚Äù quantistica o adiabatici per problemi di ottimizzazione combinatoria.</li>
</ul>
<p>Tuttavia, gli stati quantistici sono fragili e soggetti a errori che richiedono protocolli di correzione degli errori. La natura non intuitiva della programmazione quantistica introduce anche sfide non presenti nell‚Äôinformatica classica.</p>
<ul>
<li>I bit quantistici rumorosi e fragili sono difficili da scalare. Il pi√π grande computer quantistico odierno ha meno di 1000 qubit.</li>
<li>Insieme limitato di porte e circuiti quantistici disponibili rispetto alla programmazione classica.</li>
<li>Mancanza di set di dati e benchmark per valutare l‚Äôapprendimento automatico quantistico in domini pratici.</li>
</ul>
<p>Sebbene un vantaggio quantistico significativo per l‚Äôapprendimento automatico sia ancora lontano, la ricerca attiva presso aziende come <a href="https://www.dwavesys.com/company/about-d-wave/">D-Wave</a>, <a href="https://www.rigetti.com/">Rigetti</a> e <a href="https://ionq.com/">IonQ</a> sta facendo progredire l‚Äôingegneria informatica quantistica e gli algoritmi quantistici. Le principali aziende tecnologiche come Google, <a href="https://www.ibm.com/quantum?utm_content=SRCWW&amp;p1=Search&amp;p4C700050385964705&amp;p5=e&amp;gclid=Cj0KCQjw-pyqBhDmARIsAKd9XIPD9U1Sjez_S0z5jeDDE4nRyd6X_gtVDUKJ-HIolx2vOc599KgW8gAaAv8gEALw_wcB&amp;gclsrc=aw.ds">IBM</a> e Microsoft stanno esplorando attivamente l‚Äôinformatica quantistica. Google ha recentemente annunciato un processore quantistico a 72 qubit chiamato <a href="https://blog.research.google/2018/03/a-preview-of-bristlecone-googles-new.html">Bristlecone</a> e prevede di costruire un sistema quantistico commerciale a 49 qubit. Microsoft ha anche un programma di ricerca attivo nell‚Äôinformatica quantistica topologica e collabora con la startup quantistica <a href="https://ionq.com/">IonQ</a></p>
<p>Le tecniche quantistiche potrebbero prima fare breccia nell‚Äôottimizzazione prima di un‚Äôadozione pi√π generalizzata dell‚Äôapprendimento automatico. La realizzazione del pieno potenziale dell‚Äôapprendimento automatico quantistico attende importanti traguardi nello sviluppo dell‚Äôhardware quantistico e nella maturit√† dell‚Äôecosistema. <a href="#fig-q-computing" class="quarto-xref">Figura&nbsp;<span>10.11</span></a> confronta a titolo esemplificativo l‚Äôinformatica quantistica e quella classica.</p>
<div id="fig-q-computing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-q-computing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/qcomputing.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-q-computing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.11: Confronto tra il calcolo quantistico e il calcolo classico. Fonte: <a href="‚Äã‚Äãhttps://devopedia.org/quantum-computing">Devopedia</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="tendenze-future" class="level2 page-columns page-full" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="tendenze-future"><span class="header-section-number">10.9</span> Tendenze Future</h2>
<p>In questo capitolo, l‚Äôattenzione principale √® stata rivolta alla progettazione di hardware specializzato ottimizzato per carichi di lavoro e algoritmi di machine learning. Questa discussione ha riguardato le architetture personalizzate di GPU e TPU per l‚Äôaddestramento e l‚Äôinferenza delle reti neurali. Tuttavia, una direzione di ricerca emergente sta sfruttando l‚Äôapprendimento automatico per facilitare il processo di progettazione hardware stesso.</p>
<p>Il processo di progettazione hardware comprende molte fasi complesse, tra cui specifica, modellazione di alto livello, simulazione, sintesi, verifica, prototipazione e fabbricazione. Gran parte di questo processo richiede tradizionalmente una vasta competenza umana, impegno e tempo. Tuttavia, i recenti progressi nell‚Äôapprendimento automatico stanno consentendo l‚Äôautomazione e il miglioramento di parti del flusso di lavoro di progettazione hardware utilizzando tecniche di apprendimento automatico.</p>
<p>Ecco alcuni esempi di come l‚Äôapprendimento automatico sta trasformando la progettazione hardware:</p>
<ul>
<li><strong>Sintesi di circuiti automatizzata tramite apprendimento per rinforzo:</strong> Anzich√© realizzare manualmente progetti a livello di transistor, gli agenti di apprendimento automatico come l‚Äôapprendimento per rinforzo possono imparare a collegare porte logiche e generare automaticamente layout di circuiti. Ci√≤ pu√≤ accelerare il lungo processo di sintesi.</li>
<li><strong>Simulazione ed emulazione hardware basate su ML:</strong> I modelli di reti neurali profonde possono essere addestrati per prevedere come si comporter√† un progetto hardware in diverse condizioni. Ad esempio, i modelli di apprendimento profondo possono essere addestrati per prevedere i conteggi dei cicli per determinati carichi di lavoro. Ci√≤ consente una simulazione pi√π rapida e accurata rispetto alle simulazioni RTL tradizionali.</li>
<li><strong>Pianificazione automatizzata dei chip mediante algoritmi ML:</strong> La pianificazione dei chip comporta il posizionamento ottimale di diversi componenti su un die. Algoritmi evolutivi come quelli genetici e altri algoritmi ML come l‚Äôapprendimento per rinforzo vengono utilizzati per esplorare le opzioni di pianificazione. Ci√≤ pu√≤ migliorare significativamente i posizionamenti manuali di pianificazione in termini di tempi di consegna pi√π rapidi e qualit√† dei posizionamenti.</li>
<li><strong>Ottimizzazione dell‚Äôarchitettura basata su ML:</strong> Le nuove architetture hardware, come quelle per gli acceleratori ML efficienti, possono essere generate e ottimizzate automaticamente tramite la ricerca nello spazio di progettazione architettonica. Gli algoritmi di apprendimento automatico possono cercare efficacemente ampi spazi di progettazione architettonica.</li>
</ul>
<p>L‚Äôapplicazione del ML all‚Äôautomazione della progettazione hardware promette di rendere il processo pi√π veloce, pi√π economico e pi√π efficiente. Apre possibilit√† di progettazione che richiederebbero pi√π di una progettazione manuale. L‚Äôuso del ML nella progettazione hardware √® un‚Äôarea di ricerca attiva e di distribuzione precoce, e studieremo le tecniche coinvolte e il loro potenziale trasformativo.</p>
<section id="ml-per-lautomazione-della-progettazione-hardware" class="level3 page-columns page-full" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="ml-per-lautomazione-della-progettazione-hardware"><span class="header-section-number">10.9.1</span> ML per l‚Äôautomazione della progettazione hardware</h3>
<p>Una grande opportunit√† per l‚Äôapprendimento automatico nella progettazione hardware √® l‚Äôautomazione di parti del complesso e noioso flusso di lavoro di progettazione. Con ‚ÄúHardware design automation (HDA)‚Äù ci si riferisce in generale all‚Äôuso di tecniche ML come l‚Äôapprendimento per rinforzo, algoritmi genetici e reti neurali per automatizzare attivit√† come sintesi, verifica, floorplanning e altro. Ecco alcuni esempi di dove l‚ÄôML per HDA mostra una vera promessa:</p>
<ul>
<li><strong>Sintesi di circuiti automatizzata:</strong> La sintesi di circuiti comporta la conversione di una descrizione di alto livello della logica desiderata in un‚Äôimplementazione di netlist a livello di gate ottimizzata. Questo processo complesso ha molte considerazioni e compromessi di progettazione. Gli agenti ML possono essere addestrati tramite l‚Äôapprendimento per rinforzo <span class="citation" data-cites="yu2023rl">G. Zhou e Anderson (<a href="../../../references.it.html#ref-zhou2023area" role="doc-biblioref">2023</a>)</span> per esplorare lo spazio di progettazione e produrre automaticamente sintesi ottimizzate. Startup come <a href="https://www.symbioticeda.com/">Symbiotic EDA</a> stanno portando questa tecnologia sul mercato.</li>
<li><strong>Automated chip floorplanning:</strong> Il Floorplanning si riferisce al posizionamento strategico di diversi componenti su un‚Äôarea del chip. Algoritmi di ricerca come algoritmi genetici <span class="citation" data-cites="valenzuela2000genetic">(<a href="../../../references.it.html#ref-valenzuela2000genetic" role="doc-biblioref">Valenzuela e Wang 2000</a>)</span> e apprendimento per rinforzo (<span class="citation" data-cites="mirhoseini2021graph">Mirhoseini et al. (<a href="../../../references.it.html#ref-mirhoseini2021graph" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="agnesina2023autodmp">Agnesina et al. (<a href="../../../references.it.html#ref-agnesina2023autodmp" role="doc-biblioref">2023</a>)</span>) possono essere utilizzati per automatizzare l‚Äôottimizzazione il floorplan per ridurre al minimo la lunghezza dei collegamenti, il consumo di energia e altri obiettivi. Questi ‚Äúfloor planners‚Äù assistiti da ML automatizzati sono estremamente preziosi man mano che aumenta la complessit√† dei chip.</li>
<li><strong>Simulatori hardware ML:</strong> L‚Äôaddestramento di modelli di reti neurali profonde per prevedere le prestazioni dei progetti hardware, poich√© i simulatori possono accelerare il processo di simulazione di oltre 100 volte rispetto alle simulazioni architettoniche e RTL tradizionali.</li>
<li><strong>Traduzione automatica del codice:</strong> La conversione di linguaggi di descrizione hardware come Verilog in implementazioni RTL ottimizzate √® fondamentale ma richiede molto tempo. I modelli ML possono essere addestrati per agire come agenti traduttori e automatizzare questo processo.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-zhou2023area" class="csl-entry" role="listitem">
Zhou, Guanglei, e Jason H. Anderson. 2023. <span>¬´Area-Driven <span>FPGA</span> Logic Synthesis Using Reinforcement Learning¬ª</span>. In <em>Proceedings of the 28th Asia and South Pacific Design Automation Conference</em>, 159‚Äì65. ACM. <a href="https://doi.org/10.1145/3566097.3567894">https://doi.org/10.1145/3566097.3567894</a>.
</div><div id="ref-valenzuela2000genetic" class="csl-entry" role="listitem">
Valenzuela, Christine L, e Pearl Y Wang. 2000. <span>¬´A genetic algorithm for <span>VLSI</span> floorplanning¬ª</span>. In <em>Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 18<span></span>20, 2000 Proceedings 6</em>, 671‚Äì80. Springer.
</div><div id="ref-mirhoseini2021graph" class="csl-entry" role="listitem">
Mirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. <span>¬´A graph placement methodology for fast chip design¬ª</span>. <em>Nature</em> 594 (7862): 207‚Äì12. <a href="https://doi.org/10.1038/s41586-021-03544-w">https://doi.org/10.1038/s41586-021-03544-w</a>.
</div><div id="ref-agnesina2023autodmp" class="csl-entry" role="listitem">
Agnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta, Austin Jiao, Ben Keller, Brucek Khailany, e Haoxing Ren. 2023. <span>¬´<span>AutoDMP</span>: Automated DREAMPlace-based Macro Placement¬ª</span>. In <em>Proceedings of the 2023 International Symposium on Physical Design</em>, 149‚Äì57. ACM. <a href="https://doi.org/10.1145/3569052.3578923">https://doi.org/10.1145/3569052.3578923</a>.
</div></div><p>I vantaggi dell‚ÄôHDA che utilizza ML sono tempi di progettazione ridotti, ottimizzazioni superiori ed esplorazione di spazi di progettazione troppo complessi per approcci manuali. Ci√≤ pu√≤ accelerare lo sviluppo hardware e portare a progetti migliori.</p>
<p>Le sfide includono i limiti della generalizzazione ML, la natura black-box di alcune tecniche e compromessi sull‚Äôaccuratezza. Tuttavia, la ricerca sta rapidamente avanzando per affrontare questi problemi e rendere le soluzioni HDA ML robuste e affidabili per l‚Äôuso in produzione. HDA fornisce un‚Äôimportante via per ML per trasformare la progettazione hardware.</p>
</section>
<section id="simulazione-e-verifica-hardware-basate-su-ml" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="simulazione-e-verifica-hardware-basate-su-ml"><span class="header-section-number">10.9.2</span> Simulazione e Verifica Hardware Basate su ML</h3>
<p>La simulazione e la verifica dei progetti hardware sono fondamentali prima della produzione per garantire che il progetto si comporti come previsto. Gli approcci tradizionali come la simulazione ‚Äúregister-transfer level‚Äù (RTL) sono complessi e richiedono molto tempo. Il ML introduce nuove opportunit√† per migliorare la simulazione e la verifica dell‚Äôhardware. Ecco alcuni esempi:</p>
<ul>
<li><strong>Modellazione surrogata per la simulazione:</strong> Modelli surrogati di un progetto altamente accurati possono essere creati utilizzando reti neurali. Questi modelli prevedono gli output dagli input molto pi√π velocemente della simulazione RTL, consentendo una rapida esplorazione dello spazio di progettazione. Aziende come Ansys utilizzano questa tecnica.</li>
<li><strong>Simulatori ML:</strong> Grandi modelli di reti neurali possono essere addestrati su simulazioni RTL per imparare a imitare la funzionalit√† di un progetto hardware. Una volta addestrato, il modello NN pu√≤ essere un simulatore altamente efficiente per test di regressione e altre attivit√†. <a href="https://www.graphcore.ai/posts/ai-for-simulation-how-graphcore-is-helping-transform-traditional-hpc">Graphcore</a> ha dimostrato un‚Äôaccelerazione di oltre 100 volte con questo approccio.</li>
<li><strong>Verifica formale tramite ML:</strong> La verifica formale dimostra matematicamente le propriet√† di un progetto. Le tecniche di ML possono aiutare a generare propriet√† di verifica e imparare a risolvere le complesse prove formali necessarie, automatizzando parti di questo processo impegnativo. Startup come Cortical.io stanno introducendo sul mercato soluzioni di verifica ML formali.</li>
<li><strong>Rilevamento di bug:</strong> I modelli ML possono essere addestrati per elaborare progetti hardware e identificare potenziali problemi. Ci√≤ aiuta i progettisti umani a ispezionare progetti complessi e a trovare bug. Facebook ha mostrato modelli di rilevamento di bug per l‚Äôhardware dei suoi server.</li>
</ul>
<p>I principali vantaggi dell‚Äôapplicazione di ML alla simulazione e alla verifica sono tempi di esecuzione pi√π rapidi per la convalida del progetto, test pi√π rigorosi e riduzione del lavoro umano. Le sfide includono la verifica della correttezza del modello ML e la gestione dei casi limite. ML promette di accelerare significativamente i flussi di lavoro di test.</p>
</section>
<section id="ml-per-architetture-hardware-efficienti" class="level3 page-columns page-full" data-number="10.9.3">
<h3 data-number="10.9.3" class="anchored" data-anchor-id="ml-per-architetture-hardware-efficienti"><span class="header-section-number">10.9.3</span> ML per Architetture Hardware Efficienti</h3>
<p>Un obiettivo chiave √® la progettazione di architetture hardware ottimizzate per prestazioni, potenza ed efficienza. ML introduce nuove tecniche per automatizzare e migliorare l‚Äôesplorazione dello spazio di progettazione dell‚Äôarchitettura per hardware generico e specializzato come gli acceleratori ML. Alcuni esempi promettenti sono:</p>
<ul>
<li><strong>Ricerca di architetture per hardware:</strong> Tecniche di ricerca come algoritmi evolutivi <span class="citation" data-cites="kao2020gamma">(<a href="../../../references.it.html#ref-kao2020gamma" role="doc-biblioref">Kao e Krishna 2020</a>)</span>, ottimizzazione bayesiana (<span class="citation" data-cites="reagen2017case">Reagen et al. (<a href="../../../references.it.html#ref-reagen2017case" role="doc-biblioref">2017</a>)</span>, <span class="citation" data-cites="bhardwaj2020comprehensive">Bhardwaj et al. (<a href="../../../references.it.html#ref-bhardwaj2020comprehensive" role="doc-biblioref">2020</a>)</span>), apprendimento per rinforzo (<span class="citation" data-cites="kao2020confuciux">Kao, Jeong, e Krishna (<a href="../../../references.it.html#ref-kao2020confuciux" role="doc-biblioref">2020</a>)</span>, <span class="citation" data-cites="krishnan2022multiagent">Krishnan et al. (<a href="../../../references.it.html#ref-krishnan2022multiagent" role="doc-biblioref">2022</a>)</span>) possono generare automaticamente nuove architetture hardware mutando e mescolando attributi di progettazione come dimensione della cache, numero di unit√† parallele, larghezza di banda della memoria e cos√¨ via. Ci√≤ consente un‚Äôesplorazione efficiente di ampi spazi di progettazione.</li>
<li><strong>Modellazione predittiva per l‚Äôottimizzazione:</strong> I modelli ML possono essere addestrati per prevedere metriche di prestazioni, potenza ed efficienza hardware per una determinata architettura. Questi diventano ‚Äúmodelli surrogati‚Äù <span class="citation" data-cites="krishnan2023archgym">(<a href="../../../references.it.html#ref-krishnan2023archgym" role="doc-biblioref">Krishnan et al. 2023</a>)</span> per una rapida ottimizzazione ed esplorazione dello spazio sostituendo lunghe simulazioni.</li>
<li><strong>Ottimizzazione dell‚Äôacceleratore specializzato:</strong> Per chip specializzati come unit√† di elaborazione tensore per AI, tecniche di ricerca architettura automatizzata basate su algoritmi ML <span class="citation" data-cites="zhang2022fullstack">(<a href="../../../references.it.html#ref-zhang2022fullstack" role="doc-biblioref">D. Zhang et al. 2022</a>)</span> promettono di trovare progetti rapidi ed efficienti.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-kao2020gamma" class="csl-entry" role="listitem">
Kao, Sheng-Chun, e Tushar Krishna. 2020. <span>¬´Gamma: automating the HW mapping of DNN models on accelerators via genetic algorithm¬ª</span>. In <em>Proceedings of the 39th International Conference on Computer-Aided Design</em>, 1‚Äì9. ACM. <a href="https://doi.org/10.1145/3400302.3415639">https://doi.org/10.1145/3400302.3415639</a>.
</div><div id="ref-reagen2017case" class="csl-entry" role="listitem">
Reagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, e David Brooks. 2017. <span>¬´A case for efficient accelerator design space exploration via <span>Bayesian</span> optimization¬ª</span>. In <em>2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)</em>, 1‚Äì6. IEEE; IEEE. <a href="https://doi.org/10.1109/islped.2017.8009208">https://doi.org/10.1109/islped.2017.8009208</a>.
</div><div id="ref-bhardwaj2020comprehensive" class="csl-entry" role="listitem">
Bhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, Jos√© Miguel Hern√°ndez-Lobato, e Gu-Yeon Wei. 2020. <span>¬´A comprehensive methodology to determine optimal coherence interfaces for many-accelerator <span>SoCs</span>¬ª</span>. In <em>Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design</em>, 145‚Äì50. ACM. <a href="https://doi.org/10.1145/3370748.3406564">https://doi.org/10.1145/3370748.3406564</a>.
</div><div id="ref-kao2020confuciux" class="csl-entry" role="listitem">
Kao, Sheng-Chun, Geonhwa Jeong, e Tushar Krishna. 2020. <span>¬´<span>ConfuciuX:</span> <span>Autonomous</span> Hardware Resource Assignment for <span>DNN</span> Accelerators using Reinforcement Learning¬ª</span>. In <em>2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</em>, 622‚Äì36. IEEE; IEEE. <a href="https://doi.org/10.1109/micro50266.2020.00058">https://doi.org/10.1109/micro50266.2020.00058</a>.
</div><div id="ref-krishnan2022multiagent" class="csl-entry" role="listitem">
Krishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang, Izzeddin Gur, Vijay Janapa Reddi, e Aleksandra Faust. 2022. <span>¬´Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration¬ª</span>. <a href="https://arxiv.org/abs/2211.16385">https://arxiv.org/abs/2211.16385</a>.
</div><div id="ref-zhang2022fullstack" class="csl-entry" role="listitem">
Zhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, e Azalia Mirhoseini. 2022. <span>¬´A full-stack search technique for domain optimized deep learning accelerators¬ª</span>. In <em>Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>, 27‚Äì42. ASPLOS ‚Äô22. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3503222.3507767">https://doi.org/10.1145/3503222.3507767</a>.
</div></div><p>I vantaggi dell‚Äôutilizzo di ML includono un‚Äôesplorazione dello spazio di progettazione superiore, ottimizzazione automatizzata e riduzione dello sforzo manuale. Le sfide includono lunghi tempi di training per alcune tecniche e limitazioni degli ottimi locali. Tuttavia, ML per l‚Äôarchitettura hardware ha un grande potenziale per rivelare miglioramenti in termini di prestazioni ed efficienza.</p>
</section>
<section id="ml-per-ottimizzare-la-produzione-e-ridurre-i-difetti" class="level3" data-number="10.9.4">
<h3 data-number="10.9.4" class="anchored" data-anchor-id="ml-per-ottimizzare-la-produzione-e-ridurre-i-difetti"><span class="header-section-number">10.9.4</span> ML per Ottimizzare la Produzione e Ridurre i Difetti</h3>
<p>Una volta completata la progettazione hardware, si passa alla produzione. Tuttavia, variabilit√† e difetti durante la produzione possono influire su rese e qualit√†. Le tecniche ML vengono ora applicate per migliorare i processi di fabbricazione e ridurre i difetti. Ecco alcuni esempi:</p>
<ul>
<li><strong>Manutenzione predittiva:</strong> I modelli ML possono analizzare i dati dei sensori delle apparecchiature nel tempo e identificare segnali che prevedono le esigenze di manutenzione prima del guasto. Ci√≤ consente una manutenzione proattiva, che pu√≤ essere molto utile nel costoso processo di fabbricazione.</li>
<li><strong>Ottimizzazione del processo:</strong> I modelli di apprendimento supervisionato possono essere addestrati sui dati di processo per identificare i fattori che portano a basse rese. I modelli possono quindi ottimizzare i parametri per migliorare rese, produttivit√† o coerenza.</li>
<li><strong>Previsione della resa:</strong> Analizzando i dati di prova da progetti realizzati utilizzando tecniche come alberi di regressione, i modelli ML possono prevedere le rese all‚Äôinizio della produzione, consentendo aggiustamenti del processo.</li>
<li><strong>Rilevamento dei difetti:</strong> Le tecniche di visione artificiale ML possono essere applicate alle immagini dei progetti per identificare difetti invisibili all‚Äôocchio umano. Ci√≤ consente un controllo di qualit√† di precisione e un‚Äôanalisi delle cause principali.</li>
<li><strong>Analisi proattiva dei guasti:</strong> I modelli ML possono aiutare a prevedere, diagnosticare e prevenire i problemi che portano a difetti e guasti a valle analizzando i dati di processo strutturati e non strutturati.</li>
</ul>
<p>L‚Äôapplicazione del ML alla produzione consente l‚Äôottimizzazione dei processi, il controllo di qualit√† in tempo reale, la manutenzione predittiva e rese pi√π elevate. Le sfide includono la gestione di dati di produzione complessi e varianti. Ma il ML √® pronto a trasformare la produzione di semiconduttori.</p>
</section>
<section id="verso-modelli-di-base-per-la-progettazione-hardware" class="level3 page-columns page-full" data-number="10.9.5">
<h3 data-number="10.9.5" class="anchored" data-anchor-id="verso-modelli-di-base-per-la-progettazione-hardware"><span class="header-section-number">10.9.5</span> Verso Modelli di Base per la Progettazione Hardware</h3>
<p>Come abbiamo visto, l‚Äôapprendimento automatico sta aprendo nuove possibilit√† nel flusso di lavoro di progettazione hardware, dalle specifiche alla produzione. Tuttavia, le attuali tecniche di ML hanno ancora una portata limitata e richiedono un‚Äôampia progettazione specifica per dominio. La visione a lungo termine √® lo sviluppo di sistemi di intelligenza artificiale generali che possono essere applicati con versatilit√† in tutte le attivit√† di progettazione hardware.</p>
<p>Per realizzare appieno questa visione, sono necessari investimenti e ricerca per sviluppare modelli di base per la progettazione hardware. Si tratta di modelli e architetture ML unificati e generici che possono apprendere complesse competenze di progettazione hardware con i dati di training e gli obiettivi corretti.</p>
<p>La realizzazione di modelli di base per la progettazione hardware end-to-end richieder√† quanto segue:</p>
<ul>
<li>Accumulare grandi set di dati di alta qualit√† ed etichettati in tutte le fasi di progettazione hardware per addestrare i modelli di base.</li>
<li>Progressi nelle tecniche ML multimodali e multi-task per gestire la diversit√† di dati e attivit√† di progettazione hardware.</li>
<li>Interfacce e layer di astrazione per collegare i modelli di base ai flussi e agli strumenti di progettazione esistenti.</li>
<li>Sviluppo di ambienti di simulazione e benchmark per addestrare e testare i modelli di base sulle capacit√† di progettazione hardware.</li>
<li>Metodi per spiegare e interpretare le decisioni di progettazione dei modelli ML e le ottimizzazioni per attendibilit√† e verifica.</li>
<li>Tecniche di compilazione per ottimizzare i modelli di base per un‚Äôimplementazione efficiente su piattaforme hardware.</li>
</ul>
<p>Sebbene siano ancora in corso ricerche significative, i modelli di base rappresentano l‚Äôobiettivo a lungo termine pi√π trasformativo per l‚Äôinfusione dell‚ÄôIA nel processo della progettazione hardware. Democratizzare la progettazione hardware tramite sistemi ML versatili e automatizzati promette di aprire una nuova era di progettazione di chip ottimizzata, efficiente e innovativa. Il viaggio che ci attende √® pieno di sfide e opportunit√† aperte.</p>
<p>Se sei interessato alla progettazione di architetture per computer assistite da ML <span class="citation" data-cites="krishnan2023archgym">(<a href="../../../references.it.html#ref-krishnan2023archgym" role="doc-biblioref">Krishnan et al. 2023</a>)</span>, invitiamo a leggere <a href="https://www.sigarch.org/architecture-2-0-why-computer-architects-need-a-data-centric-ai-gymnasium/">Architecture 2.0</a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-krishnan2023archgym" class="csl-entry" role="listitem">
Krishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023. <span>¬´<span>ArchGym:</span> <span>An</span> Open-Source Gymnasium for Machine Learning Assisted Architecture Design¬ª</span>. In <em>Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 1‚Äì16. ACM. <a href="https://doi.org/10.1145/3579371.3589049">https://doi.org/10.1145/3579371.3589049</a>.
</div></div><p>In alternativa, si pu√≤ guardare <a href="#vid-arch" class="quarto-xref">Video&nbsp;<span>10.3</span></a> for more details.</p>
<div id="vid-arch" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;10.3: Architecture 2.0
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/F5Eieaz7u1I" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
</section>
<section id="conclusione" class="level2" data-number="10.10">
<h2 data-number="10.10" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">10.10</span> Conclusione</h2>
<p>L‚Äôaccelerazione hardware specializzata √® diventata indispensabile per abilitare applicazioni di intelligenza artificiale performanti ed efficienti, poich√© modelli e set di dati esplodono in complessit√†. Questo capitolo ha esaminato i limiti dei processori generici come le CPU per i carichi di lavoro di intelligenza artificiale. La loro mancanza di parallelismo e di throughput computazionale non consente di addestrare o eseguire rapidamente reti neurali profonde all‚Äôavanguardia. Queste motivazioni hanno guidato le innovazioni negli acceleratori personalizzati.</p>
<p>Abbiamo esaminato GPU, TPU, FPGA e ASIC progettati specificamente per le operazioni matematiche intensive inerenti alle reti neurali. Coprendo questo spettro di opzioni, abbiamo mirato a fornire un framework per ragionare attraverso la selezione dell‚Äôacceleratore in base a vincoli relativi a flessibilit√†, prestazioni, potenza, costi e altri fattori.</p>
<p>Abbiamo anche esplorato il ruolo del software nell‚Äôabilitazione e nell‚Äôottimizzazione attive dell‚Äôaccelerazione dell‚Äôintelligenza artificiale. Ci√≤ abbraccia astrazioni di programmazione, framework, compilatori e simulatori. Abbiamo discusso della progettazione congiunta hardware-software come metodologia proattiva per la creazione di sistemi di intelligenza artificiale pi√π olistici integrando strettamente l‚Äôinnovazione degli algoritmi e i progressi hardware.</p>
<p>Ma c‚Äô√® molto di pi√π in arrivo! Frontiere entusiasmanti come l‚Äôinformatica analogica, le reti neurali ottiche e l‚Äôapprendimento automatico quantistico rappresentano direzioni di ricerca attive che potrebbero sbloccare miglioramenti di ordini di grandezza in termini di efficienza, velocit√† e scala rispetto ai paradigmi attuali.</p>
<p>In definitiva, l‚Äôaccelerazione hardware specializzata rimane indispensabile per sbloccare le prestazioni e l‚Äôefficienza necessarie per soddisfare la promessa dell‚Äôintelligenza artificiale dal cloud all‚Äôedge. Ci auguriamo che questo capitolo fornisca utili informazioni di base e approfondimenti sulla rapida innovazione che si sta verificando in questo dominio.</p>
</section>
<section id="sec-ai-acceleration-resource" class="level2" data-number="10.11">
<h2 data-number="10.11" class="anchored" data-anchor-id="sec-ai-acceleration-resource"><span class="header-section-number">10.11</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><p><a href="#vid-wfscale" class="quarto-xref">Video&nbsp;<span>10.1</span></a></p></li>
<li><p><a href="#vid-snn" class="quarto-xref">Video&nbsp;<span>10.2</span></a></p></li>
<li><p><a href="#vid-arch" class="quarto-xref">Video&nbsp;<span>10.3</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><a href="#exr-tvm" class="quarto-xref">Esercizio&nbsp;<span>10.1</span></a></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/optimizations/optimizations.it.html" class="pagination-link" aria-label="Ottimizzazioni dei Modelli">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/benchmarking/benchmarking.it.html" class="pagination-link" aria-label="Benchmarking dell'IA">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University). Traduzione di <a href="https://github.com/BravoBaldo">Baldassarre Cesarano</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/hw_acceleration/hw_acceleration.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/hw_acceleration/hw_acceleration.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>