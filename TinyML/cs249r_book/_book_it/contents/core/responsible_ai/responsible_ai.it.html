<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; IA Responsabile ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/sustainable_ai/sustainable_ai.it.html" rel="next">
<link href="../../../contents/core/privacy_security/privacy_security.it.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/responsible_ai/responsible_ai.it.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2b8d2ba3a08f8b4ab16660e3d0aa1206" class="alert alert-primary hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>‚≠ê [18 Ott] <b>Abbiamo raggiunto 1.000 stelle GitHub</b> üéâ Grazie a voi, Arduino e SEEED hanno donato kit hardware di IA per <a href="https://tinyml.seas.harvard.edu/4D/pastEvents">per i workshop TinyML</a> nei paesi in via di sviluppo <br> üéì [15 Nov] La <a href="https://www.edgeaifoundation.org/">EDGE AI Foundation</a> <strong>equipara i fondi per borse di studio accademiche</strong> per ogni nuovo GitHub ‚≠ê (fino a 10.000 stelle). <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per supportare!</a> üôè <br> üöÄ <b>La nostra missione. 1 ‚≠ê = 1 üë©‚Äçüéì Studente</b>. Ogni stella racconta una storia: studenti che acquisiscono conoscenze e sostenitori che guidano la missione. Insieme, stiamo facendo la differenza.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/about/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/ai/socratiq.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABORATORI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/part_LABS.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">LABORATORI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/part_nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">part_nicla_vision.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/part_xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">part_xiao_esp32s3.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/part_raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">part_raspi.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/part_shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">part_shared.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#panoramica" id="toc-panoramica" class="nav-link active" data-scroll-target="#panoramica"><span class="header-section-number">15.1</span> Panoramica</a></li>
  <li><a href="#terminologia" id="toc-terminologia" class="nav-link" data-scroll-target="#terminologia"><span class="header-section-number">15.2</span> Terminologia</a></li>
  <li><a href="#principi-e-concetti" id="toc-principi-e-concetti" class="nav-link" data-scroll-target="#principi-e-concetti"><span class="header-section-number">15.3</span> Principi e Concetti</a>
  <ul>
  <li><a href="#trasparenza-e-spiegabilit√†" id="toc-trasparenza-e-spiegabilit√†" class="nav-link" data-scroll-target="#trasparenza-e-spiegabilit√†"><span class="header-section-number">15.3.1</span> Trasparenza e Spiegabilit√†</a></li>
  <li><a href="#equit√†-bias-pregiudizi-e-discriminazione" id="toc-equit√†-bias-pregiudizi-e-discriminazione" class="nav-link" data-scroll-target="#equit√†-bias-pregiudizi-e-discriminazione"><span class="header-section-number">15.3.2</span> Equit√†, Bias [pregiudizi] e Discriminazione</a></li>
  <li><a href="#privacy-e-governance-dei-dati" id="toc-privacy-e-governance-dei-dati" class="nav-link" data-scroll-target="#privacy-e-governance-dei-dati"><span class="header-section-number">15.3.3</span> Privacy e Governance dei Dati</a></li>
  <li><a href="#sicurezza-e-robustezza" id="toc-sicurezza-e-robustezza" class="nav-link" data-scroll-target="#sicurezza-e-robustezza"><span class="header-section-number">15.3.4</span> Sicurezza e Robustezza</a></li>
  <li><a href="#responsabilit√†-e-governance" id="toc-responsabilit√†-e-governance" class="nav-link" data-scroll-target="#responsabilit√†-e-governance"><span class="header-section-number">15.3.5</span> Responsabilit√† e Governance</a></li>
  </ul></li>
  <li><a href="#cloud-edge-e-tiny-ml" id="toc-cloud-edge-e-tiny-ml" class="nav-link" data-scroll-target="#cloud-edge-e-tiny-ml"><span class="header-section-number">15.4</span> Cloud, Edge e Tiny ML</a>
  <ul>
  <li><a href="#spiegabilit√†" id="toc-spiegabilit√†" class="nav-link" data-scroll-target="#spiegabilit√†"><span class="header-section-number">15.4.1</span> Spiegabilit√†</a></li>
  <li><a href="#equit√†" id="toc-equit√†" class="nav-link" data-scroll-target="#equit√†"><span class="header-section-number">15.4.2</span> Equit√†</a></li>
  <li><a href="#privacy" id="toc-privacy" class="nav-link" data-scroll-target="#privacy"><span class="header-section-number">15.4.3</span> Privacy</a></li>
  <li><a href="#sicurezza" id="toc-sicurezza" class="nav-link" data-scroll-target="#sicurezza"><span class="header-section-number">15.4.4</span> Sicurezza</a></li>
  <li><a href="#responsabilit√†" id="toc-responsabilit√†" class="nav-link" data-scroll-target="#responsabilit√†"><span class="header-section-number">15.4.5</span> Responsabilit√†</a></li>
  <li><a href="#governance" id="toc-governance" class="nav-link" data-scroll-target="#governance"><span class="header-section-number">15.4.6</span> Governance</a></li>
  <li><a href="#riepilogo" id="toc-riepilogo" class="nav-link" data-scroll-target="#riepilogo"><span class="header-section-number">15.4.7</span> Riepilogo</a></li>
  </ul></li>
  <li><a href="#aspetti-tecnici" id="toc-aspetti-tecnici" class="nav-link" data-scroll-target="#aspetti-tecnici"><span class="header-section-number">15.5</span> Aspetti Tecnici</a>
  <ul>
  <li><a href="#rilevamento-e-mitigazione-dei-pregiudizi" id="toc-rilevamento-e-mitigazione-dei-pregiudizi" class="nav-link" data-scroll-target="#rilevamento-e-mitigazione-dei-pregiudizi"><span class="header-section-number">15.5.1</span> Rilevamento e Mitigazione dei Pregiudizi</a>
  <ul class="collapse">
  <li><a href="#il-contesto-√®-importante" id="toc-il-contesto-√®-importante" class="nav-link" data-scroll-target="#il-contesto-√®-importante">Il Contesto √® Importante</a></li>
  <li><a href="#distribuzione-ponderata" id="toc-distribuzione-ponderata" class="nav-link" data-scroll-target="#distribuzione-ponderata">Distribuzione Ponderata</a></li>
  </ul></li>
  <li><a href="#preservare-la-privacy" id="toc-preservare-la-privacy" class="nav-link" data-scroll-target="#preservare-la-privacy"><span class="header-section-number">15.5.2</span> Preservare la Privacy</a></li>
  <li><a href="#machine-unlearning" id="toc-machine-unlearning" class="nav-link" data-scroll-target="#machine-unlearning"><span class="header-section-number">15.5.3</span> Machine Unlearning</a></li>
  <li><a href="#esempi-avversari-e-robustezza" id="toc-esempi-avversari-e-robustezza" class="nav-link" data-scroll-target="#esempi-avversari-e-robustezza"><span class="header-section-number">15.5.4</span> Esempi Avversari e Robustezza</a></li>
  <li><a href="#creazione-di-modelli-interpretabili" id="toc-creazione-di-modelli-interpretabili" class="nav-link" data-scroll-target="#creazione-di-modelli-interpretabili"><span class="header-section-number">15.5.5</span> Creazione di Modelli Interpretabili</a>
  <ul class="collapse">
  <li><a href="#spiegabilit√†-post-hoc" id="toc-spiegabilit√†-post-hoc" class="nav-link" data-scroll-target="#spiegabilit√†-post-hoc">Spiegabilit√† Post Hoc</a></li>
  <li><a href="#interpretabilit√†-intrinseca" id="toc-interpretabilit√†-intrinseca" class="nav-link" data-scroll-target="#interpretabilit√†-intrinseca">Interpretabilit√† Intrinseca</a></li>
  <li><a href="#interpretabilit√†-meccanicistica" id="toc-interpretabilit√†-meccanicistica" class="nav-link" data-scroll-target="#interpretabilit√†-meccanicistica">Interpretabilit√† Meccanicistica</a></li>
  <li><a href="#sfide-e-considerazioni" id="toc-sfide-e-considerazioni" class="nav-link" data-scroll-target="#sfide-e-considerazioni">Sfide e Considerazioni</a></li>
  </ul></li>
  <li><a href="#monitoraggio-delle-prestazioni-del-modello" id="toc-monitoraggio-delle-prestazioni-del-modello" class="nav-link" data-scroll-target="#monitoraggio-delle-prestazioni-del-modello"><span class="header-section-number">15.5.6</span> Monitoraggio delle Prestazioni del Modello</a></li>
  </ul></li>
  <li><a href="#sfide-di-implementazione" id="toc-sfide-di-implementazione" class="nav-link" data-scroll-target="#sfide-di-implementazione"><span class="header-section-number">15.6</span> Sfide di Implementazione</a>
  <ul>
  <li><a href="#strutture-organizzative-e-culturali" id="toc-strutture-organizzative-e-culturali" class="nav-link" data-scroll-target="#strutture-organizzative-e-culturali"><span class="header-section-number">15.6.1</span> Strutture Organizzative e Culturali</a></li>
  <li><a href="#ottenere-dati-di-qualit√†-e-rappresentativi" id="toc-ottenere-dati-di-qualit√†-e-rappresentativi" class="nav-link" data-scroll-target="#ottenere-dati-di-qualit√†-e-rappresentativi"><span class="header-section-number">15.6.2</span> Ottenere Dati di Qualit√† e Rappresentativi</a>
  <ul class="collapse">
  <li><a href="#squilibrio-dei-sottogruppi" id="toc-squilibrio-dei-sottogruppi" class="nav-link" data-scroll-target="#squilibrio-dei-sottogruppi">Squilibrio dei Sottogruppi</a></li>
  <li><a href="#quantificazione-dei-risultati-target" id="toc-quantificazione-dei-risultati-target" class="nav-link" data-scroll-target="#quantificazione-dei-risultati-target">Quantificazione dei Risultati Target</a></li>
  <li><a href="#spostamento-della-distribuzione" id="toc-spostamento-della-distribuzione" class="nav-link" data-scroll-target="#spostamento-della-distribuzione">Spostamento della Distribuzione</a></li>
  <li><a href="#raccolta-dati" id="toc-raccolta-dati" class="nav-link" data-scroll-target="#raccolta-dati">Raccolta Dati</a></li>
  </ul></li>
  <li><a href="#bilanciamento-di-accuratezza-e-altri-obiettivi" id="toc-bilanciamento-di-accuratezza-e-altri-obiettivi" class="nav-link" data-scroll-target="#bilanciamento-di-accuratezza-e-altri-obiettivi"><span class="header-section-number">15.6.3</span> Bilanciamento di Accuratezza e Altri Obiettivi</a></li>
  </ul></li>
  <li><a href="#considerazioni-etiche-nella-progettazione-dellia" id="toc-considerazioni-etiche-nella-progettazione-dellia" class="nav-link" data-scroll-target="#considerazioni-etiche-nella-progettazione-dellia"><span class="header-section-number">15.7</span> Considerazioni Etiche Nella Progettazione dell‚ÄôIA</a>
  <ul>
  <li><a href="#sicurezza-dellintelligenza-artificiale-e-allineamento-dei-valori" id="toc-sicurezza-dellintelligenza-artificiale-e-allineamento-dei-valori" class="nav-link" data-scroll-target="#sicurezza-dellintelligenza-artificiale-e-allineamento-dei-valori"><span class="header-section-number">15.7.1</span> Sicurezza dell‚ÄôIntelligenza Artificiale e Allineamento dei Valori</a></li>
  <li><a href="#sistemi-autonomi-e-controllo-e-fiducia" id="toc-sistemi-autonomi-e-controllo-e-fiducia" class="nav-link" data-scroll-target="#sistemi-autonomi-e-controllo-e-fiducia"><span class="header-section-number">15.7.2</span> Sistemi Autonomi e Controllo [e Fiducia]</a></li>
  <li><a href="#impatti-economici-su-posti-di-lavoro-competenze-salari" id="toc-impatti-economici-su-posti-di-lavoro-competenze-salari" class="nav-link" data-scroll-target="#impatti-economici-su-posti-di-lavoro-competenze-salari"><span class="header-section-number">15.7.3</span> Impatti Economici su Posti di Lavoro, Competenze, Salari</a></li>
  <li><a href="#comunicazione-scientifica-e-alfabetizzazione-ia" id="toc-comunicazione-scientifica-e-alfabetizzazione-ia" class="nav-link" data-scroll-target="#comunicazione-scientifica-e-alfabetizzazione-ia"><span class="header-section-number">15.7.4</span> Comunicazione Scientifica e Alfabetizzazione IA</a></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">15.8</span> Conclusione</a></li>
  <li><a href="#sec-responsible-ai-resource" id="toc-sec-responsible-ai-resource" class="nav-link" data-scroll-target="#sec-responsible-ai-resource"><span class="header-section-number">15.9</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/responsible_ai/responsible_ai.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/responsible_ai/responsible_ai.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-responsible_ai" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-responsible-ai-resource">Slide</a>, <a href="#sec-responsible-ai-resource">Video</a>, <a href="#sec-responsible-ai-resource">Esercizi</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_responsible_ai.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Illustrazione di un‚ÄôIA responsabile in un contesto futuristico con l‚Äôuniverso sullo sfondo: Una o pi√π mani umane che coltivano una piantina che cresce in un albero di IA, che simboleggia una rete neurale. L‚Äôalbero ha rami e foglie digitali, che ricordano una rete neurale, per rappresentare la natura interconnessa dell‚ÄôIA. Lo sfondo raffigura un universo futuro in cui esseri umani e animali con intelligenza generale collaborano armoniosamente. La scena cattura la coltivazione iniziale dell‚ÄôIA come piantina, sottolineando lo sviluppo etico della tecnologia di IA in armonia con l‚Äôumanit√† e l‚Äôuniverso.</em></figcaption>
</figure>
</div>
<p>Man mano che i modelli di apprendimento automatico crescono in vari domini, questi algoritmi hanno il potenziale per perpetuare pregiudizi storici, violare la privacy o abilitare decisioni automatizzate non etiche se sviluppati senza un‚Äôattenta considerazione dei loro impatti sociali. Anche i sistemi creati con buone intenzioni possono in ultima analisi discriminare determinati gruppi demografici, abilitare la sorveglianza o mancare di trasparenza nei loro comportamenti e processi decisionali. Pertanto, gli ingegneri e le aziende che si occupano di apprendimento automatico hanno la responsabilit√† etica di garantire in modo proattivo che i principi di equit√†, responsabilit√†, sicurezza e trasparenza siano rispecchiati nei loro modelli, per prevenire danni e creare fiducia nel pubblico.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere i principi fondamentali e le motivazioni dell‚ÄôIA responsabile, tra cui correttezza, trasparenza, privacy, sicurezza e responsabilit√†.</p></li>
<li><p>Imparare metodi tecnici per implementare i principi dell‚ÄôIA responsabile, come rilevare pregiudizi nei set di dati, creare modelli interpretabili, aggiungere rumore per la privacy e testare la robustezza del modello.</p></li>
<li><p>Riconoscere le sfide organizzative e sociali per raggiungere l‚ÄôIA responsabile, tra cui qualit√† dei dati, obiettivi del modello, comunicazione e impatti sul lavoro.</p></li>
<li><p>Conoscenza di quadri etici e considerazioni per i sistemi di IA, che spaziano dalla sicurezza dell‚ÄôIA, all‚Äôautonomia umana e alle conseguenze economiche.</p></li>
<li><p>Apprezzare la maggiore complessit√† e i costi dello sviluppo di sistemi di IA etici e affidabili rispetto all‚ÄôIA senza principi.</p></li>
</ul>
</div>
</div>
<section id="panoramica" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="panoramica"><span class="header-section-number">15.1</span> Panoramica</h2>
<p>I modelli di apprendimento automatico sono sempre pi√π utilizzati per automatizzare le decisioni in ambiti sociali ad alto rischio come sanit√†, giustizia penale e occupazione. Tuttavia, senza un‚Äôattenzione deliberata, questi algoritmi possono perpetuare pregiudizi, violare la privacy o causare altri danni. Ad esempio, un modello di approvazione di prestiti addestrato esclusivamente su dati provenienti da quartieri ad alto reddito potrebbe svantaggiare i richiedenti provenienti da aree a basso reddito. Ci√≤ motiva la necessit√† di un apprendimento automatico responsabile, ovvero la creazione di modelli equi, responsabili, trasparenti ed etici.</p>
<p>Diversi principi fondamentali sono alla base di un apprendimento automatico responsabile. L‚Äôequit√† garantisce che i modelli non discriminino in base a genere, razza, et√† e altri attributi. La spiegabilit√† consente agli esseri umani di interpretare i comportamenti del modello e migliorare la trasparenza. Le tecniche di robustezza e sicurezza prevengono vulnerabilit√† come gli esempi avversari. Test e convalide rigorosi aiutano a ridurre le debolezze indesiderate del modello o gli effetti collaterali.</p>
<p>L‚Äôimplementazione di un apprendimento automatico responsabile presenta sfide sia tecniche che etiche. Gli sviluppatori devono confrontarsi con la definizione matematica dell‚Äôequit√†, bilanciando obiettivi concorrenti come accuratezza e interpretabilit√† e assicurando dati di training di qualit√†. Le organizzazioni devono anche allineare incentivi, politiche e cultura per sostenere l‚ÄôIA etica.</p>
<p>Questo capitolo fornir√† gli strumenti per valutare criticamente i sistemi di IA e contribuire allo sviluppo di applicazioni di apprendimento automatico utili ed etiche, coprendo le basi, i metodi e le implicazioni nel mondo reale dell‚ÄôML responsabile. I principi dell‚ÄôML responsabile discussi sono conoscenze cruciali poich√© gli algoritmi mediano pi√π aspetti della societ√† umana.</p>
</section>
<section id="terminologia" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="terminologia"><span class="header-section-number">15.2</span> Terminologia</h2>
<p>L‚ÄôIA responsabile riguarda lo sviluppo di un‚ÄôIA che abbia un impatto positivo sulla societ√† in base all‚Äôetica e ai valori umani. Non esiste una definizione universalmente accettata di ‚ÄúIA responsabile‚Äù, ma ecco un riassunto di come viene comunemente descritta. L‚ÄôIA responsabile si riferisce alla progettazione, allo sviluppo e all‚Äôimplementazione di sistemi di intelligenza artificiale in modo etico e socialmente utile. L‚Äôobiettivo principale √® creare un‚ÄôIA affidabile, imparziale, equa, trasparente, responsabile e sicura. Sebbene non esista una definizione canonica, si ritiene generalmente che l‚ÄôIA responsabile comprenda principi quali:</p>
<ul>
<li><p><strong>Equit√†:</strong> Evitare pregiudizi, discriminazioni e potenziali danni a determinati gruppi o popolazioni</p></li>
<li><p><strong>Spiegabilit√†:</strong> Consentire agli esseri umani di comprendere e interpretare il modo in cui i modelli di IA prendono decisioni</p></li>
<li><p><strong>Trasparenza:</strong> Comunicare apertamente come funzionano, sono costruiti e valutati i sistemi di IA</p></li>
<li><p><strong>Responsabilit√†:</strong> Avere processi per determinare responsabilit√† e obblighi per guasti o impatti negativi dell‚ÄôIA</p></li>
<li><p><strong>Robustezza:</strong> Garantire che i sistemi di IA siano sicuri, affidabili e si comportino come previsto</p></li>
<li><p><strong>Privacy:</strong> Proteggere i dati sensibili degli utenti e rispettare le leggi e l‚Äôetica sulla privacy</p></li>
</ul>
<p>Mettere in pratica questi principi implica tecniche tecniche, politiche aziendali, quadri di governance e filosofia morale. Sono inoltre in corso dibattiti sulla definizione di concetti ambigui come l‚Äôequit√† e sulla determinazione di come bilanciare obiettivi in competizione.</p>
</section>
<section id="principi-e-concetti" class="level2 page-columns page-full" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="principi-e-concetti"><span class="header-section-number">15.3</span> Principi e Concetti</h2>
<section id="trasparenza-e-spiegabilit√†" class="level3" data-number="15.3.1">
<h3 data-number="15.3.1" class="anchored" data-anchor-id="trasparenza-e-spiegabilit√†"><span class="header-section-number">15.3.1</span> Trasparenza e Spiegabilit√†</h3>
<p>I modelli di apprendimento automatico sono spesso criticati come misteriose ‚Äúscatole nere‚Äù, sistemi opachi in cui non √® chiaro come siano arrivati a particolari previsioni o decisioni. Ad esempio, un sistema di intelligenza artificiale chiamato <a href="https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx">COMPAS</a> utilizzato per valutare il rischio di recidiva criminale negli Stati Uniti si √® rivelato razzialmente discriminatorio nei confronti degli imputati neri. Tuttavia, l‚Äôopacit√† dell‚Äôalgoritmo ha reso difficile comprendere e risolvere il problema. Questa mancanza di trasparenza pu√≤ nascondere pregiudizi, errori e carenze.</p>
<p>Spiegare i comportamenti del modello aiuta a generare fiducia da parte del pubblico e degli esperti del settore e consente di identificare i problemi da affrontare. Le tecniche di interpretabilit√† svolgono un ruolo chiave in questo processo. Ad esempio, <a href="https://homes.cs.washington.edu/~marcotcr/blog/lime/">LIME</a> (Local Interpretable Model-Agnostic Explanations) evidenzia come le singole funzionalit√† di input contribuiscano a una previsione specifica, mentre i valori Shapley quantificano il contributo di ciascuna funzionalit√† all‚Äôoutput di un modello in base alla teoria dei giochi cooperativi. Le ‚Äúmappe di salienza‚Äù, comunemente utilizzate nei modelli basati su immagini, evidenziano visivamente le aree di un‚Äôimmagine che hanno maggiormente influenzato la decisione del modello. Questi strumenti consentono agli utenti di comprendere la logica del modello.</p>
<p>Oltre ai vantaggi pratici, la trasparenza √® sempre pi√π richiesta dalla legge. Regolamenti come l‚Äô‚ÄúEuropean Union‚Äôs General Data Protection Regulation (<a href="https://gdpr.eu/tag/gdpr/">GDPR</a>)‚Äù dell‚ÄôUnione Europea impongono alle organizzazioni di fornire spiegazioni per determinate decisioni automatizzate, soprattutto quando hanno un impatto significativo sugli individui. Ci√≤ rende la spiegabilit√† non solo una buona pratica, ma una necessit√† legale in alcuni contesti. Insieme, trasparenza e spiegabilit√† costituiscono pilastri fondamentali per la creazione di sistemi di IA responsabili e affidabili.</p>
</section>
<section id="equit√†-bias-pregiudizi-e-discriminazione" class="level3 page-columns page-full" data-number="15.3.2">
<h3 data-number="15.3.2" class="anchored" data-anchor-id="equit√†-bias-pregiudizi-e-discriminazione"><span class="header-section-number">15.3.2</span> Equit√†, Bias [pregiudizi] e Discriminazione</h3>
<p>I modelli di ML addestrati su dati storicamente distorti spesso perpetuano e amplificano tali pregiudizi. √à stato dimostrato che gli algoritmi sanitari svantaggiano i pazienti neri sottostimandone le esigenze <span class="citation" data-cites="obermeyer2019dissecting">(<a href="../../../references.it.html#ref-obermeyer2019dissecting" role="doc-biblioref">Obermeyer et al. 2019</a>)</span>. Il riconoscimento facciale deve essere pi√π accurato per le donne e le persone di colore. Tale discriminazione algoritmica pu√≤ avere un impatto negativo profondo sulla vita delle persone.</p>
<div class="no-row-height column-margin column-container"><div id="ref-obermeyer2019dissecting" class="csl-entry" role="listitem">
Obermeyer, Ziad, Brian Powers, Christine Vogeli, e Sendhil Mullainathan. 2019. <span>¬´Dissecting racial bias in an algorithm used to manage the health of populations¬ª</span>. <em>Science</em> 366 (6464): 447‚Äì53. <a href="https://doi.org/10.1126/science.aax2342">https://doi.org/10.1126/science.aax2342</a>.
</div></div><p>Esistono anche diverse prospettive filosofiche sull‚Äôequit√†, ad esempio, √® pi√π giusto trattare tutti gli individui allo stesso modo o cercare di ottenere risultati uguali per i gruppi? Garantire l‚Äôequit√† richiede di rilevare e mitigare in modo proattivo i pregiudizi nei dati e nei modelli. Tuttavia, raggiungere l‚Äôequit√† perfetta √® tremendamente difficile a causa di definizioni matematiche e prospettive etiche contrastanti. Tuttavia, promuovere l‚Äôequit√† algoritmica e la non discriminazione √® una responsabilit√† fondamentale nello sviluppo dell‚Äôintelligenza artificiale.</p>
</section>
<section id="privacy-e-governance-dei-dati" class="level3" data-number="15.3.3">
<h3 data-number="15.3.3" class="anchored" data-anchor-id="privacy-e-governance-dei-dati"><span class="header-section-number">15.3.3</span> Privacy e Governance dei Dati</h3>
<p>Mantenere la privacy degli individui √® un obbligo etico e un requisito legale per le organizzazioni che implementano sistemi di intelligenza artificiale. Regolamentazioni come il GDPR dell‚ÄôUE impongono protezioni e diritti sulla privacy dei dati, come la possibilit√† di accedere ed eliminare i propri dati.</p>
<p>Tuttavia, massimizzare l‚Äôutilit√† e l‚Äôaccuratezza dei dati per i modelli di addestramento pu√≤ entrare in conflitto con la tutela della privacy: la modellazione della progressione della malattia potrebbe trarre vantaggio dall‚Äôaccesso ai genomi completi dei pazienti, ma la condivisione di tali dati viola ampiamente la privacy.</p>
<p>Una governance dei dati responsabile implica l‚Äôanonimizzazione attenta dei dati, il controllo dell‚Äôaccesso tramite crittografia, l‚Äôottenimento del consenso informato degli interessati e la raccolta dei dati minimi necessari. Rispettare la privacy √® difficile ma fondamentale man mano che le capacit√† e l‚Äôadozione dell‚Äôintelligenza artificiale si espandono.</p>
</section>
<section id="sicurezza-e-robustezza" class="level3" data-number="15.3.4">
<h3 data-number="15.3.4" class="anchored" data-anchor-id="sicurezza-e-robustezza"><span class="header-section-number">15.3.4</span> Sicurezza e Robustezza</h3>
<p>Mettere in funzione i sistemi di intelligenza artificiale nel mondo reale richiede di garantire che siano sicuri, affidabili e robusti, soprattutto per gli scenari di interazione umana. Le auto a guida autonoma di <a href="https://www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html">Uber</a> e <a href="https://www.washingtonpost.com/technology/2022/06/15/tesla-autopilot-crashes/">Tesla</a> sono state coinvolte in incidenti mortali a causa di comportamenti non sicuri.</p>
<p>Gli attacchi avversari che alterano in modo sottile i dati di input possono anche ingannare i modelli ML e causare guasti pericolosi se i sistemi non sono resistenti. I deepfake rappresentano un‚Äôaltra area di minaccia emergente.</p>
<p><a href="#vid-fakeobama" class="quarto-xref">Video&nbsp;<span>15.1</span></a> √® un video deepfake di Barack Obama che √® diventato virale qualche anno fa.</p>
<div id="vid-fakeobama" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;15.1: Fake Obama
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AmUC4m6w1wo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
<p>La promozione della sicurezza richiede test approfonditi, analisi dei rischi, supervisione umana e progettazione di sistemi che combinano pi√π modelli deboli per evitare singoli punti di errore. Rigorosi meccanismi di sicurezza sono essenziali per l‚Äôimplementazione responsabile di un‚ÄôIA efficiente.</p>
</section>
<section id="responsabilit√†-e-governance" class="level3" data-number="15.3.5">
<h3 data-number="15.3.5" class="anchored" data-anchor-id="responsabilit√†-e-governance"><span class="header-section-number">15.3.5</span> Responsabilit√† e Governance</h3>
<p>Quando i sistemi di IA alla fine falliscono o producono risultati dannosi, devono esistere meccanismi per affrontare i problemi risultanti, risarcire le parti interessate e assegnare la responsabilit√†. Sia le politiche di responsabilit√† aziendale che le normative governative sono indispensabili per una governance responsabile dell‚ÄôIA. Ad esempio, l‚Äô<a href="https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID@15&amp;ChapterIDh">Artificial Intelligence Video Interview Act dell‚ÄôIllinois</a> richiede alle aziende di divulgare e ottenere il consenso per l‚Äôanalisi video dell‚ÄôIA, promuovendo la responsabilit√†.</p>
<p>Senza una chiara responsabilit√†, anche i danni causati involontariamente potrebbero rimanere irrisolti, alimentando ulteriormente l‚Äôindignazione e la sfiducia pubblica. I comitati di vigilanza, le valutazioni di impatto, i processi di risoluzione dei reclami e gli audit indipendenti promuovono lo sviluppo e l‚Äôimplementazione responsabili.</p>
</section>
</section>
<section id="cloud-edge-e-tiny-ml" class="level2 page-columns page-full" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="cloud-edge-e-tiny-ml"><span class="header-section-number">15.4</span> Cloud, Edge e Tiny ML</h2>
<p>Sebbene questi principi siano ampiamente applicabili a tutti i sistemi di intelligenza artificiale, alcune considerazioni di IA responsabile sono uniche o pronunciate quando si ha a che fare con l‚Äôapprendimento automatico su dispositivi embedded rispetto alla modellazione tradizionale basata su server. Pertanto, presentiamo una tassonomia di alto livello che confronta le considerazioni di intelligenza artificiale responsabile nei sistemi cloud, edge e TinyML.</p>
<section id="spiegabilit√†" class="level3 page-columns page-full" data-number="15.4.1">
<h3 data-number="15.4.1" class="anchored" data-anchor-id="spiegabilit√†"><span class="header-section-number">15.4.1</span> Spiegabilit√†</h3>
<p>Per l‚Äôapprendimento automatico basato su cloud, le tecniche di spiegabilit√† possono sfruttare risorse di elaborazione significative, consentendo metodi complessi come valori SHAP o approcci basati sul campionamento per interpretare i comportamenti del modello. Ad esempio, il toolkit <a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/05/InterpretML-Whitepaper.pdf">InterpretML di Microsoft</a> fornisce tecniche di spiegabilit√† su misura per gli ambienti cloud.</p>
<p>Tuttavia, l‚Äôedge ML opera su dispositivi con risorse limitate, richiedendo metodi di spiegabilit√† pi√π leggeri che possono essere eseguiti localmente senza latenza eccessiva. Tecniche come LIME <span class="citation" data-cites="ribeiro2016should">(<a href="../../../references.it.html#ref-ribeiro2016should" role="doc-biblioref">Ribeiro, Singh, e Guestrin 2016</a>)</span> approssimano le spiegazioni del modello utilizzando modelli lineari o alberi decisionali per evitare calcoli costosi, il che le rende ideali per dispositivi con risorse limitate. Tuttavia, LIME richiede l‚Äôaddestramento di centinaia o persino migliaia di modelli per generare buone spiegazioni, il che √® spesso irrealizzabile dati i vincoli dell‚Äôedge computing. Al contrario, i metodi basati sulla salienza sono spesso molto pi√π rapidi nella pratica, richiedendo solo un singolo passaggio in avanti attraverso la rete per stimare l‚Äôimportanza delle funzionalit√†. Questa maggiore efficienza rende tali metodi pi√π adatti ai dispositivi edge con risorse di elaborazione limitate, in cui le spiegazioni a bassa latenza sono fondamentali.</p>
<div class="no-row-height column-margin column-container"></div><p>Date le ridotte capacit√† hardware, i sistemi embedded pongono le sfide pi√π significative per la spiegabilit√†. Modelli pi√π compatti e dati limitati semplificano la trasparenza intrinseca del modello. Spiegare le decisioni potrebbe non essere fattibile su microcontrollori di grandi dimensioni e con potenza ottimizzata. Il programma <a href="https://www.darpa.mil/program/transparent-computing">Transparent Computing</a> della DARPA cerca di sviluppare una spiegabilit√† con costi di gestione estremamente bassi, in particolare per i dispositivi TinyML come sensori e dispositivi indossabili.</p>
</section>
<section id="equit√†" class="level3" data-number="15.4.2">
<h3 data-number="15.4.2" class="anchored" data-anchor-id="equit√†"><span class="header-section-number">15.4.2</span> Equit√†</h3>
<p>Per il machine learning nel cloud, vasti set di dati e potenza di calcolo consentono di rilevare pregiudizi su grandi popolazioni eterogenee e di mitigarli tramite tecniche come la riponderazione dei campioni di dati. Tuttavia, i pregiudizi possono emergere dagli ampi dati comportamentali utilizzati per addestrare i modelli cloud. Il framework Fairness Flow di Amazon aiuta a valutare l‚Äôequit√† del ML cloud.</p>
<p>Edge ML si basa su dati limitati sul dispositivo, rendendo pi√π difficile l‚Äôanalisi dei pregiudizi tra gruppi diversi. Tuttavia, i dispositivi edge interagiscono strettamente con gli individui, offrendo un‚Äôopportunit√† di adattamento locale per l‚Äôequit√†. <a href="https://blog.research.google/2017/04/federated-learning-collaborative.html">Federated Learning di Google</a> distribuisce l‚Äôaddestramento del modello tra i dispositivi per incorporare le differenze individuali.</p>
<p>TinyML pone sfide uniche per l‚Äôequit√† con hardware specializzato altamente disperso e dati di addestramento minimi. I test sui pregiudizi sono difficili su dispositivi diversi. La raccolta di dati rappresentativi da molti dispositivi per mitigare i pregiudizi presenta ostacoli di scala e privacy. Gli sforzi di <a href="https://www.darpa.mil/news-events/2022-06-03">Assured Neuro Symbolic Learning and Reasoning (ANSR) di DARPA</a> sono orientati allo sviluppo di tecniche di equit√† dati i vincoli hardware estremi.</p>
</section>
<section id="privacy" class="level3" data-number="15.4.3">
<h3 data-number="15.4.3" class="anchored" data-anchor-id="privacy"><span class="header-section-number">15.4.3</span> Privacy</h3>
<p>Per il cloud ML, grandi quantit√† di dati utente sono concentrate nel cloud, creando rischi di esposizione tramite violazioni. Le tecniche di privacy differenziali aggiungono rumore ai dati cloud per preservare la privacy. Rigidi controlli di accesso e crittografia proteggono i dati cloud a riposo e in transito.</p>
<p>Edge ML sposta l‚Äôelaborazione dei dati sui dispositivi utente, riducendo la raccolta di dati aggregati ma aumentando la potenziale sensibilit√† poich√© i dati personali risiedono sul dispositivo. Apple utilizza ML on-device e privacy differenziale per addestrare modelli riducendo al minimo la condivisione dei dati. L‚Äôanonimizzazione dei dati e le enclave sicure proteggono i dati on-device.</p>
<p>TinyML distribuisce i dati su molti dispositivi con risorse limitate, rendendo improbabili le violazioni centralizzate e rendendo difficile l‚Äôanonimizzazione su larga scala. La minimizzazione dei dati e l‚Äôutilizzo di dispositivi edge come intermediari aiutano la privacy di TinyML.</p>
<p>Quindi, mentre il cloud ML deve proteggere dati centralizzati espansivi, l‚Äôedge ML protegge i dati sensibili on-device e TinyML mira a una condivisione minima dei dati distribuiti a causa dei vincoli. Mentre la privacy √® fondamentale in tutto, le tecniche devono adattarsi all‚Äôambiente. La comprensione delle sfumature consente di selezionare approcci appropriati per la tutela della privacy.</p>
</section>
<section id="sicurezza" class="level3" data-number="15.4.4">
<h3 data-number="15.4.4" class="anchored" data-anchor-id="sicurezza"><span class="header-section-number">15.4.4</span> Sicurezza</h3>
<p>I principali rischi per la sicurezza del cloud ML includono hacking dei modelli, avvelenamento dei dati e malware che interrompono i servizi cloud. Le tecniche di robustezza come l‚Äôaddestramento avversario, il rilevamento delle anomalie e i modelli diversificati mirano a rafforzare il cloud ML contro gli attacchi. La ridondanza pu√≤ aiutare a prevenire singoli punti di errore.</p>
<p>Edge ML e TinyML interagiscono con il mondo fisico, quindi l‚Äôaffidabilit√† e la convalida della sicurezza sono fondamentali. Piattaforme di test rigorose come <a href="https://www.foretellix.com/">Foretellix</a> generano sinteticamente scenari edge per convalidare la sicurezza. La sicurezza di TinyML √® amplificata da dispositivi autonomi con supervisione limitata. La sicurezza di TinyML spesso si basa sul coordinamento collettivo: sciami di droni mantengono la sicurezza tramite ridondanza. Anche le barriere di controllo fisiche limitano i comportamenti non sicuri dei dispositivi TinyML.</p>
<p>Le considerazioni sulla sicurezza variano notevolmente tra i domini, riflettendo le loro sfide uniche. Cloud ML si concentra sulla protezione da hacking e violazioni dei dati, Edge ML enfatizza l‚Äôaffidabilit√† grazie alle sue interazioni fisiche con l‚Äôambiente e TinyML spesso si basa sul coordinamento distribuito per mantenere la sicurezza nei sistemi autonomi. Riconoscere queste sfumature √® essenziale per applicare le tecniche di sicurezza appropriate a ciascun dominio.</p>
</section>
<section id="responsabilit√†" class="level3" data-number="15.4.5">
<h3 data-number="15.4.5" class="anchored" data-anchor-id="responsabilit√†"><span class="header-section-number">15.4.5</span> Responsabilit√†</h3>
<p>La responsabilit√† di Cloud ML si concentra su pratiche aziendali come comitati AI responsabili, carte etiche e processi per affrontare incidenti dannosi. Audit di terze parti e supervisione governativa esterna promuovono la responsabilit√† di Cloud ML.</p>
<p>La responsabilit√† di Edge ML √® pi√π complessa con dispositivi distribuiti e frammentazione della supply chain. Le aziende sono responsabili dei dispositivi, ma i componenti provengono da vari fornitori. Gli standard di settore aiutano a coordinare la responsabilit√† di Edge ML tra le parti interessate.</p>
<p>Con TinyML, i meccanismi di responsabilit√† devono essere tracciati attraverso lunghe e complesse supply chain di circuiti integrati, sensori e altro hardware. Gli schemi di certificazione TinyML aiutano a tracciare la provenienza dei componenti. Le associazioni di categoria dovrebbero idealmente promuovere la responsabilit√† condivisa per TinyML etico.</p>
</section>
<section id="governance" class="level3" data-number="15.4.6">
<h3 data-number="15.4.6" class="anchored" data-anchor-id="governance"><span class="header-section-number">15.4.6</span> Governance</h3>
<p>Le organizzazioni istituiscono una governance interna per il cloud ML, come comitati etici, audit e gestione del rischio del modello. Anche la governance esterna svolge un ruolo significativo nel garantire responsabilit√† ed equit√†. Abbiamo gi√† introdotto il <a href="https://gdpr-info.eu/">General Data Protection Regulation (GDPR)</a>, che stabilisce requisiti rigorosi per la protezione dei dati e la trasparenza. Tuttavia, non √® l‚Äôunico quadro che guida pratiche di IA responsabili. L‚Äô<a href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/">AI Bill of Rights</a> stabilisce principi per un uso etico dell‚ÄôIA negli Stati Uniti e il <a href="https://oag.ca.gov/privacy/ccpa">California Consumer Protection Act (CCPA)</a> si concentra sulla salvaguardia della privacy dei dati dei consumatori in California. Gli audit di terze parti rafforzano ulteriormente la governance del ML nel cloud fornendo una supervisione esterna.</p>
<p>Edge ML √® pi√π decentralizzato e richiede un‚Äôautogovernance responsabile da parte di sviluppatori e aziende che distribuiscono modelli localmente. Le associazioni di settore coordinano la governance tra i fornitori di edge ML e il software aperto aiuta ad allineare gli incentivi per l‚Äôedge ML etico.</p>
<p>L‚Äôestrema decentralizzazione e complessit√† rendono la governance esterna impraticabile con TinyML. TinyML si basa su protocolli e standard per l‚Äôautogovernance integrati nella progettazione del modello e nell‚Äôhardware. La crittografia consente l‚Äôaffidabilit√† dimostrabile dei dispositivi TinyML.</p>
</section>
<section id="riepilogo" class="level3" data-number="15.4.7">
<h3 data-number="15.4.7" class="anchored" data-anchor-id="riepilogo"><span class="header-section-number">15.4.7</span> Riepilogo</h3>
<p><a href="#tbl-ml-principles-comparison" class="quarto-xref">Tabella&nbsp;<span>15.1</span></a> riassume come i principi di intelligenza artificiale responsabile si manifestino in modo diverso nelle architetture cloud, edge e TinyML e come le considerazioni fondamentali si leghino alle loro capacit√† e limitazioni uniche. I vincoli e i compromessi di ogni ambiente modellano il modo in cui affrontiamo la trasparenza, la responsabilit√†, la governance e altri pilastri dell‚Äôintelligenza artificiale responsabile.</p>
<div id="tbl-ml-principles-comparison" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ml-principles-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;15.1: Confronto dei principi chiave di Cloud ML, Edge ML e TinyML.
</figcaption>
<div aria-describedby="tbl-ml-principles-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 30%">
<col style="width: 31%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Principio</th>
<th style="text-align: left;">Cloud ML</th>
<th style="text-align: left;">Edge ML</th>
<th style="text-align: left;">TinyML</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Spiegabilit√†</td>
<td style="text-align: left;">Supporta modelli e metodi complessi come SHAP e approcci di campionamento</td>
<td style="text-align: left;">Richiede metodi leggeri e a bassa latenza come le mappe di salienza</td>
<td style="text-align: left;">Gravemente limitato a causa dell‚Äôhardware vincolato</td>
</tr>
<tr class="even">
<td style="text-align: left;">Equit√†</td>
<td style="text-align: left;">Grandi set di dati consentono il rilevamento e l‚Äôattenuazione dei bias</td>
<td style="text-align: left;">I bias localizzati sono pi√π difficili da rilevare ma consentono regolazioni sul dispositivo</td>
<td style="text-align: left;">I dati minimi limitano l‚Äôanalisi e l‚Äôattenuazione dei bias</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Privacy</td>
<td style="text-align: left;">I dati centralizzati sono a rischio di violazioni ma possono sfruttare una crittografia avanzata e la privacy differenziale</td>
<td style="text-align: left;">I dati personali sensibili sul dispositivo richiedono protezioni sul dispositivo</td>
<td style="text-align: left;">I dati distribuiti riducono i rischi centralizzati ma pongono sfide per l‚Äôanonimizzazione</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sicurezza</td>
<td style="text-align: left;">Vulnerabile all‚Äôhacking e agli attacchi su larga scala</td>
<td style="text-align: left;">Le interazioni nel mondo reale rendono fondamentale l‚Äôaffidabilit√†</td>
<td style="text-align: left;">Richiede meccanismi di sicurezza distribuiti a causa dell‚Äôautonomia</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Responsabilit√†</td>
<td style="text-align: left;">Le policy e gli audit aziendali garantiscono la responsabilit√†</td>
<td style="text-align: left;">Le catene di fornitura frammentate complicano la responsabilit√†</td>
<td style="text-align: left;">Tracciabilit√† richiesta su lunghe e complesse catene hardware</td>
</tr>
<tr class="even">
<td style="text-align: left;">Governance</td>
<td style="text-align: left;">Supervisione esterna e normative come GDPR o CCPA sono fattibili</td>
<td style="text-align: left;">Richiede autogoverno da parte di sviluppatori e stakeholder</td>
<td style="text-align: left;">Si basa su protocolli integrati e garanzie crittografiche</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="aspetti-tecnici" class="level2 page-columns page-full" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="aspetti-tecnici"><span class="header-section-number">15.5</span> Aspetti Tecnici</h2>
<section id="rilevamento-e-mitigazione-dei-pregiudizi" class="level3 page-columns page-full" data-number="15.5.1">
<h3 data-number="15.5.1" class="anchored" data-anchor-id="rilevamento-e-mitigazione-dei-pregiudizi"><span class="header-section-number">15.5.1</span> Rilevamento e Mitigazione dei Pregiudizi</h3>
<p>I modelli di apprendimento automatico, come qualsiasi sistema complesso, possono talvolta presentare ‚Äúbias‚Äù [distorsioni] nelle loro previsioni. Queste distorsioni possono manifestarsi in prestazioni insufficienti per gruppi specifici o in decisioni che limitano inavvertitamente l‚Äôaccesso a determinate opportunit√† o risorse <span class="citation" data-cites="buolamwini2018genderShades">(<a href="../../../references.it.html#ref-buolamwini2018genderShades" role="doc-biblioref">Buolamwini e Gebru 2018</a>)</span>. Comprendere e affrontare queste distorsioni √® fondamentale, soprattutto perch√© i sistemi di apprendimento automatico sono sempre pi√π utilizzati in settori sensibili come prestiti, assistenza sanitaria e giustizia penale.</p>
<div class="no-row-height column-margin column-container"></div><p>Per valutare e affrontare questi problemi, l‚Äôequit√† nell‚Äôapprendimento automatico viene in genere valutata analizzando gli ‚Äúattributi del sottogruppo‚Äù, che sono caratteristiche non correlate all‚Äôattivit√† di previsione, come posizione geografica, fascia d‚Äôet√†, livello di reddito, razza, genere o religione. Ad esempio, in un modello di previsione di inadempienza del prestito, i sottogruppi potrebbero includere razza, genere o religione. Quando i modelli vengono addestrati con l‚Äôunico obiettivo di massimizzare l‚Äôaccuratezza, potrebbero trascurare le differenze di performance tra questi sottogruppi, con conseguenti potenziali risultati distorti o incoerenti.</p>
<p>Questo concetto √® illustrato in <a href="#fig-fairness-example" class="quarto-xref">Figura&nbsp;<span>15.1</span></a>, che visualizza le performance di un modello di apprendimento automatico che prevede il rimborso del prestito per due sottogruppi, Sottogruppo A (blu) e Sottogruppo B (rosso). Ogni individuo nel set di dati √® rappresentato da un simbolo: i pi√π (+) indicano gli individui che rimborseranno i loro prestiti (veri positivi), mentre i cerchi (O) indicano gli individui che saranno inadempienti sui loro prestiti (veri negativi). L‚Äôobiettivo del modello √® classificare correttamente questi individui in rimborsatori e inadempienti.</p>
<div id="fig-fairness-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fairness-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/fairness_cartoon.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fairness-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;15.1: Illustra il compromesso nell‚Äôimpostazione delle soglie di classificazione per due sottogruppi (A e B) in un modello di rimborso del prestito. I pi√π (+) rappresentano i veri positivi (rimborsatori) e i cerchi (O) rappresentano i veri negativi (inadempienti). Soglie diverse (75% per B e 81,25% per A) massimizzano l‚Äôaccuratezza del sottogruppo ma rivelano problemi di equit√†.
</figcaption>
</figure>
</div>
<p>Per valutare le prestazioni, vengono mostrate due linee tratteggiate, che rappresentano le soglie alle quali il modello raggiunge un‚Äôaccuratezza accettabile per ciascun sottogruppo. Per il Sottogruppo A, la soglia deve essere impostata all‚Äô81,25% di accuratezza (la seconda linea tratteggiata) per classificare correttamente tutti i rimborsatori (pi√π). Tuttavia, l‚Äôutilizzo di questa stessa soglia per il Sottogruppo B comporterebbe classificazioni errate, poich√© alcuni rimborsatori nel Sottogruppo B scenderebbero erroneamente al di sotto di questa soglia e verrebbero classificati come inadempienti. Per il Sottogruppo B, √® necessaria una soglia inferiore del 75% di accuratezza (la prima linea tratteggiata) per classificare correttamente i suoi rimborsatori. Tuttavia, l‚Äôapplicazione di questa soglia inferiore al Sottogruppo A comporterebbe classificazioni errate per quel gruppo. Ci√≤ illustra come il modello funzioni in modo diseguale nei due sottogruppi, con ciascuno che richiede una soglia diversa per massimizzare i propri tassi di veri positivi.</p>
<p>La disparit√† nelle soglie richieste evidenzia la sfida di raggiungere l‚Äôequit√† nelle previsioni del modello. Se le classificazioni positive portano all‚Äôapprovazione dei prestiti, gli individui nel Sottogruppo B sarebbero svantaggiati a meno che la soglia non venga regolata specificamente per il loro sottogruppo. Tuttavia, la regolazione delle soglie introduce compromessi tra accuratezza e correttezza a livello di gruppo, dimostrando la tensione intrinseca nell‚Äôottimizzazione per questi obiettivi nei sistemi di apprendimento automatico.</p>
<p>Pertanto, la letteratura sull‚Äôequit√† ha proposto tre principali <em>metriche di equit√†</em> per quantificare quanto sia equo un modello su un set di dati <span class="citation" data-cites="hardt2016equality">(<a href="../../../references.it.html#ref-hardt2016equality" role="doc-biblioref">Hardt, Price, e Srebro 2016</a>)</span>. Dato un modello <span class="math inline">\(h\)</span> e un set di dati <span class="math inline">\(D\)</span> costituito da campioni <span class="math inline">\((x, y, s)\)</span>, dove <span class="math inline">\(x\)</span> sono le caratteristiche dei dati, <span class="math inline">\(y\)</span> √® l‚Äôetichetta e <span class="math inline">\(s\)</span> √® l‚Äôattributo del sottogruppo, e supponiamo che ci siano semplicemente due sottogruppi <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>, possiamo definire quanto segue:</p>
<div class="no-row-height column-margin column-container"></div><ol type="1">
<li><p><strong>Parit√† Demografica</strong> chiede quanto √® accurato un modello per ogni sottogruppo. In altre parole, <span class="math inline">\(P(h(X) = Y \mid S = a) = P(h(X) = Y \mid S = b)\)</span>.</p></li>
<li><p><strong>Quote Equalizzate</strong> chiede quanto √® preciso un modello su campioni positivi e negativi per ogni sottogruppo. <span class="math inline">\(P(h(X) = y \mid S = a, Y = y) = P(h(X) = y \mid S = b, Y = y)\)</span>.</p></li>
<li><p><strong>Uguaglianza di Opportunit√†</strong> √® un caso speciale di probabilit√† equalizzate che chiede solo quanto √® preciso un modello su campioni positivi. Ci√≤ √® rilevante in casi come l‚Äôallocazione delle risorse, in cui ci preoccupiamo di come le etichette positive (vale a dire, allocate in base alle risorse) siano distribuite tra i gruppi. Ad esempio, ci preoccupiamo che una proporzione uguale di prestiti venga concessa sia agli uomini che alle donne. <span class="math inline">\(P(h(X) = 1 \mid S = a, Y = 1) = P(h(X) = 1 \mid S = b, Y = 1)\)</span>.</p></li>
</ol>
<p>Nota: Queste definizioni spesso adottano una visione ristretta quando si considerano confronti binari tra due sottogruppi. Un altro filone di ricerca di apprendimento automatico equo incentrato su <em>multi-calibrazione</em> e <em>multi-accuratezza</em> considera le interazioni tra un numero arbitrario di identit√†, riconoscendo l‚Äôintersezionalit√† intrinseca delle identit√† individuali nel mondo reale <span class="citation" data-cites="hebert2018multicalibration">(<a href="../../../references.it.html#ref-hebert2018multicalibration" role="doc-biblioref">H√©bert-Johnson et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hebert2018multicalibration" class="csl-entry" role="listitem">
H√©bert-Johnson, √örsula, Michael P. Kim, Omer Reingold, e Guy N. Rothblum. 2018. <span>¬´Multicalibration: <span>Calibration</span> for the <span>(Computationally</span>-Identifiable) Masses¬ª</span>. In <em>Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm√§ssan, Stockholm, Sweden, July 10-15, 2018</em>, a cura di Jennifer G. Dy e Andreas Krause, 80:1944‚Äì53. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v80/hebert-johnson18a.html">http://proceedings.mlr.press/v80/hebert-johnson18a.html</a>.
</div></div><section id="il-contesto-√®-importante" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="il-contesto-√®-importante">Il Contesto √® Importante</h4>
<p>Prima di prendere qualsiasi decisione tecnica per sviluppare un algoritmo ML imparziale, dobbiamo comprendere il contesto che circonda il nostro modello. Ecco alcune delle domande chiave su cui riflettere:</p>
<ul>
<li>Per chi prender√† decisioni questo modello?</li>
<li>Chi √® rappresentato nei dati di training?</li>
<li>Chi √® rappresentato e chi manca al tavolo di ingegneri, progettisti e manager?</li>
<li>Che tipo di impatti duraturi potrebbe avere questo modello? Ad esempio, avr√† un impatto sulla sicurezza finanziaria di un individuo su scala generazionale, come la determinazione delle ammissioni al college o l‚Äôammissione di un prestito per una casa?</li>
<li>Quali pregiudizi storici e sistematici sono presenti in questo contesto e sono presenti nei dati di training da cui il modello generalizzer√†?</li>
</ul>
<p>Comprendere il background sociale, etico e storico di un sistema √® fondamentale per prevenire danni e dovrebbe informare le decisioni durante tutto il ciclo di sviluppo del modello. Dopo aver compreso il contesto, si possono prendere varie decisioni tecniche per rimuovere i pregiudizi. Innanzitutto, si deve decidere quale metrica di equit√† √® il criterio pi√π appropriato per l‚Äôottimizzazione. Successivamente, ci sono generalmente tre aree principali in cui si pu√≤ intervenire per eliminare i pregiudizi di un sistema ML.</p>
<p>Innanzitutto, la preelaborazione √® quando si bilancia un set di dati per garantire una rappresentazione equa o addirittura si aumenta il peso su determinati gruppi sottorappresentati per garantire che il modello funzioni bene. In secondo luogo, nell‚Äôelaborazione si tenta di modificare il processo di training di un sistema ML per garantire che dia priorit√† all‚Äôequit√†. Questo pu√≤ essere semplice come aggiungere un regolarizzatore di equit√† <span class="citation" data-cites="lowy2021fermi">(<a href="../../../references.it.html#ref-lowy2021fermi" role="doc-biblioref">Lowy et al. 2021</a>)</span> al training di un insieme di modelli e campionarli in un modo specifico <span class="citation" data-cites="agarwal2018reductions">(<a href="../../../references.it.html#ref-agarwal2018reductions" role="doc-biblioref">Agarwal et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lowy2021fermi" class="csl-entry" role="listitem">
Lowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, e Ahmad Beirami. 2021. <span>¬´Fermi: <span>Fair</span> empirical risk minimization via exponential R√©nyi mutual information¬ª</span>.
</div><div id="ref-agarwal2018reductions" class="csl-entry" role="listitem">
Agarwal, Alekh, Alina Beygelzimer, Miroslav Dudƒ±ÃÅk, John Langford, e Hanna M. Wallach. 2018. <span>¬´A Reductions Approach to Fair Classification¬ª</span>. In <em>Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm√§ssan, Stockholm, Sweden, July 10-15, 2018</em>, a cura di Jennifer G. Dy e Andreas Krause, 80:60‚Äì69. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v80/agarwal18a.html">http://proceedings.mlr.press/v80/agarwal18a.html</a>.
</div><div id="ref-alghamdi2022beyond" class="csl-entry" role="listitem">
Alghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, e Flavio Calmon. 2022. <span>¬´Beyond Adult and <span>COMPAS:</span> <span>Fair</span> multi-class prediction via information projection¬ª</span>. <em>Adv. Neur. In.</em> 35: 38747‚Äì60.
</div><div id="ref-hardt2016equality" class="csl-entry" role="listitem">
Hardt, Moritz, Eric Price, e Nati Srebro. 2016. <span>¬´Equality of Opportunity in Supervised Learning¬ª</span>. In <em>Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain</em>, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 3315‚Äì23. <a href="https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html">https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html</a>.
</div></div><p>Infine, la post-elaborazione degrada un modello dopo il fatto, prendendo un modello addestrato e modificandone le previsioni in un modo specifico per garantire che l‚Äôequit√† venga preservata <span class="citation" data-cites="alghamdi2022beyond hardt2016equality">(<a href="../../../references.it.html#ref-alghamdi2022beyond" role="doc-biblioref">Alghamdi et al. 2022</a>; <a href="../../../references.it.html#ref-hardt2016equality" role="doc-biblioref">Hardt, Price, e Srebro 2016</a>)</span>. La post-elaborazione si basa sulle fasi di pre-elaborazione e in-elaborazione offrendo un‚Äôaltra opportunit√† per affrontare i problemi di bias [pregiudizi] e equit√† nel modello dopo che √® gi√† stato addestrato.</p>
<p>Il processo in tre fasi di pre-elaborazione, in-elaborazione e post-elaborazione fornisce un framework per intervenire in diverse fasi dello sviluppo del modello per mitigare i problemi relativi a pregiudizi ed equit√†. Mentre la pre-elaborazione e l‚Äôin-elaborazione si concentrano sui dati e sul training, la post-elaborazione consente di apportare modifiche dopo che il modello √® stato completamente formato. Insieme, questi tre approcci offrono molteplici opportunit√† per rilevare e rimuovere pregiudizi ingiusti.</p>
</section>
<section id="distribuzione-ponderata" class="level4">
<h4 class="anchored" data-anchor-id="distribuzione-ponderata">Distribuzione Ponderata</h4>
<p>L‚Äôampiezza delle definizioni di equit√† e degli interventi di debiasing esistenti sottolinea la necessit√† di una valutazione ponderata prima di distribuire sistemi ML. Come ricercatori e sviluppatori ML, lo sviluppo responsabile del modello richiede di istruirci in modo proattivo sul contesto del mondo reale, consultare esperti del settore e utenti finali e concentrarci sulla prevenzione dei danni.</p>
<p>Invece di vedere le considerazioni sull‚Äôequit√† come una casella da spuntare, dobbiamo impegnarci profondamente con le implicazioni sociali uniche e i compromessi etici attorno a ogni modello che costruiamo. Ogni scelta tecnica su set di dati, architetture di modelli, metriche di valutazione e vincoli di distribuzione incorpora valori. Ampliando la nostra prospettiva oltre le metriche tecniche ristrette, valutando attentamente i compromessi e ascoltando le voci interessate, possiamo lavorare per garantire che i nostri sistemi espandano le opportunit√† anzich√© codificare i pregiudizi.</p>
<p>La strada da seguire non risiede in una checklist di ‚Äúdebiasing‚Äù arbitraria, ma nell‚Äôimpegno a comprendere e sostenere la nostra responsabilit√† etica a ogni passo. Questo impegno inizia con l‚Äôeducazione proattiva di noi stessi e la consultazione degli altri, piuttosto che limitarci a seguire i movimenti di una checklist di equit√†. Richiede un profondo impegno nei compromessi etici nelle nostre scelte tecniche, la valutazione degli impatti su diversi gruppi e l‚Äôascolto delle voci maggiormente interessate.</p>
<p>In definitiva, i sistemi di intelligenza artificiale responsabili ed etici non derivano dal ‚Äúdebiasing‚Äù delle caselle di controllo, ma dal rispetto del nostro dovere di valutare i danni, ampliare le prospettive, comprendere i compromessi e garantire di offrire opportunit√† a tutti i gruppi. Questa responsabilit√† etica dovrebbe guidare ogni passo.</p>
<p>Il collegamento tra i paragrafi √® che il primo stabilisce la necessit√† di una valutazione ponderata delle questioni di equit√† piuttosto che di un approccio basato su caselle di controllo. Il secondo paragrafo si sofferma poi su come si presenta in pratica questa valutazione ponderata, ovvero impegnarsi con i compromessi, valutare gli impatti sui gruppi e ascoltare le voci interessate. Infine, l‚Äôultimo paragrafo fa riferimento all‚Äôevitare una ‚Äúchecklist di debiasing arbitraria‚Äù e impegnarsi nella responsabilit√† etica attraverso la valutazione, la comprensione dei compromessi e l‚Äôofferta di opportunit√†.</p>
</section>
</section>
<section id="preservare-la-privacy" class="level3 page-columns page-full" data-number="15.5.2">
<h3 data-number="15.5.2" class="anchored" data-anchor-id="preservare-la-privacy"><span class="header-section-number">15.5.2</span> Preservare la Privacy</h3>
<p>Incidenti recenti hanno fatto luce su come i modelli di intelligenza artificiale possano memorizzare dati sensibili degli utenti in modi che violano la privacy. <span class="citation" data-cites="carlini2023extracting_llm">Ippolito et al. (<a href="../../../references.it.html#ref-carlini2023extracting_llm" role="doc-biblioref">2023</a>)</span> dimostra che i modelli linguistici tendono a memorizzare i dati di addestramento e possono persino riprodurre esempi di addestramento specifici. Questi rischi sono amplificati con sistemi ML personalizzati distribuiti in ambienti intimi come case o dispositivi indossabili. Prendiamo in considerazione uno smart speaker che usa le nostre conversazioni per migliorare la qualit√† del servizio per gli utenti che apprezzano tali miglioramenti. Sebbene potenzialmente vantaggioso, questo crea anche rischi per la privacy, poich√© i malintenzionati potrebbero tentare di estrarre ci√≤ che lo speaker ‚Äúricorda‚Äù. Il problema si estende oltre i modelli linguistici. <a href="#fig-diffusion-model-example" class="quarto-xref">Figura&nbsp;<span>15.2</span></a> mostra come i modelli di diffusione possono memorizzare e generare esempi di training individuali <span class="citation" data-cites="carlini2023extracting">(<a href="../../../references.it.html#ref-carlini2023extracting" role="doc-biblioref">Nicolas Carlini et al. 2023</a>)</span>, dimostrando ulteriormente i potenziali rischi per la privacy associati ai sistemi di intelligenza artificiale che apprendono dai dati degli utenti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-carlini2023extracting" class="csl-entry" role="listitem">
Carlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, e Eric Wallace. 2023. <span>¬´Extracting training data from diffusion models¬ª</span>. In <em>32nd USENIX Security Symposium (USENIX Security 23)</em>, 5253‚Äì70.
</div></div><div id="fig-diffusion-model-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-diffusion-model-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/diffusion_memorization.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diffusion-model-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;15.2: Modelli di diffusione che memorizzano campioni dai dati di training. Fonte: <span class="citation" data-cites="carlini2023extracting_llm">Ippolito et al. (<a href="../../../references.it.html#ref-carlini2023extracting_llm" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-carlini2023extracting_llm" class="csl-entry" role="listitem">
Ippolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, e Nicholas Carlini. 2023. <span>¬´Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy¬ª</span>. In <em>Proceedings of the 16th International Natural Language Generation Conference</em>, 5253‚Äì70. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2023.inlg-main.3">https://doi.org/10.18653/v1/2023.inlg-main.3</a>.
</div></div></figure>
</div>
<p>Man mano che l‚Äôintelligenza artificiale si integra sempre di pi√π nella nostra vita quotidiana, sta diventando sempre pi√π importante che le preoccupazioni sulla privacy e le solide misure di sicurezza per proteggere le informazioni degli utenti siano sviluppate con occhio critico. La sfida sta nel bilanciare i vantaggi dell‚Äôintelligenza artificiale personalizzata con il diritto fondamentale alla privacy.</p>
<p>Gli avversari possono usare queste capacit√† di memorizzazione e addestrare modelli per rilevare se specifici dati di addestramento hanno influenzato un modello target. Ad esempio, gli attacchi di inferenza di appartenenza addestrano un modello secondario che impara a rilevare un cambiamento negli output del modello target quando si effettuano inferenze sui dati su cui √® stato addestrato rispetto a quelli su cui non √® stato addestrato <span class="citation" data-cites="shokri2017membership">(<a href="../../../references.it.html#ref-shokri2017membership" role="doc-biblioref">Shokri et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shokri2017membership" class="csl-entry" role="listitem">
Shokri, Reza, Marco Stronati, Congzheng Song, e Vitaly Shmatikov. 2017. <span>¬´Membership Inference Attacks Against Machine Learning Models¬ª</span>. In <em>2017 IEEE Symposium on Security and Privacy (SP)</em>, 3‚Äì18. IEEE; IEEE. <a href="https://doi.org/10.1109/sp.2017.41">https://doi.org/10.1109/sp.2017.41</a>.
</div><div id="ref-abadi2016deep" class="csl-entry" role="listitem">
Abadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. <span>¬´Deep Learning with Differential Privacy¬ª</span>. In <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>, 308‚Äì18. CCS ‚Äô16. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/2976749.2978318">https://doi.org/10.1145/2976749.2978318</a>.
</div></div><p>I dispositivi ML sono particolarmente vulnerabili perch√© sono spesso personalizzati sui dati degli utenti e vengono distribuiti in contesti ancora pi√π intimi come la casa. Le tecniche di apprendimento automatico privato si sono evolute per stabilire misure di sicurezza contro gli avversari, come menzionato nel capitolo <a href="../../../contents/core/privacy_security/privacy_security.it.html">Sicurezza e Privacy</a> per combattere questi problemi di privacy. Metodi come la privacy differenziale aggiungono rumore matematico durante l‚Äôaddestramento per oscurare l‚Äôinfluenza dei singoli punti dati sul modello. Tecniche popolari come DP-SGD <span class="citation" data-cites="abadi2016deep">(<a href="../../../references.it.html#ref-abadi2016deep" role="doc-biblioref">Abadi et al. 2016</a>)</span> tagliano anche i gradienti per limitare ci√≤ che il modello trapeler√† sui dati. Tuttavia, gli utenti dovrebbero anche avere la possibilit√† di eliminare l‚Äôimpatto dei propri dati in un secondo momento.</p>
</section>
<section id="machine-unlearning" class="level3 page-columns page-full" data-number="15.5.3">
<h3 data-number="15.5.3" class="anchored" data-anchor-id="machine-unlearning"><span class="header-section-number">15.5.3</span> Machine Unlearning</h3>
<p>Con dispositivi ML personalizzati per singoli utenti e poi distribuiti su edge remoti senza connettivit√†, sorge una sfida: come possono i modelli ‚Äúdimenticare‚Äù in modo reattivo i dati dopo la distribuzione? Se gli utenti richiedono che i loro dati vengano rimossi da un modello personalizzato, la mancanza di connettivit√† rende impossibile la riqualificazione. Pertanto, un‚Äôefficiente dimenticanza dei dati sul dispositivo √® necessaria, ma pone degli ostacoli.</p>
<p>Gli approcci iniziali di disapprendimento hanno incontrato delle limitazioni in questo contesto. Date le limitazioni delle risorse, recuperare modelli da zero sul dispositivo per dimenticare i dati si rivela inefficiente o addirittura impossibile. La riqualificazione completa richiede anche di conservare tutti i dati di training originali sul dispositivo, il che comporta dei rischi per la sicurezza e la privacy. Le comuni tecniche di ‚Äúmachine unlearning‚Äù [disapprendimento automatico] <span class="citation" data-cites="bourtoule2021machine">(<a href="../../../references.it.html#ref-bourtoule2021machine" role="doc-biblioref">Bourtoule et al. 2021</a>)</span> per sistemi ML embedded remoti non riescono a consentire la rimozione dei dati reattiva e sicura.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bourtoule2021machine" class="csl-entry" role="listitem">
Bourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, e Nicolas Papernot. 2021. <span>¬´Machine Unlearning¬ª</span>. In <em>2021 IEEE Symposium on Security and Privacy (SP)</em>, 141‚Äì59. IEEE; IEEE. <a href="https://doi.org/10.1109/sp40001.2021.00019">https://doi.org/10.1109/sp40001.2021.00019</a>.
</div></div><p>Tuttavia, metodi pi√π recenti sembrano promettenti nel modificare i modelli in modo da dimenticare approssimativamente i dati senza doverli riqualificare completamente. Sebbene la perdita di accuratezza derivante dall‚Äôevitare ricostruzioni complete sia modesta, garantire la privacy dei dati dovrebbe comunque essere la priorit√† quando si gestiscono eticamente le informazioni sensibili degli utenti. Anche una minima esposizione a dati privati pu√≤ violare la fiducia degli utenti. Poich√© i sistemi ML diventano profondamente personalizzati, efficienza e privacy devono essere abilitate fin dall‚Äôinizio, non ripensamenti.</p>
<p>Le normative globali sulla privacy, come il consolidato <a href="https://gdpr-info.eu">GDPR</a> nell‚ÄôUnione Europea, il <a href="https://oag.ca.gov/privacy/ccpa">CCPA</a> in California e le proposte pi√π recenti come il <a href="https://blog.didomi.io/en-us/canada-data-privacy-law">CPPA</a> del Canada e l‚Äô<a href="https://www.dataguidance.com/notes/japan-data-protection-overview">APPI</a> del Giappone, sottolineano il diritto di eliminare i dati personali. Queste politiche, insieme a incidenti di IA di alto profilo come la memorizzazione dei dati degli artisti da parte di Stable Diffusion, hanno evidenziato l‚Äôimperativo etico per i modelli di consentire agli utenti di eliminare i propri dati anche dopo l‚Äôaddestramento.</p>
<p>Il diritto di rimuovere i dati nasce da preoccupazioni sulla privacy relative alle aziende o agli avversari che abusano delle informazioni sensibili degli utenti. L‚Äôunlearning automatico si riferisce alla rimozione dell‚Äôinfluenza di punti specifici da un modello gi√† addestrato. Ingenuamente, ci√≤ comporta una riqualificazione completa senza i dati eliminati. Tuttavia, i vincoli di connettivit√† spesso rendono la riqualificazione non fattibile per i sistemi ML personalizzati e distribuiti su edge remoti. Se uno smart speaker impara da conversazioni domestiche private, √® importante mantenere l‚Äôaccesso per eliminare tali dati.</p>
<p>Sebbene limitati, i metodi si stanno evolvendo per consentire approssimazioni efficienti della riqualificazione per l‚Äôunlearning. Modificando il tempo di inferenza dei modelli, possono imitare i dati ‚Äúdimenticati‚Äù senza accesso completo ai dati di addestramento. Tuttavia, la maggior parte delle tecniche attuali √® limitata a modelli semplici, ha ancora costi di risorse e scambia una certa accuratezza. Sebbene i metodi si stiano evolvendo, consentire una rimozione efficiente dei dati e rispettare la privacy degli utenti rimane fondamentale per una distribuzione TinyML responsabile.</p>
</section>
<section id="esempi-avversari-e-robustezza" class="level3 page-columns page-full" data-number="15.5.4">
<h3 data-number="15.5.4" class="anchored" data-anchor-id="esempi-avversari-e-robustezza"><span class="header-section-number">15.5.4</span> Esempi Avversari e Robustezza</h3>
<p>I modelli di apprendimento automatico, in particolare le reti neurali profonde, hanno un tallone d‚ÄôAchille ben documentato: spesso si rompono quando vengono apportate anche piccole perturbazioni ai loro input <span class="citation" data-cites="szegedy2013intriguing">(<a href="../../../references.it.html#ref-szegedy2013intriguing" role="doc-biblioref">Szegedy et al. 2014</a>)</span>. Questa sorprendente fragilit√† evidenzia un importante divario di robustezza che minaccia l‚Äôimplementazione nel mondo reale in domini ad alto rischio. Apre anche la porta ad attacchi avversari progettati per ingannare deliberatamente i modelli.</p>
<div class="no-row-height column-margin column-container"></div><p>I modelli di apprendimento automatico possono mostrare una sorprendente fragilit√†: piccole modifiche agli input possono causare malfunzionamenti scioccanti, anche nelle reti neurali profonde all‚Äôavanguardia <span class="citation" data-cites="szegedy2013intriguing">(<a href="../../../references.it.html#ref-szegedy2013intriguing" role="doc-biblioref">Szegedy et al. 2014</a>)</span>. Questa imprevedibilit√† sui dati fuori campione sottolinea le lacune nella generalizzazione e nella robustezza del modello. Data la crescente ubiquit√† dell‚Äôapprendimento automatico, consente anche minacce avversarie che sfruttano i punti ciechi dei modelli.</p>
<p>Le reti neurali profonde dimostrano una doppia natura quasi paradossale: competenza umana nelle distribuzioni di training abbinata a un‚Äôestrema fragilit√† alle piccole perturbazioni di input <span class="citation" data-cites="szegedy2013intriguing">(<a href="../../../references.it.html#ref-szegedy2013intriguing" role="doc-biblioref">Szegedy et al. 2014</a>)</span>. Questa lacuna di vulnerabilit√† avversaria ne evidenzia altre nelle procedure ML standard e minacce all‚Äôaffidabilit√† nel mondo reale. Allo stesso tempo, pu√≤ essere sfruttata: gli aggressori possono trovare punti di rottura del modello che gli umani non percepirebbero.</p>
<div class="no-row-height column-margin column-container"><div id="ref-szegedy2013intriguing" class="csl-entry" role="listitem">
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, e Rob Fergus. 2014. <span>¬´Intriguing properties of neural networks¬ª</span>. In <em>2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings</em>, a cura di Yoshua Bengio e Yann LeCun. <a href="http://arxiv.org/abs/1312.6199">http://arxiv.org/abs/1312.6199</a>.
</div></div><p><a href="#fig-adversarial-example" class="quarto-xref">Figura&nbsp;<span>15.3</span></a> include un esempio di una piccola perturbazione insignificante che modifica una previsione del modello. Questa fragilit√† ha impatti nel mondo reale: la mancanza di robustezza mina la fiducia nell‚Äôimplementazione di modelli per applicazioni ad alto rischio come auto a guida autonoma o diagnosi mediche. Inoltre, la vulnerabilit√† porta a minacce alla sicurezza: gli aggressori possono creare deliberatamente esempi avversari che sono percettivamente indistinguibili dai dati normali ma causano errori del modello.</p>
<div id="fig-adversarial-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/adversarial_robustness.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;15.3: Effetto della perturbazione sulla previsione. Fonte: <a href="https://www.microsoft.com/en-us/research/blog/adversarial-robustness-as-a-prior-for-better-transfer-learning/">Microsoft.</a>
</figcaption>
</figure>
</div>
<p>Ad esempio, lavori passati mostrano attacchi riusciti che ingannano i modelli per attivit√† come il rilevamento NSFW <span class="citation" data-cites="bhagoji2018practical">(<a href="../../../references.it.html#ref-bhagoji2018practical" role="doc-biblioref">Bhagoji et al. 2018</a>)</span>, il blocco degli annunci <span class="citation" data-cites="tramer2019adversarial">(<a href="../../../references.it.html#ref-tramer2019adversarial" role="doc-biblioref">Tram√®r et al. 2019</a>)</span> e il riconoscimento vocale <span class="citation" data-cites="carlini2016hidden">(<a href="../../../references.it.html#ref-carlini2016hidden" role="doc-biblioref">Nicholas Carlini et al. 2016</a>)</span>. Sebbene gli errori in questi domini rappresentino gi√† dei rischi per la sicurezza, il problema si estende oltre la sicurezza IT. Di recente, la robustezza avversaria √® stata proposta come metrica di prestazioni aggiuntiva approssimando il comportamento del caso peggiore.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bhagoji2018practical" class="csl-entry" role="listitem">
Bhagoji, Arjun Nitin, Warren He, Bo Li, e Dawn Song. 2018. <span>¬´Practical black-box attacks on deep neural networks using efficient query mechanisms¬ª</span>. In <em>Proceedings of the European conference on computer vision (ECCV)</em>, 154‚Äì69.
</div><div id="ref-tramer2019adversarial" class="csl-entry" role="listitem">
Tram√®r, Florian, Pascal Dupr√©, Gili Rusak, Giancarlo Pellegrino, e Dan Boneh. 2019. <span>¬´<span>AdVersarial</span>: Perceptual Ad Blocking meets Adversarial Machine Learning¬ª</span>. In <em>Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</em>, 2005‚Äì21. ACM. <a href="https://doi.org/10.1145/3319535.3354222">https://doi.org/10.1145/3319535.3354222</a>.
</div><div id="ref-carlini2016hidden" class="csl-entry" role="listitem">
Carlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, e Wenchao Zhou. 2016. <span>¬´Hidden voice commands¬ª</span>. In <em>25th USENIX security symposium (USENIX security 16)</em>, 513‚Äì30.
</div></div><p>La sorprendente fragilit√† del modello evidenziata sopra mette in dubbio l‚Äôaffidabilit√† nel mondo reale e apre la porta alla manipolazione avversaria. Questa crescente vulnerabilit√† sottolinea diverse esigenze. In primo luogo, le valutazioni della robustezza morale sono essenziali per quantificare le vulnerabilit√† del modello prima dell‚Äôimplementazione. L‚Äôapprossimazione del comportamento del caso peggiore fa emergere punti ciechi.</p>
<p>In secondo luogo, devono essere sviluppate difese efficaci in tutti i domini per colmare queste lacune di robustezza. Con la sicurezza in gioco, gli sviluppatori non possono ignorare la minaccia di attacchi che sfruttano le debolezze del modello. Inoltre, non possiamo permetterci guasti indotti dalla fragilit√† per applicazioni critiche per la sicurezza come veicoli a guida autonoma e diagnosi mediche. Sono in gioco delle vite.</p>
<p>Infine, la comunit√† di ricerca continua a mobilitarsi rapidamente in risposta. L‚Äôinteresse per l‚Äôapprendimento automatico avversario √® esploso poich√© gli attacchi rivelano la necessit√† di colmare il divario di robustezza tra dati sintetici e dati del mondo reale. Le conferenze ora comunemente presentano difese per proteggere e stabilizzare i modelli. La comunit√† riconosce che la fragilit√† del modello √® un problema critico che deve essere affrontato tramite test di robustezza, sviluppo di difese e ricerca continua. Evidenziando i punti ciechi e rispondendo con difese basate su principi, possiamo lavorare per garantire affidabilit√† e sicurezza per i sistemi di apprendimento automatico, specialmente in domini ad alto rischio.</p>
</section>
<section id="creazione-di-modelli-interpretabili" class="level3 page-columns page-full" data-number="15.5.5">
<h3 data-number="15.5.5" class="anchored" data-anchor-id="creazione-di-modelli-interpretabili"><span class="header-section-number">15.5.5</span> Creazione di Modelli Interpretabili</h3>
<p>Poich√© i modelli vengono distribuiti pi√π frequentemente in contesti ad alto rischio, professionisti, sviluppatori, utenti finali a valle e una regolamentazione crescente hanno evidenziato la necessit√† di spiegabilit√† nell‚Äôapprendimento automatico. L‚Äôobiettivo di molti metodi di interpretabilit√† e spiegabilit√† √® fornire ai professionisti maggiori informazioni sul comportamento complessivo dei modelli o sul comportamento dato un input specifico. Ci√≤ consente agli utenti di decidere se l‚Äôoutput o la previsione di un modello sono affidabili o meno.</p>
<p>Tale analisi pu√≤ aiutare gli sviluppatori a eseguire il debug dei modelli e migliorare le prestazioni evidenziando distorsioni, correlazioni spurie e modalit√† di errore dei modelli. Nei casi in cui i modelli possono superare le prestazioni umane in un‚Äôattivit√†, l‚Äôinterpretabilit√† pu√≤ aiutare utenti e ricercatori a comprendere meglio le relazioni nei loro dati e pattern precedentemente sconosciuti.</p>
<p>Esistono molte classi di metodi di spiegabilit√†/interpretabilit√†, tra cui la spiegabilit√† post hoc, l‚Äôinterpretabilit√† intrinseca e l‚Äôinterpretabilit√† meccanicistica. Questi metodi mirano a rendere pi√π comprensibili i modelli di apprendimento automatico complessi e a garantire che gli utenti possano fidarsi delle previsioni del modello, soprattutto in contesti critici. Fornendo trasparenza nel comportamento del modello, le tecniche di spiegabilit√† sono uno strumento importante per sviluppare sistemi di intelligenza artificiale sicuri, equi e affidabili.</p>
<section id="spiegabilit√†-post-hoc" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="spiegabilit√†-post-hoc">Spiegabilit√† Post Hoc</h4>
<p>I metodi di spiegabilit√† ‚Äúpost hoc‚Äù in genere spiegano il comportamento di output di un modello black-box su un input specifico. metodi pi√π diffusi includono spiegazioni controfattuali, metodi di attribuzione delle caratteristiche e spiegazioni basate sui concetti.</p>
<p><strong>Spiegazioni controfattuali</strong>, spesso chiamate anche ricorso algoritmico, ‚ÄúSe X non si fosse verificato, Y non si sarebbe verificato‚Äù <span class="citation" data-cites="wachter2017counterfactual">(<a href="../../../references.it.html#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, e Russell 2017</a>)</span>. Ad esempio, si consideri una persona che richiede un prestito bancario la cui richiesta viene respinta da un modello. Potrebbe chiedere alla propria banca un ricorso o come modificare per essere idonea a un prestito. Una spiegazione controfattuale indicherebbe loro quali caratteristiche devono modificare e di quanto, in modo che la previsione del modello cambi.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wachter2017counterfactual" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, e Chris Russell. 2017. <span>¬´Counterfactual Explanations Without Opening the Black Box: <span>Automated</span> Decisions and the <span>GDPR</span>¬ª</span>. <em>SSRN Electronic Journal</em> 31: 841. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.
</div><div id="ref-selvaraju2017grad" class="csl-entry" role="listitem">
Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, e Dhruv Batra. 2017. <span>¬´Grad-<span>CAM:</span> <span>Visual</span> Explanations from Deep Networks via Gradient-Based Localization¬ª</span>. In <em>2017 IEEE International Conference on Computer Vision (ICCV)</em>, 618‚Äì26. IEEE. <a href="https://doi.org/10.1109/iccv.2017.74">https://doi.org/10.1109/iccv.2017.74</a>.
</div><div id="ref-smilkov2017smoothgrad" class="csl-entry" role="listitem">
Smilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Vi√©gas, e Martin Wattenberg. 2017. <span>¬´Smoothgrad: <span>Removing</span> noise by adding noise¬ª</span>. <em>ArXiv preprint</em> abs/1706.03825. <a href="https://arxiv.org/abs/1706.03825">https://arxiv.org/abs/1706.03825</a>.
</div><div id="ref-ribeiro2016should" class="csl-entry" role="listitem">
Ribeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. <span>¬´<span>‚Äù</span> Why should i trust you?<span>‚Äù</span> Explaining the predictions of any classifier¬ª</span>. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, 1135‚Äì44.
</div><div id="ref-lundberg2017unified" class="csl-entry" role="listitem">
Lundberg, Scott M., e Su-In Lee. 2017. <span>¬´A Unified Approach to Interpreting Model Predictions¬ª</span>. In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765‚Äì74. <a href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</a>.
</div></div><p><strong>I metodi di attribuzione delle caratteristiche</strong> evidenziano le caratteristiche di input che sono importanti o necessarie per una particolare previsione. Per un modello di visione artificiale, ci√≤ significherebbe evidenziare i singoli pixel che hanno contribuito maggiormente all‚Äôetichetta prevista dell‚Äôimmagine. Si noti che questi metodi non spiegano in che modo quei pixel/caratteristiche influenzano la previsione, ma solo che lo fanno. I metodi comuni includono gradienti di input, GradCAM <span class="citation" data-cites="selvaraju2017grad">(<a href="../../../references.it.html#ref-selvaraju2017grad" role="doc-biblioref">Selvaraju et al. 2017</a>)</span>, SmoothGrad <span class="citation" data-cites="smilkov2017smoothgrad">(<a href="../../../references.it.html#ref-smilkov2017smoothgrad" role="doc-biblioref">Smilkov et al. 2017</a>)</span>, LIME <span class="citation" data-cites="ribeiro2016should">(<a href="../../../references.it.html#ref-ribeiro2016should" role="doc-biblioref">Ribeiro, Singh, e Guestrin 2016</a>)</span> e SHAP <span class="citation" data-cites="lundberg2017unified">(<a href="../../../references.it.html#ref-lundberg2017unified" role="doc-biblioref">Lundberg e Lee 2017</a>)</span>.</p>
<p>Fornendo esempi di modifiche alle caratteristiche di input che altererebbero una previsione (controfattuali) o indicando le caratteristiche pi√π influenti per una data previsione (attribuzione), queste tecniche di spiegazione post hoc fanno luce sul comportamento del modello per input individuali. Questa trasparenza granulare aiuta gli utenti a determinare se possono fidarsi e agire su output di modelli specifici.</p>
<p><strong>Le spiegazioni basate sui concetti</strong> mirano a spiegare il comportamento del modello e gli output utilizzando un set predefinito di concetti semantici (ad esempio, il modello riconosce la classe di scena ‚Äúcamera da letto‚Äù in base alla presenza dei concetti ‚Äúletto‚Äù e ‚Äúcuscino‚Äù). Lavori recenti mostrano che gli utenti spesso preferiscono queste spiegazioni a quelle basate sull‚Äôattribuzione e sugli esempi perch√© ‚Äúassomigliano al ragionamento e alle spiegazioni umane‚Äù <span class="citation" data-cites="ramaswamy2023ufo">(<a href="../../../references.it.html#ref-ramaswamy2023ufo" role="doc-biblioref">Vikram V. Ramaswamy et al. 2023b</a>)</span>. I metodi di spiegazione basati sui concetti pi√π diffusi includono TCAV <span class="citation" data-cites="kim2018interpretability">(<a href="../../../references.it.html#ref-kim2018interpretability" role="doc-biblioref">Cai et al. 2019</a>)</span>, Network Dissection <span class="citation" data-cites="bau2017network">(<a href="../../../references.it.html#ref-bau2017network" role="doc-biblioref">Bau et al. 2017</a>)</span> e decomposizione della base interpretabile <span class="citation" data-cites="zhou2018interpretable">(<a href="../../../references.it.html#ref-zhou2018interpretable" role="doc-biblioref">Zhou et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ramaswamy2023ufo" class="csl-entry" role="listitem">
Ramaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, e Olga Russakovsky. 2023b. <span>¬´<span>UFO:</span> <span>A</span> unified method for controlling Understandability and Faithfulness Objectives in concept-based explanations for <span>CNNs</span>¬ª</span>. <em>ArXiv preprint</em> abs/2303.15632. <a href="https://arxiv.org/abs/2303.15632">https://arxiv.org/abs/2303.15632</a>.
</div><div id="ref-kim2018interpretability" class="csl-entry" role="listitem">
Cai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, et al. 2019. <span>¬´Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making¬ª</span>. In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>, a cura di Jennifer G. Dy e Andreas Krause, 80:2673‚Äì82. Proceedings of Machine Learning Research. ACM. <a href="https://doi.org/10.1145/3290605.3300234">https://doi.org/10.1145/3290605.3300234</a>.
</div><div id="ref-bau2017network" class="csl-entry" role="listitem">
Bau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, e Antonio Torralba. 2017. <span>¬´Network Dissection: <span>Quantifying</span> Interpretability of Deep Visual Representations¬ª</span>. In <em>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 3319‚Äì27. IEEE. <a href="https://doi.org/10.1109/cvpr.2017.354">https://doi.org/10.1109/cvpr.2017.354</a>.
</div><div id="ref-zhou2018interpretable" class="csl-entry" role="listitem">
Zhou, Bolei, Yiyou Sun, David Bau, e Antonio Torralba. 2018. <span>¬´Interpretable basis decomposition for visual explanation¬ª</span>. In <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>, 119‚Äì34.
</div><div id="ref-ramaswamy2023overlooked" class="csl-entry" role="listitem">
Ramaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, e Olga Russakovsky. 2023a. <span>¬´Overlooked Factors in Concept-Based Explanations: <span>Dataset</span> Choice, Concept Learnability, and Human Capability¬ª</span>. In <em>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10932‚Äì41. IEEE. <a href="https://doi.org/10.1109/cvpr52729.2023.01052">https://doi.org/10.1109/cvpr52729.2023.01052</a>.
</div></div><p>Si noti che questi metodi sono estremamente sensibili alla dimensione e alla qualit√† del set di concetti e c‚Äô√® un compromesso tra la loro accuratezza e fedelt√† e la loro interpretabilit√† o comprensibilit√† per gli esseri umani <span class="citation" data-cites="ramaswamy2023overlooked">(<a href="../../../references.it.html#ref-ramaswamy2023overlooked" role="doc-biblioref">Vikram V. Ramaswamy et al. 2023a</a>)</span>. Tuttavia, mappando le previsioni del modello su concetti comprensibili per gli esseri umani, le spiegazioni basate sui concetti possono fornire trasparenza nel ragionamento alla base degli output del modello.</p>
</section>
<section id="interpretabilit√†-intrinseca" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="interpretabilit√†-intrinseca">Interpretabilit√† Intrinseca</h4>
<p>I modelli intrinsecamente interpretabili sono costruiti in modo tale che le loro spiegazioni siano parte dell‚Äôarchitettura del modello e siano quindi naturalmente fedeli, il che a volte li rende preferibili alle spiegazioni post-hoc applicate ai modelli black-box, specialmente in domini ad alto rischio in cui la trasparenza √® fondamentale <span class="citation" data-cites="rudin2019stop">(<a href="../../../references.it.html#ref-rudin2019stop" role="doc-biblioref">Rudin 2019</a>)</span>. Spesso, questi modelli sono vincolati in modo che le relazioni tra le caratteristiche di input e le previsioni siano facili da seguire per gli esseri umani (modelli lineari, alberi decisionali, set di decisioni, modelli k-NN) o obbediscano alla conoscenza strutturale del dominio, come la monotonicit√† <span class="citation" data-cites="gupta2016monotonic">(<a href="../../../references.it.html#ref-gupta2016monotonic" role="doc-biblioref">Gupta et al. 2016</a>)</span>, la causalit√† o l‚Äôadditivit√† <span class="citation" data-cites="lou2013accurate beck1998beyond">(<a href="../../../references.it.html#ref-lou2013accurate" role="doc-biblioref">Lou et al. 2013</a>; <a href="../../../references.it.html#ref-beck1998beyond" role="doc-biblioref">Beck e Jackman 1998</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rudin2019stop" class="csl-entry" role="listitem">
Rudin, Cynthia. 2019. <span>¬´Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead¬ª</span>. <em>Nature Machine Intelligence</em> 1 (5): 206‚Äì15. <a href="https://doi.org/10.1038/s42256-019-0048-x">https://doi.org/10.1038/s42256-019-0048-x</a>.
</div><div id="ref-gupta2016monotonic" class="csl-entry" role="listitem">
Gupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, e Alexander Van Esbroeck. 2016. <span>¬´Monotonic calibrated interpolated look-up tables¬ª</span>. <em>The Journal of Machine Learning Research</em> 17 (1): 3790‚Äì3836.
</div><div id="ref-lou2013accurate" class="csl-entry" role="listitem">
Lou, Yin, Rich Caruana, Johannes Gehrke, e Giles Hooker. 2013. <span>¬´Accurate intelligible models with pairwise interactions¬ª</span>. In <em>Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, a cura di Inderjit S. Dhillon, Yehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh, Jingrui He, Robert L. Grossman, e Ramasamy Uthurusamy, 623‚Äì31. ACM. <a href="https://doi.org/10.1145/2487575.2487579">https://doi.org/10.1145/2487575.2487579</a>.
</div><div id="ref-beck1998beyond" class="csl-entry" role="listitem">
Beck, Nathaniel, e Simon Jackman. 1998. <span>¬´Beyond Linearity by Default: <span>Generalized</span> Additive Models¬ª</span>. <em>Am. J. Polit. Sci.</em> 42 (2): 596. <a href="https://doi.org/10.2307/2991772">https://doi.org/10.2307/2991772</a>.
</div><div id="ref-koh2020concept" class="csl-entry" role="listitem">
Koh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, e Percy Liang. 2020. <span>¬´Concept Bottleneck Models¬ª</span>. In <em>Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event</em>, 119:5338‚Äì48. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v119/koh20a.html">http://proceedings.mlr.press/v119/koh20a.html</a>.
</div><div id="ref-chen2019looks" class="csl-entry" role="listitem">
Chen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, e Jonathan Su. 2019. <span>¬´This Looks Like That: <span>Deep</span> Learning for Interpretable Image Recognition¬ª</span>. In <em>Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</em>, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, e Roman Garnett, 8928‚Äì39. <a href="https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html">https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html</a>.
</div></div><p>Tuttavia, lavori pi√π recenti hanno allentato le restrizioni sui modelli intrinsecamente interpretabili, utilizzando modelli black-box per l‚Äôestrazione delle caratteristiche e un modello intrinsecamente interpretabile pi√π semplice per la classificazione, consentendo spiegazioni fedeli che collegano le caratteristiche di alto livello alla previsione. Ad esempio, i Concept Bottleneck Models <span class="citation" data-cites="koh2020concept">(<a href="../../../references.it.html#ref-koh2020concept" role="doc-biblioref">Koh et al. 2020</a>)</span> prevedono un set di concetti c che viene passato in un classificatore lineare. I ProtoPNets <span class="citation" data-cites="chen2019looks">(<a href="../../../references.it.html#ref-chen2019looks" role="doc-biblioref">Chen et al. 2019</a>)</span> sezionano gli input in combinazioni lineari di somiglianze con parti prototipiche del set di training.</p>
</section>
<section id="interpretabilit√†-meccanicistica" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="interpretabilit√†-meccanicistica">Interpretabilit√† Meccanicistica</h4>
<p>I metodi di interpretabilit√† meccanicistica cercano di effettuare il reverse engineering delle reti neurali, spesso paragonandoli a come si potrebbe effettuare quello di un binario compilato o a come i neuroscienziati tentano di decodificare la funzione di singoli neuroni e circuiti nel cervello. La maggior parte delle ricerche sull‚Äôinterpretabilit√† meccanicistica vede i modelli come un grafo computazionale <span class="citation" data-cites="geiger2021causal">(<a href="../../../references.it.html#ref-geiger2021causal" role="doc-biblioref">Geiger et al. 2021</a>)</span> e i circuiti sono sottografi con funzionalit√† distinte <span class="citation" data-cites="wang2022interpretability">(<a href="../../../references.it.html#ref-wang2022interpretability" role="doc-biblioref">Wang e Zhan 2019</a>)</span>. Gli attuali approcci all‚Äôestrazione di circuiti dalle reti neurali e alla comprensione della loro funzionalit√† si basano sull‚Äôispezione manuale umana delle visualizzazioni prodotte dai circuiti <span class="citation" data-cites="olah2020zoom">(<a href="../../../references.it.html#ref-olah2020zoom" role="doc-biblioref">Olah et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-geiger2021causal" class="csl-entry" role="listitem">
Geiger, Atticus, Hanson Lu, Thomas Icard, e Christopher Potts. 2021. <span>¬´Causal Abstractions of Neural Networks¬ª</span>. In <em>Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual</em>, a cura di Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 9574‚Äì86. <a href="https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html</a>.
</div><div id="ref-wang2022interpretability" class="csl-entry" role="listitem">
Wang, LingFeng, e YaQing Zhan. 2019. <span>¬´A conceptual peer review model for <span>arXiv</span> and other preprint databases¬ª</span>. <em>Learn. Publ.</em> 32 (3): 213‚Äì19. <a href="https://doi.org/10.1002/leap.1229">https://doi.org/10.1002/leap.1229</a>.
</div><div id="ref-olah2020zoom" class="csl-entry" role="listitem">
Olah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, e Shan Carter. 2020. <span>¬´Zoom In: <span>An</span> Introduction to Circuits¬ª</span>. <em>Distill</em> 5 (3): e00024‚Äì001. <a href="https://doi.org/10.23915/distill.00024.001">https://doi.org/10.23915/distill.00024.001</a>.
</div><div id="ref-bricken2023towards" class="csl-entry" role="listitem">
Davarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana Turner, Carver Middleton, Will Carroll, et al. 2023. <span>¬´Closing the Wearable Gap: <span>Foot<span></span>ankle</span> kinematic modeling via deep learning models based on a smart sock wearable¬ª</span>. <em>Wearable Technologies</em> 4. <a href="https://doi.org/10.1017/wtc.2023.3">https://doi.org/10.1017/wtc.2023.3</a>.
</div></div><p>In alternativa, alcuni approcci creano autoencoder sparsi che incoraggiano i neuroni a codificare caratteristiche interpretabili districate <span class="citation" data-cites="bricken2023towards">(<a href="../../../references.it.html#ref-bricken2023towards" role="doc-biblioref">Davarzani et al. 2023</a>)</span>. Questo campo √® molto pi√π nuovo rispetto alle aree esistenti in spiegabilit√† e interpretabilit√† e, in quanto tale, la maggior parte dei lavori √® generalmente esplorativa piuttosto che orientata alla soluzione.</p>
<p>Ci sono molti problemi nell‚Äôinterpretabilit√† meccanicistica, tra cui la polisemanticit√† di neuroni e circuiti, l‚Äôinconveniente e la soggettivit√† dell‚Äôetichettatura umana e lo spazio di ricerca esponenziale per l‚Äôidentificazione dei circuiti in grandi modelli con miliardi o trilioni di neuroni.</p>
</section>
<section id="sfide-e-considerazioni" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sfide-e-considerazioni">Sfide e Considerazioni</h4>
<p>Man mano che i metodi per interpretare e spiegare i modelli progrediscono, √® importante notare che gli esseri umani si fidano troppo e abusano degli strumenti di interpretabilit√† <span class="citation" data-cites="kaur2020interpreting">(<a href="../../../references.it.html#ref-kaur2020interpreting" role="doc-biblioref">Kaur et al. 2020</a>)</span> e che la fiducia di un utente in un modello dovuta a una spiegazione pu√≤ essere indipendente dalla correttezza delle spiegazioni <span class="citation" data-cites="lakkaraju2020fool">(<a href="../../../references.it.html#ref-lakkaraju2020fool" role="doc-biblioref">Lakkaraju e Bastani 2020</a>)</span>. Pertanto, √® necessario che oltre a valutare la fedelt√†/correttezza delle spiegazioni, i ricercatori debbano anche garantire che i metodi di interpretabilit√† siano sviluppati e implementati tenendo a mente un utente specifico e che vengano eseguiti studi sugli utenti per valutarne l‚Äôefficacia e l‚Äôutilit√† nella pratica.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kaur2020interpreting" class="csl-entry" role="listitem">
Kaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, e Jennifer Wortman Vaughan. 2020. <span>¬´Interpreting Interpretability: <span>Understanding</span> Data Scientists‚Äô Use of Interpretability Tools for Machine Learning¬ª</span>. In <em>Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>, a cura di Regina Bernhaupt, Florian ‚ÄôFloyd‚ÄôMueller, David Verweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1‚Äì14. ACM. <a href="https://doi.org/10.1145/3313831.3376219">https://doi.org/10.1145/3313831.3376219</a>.
</div><div id="ref-lakkaraju2020fool" class="csl-entry" role="listitem">
Lakkaraju, Himabindu, e Osbert Bastani. 2020. <span>¬´<span><span>‚Äù</span>How</span> do I fool you?<span>‚Äù</span>: Manipulating User Trust via Misleading Black Box Explanations¬ª</span>. In <em>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>, 79‚Äì85. ACM. <a href="https://doi.org/10.1145/3375627.3375833">https://doi.org/10.1145/3375627.3375833</a>.
</div></div><p>Inoltre, le spiegazioni devono essere adattate alle competenze dell‚Äôutente, all‚Äôattivit√† per cui stanno utilizzando la spiegazione e alla corrispondente quantit√† minima di informazioni richieste affinch√© la spiegazione sia utile per prevenire il sovraccarico di informazioni.</p>
<p>Mentre interpretabilit√†/spiegabilit√† sono aree popolari nella ricerca sull‚Äôapprendimento automatico, pochissimi lavori studiano la loro intersezione con TinyML ed edge computing. Dato che un‚Äôapplicazione significativa di TinyML √® l‚Äôassistenza sanitaria, che spesso richiede elevata trasparenza e interpretabilit√†, le tecniche esistenti devono essere testate per scalabilit√† ed efficienza relativamente ai dispositivi edge. Molti metodi si basano su passaggi aggiuntivi ‚Äúforward‚Äù e ‚Äúbackward‚Äù e alcuni richiedono persino un training approfondito nei modelli proxy, che non sono fattibili su microcontrollori con risorse limitate.</p>
<p>Detto questo, i metodi di spiegabilit√† possono essere molto utili nello sviluppo di modelli per dispositivi edge, in quanto possono fornire informazioni su come i dati di input e i modelli possono essere compressi e su come le rappresentazioni possono cambiare dopo la compressione. Inoltre, molti modelli interpretabili sono spesso pi√π piccoli delle loro controparti black-box, il che potrebbe essere utile per le applicazioni TinyML.</p>
</section>
</section>
<section id="monitoraggio-delle-prestazioni-del-modello" class="level3" data-number="15.5.6">
<h3 data-number="15.5.6" class="anchored" data-anchor-id="monitoraggio-delle-prestazioni-del-modello"><span class="header-section-number">15.5.6</span> Monitoraggio delle Prestazioni del Modello</h3>
<p>Mentre gli sviluppatori possono addestrare modelli che sembrano avversarialmente robusti, equi e interpretabili prima della distribuzione, √® fondamentale che sia gli utenti sia i proprietari del modello ne continuino a monitorare le prestazioni e l‚Äôaffidabilit√† durante l‚Äôintero ciclo di vita. I dati cambiano frequentemente nella pratica, il che pu√≤ spesso comportare cambiamenti nella distribuzione. Questi cambiamenti nella distribuzione possono avere un impatto profondo sulle prestazioni predittive ‚Äúvanilla‚Äù del modello e sulla sua affidabilit√† (equit√†, robustezza e interpretabilit√†) nei dati del mondo reale.</p>
<p>Inoltre, le definizioni di equit√† cambiano frequentemente nel tempo, come ci√≤ che la societ√† considera un attributo protetto, e anche le competenze degli utenti che chiedono spiegazioni possono cambiare.</p>
<p>Per garantire che i modelli rimangano aggiornati con tali cambiamenti nel mondo reale, gli sviluppatori devono valutare continuamente i loro modelli su dati e standard attuali e rappresentativi e aggiornare i modelli quando necessario.</p>
</section>
</section>
<section id="sfide-di-implementazione" class="level2 page-columns page-full" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="sfide-di-implementazione"><span class="header-section-number">15.6</span> Sfide di Implementazione</h2>
<section id="strutture-organizzative-e-culturali" class="level3 page-columns page-full" data-number="15.6.1">
<h3 data-number="15.6.1" class="anchored" data-anchor-id="strutture-organizzative-e-culturali"><span class="header-section-number">15.6.1</span> Strutture Organizzative e Culturali</h3>
<p>Sebbene innovazione e regolamentazione siano spesso viste come interessi contrapposti, molti paesi hanno ritenuto necessario fornire supervisione man mano che i sistemi di intelligenza artificiale si espandono in pi√π settori. Come mostrato in <a href="#fig-human-centered-ai" class="quarto-xref">Figura&nbsp;<span>15.4</span></a>, questa supervisione √® diventata cruciale poich√© questi sistemi continuano a permeare vari settori e ad avere un impatto sulla vita delle persone. Ulteriori discussioni su questo argomento sono disponibili in <a href="https://academic-oup-com.ezp-prod1.hul.harvard.edu/book/41126/chapter/350465542">Human-Centered AI, Capitolo 22 ‚ÄúGovernment Interventions and Regulations‚Äù</a>.</p>
<div id="fig-human-centered-ai" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-human-centered-ai-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/human_centered_ai.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-human-centered-ai-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;15.4: Come vari gruppi influenzano l‚ÄôAI incentrata sull‚Äôuomo. Fonte: <span class="citation" data-cites="schneiderman2020">Shneiderman (<a href="../../../references.it.html#ref-schneiderman2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-schneiderman2020" class="csl-entry" role="listitem">
Shneiderman, Ben. 2020. <span>¬´Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-centered AI Systems¬ª</span>. <em>ACM Trans. Interact. Intell. Syst.</em> 10 (4): 1‚Äì31. <a href="https://doi.org/10.1145/3419764">https://doi.org/10.1145/3419764</a>.
</div></div></figure>
</div>
<p>In questo capitolo abbiamo trattato diverse politiche chiave volte a guidare lo sviluppo e l‚Äôimplementazione dell‚ÄôIA responsabile. Di seguito √® riportato un riepilogo di queste politiche, insieme ad altri framework degni di nota che riflettono una spinta globale per la trasparenza nei sistemi di IA:</p>
<ul>
<li>Il <a href="https://gdpr-info.eu/">General Data Protection Regulation (GDPR)</a> dell‚ÄôUnione Europea impone misure di trasparenza e protezione dei dati per i sistemi di IA che gestiscono dati personali.</li>
<li>L‚Äô<a href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/">AI Bill of Rights</a> delinea i principi per un utilizzo etico dell‚ÄôIA negli Stati Uniti, sottolineando correttezza, privacy e responsabilit√†.</li>
<li>Il <a href="https://oag.ca.gov/privacy/ccpa">California Consumer Privacy Act (CCPA)</a> protegge i dati dei consumatori e ritiene le organizzazioni responsabili per l‚Äôuso improprio dei dati.</li>
<li>Il <a href="https://www.canada.ca/en/government/system/digital-government/digital-government-innovations/responsible-use-ai.html">Responsible Use of Artificial Intelligence</a> del Canada delinea le migliori pratiche per l‚Äôimplementazione etica dell‚ÄôIA.</li>
<li>L‚Äô<a href="https://www.dataguidance.com/notes/japan-data-protection-overview">Act on the Protection of Personal Information (APPI)</a> del Giappone stabilisce linee guida per la gestione dei dati personali nei sistemi di IA.</li>
<li>La proposta canadese del <a href="https://blog.didomi.io/en-us/canada-data-privacy-law">Consumer Privacy Protection Act (CPPA)</a> mira a rafforzare la protezione della privacy negli ecosistemi digitali.</li>
<li>Il <a href="https://commission.europa.eu/publications/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en">White Paper on Artificial Intelligence: A European Approach to Excellence and Trust</a> della Commissione Europea sottolinea lo sviluppo etico dell‚ÄôIA insieme all‚Äôinnovazione.</li>
<li>L‚ÄôInformation Commissioner‚Äôs Office del Regno Unito e la <a href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence">Guidance on Explaining AI Decisions</a> dell‚ÄôAlan Turing Institute forniscono raccomandazioni per aumentare la trasparenza dell‚ÄôIA.</li>
</ul>
<p>Queste politiche evidenziano uno sforzo globale in corso per bilanciare innovazione e responsabilit√† e garantire che i sistemi di IA siano sviluppati e distribuiti in modo responsabile.</p>
</section>
<section id="ottenere-dati-di-qualit√†-e-rappresentativi" class="level3 page-columns page-full" data-number="15.6.2">
<h3 data-number="15.6.2" class="anchored" data-anchor-id="ottenere-dati-di-qualit√†-e-rappresentativi"><span class="header-section-number">15.6.2</span> Ottenere Dati di Qualit√† e Rappresentativi</h3>
<p>Come discusso nel capitolo <a href="../../../contents/core/data_engineering/data_engineering.it.html">Data Engineering</a>, la progettazione responsabile dell‚ÄôIA deve avvenire in tutte le fasi della pipeline, inclusa la raccolta dei dati. Ci√≤ solleva la domanda: cosa significa che i dati siano di alta qualit√† e rappresentativi? Consideriamo i seguenti scenari che <em>ostacolano</em> la rappresentativit√† dei dati:</p>
<section id="squilibrio-dei-sottogruppi" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="squilibrio-dei-sottogruppi">Squilibrio dei Sottogruppi</h4>
<p>Questo √® probabilmente ci√≤ che viene in mente quando si sente parlare di ‚Äúdati rappresentativi‚Äù. Lo squilibrio dei sottogruppi significa che il set di dati contiene relativamente pi√π dati da un sottogruppo rispetto a un altro. Questo squilibrio pu√≤ influire negativamente sul modello ML a valle, facendolo sovradimensionare per un sottogruppo di persone e con prestazioni scadenti per un altro.</p>
<p>Un esempio di conseguenza dello squilibrio dei sottogruppi √® la discriminazione razziale nella tecnologia di riconoscimento facciale <span class="citation" data-cites="buolamwini2018genderShades">(<a href="../../../references.it.html#ref-buolamwini2018genderShades" role="doc-biblioref">Buolamwini e Gebru 2018</a>)</span>; gli algoritmi commerciali di riconoscimento facciale hanno tassi di errore fino al 34% peggiori sulle donne dalla pelle scura rispetto agli uomini dalla pelle chiara.</p>
<div class="no-row-height column-margin column-container"><div id="ref-buolamwini2018genderShades" class="csl-entry" role="listitem">
Buolamwini, Joy, e Timnit Gebru. 2018. <span>¬´Gender shades: <span>Intersectional</span> accuracy disparities in commercial gender classification¬ª</span>. In <em>Conference on fairness, accountability and transparency</em>, 77‚Äì91. PMLR.
</div></div><p>Si noti che lo squilibrio dei dati √® reciproco e i sottogruppi possono anche essere <em>sovrarappresentati</em> in modo dannoso nel set di dati. Ad esempio, l‚ÄôAllegheny Family Screening Tool (AFST) prevede la probabilit√† che un bambino venga alla fine allontanato da una casa. L‚ÄôAFST produce <a href="https://www.aclu.org/the-devil-is-in-the-details-interrogating-values-embedded-in-the-allegheny-family-screening-tool#4-2-the-more-data-the-better">punteggi sproporzionati per diversi sottogruppi</a>, uno dei motivi √® che √® basato su dati storicamente distorti, provenienti da sistemi legali penali minorili e per adulti, agenzie di assistenza pubblica e agenzie e programmi di salute comportamentale.</p>
</section>
<section id="quantificazione-dei-risultati-target" class="level4">
<h4 class="anchored" data-anchor-id="quantificazione-dei-risultati-target">Quantificazione dei Risultati Target</h4>
<p>Ci√≤ si verifica in applicazioni in cui l‚Äôetichetta di verit√† di base non pu√≤ essere misurata o √® difficile da rappresentare in una singola quantit√†. Ad esempio, un modello ML in un‚Äôapplicazione mobile per il benessere potrebbe voler prevedere i livelli di stress individuali. Le vere etichette di stress sono impossibili da ottenere direttamente e devono essere dedotte da altri segnali biologici, come la variabilit√† della frequenza cardiaca e i dati auto-riportati dall‚Äôutente. In queste situazioni, il rumore √® incorporato nei dati per progettazione, rendendo questo un compito ML impegnativo.</p>
</section>
<section id="spostamento-della-distribuzione" class="level4">
<h4 class="anchored" data-anchor-id="spostamento-della-distribuzione">Spostamento della Distribuzione</h4>
<p>I dati potrebbero non rappresentare pi√π un compito se un evento esterno importante causa un drastico cambiamento della fonte dati. Il modo pi√π comune di pensare alle ‚Äúdistribution shift‚Äù [spostamenti della distribuzione] √® rispetto al tempo; ad esempio, i dati sulle abitudini di acquisto dei consumatori raccolti prima del Covid potrebbero non essere pi√π presenti nel comportamento dei consumatori oggi.</p>
<p>Il trasferimento provoca un‚Äôaltra forma di spostamento della distribuzione. Ad esempio, quando si applica un sistema di triage addestrato sui dati di un ospedale a un altro, potrebbe verificarsi uno spostamento nella distribuzione se i due ospedali sono molto diversi.</p>
</section>
<section id="raccolta-dati" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="raccolta-dati">Raccolta Dati</h4>
<p>Una soluzione ragionevole per molti dei problemi di cui sopra con dati non rappresentativi o di bassa qualit√† √® raccoglierne di pi√π; possiamo raccogliere pi√π dati mirati a un sottogruppo sottorappresentato o dall‚Äôospedale target a cui il nostro modello potrebbe essere trasferito. Tuttavia, per alcune ragioni, raccogliere pi√π dati √® una soluzione inappropriata o non fattibile per il compito da svolgere.</p>
<ul>
<li><p><em>La raccolta dati pu√≤ essere dannosa.</em> Questo √® il <em>paradosso dell‚Äôesposizione</em>, la situazione in cui coloro che traggono un guadagno significativo dalla raccolta dei propri dati sono anche coloro che sono messi a rischio dal processo di raccolta (<span class="citation" data-cites="d2023dataFeminism">D‚Äôignazio e Klein (<a href="../../../references.it.html#ref-d2023dataFeminism" role="doc-biblioref">2023</a>)</span>, Capitolo 4). Ad esempio, raccogliere pi√π dati su individui non binari pu√≤ essere importante per garantire l‚Äôequit√† dell‚Äôapplicazione ML, ma li espone anche a rischi, a seconda di chi raccoglie i dati e di come (se i dati sono facilmente identificabili, contengono contenuti sensibili, ecc.).</p></li>
<li><p><em>La raccolta dati pu√≤ essere costosa.</em> In alcuni ambiti, come l‚Äôassistenza sanitaria, ottenere dati pu√≤ essere costoso in termini di tempo e denaro.</p></li>
<li><p><em>Raccolta dati distorta.</em> Le cartelle cliniche elettroniche sono un‚Äôenorme fonte di dati per le applicazioni sanitarie basate su ML. A parte i problemi di rappresentazione dei sottogruppi, i dati stessi possono essere raccolti in modo distorto. Ad esempio, il linguaggio negativo (‚Äúnon aderente‚Äù, ‚Äúnon disposto‚Äù) √® utilizzato in modo sproporzionato sui pazienti neri <span class="citation" data-cites="himmelstein2022examination">(<a href="../../../references.it.html#ref-himmelstein2022examination" role="doc-biblioref">Himmelstein, Bates, e Zhou 2022</a>)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-d2023dataFeminism" class="csl-entry" role="listitem">
D‚Äôignazio, Catherine, e Lauren F Klein. 2023. <em>Data feminism</em>. MIT press.
</div><div id="ref-himmelstein2022examination" class="csl-entry" role="listitem">
Himmelstein, Gracie, David Bates, e Li Zhou. 2022. <span>¬´Examination of Stigmatizing Language in the Electronic Health Record¬ª</span>. <em>JAMA Network Open</em> 5 (1): e2144967. <a href="https://doi.org/10.1001/jamanetworkopen.2021.44967">https://doi.org/10.1001/jamanetworkopen.2021.44967</a>.
</div></div><p>Concludiamo con diverse strategie aggiuntive per mantenere la qualit√† dei dati. Innanzitutto, √® fondamentale promuovere una comprensione pi√π approfondita dei dati. Ci√≤ pu√≤ essere ottenuto tramite l‚Äôimplementazione di etichette e misure standardizzate della qualit√† dei dati, come nel <a href="https://datanutrition.org/">Data Nutrition Project</a>. Collaborare con le organizzazioni responsabili della raccolta dei dati aiuta a garantire che i dati vengano interpretati correttamente. In secondo luogo, √® importante impiegare strumenti efficaci per l‚Äôesplorazione dei dati. Le tecniche di visualizzazione e le analisi statistiche possono rivelare problemi con i dati. Infine, stabilire un ciclo di feedback all‚Äôinterno della pipeline ML √® essenziale per comprendere le implicazioni reali dei dati. Le metriche, come le misure di equit√†, ci consentono di definire la ‚Äúqualit√† dei dati‚Äù nel contesto dell‚Äôapplicazione downstream; il miglioramento dell‚Äôequit√† pu√≤ migliorare direttamente la qualit√† delle previsioni che gli utenti finali ricevono.</p>
</section>
</section>
<section id="bilanciamento-di-accuratezza-e-altri-obiettivi" class="level3" data-number="15.6.3">
<h3 data-number="15.6.3" class="anchored" data-anchor-id="bilanciamento-di-accuratezza-e-altri-obiettivi"><span class="header-section-number">15.6.3</span> Bilanciamento di Accuratezza e Altri Obiettivi</h3>
<p>I modelli di apprendimento automatico vengono spesso valutati solo in base all‚Äôaccuratezza, ma questa singola metrica non riesce a catturare completamente le prestazioni del modello e i compromessi per i sistemi di intelligenza artificiale responsabili. Altre dimensioni etiche, come correttezza, robustezza, interpretabilit√† e privacy, possono competere con la pura accuratezza predittiva durante lo sviluppo del modello. Ad esempio, modelli intrinsecamente interpretabili come piccoli alberi decisionali o classificatori lineari con funzionalit√† semplificate barattano intenzionalmente una certa accuratezza per la trasparenza nel comportamento del modello e nelle previsioni. Mentre questi modelli semplificati raggiungono una minore accuratezza non catturando tutta la complessit√† nel set di dati, una migliore interpretabilit√† crea fiducia consentendo l‚Äôanalisi diretta da parte di professionisti umani.</p>
<p>Inoltre, alcune tecniche pensate per migliorare la robustezza avversaria, come esempi di training avversario o riduzione della dimensionalit√†, possono degradare l‚Äôaccuratezza dei dati di convalida puliti. In applicazioni sensibili come l‚Äôassistenza sanitaria, concentrarsi strettamente sull‚Äôaccuratezza all‚Äôavanguardia comporta rischi etici se consente ai modelli di fare pi√π affidamento su correlazioni spurie che introducono distorsioni o utilizzano ragionamenti opachi. Pertanto, gli obiettivi di prestazione appropriati dipendono in larga misura dal contesto socio-tecnico.</p>
<p>Metodologie come <a href="https://vsdesign.org/">Value Sensitive Design</a> forniscono framework per valutare formalmente le priorit√† di vari stakeholder all‚Äôinterno del sistema di distribuzione nel mondo reale. Ci√≤ spiega le tensioni tra valori quali accuratezza, interpretabilit√† ed equit√†, che possono quindi orientare decisioni di compromesso responsabili. Per un sistema di diagnosi medica, raggiungere la massima accuratezza potrebbe non essere l‚Äôobiettivo unico: migliorare la trasparenza per creare fiducia nei professionisti o ridurre i pregiudizi verso i gruppi minoritari potrebbe giustificare piccole perdite di accuratezza. L‚Äôanalisi del contesto socio-tecnico √® fondamentale per stabilire questi obiettivi.</p>
<p>Adottando una visione olistica, possiamo bilanciare responsabilmente l‚Äôaccuratezza con altri obiettivi etici per il successo del modello. Il monitoraggio continuo delle prestazioni lungo pi√π dimensioni √® fondamentale man mano che il sistema si evolve dopo la distribuzione.</p>
</section>
</section>
<section id="considerazioni-etiche-nella-progettazione-dellia" class="level2 page-columns page-full" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="considerazioni-etiche-nella-progettazione-dellia"><span class="header-section-number">15.7</span> Considerazioni Etiche Nella Progettazione dell‚ÄôIA</h2>
<p>Dobbiamo discutere almeno di alcune delle numerose questioni etiche in gioco nella progettazione e nell‚Äôapplicazione di sistemi di intelligenza artificiale e di diversi framework per affrontare tali questioni, tra cui quelle relative alla sicurezza dell‚Äôintelligenza artificiale, all‚Äôinterazione uomo-computer (HCI) e alla scienza, tecnologia e societ√† (STS).</p>
<section id="sicurezza-dellintelligenza-artificiale-e-allineamento-dei-valori" class="level3 page-columns page-full" data-number="15.7.1">
<h3 data-number="15.7.1" class="anchored" data-anchor-id="sicurezza-dellintelligenza-artificiale-e-allineamento-dei-valori"><span class="header-section-number">15.7.1</span> Sicurezza dell‚ÄôIntelligenza Artificiale e Allineamento dei Valori</h3>
<p>Nel 1960, Norbert Weiner scrisse: ‚Äú‚Äôse utilizziamo, per raggiungere i nostri scopi, un‚Äôagenzia meccanica con il cui funzionamento non possiamo interferire efficacemente‚Ä¶ faremmo meglio ad essere abbastanza sicuri che lo scopo attribuito alla macchina sia lo scopo che desideriamo‚Äù <span class="citation" data-cites="wiener1960some">(<a href="../../../references.it.html#ref-wiener1960some" role="doc-biblioref">Wiener 1960</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wiener1960some" class="csl-entry" role="listitem">
Wiener, Norbert. 1960. <span>¬´Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers.¬ª</span> <em>Science</em> 131 (3410): 1355‚Äì58. <a href="https://doi.org/10.1126/science.131.3410.1355">https://doi.org/10.1126/science.131.3410.1355</a>.
</div><div id="ref-russell2021human" class="csl-entry" role="listitem">
Russell, Stuart. 2021. <span>¬´Human-compatible artificial intelligence¬ª</span>. <em>Human-like machine intelligence</em>, 3‚Äì23.
</div></div><p>Negli ultimi anni, poich√© le capacit√† dei modelli di deep learning hanno raggiunto, e talvolta persino superato, le capacit√† umane, la questione della creazione di sistemi di intelligenza artificiale che agiscano in accordo con le intenzioni umane invece di perseguire obiettivi non intenzionali o indesiderati √® diventata fonte di preoccupazione <span class="citation" data-cites="russell2021human">(<a href="../../../references.it.html#ref-russell2021human" role="doc-biblioref">Russell 2021</a>)</span>. Nel campo della sicurezza dell‚ÄôIA, un obiettivo particolare riguarda ‚Äúl‚Äôallineamento dei valori‚Äù, ovvero il problema di come codificare lo scopo ‚Äúgiusto‚Äù nelle macchine <a href="https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf">Intelligenza artificiale compatibile con gli esseri umani</a>. L‚Äôattuale ricerca sull‚ÄôIA presuppone che conosciamo gli obiettivi che vogliamo raggiungere e ‚Äústudia la capacit√† di raggiungere gli obiettivi, non la progettazione di tali obiettivi‚Äù.</p>
<p>Tuttavia, i complessi contesti di distribuzione nel mondo reale rendono difficile definire esplicitamente ‚Äúlo scopo giusto‚Äù per le macchine, richiedendo quadri per l‚Äôimpostazione di obiettivi responsabili ed etici. Metodologie come <a href="https://vsdesign.org/">Value Sensitive Design</a> forniscono meccanismi formali per far emergere le tensioni tra i valori e le priorit√† delle parti interessate.</p>
<p>Adottando una visione socio-tecnica olistica, possiamo garantire meglio che i sistemi intelligenti perseguano obiettivi che si allineano con ampie intenzioni umane anzich√© massimizzare metriche ristrette come la sola accuratezza. Raggiungere questo obiettivo nella pratica rimane una questione di ricerca aperta e critica man mano che le capacit√† dell‚ÄôIA avanzano rapidamente.</p>
<p>L‚Äôassenza di questo allineamento pu√≤ portare a diversi problemi di sicurezza dell‚ÄôIA, come documentato in una variet√† di <a href="https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/">modelli di deep learning</a>. Una caratteristica comune dei sistemi che ottimizzano per un obiettivo √® che le variabili non direttamente incluse nell‚Äôobiettivo possono essere impostate su valori estremi per aiutare a ottimizzare per quell‚Äôobiettivo, portando a problemi caratterizzati come gioco di specifiche, hacking di ricompensa, ecc., nel ‚Äúreinforcement learning (RL)‚Äù [apprendimento per rinforzo].</p>
<p>Negli ultimi anni, un‚Äôimplementazione particolarmente popolare di RL √® stata quella dei modelli pre-addestrati utilizzando apprendimento auto-supervisionato e ‚ÄúReinforcement Learning From Human Feedback (RLHF)‚Äù [apprendimento per rinforzo fine-tuned da feedback umano] <span class="citation" data-cites="christiano2017deep">(<a href="../../../references.it.html#ref-christiano2017deep" role="doc-biblioref">Christiano et al. 2017</a>)</span>. Ngo 2022 <span class="citation" data-cites="ngo2022alignment">(<a href="../../../references.it.html#ref-ngo2022alignment" role="doc-biblioref">Ngo, Chan, e Mindermann 2022</a>)</span> sostiene che premiando i modelli per apparire innocui ed etici e massimizzando al contempo i risultati utili, RLHF potrebbe incoraggiare l‚Äôemergere di tre propriet√† problematiche: hacking della ricompensa consapevole della situazione, in cui le politiche sfruttano la fallibilit√† umana per ottenere un‚Äôelevata ricompensa, obiettivi rappresentati internamente non allineati che si generalizzano oltre la distribuzione di messa a punto RLHF e strategie di ricerca del potere.</p>
<div class="no-row-height column-margin column-container"><div id="ref-christiano2017deep" class="csl-entry" role="listitem">
Christiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, e Dario Amodei. 2017. <span>¬´Deep Reinforcement Learning from Human Preferences¬ª</span>. In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4299‚Äì4307. <a href="https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>.
</div><div id="ref-ngo2022alignment" class="csl-entry" role="listitem">
Ngo, Richard, Lawrence Chan, e S√∂ren Mindermann. 2022. <span>¬´The alignment problem from a deep learning perspective¬ª</span>. <em>ArXiv preprint</em> abs/2209.00626. <a href="https://arxiv.org/abs/2209.00626">https://arxiv.org/abs/2209.00626</a>.
</div><div id="ref-amodei2016concrete" class="csl-entry" role="listitem">
Van Noorden, Richard. 2016. <span>¬´<span>ArXiv</span> preprint server plans multimillion-dollar overhaul¬ª</span>. <em>Nature</em> 534 (7609): 602‚Äì2. <a href="https://doi.org/10.1038/534602a">https://doi.org/10.1038/534602a</a>.
</div></div><p>Allo stesso modo, <span class="citation" data-cites="amodei2016concrete">Van Noorden (<a href="../../../references.it.html#ref-amodei2016concrete" role="doc-biblioref">2016</a>)</span> delinea sei problemi concreti per la sicurezza dell‚ÄôIA, tra cui evitare effetti collaterali negativi, evitare hacking della ricompensa, supervisione scalabile per aspetti dell‚Äôobiettivo che sono troppo costosi per essere valutati frequentemente durante il training, strategie di esplorazione sicure che incoraggiano la creativit√† prevenendo al contempo i danni e robustezza allo spostamento distributivo in ambienti di test invisibili.</p>
</section>
<section id="sistemi-autonomi-e-controllo-e-fiducia" class="level3 page-columns page-full" data-number="15.7.2">
<h3 data-number="15.7.2" class="anchored" data-anchor-id="sistemi-autonomi-e-controllo-e-fiducia"><span class="header-section-number">15.7.2</span> Sistemi Autonomi e Controllo [e Fiducia]</h3>
<p>Le conseguenze dei sistemi autonomi che agiscono indipendentemente dalla supervisione umana e spesso al di fuori del giudizio umano sono state ampiamente documentate in diversi settori e casi d‚Äôuso. Pi√π di recente, il Dipartimento dei veicoli a motore della California ha sospeso i permessi di distribuzione e collaudo di Cruise per i suoi veicoli autonomi, citando <a href="https://www.cnbc.com/2023/10/24/california-dmv-suspends-cruises-self-driving-car-permits.html">‚Äúrischi irragionevoli per la sicurezza pubblica‚Äù</a>. Uno di questi <a href="https://www.cnbc.com/2023/10/17/cruise-under-nhtsa-probe-into-autonomous-driving-pedestrian-injuries.html">incidenti</a> si √® verificato quando un veicolo ha colpito un pedone che stava attraversando le strisce pedonali dopo che il semaforo era diventato verde e al veicolo √® stato permesso di procedere. Nel 2018, un pedone che attraversava la strada con la sua bicicletta √® morto quando un‚Äôauto Uber a guida autonoma, che operava in modalit√† autonoma, <a href="https://www.bbc.com/news/technology-54175359">non √® riuscita a classificare accuratamente il suo corpo in movimento come un oggetto da evitare</a>.</p>
<p>Anche i sistemi autonomi oltre ai veicoli a guida autonoma sono suscettibili a tali problemi, con conseguenze potenzialmente pi√π gravi, poich√© i droni alimentati da remoto stanno gi√† <a href="https://www.reuters.com/technology/human-machine-teams-driven-by-ai-are-about-reshape-warfare-2023-09-08/">rimodellando la guerra</a>. Sebbene tali incidenti sollevino importanti questioni etiche su <a href="https://www.cigionline.org/articles/who-responsible-when-autonomous-systems-fail/">chi dovrebbe essere ritenuto responsabile</a> quando questi sistemi falliscono, evidenziano anche le sfide tecniche nel dare il pieno controllo di attivit√† complesse e reali alle macchine.</p>
<p>In sostanza, c‚Äô√® una tensione tra autonomia umana e delle macchine. Le discipline ingegneristiche e informatiche hanno teso a concentrarsi sull‚Äôautonomia delle macchine. Ad esempio, a partire dal 2019, una ricerca della parola ‚Äúautonomia‚Äù nella Digital Library dell‚ÄôAssociation for Computing Machinery (ACM) rivela che dei 100 articoli pi√π citati, il 90% riguarda l‚Äôautonomia delle macchine <span class="citation" data-cites="calvo2020supporting">(<a href="../../../references.it.html#ref-calvo2020supporting" role="doc-biblioref">Calvo et al. 2020</a>)</span>. Nel tentativo di costruire sistemi a beneficio dell‚Äôumanit√†, queste discipline hanno assunto, senza dubbio, l‚Äôaumento della produttivit√†, dell‚Äôefficienza e dell‚Äôautomazione come strategie primarie per il beneficio dell‚Äôumanit√†.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mccarthy1981epistemological" class="csl-entry" role="listitem">
McCarthy, John. 1981. <span>¬´Epistemological Problems Of Artificial Intelligence¬ª</span>. In <em>Readings in Artificial Intelligence</em>, 459‚Äì65. Elsevier. <a href="https://doi.org/10.1016/b978-0-934613-03-3.50035-0">https://doi.org/10.1016/b978-0-934613-03-3.50035-0</a>.
</div></div><p>Questi obiettivi pongono l‚Äôautomazione delle macchine in prima linea, spesso a spese dell‚Äôuomo. Questo approccio soffre di sfide intrinseche, come notato fin dai primi giorni dell‚ÄôIA attraverso il ‚ÄúFrame problem‚Äù [specifica degli effetti] e il ‚Äúqualification problem‚Äù [qualificazione delle precondizioni] (cfr. http://www.diag.uniroma1.it/~nardi/Didattica/RC/lezioni/sitcalc-1.pdf), che formalizza l‚Äôosservazione che √® impossibile specificare tutte le precondizioni necessarie per il successo di un‚Äôazione nel mondo reale <span class="citation" data-cites="mccarthy1981epistemological">(<a href="../../../references.it.html#ref-mccarthy1981epistemological" role="doc-biblioref">McCarthy 1981</a>)</span>.</p>
<p>Queste limitazioni logiche hanno dato origine ad approcci matematici come la ‚ÄúResponsibility-sensitive safety (RSS)‚Äù [sicurezza sensibile alla responsabilit√†] <span class="citation" data-cites="shalev2017formal">(<a href="../../../references.it.html#ref-shalev2017formal" role="doc-biblioref">Shalev-Shwartz, Shammah, e Shashua 2017</a>)</span>, che mira a scomporre l‚Äôobiettivo finale di un sistema di guida automatizzato (vale a dire la sicurezza) in condizioni concrete e verificabili che possono essere rigorosamente formulate in termini matematici. L‚Äôobiettivo dell‚ÄôRSS √® che tali norme di sicurezza garantiscano la sicurezza del ‚ÄúAutomated Driving System (ADS)‚Äù [sistema di guida autonoma] nella rigorosa forma di dimostrazione matematica. Tuttavia, tali approcci tendono a utilizzare l‚Äôautomazione per affrontare i problemi dell‚Äôautomazione e sono suscettibili a molti degli stessi problemi.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shalev2017formal" class="csl-entry" role="listitem">
Shalev-Shwartz, Shai, Shaked Shammah, e Amnon Shashua. 2017. <span>¬´On a formal model of safe and scalable self-driving cars¬ª</span>. <em>ArXiv preprint</em> abs/1708.06374. <a href="https://arxiv.org/abs/1708.06374">https://arxiv.org/abs/1708.06374</a>.
</div><div id="ref-friedman1996value" class="csl-entry" role="listitem">
Friedman, Batya. 1996. <span>¬´Value-sensitive design¬ª</span>. <em>Interactions</em> 3 (6): 16‚Äì23. <a href="https://doi.org/10.1145/242485.242493">https://doi.org/10.1145/242485.242493</a>.
</div><div id="ref-peters2018designing" class="csl-entry" role="listitem">
Peters, Dorian, Rafael A. Calvo, e Richard M. Ryan. 2018. <span>¬´Designing for Motivation, Engagement and Wellbeing in Digital Experience¬ª</span>. <em>Front. Psychol.</em> 9 (maggio): 797. <a href="https://doi.org/10.3389/fpsyg.2018.00797">https://doi.org/10.3389/fpsyg.2018.00797</a>.
</div><div id="ref-ryan2000self" class="csl-entry" role="listitem">
Ryan, Richard M., e Edward L. Deci. 2000. <span>¬´Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.¬ª</span> <em>Am. Psychol.</em> 55 (1): 68‚Äì78. <a href="https://doi.org/10.1037/0003-066x.55.1.68">https://doi.org/10.1037/0003-066x.55.1.68</a>.
</div></div><p>Un altro approccio per combattere questi problemi √® concentrarsi sulla progettazione ‚Äúhuman-centered‚Äù di sistemi interattivi che incorporano il controllo umano. Il design sensibile al valore <span class="citation" data-cites="friedman1996value">(<a href="../../../references.it.html#ref-friedman1996value" role="doc-biblioref">Friedman 1996</a>)</span> ha descritto tre fattori di progettazione chiave per un‚Äôinterfaccia utente che hanno un impatto sull‚Äôautonomia, tra cui capacit√† del sistema, complessit√†, rappresentazione errata e fluidit√†. Un modello pi√π recente, chiamato METUX (A Model for Motivation, Engagement, and Thriving in the User Experience), sfrutta le intuizioni della ‚ÄúSelf-determination Theory (SDT)‚Äù in psicologia per identificare sei sfere distinte dell‚Äôesperienza tecnologica che contribuiscono ai sistemi di progettazione che promuovono il benessere e la prosperit√† umana <span class="citation" data-cites="peters2018designing">(<a href="../../../references.it.html#ref-peters2018designing" role="doc-biblioref">Peters, Calvo, e Ryan 2018</a>)</span>. SDT definisce l‚Äôautonomia come agire in base ai propri obiettivi e valori, il che √® distinto dall‚Äôuso dell‚Äôautonomia come semplice sinonimo di indipendenza o di controllo <span class="citation" data-cites="ryan2000self">(<a href="../../../references.it.html#ref-ryan2000self" role="doc-biblioref">Ryan e Deci 2000</a>)</span>.</p>
<p><span class="citation" data-cites="calvo2020supporting">Calvo et al. (<a href="../../../references.it.html#ref-calvo2020supporting" role="doc-biblioref">2020</a>)</span> elabora METUX e le sue sei ‚Äúsfere di esperienza tecnologica‚Äù nel contesto dei sistemi di raccomandazione AI. Propongono queste sfere (Adozione, Interfaccia, Attivit√†, Comportamento, Vita e Societ√†) come un modo per organizzare il pensiero e la valutazione della progettazione tecnologica al fine di catturare in modo appropriato gli impatti contraddittori e a valle sull‚Äôautonomia umana quando interagisce con i sistemi AI.</p>
<div class="no-row-height column-margin column-container"><div id="ref-calvo2020supporting" class="csl-entry" role="listitem">
Calvo, Rafael A, Dorian Peters, Karina Vold, e Richard M Ryan. 2020. <span>¬´Supporting human autonomy in <span>AI</span> systems: <span>A</span> framework for ethical enquiry¬ª</span>. <em>Ethics of digital well-being: A multidisciplinary approach</em>, 31‚Äì54.
</div></div></section>
<section id="impatti-economici-su-posti-di-lavoro-competenze-salari" class="level3 page-columns page-full" data-number="15.7.3">
<h3 data-number="15.7.3" class="anchored" data-anchor-id="impatti-economici-su-posti-di-lavoro-competenze-salari"><span class="header-section-number">15.7.3</span> Impatti Economici su Posti di Lavoro, Competenze, Salari</h3>
<p>Una delle principali preoccupazioni dell‚Äôattuale ascesa delle tecnologie AI √® la disoccupazione diffusa. Con l‚Äôespansione delle capacit√† dei sistemi AI, molti temono che queste tecnologie causeranno una perdita assoluta di posti di lavoro, poich√© sostituiranno i lavoratori attuali e supereranno ruoli occupazionali alternativi in tutti i settori. Tuttavia, il cambiamento dei panorami economici per mano dell‚Äôautomazione non √® una novit√† e, storicamente, si √® scoperto che riflette pattern di <em>spostamento</em> piuttosto che di sostituzione <span class="citation" data-cites="shneiderman2022human">(<a href="../../../references.it.html#ref-shneiderman2022human" role="doc-biblioref">Shneiderman 2022</a>)</span>‚ÄîCapitolo 4. In particolare, l‚Äôautomazione di solito riduce i costi e aumenta la qualit√†, aumentando notevolmente l‚Äôaccesso e la domanda. La necessit√† di servire questi mercati in crescita spinge la produzione, creando nuovi posti di lavoro.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shneiderman2022human" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî. 2022. <em>Human-centered <span>AI</span></em>. Oxford University Press.
</div></div><p>Inoltre, gli studi hanno scoperto che i tentativi di raggiungere un‚Äôautomazione ‚Äúlights-out‚Äù, ovvero un‚Äôautomazione produttiva e flessibile con un numero minimo di lavoratori umani, non hanno avuto successo. I tentativi di farlo hanno portato a quella che la task force del MIT Work of the Future ha definito <a href="https://hbr.org/2023/03/a-smarter-strategy-for-using-robots">‚Äúautomazione a somma zero‚Äù</a>, in cui la flessibilit√† dei processi viene sacrificata per aumentare la produttivit√†.</p>
<p>Al contrario, la task force propone un approccio di ‚Äúautomazione a somma positiva‚Äù in cui la flessibilit√† viene aumentata progettando una tecnologia che incorpora strategicamente gli esseri umani dove sono molto necessari, rendendo pi√π facile per i dipendenti della linea addestrare e correggere i robot, utilizzando un approccio bottom-up per identificare quali attivit√† dovrebbero essere automatizzate; e scegliendo le giuste metriche per misurare il successo (vedi <a href="https://workofthefuture-mit-edu.ezp-prod1.hul.harvard.edu/wp-content/uploads/2021/01/2020-Final-Report4.pdf">Work of the Future</a> del MIT).</p>
<p>Tuttavia, l‚Äôottimismo delle prospettive di alto livello non esclude danni individuali, specialmente per coloro le cui competenze e lavori saranno resi obsoleti dall‚Äôautomazione. La pressione pubblica e legislativa, cos√¨ come gli sforzi di responsabilit√† sociale delle aziende, dovranno essere diretti alla creazione di politiche che condividano i vantaggi dell‚Äôautomazione con i lavoratori e si traducano in salari minimi e benefici pi√π elevati.</p>
</section>
<section id="comunicazione-scientifica-e-alfabetizzazione-ia" class="level3 page-columns page-full" data-number="15.7.4">
<h3 data-number="15.7.4" class="anchored" data-anchor-id="comunicazione-scientifica-e-alfabetizzazione-ia"><span class="header-section-number">15.7.4</span> Comunicazione Scientifica e Alfabetizzazione IA</h3>
<p>Un sondaggio del 1993 sulle convinzioni di 3000 adulti nordamericani sulla ‚Äúmacchina pensante elettronica‚Äù ha rivelato due prospettive principali del primo computer: la prospettiva dello ‚Äústrumento utile dell‚Äôuomo‚Äù e la prospettiva della ‚Äúmacchina pensante fantastica‚Äù. Gli atteggiamenti che contribuiscono alla visione della ‚Äúmacchina pensante fantastica‚Äù in questo e altri studi hanno rivelato una caratterizzazione dei computer come ‚Äúcervelli intelligenti, pi√π intelligenti delle persone, illimitati, veloci, misteriosi e spaventosi‚Äù <span class="citation" data-cites="martin1993myth">(<a href="../../../references.it.html#ref-martin1993myth" role="doc-biblioref">Martin 1993</a>)</span>. Questi timori evidenziano una componente facilmente trascurata dell‚ÄôIA responsabile, specialmente in mezzo alla corsa alla commercializzazione di tali tecnologie: la comunicazione scientifica che comunica accuratamente le capacit√† <em>e</em> le limitazioni di questi sistemi, fornendo al contempo trasparenza sui limiti della conoscenza degli esperti su questi sistemi.</p>
<div class="no-row-height column-margin column-container"><div id="ref-martin1993myth" class="csl-entry" role="listitem">
Martin, C. Dianne. 1993. <span>¬´The myth of the awesome thinking machine¬ª</span>. <em>Commun. ACM</em> 36 (4): 120‚Äì33. <a href="https://doi.org/10.1145/255950.153587">https://doi.org/10.1145/255950.153587</a>.
</div><div id="ref-handlin1965science" class="csl-entry" role="listitem">
Handlin, Oscar. 1965. <span>¬´Science and technology in popular culture¬ª</span>. <em>Daedalus-us.</em>, 156‚Äì70.
</div></div><p>Man mano che le capacit√† dei sistemi di IA si espandono oltre la comprensione della maggior parte delle persone, c‚Äô√® una tendenza naturale a presumere i tipi di mondi apocalittici dipinti dai nostri media. Ci√≤ √® dovuto in parte all‚Äôapparente difficolt√† di assimilare informazioni scientifiche, persino in culture tecnologicamente avanzate, che porta i prodotti della scienza a essere percepiti come magia, ‚Äúcomprensibili solo in termini di ci√≤ che hanno fatto, non di come hanno funzionato‚Äù <span class="citation" data-cites="handlin1965science">(<a href="../../../references.it.html#ref-handlin1965science" role="doc-biblioref">Handlin 1965</a>)</span>.</p>
<p>Mentre le aziende tecnologiche dovrebbero essere ritenute responsabili per aver limitato le affermazioni grandiose e non essere cadute in cicli di clamore, la ricerca che studia la comunicazione scientifica, in particolare per quanto riguarda l‚Äôintelligenza artificiale (generativa), sar√† utile anche per tracciare e correggere la comprensione pubblica di queste tecnologie. Un‚Äôanalisi del database accademico Scopus ha scoperto che tale ricerca √® scarsa, con solo una manciata di articoli che menzionano sia ‚Äúcomunicazione scientifica‚Äù che ‚Äúintelligenza artificiale‚Äù <span class="citation" data-cites="schafer2023notorious">(<a href="../../../references.it.html#ref-schafer2023notorious" role="doc-biblioref">Sch√§fer 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-schafer2023notorious" class="csl-entry" role="listitem">
Sch√§fer, Mike S. 2023. <span>¬´The Notorious <span>GPT:</span> <span>Science</span> communication in the age of artificial intelligence¬ª</span>. <em>Journal of Science Communication</em> 22 (02): Y02. <a href="https://doi.org/10.22323/2.22020402">https://doi.org/10.22323/2.22020402</a>.
</div><div id="ref-lindgren2023handbook" class="csl-entry" role="listitem">
Lindgren, Simon. 2023. <em>Handbook of Critical Studies of Artificial Intelligence</em>. Edward Elgar Publishing.
</div><div id="ref-ng2021ai" class="csl-entry" role="listitem">
Ng, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, e Maggie Shen Qiao. 2021. <span>¬´<span>AI</span> literacy: <span>Definition,</span> teaching, evaluation and ethical issues¬ª</span>. <em>Proceedings of the Association for Information Science and Technology</em> 58 (1): 504‚Äì9.
</div></div><p>La ricerca che espone le prospettive, i ‚Äúframe‚Äù e le immagini del futuro promosse da istituzioni accademiche, aziende tecnologiche, stakeholder, enti regolatori, giornalisti, ONG e altri aiuter√† anche a identificare potenziali lacune nell‚Äôalfabetizzazione AI tra gli adulti <span class="citation" data-cites="lindgren2023handbook">(<a href="../../../references.it.html#ref-lindgren2023handbook" role="doc-biblioref">Lindgren 2023</a>)</span>. Una maggiore attenzione all‚Äôalfabetizzazione AI da parte di tutti gli stakeholder sar√† importante per aiutare le persone le cui competenze sono rese obsolete dall‚Äôautomazione AI <span class="citation" data-cites="ng2021ai">(<a href="../../../references.it.html#ref-ng2021ai" role="doc-biblioref">Ng et al. 2021</a>)</span>.</p>
<p><em>‚ÄúMa anche coloro che non acquisiscono mai quella comprensione hanno bisogno di rassicurazioni sul fatto che esista una connessione tra gli obiettivi della scienza e il loro benessere e, soprattutto, che lo scienziato non sia un uomo completamente a parte, ma uno che condivide parte del loro valore.‚Äù</em> (Handlin, 1965)</p>
</section>
</section>
<section id="conclusione" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">15.8</span> Conclusione</h2>
<p>Un‚Äôintelligenza artificiale responsabile √® fondamentale poich√© i sistemi di apprendimento automatico esercitano una crescente influenza nei settori sanitario, lavorativo, finanziario e della giustizia penale. Mentre l‚Äôintelligenza artificiale promette immensi benefici, i modelli progettati in modo sconsiderato rischiano di perpetrare danni attraverso pregiudizi, violazioni della privacy, comportamenti indesiderati e altre insidie.</p>
<p>Mantenere i principi di equit√†, spiegabilit√†, responsabilit√†, sicurezza e trasparenza consente lo sviluppo di un‚Äôintelligenza artificiale etica allineata ai valori umani. Tuttavia, l‚Äôimplementazione di questi principi comporta il superamento di complesse sfide tecniche e sociali relative al rilevamento di pregiudizi nei set di dati, alla scelta di appropriati compromessi nei modelli, alla protezione di dati di training di qualit√† e altro ancora. Framework come la progettazione sensibile al valore guidano il bilanciamento dell‚Äôaccuratezza rispetto ad altri obiettivi in base alle esigenze delle parti interessate.</p>
<p>Guardando al futuro, il progresso dell‚Äôintelligenza artificiale responsabile richiede una ricerca continua e l‚Äôimpegno del settore. Sono necessari benchmark pi√π standardizzati per confrontare pregiudizi e robustezza dei modelli. Man mano che il TinyML personalizzato si espande, abilitare una trasparenza efficiente e il controllo dell‚Äôutente per i dispositivi edge giustifica l‚Äôattenzione. Le strutture e le politiche di incentivazione riviste devono incoraggiare uno sviluppo deliberato ed etico prima di un‚Äôimplementazione sconsiderata. L‚Äôistruzione sulla cultura dell‚Äôintelligenza artificiale e sui suoi limiti contribuir√† ulteriormente alla comprensione pubblica.</p>
<p>I metodi responsabili sottolineano che, mentre l‚Äôapprendimento automatico offre un potenziale immenso, un‚Äôapplicazione sconsiderata rischia di avere conseguenze negative. La collaborazione interdisciplinare e la progettazione incentrata sull‚Äôuomo sono essenziali affinch√© l‚Äôintelligenza artificiale possa promuovere un ampio beneficio sociale. Il percorso da seguire non risiede in una checklist arbitraria, ma in un impegno costante per comprendere e sostenere la nostra responsabilit√† etica a ogni passo. Intraprendendo un‚Äôazione coscienziosa, la comunit√† dell‚Äôapprendimento automatico pu√≤ guidare l‚Äôintelligenza artificiale verso l‚Äôemancipazione di tutte le persone in modo equo e sicuro.</p>
</section>
<section id="sec-responsible-ai-resource" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="sec-responsible-ai-resource"><span class="header-section-number">15.9</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1Z9VpUKGOOfUIg6x04aXLVYl-9QoablElOlxhTLkAVno/edit?usp=drive_link&amp;resourcekey=0-Nr9tvJ9KGgaL44O_iJpe4A">What am I building? What is the goal?</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1IwIXrTQNf6MLlXKV-qOuafZhWS9saTxpY2uawQUHKfg/edit?usp=drive_link&amp;resourcekey=0-Jc1kfKFb4OOhs919kyR2mA">Who is the audience?</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1UDmrEZAJtH5LkHA_mDuFovOh6kam9FnC3uBAAah4RJo/edit?usp=drive_link&amp;resourcekey=0-HFb4nRGGNRxJHz8wHXpgtg">What are the consequences?</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1vcmuhLVNFT2asKSCSGh_Ix9ht0mJZxMii8MufEMQhFA/edit?resourcekey=0-_pYLcW5aF3p3Bvud0PPQNg#slide=id.ga4ca29c69e_0_195">Responsible Data Collection.</a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><a href="#vid-fakeobama" class="quarto-xref">Video&nbsp;<span>15.1</span></a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/privacy_security/privacy_security.it.html" class="pagination-link" aria-label="Sicurezza e Privacy">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="pagination-link" aria-label="IA Sostenibile">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University). Traduzione di <a href="https://github.com/BravoBaldo">Baldassarre Cesarano</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/responsible_ai/responsible_ai.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/responsible_ai/responsible_ai.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>