<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Ottimizzazioni dei Modelli – Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" rel="next">
<link href="../../../contents/core/efficient_ai/efficient_ai.it.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalità oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/optimizations/optimizations.it.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABORATORI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/part_LABS.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">LABORATORI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/part_nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">part_nicla_vision.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/part_xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">part_xiao_esp32s3.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/part_raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">part_raspi.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/part_shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">part_shared.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">9.1</span> Introduzione</a></li>
  <li><a href="#sec-model_ops_representation" id="toc-sec-model_ops_representation" class="nav-link" data-scroll-target="#sec-model_ops_representation"><span class="header-section-number">9.2</span> Rappresentazione Efficiente del Modello</a>
  <ul>
  <li><a href="#sec-pruning" id="toc-sec-pruning" class="nav-link" data-scroll-target="#sec-pruning"><span class="header-section-number">9.2.1</span> Il Pruning</a>
  <ul class="collapse">
  <li><a href="#panoramica" id="toc-panoramica" class="nav-link" data-scroll-target="#panoramica">Panoramica</a></li>
  <li><a href="#potatura-strutturata" id="toc-potatura-strutturata" class="nav-link" data-scroll-target="#potatura-strutturata">Potatura Strutturata</a></li>
  <li><a href="#vantaggi-della-potatura-strutturata" id="toc-vantaggi-della-potatura-strutturata" class="nav-link" data-scroll-target="#vantaggi-della-potatura-strutturata">Vantaggi della Potatura Strutturata</a></li>
  <li><a href="#potatura-non-strutturata" id="toc-potatura-non-strutturata" class="nav-link" data-scroll-target="#potatura-non-strutturata">Potatura non Strutturata</a></li>
  <li><a href="#ipotesi-del-biglietto-della-lotteria" id="toc-ipotesi-del-biglietto-della-lotteria" class="nav-link" data-scroll-target="#ipotesi-del-biglietto-della-lotteria">Ipotesi del Biglietto della Lotteria</a></li>
  <li><a href="#problemi-e-limitazioni" id="toc-problemi-e-limitazioni" class="nav-link" data-scroll-target="#problemi-e-limitazioni">Problemi e Limitazioni</a></li>
  </ul></li>
  <li><a href="#compressione-del-modello" id="toc-compressione-del-modello" class="nav-link" data-scroll-target="#compressione-del-modello"><span class="header-section-number">9.2.2</span> Compressione del Modello</a>
  <ul class="collapse">
  <li><a href="#sec-kd" id="toc-sec-kd" class="nav-link" data-scroll-target="#sec-kd">Distillazione della Conoscenza</a></li>
  <li><a href="#fattorizzazione-di-matrici-di-basso-rango" id="toc-fattorizzazione-di-matrici-di-basso-rango" class="nav-link" data-scroll-target="#fattorizzazione-di-matrici-di-basso-rango">Fattorizzazione di Matrici di Basso Rango</a></li>
  <li><a href="#decomposizione-dei-tensori" id="toc-decomposizione-dei-tensori" class="nav-link" data-scroll-target="#decomposizione-dei-tensori">Decomposizione dei Tensori</a></li>
  </ul></li>
  <li><a href="#modelli-progettati-per-ledge" id="toc-modelli-progettati-per-ledge" class="nav-link" data-scroll-target="#modelli-progettati-per-ledge"><span class="header-section-number">9.2.3</span> Modelli Progettati per l’Edge</a>
  <ul class="collapse">
  <li><a href="#tecniche-di-progettazione-del-modello" id="toc-tecniche-di-progettazione-del-modello" class="nav-link" data-scroll-target="#tecniche-di-progettazione-del-modello">Tecniche di Progettazione del Modello</a></li>
  <li><a href="#architetture-di-modello-di-esempio" id="toc-architetture-di-modello-di-esempio" class="nav-link" data-scroll-target="#architetture-di-modello-di-esempio">Architetture di Modello di Esempio</a></li>
  <li><a href="#semplificazione-della-ricerca-di-architetture-di-modelli" id="toc-semplificazione-della-ricerca-di-architetture-di-modelli" class="nav-link" data-scroll-target="#semplificazione-della-ricerca-di-architetture-di-modelli">Semplificazione della Ricerca di Architetture di Modelli</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-model_ops_numerics" id="toc-sec-model_ops_numerics" class="nav-link" data-scroll-target="#sec-model_ops_numerics"><span class="header-section-number">9.3</span> Rappresentazione Numerica Efficiente</a>
  <ul>
  <li><a href="#motivazione" id="toc-motivazione" class="nav-link" data-scroll-target="#motivazione">Motivazione</a></li>
  <li><a href="#le-basi" id="toc-le-basi" class="nav-link" data-scroll-target="#le-basi"><span class="header-section-number">9.3.1</span> Le Basi</a>
  <ul class="collapse">
  <li><a href="#i-tipi" id="toc-i-tipi" class="nav-link" data-scroll-target="#i-tipi">I Tipi</a></li>
  <li><a href="#precisione" id="toc-precisione" class="nav-link" data-scroll-target="#precisione">Precisione</a></li>
  <li><a href="#codifica-e-archiviazione-numerica" id="toc-codifica-e-archiviazione-numerica" class="nav-link" data-scroll-target="#codifica-e-archiviazione-numerica">Codifica e Archiviazione Numerica</a></li>
  </ul></li>
  <li><a href="#vantaggi-dellefficienza" id="toc-vantaggi-dellefficienza" class="nav-link" data-scroll-target="#vantaggi-dellefficienza"><span class="header-section-number">9.3.2</span> Vantaggi dell’Efficienza</a></li>
  <li><a href="#sfumature-della-rappresentazione-numerica" id="toc-sfumature-della-rappresentazione-numerica" class="nav-link" data-scroll-target="#sfumature-della-rappresentazione-numerica"><span class="header-section-number">9.3.3</span> Sfumature della Rappresentazione Numerica</a>
  <ul class="collapse">
  <li><a href="#utilizzo-della-memoria" id="toc-utilizzo-della-memoria" class="nav-link" data-scroll-target="#utilizzo-della-memoria">Utilizzo della Memoria</a></li>
  <li><a href="#complessità-computazionale" id="toc-complessità-computazionale" class="nav-link" data-scroll-target="#complessità-computazionale">Complessità Computazionale</a></li>
  <li><a href="#compatibilità-hardware" id="toc-compatibilità-hardware" class="nav-link" data-scroll-target="#compatibilità-hardware">Compatibilità Hardware</a></li>
  <li><a href="#compromessi-di-precisione-e-accuratezza" id="toc-compromessi-di-precisione-e-accuratezza" class="nav-link" data-scroll-target="#compromessi-di-precisione-e-accuratezza">Compromessi di Precisione e Accuratezza</a></li>
  <li><a href="#esempi-di-compromessi" id="toc-esempi-di-compromessi" class="nav-link" data-scroll-target="#esempi-di-compromessi">Esempi di Compromessi</a></li>
  </ul></li>
  <li><a href="#sec-quant" id="toc-sec-quant" class="nav-link" data-scroll-target="#sec-quant"><span class="header-section-number">9.3.4</span> Quantizzazione</a>
  <ul class="collapse">
  <li><a href="#analisi-iniziale" id="toc-analisi-iniziale" class="nav-link" data-scroll-target="#analisi-iniziale">Analisi Iniziale</a></li>
  </ul></li>
  <li><a href="#i-tipi-1" id="toc-i-tipi-1" class="nav-link" data-scroll-target="#i-tipi-1"><span class="header-section-number">9.3.5</span> I Tipi</a>
  <ul class="collapse">
  <li><a href="#quantizzazione-uniforme" id="toc-quantizzazione-uniforme" class="nav-link" data-scroll-target="#quantizzazione-uniforme">Quantizzazione Uniforme</a></li>
  <li><a href="#quantizzazione-non-uniforme" id="toc-quantizzazione-non-uniforme" class="nav-link" data-scroll-target="#quantizzazione-non-uniforme">Quantizzazione Non-Uniforme</a></li>
  <li><a href="#quantizzazione-stocastica" id="toc-quantizzazione-stocastica" class="nav-link" data-scroll-target="#quantizzazione-stocastica">Quantizzazione Stocastica</a></li>
  <li><a href="#quantizzazione-zero-shot" id="toc-quantizzazione-zero-shot" class="nav-link" data-scroll-target="#quantizzazione-zero-shot">Quantizzazione “Zero Shot”</a></li>
  </ul></li>
  <li><a href="#calibrazione" id="toc-calibrazione" class="nav-link" data-scroll-target="#calibrazione"><span class="header-section-number">9.3.6</span> Calibrazione</a>
  <ul class="collapse">
  <li><a href="#quantizzazione-simmetrica" id="toc-quantizzazione-simmetrica" class="nav-link" data-scroll-target="#quantizzazione-simmetrica">Quantizzazione Simmetrica</a></li>
  <li><a href="#quantizzazione-asimmetrica" id="toc-quantizzazione-asimmetrica" class="nav-link" data-scroll-target="#quantizzazione-asimmetrica">Quantizzazione Asimmetrica</a></li>
  <li><a href="#granularità" id="toc-granularità" class="nav-link" data-scroll-target="#granularità">Granularità</a></li>
  <li><a href="#quantizzazione-statica-e-dinamica" id="toc-quantizzazione-statica-e-dinamica" class="nav-link" data-scroll-target="#quantizzazione-statica-e-dinamica">Quantizzazione Statica e Dinamica</a></li>
  </ul></li>
  <li><a href="#tecniche" id="toc-tecniche" class="nav-link" data-scroll-target="#tecniche"><span class="header-section-number">9.3.7</span> Tecniche</a></li>
  <li><a href="#pesi-vs.-attivazioni" id="toc-pesi-vs.-attivazioni" class="nav-link" data-scroll-target="#pesi-vs.-attivazioni"><span class="header-section-number">9.3.8</span> Pesi vs.&nbsp;Attivazioni</a></li>
  <li><a href="#compromessi" id="toc-compromessi" class="nav-link" data-scroll-target="#compromessi"><span class="header-section-number">9.3.9</span> Compromessi</a></li>
  <li><a href="#quantizzazione-e-potatura" id="toc-quantizzazione-e-potatura" class="nav-link" data-scroll-target="#quantizzazione-e-potatura"><span class="header-section-number">9.3.10</span> Quantizzazione e Potatura</a></li>
  <li><a href="#quantizzazione-edge-aware" id="toc-quantizzazione-edge-aware" class="nav-link" data-scroll-target="#quantizzazione-edge-aware"><span class="header-section-number">9.3.11</span> Quantizzazione Edge-aware</a></li>
  </ul></li>
  <li><a href="#sec-model_ops_hw" id="toc-sec-model_ops_hw" class="nav-link" data-scroll-target="#sec-model_ops_hw"><span class="header-section-number">9.4</span> Implementazione Hardware Efficiente</a>
  <ul>
  <li><a href="#ricerca-di-architettura-neurale-basata-sullhardware" id="toc-ricerca-di-architettura-neurale-basata-sullhardware" class="nav-link" data-scroll-target="#ricerca-di-architettura-neurale-basata-sullhardware"><span class="header-section-number">9.4.1</span> Ricerca di Architettura Neurale Basata sull’Hardware</a>
  <ul class="collapse">
  <li><a href="#configurazione-single-target-fixed-platfrom" id="toc-configurazione-single-target-fixed-platfrom" class="nav-link" data-scroll-target="#configurazione-single-target-fixed-platfrom">Configurazione Single Target, Fixed Platfrom</a></li>
  <li><a href="#configurazioni-single-target-multiple-platform" id="toc-configurazioni-single-target-multiple-platform" class="nav-link" data-scroll-target="#configurazioni-single-target-multiple-platform">Configurazioni Single Target, Multiple Platform</a></li>
  <li><a href="#target-multipli" id="toc-target-multipli" class="nav-link" data-scroll-target="#target-multipli">Target Multipli</a></li>
  <li><a href="#esempi-di-hardware-aware-neural-architecture-search" id="toc-esempi-di-hardware-aware-neural-architecture-search" class="nav-link" data-scroll-target="#esempi-di-hardware-aware-neural-architecture-search">Esempi di “Hardware-Aware Neural Architecture Search”</a></li>
  <li><a href="#topology-aware-nas" id="toc-topology-aware-nas" class="nav-link" data-scroll-target="#topology-aware-nas">Topology-Aware NAS</a></li>
  </ul></li>
  <li><a href="#sfide-nella-hardware-aware-neural-architecture-search" id="toc-sfide-nella-hardware-aware-neural-architecture-search" class="nav-link" data-scroll-target="#sfide-nella-hardware-aware-neural-architecture-search"><span class="header-section-number">9.4.2</span> Sfide nella “Hardware-Aware Neural Architecture Search”</a></li>
  <li><a href="#ottimizzazioni-del-kernel" id="toc-ottimizzazioni-del-kernel" class="nav-link" data-scroll-target="#ottimizzazioni-del-kernel"><span class="header-section-number">9.4.3</span> Ottimizzazioni del Kernel</a>
  <ul class="collapse">
  <li><a href="#ottimizzazioni-del-kernel-generali" id="toc-ottimizzazioni-del-kernel-generali" class="nav-link" data-scroll-target="#ottimizzazioni-del-kernel-generali">Ottimizzazioni del kernel Generali</a></li>
  </ul></li>
  <li><a href="#compute-in-memory-cim" id="toc-compute-in-memory-cim" class="nav-link" data-scroll-target="#compute-in-memory-cim"><span class="header-section-number">9.4.4</span> Compute-in-Memory (CiM)</a></li>
  <li><a href="#ottimizzazione-dellaccesso-alla-memoria" id="toc-ottimizzazione-dellaccesso-alla-memoria" class="nav-link" data-scroll-target="#ottimizzazione-dellaccesso-alla-memoria"><span class="header-section-number">9.4.5</span> Ottimizzazione dell’Accesso alla Memoria</a>
  <ul class="collapse">
  <li><a href="#sfruttamento-dei-dati-sparsi" id="toc-sfruttamento-dei-dati-sparsi" class="nav-link" data-scroll-target="#sfruttamento-dei-dati-sparsi">Sfruttamento dei Dati Sparsi</a></li>
  <li><a href="#framework-di-ottimizzazione" id="toc-framework-di-ottimizzazione" class="nav-link" data-scroll-target="#framework-di-ottimizzazione">Framework di Ottimizzazione</a></li>
  <li><a href="#hardware-costruito-attorno-al-software" id="toc-hardware-costruito-attorno-al-software" class="nav-link" data-scroll-target="#hardware-costruito-attorno-al-software">Hardware Costruito Attorno al Software</a></li>
  <li><a href="#splitnet" id="toc-splitnet" class="nav-link" data-scroll-target="#splitnet">SplitNet</a></li>
  <li><a href="#hardware-specifico-per-il-data-augmentation" id="toc-hardware-specifico-per-il-data-augmentation" class="nav-link" data-scroll-target="#hardware-specifico-per-il-data-augmentation">Hardware Specifico per il “Data Augmentation”</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#supporto-software-e-framework" id="toc-supporto-software-e-framework" class="nav-link" data-scroll-target="#supporto-software-e-framework"><span class="header-section-number">9.5</span> Supporto Software e Framework</a>
  <ul>
  <li><a href="#api-native-di-ottimizzazione" id="toc-api-native-di-ottimizzazione" class="nav-link" data-scroll-target="#api-native-di-ottimizzazione"><span class="header-section-number">9.5.1</span> API Native di Ottimizzazione</a></li>
  <li><a href="#strumenti-di-ottimizzazione-automatizzata" id="toc-strumenti-di-ottimizzazione-automatizzata" class="nav-link" data-scroll-target="#strumenti-di-ottimizzazione-automatizzata"><span class="header-section-number">9.5.2</span> Strumenti di Ottimizzazione Automatizzata</a></li>
  <li><a href="#librerie-di-ottimizzazione-hardware" id="toc-librerie-di-ottimizzazione-hardware" class="nav-link" data-scroll-target="#librerie-di-ottimizzazione-hardware"><span class="header-section-number">9.5.3</span> Librerie di Ottimizzazione Hardware</a></li>
  <li><a href="#visualizzazione-delle-ottimizzazioni" id="toc-visualizzazione-delle-ottimizzazioni" class="nav-link" data-scroll-target="#visualizzazione-delle-ottimizzazioni"><span class="header-section-number">9.5.4</span> Visualizzazione delle Ottimizzazioni</a></li>
  <li><a href="#conversione-e-distribuzione-del-modello" id="toc-conversione-e-distribuzione-del-modello" class="nav-link" data-scroll-target="#conversione-e-distribuzione-del-modello"><span class="header-section-number">9.5.5</span> Conversione e Distribuzione del Modello</a></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">9.6</span> Conclusione</a></li>
  <li><a href="#sec-model-optimizations-resource" id="toc-sec-model-optimizations-resource" class="nav-link" data-scroll-target="#sec-model-optimizations-resource"><span class="header-section-number">9.7</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/optimizations/optimizations.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/optimizations/optimizations.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-model_optimizations" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-model-optimizations-resource">Slide</a>, <a href="#sec-model-optimizations-resource">Video</a>, <a href="#sec-model-optimizations-resource">Esercizi</a>, <a href="#sec-model-optimizations-resource">Laboratori</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_model_optimizations.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Illustrazione di un modello di rete neurale rappresentato come un cantiere edile affollato, con un gruppo eterogeneo di operai edili, sia uomini che donne, di varie etnie, etichettati come ‘pruning’, ‘quantization’ e ‘sparsity’. Stanno lavorando insieme per rendere la rete neurale più efficiente e più piccola, mantenendo un’elevata precisione. L’operaio che si occupa del ‘pruning’, una donna ispanica, sta tagliando le connessioni non necessarie dal centro della rete. L’addetto alla ‘quantization’, un uomo caucasico, sta regolando o modificando i pesi ovunque. L’operaio che si occupa di ‘sparsity’, una donna africana, sta rimuovendo i nodi non necessari per ridurre il modello. Sullo sfondo, camion e gru edili assistono gli operai nei loro compiti. La rete neurale si sta trasformando visivamente da una struttura complessa e grande a una più snella e piccola.</em></figcaption>
</figure>
</div>
<p>Quando i modelli di apprendimento automatico vengono distribuiti su sistemi, in particolare su sistemi embedded con risorse limitate, l’ottimizzazione dei modelli è una necessità. Mentre il machine learning richiede spesso risorse computazionali sostanziali, i sistemi sono intrinsecamente limitati in termini di memoria, potenza di elaborazione ed energia. Questo capitolo si immergerà nell’arte e nella scienza dell’ottimizzazione dei modelli di machine learning per garantire che siano leggeri, efficienti ed efficaci quando distribuiti in scenari TinyML.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell’Apprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Apprendere tecniche come “pruning”, “knowledge distillation” e architetture di modelli specializzate per rappresentare i modelli in modo più efficiente</p></li>
<li><p>Comprendere i metodi di quantizzazione per ridurre le dimensioni del modello e consentire un’inferenza più rapida tramite numeri con precisione ridotta</p></li>
<li><p>Esplorare approcci di ottimizzazione basati sull’hardware per abbinare i modelli alle capacità del dispositivo target</p></li>
<li><p>Sviluppare un pensiero olistico per bilanciare i compromessi in termini di complessità del modello, accuratezza, latenza, potenza ecc. in base ai requisiti dell’applicazione</p></li>
<li><p>Scoprire strumenti software come framework e piattaforme di conversione del modello che consentono l’implementazione di modelli ottimizzati</p></li>
<li><p>Ottenere informazioni strategiche sulla selezione e l’applicazione di ottimizzazioni del modello in base ai vincoli del caso d’uso e ai target hardware</p></li>
</ul>
</div>
</div>
<section id="introduzione" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="introduzione"><span class="header-section-number">9.1</span> Introduzione</h2>
<p>L’ottimizzazione dei modelli di apprendimento automatico per l’implementazione pratica è un aspetto critico dei sistemi di IA. Questo capitolo si concentra sull’esplorazione delle tecniche di ottimizzazione dei modelli in relazione allo sviluppo di sistemi di apprendimento automatico, che vanno dalle considerazioni di architettura del modello di alto livello agli adattamenti hardware di basso livello. <a href="#fig-3-sections" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-3-sections</span></a> Illustra i tre livelli dello stack di ottimizzazione che trattiamo.</p>
<div id="fig-3-sections" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-sections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_structure.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-sections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.1: Tre livelli da coprire.
</figcaption>
</figure>
</div>
<p>Al livello più alto, esaminiamo le metodologie per ridurre la complessità dei parametri del modello senza compromettere le capacità inferenziali. Tecniche come la potatura e la distillazione della conoscenza offrono approcci potenti per comprimere e perfezionare i modelli mantenendo o addirittura migliorandone le prestazioni, non solo in termini di qualità del modello ma anche nelle prestazioni effettive del runtime del sistema. Questi metodi sono fondamentali per creare modelli efficienti che possono essere implementati in ambienti con risorse limitate.</p>
<p>Inoltre, esploriamo il ruolo della precisione numerica nei calcoli del modello. Comprendere come diversi livelli di precisione numerica influenzino le dimensioni, la velocità e l’accuratezza del modello è essenziale per ottimizzare le prestazioni. Analizziamo vari formati numerici e l’applicazione dell’aritmetica a precisione ridotta, particolarmente rilevante per le distribuzioni di sistemi embedded in cui le risorse computazionali sono spesso limitate.</p>
<p>Al livello più basso, esploriamo l’intricato panorama della progettazione congiunta hardware-software. Questa esplorazione rivela come i modelli possono essere personalizzati per sfruttare le caratteristiche e le capacità specifiche delle piattaforme hardware di destinazione. Allineando la progettazione del modello all’architettura hardware, possiamo migliorare significativamente le prestazioni e l’efficienza.</p>
<p>Questo approccio collettivo si concentra sull’aiutarci a sviluppare e distribuire modelli di apprendimento automatico efficienti, potenti e consapevoli dell’hardware. Dalla semplificazione delle architetture del modello alla messa a punto della precisione numerica e all’adattamento a hardware specifico, questo capitolo copre l’intero spettro di strategie di ottimizzazione. Alla conclusione di questo capitolo, i lettori avranno acquisito una conoscenza approfondita di varie tecniche di ottimizzazione e delle loro applicazioni pratiche in scenari del mondo reale. Questa conoscenza è importante per creare modelli di apprendimento automatico che non solo funzionino bene, ma siano anche ottimizzati per i vincoli e le opportunità offerti dagli ambienti informatici moderni.</p>
</section>
<section id="sec-model_ops_representation" class="level2 page-columns page-full" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="sec-model_ops_representation"><span class="header-section-number">9.2</span> Rappresentazione Efficiente del Modello</h2>
<p>Il primo passo per l’ottimizzazione del modello inizia in un territorio familiare per la maggior parte dei professionisti del ML: la rappresentazione efficiente del modello viene spesso affrontata per la prima volta al livello più alto di astrazione della parametrizzazione, ovvero l’architettura stessa del modello.</p>
<p>La maggior parte dei professionisti del ML tradizionali progetta modelli con un obiettivo generale di alto livello in mente, che si tratti di classificazione delle immagini, rilevamento di persone o individuazione di parole chiave come menzionato in precedenza in questo testo. I loro progetti in genere finiscono per adattarsi naturalmente ad alcuni vincoli soft dovuti a risorse di elaborazione limitate durante lo sviluppo, ma in genere questi progetti non sono a conoscenza di vincoli successivi, come quelli richiesti se il modello deve essere distribuito su un dispositivo più limitato anziché sul cloud.</p>
<p>In questa sezione, discuteremo di come i professionisti possono sfruttare i principi della progettazione congiunta hardware-software anche nell’architettura di alto livello di un modello per rendere i loro modelli compatibili con i dispositivi edge. Da quelli più consapevoli dell’hardware a quelli meno consapevoli a questo livello di modifica, discutiamo alcune delle strategie più comuni per una parametrizzazione efficiente del modello: pruning, compressione e architetture edge-friendly. Abbiamo già parlato di pruning e compressione del modello in <a href="#sec-efficient-model-compression" class="quarto-xref"><span class="quarto-unresolved-ref">sec-efficient-model-compression</span></a>; questa sezione andrà oltre le definizioni per fornire una comprensione tecnica del loro funzionamento.</p>
<section id="sec-pruning" class="level3 page-columns page-full" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="sec-pruning"><span class="header-section-number">9.2.1</span> Il Pruning</h3>
<section id="panoramica" class="level4">
<h4 class="anchored" data-anchor-id="panoramica">Panoramica</h4>
<p>Il model pruning [potatura] è una tecnica di apprendimento automatico che riduce le dimensioni e la complessità di un modello di rete neurale, mantenendone il più possibile le capacità predittive. L’obiettivo della potatura è quello di rimuovere componenti ridondanti o non essenziali del modello, tra cui connessioni tra neuroni, singoli neuroni o persino interi layer della rete.</p>
<p>Questo processo in genere comporta l’analisi del modello di machine learning per identificare e rimuovere pesi, nodi o layer che hanno scarso impatto sugli output del modello. Potando selettivamente un modello in questo modo, il numero totale di parametri può essere ridotto in modo significativo senza cali sostanziali nell’accuratezza del modello. Il modello compresso risultante richiede meno memoria e risorse di calcolo per l’addestramento e l’esecuzione, consentendo tempi di inferenza più rapidi.</p>
<p>Il pruning del modello è particolarmente utile quando si distribuiscono modelli di apprendimento automatico su dispositivi con risorse di calcolo limitate, come telefoni cellulari o sistemi TinyML. La tecnica facilita la distribuzione di modelli più grandi e complessi su questi dispositivi riducendo le loro richieste di risorse. Inoltre, i modelli più piccoli richiedono meno dati per generalizzare bene e sono meno inclini all’overfitting [sovradattamento]. Fornendo un modo efficiente per semplificare i modelli, la potatura dei modelli è diventata una tecnica fondamentale per ottimizzare le reti neurali nell’apprendimento automatico.</p>
<p>Esistono diverse tecniche di potatura comuni utilizzate nell’apprendimento automatico, tra cui la potatura strutturata, la potatura non strutturata, la potatura iterativa, la potatura bayesiana e persino la potatura casuale. Oltre a potare i pesi, si possono anche potare le attivazioni. La potatura di attivazioni prende di mira specificamente neuroni o filtri che si attivano raramente o hanno un’attivazione complessivamente bassa. Esistono numerosi altri metodi, come la potatura di sensibilità e movimento. Per un elenco completo dei metodi, si consiglia al lettore di leggere il seguente articolo: <a href="https://arxiv.org/pdf/2308.06767.pdf">“A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations” (2023)</a>.</p>
<p>Quindi, come si scelgono i metodi di potatura? Esistono molte varianti di tecniche di potatura, ciascuna delle quali varia l’euristica di ciò che dovrebbe essere mantenuto e potato dal modello, nonché il numero di volte in cui cui deve essere eseguita. Tradizionalmente, la potatura avviene dopo che il modello è completamente addestrato, dove il modello potato può subire una lieve perdita di accuratezza. Tuttavia, come discuteremo più avanti, recenti scoperte hanno trovato che la potatura può essere utilizzata durante l’addestramento (ad esempio, in modo iterativo) per identificare rappresentazioni del modello più efficienti e accurate.</p>
</section>
<section id="potatura-strutturata" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="potatura-strutturata">Potatura Strutturata</h4>
<p>Iniziamo con la “potatura strutturata”, una tecnica che riduce le dimensioni di una rete neurale eliminando intere sotto-strutture specifiche del modello mantenendone la struttura generale. Rimuove interi neuroni/canali o layer in base a criteri di importanza. Ad esempio, per una rete neurale convoluzionale (CNN), potrebbero essere determinate istanze di filtro o canali. Per reti completamente connesse, potrebbero essere i neuroni stessi mantenendo la piena connettività o persino l’eliminazione di interi layer del modello che sono considerati insignificanti. Questo tipo di potatura spesso porta a reti sparse regolari e strutturate che sono compatibili con l’hardware.</p>
<p>Sono iniziate a emergere le “best practice” su come pensare alla potatura strutturata. Ci sono tre componenti principali:</p>
<section id="strutture-candidate-per-il-pruning" class="level5">
<h5 class="anchored" data-anchor-id="strutture-candidate-per-il-pruning">1. Strutture Candidate per il Pruning</h5>
<p>Data la varietà di approcci, diverse strutture all’interno di una rete neurale vengono potate in base a criteri specifici. Le strutture primarie per la potatura includono neuroni, canali e talvolta interi layer, ognuno con le sue implicazioni e metodologie uniche. L’obiettivo di ogni approccio è garantire che il modello ridotto mantenga il più possibile la capacità predittiva del modello originale, migliorando al contempo l’efficienza computazionale e riducendo le dimensioni.</p>
<p>Quando i <strong>neuroni</strong> vengono potati, rimuoviamo interi neuroni insieme ai loro pesi e bias associati, riducendo così la larghezza del layer. Questo tipo di potatura viene spesso utilizzato in layer completamente connessi.</p>
<p>La potatura del <strong>canale</strong>, che viene applicata prevalentemente nelle reti neurali convoluzionali (CNN), comporta l’eliminazione di interi canali o filtri, il che a sua volta riduce la profondità delle mappe delle feature e influisce sulla capacità della rete di estrarre determinate feature dai dati di input. Ciò è particolarmente cruciale nelle attività di elaborazione delle immagini in cui l’efficienza computazionale è fondamentale.</p>
<p>Infine, la potatura dei <strong>layer</strong> adotta un approccio più aggressivo rimuovendo interi layer della rete. Ciò riduce significativamente la profondità della rete e quindi la sua capacità di plasmare pattern e gerarchie complesse nei dati. Questo approccio richiede un attento equilibrio per garantire che la capacità predittiva del modello non venga indebitamente compromessa.</p>
<p><a href="#fig-channel-layer-pruning" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-channel-layer-pruning</span></a> mostra la differenza tra la potatura di canale/filtro e quella del layer. Quando potiamo un canale, dobbiamo riconfigurare l’architettura del modello per adattarla ai cambiamenti strutturali. Una modifica consiste nel cambiare il numero di canali di input nel layer successivo (qui, il terzo e il layer più profondo): modificando le profondità dei filtri applicati al layer con il canale potato. D’altra parte, la potatura di un intero layer (rimuovendo tutti i canali nel layer) richiede modifiche più drastiche. Quella principale riguarda la modifica delle connessioni tra i layer rimanenti per sostituire o bypassare il layer potato. Nel nostro caso, riconfiguriamo per connettere il primo e l’ultimo layer. In tutti i casi di potatura, dobbiamo mettere a punto la nuova struttura per regolare i pesi.</p>
<div id="fig-channel-layer-pruning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-channel-layer-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_channel_layer_pruning.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-channel-layer-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.2: Potatura del canale e quella del layer.
</figcaption>
</figure>
</div>
</section>
<section id="stabilire-un-criterio-per-il-pruning" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="stabilire-un-criterio-per-il-pruning">2. Stabilire un criterio per il Pruning</h5>
<p>Stabilire criteri ben definiti per determinare quali strutture specifiche potare da un modello di rete neurale è una componente cruciale del processo di “pruning” del modello. L’obiettivo principale qui è identificare e rimuovere i componenti che contribuiscono meno alle capacità predittive del modello, mantenendo al contempo le strutture integrali per preservare l’accuratezza.</p>
<p>Una strategia ampiamente adottata ed efficace per potare sistematicamente le strutture si basa sul calcolo di punteggi di importanza per singoli componenti come neuroni, filtri, canali o layer. Questi punteggi servono come metriche quantitative per valutare la significatività di ciascuna struttura e il suo effetto sull’output del modello.</p>
<p>Esistono diverse tecniche per assegnare questi punteggi sull’importanza:</p>
<ul>
<li><strong>Pruning Basato sulla Magnitudo del peso</strong>: Questo approccio assegna punteggi di importanza a una struttura valutando la magnitudo aggregata dei pesi associati. Le strutture con magnitudo del peso complessivo inferiore sono considerate meno critiche per le prestazioni della rete.</li>
<li><strong>Pruning Basato sul Bradiente</strong>: Questa tecnica utilizza i gradienti della funzione di los [perdita] rispetto ai pesi associati a una struttura. Le strutture con magnitudo del gradiente cumulativo basso, che indica un impatto minimo sulla perdita quando alterato, sono le candidate principali per la potatura.</li>
<li><strong>Pruning Basato sull’Attivazione</strong>: Questo metodo tiene traccia della frequenza con cui un neurone o un filtro viene attivato memorizzando queste informazioni in un parametro chiamato contatore delle attivazioni. Ogni volta che la struttura viene attivata, il contatore viene incrementato. Un conteggio di attivazione basso suggerisce che la struttura è meno rilevante.</li>
<li><strong>Pruning Basato sull’Espansione di Taylor</strong>: Questo approccio approssima la modifica nella funzione di perdita derivante dalla rimozione di un dato peso. Valutando la perturbazione della perdita cumulativa derivante dalla rimozione di tutti i pesi associati a una struttura, è possibile identificare le strutture con un impatto trascurabile sulla perdita, rendendole candidate idonee per la potatura.</li>
</ul>
<p>L’idea è di misurare, direttamente o indirettamente, il contributo di ogni componente all’output del modello. Le strutture con un’influenza minima in base ai criteri definiti vengono potate per prime. Ciò consente una potatura selettiva e ottimizzata che comprime al massimo i modelli preservando al contempo la capacità predittiva. In generale, è importante valutare l’impatto della rimozione di particolari strutture sull’output del modello, con lavori recenti come <span class="citation" data-cites="rachwan2022winning">(<a href="#ref-rachwan2022winning" role="doc-biblioref">Rachwan et al. 2022</a>)</span> e <span class="citation" data-cites="lubana2020gradient">(<a href="#ref-lubana2020gradient" role="doc-biblioref">Lubana e Dick 2020</a>)</span> che studiano combinazioni di tecniche come la potatura basata sulla magnitudine e la potatura basata sul gradiente.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rachwan2022winning" class="csl-entry" role="listitem">
Rachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, e Stephan Günnemann. 2022. <span>«Winning the lottery ahead of time: <span>Efficient</span> early network pruning»</span>. In <em>International Conference on Machine Learning</em>, 18293–309. PMLR.
</div><div id="ref-lubana2020gradient" class="csl-entry" role="listitem">
Lubana, Ekdeep Singh, e Robert P Dick. 2020. <span>«A gradient flow framework for analyzing network pruning»</span>. <em>arXiv preprint arXiv:2009.11839</em>.
</div></div></section>
<section id="selezione-di-una-strategia-di-potatura" class="level5">
<h5 class="anchored" data-anchor-id="selezione-di-una-strategia-di-potatura">3. Selezione di una Strategia di Potatura</h5>
<p>Ora che abbiamo capito alcune tecniche per determinare l’importanza delle strutture all’interno di una rete neurale, il passo successivo è decidere come applicare queste intuizioni. Ciò comporta la selezione di una strategia di potatura appropriata, che stabilisce come e quando le strutture identificate vengono rimosse e come il modello viene messo a punto per mantenere le sue prestazioni. Esistono due principali strategie di potatura strutturata: quella iterativa e la one-shot.</p>
<p>La <strong>potatura iterativa</strong> rimuove gradualmente le strutture attraverso più cicli di potatura seguiti da messa a punto. In ogni ciclo, un piccolo set di strutture viene potato in base a criteri di importanza. Il modello viene poi messo a punto, consentendogli di adattarsi senza problemi ai cambiamenti strutturali prima della successiva iterazione di potatura. Questo approccio graduale e ciclico impedisce bruschi cali di accuratezza. Consente al modello di adattarsi lentamente man mano che le strutture vengono ridotte attraverso le iterazioni.</p>
<p>Consideriamo una situazione in cui desideriamo potare i 6 canali meno efficaci (in base ad alcuni criteri specifici) da una rete neurale convoluzionale. In <a href="#fig-iterative-pruning" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-iterative-pruning</span></a>, mostriamo un processo di potatura semplificato eseguito su 3 iterazioni. In ogni iterazione, eliminiamo solo 2 canali. La rimozione dei canali comporta un degrado della precisione. Nella prima iterazione, la precisione scende da 0.995 a 0.971. Tuttavia, dopo aver perfezionato il modello sulla nuova struttura, siamo in grado di recuperare dalla perdita di prestazioni, portando la precisione a 0.992. Poiché i cambiamenti strutturali sono minori e graduali, la rete può adattarsi più facilmente a essi. Eseguendo lo stesso processo altre 2 volte, finiamo con una precisione finale di 0.991 (una perdita di solo lo 0.4% rispetto all’originale) e una riduzione del 27% nel numero di canali. Pertanto, la potatura iterativa ci consente di mantenere le prestazioni beneficiando di una maggiore efficienza computazionale dovuta alla riduzione delle dimensioni del modello.</p>
<div id="fig-iterative-pruning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iterative-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_iterative_pruning.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iterative-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.3: Potatura iterativa.
</figcaption>
</figure>
</div>
<p>La <strong>potatura one-shot</strong> adotta un approccio più aggressivo, potando una grande porzione di strutture simultaneamente in un’unica operazione in base a criteri di importanza predefiniti. Segue un’ampia messa a punto per recuperare l’accuratezza del modello. Sebbene più rapida, questa strategia aggressiva può degradare l’accuratezza se il modello non riesce a recuperare durante la messa a punto.</p>
<p>La scelta tra queste strategie comporta la valutazione di fattori quali dimensioni del modello, quanto è sparso il target, calcolo disponibile e perdite di accuratezza accettabili. La potatura one-shot può comprimere rapidamente i modelli, ma quella iterativa può consentire una migliore conservazione dell’accuratezza per un livello target di potatura. In pratica, la strategia è personalizzata in base ai vincoli del caso d’uso. L’obiettivo generale è quello di generare una strategia ottimale che rimuova la ridondanza, ottenga guadagni di efficienza tramite la potatura e metta a punto il modello per stabilizzare l’accuratezza a un livello accettabile per l’implementazione.</p>
<p>Ora si consideri la stessa rete che avevamo nell’esempio di potatura iterativa. Mentre nel processo iterativo abbiamo potato 2 canali alla volta, nel processo one-shot poteremo i 6 canali contemporaneamente, come mostrato in <a href="#fig-oneshot-pruning" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-oneshot-pruning</span></a>. La rimozione simultanea del 27% del canale della rete altera significativamente la struttura, causando un calo della precisione da 0.995 a 0.914. Date le modifiche principali, la rete non è in grado di adattarsi correttamente durante la messa a punto e la precisione è salita a 0.943, un degrado del 5% rispetto alla precisione della rete non potata. Mentre le strutture finali nei processi di potatura iterativa e di potatura one-shot sono identiche, la prima è in grado di mantenere prestazioni elevate mentre la seconda subisce degradi significativi.</p>
<div id="fig-oneshot-pruning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-oneshot-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_oneshot_pruning.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-oneshot-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.4: Potatura one-shot.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="vantaggi-della-potatura-strutturata" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi-della-potatura-strutturata">Vantaggi della Potatura Strutturata</h4>
<p>La potatura strutturata offre una miriade di vantaggi che soddisfano vari aspetti dell’implementazione e dell’utilizzo del modello, specialmente in ambienti in cui le risorse computazionali sono limitate.</p>
<ul>
<li><p><strong>Efficienza Computazionale:</strong> Eliminando intere strutture, come neuroni o canali, si riduce significativamente il carico computazionale durante le fasi di training e inferenza, consentendo così previsioni più rapide del modello e convergenza del training. Inoltre, la rimozione delle strutture riduce intrinsecamente il “footprint” [impronta] di memoria del modello, assicurando che richieda meno spazio di archiviazione e memoria durante il funzionamento, il che è particolarmente vantaggioso in ambienti con limiti di memoria come i sistemi TinyML.</p></li>
<li><p><strong>Efficienza Hardware:</strong> La potatura strutturata spesso si traduce in modelli più adatti all’implementazione su hardware specializzato, come i Field-Programmable Gate Arrays (FPGA) o Application-Specific Integrated Circuits (ASIC), a causa della regolarità e la semplicità dell’architettura potata. Con requisiti di elaborazione ridotti, si traduce in un consumo energetico inferiore, fondamentale per i dispositivi alimentati a batteria e i metodi di elaborazione sostenibili.</p></li>
<li><p><strong>Manutenzione e Distribuzione:</strong> Il modello ridotto, sebbene più piccolo, mantiene la sua forma architettonica originale, che può semplificare la pipeline di distribuzione e garantire la compatibilità con i sistemi e i framework esistenti. Inoltre, con meno parametri e strutture più semplici, il modello potato diventa più facile da gestire e monitorare negli ambienti di produzione, riducendo potenzialmente le spese generali associate alla manutenzione e agli aggiornamenti del modello. Più avanti, quando approfondiremo <a href="../../../contents/core/ops/ops.it.html">MLOps</a>, questa necessità diventerà evidente.</p></li>
</ul>
</section>
<section id="potatura-non-strutturata" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="potatura-non-strutturata">Potatura non Strutturata</h4>
<p>Il “pruning” non-strutturato è, come suggerisce il nome, la potatura del modello senza riguardo alla sotto-struttura specifica del modello. Come accennato in precedenza, offre una maggiore aggressività nella potatura e può raggiungere maggiori diradazione del modello mantenendo la precisione, dati meno vincoli su ciò che può e non può essere potato. In genere, la potatura non-strutturata post-training consiste in un criterio di importanza per i singoli parametri/pesi del modello, potatura/rimozione dei pesi che scendono al di sotto dei criteri e una successiva messa a punto facoltativa per provare a recuperare la precisione persa durante la rimozione dei pesi.</p>
<p>La potatura non-strutturata presenta alcuni vantaggi rispetto a quella strutturata: la rimozione di singoli pesi anziché di intere sotto-strutture del modello spesso porta in pratica a minori diminuzioni della precisione del modello. Inoltre, in genere determinare il criterio di importanza per un singolo peso è molto più semplice che per un’intera sotto-struttura di parametri nella potatura strutturata, rendendo la prima preferibile nei casi in cui tale overhead è difficile o poco chiaro da calcolare. Analogamente, il processo effettivo di potatura strutturata è generalmente meno flessibile, poiché la rimozione di singoli pesi è generalmente più semplice della rimozione di intere sotto-strutture e della garanzia che il modello funzioni ancora.</p>
<p>La potatura non strutturata, pur offrendo il potenziale per una significativa riduzione delle dimensioni del modello e una migliore implementabilità, porta con sé problemi legati alla gestione di rappresentazioni sparse e alla garanzia dell’efficienza computazionale. È particolarmente utile in scenari in cui è fondamentale ottenere la massima compressione possibile del modello e in cui l’ambiente di distribuzione può gestire in modo efficiente i calcoli sparsi.</p>
<p><a href="#tbl-pruning_methods" class="quarto-xref">Tabella&nbsp;<span class="quarto-unresolved-ref">tbl-pruning_methods</span></a> fornisce un confronto conciso tra potatura strutturata e la non-strutturata. In questa tabella, gli aspetti relativi alla natura e all’architettura del modello potato (Definizione, Regolarità del modello e Livello di compressione) sono raggruppati insieme, seguiti dagli aspetti relativi alle considerazioni computazionali (Efficienza computazionale e Compatibilità hardware) e terminando con gli aspetti relativi all’implementazione e all’adattamento del modello potato (Complessità di implementazione e Complessità di messa a punto). Entrambe le strategie di potatura offrono vantaggi e problemi unici, come mostrato in <a href="#tbl-pruning_methods" class="quarto-xref">Tabella&nbsp;<span class="quarto-unresolved-ref">tbl-pruning_methods</span></a>, e la selezione tra di esse dovrebbe essere influenzata da requisiti specifici del progetto e della distribuzione.</p>
<div id="tbl-pruning_methods" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pruning_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;9.1: Confronto tra potatura strutturata e non-strutturata.
</figcaption>
<div aria-describedby="tbl-pruning_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 32%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspetto</th>
<th style="text-align: left;">Potatura strutturata</th>
<th style="text-align: left;">Potatura non strutturata</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Definizione</td>
<td style="text-align: left;">Potatura di intere strutture (ad esempio, neuroni, canali, layer) all’interno della rete</td>
<td style="text-align: left;">Potatura di singoli pesi o neuroni, con conseguenti matrici sparse o strutture di rete non regolari</td>
</tr>
<tr class="even">
<td style="text-align: left;">Regolarità del Modello</td>
<td style="text-align: left;">Mantiene un’architettura di rete regolare e strutturata</td>
<td style="text-align: left;">Si traduce in architetture di rete irregolari e sparse</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Livello di Compressione</td>
<td style="text-align: left;">Può offrire una compressione del modello limitata rispetto alla potatura non-strutturata</td>
<td style="text-align: left;">Può ottenere una compressione del modello più elevata grazie alla potatura a grana fine</td>
</tr>
<tr class="even">
<td style="text-align: left;">Efficienza Computazionale</td>
<td style="text-align: left;">In genere più efficiente computazionalmente grazie al mantenimento di strutture regolari</td>
<td style="text-align: left;">Può essere inefficiente dal punto di vista computazionale a causa di matrici di peso sparse, a meno che non venga utilizzato hardware/software specializzato</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Compatibilità Hardware</td>
<td style="text-align: left;">In genere più compatibile con vari hardware grazie alle strutture regolari</td>
<td style="text-align: left;">Potrebbe richiedere hardware che gestisca in modo efficiente i calcoli sparsi per ottenere vantaggi</td>
</tr>
<tr class="even">
<td style="text-align: left;">Complessità di Implementazione</td>
<td style="text-align: left;">Spesso più semplice da implementare e gestire grazie al mantenimento della struttura della rete</td>
<td style="text-align: left;">Può essere complesso da gestire e calcolare a causa delle rappresentazioni sparse</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Complessità di Messa a Punto Fine</td>
<td style="text-align: left;">Potrebbe richiedere strategie di messa a punto fine meno complesse dopo la potatura</td>
<td style="text-align: left;">Potrebbe richiedere strategie di riaddestramento o messa a punto fine più complesse dopo la potatura</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In <a href="#fig-structured-unstructured" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-structured-unstructured</span></a> abbiamo esempi che illustrano le differenze tra potatura non-strutturata e strutturata. Osservare che la potatura non-strutturata può portare a modelli che non rispettano più le garanzie strutturali di alto livello delle loro controparti originali non potate: la rete di sinistra non è più una rete completamente connessa dopo la potatura. La potatura strutturata, d’altro canto, mantiene quelle invarianti: al centro, la rete completamente connessa viene potata in modo che resti ancora completamente connessa; allo stesso modo, la CNN mantiene la sua struttura convoluzionale, sebbene con meno filtri.</p>
<div id="fig-structured-unstructured" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-structured-unstructured-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_pruning_comparison.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-structured-unstructured-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.5: Potatura non-strutturata e strutturata. Fonte: <span class="citation" data-cites="qi2021efficient">Qi et al. (<a href="#ref-qi2021efficient" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-qi2021efficient" class="csl-entry" role="listitem">
Qi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang, e Honggang Zhang. 2021. <span>«An efficient pruning scheme of deep neural networks for Internet of Things applications»</span>. <em>EURASIP Journal on Advances in Signal Processing</em> 2021 (1): 31. <a href="https://doi.org/10.1186/s13634-021-00744-4">https://doi.org/10.1186/s13634-021-00744-4</a>.
</div></div></figure>
</div>
</section>
<section id="ipotesi-del-biglietto-della-lotteria" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="ipotesi-del-biglietto-della-lotteria">Ipotesi del Biglietto della Lotteria</h4>
<p>La potatura si è evoluta da una tecnica puramente post-addestramento che comportava un costo per una certa accuratezza, a un potente approccio di meta-apprendimento applicato durante l’addestramento per ridurre la complessità del modello. Questo progresso a sua volta migliora l’efficienza di calcolo, memoria e latenza sia nell’addestramento che nell’inferenza.</p>
<p>Una scoperta rivoluzionaria che ha catalizzato questa evoluzione è stata l’<a href="https://arxiv.org/abs/1803.03635">ipotesi del biglietto della lotteria</a> di <span class="citation" data-cites="jonathan2019lottery">Frankle e Carbin (<a href="#ref-jonathan2019lottery" role="doc-biblioref">2019</a>)</span>. Il loro lavoro afferma che all’interno di reti neurali dense esistono sotto-reti sparse, denominate “biglietti vincenti”, che possono eguagliare o addirittura superare le prestazioni del modello originale quando addestrate in isolamento. In particolare, questi biglietti vincenti, quando inizializzati utilizzando gli stessi pesi della rete originale, possono raggiungere una convergenza e un’accuratezza di addestramento altrettanto elevate su un dato compito. Vale la pena sottolineare che hanno scoperto empiricamente l’ipotesi del biglietto della lotteria, che è stata successivamente formalizzata.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jonathan2019lottery" class="csl-entry" role="listitem">
Frankle, Jonathan, e Michael Carbin. 2019. <span>«The Lottery Ticket Hypothesis: <span>Finding</span> Sparse, Trainable Neural Networks»</span>. In <em>7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net. <a href="https://openreview.net/forum?id=rJl-b3RcF7">https://openreview.net/forum?id=rJl-b3RcF7</a>.
</div></div><p>L’intuizione alla base di questa ipotesi è che, durante il processo di addestramento di una rete neurale, molti neuroni e connessioni diventano ridondanti o non importanti, in particolare con l’inclusione di tecniche di addestramento che incoraggiano la ridondanza come il “dropout” [abbandono]. L’identificazione, la potatura e l’inizializzazione di questi “biglietti vincenti” consentono un addestramento più rapido e modelli più efficienti, poiché contengono le informazioni essenziali per la decisione del modello per l’attività. Inoltre, come generalmente noto con la teoria del “bias-variance tradeoff” [compromesso tra bias e varianza], questi biglietti soffrono meno di sovra-parametrizzazione e quindi si generalizzano meglio piuttosto che sovra-adattarsi all’attività.</p>
<p>In <a href="#fig-lottery-ticket-hypothesis" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-lottery-ticket-hypothesis</span></a> abbiamo un esempio che mostra esperimenti di potatura e addestramento su una LeNet completamente connessa su una varietà di rapporti di potatura. Nel grafico a sinistra, si nota come una potatura pesante riveli una sotto-rete più efficiente (in verde) che è il 21,1% delle dimensioni della rete originale (in blu). La sotto-rete raggiunge una maggiore accuratezza e in modo più rapido rispetto alla versione non potata (la linea verde è sopra la linea blu). Tuttavia, la potatura ha un limite (punto ottimale) e un’ulteriore potatura produrrà degradi delle prestazioni e alla fine scenderà al di sotto delle prestazioni della versione non potata (nota come le sotto-reti rossa, viola e marrone diminuiscono gradualmente nelle prestazioni di accuratezza) a causa della significativa perdita nel numero di parametri.</p>
<div id="fig-lottery-ticket-hypothesis" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lottery-ticket-hypothesis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_lottery_ticket_hypothesis.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lottery-ticket-hypothesis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.6: Esperimenti sull’ipotesi del biglietto della lotteria.
</figcaption>
</figure>
</div>
<p>Per scoprire questi biglietti vincenti della lotteria all’interno di una rete neurale, viene seguito un processo sistematico. Questo processo, illustrato in <a href="#fig-winning-ticket" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-winning-ticket</span></a> (a sinistra), prevede l’addestramento iterativo, la potatura e la reinizializzazione della rete. I passaggi seguenti delineano questo approccio:</p>
<ol type="1">
<li><p>Inizializzare i pesi della rete a valori casuali.</p></li>
<li><p>Addestrare la rete finché non converge alle prestazioni desiderate.</p></li>
<li><p>Eliminare una percentuale di rami con i valori di peso più bassi.</p></li>
<li><p>Reinizializzare la rete con gli stessi valori casuali del passaggio 1.</p></li>
<li><p>Ripetere i passaggi 2-4 più volte o finché la precisione non peggiora in modo significativo.</p></li>
</ol>
<p>Alla fine, ci si ritrova con una rete potata (<a href="#fig-winning-ticket" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-winning-ticket</span></a> lato destro), che è una sotto-rete di quella di partenza. La sotto-rete dovrebbe avere una struttura significativamente più piccola, pur mantenendo un livello di precisione comparabile.</p>
<div id="fig-winning-ticket" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-winning-ticket-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_winning_ticket.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-winning-ticket-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.7: Trovare la sottorete del biglietto vincente.
</figcaption>
</figure>
</div>
</section>
<section id="problemi-e-limitazioni" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="problemi-e-limitazioni">Problemi e Limitazioni</h4>
<p>Non c’è niente di gratuito con le ottimizzazioni di potatura, con alcune scelte che comportano sia miglioramenti che costi da considerare. Di seguito, discutiamo alcuni compromessi che gli esperti devono considerare.</p>
<ul>
<li><p><strong>Gestione di Matrici di Peso Sparse:</strong> Una matrice di peso sparsa è una matrice in cui molti degli elementi sono pari a zero. La potatura non strutturata spesso produce matrici di peso sparse, in cui molti pesi vengono potati a zero. Sebbene ciò riduca le dimensioni del modello, introduce anche diversi problemi. L’inefficienza computazionale può sorgere perché l’hardware standard è ottimizzato per operazioni di matrice densa. Senza ottimizzazioni che sfruttano la sparsità, i risparmi computazionali derivanti dalla potatura possono essere persi. Sebbene le matrici sparse possano essere archiviate senza formati specializzati, sfruttare efficacemente la loro sparsità richiede una gestione attenta per evitare di sprecare risorse. Algoritmicamente, la navigazione in strutture sparse richiede di saltare in modo efficiente le voci zero, il che aggiunge complessità al calcolo e agli aggiornamenti del modello.</p></li>
<li><p><strong>Qualità vs.&nbsp;Riduzione delle Dimensioni:</strong> Una sfida fondamentale sia nella potatura strutturata che in quella non-strutturata è bilanciare la riduzione delle dimensioni con il mantenimento o il miglioramento delle prestazioni predittive. È essenziale stabilire criteri di potatura robusti, sia per rimuovere intere strutture (potatura strutturata) sia singoli pesi (potatura non strutturata). Questi criteri di potatura scelti devono identificare accuratamente gli elementi la cui rimozione ha un impatto minimo sulle prestazioni. Spesso è necessaria un’attenta sperimentazione per garantire che il modello potato rimanga efficiente mantenendo al contempo le sue prestazioni predittive.</p></li>
<li><p><strong>Fine-Tuning e Riaddestramento:</strong> La messa a punto post-potatura è fondamentale sia nella potatura strutturata che in quella non-strutturata per recuperare le prestazioni perse e stabilizzare il modello. La sfida comprende la determinazione dell’estensione, della durata e della natura del processo di messa a punto, che può essere influenzato dal metodo di potatura e dal grado di potatura applicato.</p></li>
<li><p><strong>Compatibilità ed Efficienza Hardware:</strong> Particolarmente pertinenti alla potatura non-strutturata, la compatibilità e l’efficienza hardware diventano critiche. La potatura non strutturata spesso si traduce in matrici di peso sparse, che potrebbero non essere gestite in modo efficiente da un certo hardware, annullando potenzialmente i vantaggi computazionali della potatura (vedere <a href="#fig-sparse-matrix" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-sparse-matrix</span></a>). Garantire che i modelli potati, in particolare quelli risultanti dall’eliminazione non-strutturata, siano scalabili, compatibili ed efficienti sull’hardware target è una considerazione importante.</p></li>
<li><p><strong>Considerazioni Legali ed Etiche:</strong> Ultimo ma non meno importante, il rispetto delle linee guida legali ed etiche è importante, soprattutto in ambiti con conseguenze significative. I metodi di potatura devono essere sottoposti a rigorosi processi di validazione, test e potenzialmente certificazione per garantire la conformità alle normative e agli standard pertinenti, sebbene al momento non esistano standard formali e “best practice” che siano esaminati e convalidati da entità terze. Ciò è particolarmente cruciale in applicazioni ad alto rischio come l’intelligenza artificiale medica e la guida autonoma, dove i cali di qualità dovuti a ottimizzazioni simili alla potatura possono essere pericolosi per la vita. Inoltre, le considerazioni etiche si estendono oltre la sicurezza fino all’equità e all’uguaglianza; un recente lavoro di <span class="citation" data-cites="tran2022pruning">(<a href="#ref-tran2022pruning" role="doc-biblioref">Tran et al. 2022</a>)</span> ha rivelato che la potatura può avere un impatto sproporzionato sulle persone di colore, sottolineando la necessità di una valutazione etica completa nel processo di potatura.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-tran2022pruning" class="csl-entry" role="listitem">
Tran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, e Rakshit Naidu. 2022. <span>«Pruning has a disparate impact on model accuracy»</span>. <em>Adv Neural Inf Process Syst</em> 35: 17652–64.
</div></div><div id="fig-sparse-matrix" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sparse-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_sprase_matrix.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sparse-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.8: Matrice dei pesi sparsi.
</figcaption>
</figure>
</div>
<div id="exr-p" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;9.1: Pruning
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Si immagini che la rete neurale sia un cespuglio gigante e troppo cresciuto. La potatura è come tagliare strategicamente i rami per renderla più forte ed efficiente! Nel Colab, si imparerà come fare questa potatura in TensorFlow. La comprensione di questi concetti fornirà le basi per vedere come la potatura rende i modelli abbastanza piccoli da poter essere eseguiti sul telefono!</p>
<p><a href="https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="compressione-del-modello" class="level3 page-columns page-full" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="compressione-del-modello"><span class="header-section-number">9.2.2</span> Compressione del Modello</h3>
<p>Le tecniche di compressione del modello sono fondamentali per distribuire modelli di deep learning su dispositivi con risorse limitate. Queste tecniche mirano a creare modelli più piccoli ed efficienti che preservino le prestazioni predittive dei modelli originali.</p>
<section id="sec-kd" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-kd">Distillazione della Conoscenza</h4>
<p>Una tecnica popolare è la <strong>knowledge distillation (KD)</strong> <a href="#sec-kd">distillazione della conoscenza</a>, che trasferisce la conoscenza da un modello “insegnante” ampio e complesso a un modello “studente” più piccolo. L’idea chiave è addestrare il modello studente a imitare gli output dell’insegnante. Il concetto di KD è stato reso popolare per la prima volta da <span class="citation" data-cites="hinton2015distilling">Hinton (<a href="#ref-hinton2015distilling" role="doc-biblioref">2005</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hinton2015distilling" class="csl-entry" role="listitem">
Hinton, Geoffrey. 2005. <span>«Van <span>Nostrand’s</span> Scientific Encyclopedia»</span>. Wiley. <a href="https://doi.org/10.1002/0471743984.vse0673">https://doi.org/10.1002/0471743984.vse0673</a>.
</div></div><section id="panoramica-e-vantaggi" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="panoramica-e-vantaggi">Panoramica e Vantaggi</h5>
<p>La distillazione della conoscenza implica il trasferimento della conoscenza da un modello insegnante ampio e complesso a un modello studente più piccolo. L’idea di base è quella di utilizzare gli output dell’insegnante, noti come <strong>soft targets</strong>, per guidare il training del modello studente. A differenza dei tradizionali “hard targets” (le vere etichette), quelli soft sono le distribuzioni di probabilità sulle classi che il modello insegnante prevede. Queste distribuzioni forniscono informazioni più complete sulle relazioni tra le classi, il che può aiutare il modello studente ad apprendere in modo più efficace.</p>
<p>Abbiamo imparato che la funzione softmax converte gli output grezzi di un modello in una distribuzione di probabilità sulle classi. Una tecnica chiave in KD è la <strong>scalatura della temperatura</strong>, che viene applicata alla funzione softmax degli output del modello insegnante. Introducendo un parametro di temperatura, la distribuzione può essere regolata: una temperatura più alta produce probabilità più soft, il che significa che le differenze tra le probabilità di classe diventano meno estreme. Questo effetto di ammorbidimento determina una distribuzione più uniforme, in cui la fiducia del modello nella classe più probabile è ridotta e altre classi hanno probabilità più elevate, diverse da zero. Ciò è prezioso per il modello studente perché gli consente di apprendere non solo dalla classe più probabile, ma anche dalle probabilità relative di tutte le classi, catturando pattern sottili che potrebbero essere persi se addestrati solo su obiettivi difficili. Pertanto, la scalabilità della temperatura facilita il trasferimento di conoscenze più sfumate dal modello insegnante a quello studente.</p>
<p>La funzione di perdita nella distillazione della conoscenza in genere combina due componenti: una perdita di distillazione e una perdita di classificazione. La perdita di distillazione, spesso calcolata utilizzando la divergenza di Kullback-Leibler (KL), misura la differenza tra gli soft target prodotti dal modello insegnante e gli output del modello studente, incoraggiando lo studente a imitare le previsioni dell’insegnante. Nel frattempo, la perdita di classificazione assicura che il modello studente preveda correttamente le etichette vere in base ai dati originali. Insieme, queste due componenti aiutano lo studente modello a conservare le conoscenze dell’insegnante, rispettando al contempo le etichette di verità di base.</p>
<p>Questi componenti, quando configurati e armonizzati abilmente, consentono al modello studente di assimilare la conoscenza del modello insegnante, creando un percorso verso modelli più piccoli, efficienti e robusti, che mantengono la capacità predittiva delle loro controparti più grandi. <a href="#fig-knowledge-distillation" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-knowledge-distillation</span></a> visualizza la procedura di training della “knowledge distillation”. Notare come i logit o le soft label del modello insegnante vengono utilizzati per fornire una perdita di distillazione da cui il modello studente può imparare.</p>
<div id="fig-knowledge-distillation" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-knowledge-distillation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_knowledge_distillation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knowledge-distillation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.9: Processo di training della distillazione della conoscenza. Fonte: <span class="citation" data-cites="intellabs2023knowledge">IntelLabs (<a href="#ref-intellabs2023knowledge" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-intellabs2023knowledge" class="csl-entry" role="listitem">
IntelLabs. 2023. <span>«Knowledge Distillation - Neural Network Distiller»</span>. <a href="https://intellabs.github.io/distiller/knowledge_distillation.html">https://intellabs.github.io/distiller/knowledge_distillation.html</a>.
</div></div></figure>
</div>
</section>
<section id="sfide" class="level5">
<h5 class="anchored" data-anchor-id="sfide">Sfide</h5>
<p>Tuttavia, KD presenta una serie unica di sfide e considerazioni che ricercatori e professionisti devono affrontare attentamente. Una delle sfide è nella messa a punto meticolosa degli iperparametri, come il parametro “temperatura” nella funzione softmax e la ponderazione tra la distillazione e la perdita di classificazione nella funzione obiettivo. Raggiungere un equilibrio che sfrutti efficacemente gli output ammorbiditi del modello insegnante mantenendo al contempo la fedeltà alle etichette dei dati reali non è banale e può avere un impatto significativo sulle prestazioni e sulle capacità di generalizzazione del modello studente.</p>
<p>Inoltre, l’architettura del modello studente stesso pone un problema considerevole. Progettare un modello compatto per soddisfare i vincoli di calcolo e memoria, pur essendo in grado di assimilare le conoscenze essenziali dal modello insegnante, richiede una comprensione sfumata della capacità del modello e dei compromessi intrinseci coinvolti nella compressione. Il modello studente deve essere attentamente progettato per navigare nella dicotomia di dimensioni e prestazioni, assicurando che la conoscenza distillata venga catturata e utilizzata in modo significativo. Inoltre, la scelta del modello dell’insegnante, che influenza intrinsecamente la qualità e la natura della conoscenza da trasferire, è importante e introduce un ulteriore livello di complessità nel processo KD.</p>
<p>Queste sfide sottolineano la necessità di un approccio completo e sfumato all’implementazione di KD, assicurando che i modelli degli studenti risultanti siano sia efficienti che efficaci nei loro contesti operativi.</p>
</section>
</section>
<section id="fattorizzazione-di-matrici-di-basso-rango" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="fattorizzazione-di-matrici-di-basso-rango">Fattorizzazione di Matrici di Basso Rango</h4>
<p>Simile nel tema dell’approssimazione, la Low-Rank Matrix Factorization (LRMF) <a href="#fattorizzazione-di-matrici-di-basso-rango">fattorizzazione di matrici di basso rango</a> è una tecnica matematica utilizzata in algebra lineare e analisi dei dati per approssimare una matrice data scomponendola in due o più matrici di dimensione inferiore. L’idea fondamentale è di esprimere una matrice di grandi dimensioni come prodotto di matrici di rango inferiore, il che può aiutare a ridurre la complessità dei dati preservandone la struttura essenziale. Matematicamente, data una matrice <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, LRMF cercare le matrici <span class="math inline">\(U \in \mathbb{R}^{m \times k}\)</span> e <span class="math inline">\(V \in \mathbb{R}^{k \times n}\)</span> tali che <span class="math inline">\(A \approx UV\)</span>, dove <span class="math inline">\(k\)</span> è il rango ed è in genere molto più piccolo di <span class="math inline">\(m\)</span> e <span class="math inline">\(n\)</span>.</p>
<section id="background-e-benefici" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="background-e-benefici">Background e Benefici</h5>
<p>Uno dei primo lavori nel campo della fattorizzazione di matrici, in particolare nel contesto dei sistemi di raccomandazione, è il documento di <span class="citation" data-cites="koren2009matrix">Koren, Bell, e Volinsky (<a href="#ref-koren2009matrix" role="doc-biblioref">2009</a>)</span>. Gli autori esaminano vari modelli di fattorizzazione, fornendo approfondimenti sulla loro efficacia nel catturare i pattern sottostanti nei dati e nel migliorare l’accuratezza predittiva nel filtraggio collaborativo. LRMF è stato ampiamente applicato nei sistemi di raccomandazione (come Netflix, Facebook, ecc.), dove la matrice di interazione utente-elemento è fattorizzata per catturare fattori latenti corrispondenti alle preferenze dell’utente e agli attributi dell’elemento.</p>
<div class="no-row-height column-margin column-container"><div id="ref-koren2009matrix" class="csl-entry" role="listitem">
Koren, Yehuda, Robert Bell, e Chris Volinsky. 2009. <span>«Matrix Factorization Techniques for Recommender Systems»</span>. <em>Computer</em> 42 (8): 30–37. <a href="https://doi.org/10.1109/mc.2009.263">https://doi.org/10.1109/mc.2009.263</a>.
</div></div><p>Il vantaggio principale della “fattorizzazione di matrici di basso rango” risiede nella sua capacità di ridurre la dimensionalità dei dati come mostrato in <a href="#fig-matrix-factorization" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-matrix-factorization</span></a>, dove ci sono meno parametri da memorizzare, rendendola più efficiente dal punto di vista computazionale e riducendo i requisiti di archiviazione a costo di un po’ di elaborazione aggiuntiva. Ciò può portare a calcoli più rapidi e rappresentazioni di dati più compatte, il che è particolarmente prezioso quando si ha a che fare con grandi set di dati. Inoltre, può aiutare nella riduzione del rumore e può rivelare pattern e relazioni sottostanti nei dati.</p>
<p><a href="#fig-matrix-factorization" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-matrix-factorization</span></a> illustra la diminuzione della parametrizzazione abilitata dalla fattorizzazione di matrici di basso rango. Osservare come la matrice <span class="math inline">\(M\)</span> può essere approssimata dal prodotto delle matrici <span class="math inline">\(L_k\)</span> e <span class="math inline">\(R_k^T\)</span>. Per intuizione, la maggior parte dei layer completamente connessi nelle reti sono archiviati come matrice di proiezione <span class="math inline">\(M\)</span>, che richiede il caricamento di <span class="math inline">\(m \times n\)</span> parametri durante il calcolo. Tuttavia, scomponendola e approssimandola come prodotto di due matrici di rango inferiore, abbiamo bisogno di archiviare solo <span class="math inline">\(m \times k + k\times n\)</span> parametri in termini di archiviazione, sostenendo al contempo un costo di calcolo aggiuntivo per la moltiplicazione delle matrici. Finché <span class="math inline">\(k &lt; n/2\)</span>, questa fattorizzazione ha meno parametri totali da archiviare, aggiungendo un calcolo di runtime <span class="math inline">\(O(mkn)\)</span> <span class="citation" data-cites="gu2023deep">(<a href="#ref-gu2023deep" role="doc-biblioref">Gu 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gu2023deep" class="csl-entry" role="listitem">
Gu, Ivy. 2023. <span>«Deep Learning Model Compression (ii) by Ivy Gu Medium»</span>. <a href="https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453">https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453</a>.
</div></div><div id="fig-matrix-factorization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-matrix-factorization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_low_rank_matrix_factorization.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-matrix-factorization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.10: Fattorizzazione di matrici di basso rango. Fonte: <a href="https://dustinstansbury.github.io/theclevermachine/svd-data-compression">The Clever Machine.</a>
</figcaption>
</figure>
</div>
</section>
<section id="sfide-1" class="level5">
<h5 class="anchored" data-anchor-id="sfide-1">Sfide</h5>
<p>Ma professionisti e ricercatori incontrano una serie di problemi e considerazioni che richiedono una particolare attenzione e approcci strategici. Come con qualsiasi tecnica di compressione lossy [con perdita], potremmo perdere informazioni durante questo processo di approssimazione: scegliere il rango corretto che bilanci le informazioni perse e i costi computazionali è altrettanto complicato e aggiunge un ulteriore iperparametro da regolare.</p>
<p>La fattorizzazione di matrici di basso rango è uno strumento prezioso per la riduzione della dimensionalità e per adattare il calcolo ai dispositivi edge ma, come altre tecniche, deve essere attentamente regolata in base al modello e all’attività da svolgere. Una sfida fondamentale risiede nella gestione della complessità computazionale inerente a LRMF, soprattutto quando si hanno a che fare con dati ad alta dimensionalità e su larga scala. L’onere computazionale, in particolare nel contesto di applicazioni in tempo reale e set di dati massicci, rimane un ostacolo significativo per un utilizzo efficace di LRMF.</p>
<p>Inoltre, l’enigma della scelta del rango ottimale <span class="math inline">\(k\)</span> per la fattorizzazione introduce un ulteriore livello di complessità. La selezione di <span class="math inline">\(k\)</span> implica intrinsecamente un compromesso tra accuratezza dell’approssimazione e semplicità del modello, e l’identificazione di un rango che bilanci abilmente questi obiettivi contrastanti spesso richiede una combinazione di competenza di dominio, convalida empirica e, a volte, approcci euristici. La sfida è ulteriormente amplificata quando i dati comprendono rumore o quando la struttura intrinseca di basso rango non è pronunciata, rendendo la determinazione di un <span class="math inline">\(k\)</span> adatto ancora più sfuggente.</p>
<p>La gestione di dati mancanti o sparsi, un evento comune in applicazioni come i sistemi di raccomandazione, pone un’altra sfida sostanziale. Le tecniche tradizionali di fattorizzazione delle matrici, come la Singular Value Decomposition (SVD), non sono direttamente applicabili alle matrici con voci mancanti, rendendo necessario lo sviluppo e l’applicazione di algoritmi specializzati in grado di fattorizzare matrici incomplete mitigando al contempo i rischi di overfitting alle voci osservate. Ciò spesso comporta l’incorporazione di termini di regolarizzazione o la limitazione della fattorizzazione in modi specifici, il che a sua volta introduce ulteriori iperparametri che devono essere selezionati giudiziosamente.</p>
<p>Inoltre, in scenari in cui i dati evolvono o crescono nel tempo, sviluppare modelli LRMF in grado di adattarsi a nuovi dati senza richiedere una completa rifattorizzazione è un’impresa critica ma impegnativa. Gli algoritmi di fattorizzazione di matrici incrementali e online cercano di risolvere questo problema consentendo l’aggiornamento delle matrici fattorizzate all’arrivo di nuovi dati, ma garantire stabilità, accuratezza ed efficienza computazionale in queste impostazioni dinamiche rimane un compito intricato. Ciò è particolarmente impegnativo nello spazio di TinyML, in cui la ridistribuzione dei rami per i modelli aggiornati può essere piuttosto impegnativa.</p>
</section>
</section>
<section id="decomposizione-dei-tensori" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="decomposizione-dei-tensori">Decomposizione dei Tensori</h4>
<p>Abbiamo visto in <a href="#sec-tensor-data-structures" class="quarto-xref"><span class="quarto-unresolved-ref">sec-tensor-data-structures</span></a> che i tensori sono strutture flessibili, comunemente utilizzate dai framework ML, che possono rappresentare dati in dimensioni superiori. Similmente alla fattorizzazione di matrici di basso rango, i modelli più complessi possono memorizzare pesi in dimensioni superiori, come i tensori. La decomposizione tensoriale è l’analogo a più dimensioni della fattorizzazione della matrice, in cui un tensore modello viene scomposto in componenti di rango inferiore (<a href="#fig-tensor-decomposition" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-tensor-decomposition</span></a>). Questi componenti di rango inferiore sono più facili da calcolare e memorizzare, ma possono soffrire degli stessi problemi menzionati sopra, come la perdita di informazioni e la necessità di una messa a punto sfumata degli iperparametri. Matematicamente, dato un tensore <span class="math inline">\(\mathcal{A}\)</span>, la decomposizione tensoriale cerca di rappresentare <span class="math inline">\(\mathcal{A}\)</span> come una combinazione di tensori più semplici, facilitando una rappresentazione compressa che approssima i dati originali riducendo al minimo la perdita di informazioni.</p>
<div id="fig-tensor-decomposition" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-tensor-decomposition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_tensor_decomposition.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-decomposition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.11: Decomposizione dei Tensori. Fonte: <span class="citation" data-cites="xinyu">Xinyu (<a href="#ref-xinyu" role="doc-biblioref">s.d.</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-xinyu" class="csl-entry" role="listitem">
Xinyu, Chen. s.d.
</div></div></figure>
</div>
<p>Il lavoro di Tamara G. Kolda e Brett W. Bader, <a href="https://epubs.siam.org/doi/abs/10.1137/07070111X">“Tensor Decompositions and Applications”</a> (2009), si distingue come un articolo fondamentale nel campo delle decomposizioni tensoriali. Gli autori forniscono una panoramica completa di vari metodi di decomposizione tensoriale, esplorandone i fondamenti matematici, gli algoritmi e un’ampia gamma di applicazioni, che vanno dall’elaborazione del segnale al data mining. Naturalmente, il motivo per cui ne stiamo discutendo è perché ha un enorme potenziale per i miglioramenti delle prestazioni del sistema, in particolare nello spazio di TinyML, dove la produttività e i risparmi di memoria sono fondamentali per la fattibilità delle distribuzioni.</p>
<div id="exr-mc" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;9.2: Compressione di Modelli Scalabili con TensorFlow
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Questo Colab si addentra in una tecnica per comprimere i modelli mantenendo un’elevata accuratezza. L’idea chiave è quella di addestrare un modello con un termine di penalità extra che incoraggia il modello a essere più comprimibile. Quindi, il modello viene codificato utilizzando uno schema di codifica speciale che si allinea con questa penalità. Questo approccio consente di ottenere modelli compressi che funzionano altrettanto bene dei modelli originali ed è utile per distribuire modelli su dispositivi con risorse limitate come telefoni cellulari e dispositivi edge.</p>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/optimization/compression.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="modelli-progettati-per-ledge" class="level3 page-columns page-full" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="modelli-progettati-per-ledge"><span class="header-section-number">9.2.3</span> Modelli Progettati per l’Edge</h3>
<p>Ora raggiungiamo l’altro estremo del gradiente hardware-software, dove prendiamo decisioni specifiche sull’architettura del modello direttamente in base alla conoscenza dei dispositivi edge su cui desideriamo implementare.</p>
<p>Come spiegato nelle sezioni precedenti, i dispositivi edge sono vincolati specificamente da limitazioni di memoria e calcoli parallelizzabili: in quanto tali, se ci sono requisiti critici di velocità di inferenza, i calcoli devono essere sufficientemente flessibili da soddisfare i vincoli hardware, qualcosa che può essere progettato a livello di architettura del modello. Inoltre, cercare di stipare grandi modelli SOTA ML su dispositivi edge anche dopo potatura e compressione è generalmente irrealizzabile puramente a causa delle dimensioni: la complessità del modello stesso deve essere scelta con più sfumature per adattarsi più fattibilmente al dispositivo. Gli sviluppatori di Edge ML hanno affrontato questa sfida architettonica sia attraverso la progettazione di architetture di modelli edge ML su misura sia attraverso la Neural Architecture Search (NAS) [ricerca di architettura neurale] avente il dispositivo come target, che può generare in modo più sistematico architetture fattibili di modelli su dispositivo.</p>
<section id="tecniche-di-progettazione-del-modello" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="tecniche-di-progettazione-del-modello">Tecniche di Progettazione del Modello</h4>
<p>Un design di architettura edge friendly, comunemente utilizzato nel deep learning per l’elaborazione delle immagini, è quello delle convoluzioni separabili in profondità. Consiste in due fasi distinte: la prima è la convoluzione in profondità, in cui ogni canale di input viene convoluto in modo indipendente con il proprio set di filtri apprendibili, come mostrato in <a href="#fig-depthwise-convolution" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-depthwise-convolution</span></a>. Questa fase riduce la complessità computazionale in modo significativo rispetto alle convoluzioni standard, poiché riduce drasticamente il numero di parametri e calcoli coinvolti. La seconda fase è la convoluzione puntuale, che combina l’output dei canali di convoluzione in profondità tramite una convoluzione 1x1, creando interazioni tra canali. Questo approccio offre diversi vantaggi. I vantaggi includono dimensioni ridotte del modello, tempi di inferenza più rapidi e spesso una migliore generalizzazione grazie al minor numero di parametri, rendendolo adatto ad applicazioni mobili ed embedded. Tuttavia, le convoluzioni separabili in profondità potrebbero non catturare interazioni spaziali complesse in modo efficace come le convoluzioni standard e potrebbero richiedere più profondità (livelli) per raggiungere lo stesso livello di potenza rappresentativa, portando potenzialmente a tempi di addestramento più lunghi. Tuttavia, la loro efficienza in termini di parametri e calcolo le rende una scelta popolare nelle moderne architetture di reti neurali convoluzionali.</p>
<div id="fig-depthwise-convolution" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-depthwise-convolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_depthwise_separable_convolution.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-depthwise-convolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.12: Convoluzioni separabili in profondità. Fonte: <span class="citation" data-cites="hegde2023introduction">Hegde (<a href="#ref-hegde2023introduction" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-hegde2023introduction" class="csl-entry" role="listitem">
Hegde, Sumant. 2023. <span>«An Introduction to Separable Convolutions - Analytics Vidhya»</span>. <a href="https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/">https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/</a>.
</div></div></figure>
</div>
</section>
<section id="architetture-di-modello-di-esempio" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="architetture-di-modello-di-esempio">Architetture di Modello di Esempio</h4>
<p>In quest’ottica, diverse architetture recenti sono state, fin dall’inizio, progettate specificamente per massimizzare la precisione in un’implementazione edge, in particolare SqueezeNet, MobileNet ed EfficientNet.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/1602.07360">SqueezeNet</a> di <span class="citation" data-cites="iandola2016squeezenet">Iandola et al. (<a href="#ref-iandola2016squeezenet" role="doc-biblioref">2016</a>)</span>, ad esempio, utilizza un’architettura compatta con convoluzioni 1x1 e moduli “fire” per ridurre al minimo il numero di parametri mantenendo al contempo una forte accuratezza.</p></li>
<li><p><a href="https://arxiv.org/abs/1704.04861">MobileNet</a> di <span class="citation" data-cites="howard2017mobilenets">Howard et al. (<a href="#ref-howard2017mobilenets" role="doc-biblioref">2017</a>)</span>, d’altra parte, impiega le suddette convoluzioni separabili in profondità per ridurre sia il calcolo che le dimensioni del modello.</p></li>
<li><p><a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> di <span class="citation" data-cites="tan2020efficientnet">Tan e Le (<a href="#ref-tan2020efficientnet" role="doc-biblioref">2023</a>)</span> adotta un approccio diverso ottimizzando il ridimensionamento della rete (ovvero variando la profondità, la larghezza e la risoluzione di una rete) e il ridimensionamento composto, una variazione più sfumata del ridimensionamento della rete, per ottenere prestazioni superiori con meno parametri.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-iandola2016squeezenet" class="csl-entry" role="listitem">
Iandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. <span>«<span>SqueezeNet:</span> <span>Alexnet-level</span> accuracy with 50x fewer parameters and 0.5 <span>MB</span> model size»</span>. <em>ArXiv preprint</em> abs/1602.07360. <a href="https://arxiv.org/abs/1602.07360">https://arxiv.org/abs/1602.07360</a>.
</div><div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. <span>«<span>MobileNets:</span> <span>Efficient</span> Convolutional Neural Networks for Mobile Vision Applications»</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a>.
</div><div id="ref-tan2020efficientnet" class="csl-entry" role="listitem">
Tan, Mingxing, e Quoc V. Le. 2023. <span>«Demystifying Deep Learning»</span>. Wiley. <a href="https://doi.org/10.1002/9781394205639.ch6">https://doi.org/10.1002/9781394205639.ch6</a>.
</div></div><p>Questi modelli sono essenziali nel contesto dell’edge computing in cui la limitazione di potenza di elaborazione e di memoria richiede modelli leggeri ma efficaci in grado di eseguire in modo efficiente attività quali il riconoscimento delle immagini, il rilevamento di oggetti e altro ancora. I loro principi di progettazione mostrano l’importanza di un’architettura di modelli intenzionalmente personalizzata per l’edge computing, in cui prestazioni ed efficienza devono rientrare nei vincoli.</p>
</section>
<section id="semplificazione-della-ricerca-di-architetture-di-modelli" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="semplificazione-della-ricerca-di-architetture-di-modelli">Semplificazione della Ricerca di Architetture di Modelli</h4>
<p>Infine, per affrontare la sfida di trovare architetture di modelli efficienti che siano compatibili con i dispositivi edge, i ricercatori hanno sviluppato pipeline sistematizzate che semplificano la ricerca di progetti performanti. Due framework degni di nota in questo spazio sono <a href="https://arxiv.org/abs/2007.10319">TinyNAS</a> di <span class="citation" data-cites="lin2020mcunet">J. Lin et al. (<a href="#ref-lin2020mcunet" role="doc-biblioref">2020</a>)</span> e <a href="https://arxiv.org/abs/1711.06798">MorphNet</a> di <span class="citation" data-cites="gordon2018morphnet">Gordon et al. (<a href="#ref-gordon2018morphnet" role="doc-biblioref">2018</a>)</span>, che automatizzano il processo di ottimizzazione delle architetture di reti neurali per l’implementazione edge.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gordon2018morphnet" class="csl-entry" role="listitem">
Gordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, e Edward Choi. 2018. <span>«<span>MorphNet:</span> <span>Fast</span> &amp;amp; Simple Resource-Constrained Structure Learning of Deep Networks»</span>. In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 1586–95. IEEE. <a href="https://doi.org/10.1109/cvpr.2018.00171">https://doi.org/10.1109/cvpr.2018.00171</a>.
</div></div><p>TinyNAS è un innovativo framework di ricerca di architetture neurali introdotto nel documento MCUNet, progettato per scoprire in modo efficiente architetture di reti neurali leggere per dispositivi edge con risorse computazionali limitate. Sfruttando l’apprendimento per rinforzo e uno spazio di ricerca compatto di micromoduli neurali, TinyNAS ottimizza sia l’accuratezza che la latenza, consentendo l’implementazione di modelli di deep learning su microcontrollori, dispositivi IoT e altre piattaforme con risorse limitate. Nello specifico, TinyNAS, in combinazione con un ottimizzatore di rete, TinyEngine, genera diversi spazi di ricerca ridimensionando la risoluzione di input e la larghezza del modello, poi raccoglie la distribuzione FLOP di calcolo delle reti soddisfacenti all’interno dello spazio di ricerca per valutarne la priorità. TinyNAS si basa sul presupposto che uno spazio di ricerca che ospita FLOP più elevati con vincoli di memoria possa produrre modelli di accuratezza più elevata, cosa che gli autori hanno verificato in pratica nel loro lavoro. In termini di prestazioni empiriche, TinyEngine ha ridotto l’utilizzo di memoria di picco dei modelli di circa 3.4 volte e ha accelerato l’inferenza da 1.7 a 3.3 volte rispetto a <a href="https://www.tensorflow.org/lite">TFLite</a> e a <a href="https://www.keil.com/pack/doc/CMSIS/NN/html/index.html">CMSIS-NN</a>.</p>
<p>Analogamente, MorphNet è un framework di ottimizzazione delle reti neurali progettato per rimodellare e trasformare automaticamente l’architettura delle reti neurali profonde, ottimizzandole per requisiti di distribuzione specifici. Ciò avviene in due fasi: in primo luogo, sfrutta un set di operazioni di morphing della rete personalizzabili, come l’ampliamento o l’approfondimento dei layer, per regolare dinamicamente la struttura della rete. Queste operazioni consentono alla rete di adattarsi a vari vincoli computazionali, tra cui dimensioni del modello, latenza e obiettivi di accuratezza, che sono estremamente diffusi nell’utilizzo dell’edge computing. Nella seconda fase, MorphNet utilizza un approccio basato sull’apprendimento di rinforzo per cercare la permutazione ottimale delle operazioni di morphing, bilanciando efficacemente il compromesso tra dimensioni del modello e prestazioni. Questo metodo innovativo consente ai professionisti del deep learning di adattare automaticamente le architetture delle reti neurali a requisiti hardware e applicativi specifici, garantendo un’implementazione efficiente ed efficace su diverse piattaforme.</p>
<p>TinyNAS e MorphNet rappresentano alcuni dei numerosi progressi significativi nel campo dell’ottimizzazione sistematica delle reti neurali, consentendo di scegliere e generare sistematicamente architetture per adattarsi perfettamente ai vincoli del problema.</p>
<div id="exr-md" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;9.3: Modelli Progettati per l’Edge
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Si Immagini di costruire un piccolo robot in grado di identificare diversi fiori. Deve essere intelligente, ma anche piccolo ed efficiente dal punto di vista energetico! Nel mondo dell’“Edge-Aware Model Design”, abbiamo appreso tecniche come le convoluzioni separabili in base alla profondità e architetture come SqueezeNet, MobileNet ed EfficientNet, tutte progettate per concentrare l’intelligenza in modelli compatti. Ora, vediamo queste idee in azione con alcuni xColab:</p>
<p><strong>SqueezeNet in Action:</strong> Forse piacerebbe un Colab che mostra come addestrare un modello SqueezeNet su un set di dati di immagini di fiori. Ciò dimostrerebbe le sue piccole dimensioni e come impara a riconoscere i pattern nonostante la sua efficienza.</p>
<p><a href="https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_squeezenet.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
<p><strong>MobileNet Exploration:</strong> Ci si è mai chiesto se quei piccoli modelli di immagini sono buoni quanto quelli grandi? Scopriamolo! In questo Colab, mettiamo a confronto MobileNet, il campione dei pesi leggeri, con un modello di classificazione delle immagini classico. Li faremo gareggiare per la velocità, misureremo le loro esigenze di memoria e vedremo chi vincerà per accuratezza. Preparatevi per una battaglia di cervelli di immagini!</p>
<p><a href="https://colab.research.google.com/drive/1bOzVaDQo8h6Ngstb7AcfzC35OihpHspt"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-model_ops_numerics" class="level2 page-columns page-full" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="sec-model_ops_numerics"><span class="header-section-number">9.3</span> Rappresentazione Numerica Efficiente</h2>
<p>La rappresentazione numerica implica una miriade di considerazioni, tra cui, ma non solo, la precisione dei numeri, i loro formati di codifica e le operazioni aritmetiche facilitate. Implica invariabilmente una vasta gamma di diversi compromessi, in cui i professionisti sono incaricati di destreggiarsi tra accuratezza numerica ed efficienza computazionale. Ad esempio, mentre i numeri a bassa precisione possono offrire il fascino di un utilizzo di memoria ridotto e calcoli accelerati, presentano contemporaneamente sfide relative alla stabilità numerica e al potenziale degrado dell’accuratezza del modello.</p>
<section id="motivazione" class="level4">
<h4 class="anchored" data-anchor-id="motivazione">Motivazione</h4>
<p>Emerge l’imperativo per una rappresentazione numerica efficiente, in particolare perché l’ottimizzazione efficiente del modello da sola non è sufficiente quando si adattano i modelli per l’implementazione su dispositivi edge a bassa potenza che operano con vincoli rigorosi.</p>
<p>Oltre a ridurre al minimo le richieste di memoria, l’enorme potenziale di una rappresentazione numerica efficiente risiede, ma non è limitato a, queste modalità fondamentali. Riducendo l’intensità computazionale, la matematica efficiente può amplificare la velocità computazionale, consentendo di elaborare modelli più complessi su dispositivi a bassa potenza. Ridurre la precisione in bit di pesi e attivazioni su modelli fortemente sovra-parametrizzati consente la condensazione delle dimensioni del modello per dispositivi edge senza danneggiare significativamente l’accuratezza predittiva del modello. Con l’onnipresenza delle reti neurali nei modelli, la matematica efficiente ha un vantaggio unico nello sfruttare la struttura a layer delle NN per variare la precisione numerica tra i layer, riducendo al minimo la precisione nei layer resistenti e preservando una maggiore precisione in quelli sensibili.</p>
<p>In questa sezione, approfondiremo il modo in cui i professionisti possono sfruttare i principi della progettazione congiunta hardware-software ai livelli più bassi di un modello per facilitare la compatibilità con i dispositivi edge. Iniziando con un’introduzione ai numeri, esamineremo le sue implicazioni per la memoria del dispositivo e la complessità computazionale. Successivamente, intraprenderemo una discussione sui compromessi implicati nell’adozione di questa strategia, seguita da un’analisi approfondita di un metodo fondamentale della matematica efficiente: la quantizzazione.</p>
</section>
<section id="le-basi" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="le-basi"><span class="header-section-number">9.3.1</span> Le Basi</h3>
<section id="i-tipi" class="level4">
<h4 class="anchored" data-anchor-id="i-tipi">I Tipi</h4>
<p>I dati numerici, il fondamento su cui si basano i modelli di apprendimento automatico, si manifestano in due forme principali. Si tratta di numeri interi e numeri in virgola mobile.</p>
<p><strong>Numeri Interi:</strong> Numeri interi, privi di componenti frazionarie, (ad esempio, -3, 0, 42) sono fondamentali negli scenari che richiedono valori discreti. Ad esempio, in ML, le etichette di classe in un’attività di classificazione potrebbero essere rappresentate come numeri interi, dove “gatto”, “cane” e “uccello” potrebbero essere codificati rispettivamente come 0, 1 e 2.</p>
<p><strong>Numeri in virgola mobile:</strong> Comprendendo numeri reali, (ad esempio, -3.14, 0.01, 2.71828) consentono la rappresentazione di valori con componenti frazionarie. Nei parametri del modello ML, i pesi potrebbero essere inizializzati con piccoli valori a virgola mobile, ad esempio 0.001 o -0.045, per avviare il processo di training. Attualmente, ci sono 4 popolari formati di precisione discussi di seguito.</p>
<p><strong>Larghezze di bit variabili:</strong> Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezza di bit estremamente ridotta possono offrire accelerazioni significative e ridurre ulteriormente il consumo energetico. Sebbene permangano dei problemi nel mantenere l’accuratezza del modello con una quantizzazione così drastica, si continuano a fare progressi in quest’area.</p>
</section>
<section id="precisione" class="level4">
<h4 class="anchored" data-anchor-id="precisione">Precisione</h4>
<p>La precisione, che delinea l’esattezza con cui un numero è rappresentato, si biforca tipicamente in singola, doppia, mezza e negli ultimi anni sono emerse numerose altre precisioni per supportare meglio e in modo efficiente le attività di apprendimento automatico sull’hardware sottostante.</p>
<p><strong>Doppia precisione (Float64):</strong> Allocando 64 bit, la doppia precisione (ad esempio, 3.141592653589793) fornisce una precisione elevata, sebbene richieda più memoria e più risorse di calcolo. Nei calcoli scientifici, dove la precisione è fondamentale, variabili come π potrebbero essere rappresentate con Float64.</p>
<p><strong>Singola precisione (Float32):</strong> Con 32 bit a disposizione, la singola precisione (ad esempio, 3.1415927) raggiunge un equilibrio tra precisione numerica e risparmio della memoria. In ML, Float32 potrebbe essere impiegato per memorizzare i pesi durante l’addestramento per mantenere un livello ragionevole di precisione.</p>
<p><strong>Half Precision (Float16):</strong> Limitata a 16 bit, la half precision (ad esempio, 3.14) riduce l’utilizzo della memoria e può velocizzare i calcoli, sebbene sacrifichi l’accuratezza e l’intervallo numerico. In ML, specialmente durante l’inferenza su dispositivi con risorse limitate, Float16 potrebbe essere utilizzato per ridurre l’impronta di memoria del modello.</p>
<p><strong>Bfloat16:</strong> Brain Floating-Point Format o Bfloat16, impiega anche 16 bit ma li alloca in modo diverso rispetto a FP16: 1 bit per il segno, 8 bit per l’esponente (che si traduce nello stesso intervallo numerico di float32) e 7 bit per la frazione. Questo formato, sviluppato da Google, dà priorità a un intervallo di esponenti più ampio rispetto alla precisione, rendendolo particolarmente utile nelle applicazioni di apprendimento profondo in cui l’intervallo dinamico è cruciale.</p>
<p><a href="#fig-3float" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-3float</span></a> illustra le differenze tra i tre formati a virgola mobile: Float32, Float16 e BFloat16.</p>
<div id="fig-3float" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3float-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_3float_types.jpeg" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3float-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.13: Tre formati a virgola mobile.
</figcaption>
</figure>
</div>
<p><strong>Intero:</strong> Le rappresentazioni di numeri interi sono realizzate utilizzando 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocità e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attività di inferenza, in particolare su dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione è spesso accettabile dati i guadagni di efficienza. Una forma estrema di numeri interi è per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno di due valori: +1 o -1.</p>
<p>È possibile fare riferimento a <a href="#sec-numerical-formats" class="quarto-xref"><span class="quarto-unresolved-ref">sec-numerical-formats</span></a> per una tabella di confronto tra i compromessi dei diversi tipi numerici.</p>
</section>
<section id="codifica-e-archiviazione-numerica" class="level4">
<h4 class="anchored" data-anchor-id="codifica-e-archiviazione-numerica">Codifica e Archiviazione Numerica</h4>
<p>La codifica numerica, l’arte di trasformare i numeri in un formato utilizzabile dal computer e la loro successiva memorizzazione sono fondamentali per l’efficienza computazionale. Ad esempio, i numeri in virgola mobile potrebbero essere codificati utilizzando lo standard IEEE 754, che ripartisce i bit tra i componenti segno, esponente e frazione, consentendo così la rappresentazione di una vasta gamma di valori con un singolo formato. Esistono alcuni nuovi formati in virgola mobile IEEE che sono stati definiti specificamente per i carichi di lavoro AI:</p>
<ul>
<li><a href="https://cloud.google.com/tpu/docs/bfloat16">bfloat16</a>- Un formato in virgola mobile a 16 bit introdotto da Google. Ha 8 bit per esponente, 7 bit per mantissa e 1 bit per segno. Offre un compromesso di precisione ridotto tra float a 32 bit e interi a 8 bit. Supportato su molti acceleratori hardware.</li>
<li><a href="https://ieeexplore.ieee.org/document/9399648">posit</a> - Un formato configurabile che può rappresentare diversi livelli di precisione in base ai bit esponente. È più efficiente dei numeri binari in virgola mobile IEEE 754. Ha una gamma dinamica e una precisione regolabili.</li>
<li><a href="https://arxiv.org/abs/1711.02213">Flexpoint</a> - Un formato introdotto da Intel che può regolare dinamicamente la precisione tra livelli o all’interno di un layer. Consente di adattare la precisione all’accuratezza e ai requisiti hardware.</li>
<li><a href="https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/BFMLALB--BFMLALT--vector---BFloat16-floating-point-widening-multiply-add-long--vector--">BF16ALT</a> - Un formato a 16 bit proposto da ARM come alternativa a bfloat16. Utilizza un bit aggiuntivo nell’esponente per evitare overflow/underflow.</li>
<li><a href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/">TF32</a> - Introdotto da Nvidia per le GPU Ampere. Utilizza 10 bit per l’esponente invece di 8 bit come FP32. Migliora le prestazioni di training del modello mantenendo l’accuratezza.</li>
<li><a href="https://arxiv.org/abs/2209.05433">FP8</a> - Formato a virgola mobile a 8 bit che mantiene 6 bit per la mantissa e 2 bit per l’esponente. Consente una gamma dinamica migliore rispetto agli interi.</li>
</ul>
<p>Gli obiettivi principali di questi nuovi formati sono di fornire alternative di precisione inferiore ai float a 32 bit per una migliore efficienza computazionale e prestazioni sugli acceleratori AI, mantenendo al contempo l’accuratezza del modello. Offrono diversi compromessi in termini di precisione, portata e costo/complessità di implementazione.</p>
</section>
</section>
<section id="vantaggi-dellefficienza" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="vantaggi-dellefficienza"><span class="header-section-number">9.3.2</span> Vantaggi dell’Efficienza</h3>
<p>Come visto in <a href="#sec-efficiency-benefits" class="quarto-xref"><span class="quarto-unresolved-ref">sec-efficiency-benefits</span></a>, l’efficienza numerica è importante per i carichi di lavoro di apprendimento automatico per una serie di motivi. L’efficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano più pervasivi, soprattutto in ambienti reali con risorse limitate, l’attenzione su una numerica efficiente continuerà a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, è possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocità, memoria ed energia.</p>
</section>
<section id="sfumature-della-rappresentazione-numerica" class="level3 page-columns page-full" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="sfumature-della-rappresentazione-numerica"><span class="header-section-number">9.3.3</span> Sfumature della Rappresentazione Numerica</h3>
<p>Ci sono diverse sfumature con le rappresentazioni numeriche per ML che richiedono di avere una comprensione sia degli aspetti teorici che pratici della rappresentazione numerica, nonché una profonda consapevolezza dei requisiti e dei vincoli specifici del dominio applicativo.</p>
<section id="utilizzo-della-memoria" class="level4">
<h4 class="anchored" data-anchor-id="utilizzo-della-memoria">Utilizzo della Memoria</h4>
<p>L’impronta di memoria dei modelli ML, in particolare quelli di notevole complessità e profondità, può essere sostanziale, ponendo quindi una sfida significativa sia nelle fasi di training che di deployment. Ad esempio, una rete neurale profonda con 100 milioni di parametri, rappresentata utilizzando Float32 (32 bit o 4 byte per parametro), richiederebbe circa 400 MB di memoria solo per l’archiviazione dei pesi del modello. Ciò non tiene conto dei requisiti di memoria aggiuntivi durante il training per l’archiviazione di gradienti, stati dell’ottimizzatore e cache di passaggio forward [in avanti], che possono amplificare ulteriormente l’utilizzo della memoria, potenzialmente mettendo a dura prova le risorse su determinati hardware, in particolare dispositivi edge con capacità di memoria limitata.</p>
<p>La scelta della rappresentazione numerica ha un impatto ulteriore sull’utilizzo della memoria e sull’efficienza computazionale. Ad esempio, l’utilizzo di Float64 per i pesi del modello raddoppierebbe i requisiti di memoria rispetto a Float32 e potrebbe potenzialmente aumentare anche il tempo di elaborazione. Per una matrice di peso con dimensioni [1000, 1000], Float64 consumerebbe circa 8 MB di memoria, mentre Float32 la ridurrebbe a circa 4 MB. Pertanto, la selezione di un formato numerico appropriato è fondamentale per ottimizzare sia la memoria che l’efficienza computazionale.</p>
</section>
<section id="complessità-computazionale" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="complessità-computazionale">Complessità Computazionale</h4>
<p>La precisione numerica ha un impatto diretto sulla complessità computazionale, influenzando il tempo e le risorse necessarie per eseguire operazioni aritmetiche. Ad esempio, le operazioni che utilizzano Float64 generalmente consumano più risorse computazionali rispetto alle loro controparti Float32 o Float16 (vedere <a href="#fig-quantized-energy" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-quantized-energy</span></a>). Nel regno del ML, dove i modelli potrebbero dover elaborare milioni di operazioni (ad esempio, moltiplicazioni e addizioni in operazioni di matrice durante passaggi in forward e backward), anche piccole differenze nella complessità computazionale per operazione possono aggregarsi in un impatto sostanziale sui tempi di training e inferenza. Come mostrato in <a href="#fig-models-speeds" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-models-speeds</span></a>, i modelli quantizzati possono essere molte volte più veloci delle loro versioni non-quantizzate.</p>
<div id="fig-quantized-energy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantized-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_horowitz.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.14: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Mark Horowitz, Stanford University.
</figcaption>
</figure>
</div>
<div id="fig-models-speeds" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-models-speeds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_int8vsfloat.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-models-speeds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.15: Velocità di tre diversi modelli in forma normale e quantizzata.
</figcaption>
</figure>
</div>
<p>Oltre ai tempi di esecuzione puri, c’è anche una preoccupazione per l’efficienza energetica. Non tutti i calcoli numerici sono creati uguali dal punto di vista dell’hardware sottostante. Alcune operazioni numeriche sono più efficienti dal punto di vista energetico di altre. Ad esempio, <a href="#fig-operations-energy-comparison" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-operations-energy-comparison</span></a> di seguito mostra che l’addizione di interi è molto più efficiente dal punto di vista energetico della moltiplicazione di interi.</p>
<div id="fig-operations-energy-comparison" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-operations-energy-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_100x.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-operations-energy-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.16: Utilizzo di energia da parte di operazioni quantizzate. Fonte: <span class="citation" data-cites="isscc2014computings">Isscc (<a href="#ref-isscc2014computings" role="doc-biblioref">2014</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-isscc2014computings" class="csl-entry" role="listitem">
Isscc. 2014. <span>«Computing’s energy problem (and what we can do about it)»</span>. <a href="https://ieeexplore.ieee.org/document/6757323">https://ieeexplore.ieee.org/document/6757323</a>.
</div></div></figure>
</div>
</section>
<section id="compatibilità-hardware" class="level4">
<h4 class="anchored" data-anchor-id="compatibilità-hardware">Compatibilità Hardware</h4>
<p>Garantire la compatibilità e le prestazioni ottimizzate su diverse piattaforme hardware è un’altra sfida nella rappresentazione numerica. Hardware diversi, come CPU, GPU, TPU e FPGA, hanno capacità e ottimizzazioni diverse per gestire diverse precisioni numeriche. Ad esempio, alcune GPU potrebbero essere ottimizzate per i calcoli Float32, mentre altre potrebbero fornire accelerazioni per Float16. Sviluppare e ottimizzare modelli ML in grado di sfruttare le capacità numeriche specifiche di hardware diversi, garantendo al contempo che il modello mantenga la sua accuratezza e robustezza, richiede un’attenta considerazione e potenzialmente ulteriori sforzi di sviluppo e test.</p>
</section>
<section id="compromessi-di-precisione-e-accuratezza" class="level4">
<h4 class="anchored" data-anchor-id="compromessi-di-precisione-e-accuratezza">Compromessi di Precisione e Accuratezza</h4>
<p>Il compromesso tra precisione numerica e accuratezza del modello è una sfida “sfumata” nella rappresentazione numerica. L’utilizzo di numeri a bassa precisione, come Float16, potrebbe risparmiare memoria e velocizzare i calcoli, ma può anche introdurre problemi come errore di quantizzazione e intervallo numerico ridotto. Ad esempio, addestrare un modello con Float16 potrebbe introdurre problemi nella rappresentazione di valori di gradiente molto piccoli, potenzialmente influenzando la convergenza e la stabilità del processo di addestramento. Inoltre, in alcune applicazioni, come simulazioni scientifiche o calcoli finanziari, in cui l’elevata precisione è fondamentale, l’uso di numeri a bassa precisione potrebbe non essere consentito a causa del rischio di accumulare errori significativi.</p>
</section>
<section id="esempi-di-compromessi" class="level4">
<h4 class="anchored" data-anchor-id="esempi-di-compromessi">Esempi di Compromessi</h4>
<p>Per comprendere e apprezzare le sfumature, prendiamo in considerazione alcuni esempi di casi d’uso. Attraverso questi, ci renderemo conto che la scelta della rappresentazione numerica non è semplicemente una decisione tecnica, ma strategica, che influenza l’acume predittivo del modello, le sue esigenze computazionali e la sua implementabilità in diversi ambienti computazionali. In questa sezione esamineremo un paio di esempi per comprendere meglio i compromessi con i numeri e come si collegano al mondo reale.</p>
<section id="veicoli-autonomi" class="level5">
<h5 class="anchored" data-anchor-id="veicoli-autonomi">Veicoli Autonomi</h5>
<p>Nel dominio dei veicoli autonomi, i modelli ML vengono impiegati per interpretare i dati dei sensori e prendere decisioni in tempo reale. I modelli devono elaborare dati ad alta dimensionalità da vari sensori (ad esempio, LiDAR, telecamere, radar) ed eseguire numerosi calcoli entro un intervallo di tempo limitato per garantire un funzionamento sicuro e reattivo del veicolo. Quindi i compromessi qui includerebbero:</p>
<ul>
<li>Utilizzo della Memoria: L’archiviazione e l’elaborazione di dati dei sensori ad alta risoluzione, specialmente in formati a virgola mobile, possono consumare una quantità di memoria sostanziale.</li>
<li>Complessità Computazionale: L’elaborazione in tempo reale richiede calcoli efficienti, in cui numeri di precisione più elevata potrebbero impedire l’esecuzione tempestiva delle azioni di controllo.</li>
</ul>
</section>
<section id="applicazioni-sanitarie-mobili" class="level5">
<h5 class="anchored" data-anchor-id="applicazioni-sanitarie-mobili">Applicazioni Sanitarie Mobili</h5>
<p>Le applicazioni sanitarie mobili spesso utilizzano modelli ML per attività come il riconoscimento delle attività, il monitoraggio della salute o l’analisi predittiva, operando nell’ambiente con risorse limitate dei dispositivi mobili. I compromessi in questo caso includerebbero:</p>
<ul>
<li>Compromessi di Precisione e Accuratezza: L’impiego di numeri a bassa precisione per conservare risorse potrebbe influire sull’accuratezza delle previsioni sanitarie o delle rilevazioni di anomalie, il che potrebbe avere implicazioni significative per la salute e la sicurezza degli utenti.</li>
<li>Compatibilità Hardware: I modelli devono essere ottimizzati per diversi hardware mobili, garantendo un funzionamento efficiente su un’ampia gamma di dispositivi con diverse capacità di calcolo numerico.</li>
</ul>
</section>
<section id="sistemi-di-trading-ad-alta-frequenza-hft" class="level5">
<h5 class="anchored" data-anchor-id="sistemi-di-trading-ad-alta-frequenza-hft">Sistemi di Trading ad Alta Frequenza (HFT)</h5>
<p>I sistemi HFT sfruttano i modelli ML per prendere decisioni di trading rapide basate su dati di mercato in tempo reale. Questi sistemi richiedono risposte a bassissima latenza per capitalizzare le opportunità di trading di breve durata.</p>
<ul>
<li>Complessità Computazionale: I modelli devono elaborare e analizzare vasti flussi di dati di mercato con una latenza minima, dove anche lievi ritardi, potenzialmente introdotti da numeri a precisione più elevata, possono comportare opportunità perse.</li>
<li>Compromessi di Precisione e Accuratezza: I calcoli finanziari spesso richiedono un’elevata precisione numerica per garantire valutazioni accurate dei prezzi e dei rischi, ponendo sfide nel bilanciamento tra efficienza computazionale e accuratezza numerica.</li>
</ul>
</section>
<section id="sistemi-di-sorveglianza-basati-su-edge" class="level5">
<h5 class="anchored" data-anchor-id="sistemi-di-sorveglianza-basati-su-edge">Sistemi di Sorveglianza Basati su Edge</h5>
<p>I sistemi di sorveglianza distribuiti su dispositivi edge, come le telecamere di sicurezza, utilizzano modelli ML per attività come rilevamento di oggetti, riconoscimento di attività e rilevamento di anomalie, spesso operando con vincoli di risorse rigorosi.</p>
<ul>
<li>Utilizzo della Memoria: L’archiviazione di modelli pre-addestrati e l’elaborazione di feed video in tempo reale richiedono un utilizzo efficiente della memoria, il che può essere impegnativo con numeri ad alta precisione.</li>
<li>Compatibilità Hardware: Garantire che i modelli possano funzionare in modo efficiente su dispositivi edge con diverse capacità hardware e ottimizzazioni per diverse precisioni numeriche è fondamentale per una distribuzione diffusa.</li>
</ul>
</section>
<section id="simulazioni-scientifiche" class="level5">
<h5 class="anchored" data-anchor-id="simulazioni-scientifiche">Simulazioni Scientifiche</h5>
<p>I modelli ML vengono sempre più utilizzati nelle simulazioni scientifiche, come la modellazione climatica o le simulazioni di dinamica molecolare, per migliorare le capacità predittive e ridurre le richieste di calcolo.</p>
<ul>
<li>Compromessi di Precisione e Accuratezza: Le simulazioni scientifiche spesso richiedono un’elevata precisione numerica per garantire risultati accurati e affidabili, il che può entrare in conflitto con il desiderio di ridurre le richieste di calcolo tramite numeri a bassa precisione.</li>
<li>Complessità Computazionale: I modelli devono gestire ed elaborare dati di simulazione complessi e ad alta dimensionalità in modo efficiente per garantire risultati tempestivi e consentire simulazioni su larga scala o di lunga durata.</li>
</ul>
<p>Questi esempi illustrano diversi scenari in cui le sfide della rappresentazione numerica nei modelli ML sono palesemente manifestate. Ogni sistema presenta un set unico di requisiti e vincoli, che richiedono strategie e soluzioni personalizzate per affrontare i problemi dell’utilizzo della memoria, della complessità computazionale, dei compromessi tra precisione e accuratezza e della compatibilità hardware.</p>
</section>
</section>
</section>
<section id="sec-quant" class="level3" data-number="9.3.4">
<h3 data-number="9.3.4" class="anchored" data-anchor-id="sec-quant"><span class="header-section-number">9.3.4</span> Quantizzazione</h3>
<p>La quantizzazione è prevalente in vari domini scientifici e tecnologici e comporta essenzialmente la mappatura o la limitazione di un set o intervallo continuo in una controparte discreta per ridurre al minimo il numero di bit richiesti.</p>
<section id="analisi-iniziale" class="level4">
<h4 class="anchored" data-anchor-id="analisi-iniziale">Analisi Iniziale</h4>
<p>Iniziamo la nostra incursione nella quantizzazione con una breve analisi di un importante utilizzo della quantizzazione.</p>
<p>Nel signal processing [elaborazione del segnale], l’onda sinusoidale continua (mostrata in <a href="#fig-sine-wave" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-sine-wave</span></a>) può essere quantizzata in valori discreti tramite un processo noto come campionamento. Questo è un concetto fondamentale nell’elaborazione del segnale digitale ed è cruciale per convertire segnali analogici (come l’onda sinusoidale continua) in una forma digitale che possa essere elaborata dai computer. L’onda sinusoidale è un esempio prevalente grazie alla sua natura periodica e regolare, il che la rende uno strumento utile per spiegare concetti come frequenza, ampiezza, fase e, naturalmente, quantizzazione.</p>
<div id="fig-sine-wave" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_sinewave.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.17: Onda Sinusoidale.
</figcaption>
</figure>
</div>
<p>Nella versione quantizzata mostrata in <a href="#fig-quantized-sine-wave" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-quantized-sine-wave</span></a>, l’onda sinusoidale continua (<a href="#fig-sine-wave" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-sine-wave</span></a>) viene campionata a intervalli regolari (in questo caso, ogni <span class="math inline">\(\frac{\pi}{4}\)</span> radianti) e solo questi valori campionati vengono rappresentati nella versione digitale del segnale. Le linee graduali tra i punti mostrano un modo per rappresentare il segnale quantizzato in una forma costante a tratti. Questo è un esempio semplificato di come funziona la conversione analogico-digitale, in cui un segnale continuo viene mappato su un set discreto di valori, consentendone la rappresentazione e l’elaborazione digitale.</p>
<div id="fig-quantized-sine-wave" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantized-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_quantizedsine.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.18: Onda Sinusoidale Quantizzata.
</figcaption>
</figure>
</div>
<p>Tornando al contesto del Machine Learning (ML), la quantizzazione si riferisce al processo di limitazione dei possibili valori che i parametri numerici (come pesi e bias) possono assumere in un set discreto, riducendo così la precisione dei parametri e, di conseguenza, l’ingombro di memoria del modello. Se implementata correttamente, la quantizzazione può ridurre le dimensioni del modello fino a 4 volte e migliorare la latenza e la produttività dell’inferenza fino a 2-3 volte. <a href="#fig-quantized-models-size" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-quantized-models-size</span></a> illustra l’impatto che la quantizzazione ha sulle dimensioni di modelli diversi: ad esempio, un modello di classificazione delle immagini come ResNet-v2 può essere compresso da 180 MB a 45 MB con quantizzazione a 8 bit. In genere, la perdita di accuratezza del modello è inferiore all’1% con una quantizzazione ben fatta. L’accuratezza può spesso essere recuperata riaddestrando il modello quantizzato con tecniche di addestramento consapevoli della quantizzazione. Pertanto, questa tecnica è emersa come molto importante nell’implementazione di modelli ML in ambienti con risorse limitate, come dispositivi mobili, dispositivi IoT e piattaforme di edge computing, dove le risorse computazionali (memoria e potenza di elaborazione) sono limitate.</p>
<div id="fig-quantized-models-size" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantized-models-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_reducedmodelsize.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-models-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.19: Effetto della quantizzazione sulle dimensioni del modello. Fonte: HarvardX.
</figcaption>
</figure>
</div>
<p>Esistono diverse dimensioni della quantizzazione, come uniformità, stocasticità (o determinismo), simmetria, granularità (tra layer/canali/gruppi o persino all’interno dei canali), considerazioni sulla calibrazione dell’intervallo (statico o dinamico) e metodi di messa a punto (QAT, PTQ, ZSQ). Esaminiamo questi di seguito.</p>
</section>
</section>
<section id="i-tipi-1" class="level3 page-columns page-full" data-number="9.3.5">
<h3 data-number="9.3.5" class="anchored" data-anchor-id="i-tipi-1"><span class="header-section-number">9.3.5</span> I Tipi</h3>
<section id="quantizzazione-uniforme" class="level4">
<h4 class="anchored" data-anchor-id="quantizzazione-uniforme">Quantizzazione Uniforme</h4>
<p>La quantizzazione uniforme implica la mappatura di valori continui o ad alta precisione su una rappresentazione a precisione inferiore utilizzando una scala uniforme. Ciò significa che l’intervallo tra ogni possibile valore quantizzato è coerente. Ad esempio, se i pesi di un layer di rete neurale sono quantizzati su numeri interi a 8 bit (valori tra 0 e 255), un peso con un valore in virgola mobile di 0.56 potrebbe essere mappato su un valore intero di 143, presupponendo una mappatura lineare tra le scale originale e quantizzata. Grazie all’uso di pipeline matematiche intere o a virgola fissa, questa forma di quantizzazione consente il calcolo sul dominio quantizzato senza la necessità di dequantizzare in anticipo.</p>
<p>Il processo per implementare la quantizzazione uniforme inizia con la scelta di un intervallo di numeri reali da quantizzare. Il passaggio successivo consiste nel selezionare una funzione di quantizzazione e mappare i valori reali sugli interi rappresentabili dalla larghezza di bit della rappresentazione quantizzata. Ad esempio, una scelta popolare per una funzione di quantizzazione è:</p>
<p><span class="math display">\[
Q(r)=Int(r/S) - Z
\]</span></p>
<p>dove <span class="math inline">\(Q\)</span> è l’operatore di quantizzazione, <span class="math inline">\(r\)</span> è un input a valore reale (nel nostro caso, un’attivazione o un peso), <span class="math inline">\(S\)</span> è un fattore di scala a valore reale e <span class="math inline">\(Z\)</span> è un punto zero intero. La funzione <code>Int</code> mappa un valore reale in un valore intero tramite un’operazione di arrotondamento. Tramite questa funzione, abbiamo mappato in modo efficace i valori reali <span class="math inline">\(r\)</span> in alcuni valori interi, ottenendo livelli quantizzati uniformemente distanziati.</p>
<p>Quando i professionisti hanno la necessità di recuperare i valori originali di precisione più elevata, i valori reali <span class="math inline">\(r\)</span> possono essere recuperati dai valori quantizzati tramite un’operazione nota come <strong>dequantizzazione</strong>. Nell’esempio sopra, ciò significherebbe eseguire la seguente operazione sul nostro valore quantizzato:</p>
<p><span class="math display">\[
\bar{r} = S(Q(r) + Z)
\]</span></p>
<p>Come discusso, una certa precisione nel valore reale viene persa dalla quantizzazione. In questo caso, il valore recuperato <span class="math inline">\(\bar{r}\)</span> non corrisponderà esattamente a <span class="math inline">\(r\)</span> a causa dell’operazione di arrotondamento. Questo è un importante compromesso da notare; tuttavia, in molti utilizzi riusciti della quantizzazione, la perdita di precisione può essere trascurabile e l’accuratezza del test rimane elevata. Nonostante ciò, la quantizzazione uniforme continua a essere la scelta di fatto attuale per la sua semplicità e l’efficiente mappatura all’hardware.</p>
</section>
<section id="quantizzazione-non-uniforme" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="quantizzazione-non-uniforme">Quantizzazione Non-Uniforme</h4>
<p>La quantizzazione non uniforme, d’altro canto, non mantiene un intervallo coerente tra i valori quantizzati. Questo approccio potrebbe essere utilizzato per allocare più possibili valori discreti in regioni in cui i valori dei parametri sono più densamente popolati, preservando così maggiori dettagli dove sono più necessari. Ad esempio, nelle distribuzioni a campana di pesi con lunghe code, un set di pesi in un modello si trova prevalentemente all’interno di un certo intervallo; quindi, più livelli di quantizzazione potrebbero essere assegnati a tale intervallo per preservare dettagli più fini, consentendoci di acquisire meglio le informazioni. Tuttavia, una delle principali debolezze della quantizzazione non uniforme è che richiede la dequantizzazione prima di calcoli di precisione più elevata a causa della sua non uniformità, limitando la sua capacità di accelerare il calcolo rispetto alla quantizzazione uniforme.</p>
<p>In genere, una quantizzazione non uniforme basata su regole utilizza una distribuzione logaritmica di passaggi e livelli esponenzialmente crescenti anziché linearmente. Un altra tipologia popolare risiede nella quantizzazione basata su codice binario in cui i vettori di numeri reali vengono quantizzati in vettori binari con un fattore di scala. In particolare, non esiste una soluzione in forma chiusa per minimizzare gli errori tra il valore reale e il valore non uniformemente quantizzato, quindi la maggior parte delle quantizzazioni in questo campo si basa su soluzioni euristiche. Ad esempio, un <a href="https://arxiv.org/abs/1802.00150">lavoro recente</a> di <span class="citation" data-cites="xu2018alternating">Xu et al. (<a href="#ref-xu2018alternating" role="doc-biblioref">2018</a>)</span> formula la quantizzazione non uniforme come un problema di ottimizzazione in cui i passaggi/livelli di quantizzazione nel quantizzatore <span class="math inline">\(Q\)</span> vengono regolati per ridurre al minimo la differenza tra il tensore originale e la controparte quantizzata.</p>
<div class="no-row-height column-margin column-container"><div id="ref-xu2018alternating" class="csl-entry" role="listitem">
Xu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, e Hongbin Zha. 2018. <span>«Alternating Multi-bit Quantization for Recurrent Neural Networks»</span>. In <em>6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings</em>. OpenReview.net. <a href="https://openreview.net/forum?id=S19dR9x0b">https://openreview.net/forum?id=S19dR9x0b</a>.
</div></div><p><span class="math display">\[
\min_Q ||Q(r)-r||^2
\]</span></p>
<p>Inoltre, i quantizzatori addestrabili lo possono essere congiuntamente con parametri di modello e i passaggi/livelli di quantizzazione sono generalmente addestrati con ottimizzazione iterativa o discesa del gradiente. Inoltre, il clustering è stato utilizzato per alleviare la perdita di informazioni dalla quantizzazione. Sebbene in grado di catturare livelli di dettaglio più elevati, gli schemi di quantizzazione non uniformi possono essere difficili da implementare in modo efficiente su hardware di calcolo generale, rendendoli meno preferiti ai metodi che utilizzano la quantizzazione uniforme.</p>
<div id="fig-quantization-uniformity" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-uniformity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_uniformnonuniform.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-uniformity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.20: Uniformità della Quantizzazione. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
</section>
<section id="quantizzazione-stocastica" class="level4">
<h4 class="anchored" data-anchor-id="quantizzazione-stocastica">Quantizzazione Stocastica</h4>
<p>A differenza dei due approcci precedenti che generano mappature deterministiche, c’è un po’ di lavoro che esplora l’idea della quantizzazione stocastica per l’addestramento consapevole della quantizzazione e l’addestramento a precisione ridotta. Questo approccio mappa numeri fluttuanti verso l’alto o verso il basso con una probabilità associata alla grandezza dell’aggiornamento del peso. La speranza generata dall’intuizione di alto livello è che un tale approccio probabilistico possa consentire a una rete neurale di esplorare di più, rispetto alla quantizzazione deterministica. Presumibilmente, abilitare un arrotondamento stocastico potrebbe consentire alle reti neurali di sfuggire agli ottimi locali, aggiornando così i propri parametri. Di seguito sono riportati due esempi di funzioni di mappatura stocastica:</p>
<p><img src="images/png/efficientnumerics_nonuniform.png" class="img-fluid"></p>
<div id="fig-integer-binary-quantization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-integer-binary-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_binary.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-integer-binary-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.21: Funzioni di quantizzazione Intera e Binaria.
</figcaption>
</figure>
</div>
</section>
<section id="quantizzazione-zero-shot" class="level4">
<h4 class="anchored" data-anchor-id="quantizzazione-zero-shot">Quantizzazione “Zero Shot”</h4>
<p>La quantizzazione Zero-shot si riferisce al processo di conversione di un modello di deep learning a precisione completa direttamente in un modello quantizzato a bassa precisione senza la necessità di alcun riaddestramento o messa a punto sul modello quantizzato. Il vantaggio principale di questo approccio è la sua efficienza, in quanto elimina il processo, spesso dispendioso in termini di tempo e risorse, del riaddestramento post-quantizzazione. Sfruttando tecniche che anticipano e riducono al minimo gli errori di quantizzazione, la quantizzazione zero-shot mantiene l’accuratezza originale del modello anche dopo averne ridotto la precisione numerica. È particolarmente utile per i provider di “Machine Learning as a Service (MLaaS)” che mirano ad accelerare la distribuzione dei carichi di lavoro dei propri clienti senza dover accedere ai loro set di dati.</p>
</section>
</section>
<section id="calibrazione" class="level3 page-columns page-full" data-number="9.3.6">
<h3 data-number="9.3.6" class="anchored" data-anchor-id="calibrazione"><span class="header-section-number">9.3.6</span> Calibrazione</h3>
<p>La calibrazione è il processo di selezione dell’intervallo di clipping [ritaglio] più efficace [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] per pesi e attivazioni da quantizzare. Ad esempio, si consideri la quantizzazione delle attivazioni che originariamente hanno un intervallo in virgola mobile tra -6 e 6 a interi a 8 bit. Prendere solo i valori minimi e massimi possibili di interi a 8 bit (da -128 a 127) come intervallo di quantizzazione, potrebbe non essere il più efficace. Invece, la calibrazione implicherebbe il passaggio di un set di dati rappresentativo e quindi l’utilizzo di questo intervallo osservato per la quantizzazione.</p>
<p>Esistono molti metodi di calibrazione, ma alcuni comunemente utilizzati includono:</p>
<ul>
<li>Max: Utilizza il valore assoluto massimo visualizzato durante la calibrazione. Tuttavia, questo metodo è suscettibile di dati anomali. Notare come in <a href="#fig-resnet-activations-histogram" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-resnet-activations-histogram</span></a>, abbiamo un cluster anomalo intorno a 2.1, mentre il resto è raggruppato attorno a valori più piccoli.</li>
<li>Entropia: Utilizza la divergenza KL per ridurre al minimo la perdita di informazioni tra i valori originali in virgola mobile e i valori che potrebbero essere rappresentati dal formato quantizzato. Questo è il metodo predefinito utilizzato da TensorRT.</li>
<li>Percentile: Imposta l’intervallo su un percentile della distribuzione dei valori assoluti osservati durante la calibrazione. Ad esempio, una calibrazione del 99% taglierebbe l’1% dei valori di magnitudine più grandi.</li>
</ul>
<div id="fig-resnet-activations-histogram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-resnet-activations-histogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_calibrationcopy.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-resnet-activations-histogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.22: Attivazioni di input nel layer 3 in ResNet50. Fonte: @<span class="citation" data-cites="wu2020integer">Wu, Judd, e Isaev (<a href="#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>È importante notare che la qualità della calibrazione può fare la differenza tra un modello quantizzato che conserva la maggior parte della sua accuratezza e uno che si degrada in modo significativo. Quindi, è un passaggio essenziale nel processo di quantizzazione. Quando si sceglie un intervallo di calibrazione, ci sono due tipi: simmetrico e asimmetrico.</p>
<section id="quantizzazione-simmetrica" class="level4">
<h4 class="anchored" data-anchor-id="quantizzazione-simmetrica">Quantizzazione Simmetrica</h4>
<p>La quantizzazione simmetrica mappa i valori reali su un intervallo di clipping simmetrico centrato su 0. Ciò comporta la scelta di un intervallo [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] dove <span class="math inline">\(\alpha = -\beta\)</span>. Ad esempio, un intervallo simmetrico si baserebbe sui valori min/max dei valori reali in modo tale che:</p>
<p><span class="math display">\[
\alpha = \beta = max(abs(r_{max}), abs(r_{min}))
\]</span></p>
<p>Gli intervalli di clipping simmetrici sono i più ampiamente adottati nella pratica in quanto hanno il vantaggio di un’implementazione più semplice. In particolare, la mappatura da zero a zero nell’intervallo di clipping (talvolta chiamata “azzeramento del punto zero”) può portare a una riduzione del costo computazionale durante l’inferenza <a href="https://arxiv.org/abs/2004.09602"><span class="citation" data-cites="wu2020integer">(<span>Wu, Judd, e Isaev 2020</span>)</span></a>.</p>
</section>
<section id="quantizzazione-asimmetrica" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="quantizzazione-asimmetrica">Quantizzazione Asimmetrica</h4>
<p>La quantizzazione asimmetrica mappa i valori reali in un intervallo di clipping asimmetrico che non è necessariamente centrato sullo 0, come mostrato in <a href="#fig-quantization-symmetry" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-quantization-symmetry</span></a> a destra. Comporta la scelta di un intervallo [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] dove <span class="math inline">\(\alpha \neq -\beta\)</span>. Ad esempio, selezionando un intervallo basato sui valori reali minimi e massimi, o dove <span class="math inline">\(\alpha = r_{min}\)</span> and <span class="math inline">\(\beta = r_{max}\)</span>, si crea un intervallo asimmetrico. In genere, la quantizzazione asimmetrica produce intervalli di clipping più stretti rispetto a quella simmetrica, il che è importante quando i pesi e le attivazioni target sono sbilanciati, ad esempio, l’attivazione dopo la ReLU ha sempre valori non negativi. Nonostante produca intervalli di clipping più stretti, la quantizzazione asimmetrica è meno preferita di quella simmetrica in quanto non azzera sempre il valore dello zero reale.</p>
<div id="fig-quantization-symmetry" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-symmetry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_symmetry.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-symmetry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.23: (a)simmetria della Quantizzazione. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
</section>
<section id="granularità" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="granularità">Granularità</h4>
<p>Dopo aver deciso il tipo di intervallo di clipping, è essenziale restringerlo per consentire a un modello di mantenere la massima accuratezza possibile. Daremo un’occhiata alle reti neurali convoluzionali come nostro modo di esplorare metodi che ottimizzano la granularità degli intervalli di clipping per la quantizzazione. L’attivazione di input di un layer nella nostra CNN subisce una convoluzione con più filtri convoluzionali. Ogni filtro convoluzionale può possedere un intervallo di valori univoco. Si noti come in <a href="#fig-quantization-granularity" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-quantization-granularity</span></a> l’intervallo per il Filtro 1 sia molto più piccolo di quello per il Filtro 3. Di conseguenza, una caratteristica distintiva degli approcci di quantizzazione è la precisione con cui l’intervallo di clipping [α,β] viene determinato per i pesi.</p>
<div id="fig-quantization-granularity" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_granularity.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.24: Granularità di quantizzazione: intervalli variabili. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<ol type="1">
<li><strong>Quantizzazione a Layer:</strong> Questo approccio determina l’intervallo di clipping considerando tutti i pesi nei filtri convoluzionali di un layer. Quindi, lo stesso intervallo di clipping viene utilizzato per tutti i filtri convoluzionali. È il più semplice da implementare e, come tale, spesso si traduce in una precisione non ottimale a causa dell’ampia varietà di intervalli diversi tra i filtri. Ad esempio, un kernel convoluzionale con un intervallo di parametri più ristretto perde la sua risoluzione di quantizzazione a causa di un altro kernel nello stesso layer che ha un intervallo più ampio.</li>
<li><strong>Groupwise Quantization:</strong> Questo approccio raggruppa diversi canali all’interno di un layer per calcolare l’intervallo di clipping. Questo metodo può essere utile quando la distribuzione dei parametri su una singola convoluzione/attivazione varia molto. In pratica, questo metodo è stato utile in Q-BERT <span class="citation" data-cites="sheng2019qbert">(<a href="#ref-sheng2019qbert" role="doc-biblioref">Shen et al. 2020</a>)</span> per quantizzare i modelli Transformer <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span> costituiti da layer di attenzione completamente connessi. Lo svantaggio di questo approccio è il costo aggiuntivo di contabilizzazione di diversi fattori di scala.</li>
<li><strong>Channelwise Quantization:</strong> Questo metodo popolare utilizza un intervallo fisso per ogni filtro convoluzionale che è indipendente dagli altri canali. Poiché a ogni canale viene assegnato un fattore di scala dedicato, questo metodo garantisce una risoluzione di quantizzazione più elevata e spesso si traduce in una maggiore accuratezza.</li>
<li><strong>Sub-channelwise Quantization:</strong> Portando la quantizzazione canale per canale all’estremo, questo metodo determina l’intervallo di clipping rispetto a qualsiasi gruppo di parametri in una convoluzione o in un layer completamente connesso. Potrebbe comportare un overhead considerevole poiché è necessario tenere conto di diversi fattori di scala quando si elabora una singola convoluzione o un layer completamente connesso.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-sheng2019qbert" class="csl-entry" role="listitem">
Shen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, e Kurt Keutzer. 2020. <span>«Q-<span>BERT:</span> <span>Hessian</span> Based Ultra Low Precision Quantization of <span>BERT</span>»</span>. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 34 (05): 8815–21. <a href="https://doi.org/10.1609/aaai.v34i05.6409">https://doi.org/10.1609/aaai.v34i05.6409</a>.
</div><div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, e Illia Polosukhin. 2017. <span>«Attention is all you need»</span>. <em>Adv Neural Inf Process Syst</em> 30.
</div></div><p>Tra questi, la quantizzazione canale per canale è lo standard corrente utilizzato per quantizzare i kernel convoluzionali, poiché consente la regolazione degli intervalli di clipping per ogni singolo kernel con overhead trascurabile.</p>
</section>
<section id="quantizzazione-statica-e-dinamica" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="quantizzazione-statica-e-dinamica">Quantizzazione Statica e Dinamica</h4>
<p>Dopo aver determinato il tipo e la granularità dell’intervallo di clipping, gli esperti devono decidere quando gli intervalli vengono determinati nei loro algoritmi di calibrazione dell’intervallo. Esistono due approcci per quantizzare le attivazioni: quantizzazione statica e quella dinamica.</p>
<p>La quantizzazione statica è l’approccio più frequentemente utilizzato. In questo, l’intervallo di clipping è precalcolato e statico durante l’inferenza. Non aggiunge alcun sovraccarico computazionale, ma, di conseguenza, comporta una minore accuratezza rispetto alla quantizzazione dinamica. Un metodo popolare per implementarlo è eseguire una serie di input di calibrazione per calcolare l’intervallo tipico di attivazioni <span class="citation" data-cites="jacob2018quantization yao2021hawq">(<a href="#ref-jacob2018quantization" role="doc-biblioref">Jacob et al. 2018</a>; <a href="#ref-yao2021hawq" role="doc-biblioref">Yao et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jacob2018quantization" class="csl-entry" role="listitem">
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. <span>«Quantization and training of neural networks for efficient integer-arithmetic-only inference»</span>. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2704–13.
</div><div id="ref-yao2021hawq" class="csl-entry" role="listitem">
Yao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, et al. 2021. <span>«Hawq-v3: <span>Dyadic</span> neural network quantization»</span>. In <em>International Conference on Machine Learning</em>, 11875–86. PMLR.
</div></div><p>La quantizzazione dinamica è un approccio alternativo che calcola dinamicamente l’intervallo per ogni mappa di attivazione durante il runtime. L’approccio richiede calcoli in tempo reale che potrebbero avere un sovraccarico molto elevato. In questo modo, la quantizzazione dinamica spesso raggiunge la massima accuratezza poiché l’intervallo viene calcolato specificamente per ogni input.</p>
<p>Tra i due, il calcolo dell’intervallo in modo dinamico è solitamente molto costoso, quindi la maggior parte dei professionisti utilizzerà spesso la quantizzazione statica.</p>
</section>
</section>
<section id="tecniche" class="level3 page-columns page-full" data-number="9.3.7">
<h3 data-number="9.3.7" class="anchored" data-anchor-id="tecniche"><span class="header-section-number">9.3.7</span> Tecniche</h3>
<p>Quando si ottimizzano i modelli di apprendimento automatico per l’implementazione, vengono utilizzate varie tecniche di quantizzazione per bilanciare efficienza, accuratezza e adattabilità del modello. Ogni metodo (quantizzazione post-addestramento, addestramento consapevole della quantizzazione e quantizzazione dinamica) offre vantaggi e compromessi unici, che incidono su fattori quali complessità di implementazione, sovraccarico computazionale e ottimizzazione delle prestazioni.</p>
<p><a href="#tbl-quantization_methods" class="quarto-xref">Tabella&nbsp;<span class="quarto-unresolved-ref">tbl-quantization_methods</span></a> fornisce una panoramica di questi metodi di quantizzazione, evidenziandone i rispettivi punti di forza, limiti e compromessi. Ci addentreremo più a fondo in ciascuno di questi metodi perché sono ampiamente distribuiti e utilizzati in tutti i sistemi ML di scale molto diverse.</p>
<div id="tbl-quantization_methods" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-quantization_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;9.2: Confronto tra quantizzazione post-training, training sensibile alla quantizzazione e quantizzazione dinamica.
</figcaption>
<div aria-describedby="tbl-quantization_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspetto</th>
<th style="text-align: left;">Quantizzazione Post Training</th>
<th style="text-align: left;">Training Quantization-Aware</th>
<th style="text-align: left;">Quantization Dinamica</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Pro</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Semplicità</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✗</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Preservazione della Precisione</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✓</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adattabilità</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Prestazioni Ottimizzate</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">Potentialmente</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Contro</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Degrado della Precisione</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">Potentialmente</td>
</tr>
<tr class="even">
<td style="text-align: left;">Overhead Computazionale</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✓</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Complessità di Implementazione</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✓</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Compromessi</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Velocità vs.&nbsp;Precisione</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✗</td>
</tr>
<tr class="even">
<td style="text-align: left;">Precisione vs.&nbsp;Costo</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✗</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adaptability vs.&nbsp;Overhead</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Post Training Quantization:</strong> La quantizzazione post-addestramento (PTQ) è una tecnica di quantizzazione in cui il modello viene quantizzato dopo essere stato addestrato. Il modello viene addestrato in virgola mobile e poi i pesi e le attivazioni vengono quantizzati come fase di post-elaborazione. Questo è l’approccio più semplice e non richiede l’accesso ai dati di addestramento. Diversamente la “Quantization-Aware Training (QAT), PTQ” imposta direttamente i parametri di quantizzazione del peso e dell’attivazione, rendendolo poco costoso e adatto a situazioni con dati limitati o non etichettati. Tuttavia, non riaggiustare i pesi dopo la quantizzazione, specialmente nella quantizzazione a bassa precisione, può portare a un comportamento molto diverso e quindi a una minore accuratezza. Per affrontare questo problema, sono state sviluppate tecniche come la correzione della distorsione, l’equalizzazione degli intervalli di peso e i metodi di arrotondamento adattivo. PTQ può essere applicato anche in scenari zero-shot, in cui non sono disponibili dati di addestramento o di test. Questo metodo è stato reso ancora più efficiente per avvantaggiare modelli linguistici di grandi dimensioni che richiedono molta elaborazione e memoria. Di recente, è stata sviluppata SmoothQuant, una soluzione PTQ senza training, che preserva l’accuratezza ed è di uso generale che consente la quantizzazione di peso a 8 bit e attivazione a 8 bit per LLM, dimostrando un’accelerazione fino a 1.56x e una riduzione della memoria di 2x per LLM con una perdita trascurabile di accuratezza <a href="https://arxiv.org/abs/2211.10438"><span class="citation" data-cites="xiao2022smoothquant">(<span>Xiao et al. 2022</span>)</span></a>.</p>
<p>In PTQ, un modello pre-addestrato subisce un processo di calibrazione, come mostrato in <a href="#fig-PTQ-diagram" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-PTQ-diagram</span></a>. La calibrazione comporta l’utilizzo di un set di dati separato noto come dati di calibrazione, un sottoinsieme specifico dei dati di training riservato alla quantizzazione per aiutare a trovare gli intervalli di clipping e i fattori di scala appropriati.</p>
<div id="fig-PTQ-diagram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-PTQ-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQ.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-PTQ-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.25: Quantizzazione e Calibrazione Post-Training. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p><strong>Quantization-Aware Training:</strong> L’addestramento consapevole della quantizzazione (QAT) è una messa a punto del modello PTQ. Il modello viene addestrato in modo consapevole della quantizzazione, consentendogli di adattarsi agli effetti della quantizzazione. Ciò produce una migliore accuratezza con l’inferenza quantizzata. La quantizzazione di un modello di rete neurale addestrato con metodi come PTQ introduce perturbazioni che possono deviare il modello dal suo punto di convergenza originale. Ad esempio, Krishnamoorthi ha dimostrato che anche con la quantizzazione per canale, reti come MobileNet non raggiungono la precisione di base con int8 PTQ e richiedono QAT <a href="https://arxiv.org/abs/1806.08342"><span class="citation" data-cites="krishnamoorthi2018quantizing">(<span>Krishnamoorthi 2018</span>)</span></a>. Per risolvere questo problema, QAT riqualifica il modello con parametri quantizzati, utilizzando passaggi forward [avanti] e backward [indietro] in virgola mobile ma quantizzando i parametri dopo ogni aggiornamento del gradiente. La gestione dell’operatore di quantizzazione non differenziabile è fondamentale; un metodo ampiamente utilizzato è lo “Straight Through Estimator (STE)”, che approssima l’operazione di arrotondamento come una funzione identità. Sebbene esistano altri metodi e varianti, STE rimane il più comunemente utilizzato per la sua efficacia pratica. In QAT, un modello pre-addestrato viene quantizzato e poi messo a punto utilizzando i dati di addestramento per regolare i parametri e recuperare il degrado della precisione, come mostrato in <a href="#fig-QAT-diagram" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-QAT-diagram</span></a>. Il processo di calibrazione viene spesso condotto parallelamente al processo di messa a punto per QAT.</p>
<div id="fig-QAT-diagram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-QAT-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_QAT.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-QAT-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.26: Quantization-Aware Training. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-gholami2021survey" class="csl-entry" role="listitem">
Gholami, Dong Kim, Mahoney Yao, e Keutzer. 2021. <span>«A Survey of Quantization Methods for Efficient Neural Network Inference)»</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/2103.13630">https://arxiv.org/abs/2103.13630</a>.
</div></div></figure>
</div>
<p>La “Quantization-Aware Training” funge da estensione naturale della “Post-Training Quantization”. Dopo la quantizzazione iniziale eseguita da PTQ, QAT viene utilizzata per perfezionare e mettere a punto ulteriormente i parametri quantizzati: vedere come in <a href="#fig-QAT-PTQ-relation" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-QAT-PTQ-relation</span></a>, il modello PTQ subisce un ulteriore passaggio, QAT. Comporta un processo di riqualificazione in cui il modello viene esposto a ulteriori iterazioni di training utilizzando i dati originali. Questo approccio di training dinamico consente al modello di adattare e regolare i suoi parametri, compensando il degrado delle prestazioni causato dalla quantizzazione.</p>
<div id="fig-QAT-PTQ-relation" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-QAT-PTQ-relation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQQAT.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-QAT-PTQ-relation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.27: PTQ e QAT. Fonte: <span class="citation" data-cites="ultimate"><span>«The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training»</span> (<a href="#ref-ultimate" role="doc-biblioref">s.d.</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-ultimate" class="csl-entry" role="listitem">
<span>«The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training»</span>. s.d. <a href="https://deci.ai/quantization-and-quantization-aware-training/">https://deci.ai/quantization-and-quantization-aware-training/</a>.
</div></div></figure>
</div>
<p><a href="#fig-quantization-methods-summary" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-quantization-methods-summary</span></a> mostra l’accuratezza relativa di diversi modelli dopo PTQ e QAT. In quasi tutti i casi, QAT produce un’accuratezza migliore di PTQ. Si consideri ad esempio EfficientNet b0. Dopo PTQ, l’accuratezza scende dal 76.85% a 72.06%. Ma quando applichiamo QAT, l’accuratezza rimbalza al 76.95% (con persino un leggero miglioramento rispetto all’accuratezza originale).</p>
<div id="fig-quantization-methods-summary" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-methods-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQQATsummary.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-methods-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.28: Accuratezza relativa di PTQ e QAT. Fonte: <span class="citation" data-cites="wu2020integer">Wu, Judd, e Isaev (<a href="#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
</section>
<section id="pesi-vs.-attivazioni" class="level3" data-number="9.3.8">
<h3 data-number="9.3.8" class="anchored" data-anchor-id="pesi-vs.-attivazioni"><span class="header-section-number">9.3.8</span> Pesi vs.&nbsp;Attivazioni</h3>
<p><strong>Quantizzazione del peso:</strong> Comporta la conversione dei pesi continui o ad alta precisione di un modello in pesi a bassa precisione, come la conversione dei pesi Float32 in pesi INT8 (interi) quantizzati - in <a href="#fig-weight-activations-quantization" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-weight-activations-quantization</span></a>, la quantizzazione del peso avviene nel secondo passaggio (quadrati rossi) quando moltiplichiamo gli input. Ciò riduce le dimensioni del modello, riducendo così la memoria richiesta per archiviare il modello e le risorse computazionali necessarie per eseguire l’inferenza. Ad esempio, si consideri una matrice di pesi in un layer di rete neurale con pesi Float32 come [0.215, -1.432, 0.902, …]. Attraverso la quantizzazione del peso, questi potrebbero essere mappati su valori INT8 come [27, -183, 115, …], riducendo significativamente la memoria richiesta per memorizzarli.</p>
<div id="fig-weight-activations-quantization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weight-activations-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_weightsactivations.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weight-activations-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.29: Quantizzazione del peso e dell’attivazione. Fonte: HarvardX.
</figcaption>
</figure>
</div>
<p><strong>Quantizzazione dell’Attivazione:</strong> Comporta la quantizzazione dei valori di attivazione (output dei livelli) durante l’inferenza del modello. Ciò può ridurre le risorse computazionali richieste durante l’inferenza, ma introduce ulteriori problemi nel mantenimento dell’accuratezza del modello a causa della ridotta precisione dei calcoli intermedi. Ad esempio, in una rete neurale convoluzionale (CNN), le mappe di attivazione (mappe delle feature) prodotte dai layer convoluzionali, originariamente in Float32, potrebbero essere quantizzate su INT8 durante l’inferenza per accelerare il calcolo, in particolare su hardware ottimizzato per l’aritmetica degli interi. Inoltre, un lavoro recente ha esplorato l’uso della quantizzazione del “Activation-aware Weight Quantization” per la compressione e l’accelerazione LLM, che comporta la protezione di solo l’1% dei pesi salienti più importanti osservando le attivazioni, non i pesi <a href="https://arxiv.org/pdf/2306.00978.pdf"><span class="citation" data-cites="lin2023awq">(<span>Lin et al. 2023</span>)</span></a>.</p>
</section>
<section id="compromessi" class="level3 page-columns page-full" data-number="9.3.9">
<h3 data-number="9.3.9" class="anchored" data-anchor-id="compromessi"><span class="header-section-number">9.3.9</span> Compromessi</h3>
<p>La quantizzazione introduce invariabilmente un compromesso tra dimensioni/prestazioni del modello e accuratezza. Sebbene riduca significativamente l’ingombro della memoria e possa accelerare l’inferenza, specialmente su hardware ottimizzato per aritmetica a bassa precisione, la precisione ridotta può degradare l’accuratezza del modello.</p>
<p><strong>Dimensioni del Modello:</strong> Un modello con pesi rappresentati come Float32 quantizzato a INT8 può teoricamente ridurre le dimensioni del modello di un fattore 4, consentendone l’implementazione su dispositivi con memoria limitata. Le dimensioni di grandi modelli linguistici si stanno sviluppando a un ritmo più veloce della memoria GPU negli ultimi anni, portando a un grande divario tra domanda e offerta di memoria. <a href="#fig-model-size-pace" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-model-size-pace</span></a> illustra la recente tendenza del divario crescente tra le dimensioni del modello (linea rossa) e la memoria dell’acceleratore (linea gialla). Le tecniche di quantizzazione e compressione del modello possono aiutare a colmare il divario</p>
<div id="fig-model-size-pace" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-model-size-pace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_modelsizes.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-size-pace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.30: Dimensioni del modello vs.&nbsp;memoria dell’acceleratore. Fonte: <span class="citation" data-cites="xiao2022smoothquant">Xiao et al. (<a href="#ref-xiao2022smoothquant" role="doc-biblioref">2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-xiao2022smoothquant" class="csl-entry" role="listitem">
Xiao, Seznec Lin, Demouth Wu, e Han. 2022. <span>«<span>SmoothQuant:</span> <span>Accurate</span> and Efficient Post-Training Quantization for Large Language Models»</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/2211.10438">https://arxiv.org/abs/2211.10438</a>.
</div></div></figure>
</div>
<p><strong>Velocità di Inferenza:</strong> La quantizzazione può anche accelerare l’inferenza, poiché l’aritmetica a precisione inferiore è computazionalmente meno costosa. Ad esempio, alcuni acceleratori hardware, come Edge TPU di Google, sono ottimizzati per l’aritmetica INT8 e possono eseguire l’inferenza in modo significativamente più rapido con modelli quantizzati INT8 rispetto alle loro controparti in virgola mobile. La riduzione della memoria dalla quantizzazione aiuta a ridurre la quantità di trasmissione dei dati, risparmiando memoria e velocizzando il processo. <a href="#fig-nvidia-turing" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-nvidia-turing</span></a> confronta l’aumento della produttività e la riduzione della memoria della larghezza di banda per diversi tipi di dati sulla NVIDIA Turing GPU.</p>
<div id="fig-nvidia-turing" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-nvidia-turing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_benefitsofprecision.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nvidia-turing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.31: Vantaggi dei tipi di dati a precisione inferiore. Fonte: <span class="citation" data-cites="wu2020integer">Wu, Judd, e Isaev (<a href="#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-wu2020integer" class="csl-entry" role="listitem">
Wu, Zhang Judd, e Micikevicius Isaev. 2020. <span>«Integer Quantization for Deep Learning Inference: <span>Principles</span> and Empirical Evaluation)»</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/2004.09602">https://arxiv.org/abs/2004.09602</a>.
</div></div></figure>
</div>
<p><strong>Precisione:</strong> La riduzione della precisione numerica post-quantizzazione può portare a un degrado della precisione del modello, che potrebbe essere accettabile in alcune applicazioni (ad esempio, classificazione delle immagini) ma non in altre (ad esempio, diagnosi medica). Pertanto, dopo la quantizzazione, il modello richiede in genere una ricalibrazione o una messa a punto per mitigare la perdita di accuratezza. Inoltre, un lavoro recente ha esplorato l’uso di <a href="https://arxiv.org/pdf/2306.00978.pdf">Activation-aware Weight Quantization <span class="citation" data-cites="lin2023awq">(<span>Lin et al. 2023</span>)</span></a> che si basa sull’osservazione che proteggere solo l’1% dei pesi salienti può ridurre notevolmente l’errore di quantizzazione.</p>
</section>
<section id="quantizzazione-e-potatura" class="level3 page-columns page-full" data-number="9.3.10">
<h3 data-number="9.3.10" class="anchored" data-anchor-id="quantizzazione-e-potatura"><span class="header-section-number">9.3.10</span> Quantizzazione e Potatura</h3>
<p>Pruning [potatura] e quantizzazione funzionano bene insieme ed è stato scoperto che il pruning non ostacola la quantizzazione. In effetti, il pruning può aiutare a ridurre l’errore di quantizzazione. Intuitivamente, ciò è dovuto al pruning che riduce il numero di pesi da quantizzare, riducendo così l’errore accumulato dalla quantizzazione. Ad esempio, una AlexNet non potata ha 60 milioni di pesi da quantizzare mentre una AlexNet potata ha solo 6.7 milioni di pesi da quantizzare. Questa significativa riduzione dei pesi aiuta a ridurre l’errore tra la quantizzazione dell’AlexNet non potato rispetto all’AlexNet potato. Inoltre, studi recenti hanno scoperto che il pruning consapevole della quantizzazione genera modelli più efficienti dal punto di vista computazionale rispetto al pruning o alla quantizzazione da soli; in genere, ha prestazioni simili o migliori in termini di efficienza computazionale rispetto ad altre tecniche di ricerca dell’architettura neurale come l’ottimizzazione bayesiana <a href="https://arxiv.org/pdf/2102.11289.pdf"><span class="citation" data-cites="hawks2021psandqs">(<span>Hawks et al. 2021</span>)</span></a>.</p>
<div id="fig-compression-methods" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-compression-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_qp1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compression-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.32: Precisione rispetto al tasso di compressione con diversi metodi di compressione. Fonte: <span class="citation" data-cites="han2015deep">Han, Mao, e Dally (<a href="#ref-han2015deep" role="doc-biblioref">2015</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-han2015deep" class="csl-entry" role="listitem">
Han, Song, Huizi Mao, e William J Dally. 2015. <span>«Deep compression: <span>Compressing</span> deep neural networks with pruning, trained quantization and huffman coding»</span>. <em>arXiv preprint arXiv:1510.00149</em>.
</div></div></figure>
</div>
</section>
<section id="quantizzazione-edge-aware" class="level3" data-number="9.3.11">
<h3 data-number="9.3.11" class="anchored" data-anchor-id="quantizzazione-edge-aware"><span class="header-section-number">9.3.11</span> Quantizzazione Edge-aware</h3>
<p>La quantizzazione non solo riduce le dimensioni del modello, ma consente anche calcoli più rapidi e consuma meno energia, rendendola fondamentale per lo sviluppo per edge. I dispositivi edge in genere hanno vincoli di risorse rigidi con elaborazione, memoria e potenza, impossibili da soddisfare per molti dei modelli deep NN profondi odierni. Inoltre, i processori edge non supportano le operazioni in virgola mobile, rendendo la quantizzazione intera particolarmente importante per chip come GAP-8, un SoC RISC-V per l’inferenza edge con un acceleratore CNN dedicato, che supporta solo l’aritmetica intera.</p>
<p>Una piattaforma hardware che utilizza la quantizzazione è il gruppo ARM Cortex-M di core di processori ARM RISC a 32 bit. Sfruttano la quantizzazione a virgola fissa con fattori di scala di potenza di due, in modo che la quantizzazione e la de-quantizzazione possano essere eseguite in modo efficiente tramite spostamento di bit. Inoltre, Google Edge TPU, la soluzione emergente di Google per l’esecuzione di inferenze in periferia, è progettata per dispositivi piccoli e a bassa potenza e può supportare solo l’aritmetica a 8 bit. Molti modelli di reti neurali complesse che potevano essere distribuiti solo su server a causa delle loro elevate esigenze di elaborazione possono ora essere eseguiti su dispositivi edge grazie ai recenti progressi (ad esempio metodi di quantizzazione) nel campo dell’edge computing.</p>
<p>Oltre a essere una tecnica indispensabile per molti processori edge, la quantizzazione ha anche apportato notevoli miglioramenti ai processori non edge, incoraggiando tali processori a soddisfare i requisiti del Service Level Agreement (SLA) come la latenza del 99° percentile.</p>
<p>Pertanto, la quantizzazione combinata con una logica efficiente a bassa precisione e acceleratori dedicati di deep learning, è stata una forza trainante cruciale per l’evoluzione di tali processori edge.</p>
<p><a href="#vid-quant" class="quarto-xref">Video&nbsp;<span class="quarto-unresolved-ref">vid-quant</span></a> è una lezione sulla quantizzazione e sui diversi metodi di quantizzazione.</p>
<div id="vid-quant" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;9.1: Quantizzazione
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AlASZb93rrc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
</section>
<section id="sec-model_ops_hw" class="level2 page-columns page-full" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="sec-model_ops_hw"><span class="header-section-number">9.4</span> Implementazione Hardware Efficiente</h2>
<p>L’implementazione hardware efficiente trascende la selezione di componenti adatti; richiede una comprensione olistica di come il software interagirà con le architetture sottostanti. L’essenza del raggiungimento delle massime prestazioni nelle applicazioni TinyML non risiede solo nell’affinare gli algoritmi per l’hardware, ma anche nell’assicurare che l’hardware sia strategicamente adattato per supportare questi algoritmi. Questa sinergia tra hardware e software è fondamentale. Mentre esaminiamo più a fondo le complessità dell’implementazione hardware efficiente, il significato di un approccio di progettazione congiunta, in cui hardware e software vengono sviluppati in tandem, diventa sempre più evidente. Questa sezione fornisce una panoramica delle tecniche di come l’hardware e le interazioni tra hardware e software possono essere ottimizzati per migliorare le prestazioni dei modelli.</p>
<section id="ricerca-di-architettura-neurale-basata-sullhardware" class="level3 page-columns page-full" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="ricerca-di-architettura-neurale-basata-sullhardware"><span class="header-section-number">9.4.1</span> Ricerca di Architettura Neurale Basata sull’Hardware</h3>
<p>Concentrarsi solo sulla precisione durante l’esecuzione della ricerca di architettura neurale porta a modelli esponenzialmente complessi e che richiedono memoria e capacità di elaborazione crescenti. Ciò ha portato a vincoli hardware che limitano lo sfruttamento dei modelli di apprendimento profondo al loro pieno potenziale. Progettare manualmente l’architettura del modello è ancora più difficile se si considerano la varietà e le limitazioni dell’hardware. Ciò ha portato alla creazione di Hardware-aware Neural Architecture Search che incorpora le contrazioni hardware nella loro ricerca e ottimizza lo spazio di ricerca per un hardware e una precisione specifici. HW-NAS può essere categorizzato in base a come ottimizza per l’hardware. Esploreremo brevemente queste categorie e lasceremo dei link a documenti correlati per il lettore interessato.</p>
<section id="configurazione-single-target-fixed-platfrom" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="configurazione-single-target-fixed-platfrom">Configurazione Single Target, Fixed Platfrom</h4>
<p>L’obiettivo qui è trovare la migliore architettura in termini di precisione ed efficienza hardware per un hardware target fisso. Per un hardware specifico, ad esempio Arduino Nicla Vision, questa categoria di HW-NAS cercherà l’architettura che ottimizza precisione, latenza, consumo energetico, ecc.</p>
<section id="strategia-di-ricerca-hardware-aware" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="strategia-di-ricerca-hardware-aware">Strategia di Ricerca Hardware-aware</h5>
<p>Qui, la ricerca è un problema di ottimizzazione multi-obiettivo, in cui sia l’accuratezza che il costo dell’hardware guidano l’algoritmo di ricerca per trovare l’architettura più efficiente <span class="citation" data-cites="tan2019mnasnet cai2018proxylessnas wu2019fbnet">(<a href="#ref-tan2019mnasnet" role="doc-biblioref">Tan et al. 2019</a>; <a href="#ref-cai2018proxylessnas" role="doc-biblioref">Cai, Zhu, e Han 2019</a>; <a href="#ref-wu2019fbnet" role="doc-biblioref">B. Wu et al. 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tan2019mnasnet" class="csl-entry" role="listitem">
Tan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, e Quoc V. Le. 2019. <span>«<span>MnasNet:</span> <span>Platform-aware</span> Neural Architecture Search for Mobile»</span>. In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2820–28. IEEE. <a href="https://doi.org/10.1109/cvpr.2019.00293">https://doi.org/10.1109/cvpr.2019.00293</a>.
</div><div id="ref-cai2018proxylessnas" class="csl-entry" role="listitem">
Cai, Han, Ligeng Zhu, e Song Han. 2019. <span>«<span>ProxylessNAS:</span> <span>Direct</span> Neural Architecture Search on Target Task and Hardware»</span>. In <em>7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net. <a href="https://openreview.net/forum?id=HylVB3AqYm">https://openreview.net/forum?id=HylVB3AqYm</a>.
</div><div id="ref-wu2019fbnet" class="csl-entry" role="listitem">
Wu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, e Yangqing Jia. 2019. <span>«<span>FBNet:</span> <span>Hardware-aware</span> Efficient <span>ConvNet</span> Design via Differentiable Neural Architecture Search»</span>. In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10734–42. IEEE. <a href="https://doi.org/10.1109/cvpr.2019.01099">https://doi.org/10.1109/cvpr.2019.01099</a>.
</div></div></section>
<section id="spazio-di-ricerca-hardware-aware" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="spazio-di-ricerca-hardware-aware">Spazio di Ricerca Hardware-aware</h5>
<p>Qui, lo spazio di ricerca è limitato alle architetture che funzionano bene sull’hardware specifico. Questo può essere ottenuto misurando le prestazioni degli operatori (operatore Conv, operatore Pool, …) o definendo un set di regole che limitano lo spazio di ricerca. <span class="citation" data-cites="zhang2020fast">(<a href="#ref-zhang2020fast" role="doc-biblioref">L. L. Zhang et al. 2020</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2020fast" class="csl-entry" role="listitem">
Zhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, e Yunxin Liu. 2020. <span>«Fast Hardware-Aware Neural Architecture Search»</span>. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>. IEEE. <a href="https://doi.org/10.1109/cvprw50498.2020.00354">https://doi.org/10.1109/cvprw50498.2020.00354</a>.
</div></div></section>
</section>
<section id="configurazioni-single-target-multiple-platform" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="configurazioni-single-target-multiple-platform">Configurazioni Single Target, Multiple Platform</h4>
<p>Alcuni hardware possono avere configurazioni diverse. Ad esempio, gli FPGA hanno blocchi logici configurabili (CLB) che possono essere configurati dal firmware. Questo metodo consente all’HW-NAS di esplorare diverse configurazioni. <span class="citation" data-cites="jiang2019accuracy yang2020coexploration">(<a href="#ref-jiang2019accuracy" role="doc-biblioref">Hu et al. 2023</a>; <a href="#ref-yang2020coexploration" role="doc-biblioref">Ho Yoon et al. 2012</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-yang2020coexploration" class="csl-entry" role="listitem">
Ho Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song, Jun Yeong Seok, Kyung Jean Yoon, et al. 2012. <span>«Frontiers in Electronic Materials»</span>. Wiley. <a href="https://doi.org/10.1002/9783527667703.ch67">https://doi.org/10.1002/9783527667703.ch67</a>.
</div></div></section>
<section id="target-multipli" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="target-multipli">Target Multipli</h4>
<p>Questa categoria mira a ottimizzare un singolo modello per più hardware. Questo può essere utile per lo sviluppo di dispositivi mobili in quanto può ottimizzare diversi modelli di telefoni. <span class="citation" data-cites="chu2021discovering jiang2019accuracy">(<a href="#ref-chu2021discovering" role="doc-biblioref">Chu et al. 2021</a>; <a href="#ref-jiang2019accuracy" role="doc-biblioref">Hu et al. 2023</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-chu2021discovering" class="csl-entry" role="listitem">
Chu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. <span>«Discovering Multi-Hardware Mobile Models via Architecture Search»</span>. In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 3022–31. IEEE. <a href="https://doi.org/10.1109/cvprw53098.2021.00337">https://doi.org/10.1109/cvprw53098.2021.00337</a>.
</div><div id="ref-jiang2019accuracy" class="csl-entry" role="listitem">
Hu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, e Jian Shi. 2023. <span>«Halide Perovskite Semiconductors»</span>. Wiley. <a href="https://doi.org/10.1002/9783527829026.ch13">https://doi.org/10.1002/9783527829026.ch13</a>.
</div></div></section>
<section id="esempi-di-hardware-aware-neural-architecture-search" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="esempi-di-hardware-aware-neural-architecture-search">Esempi di “Hardware-Aware Neural Architecture Search”</h4>
<section id="tinynas" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="tinynas">TinyNAS</h5>
<p>TinyNAS adotta un approccio in due fasi per trovare un’architettura ottimale per il modello tenendo a mente i vincoli del microcontrollore specifico.</p>
<p>Innanzitutto, TinyNAS genera più spazi di ricerca variando la risoluzione di input del modello e il numero di canali dei layer. Quindi, TinyNAS sceglie uno spazio di ricerca in base ai FLOP (operazioni in virgola mobile al secondo) di ogni spazio di ricerca. Gli spazi con una probabilità maggiore di contenere architetture con un numero elevato di FLOP producono modelli con maggiore accuratezza: confrontare la linea rossa con la linea nera in <a href="#fig-search-space-flops" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-search-space-flops</span></a>. Poiché un numero maggiore di FLOP significa che il modello ha una maggiore capacità di calcolo, è più probabile che il modello abbia una maggiore accuratezza.</p>
<p>Poi, TinyNAS esegue un’operazione di ricerca sullo spazio scelto per trovare l’architettura ottimale per i vincoli specifici del microcontrollore. <span class="citation" data-cites="lin2020mcunet">(<a href="#ref-lin2020mcunet" role="doc-biblioref">J. Lin et al. 2020</a>)</span></p>
<div class="no-row-height column-margin column-container"></div><div id="fig-search-space-flops" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-search-space-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_TinyNAS.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-search-space-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.33: Precisione degli spazi di ricerca. Fonte: <span class="citation" data-cites="lin2020mcunet">J. Lin et al. (<a href="#ref-lin2020mcunet" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. <span>«<span>MCUNet:</span> <span>Tiny</span> Deep Learning on <span>IoT</span> Devices»</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html</a>.
</div></div></figure>
</div>
</section>
</section>
<section id="topology-aware-nas" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="topology-aware-nas">Topology-Aware NAS</h4>
<p>Si concentra sulla creazione e l’ottimizzazione di uno spazio di ricerca allineato alla topologia hardware del dispositivo. <span class="citation" data-cites="zhang2019autoshrink">(<a href="#ref-zhang2019autoshrink" role="doc-biblioref">T. Zhang et al. 2020</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2019autoshrink" class="csl-entry" role="listitem">
Zhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Helen Li, e Yiran Chen. 2020. <span>«<span>AutoShrink:</span> <span>A</span> Topology-Aware <span>NAS</span> for Discovering Efficient Neural Architecture»</span>. In <em>The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, 6829–36. AAAI Press. <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6163">https://aaai.org/ojs/index.php/AAAI/article/view/6163</a>.
</div></div></section>
</section>
<section id="sfide-nella-hardware-aware-neural-architecture-search" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="sfide-nella-hardware-aware-neural-architecture-search"><span class="header-section-number">9.4.2</span> Sfide nella “Hardware-Aware Neural Architecture Search”</h3>
<p>Sebbene HW-NAS abbia un potenziale elevato per trovare architetture ottimali per TinyML, presenta alcuni problemi. Le metriche hardware come latenza, consumo energetico e utilizzo dell’hardware sono più difficili da valutare rispetto alle metriche di accuratezza o di perdita. Spesso richiedono strumenti specializzati per misure precise. Inoltre, l’aggiunta di tutte queste metriche porta a uno spazio di ricerca molto più grande. Ciò fa sì che HW-NAS sia dispendioso in termini di tempo e denaro. Deve essere applicato a ogni hardware per risultati ottimali, tra le altre cose, il che significa che se si deve distribuire il modello su più dispositivi, la ricerca deve essere condotta più volte e produrrà modelli diversi, a meno che non si ottimizzi per tutti, il che significa una minore accuratezza. Infine, l’hardware cambia frequentemente e potrebbe essere necessario eseguire HW-NAS su ogni versione.</p>
</section>
<section id="ottimizzazioni-del-kernel" class="level3 page-columns page-full" data-number="9.4.3">
<h3 data-number="9.4.3" class="anchored" data-anchor-id="ottimizzazioni-del-kernel"><span class="header-section-number">9.4.3</span> Ottimizzazioni del Kernel</h3>
<p>Le ottimizzazioni del kernel sono modifiche apportate al kernel per migliorare le prestazioni dei modelli di apprendimento automatico su dispositivi con risorse limitate. Separeremo le ottimizzazioni del kernel in due tipi.</p>
<section id="ottimizzazioni-del-kernel-generali" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="ottimizzazioni-del-kernel-generali">Ottimizzazioni del kernel Generali</h4>
<p>Queste sono ottimizzazioni del kernel da cui tutti i dispositivi possono trarre vantaggio. Forniscono tecniche per convertire il codice in istruzioni più efficienti.</p>
<section id="srotolamento-del-loop" class="level5">
<h5 class="anchored" data-anchor-id="srotolamento-del-loop">“Srotolamento” del Loop</h5>
<p>Invece di avere un loop con “loop control” (incrementando il contatore, si controlla la condizione di terminazione del loop), il loop può essere srotolato e il sovraccarico del “loop control” può essere omesso. Questo può anche fornire ulteriori opportunità di parallelismo che potrebbero non essere possibili con la struttura con loop. Questo può essere particolarmente utile per loop stretti, in cui il corpo del loop è un piccolo numero di istruzioni con molte iterazioni.</p>
</section>
<section id="blocking" class="level5">
<h5 class="anchored" data-anchor-id="blocking">Blocking</h5>
<p>Il Blocking viene utilizzato per rendere più efficienti i pattern di accesso alla memoria. Se abbiamo tre calcoli, il primo e l’ultimo devono accedere alla cache A e il secondo deve accedere alla cache B, il “blocking” ferma i primi due calcoli per ridurre il numero di letture di memoria necessarie.</p>
</section>
<section id="tiling" class="level5">
<h5 class="anchored" data-anchor-id="tiling">Tiling</h5>
<p>Analogamente al blocking, il tiling [piastrellatura] divide i dati e il calcolo in blocchi, ma si estende oltre i miglioramenti della cache. Il tiling crea partizioni di calcolo indipendenti che possono essere eseguite in parallelo, il che può comportare significativi miglioramenti delle prestazioni.</p>
</section>
<section id="librerie-kernel-ottimizzate" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="librerie-kernel-ottimizzate">Librerie Kernel Ottimizzate</h5>
<p>Questo comprende lo sviluppo di kernel ottimizzati che sfruttano appieno un hardware specifico. Un esempio è la libreria CMSIS-NN, che è una raccolta di kernel di reti neurali efficienti sviluppati per ottimizzare le prestazioni e ridurre al minimo l’ingombro di memoria dei modelli sui processori Arm Cortex-M, comuni sui dispositivi edge IoT. Il kernel sfrutta più capacità hardware dei processori Cortex-M come Single Instruction Multiple Data (SIMD), Floating Point Unit (FPU) e M-Profile Vector Extensions (MVE). Queste ottimizzazioni rendono più efficienti le operazioni comuni come le moltiplicazioni di matrici, aumentando le prestazioni delle operazioni del modello sui processori Cortex-M. <span class="citation" data-cites="lai2018cmsisnn">(<a href="#ref-lai2018cmsisnn" role="doc-biblioref">Lai, Suda, e Chandra 2018</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-lai2018cmsisnn" class="csl-entry" role="listitem">
Lai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. <span>«<span>CMSIS</span>-<span>NN:</span> <span>Efficient</span> Neural Network Kernels for Arm Cortex-M <span>CPUs</span>»</span>. <a href="https://arxiv.org/abs/1801.06601">https://arxiv.org/abs/1801.06601</a>.
</div></div></section>
</section>
</section>
<section id="compute-in-memory-cim" class="level3 page-columns page-full" data-number="9.4.4">
<h3 data-number="9.4.4" class="anchored" data-anchor-id="compute-in-memory-cim"><span class="header-section-number">9.4.4</span> Compute-in-Memory (CiM)</h3>
<p>Questo è un esempio di progettazione congiunta di algoritmo e hardware. CiM è un paradigma di elaborazione che esegue calcoli all’interno della memoria. Pertanto, le architetture CiM consentono di eseguire operazioni direttamente sui dati archiviati, senza la necessità di spostare i dati avanti e indietro tra unità di elaborazione e memoria separate. Questo paradigma di progettazione è particolarmente utile in scenari in cui lo spostamento dei dati è una fonte primaria di consumo energetico e latenza, come nelle applicazioni TinyML su dispositivi edge. <a href="#fig-computing-memory" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-computing-memory</span></a> è un esempio di utilizzo di CiM in TinyML: l’individuazione delle parole chiave richiede un processo sempre attivo che cerca determinate parole di attivazione (come “Hey, Siri”). Data la natura ad alta intensità di risorse di questa attività, l’integrazione di CiM per il modello di rilevamento delle parole chiave sempre attivo può migliorare l’efficienza.</p>
<p>Attraverso la progettazione congiunta di algoritmo e hardware, gli algoritmi possono essere ottimizzati per sfruttare le caratteristiche uniche delle architetture CiM e, l’hardware CiM può essere personalizzato o configurato per supportare meglio i requisiti di elaborazione e le caratteristiche degli algoritmi. Ciò si ottiene utilizzando le proprietà analogiche delle celle di memoria, come l’addizione e la moltiplicazione nella DRAM. <span class="citation" data-cites="zhou2021analognets">(<a href="#ref-zhou2021analognets" role="doc-biblioref">Zhou et al. 2021</a>)</span></p>
<div class="no-row-height column-margin column-container"></div><div id="fig-computing-memory" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-computing-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_CiM.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-computing-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.34: CiM per l’individuazione delle parole chiave. Fonte: <span class="citation" data-cites="zhou2021analognets">Zhou et al. (<a href="#ref-zhou2021analognets" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-zhou2021analognets" class="csl-entry" role="listitem">
Zhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat, Xavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian, Manuel Le Gallo, e Paul N. Whatmough. 2021. <span>«<span>AnalogNets:</span> <span>Ml-hw</span> Co-Design of Noise-robust <span>TinyML</span> Models and Always-On Analog Compute-in-Memory Accelerator»</span>. <a href="https://arxiv.org/abs/2111.06503">https://arxiv.org/abs/2111.06503</a>.
</div></div></figure>
</div>
</section>
<section id="ottimizzazione-dellaccesso-alla-memoria" class="level3 page-columns page-full" data-number="9.4.5">
<h3 data-number="9.4.5" class="anchored" data-anchor-id="ottimizzazione-dellaccesso-alla-memoria"><span class="header-section-number">9.4.5</span> Ottimizzazione dell’Accesso alla Memoria</h3>
<p>Dispositivi diversi possono avere gerarchie di memorie diverse. L’ottimizzazione per la gerarchia di memoria specifica nell’hardware specifico può portare a grandi miglioramenti delle prestazioni riducendo le costose operazioni di lettura e scrittura nella memoria. L’ottimizzazione del flusso di dati può essere ottenuta ottimizzando il riutilizzo dei dati all’interno di un singolo layer e tra più layer. Questa ottimizzazione del flusso di dati può essere adattata alla gerarchia di memoria specifica dell’hardware, il che può portare a maggiori vantaggi rispetto alle ottimizzazioni generali per diversi hardware.</p>
<section id="sfruttamento-dei-dati-sparsi" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sfruttamento-dei-dati-sparsi">Sfruttamento dei Dati Sparsi</h4>
<p>Il Pruning [potatura] è un approccio fondamentale per comprimere i modelli e renderli compatibili con dispositivi con risorse limitate. Ciò si traduce in modelli sparsi in cui molti pesi sono 0. Pertanto, sfruttare questa diradazione può portare a miglioramenti significativi nelle prestazioni. Sono stati creati degli strumenti per ottenere esattamente questo. RAMAN, è un acceleratore TinyML sparse progettato per l’inferenza su dispositivi edge. RAMAN sovrappone le attivazioni di input e output sullo stesso spazio di memoria, riducendo i requisiti di archiviazione fino al 50%. <span class="citation" data-cites="krishna2023raman">(<a href="#ref-krishna2023raman" role="doc-biblioref">Krishna et al. 2023</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-krishna2023raman" class="csl-entry" role="listitem">
Krishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh Dwivedi, André van Schaik, Mahesh Mehendale, e Chetan Singh Thakur. 2023. <span>«<span>RAMAN:</span> <span>A</span> Re-configurable and Sparse <span>TinyML</span> Accelerator for Inference on Edge»</span>. <a href="https://arxiv.org/abs/2306.06493">https://arxiv.org/abs/2306.06493</a>.
</div></div></section>
<section id="framework-di-ottimizzazione" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="framework-di-ottimizzazione">Framework di Ottimizzazione</h4>
<p>I framework di ottimizzazione sono stati introdotti per sfruttare le capacità specifiche dell’hardware per accelerare il software. Un esempio di tale framework è hls4ml: <a href="#fig-hls4ml-workflow" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-hls4ml-workflow</span></a> fornisce una panoramica del flusso di lavoro del framework. Questo flusso di lavoro di co-progettazione software-hardware open source aiuta a interpretare e tradurre algoritmi di machine learning per l’implementazione con tecnologie FPGA e ASIC. Funzionalità quali ottimizzazione di rete, nuove API Python, potatura consapevole della quantizzazione e flussi di lavoro FPGA end-to-end sono integrate nel framework hls4ml, sfruttando unità di elaborazione parallele, gerarchie di memoria e set di istruzioni specializzati per ottimizzare i modelli per hardware edge. Inoltre, hls4ml è in grado di tradurre algoritmi di apprendimento automatico direttamente nel firmware FPGA.</p>
<div id="fig-hls4ml-workflow" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hls4ml-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_hls4ml.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hls4ml-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.35: workflow del framework hls4ml. Fonte: <span class="citation" data-cites="fahim2021hls4ml">Fahim et al. (<a href="#ref-fahim2021hls4ml" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-fahim2021hls4ml" class="csl-entry" role="listitem">
Fahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, et al. 2021. <span>«hls4ml: <span>An</span> Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices»</span>. <a href="https://arxiv.org/abs/2103.05579">https://arxiv.org/abs/2103.05579</a>.
</div></div></figure>
</div>
<p>Un altro framework per FPGA che si concentra su un approccio olistico è CFU Playground <span class="citation" data-cites="prakash2022cfu">(<a href="#ref-prakash2022cfu" role="doc-biblioref">Prakash et al. 2023</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-prakash2022cfu" class="csl-entry" role="listitem">
Prakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. <span>«<span>CFU</span> Playground: <span>Full-stack</span> Open-Source Framework for Tiny Machine Learning <span>(TinyML)</span> Acceleration on <span>FPGAs</span>»</span>. In <em>2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</em>. Vol. abs/2201.01863. IEEE. <a href="https://doi.org/10.1109/ispass57527.2023.00024">https://doi.org/10.1109/ispass57527.2023.00024</a>.
</div></div></section>
<section id="hardware-costruito-attorno-al-software" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="hardware-costruito-attorno-al-software">Hardware Costruito Attorno al Software</h4>
<p>In un approccio contrastante, l’hardware può essere progettato su misura attorno ai requisiti software per ottimizzare le prestazioni per un’applicazione specifica. Questo paradigma crea hardware specializzato per adattarsi meglio alle specifiche del software, riducendo così il sovraccarico computazionale e migliorando l’efficienza operativa. Un esempio di questo approccio è un’applicazione di riconoscimento vocale di <span class="citation" data-cites="kwon2021hardwaresoftware">(<a href="#ref-kwon2021hardwaresoftware" role="doc-biblioref">Kwon e Park 2021</a>)</span>. Il documento propone una struttura in cui le operazioni di pre-elaborazione, tradizionalmente gestite dal software, sono assegnate ad un hardware progettato su misura. Questa tecnica è stata ottenuta introducendo la logica resistore-transistor in un modulo audio a circuito inter-integrato per il windowing e l’acquisizione di dati audio grezzi nell’applicazione di riconoscimento vocale. Di conseguenza, questa “delega” delle operazioni di pre-elaborazione ha portato a una riduzione del carico computazionale sul software, mostrando un’applicazione pratica della creazione di hardware attorno al software per migliorare l’efficienza e le prestazioni.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-fpga-preprocessing" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-fpga-preprocessing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_preprocessor.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fpga-preprocessing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.36: Delega dell’elaborazione dei dati a un FPGA. Fonte: <span class="citation" data-cites="kwon2021hardwaresoftware">Kwon e Park (<a href="#ref-kwon2021hardwaresoftware" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-kwon2021hardwaresoftware" class="csl-entry" role="listitem">
Kwon, Jisu, e Daejin Park. 2021. <span>«<span>Hardware/Software</span> Co-Design for <span>TinyML</span> Voice-Recognition Application on Resource Frugal Edge Devices»</span>. <em>Applied Sciences</em> 11 (22): 11073. <a href="https://doi.org/10.3390/app112211073">https://doi.org/10.3390/app112211073</a>.
</div></div></figure>
</div>
</section>
<section id="splitnet" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="splitnet">SplitNet</h4>
<p>Li SplitNet sono state introdotte nel contesto dei sistemi Head-Mounted. Distribuiscono il carico di lavoro delle Deep Neural Network (DNN) tra i sensori della telecamera e un aggregatore. Ciò è particolarmente interessante nel contesto di TinyML. Il framework SplitNet è un NAS split-aware per trovare l’architettura di rete neurale ottimale per ottenere una buona accuratezza, dividere il modello tra i sensori e l’aggregatore e ridurre al minimo la comunicazione tra i sensori e l’aggregatore.</p>
<p><a href="#fig-splitnet-performance" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-splitnet-performance</span></a> dimostra come le SplitNet (in rosso) ottengano una maggiore accuratezza per una latenza inferiore (in esecuzione su ImageNet) rispetto ad altri approcci, come l’esecuzione del DNN sul sensore (All-on-sensor; in verde) o sul cellulare (All-on-aggregator; in blu). La comunicazione minima è importante in TinyML dove la memoria è fortemente limitata, in questo modo i sensori conducono parte dell’elaborazione sui loro chip e poi inviano solo le informazioni necessarie all’aggregatore. Durante i test su ImageNet, SplitNets è stato in grado di ridurre la latenza di un ordine di grandezza sui dispositivi di visione artificiale montati sulla testa [occhiali o visori]. Ciò può essere utile quando il sensore ha il suo chip. <span class="citation" data-cites="dong2022splitnets">(<a href="#ref-dong2022splitnets" role="doc-biblioref">Dong et al. 2022</a>)</span></p>
<div class="no-row-height column-margin column-container"></div><div id="fig-splitnet-performance" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-splitnet-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_SplitNets.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-splitnet-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.37: Le SplitNet rispetto ad altri approcci. Fonte: <span class="citation" data-cites="dong2022splitnets">Dong et al. (<a href="#ref-dong2022splitnets" role="doc-biblioref">2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-dong2022splitnets" class="csl-entry" role="listitem">
Dong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T. Kung, e Ziyun Li. 2022. <span>«<span>SplitNets:</span> <span>Designing</span> Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems»</span>. In <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 12549–59. IEEE. <a href="https://doi.org/10.1109/cvpr52688.2022.01223">https://doi.org/10.1109/cvpr52688.2022.01223</a>.
</div></div></figure>
</div>
</section>
<section id="hardware-specifico-per-il-data-augmentation" class="level4">
<h4 class="anchored" data-anchor-id="hardware-specifico-per-il-data-augmentation">Hardware Specifico per il “Data Augmentation”</h4>
<p>Ogni dispositivo edge può possedere caratteristiche di sensore uniche, che portano a specifici pattern di rumore che possono influire sulle prestazioni del modello. Un esempio sono i dati audio, in cui sono prevalenti le variazioni derivanti dalla scelta del microfono. Applicazioni come le Keyword Spotting possono sperimentare miglioramenti sostanziali incorporando dati registrati da dispositivi simili a quelli destinati all’implementazione. La messa a punto dei modelli esistenti può essere impiegata per adattare i dati in modo preciso alle caratteristiche distintive del sensore.</p>
</section>
</section>
</section>
<section id="supporto-software-e-framework" class="level2 page-columns page-full" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="supporto-software-e-framework"><span class="header-section-number">9.5</span> Supporto Software e Framework</h2>
<p>Sebbene tutte le tecniche sopra menzionate come <a href="#sec-pruning">pruning</a>, <a href="#sec-quant">quantizzazione</a> e numeri efficienti siano ben note, rimarrebbero poco pratiche e inaccessibili senza un ampio supporto software. Ad esempio, la quantizzazione diretta di pesi e attivazioni in un modello richiederebbe la modifica manuale della definizione del modello e l’inserimento di operazioni di quantizzazione. Allo stesso modo, la potatura diretta dei pesi del modello richiede la manipolazione dei tensori dei pesi. Tali approcci noiosi diventano impraticabili su larga scala.</p>
<p>Senza l’ampia innovazione software nei framework, negli strumenti di ottimizzazione e nell’integrazione hardware, la maggior parte di queste tecniche rimarrebbe teorica o praticabile solo per gli esperti. Senza API del framework e automazione per semplificare l’applicazione di queste ottimizzazioni, non verrebbero adottate. Il supporto software le rende accessibili al pubblico e sblocca vantaggi concreti. Inoltre, problemi come la messa a punto degli iperparametri per la potatura, la gestione del compromesso tra dimensioni del modello e accuratezza e la garanzia della compatibilità con i dispositivi target pongono ostacoli che gli sviluppatori devono superare.</p>
<section id="api-native-di-ottimizzazione" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="api-native-di-ottimizzazione"><span class="header-section-number">9.5.1</span> API Native di Ottimizzazione</h3>
<p>I principali framework di machine learning come TensorFlow, PyTorch e MXNet forniscono librerie e API per consentire l’applicazione di tecniche comuni di ottimizzazione dei modelli senza richiedere implementazioni personalizzate. Ad esempio, TensorFlow offre il TensorFlow Model Optimization Toolkit che contiene moduli come:</p>
<ul>
<li><strong><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/quantize_model">Quantization</a></strong>: Applica un training che tiene conto della quantizzazione per convertire i modelli in virgola mobile in una precisione inferiore come int8 con una perdita di accuratezza minima. Gestisce la quantizzazione del peso e dell’attivazione.</li>
<li><strong><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras">Sparsity</a></strong>: Fornisce API di potatura per indurre la “sparsità” e rimuovere connessioni non necessarie in modelli come le reti neurali. Può potare pesi, livelli, ecc.</li>
<li><strong><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/clustering">Clustering</a></strong>: Supporta la compressione del modello raggruppando i pesi per tassi di compressione più elevati.</li>
</ul>
<p>Queste API consentono agli utenti di abilitare tecniche di ottimizzazione come la quantizzazione e la potatura senza modificare direttamente il codice del modello. È possibile configurare parametri come i tassi di “sparsità” del target, le larghezze di bit di quantizzazione, ecc. Allo stesso modo, PyTorch fornisce torch.quantization per convertire i modelli in rappresentazioni di precisione inferiore. TorchTensor e TorchModule formano le classi di base per il supporto della quantizzazione. Offre inoltre torch.nn.utils.prune per la potatura nativa dei modelli. MXNet offre layer gluon.contrib che aggiungono funzionalità di quantizzazione come l’arrotondamento a punto fisso e l’arrotondamento stocastico di pesi/attivazioni durante l’addestramento. Ciò consente di includere facilmente la quantizzazione nei modelli gluon.</p>
<p>Il vantaggio principale delle ottimizzazioni integrate è che gli utenti possono applicarle senza dover reimplementare tecniche complesse. Ciò rende i modelli ottimizzati accessibili a un’ampia gamma di professionisti. Garantisce inoltre che le best practice siano seguite basandosi sulla ricerca e sull’esperienza nell’implementazione dei metodi. Man mano che emergono nuove ottimizzazioni, i framework si sforzano di fornire supporto nativo e API ove possibile per abbassare ulteriormente la barriera verso un ML efficiente. La disponibilità di questi strumenti è fondamentale per un’adozione diffusa.</p>
</section>
<section id="strumenti-di-ottimizzazione-automatizzata" class="level3 page-columns page-full" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="strumenti-di-ottimizzazione-automatizzata"><span class="header-section-number">9.5.2</span> Strumenti di Ottimizzazione Automatizzata</h3>
<p>Gli strumenti di ottimizzazione automatizzati forniti dai framework possono analizzare i modelli e applicare automaticamente ottimizzazioni come quantizzazione, potatura e fusione degli operatori per rendere il processo più semplice e accessibile senza un’eccessiva messa a punto manuale. In effetti, questo si basa sulla sezione precedente. Ad esempio, TensorFlow fornisce il TensorFlow Model Optimization Toolkit che contiene moduli come:</p>
<ul>
<li><strong><a href="https://www.tensorflow.org/model_optimization/guide/quantization/training">QuantizationAwareTraining</a></strong>: Quantizza automaticamente pesi e attivazioni in un modello per ridurre la precisione come UINT8 o INT8 con una perdita di accuratezza minima. Inserisce nodi di quantizzazione falsi durante l’addestramento in modo che il modello possa imparare a essere compatibile con la quantizzazione.</li>
<li><strong><a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras">Pruning</a></strong>: Rimuove automaticamente le connessioni non necessarie in un modello in base all’analisi dell’importanza del peso. Può potare interi filtri in livelli convoluzionali o “attention head” [teste di attenzione] nei trasformatori. Gestisce il ri-addestramento iterativo per recuperare qualsiasi perdita di accuratezza.</li>
<li><strong><a href="https://www.tensorflow.org/guide/graph_optimization">GraphOptimizer</a></strong>: Applica ottimizzazioni grafiche come la fusione degli operatori per consolidare le operazioni e ridurre la latenza di esecuzione, in particolare per l’inferenza. In <a href="#fig-graph-optimizer" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-graph-optimizer</span></a>, si può vedere l’originale (Source Graph) a sinistra e come le sue operazioni vengono trasformate (consolidate) a destra. Notare come Block1 in Source Graph abbia 3 passaggi separati (Convolution, BiasAdd e Activation), che vengono poi consolidati insieme in Block1 su Optimized Graph.</li>
</ul>
<div id="fig-graph-optimizer" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-graph-optimizer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/source_opt.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graph-optimizer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.38: GraphOptimizer. Fonte: <span class="citation" data-cites="annette2020">Wess et al. (<a href="#ref-annette2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-annette2020" class="csl-entry" role="listitem">
Wess, Matthias, Matvey Ivanov, Christoph Unger, e Anvesh Nookala. 2020. <span>«<span>ANNETTE:</span> <span>Accurate</span> Neural Network Execution Time Estimation with Stacked Models»</span>. <em>IEEE</em>. <a href="https://doi.org/10.1109/ACCESS.2020.3047259">https://doi.org/10.1109/ACCESS.2020.3047259</a>.
</div></div></figure>
</div>
<p>Questi moduli automatizzati richiedono solo all’utente di fornire il modello originale in virgola mobile e di gestire la pipeline di ottimizzazione end-to-end, inclusa qualsiasi riqualificazione per ripristinare la precisione. Anche altri framework come PyTorch offrono un crescente supporto all’automazione, ad esempio tramite torch.quantization.quantize_dynamic. L’ottimizzazione automatizzata rende l’apprendimento automatico efficiente accessibile ai professionisti senza competenze di ottimizzazione.</p>
</section>
<section id="librerie-di-ottimizzazione-hardware" class="level3" data-number="9.5.3">
<h3 data-number="9.5.3" class="anchored" data-anchor-id="librerie-di-ottimizzazione-hardware"><span class="header-section-number">9.5.3</span> Librerie di Ottimizzazione Hardware</h3>
<p>Librerie hardware come TensorRT e TensorFlow XLA consentono di ottimizzare i modelli per l’hardware target tramite tecniche di cui abbiamo discusso in precedenza.</p>
<ul>
<li><p><strong>Quantizzazione:</strong> Ad esempio, TensorRT e TensorFlow Lite supportano entrambi la quantizzazione dei modelli durante la conversione nel loro formato. Ciò fornisce accelerazioni sui SoC mobili con supporto INT8/INT4.</p></li>
<li><p><strong>Ottimizzazione del Kernel:</strong> ad esempio, TensorRT esegue l’auto-tuning per ottimizzare i kernel CUDA in base all’architettura GPU per ogni layer nel grafo del modello. Ciò estrae la massima produttività.</p></li>
<li><p><strong>Fusione degli Operatori:</strong> TensorFlow XLA esegue una fusione aggressiva per creare un binario ottimizzato per le TPU. Sui dispositivi mobili, framework come NCNN supportano anche operatori fusi [unificati].</p></li>
<li><p><strong>Codice Specifico per l’Hardware:</strong> Le librerie vengono utilizzate per generare codice binario ottimizzato specializzato per l’hardware target. Per esempio, <a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html">TensorRT</a> usa librerie Nvidia CUDA/cuDNN che sono ottimizzate manualmente per ogni architettura GPU. Questa codifica specifica per hardware è fondamentale per le prestazioni. Sui dispositivi TinyML, questo può significare codice assembly ottimizzato per una CPU Cortex M4, ad esempio. I fornitori forniscono CMSIS-NN e altre librerie.</p></li>
<li><p><strong>Ottimizzazioni del Layout dei Dati:</strong> Possiamo sfruttare in modo efficiente la gerarchia della memoria di hardware come cache e registri tramite tecniche come riorganizzazione tensore/peso, tiling e riutilizzo. Ad esempio, TensorFlow XLA ottimizza i layout dei buffer per massimizzare l’utilizzo della TPU. Questo aiuta qualsiasi sistema con limiti di memoria.</p></li>
<li><p><strong>Ottimizzazione Basata sulla Profilazione:</strong> Possiamo usare strumenti di profilazione per identificare i colli di bottiglia. Ad esempio, regolare i livelli di fusione del kernel in base alla profilazione della latenza. Sui SoC mobili, fornitori come Qualcomm forniscono profiler in SNPE per trovare opportunità di ottimizzazione nelle CNN. Questo approccio basato sui dati è importante per le prestazioni.</p></li>
</ul>
<p>Integrando i modelli di framework con queste librerie hardware tramite pipeline di conversione ed esecuzione, gli sviluppatori di ML possono ottenere significativi incrementi di velocità e guadagni di efficienza da ottimizzazioni di basso livello su misura per l’hardware target. La stretta integrazione tra software e hardware è fondamentale per consentire un’implementazione performante delle applicazioni di ML, in particolare su dispositivi mobili e TinyML.</p>
</section>
<section id="visualizzazione-delle-ottimizzazioni" class="level3 page-columns page-full" data-number="9.5.4">
<h3 data-number="9.5.4" class="anchored" data-anchor-id="visualizzazione-delle-ottimizzazioni"><span class="header-section-number">9.5.4</span> Visualizzazione delle Ottimizzazioni</h3>
<p>L’implementazione di tecniche di ottimizzazione del modello senza visibilità degli effetti sul modello può essere impegnativa. Strumenti dedicati o strumenti di visualizzazione possono fornire informazioni critiche e utili sulle modifiche del modello e aiutano a tracciare il processo di ottimizzazione. Consideriamo le ottimizzazioni che abbiamo considerato in precedenza, come la potatura per la “sparsity” [diradazione] e la quantizzazione.</p>
<section id="sparsità" class="level5">
<h5 class="anchored" data-anchor-id="sparsità">Sparsità</h5>
<p>Ad esempio, si considerino le ottimizzazioni di sparsity. Gli strumenti di visualizzazione di sparsity possono fornire informazioni critiche sui modelli potati, mappando esattamente quali pesi sono stati rimossi. Ad esempio, le mappe di calore di sparsity possono utilizzare gradienti di colore per indicare la percentuale di pesi potati in ogni layer di una rete neurale. I layer con percentuali di potatura più elevate appaiono più scuri (cfr. <a href="#fig-sprase-heat-map" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-sprase-heat-map</span></a>). Questo identifica quali layer sono stati semplificati di più tramite potatura (<a href="https://www.numenta.com/blog/2020/10/30/case-for-sparsity-in-neural-networks-part-2-dynamic-sparsity/">Souza 2020</a>).</p>
<div id="fig-sprase-heat-map" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sprase-heat-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://www.numenta.com/wp-content/uploads/2020/10/Picture1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sprase-heat-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.39: Mappa “termica” della rete sparsa. Fonte: <a href="https://www.numenta.com/blog/2020/10/30/case-for-sparsity-in-neural-networks-part-2-dynamic-sparsity/">Numenta</a>.
</figcaption>
</figure>
</div>
<p>I grafici di tendenza possono anche tracciare la scarsità nei successivi round di potatura: possono mostrare una rapida potatura iniziale seguita da incrementi più graduali. Il tracciamento della diradazione globale corrente insieme a statistiche come la diradazione media, minima e massima per ogli layer in tabelle o grafici fornisce una panoramica della composizione del modello. Per una rete convoluzionale di esempio, questi strumenti potrebbero rivelare che il primo layer di convoluzione viene potato del 20% mentre quello di classificazione finale viene potato del 70% data la sua ridondanza. La diradazione del modello globale può aumentare dal 10% dopo la potatura iniziale al 40% dopo cinque round.</p>
<p>Rendendo i dati di diradazione visivamente accessibili, i professionisti possono comprendere meglio esattamente come il loro modello viene ottimizzato e quali aree vengono interessate. La visibilità consente loro di mettere a punto e controllare il processo di potatura per una determinata architettura.</p>
<p>La visualizzazione della diradazione trasforma la potatura in una tecnica trasparente anziché in un’operazione “black-box”.</p>
</section>
<section id="quantizzazione-1" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="quantizzazione-1">Quantizzazione</h5>
<p>La conversione di modelli in precisioni numeriche inferiori tramite quantizzazione introduce errori che possono influire sulla precisione del modello se non vengono monitorati e affrontati correttamente. La visualizzazione delle distribuzioni degli errori di quantizzazione fornisce informazioni preziose sugli effetti dei numeri di precisione ridotti applicati a diverse parti di un modello. Per questo, è possibile generare istogrammi degli errori di quantizzazione per pesi e attivazioni. Questi istogrammi possono rivelare la forma della distribuzione degli errori, se assomigliano a una distribuzione gaussiana o contengono valori anomali e picchi significativi. <a href="#fig-quantization-error" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-quantization-error</span></a> mostra le distribuzioni di diversi metodi di quantizzazione. Valori anomali elevati possono indicare problemi con particolari layer che gestiscono la quantizzazione. Il confronto degli istogrammi tra layer evidenzia eventuali aree problematiche che si distinguono con errori anormalmente elevati.</p>
<div id="fig-quantization-error" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_quant_hist.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.40: Errori di Quantizzazione. Fonte: <span class="citation" data-cites="kuzmin2022fp8">Kuzmin et al. (<a href="#ref-kuzmin2022fp8" role="doc-biblioref">2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-kuzmin2022fp8" class="csl-entry" role="listitem">
Kuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, e Tijmen Blankevoort. 2022. <span>«<span>FP8</span> Quantization: <span>The</span> Power of the Exponent»</span>. <a href="https://arxiv.org/abs/2208.09225">https://arxiv.org/abs/2208.09225</a>.
</div></div></figure>
</div>
<p>Le visualizzazioni di attivazione sono importanti anche per rilevare problemi di overflow. Con la mappatura a colori delle attivazioni prima e dopo la quantizzazione, tutti i valori spinti al di fuori degli intervalli previsti diventano visibili. Ciò rivela problemi di saturazione e troncamento che potrebbero alterare le informazioni che fluiscono attraverso il modello. Il rilevamento di questi errori consente di ricalibrare le attivazioni per evitare la perdita di informazioni (<a href="https://medium.com/exemplifyml-ai/visualizing-neural-network-activation-a27caa451ff">Mandal 2022</a>). <a href="#fig-color-mapping" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-color-mapping</span></a> è una mappatura a colori dei kernel convoluzionali AlexNet.</p>
<div id="fig-color-mapping" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-color-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://compsci697l.github.io/assets/cnnvis/filt1.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-color-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.41: Mappatura a colori delle attivazioni. Fonte: <span class="citation" data-cites="alexnet2012">Krizhevsky, Sutskever, e Hinton (<a href="#ref-alexnet2012" role="doc-biblioref">2017</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-alexnet2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2017. <span>«<span>ImageNet</span> classification with deep convolutional neural networks»</span>. A cura di F. Pereira, C. J. Burges, L. Bottou, e K. Q. Weinberger. <em>Commun. ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div></div></figure>
</div>
<p>Altre tecniche, come il tracciamento dell’errore di quantizzazione quadratico medio complessivo a ogni passaggio del processo di addestramento consapevole della quantizzazione, identificano fluttuazioni e divergenze. Picchi improvvisi nel grafico di tracciamento possono indicare punti in cui la quantizzazione sta interrompendo l’addestramento del modello. Il monitoraggio di questa metrica crea intuizione sul comportamento del modello in fase di quantizzazione. Insieme, queste tecniche trasformano la quantizzazione in un processo trasparente. Le intuizioni empiriche consentono ai professionisti di valutare correttamente gli effetti della quantizzazione. Individuano le aree dell’architettura del modello o del processo di training da ricalibrare in base ai problemi di quantizzazione osservati. Ciò aiuta a ottenere modelli quantizzati numericamente stabili e accurati.</p>
<p>Fornire questi dati consente ai professionisti di valutare correttamente l’impatto della quantizzazione e identificare potenziali aree problematiche del modello da ricalibrare o riprogettare per renderlo più adatto alla quantizzazione. Questa analisi empirica sviluppa l’intuizione sul raggiungimento di una quantizzazione ottimale.</p>
<p>Gli strumenti di visualizzazione possono fornire approfondimenti che aiutano i professionisti a comprendere meglio gli effetti delle ottimizzazioni sui loro modelli. La visibilità consente di correggere i problemi in anticipo prima che l’accuratezza o le prestazioni siano influenzate in modo significativo. Aiuta anche ad applicare le ottimizzazioni in modo più efficace per modelli specifici. Queste analisi di ottimizzazione aiutano a sviluppare l’intuizione quando si trasferiscono i modelli a rappresentazioni più efficienti.</p>
</section>
</section>
<section id="conversione-e-distribuzione-del-modello" class="level3" data-number="9.5.5">
<h3 data-number="9.5.5" class="anchored" data-anchor-id="conversione-e-distribuzione-del-modello"><span class="header-section-number">9.5.5</span> Conversione e Distribuzione del Modello</h3>
<p>Una volta che i modelli sono stati ottimizzati con successo in framework come TensorFlow e PyTorch, sono necessarie piattaforme specializzate di conversione e deployment [distribuzione] del modello per colmare il divario con l’esecuzione sui dispositivi target.</p>
<p>TensorFlow Lite - La piattaforma di TensorFlow per convertire i modelli in un formato leggero ottimizzato per dispositivi mobili, embedded ed edge. Supporta ottimizzazioni come quantizzazione, fusione del kernel e rimozione di operazioni inutilizzate. I modelli possono essere eseguiti utilizzando kernel TensorFlow Lite ottimizzati sull’hardware del dispositivo. Fondamentale per la distribuzione mobile e TinyML.</p>
<p>ONNX Runtime - Esegue la conversione e l’inferenza per i modelli nel formato “open ONNX”. Fornisce kernel ottimizzati, supporta acceleratori hardware come GPU e distribuzione multipiattaforma dal cloud all’edge. Consente la distribuzione indipendente dal framework. <a href="#fig-interop" class="quarto-xref">Figura&nbsp;<span class="quarto-unresolved-ref">fig-interop</span></a> è una mappa di interoperabilità ONNX, inclusi i principali framework più diffusi.</p>
<div id="fig-interop" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://miro.medium.com/v2/resize:fit:1400/1*3N6uPaLNEYDjtWBW1vdNoQ.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.42: Interoperabilità di ONNX. Fonte: <a href="https://towardsdatascience.com/onnx-preventing-framework-lock-in-9a798fb34c92">TowardsDataScience</a>.
</figcaption>
</figure>
</div>
<p>PyTorch Mobile - Consente l’esecuzione dei modelli PyTorch su iOS e Android convertendoli in rappresentazioni ottimizzate per dispositivi mobili. Fornisce implementazioni mobili efficienti di operazioni come convoluzione e funzioni speciali ottimizzate per hardware mobile.</p>
<p>Queste piattaforme si integrano con driver hardware, sistemi operativi e librerie di acceleratori sui dispositivi per eseguire modelli in modo efficiente utilizzando l’ottimizzazione hardware. Inoltre, delegano le operazioni ad acceleratori ML dedicati, ove presenti. La disponibilità di queste piattaforme di distribuzione collaudate e robuste colma il divario tra l’ottimizzazione dei modelli nei framework e la distribuzione effettiva su miliardi di dispositivi. Consentono agli utenti di concentrarsi sullo sviluppo del modello anziché sulla creazione di runtime mobili personalizzati. L’innovazione continua per supportare nuovi hardware e ottimizzazioni in queste piattaforme è fondamentale per le ottimizzazioni di ML diffuse.</p>
<p>Fornendo queste pipeline di distribuzione ottimizzate, l’intero flusso di lavoro, dal training al deployment [distribuzione] del dispositivo, può sfruttare le ottimizzazioni del modello per fornire applicazioni ML performanti. Questa infrastruttura software end-to-end ha contribuito a guidare l’adozione di ML sul dispositivo.</p>
</section>
</section>
<section id="conclusione" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">9.6</span> Conclusione</h2>
<p>In questo capitolo abbiamo discusso l’ottimizzazione del modello nell’ambito software-hardware. Ci siamo immersi in una rappresentazione efficiente del modello, dove abbiamo trattato le sfumature della potatura strutturata e non-strutturata e altre tecniche per la compressione del modello come la distillazione della conoscenza e la decomposizione di matrice e tensore. Ci siamo anche immersi brevemente nella progettazione del modello specifico per l’edge a livello di parametri e architettura del modello, esplorando argomenti come modelli specifici per l’edge e NAS basati sull’hardware.</p>
<p>Abbiamo quindi esplorato rappresentazioni numeriche efficienti, dove abbiamo trattato le basi della matematica, codifiche numeriche e archiviazione, vantaggi della matematica efficiente e le sfumature della rappresentazione numerica con utilizzo della memoria, complessità computazionale, compatibilità hardware e scenari di compromesso. Abbiamo concluso concentrandoci su un elemento fondamentale della matematica efficiente: la quantizzazione, dove abbiamo esaminato la sua storia, calibrazione, tecniche e interazione con la potatura.</p>
<p>Infine, abbiamo esaminato come possiamo apportare ottimizzazioni specifiche per l’hardware che abbiamo. Abbiamo esplorato come possiamo trovare architetture modello su misura per l’hardware, apportare ottimizzazioni nel kernel per gestire meglio il modello e framework creati per sfruttare al meglio l’hardware. Abbiamo anche esaminato come possiamo fare il contrario e creare hardware attorno al nostro software specifico e abbiamo parlato di come suddividere le reti per l’esecuzione su più processori disponibili sul dispositivo edge.</p>
<p>Comprendendo il quadro completo dei gradi di libertà all’interno dell’ottimizzazione del modello sia lontano che vicino all’hardware e i compromessi da considerare quando si implementano questi metodi, i professionisti possono sviluppare una pipeline più ponderata per comprimere i loro carichi di lavoro sui dispositivi edge.</p>
</section>
<section id="sec-model-optimizations-resource" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="sec-model-optimizations-resource"><span class="header-section-number">9.7</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare sia gli studenti che gli insegnanti nel loro percorso di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e aggiungeremo nuovi esercizi nel prossimo futuro.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p>Quantizzazione:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1GOlLUMkd8OTNvrNj7lDSIGricE-569Nk/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Quantization: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/18oLTdwa-dZxbBNpvHzZVyMS8bUbed4ao/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Quantization: Part 2.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1eSOyAOu8Vg_VfIHZ9gWRVjWnmFTOcZ4FavaNMc4reHQ/edit">Post-Training Quantization (PTQ).</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1qvoKLjKadK1abqUuuCCy9gaTynMZivDKLbV2Hjftri8/edit?usp=drive_link">Quantization-Aware Training (QAT).</a></p></li>
</ul></li>
<li><p>Pruning:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1KX_I71smbztdqycPXBDAYjShinrTtQeF/edit#slide=id.p1">Pruning: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1kZGDhnkeRcAw1pz3smO837ftotXQEiO7/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Pruning: Part 2.</a></p></li>
</ul></li>
<li><p><a href="https://docs.google.com/presentation/d/1SXjA3mCSwKmdouuWoxSk7r-Yjd67RG7i/edit#slide=id.g202a77b5f4f_0_110">Knowledge Distillation.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/14K9QFUjiba1NvwG0zobsJdgEklomuM_xeaCP7-5dmY8/edit?usp=drive_link">Clustering.</a></p></li>
<li><p>Neural Architecture Search (NAS):</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1aVGjhj1Q-_JEFHr6CYzPeuMOCiDivzhZCBtg1xV14QM/edit#slide=id.g202a67d8ddf_0_0">NAS overview.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1V-ZD6c8KPrFBrrw8xkAQfkqUu4u53zkX/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">NAS: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1VUf9zyGP9yascD87VSit58S494EPnd8D/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">NAS: Part 2.</a></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><a href="#vid-quant" class="quarto-xref">Video&nbsp;<span class="quarto-unresolved-ref">vid-quant</span></a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-p" class="quarto-xref">Esercizio&nbsp;<span class="quarto-unresolved-ref">exr-p</span></a></p></li>
<li><p><a href="#exr-mc" class="quarto-xref">Esercizio&nbsp;<span class="quarto-unresolved-ref">exr-mc</span></a></p></li>
<li><p><a href="#exr-md" class="quarto-xref">Esercizio&nbsp;<span class="quarto-unresolved-ref">exr-md</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laboratori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Oltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/efficient_ai/efficient_ai.it.html" class="pagination-link" aria-label="IA Efficiente">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="pagination-link" aria-label="Accelerazione IA">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/optimizations/optimizations.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/optimizations/optimizations.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro è stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>