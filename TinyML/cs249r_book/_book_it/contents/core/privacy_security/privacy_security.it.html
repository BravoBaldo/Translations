<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; Sicurezza e Privacy ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/responsible_ai/responsible_ai.it.html" rel="next">
<link href="../../../contents/core/ops/ops.it.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/privacy_security/privacy_security.it.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2b8d2ba3a08f8b4ab16660e3d0aa1206" class="alert alert-primary hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>‚≠ê [18 Ott] <b>Abbiamo raggiunto 1.000 stelle GitHub</b> üéâ Grazie a voi, Arduino e SEEED hanno donato kit hardware di IA per <a href="https://tinyml.seas.harvard.edu/4D/pastEvents">per i workshop TinyML</a> nei paesi in via di sviluppo <br> üéì [15 Nov] La <a href="https://www.edgeaifoundation.org/">EDGE AI Foundation</a> <strong>equipara i fondi per borse di studio accademiche</strong> per ogni nuovo GitHub ‚≠ê (fino a 10.000 stelle). <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per supportare!</a> üôè <br> üöÄ <b>La nostra missione. 1 ‚≠ê = 1 üë©‚Äçüéì Studente</b>. Ogni stella racconta una storia: studenti che acquisiscono conoscenze e sostenitori che guidano la missione. Insieme, stiamo facendo la differenza.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/about/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/ai/socratiq.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABORATORI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/part_LABS.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">LABORATORI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/part_nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">part_nicla_vision.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/part_xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">part_xiao_esp32s3.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/part_raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">part_raspi.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/part_shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">part_shared.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#panoramica" id="toc-panoramica" class="nav-link active" data-scroll-target="#panoramica"><span class="header-section-number">14.1</span> Panoramica</a></li>
  <li><a href="#terminologia" id="toc-terminologia" class="nav-link" data-scroll-target="#terminologia"><span class="header-section-number">14.2</span> Terminologia</a></li>
  <li><a href="#precedenti-storici" id="toc-precedenti-storici" class="nav-link" data-scroll-target="#precedenti-storici"><span class="header-section-number">14.3</span> Precedenti Storici</a>
  <ul>
  <li><a href="#stuxnet" id="toc-stuxnet" class="nav-link" data-scroll-target="#stuxnet"><span class="header-section-number">14.3.1</span> Stuxnet</a></li>
  <li><a href="#hack-della-jeep-cherokee" id="toc-hack-della-jeep-cherokee" class="nav-link" data-scroll-target="#hack-della-jeep-cherokee"><span class="header-section-number">14.3.2</span> Hack della Jeep Cherokee</a></li>
  <li><a href="#botnet-mirai" id="toc-botnet-mirai" class="nav-link" data-scroll-target="#botnet-mirai"><span class="header-section-number">14.3.3</span> Botnet Mirai</a></li>
  <li><a href="#implicazioni" id="toc-implicazioni" class="nav-link" data-scroll-target="#implicazioni"><span class="header-section-number">14.3.4</span> Implicazioni</a></li>
  </ul></li>
  <li><a href="#minacce-alla-sicurezza-per-i-modelli-ml" id="toc-minacce-alla-sicurezza-per-i-modelli-ml" class="nav-link" data-scroll-target="#minacce-alla-sicurezza-per-i-modelli-ml"><span class="header-section-number">14.4</span> Minacce alla Sicurezza per i Modelli ML</a>
  <ul>
  <li><a href="#furto-di-modelli" id="toc-furto-di-modelli" class="nav-link" data-scroll-target="#furto-di-modelli"><span class="header-section-number">14.4.1</span> Furto di Modelli</a>
  <ul class="collapse">
  <li><a href="#caso-di-studio-il-furto-di-propriet√†-intellettuale-di-tesla" id="toc-caso-di-studio-il-furto-di-propriet√†-intellettuale-di-tesla" class="nav-link" data-scroll-target="#caso-di-studio-il-furto-di-propriet√†-intellettuale-di-tesla">Caso di Studio: Il furto di Propriet√† Intellettuale di Tesla</a></li>
  </ul></li>
  <li><a href="#avvelenamento-dei-dati" id="toc-avvelenamento-dei-dati" class="nav-link" data-scroll-target="#avvelenamento-dei-dati"><span class="header-section-number">14.4.2</span> Avvelenamento dei Dati</a>
  <ul class="collapse">
  <li><a href="#caso-di-studio-avvelenamento-dei-sistemi-di-moderazione-dei-contenuti" id="toc-caso-di-studio-avvelenamento-dei-sistemi-di-moderazione-dei-contenuti" class="nav-link" data-scroll-target="#caso-di-studio-avvelenamento-dei-sistemi-di-moderazione-dei-contenuti">Caso di Studio: Avvelenamento dei Sistemi di Moderazione dei Contenuti</a></li>
  <li><a href="#caso-di-studio-proteggere-larte-attraverso-lavvelenamento-dei-dati" id="toc-caso-di-studio-proteggere-larte-attraverso-lavvelenamento-dei-dati" class="nav-link" data-scroll-target="#caso-di-studio-proteggere-larte-attraverso-lavvelenamento-dei-dati">Caso di Studio: Proteggere l‚ÄôArte Attraverso l‚ÄôAvvelenamento dei Dati</a></li>
  </ul></li>
  <li><a href="#attacchi-avversari" id="toc-attacchi-avversari" class="nav-link" data-scroll-target="#attacchi-avversari"><span class="header-section-number">14.4.3</span> Attacchi Avversari</a>
  <ul class="collapse">
  <li><a href="#caso-di-studio-inganno-dei-modelli-di-rilevamento-dei-segnali-stradali" id="toc-caso-di-studio-inganno-dei-modelli-di-rilevamento-dei-segnali-stradali" class="nav-link" data-scroll-target="#caso-di-studio-inganno-dei-modelli-di-rilevamento-dei-segnali-stradali">Caso di Studio: Inganno dei Modelli di Rilevamento dei Segnali Stradali</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#minacce-alla-sicurezza-per-lhardware-ml" id="toc-minacce-alla-sicurezza-per-lhardware-ml" class="nav-link" data-scroll-target="#minacce-alla-sicurezza-per-lhardware-ml"><span class="header-section-number">14.5</span> Minacce alla Sicurezza Per l‚ÄôHardware ML</a>
  <ul>
  <li><a href="#bug-hardware" id="toc-bug-hardware" class="nav-link" data-scroll-target="#bug-hardware"><span class="header-section-number">14.5.1</span> Bug Hardware</a></li>
  <li><a href="#attacchi-fisici" id="toc-attacchi-fisici" class="nav-link" data-scroll-target="#attacchi-fisici"><span class="header-section-number">14.5.2</span> Attacchi Fisici</a></li>
  <li><a href="#attacchi-di-fault-injection" id="toc-attacchi-di-fault-injection" class="nav-link" data-scroll-target="#attacchi-di-fault-injection"><span class="header-section-number">14.5.3</span> Attacchi di Fault-injection</a></li>
  <li><a href="#attacchi-a-canale-laterale" id="toc-attacchi-a-canale-laterale" class="nav-link" data-scroll-target="#attacchi-a-canale-laterale"><span class="header-section-number">14.5.4</span> Attacchi a canale laterale</a></li>
  <li><a href="#interfacce-con-perdite" id="toc-interfacce-con-perdite" class="nav-link" data-scroll-target="#interfacce-con-perdite"><span class="header-section-number">14.5.5</span> Interfacce con Perdite</a></li>
  <li><a href="#hardware-contraffatto" id="toc-hardware-contraffatto" class="nav-link" data-scroll-target="#hardware-contraffatto"><span class="header-section-number">14.5.6</span> Hardware Contraffatto</a></li>
  <li><a href="#rischi-della-catena-di-fornitura" id="toc-rischi-della-catena-di-fornitura" class="nav-link" data-scroll-target="#rischi-della-catena-di-fornitura"><span class="header-section-number">14.5.7</span> Rischi della Catena di Fornitura</a></li>
  <li><a href="#caso-di-studio-una-chiamata-di-risveglio-per-la-sicurezza-hardware" id="toc-caso-di-studio-una-chiamata-di-risveglio-per-la-sicurezza-hardware" class="nav-link" data-scroll-target="#caso-di-studio-una-chiamata-di-risveglio-per-la-sicurezza-hardware"><span class="header-section-number">14.5.8</span> Caso di Studio: Una Chiamata di Risveglio per la Sicurezza Hardware</a></li>
  </ul></li>
  <li><a href="#sicurezza-hardware-del-ml-embedded" id="toc-sicurezza-hardware-del-ml-embedded" class="nav-link" data-scroll-target="#sicurezza-hardware-del-ml-embedded"><span class="header-section-number">14.6</span> Sicurezza Hardware del ML Embedded</a>
  <ul>
  <li><a href="#trusted-execution-environments" id="toc-trusted-execution-environments" class="nav-link" data-scroll-target="#trusted-execution-environments"><span class="header-section-number">14.6.1</span> Trusted Execution Environments</a>
  <ul class="collapse">
  <li><a href="#informazioni-su-tee" id="toc-informazioni-su-tee" class="nav-link" data-scroll-target="#informazioni-su-tee">Informazioni su TEE</a></li>
  <li><a href="#vantaggi" id="toc-vantaggi" class="nav-link" data-scroll-target="#vantaggi">Vantaggi</a></li>
  <li><a href="#meccanica" id="toc-meccanica" class="nav-link" data-scroll-target="#meccanica">Meccanica</a></li>
  <li><a href="#compromessi" id="toc-compromessi" class="nav-link" data-scroll-target="#compromessi">Compromessi</a></li>
  </ul></li>
  <li><a href="#avvio-sicuro" id="toc-avvio-sicuro" class="nav-link" data-scroll-target="#avvio-sicuro"><span class="header-section-number">14.6.2</span> Avvio Sicuro</a>
  <ul class="collapse">
  <li><a href="#informazioni" id="toc-informazioni" class="nav-link" data-scroll-target="#informazioni">Informazioni</a></li>
  <li><a href="#vantaggi-1" id="toc-vantaggi-1" class="nav-link" data-scroll-target="#vantaggi-1">Vantaggi</a></li>
  <li><a href="#meccanica-1" id="toc-meccanica-1" class="nav-link" data-scroll-target="#meccanica-1">Meccanica</a></li>
  <li><a href="#caso-di-studio-face-id-di-apple" id="toc-caso-di-studio-face-id-di-apple" class="nav-link" data-scroll-target="#caso-di-studio-face-id-di-apple">Caso di Studio: Face ID di Apple</a></li>
  <li><a href="#sfide" id="toc-sfide" class="nav-link" data-scroll-target="#sfide">Sfide</a></li>
  </ul></li>
  <li><a href="#moduli-di-sicurezza-hardware" id="toc-moduli-di-sicurezza-hardware" class="nav-link" data-scroll-target="#moduli-di-sicurezza-hardware"><span class="header-section-number">14.6.3</span> Moduli di Sicurezza Hardware</a>
  <ul class="collapse">
  <li><a href="#hsm" id="toc-hsm" class="nav-link" data-scroll-target="#hsm">HSM</a></li>
  <li><a href="#vantaggi-2" id="toc-vantaggi-2" class="nav-link" data-scroll-target="#vantaggi-2">Vantaggi</a></li>
  <li><a href="#compromessi-1" id="toc-compromessi-1" class="nav-link" data-scroll-target="#compromessi-1">Compromessi</a></li>
  </ul></li>
  <li><a href="#physical-unclonable-functions-puf" id="toc-physical-unclonable-functions-puf" class="nav-link" data-scroll-target="#physical-unclonable-functions-puf"><span class="header-section-number">14.6.4</span> Physical Unclonable Functions (PUF)</a>
  <ul class="collapse">
  <li><a href="#informazioni-1" id="toc-informazioni-1" class="nav-link" data-scroll-target="#informazioni-1">Informazioni</a></li>
  <li><a href="#vantaggi-3" id="toc-vantaggi-3" class="nav-link" data-scroll-target="#vantaggi-3">Vantaggi</a></li>
  <li><a href="#utilit√†" id="toc-utilit√†" class="nav-link" data-scroll-target="#utilit√†">Utilit√†</a></li>
  <li><a href="#meccanica-2" id="toc-meccanica-2" class="nav-link" data-scroll-target="#meccanica-2">Meccanica</a></li>
  <li><a href="#sfide-1" id="toc-sfide-1" class="nav-link" data-scroll-target="#sfide-1">Sfide</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#problemi-di-privacy-nella-gestione-dei-dati" id="toc-problemi-di-privacy-nella-gestione-dei-dati" class="nav-link" data-scroll-target="#problemi-di-privacy-nella-gestione-dei-dati"><span class="header-section-number">14.7</span> Problemi di Privacy nella Gestione dei Dati</a>
  <ul>
  <li><a href="#tipi-di-dati-sensibili" id="toc-tipi-di-dati-sensibili" class="nav-link" data-scroll-target="#tipi-di-dati-sensibili"><span class="header-section-number">14.7.1</span> Tipi di Dati Sensibili</a></li>
  <li><a href="#regolamenti-applicabili" id="toc-regolamenti-applicabili" class="nav-link" data-scroll-target="#regolamenti-applicabili"><span class="header-section-number">14.7.2</span> Regolamenti Applicabili</a></li>
  <li><a href="#de-identificazione" id="toc-de-identificazione" class="nav-link" data-scroll-target="#de-identificazione"><span class="header-section-number">14.7.3</span> De-identificazione</a>
  <ul class="collapse">
  <li><a href="#metodi-safe-harbor" id="toc-metodi-safe-harbor" class="nav-link" data-scroll-target="#metodi-safe-harbor">Metodi Safe Harbor</a></li>
  <li><a href="#metodi-di-determinazione-degli-esperti" id="toc-metodi-di-determinazione-degli-esperti" class="nav-link" data-scroll-target="#metodi-di-determinazione-degli-esperti">Metodi di Determinazione degli Esperti</a></li>
  </ul></li>
  <li><a href="#riduzione-al-minimo-dei-dati" id="toc-riduzione-al-minimo-dei-dati" class="nav-link" data-scroll-target="#riduzione-al-minimo-dei-dati"><span class="header-section-number">14.7.4</span> Riduzione al Minimo dei Dati</a>
  <ul class="collapse">
  <li><a href="#caso-di-studio-minimizzazione-dei-dati-basata-sulle-prestazioni" id="toc-caso-di-studio-minimizzazione-dei-dati-basata-sulle-prestazioni" class="nav-link" data-scroll-target="#caso-di-studio-minimizzazione-dei-dati-basata-sulle-prestazioni">Caso di Studio: Minimizzazione dei Dati Basata sulle Prestazioni</a></li>
  </ul></li>
  <li><a href="#consenso-e-trasparenza" id="toc-consenso-e-trasparenza" class="nav-link" data-scroll-target="#consenso-e-trasparenza"><span class="header-section-number">14.7.5</span> Consenso e Trasparenza</a></li>
  <li><a href="#problemi-di-privacy-nellapprendimento-automatico" id="toc-problemi-di-privacy-nellapprendimento-automatico" class="nav-link" data-scroll-target="#problemi-di-privacy-nellapprendimento-automatico"><span class="header-section-number">14.7.6</span> Problemi di Privacy nell‚ÄôApprendimento Automatico</a>
  <ul class="collapse">
  <li><a href="#ia-generativa" id="toc-ia-generativa" class="nav-link" data-scroll-target="#ia-generativa">IA Generativa</a></li>
  <li><a href="#cancellazione-dei-dati" id="toc-cancellazione-dei-dati" class="nav-link" data-scroll-target="#cancellazione-dei-dati">Cancellazione dei Dati</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#tecniche-ml-per-la-tutela-della-privacy" id="toc-tecniche-ml-per-la-tutela-della-privacy" class="nav-link" data-scroll-target="#tecniche-ml-per-la-tutela-della-privacy"><span class="header-section-number">14.8</span> Tecniche ML per la Tutela della Privacy</a>
  <ul>
  <li><a href="#privacy-differenziale" id="toc-privacy-differenziale" class="nav-link" data-scroll-target="#privacy-differenziale"><span class="header-section-number">14.8.1</span> Privacy Differenziale</a>
  <ul class="collapse">
  <li><a href="#idea-centrale" id="toc-idea-centrale" class="nav-link" data-scroll-target="#idea-centrale">Idea Centrale</a></li>
  <li><a href="#compromessi-2" id="toc-compromessi-2" class="nav-link" data-scroll-target="#compromessi-2">Compromessi</a></li>
  <li><a href="#caso-di-studio-privacy-differenziale-in-apple" id="toc-caso-di-studio-privacy-differenziale-in-apple" class="nav-link" data-scroll-target="#caso-di-studio-privacy-differenziale-in-apple">Caso di Studio: Privacy Differenziale in Apple</a></li>
  </ul></li>
  <li><a href="#il-federated-learning" id="toc-il-federated-learning" class="nav-link" data-scroll-target="#il-federated-learning"><span class="header-section-number">14.8.2</span> Il Federated Learning</a>
  <ul class="collapse">
  <li><a href="#idea-centrale-1" id="toc-idea-centrale-1" class="nav-link" data-scroll-target="#idea-centrale-1">Idea Centrale</a></li>
  <li><a href="#compromessi-3" id="toc-compromessi-3" class="nav-link" data-scroll-target="#compromessi-3">Compromessi</a></li>
  <li><a href="#caso-di-studio-addestramento-federato-per-set-di-dati-sanitari-collaborativi" id="toc-caso-di-studio-addestramento-federato-per-set-di-dati-sanitari-collaborativi" class="nav-link" data-scroll-target="#caso-di-studio-addestramento-federato-per-set-di-dati-sanitari-collaborativi">Caso di Studio: Addestramento Federato per Set di Dati Sanitari Collaborativi</a></li>
  </ul></li>
  <li><a href="#machine-unlearning" id="toc-machine-unlearning" class="nav-link" data-scroll-target="#machine-unlearning"><span class="header-section-number">14.8.3</span> Machine Unlearning</a>
  <ul class="collapse">
  <li><a href="#idea-centrale-2" id="toc-idea-centrale-2" class="nav-link" data-scroll-target="#idea-centrale-2">Idea Centrale</a></li>
  <li><a href="#caso-di-studio-lesperimento-di-harry-potter" id="toc-caso-di-studio-lesperimento-di-harry-potter" class="nav-link" data-scroll-target="#caso-di-studio-lesperimento-di-harry-potter">Caso di Studio: L‚ÄôEsperimento di Harry Potter</a></li>
  <li><a href="#altri-utilizzi" id="toc-altri-utilizzi" class="nav-link" data-scroll-target="#altri-utilizzi">Altri Utilizzi</a></li>
  </ul></li>
  <li><a href="#crittografia-omomorfica" id="toc-crittografia-omomorfica" class="nav-link" data-scroll-target="#crittografia-omomorfica"><span class="header-section-number">14.8.4</span> Crittografia Omomorfica</a>
  <ul class="collapse">
  <li><a href="#idea-centrale-3" id="toc-idea-centrale-3" class="nav-link" data-scroll-target="#idea-centrale-3">Idea Centrale</a></li>
  <li><a href="#vantaggi-4" id="toc-vantaggi-4" class="nav-link" data-scroll-target="#vantaggi-4">Vantaggi</a></li>
  <li><a href="#meccanica-3" id="toc-meccanica-3" class="nav-link" data-scroll-target="#meccanica-3">Meccanica</a></li>
  <li><a href="#compromessi-4" id="toc-compromessi-4" class="nav-link" data-scroll-target="#compromessi-4">Compromessi</a></li>
  </ul></li>
  <li><a href="#secure-multiparty-communication" id="toc-secure-multiparty-communication" class="nav-link" data-scroll-target="#secure-multiparty-communication"><span class="header-section-number">14.8.5</span> Secure Multiparty Communication</a>
  <ul class="collapse">
  <li><a href="#idea-centrale-4" id="toc-idea-centrale-4" class="nav-link" data-scroll-target="#idea-centrale-4">Idea Centrale</a></li>
  <li><a href="#compromessi-5" id="toc-compromessi-5" class="nav-link" data-scroll-target="#compromessi-5">Compromessi</a></li>
  </ul></li>
  <li><a href="#generazione-di-dati-sintetici" id="toc-generazione-di-dati-sintetici" class="nav-link" data-scroll-target="#generazione-di-dati-sintetici"><span class="header-section-number">14.8.6</span> Generazione di Dati Sintetici</a>
  <ul class="collapse">
  <li><a href="#idea-centrale-5" id="toc-idea-centrale-5" class="nav-link" data-scroll-target="#idea-centrale-5">Idea Centrale</a></li>
  <li><a href="#vantaggi-5" id="toc-vantaggi-5" class="nav-link" data-scroll-target="#vantaggi-5">Vantaggi</a></li>
  <li><a href="#compromessi-6" id="toc-compromessi-6" class="nav-link" data-scroll-target="#compromessi-6">Compromessi</a></li>
  </ul></li>
  <li><a href="#riepilogo" id="toc-riepilogo" class="nav-link" data-scroll-target="#riepilogo"><span class="header-section-number">14.8.7</span> Riepilogo</a></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">14.9</span> Conclusione</a></li>
  <li><a href="#sec-security-and-privacy-resource" id="toc-sec-security-and-privacy-resource" class="nav-link" data-scroll-target="#sec-security-and-privacy-resource"><span class="header-section-number">14.10</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/privacy_security/privacy_security.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/privacy_security/privacy_security.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-security_privacy" class="quarto-section-identifier"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-security-and-privacy-resource">Slide</a>, <a href="#sec-security-and-privacy-resource">Video</a>, <a href="#sec-security-and-privacy-resource">Esercizi</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_security_privacy.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Un‚Äôillustrazione sulla privacy e la sicurezza nei sistemi di apprendimento automatico. L‚Äôimmagine mostra un paesaggio digitale con una rete di nodi interconnessi e flussi di dati, che simboleggiano gli algoritmi di apprendimento automatico. In primo piano, c‚Äô√® un grande lucchetto sovrapposto alla rete, che rappresenta la privacy e la sicurezza. Il lucchetto √® semi-trasparente, consentendo alla rete sottostante di essere parzialmente visibile. Lo sfondo presenta codice binario e simboli di crittografia digitale, che enfatizzano il tema della sicurezza informatica. La combinazione di colori √® un mix di blu, verdi e grigi, che suggerisce un ambiente digitale ad alta tecnologia.</em></figcaption>
</figure>
</div>
<p>Sicurezza e privacy sono fondamentali quando si sviluppano sistemi di apprendimento automatico nel mondo reale. Poich√© l‚Äôapprendimento automatico viene sempre pi√π applicato a domini sensibili come sanit√†, finanza e dati personali, proteggere la riservatezza e prevenire l‚Äôuso improprio di dati e modelli diventa imperativo. Chiunque intenda creare sistemi di apprendimento automatico solidi e responsabili deve comprendere i potenziali rischi per la sicurezza e la privacy, come perdite di dati, furto di modelli, attacchi avversari, bias [pregiudizi] e accesso involontario a informazioni private. Dobbiamo anche comprendere le best practice per mitigare questi rischi. Ancora pi√π importante, sicurezza e privacy non possono essere un ripensamento e devono essere affrontate in modo proattivo durante tutto il ciclo di vita dello sviluppo del sistema di apprendimento automatico, dalla raccolta e dall‚Äôetichettatura dei dati al training, valutazione e deployment [distribuzione] del modello. Incorporare considerazioni sulla sicurezza e sulla privacy in ogni fase di creazione, distribuzione e gestione dei sistemi di apprendimento automatico √® essenziale per sbloccare in modo sicuro i vantaggi dell‚ÄôIA.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere i principali rischi per la privacy e la sicurezza del ML, come perdite di dati, furto di modelli, attacchi avversari, pregiudizi e accesso involontario ai dati.</p></li>
<li><p>Imparare dagli incidenti storici di sicurezza di sistemi hardware ed embedded.</p></li>
<li><p>Identificare le minacce ai modelli ML come avvelenamento dei dati, estrazione di modelli, inferenza di appartenenza ed esempi avversari.</p></li>
<li><p>Riconoscere le minacce alla sicurezza hardware per il ML embedded che abbracciano bug hardware, attacchi fisici, canali laterali, componenti contraffatti, ecc.</p></li>
<li><p>Esplorare le difese ML embedded, come ambienti di esecuzione affidabili, avvio sicuro, funzioni fisiche non clonabili e moduli di sicurezza hardware.</p></li>
<li><p>Discutere i problemi di privacy nella gestione di dati utente sensibili con ML embedded, comprese le normative.</p></li>
<li><p>Apprendere tecniche ML che preservano la privacy come la privacy differenziale, apprendimento federato, crittografia omomorfica e generazione di dati sintetici.</p></li>
<li><p>Comprendere i compromessi tra privacy, accuratezza, efficienza, modelli di minaccia e ipotesi di fiducia.</p></li>
<li><p>Riconoscere la necessit√† di una prospettiva multilivello che abbracci progettazione elettrica, firmware, software e fisica quando si proteggono dispositivi ML embedded.</p></li>
</ul>
</div>
</div>
<section id="panoramica" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="panoramica"><span class="header-section-number">14.1</span> Panoramica</h2>
<p>Il ‚ÄúMachine learning‚Äù [apprendimento automatico] si √® evoluto notevolmente dalle sue origini accademiche, in cui la privacy non era una preoccupazione primaria. Con la migrazione del ML in applicazioni commerciali e consumer, i dati sono diventati pi√π sensibili, comprendendo informazioni personali come comunicazioni, acquisti e dati sanitari. Questa esplosione di disponibilit√† di dati ha alimentato rapidi progressi nelle capacit√† del ML. Tuttavia, ha anche esposto nuovi rischi per la privacy, come dimostrato da incidenti come la <a href="https://en.wikipedia.org/wiki/AOL_search_log_release">fuga di dati di AOL nel 2006</a> e lo scandalo <a href="https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html">Cambridge Analytica</a>.</p>
<p>Questi eventi hanno evidenziato la crescente necessit√† di affrontare la privacy nei sistemi ML. In questo capitolo, esploriamo insieme considerazioni sulla privacy e sulla sicurezza, poich√© sono intrinsecamente collegate nel ML. Ad esempio, una telecamera di sicurezza domestica basata su ML deve proteggere i flussi video da accessi non autorizzati e fornire protezioni della privacy per garantire che solo gli utenti previsti possano visualizzare il filmato. Una violazione della sicurezza o della privacy potrebbe esporre momenti privati degli utenti.</p>
<p>I sistemi ML embedded come assistenti intelligenti e dispositivi indossabili sono onnipresenti ed elaborano dati intimi degli utenti. Tuttavia, i loro vincoli computazionali spesso impediscono protocolli di sicurezza pesanti. I progettisti devono bilanciare le esigenze di prestazioni con rigorosi standard di sicurezza e privacy adattati alle limitazioni dell‚Äôhardware embedded.</p>
<p>Questo capitolo fornisce conoscenze essenziali per affrontare il complesso panorama di privacy e sicurezza dell‚ÄôML embedded. Esploreremo le vulnerabilit√† e tratteremo varie tecniche che migliorano la privacy e la sicurezza all‚Äôinterno dei vincoli di risorse dei sistemi embedded.</p>
<p>Ci auguriamo che sviluppando una comprensione olistica dei rischi e delle misure di sicurezza, si acquisiranno i principi per sviluppare applicazioni ML embedded sicure ed etiche.</p>
</section>
<section id="terminologia" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="terminologia"><span class="header-section-number">14.2</span> Terminologia</h2>
<p>In questo capitolo parleremo insieme di sicurezza e privacy, quindi ci sono termini chiave su cui dobbiamo essere chiari. Poich√© questi termini sono concetti generali applicati in molti domini, vogliamo definire come si relazionano al contesto di questo capitolo e fornire esempi pertinenti per illustrare la loro applicazione.</p>
<ul>
<li><p><strong>Privacy:</strong> La capacit√† di controllare l‚Äôaccesso ai dati sensibili degli utenti raccolti ed elaborati da un sistema. Nel machine learning, ci√≤ implica garantire che le informazioni personali, come i dettagli finanziari o i dati biometrici, siano accessibili solo a individui autorizzati. Ad esempio, una telecamera di sicurezza domestica basata sull‚Äôapprendimento automatico potrebbe registrare filmati video e identificare i volti dei visitatori. Le preoccupazioni relative alla privacy riguardano chi pu√≤ accedere, visualizzare o condividere questi dati sensibili.</p></li>
<li><p><strong>Sicurezza:</strong> La pratica di protezione dei sistemi di apprendimento automatico e dei loro dati da accessi non autorizzati, hacking, furti e uso improprio. Un sistema sicuro salvaguarda i propri dati e le proprie operazioni per garantire integrit√† e riservatezza. Ad esempio, nel contesto della telecamera di sicurezza domestica, le misure di sicurezza impediscono agli hacker di intercettare feed video in diretta o di manomettere i filmati archiviati e garantiscono che il modello stesso rimanga intatto.</p></li>
<li><p><strong>Minaccia:</strong> Si riferisce a qualsiasi potenziale pericolo, attore malintenzionato o evento dannoso che mira a sfruttare le debolezze di un sistema per comprometterne la sicurezza o la privacy. Una minaccia √® la forza o l‚Äôintento esterno che cerca di causare danni. Utilizzando l‚Äôesempio della telecamera di sicurezza domestica, una minaccia potrebbe coinvolgere un hacker che tenta di accedere a flussi live, rubare video archiviati o ingannare il sistema con falsi input per aggirare il riconoscimento facciale.</p></li>
<li><p><strong>Vulnerabilit√†:</strong> Si riferisce a una debolezza, un difetto o una lacuna nel sistema che crea l‚Äôopportunit√† per una minaccia di avere successo. Le vulnerabilit√† sono i punti di esposizione che le minacce prendono di mira. Le vulnerabilit√† possono esistere in configurazioni hardware, software o di rete. Ad esempio, se la telecamera di sicurezza domestica si connette a Internet tramite una rete Wi-Fi non protetta, questa vulnerabilit√† potrebbe consentire agli aggressori di intercettare o manipolare i dati video.</p></li>
</ul>
</section>
<section id="precedenti-storici" class="level2 page-columns page-full" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="precedenti-storici"><span class="header-section-number">14.3</span> Precedenti Storici</h2>
<p>Sebbene le specifiche della sicurezza hardware dell‚Äôapprendimento automatico possano essere distinte, il campo dei sistemi embedded ha una storia di incidenti di sicurezza che forniscono lezioni fondamentali per tutti i sistemi connessi, compresi quelli che utilizzano ML. Ecco analisi dettagliate di violazioni passate:</p>
<section id="stuxnet" class="level3 page-columns page-full" data-number="14.3.1">
<h3 data-number="14.3.1" class="anchored" data-anchor-id="stuxnet"><span class="header-section-number">14.3.1</span> Stuxnet</h3>
<p>Nel 2010, qualcosa di inaspettato √® stato trovato su un computer in Iran: un virus informatico molto complicato che gli esperti non avevano mai visto prima. <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf">Stuxnet</a> era un worm informatico dannoso che prendeva di mira i sistemi di controllo di supervisione e acquisizione dati (SCADA) ed era progettato per danneggiare il programma nucleare iraniano <span class="citation" data-cites="farwell2011stuxnet">(<a href="../../../references.it.html#ref-farwell2011stuxnet" role="doc-biblioref">Farwell e Rohozinski 2011</a>)</span>. Stuxnet stava utilizzando quattro ‚Äú<a href="https://en.wikipedia.org/wiki/Zero-day_(computing)">exploit zero-day</a>‚Äù, attacchi che sfruttano debolezze segrete nel software di cui nessuno √® ancora a conoscenza. Ci√≤ ha reso Stuxnet molto subdolo e difficile da rilevare.</p>
<div class="no-row-height column-margin column-container"><div id="ref-farwell2011stuxnet" class="csl-entry" role="listitem">
Farwell, James P., e Rafal Rohozinski. 2011. <span>¬´Stuxnet and the Future of Cyber War¬ª</span>. <em>Survival</em> 53 (1): 23‚Äì40. <a href="https://doi.org/10.1080/00396338.2011.555586">https://doi.org/10.1080/00396338.2011.555586</a>.
</div></div><p>Ma Stuxnet non √® stato progettato per rubare informazioni o spiare le persone. Il suo obiettivo era la distruzione fisica, sabotare le centrifughe della centrale nucleare iraniana di Natanz! Quindi come ha fatto il virus a raggiungere i computer della centrale di Natanz, che avrebbe dovuto essere disconnessa dal mondo esterno per motivi di sicurezza? Gli esperti pensano che qualcuno abbia inserito una chiavetta USB contenente Stuxnet nella rete interna di Natanz. Ci√≤ ha permesso al virus di ‚Äúsaltare‚Äù da un sistema esterno ai sistemi di controllo nucleare isolati e scatenare il caos.</p>
<p>Stuxnet era un malware incredibilmente avanzato creato dai governi nazionali per passare dal regno digitale alle infrastrutture del mondo reale. Ha preso di mira in modo specifico importanti macchine industriali, dove l‚Äôapprendimento automatico embedded √® altamente applicabile in un modo mai visto prima. Il virus ha lanciato un segnale di allarme su come i sofisticati attacchi informatici potrebbero ora distruggere fisicamente apparecchiature e strutture.</p>
<p>Questa violazione √® stata significativa a causa della sua sofisticatezza; Stuxnet ha preso di mira in modo specifico i ‚Äúprogrammable logic controllers (PLC)‚Äù utilizzati per automatizzare processi elettromeccanici come la velocit√† delle centrifughe per l‚Äôarricchimento dell‚Äôuranio. Il worm sfruttava le vulnerabilit√† del sistema operativo Windows per ottenere l‚Äôaccesso al software Siemens Step7 che controlla i PLC. Nonostante non sia un attacco diretto ai sistemi ML, Stuxnet √® rilevante per tutti i sistemi embedded in quanto mostra il potenziale degli attori a livello statale per progettare attacchi che collegano il mondo informatico e quello fisico con effetti devastanti. <a href="#fig-stuxnet" class="quarto-xref">Figura&nbsp;<span>14.1</span></a> spiega Stuxnet in modo pi√π dettagliato.</p>
<div id="fig-stuxnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stuxnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/stuxnet.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stuxnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.1: Spiegazione di Stuxnet. Fonte: <a href="https://spectrum.ieee.org/the-real-story-of-stuxnet">IEEE Spectrum</a>
</figcaption>
</figure>
</div>
</section>
<section id="hack-della-jeep-cherokee" class="level3 page-columns page-full" data-number="14.3.2">
<h3 data-number="14.3.2" class="anchored" data-anchor-id="hack-della-jeep-cherokee"><span class="header-section-number">14.3.2</span> Hack della Jeep Cherokee</h3>
<p>L‚Äôhack della Jeep Cherokee √® stato un evento rivoluzionario che ha dimostrato i rischi insiti nelle automobili sempre pi√π connesse <span class="citation" data-cites="miller2019lessons">(<a href="../../../references.it.html#ref-miller2019lessons" role="doc-biblioref">Miller 2019</a>)</span>. In una dimostrazione controllata, i ricercatori della sicurezza hanno sfruttato da remoto una vulnerabilit√† nel sistema di entertainment Uconnect, che aveva una connessione cellulare a Internet. Sono stati in grado di controllare il motore, la trasmissione e i freni del veicolo, allarmando l‚Äôindustria automobilistica e spingendola a riconoscere le gravi implicazioni per la sicurezza delle vulnerabilit√† informatiche nei veicoli. <a href="#vid-jeephack" class="quarto-xref">Video&nbsp;<span>14.1</span></a> di seguito √® riportato un breve documentario dell‚Äôattacco.</p>
<div class="no-row-height column-margin column-container"><div id="ref-miller2019lessons" class="csl-entry" role="listitem">
Miller, Charlie. 2019. <span>¬´Lessons learned from hacking a car¬ª</span>. <em>IEEE Design &amp;amp; Test</em> 36 (6): 7‚Äì9. <a href="https://doi.org/10.1109/mdat.2018.2863106">https://doi.org/10.1109/mdat.2018.2863106</a>.
</div></div><div id="vid-jeephack" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;14.1: Hack della Jeep Cherokee
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/MK0SrxBC1xs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
<p>Sebbene non si sia trattato di un attacco a un sistema ML in s√©, l‚Äôaffidamento dei veicoli moderni ai sistemi embedded per funzioni critiche per la sicurezza presenta parallelismi significativi con l‚Äôimplementazione di ML nei sistemi embedded, sottolineando la necessit√† di una sicurezza robusta a livello hardware.</p>
</section>
<section id="botnet-mirai" class="level3 page-columns page-full" data-number="14.3.3">
<h3 data-number="14.3.3" class="anchored" data-anchor-id="botnet-mirai"><span class="header-section-number">14.3.3</span> Botnet Mirai</h3>
<p>La botnet Mirai ha coinvolto l‚Äôinfezione di dispositivi in rete come fotocamere digitali e lettori DVR <span class="citation" data-cites="antonakakis2017understanding">(<a href="../../../references.it.html#ref-antonakakis2017understanding" role="doc-biblioref">Antonakakis et al. 2017</a>)</span>. Nell‚Äôottobre 2016, la botnet √® stata utilizzata per condurre uno dei pi√π grandi attacchi <a href="https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/">DDoS</a>, interrompendo l‚Äôaccesso a Internet negli Stati Uniti. L‚Äôattacco √® stato possibile perch√© molti dispositivi utilizzavano nomi utente e password predefiniti, che sono stati facilmente sfruttati dal malware Mirai per controllare i dispositivi. <a href="#vid-mirai" class="quarto-xref">Video&nbsp;<span>14.2</span></a> spiega come funziona la botnet Mirai.</p>
<div class="no-row-height column-margin column-container"><div id="ref-antonakakis2017understanding" class="csl-entry" role="listitem">
Antonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie Bursztein, Jaime Cochran, Zakir Durumeric, et al. 2017. <span>¬´Understanding the mirai botnet¬ª</span>. In <em>26th USENIX security symposium (USENIX Security 17)</em>, 1093‚Äì1110.
</div></div><div id="vid-mirai" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;14.2: Botnet Mirai
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/1pywzRTJDaY" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
<p>Sebbene i dispositivi non fossero basati su ML, l‚Äôincidente √® un duro promemoria di ci√≤ che pu√≤ accadere quando numerosi dispositivi embedded con scarsi controlli di sicurezza vengono collegati in rete, cosa che sta diventando sempre pi√π comune con la crescita dei dispositivi IoT basati su ML.</p>
</section>
<section id="implicazioni" class="level3" data-number="14.3.4">
<h3 data-number="14.3.4" class="anchored" data-anchor-id="implicazioni"><span class="header-section-number">14.3.4</span> Implicazioni</h3>
<p>Queste violazioni storiche dimostrano gli effetti a cascata delle vulnerabilit√† hardware nei sistemi embedded. Ogni incidente offre un precedente per comprendere i rischi e progettare protocolli di sicurezza migliori. Ad esempio, la botnet Mirai evidenzia l‚Äôimmenso potenziale distruttivo quando gli autori delle minacce possono ottenere il controllo su dispositivi in rete con sicurezza debole, una situazione che sta diventando sempre pi√π comune con i sistemi ML. Molti dispositivi ML attuali funzionano come dispositivi ‚Äúedge‚Äù pensati per raccogliere ed elaborare dati localmente prima di inviarli al cloud. Proprio come le telecamere e i DVR compromessi da Mirai, i dispositivi ML edge spesso si basano su hardware embedded come processori ARM ed eseguono sistemi operativi leggeri come Linux. Proteggere le credenziali del dispositivo √® fondamentale.</p>
<p>Allo stesso modo, l‚Äôhacking della Jeep Cherokee √® stato un momento spartiacque per l‚Äôindustria automobilistica. Ha esposto gravi vulnerabilit√† nei crescenti sistemi di veicoli connessi in rete e la loro mancanza di isolamento dai sistemi di guida principali come freni e sterzo. In risposta, i produttori di automobili hanno investito molto in nuove misure di sicurezza informatica, anche se probabilmente permangono delle lacune.</p>
<p>Chrysler ha effettuato un richiamo per correggere il software vulnerabile Uconnect, che consentiva l‚Äôexploit remoto. Ci√≤ includeva l‚Äôaggiunta di protezioni a livello di rete per impedire l‚Äôaccesso esterno non autorizzato e la compartimentazione dei sistemi di bordo per limitare i movimenti laterali. Sono stati aggiunti ulteriori livelli di crittografia per i comandi inviati tramite il bus CAN all‚Äôinterno dei veicoli.</p>
<p>L‚Äôincidente ha anche stimolato la creazione di nuovi standard e best practice per la sicurezza informatica. L‚Äô<a href="https://automotiveisac.com/">Auto-ISAC</a> √® stato istituito per consentire alle case automobilistiche di condividere informazioni e la NHTSA ha guidato i rischi di gestione. Sono state sviluppate nuove procedure di test e audit per valutare le vulnerabilit√† in modo proattivo. Gli effetti collaterali continuano a guidare il cambiamento nel settore automobilistico poich√© le auto diventano sempre pi√π definite dal software.</p>
<p>Sfortunatamente, i produttori spesso trascurano la sicurezza quando sviluppano nuovi dispositivi edge ML, utilizzando password predefinite, comunicazioni non crittografate, aggiornamenti firmware non protetti, ecc. Tali vulnerabilit√† potrebbero consentire agli aggressori di ottenere l‚Äôaccesso e controllare i dispositivi su larga scala infettandoli con malware. Con una botnet di dispositivi ML compromessi, gli aggressori potrebbero sfruttare la loro potenza di calcolo aggregata per attacchi DDoS su infrastrutture critiche.</p>
<p>Sebbene questi eventi non abbiano coinvolto direttamente hardware di machine learning, i principi degli attacchi si estendono ai sistemi ML, che spesso coinvolgono dispositivi embedded e architetture di rete simili. Poich√© l‚Äôhardware ML √® sempre pi√π integrato con il mondo fisico, proteggerlo da tali violazioni √® fondamentale. L‚Äôevoluzione delle misure di sicurezza in risposta a questi incidenti fornisce preziose informazioni sulla protezione dei sistemi ML attuali e futuri da vulnerabilit√† analoghe.</p>
<p>La natura distribuita dei dispositivi edge ML significa che le minacce possono propagarsi rapidamente attraverso le reti. E se i dispositivi vengono utilizzati per scopi critici come dispositivi medici, controlli industriali o veicoli a guida autonoma, il potenziale danno fisico dei bot ML armati potrebbe essere grave. Proprio come Mirai ha dimostrato il potenziale pericoloso dei dispositivi IoT scarsamente protetti, la prova del nove per la sicurezza dell‚Äôhardware ML sar√† quanto questi dispositivi siano vulnerabili o resilienti ad attacchi simili a worm. La posta in gioco aumenta man mano che il ML si diffonde in ambiti critici per la sicurezza, ponendo l‚Äôonere sui produttori e sugli operatori di sistema di incorporare le lezioni di Mirai.</p>
<p>La lezione √® l‚Äôimportanza di progettare per la sicurezza fin dall‚Äôinizio e di avere difese stratificate. Il caso Jeep evidenzia potenziali vulnerabilit√† per i sistemi ML in merito alle interfacce software esterne e all‚Äôisolamento tra sottosistemi. I produttori di dispositivi e piattaforme ML dovrebbero assumere un approccio proattivo e completo simile alla sicurezza piuttosto che lasciarlo come un ripensamento. Una risposta rapida e la diffusione delle best practice saranno cruciali man mano che le minacce si evolvono.</p>
</section>
</section>
<section id="minacce-alla-sicurezza-per-i-modelli-ml" class="level2 page-columns page-full" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="minacce-alla-sicurezza-per-i-modelli-ml"><span class="header-section-number">14.4</span> Minacce alla Sicurezza per i Modelli ML</h2>
<p>I modelli ML affrontano rischi per la sicurezza che possono comprometterne l‚Äôintegrit√†, le prestazioni e l‚Äôaffidabilit√† se non affrontati adeguatamente. Tra queste, spiccano tre minacce principali: il furto di modelli, in cui gli avversari rubano parametri di modelli proprietari e i dati sensibili in essi contenuti; l‚Äôavvelenamento dei dati, che compromette i modelli manomettendo i dati di training; e gli attacchi avversari, progettati per ingannare i modelli e indurli a fare previsioni errate o indesiderate. Discuteremo ciascuna di queste minacce in dettaglio e forniremo esempi di casi di studio per illustrare le loro implicazioni nel mondo reale.</p>
<section id="furto-di-modelli" class="level3 page-columns page-full" data-number="14.4.1">
<h3 data-number="14.4.1" class="anchored" data-anchor-id="furto-di-modelli"><span class="header-section-number">14.4.1</span> Furto di Modelli</h3>
<p>Il furto di modelli si verifica quando un aggressore ottiene l‚Äôaccesso non autorizzato a un modello ML distribuito. La preoccupazione in questo caso √® il furto della struttura del modello e dei parametri addestrati, nonch√© dei dati proprietari in esso contenuti <span class="citation" data-cites="ateniese2015hacking">(<a href="../../../references.it.html#ref-ateniese2015hacking" role="doc-biblioref">Ateniese et al. 2015</a>)</span>. Il furto di modelli √® una minaccia reale e crescente, come dimostrato da casi come quello dell‚Äôex ingegnere di Google Anthony Levandowski, che <a href="https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html">presumibilmente ha rubato i progetti di auto a guida autonoma di Waymo</a> e ha fondato un‚Äôazienda concorrente. Oltre all‚Äôimpatto economico, il furto di modelli pu√≤ seriamente compromettere la privacy e consentire ulteriori attacchi.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ateniese2015hacking" class="csl-entry" role="listitem">
Ateniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, e Giovanni Felici. 2015. <span>¬´Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers¬ª</span>. <em>International Journal of Security and Networks</em> 10 (3): 137. <a href="https://doi.org/10.1504/ijsn.2015.071829">https://doi.org/10.1504/ijsn.2015.071829</a>.
</div></div><p>Ad esempio, si consideri un modello ML sviluppato per raccomandazioni personalizzate in un‚Äôapplicazione di e-commerce. Se un concorrente ruba questo modello, ottiene informazioni su analisi aziendali, preferenze dei clienti e persino segreti commerciali racchiusi nei dati del modello. Gli aggressori potrebbero sfruttare i modelli rubati per creare input pi√π efficaci per attacchi di ‚Äúinversione del modello‚Äù, deducendo dettagli privati sui dati di addestramento del modello. Un modello di raccomandazione di e-commerce clonato potrebbe rivelare i comportamenti di acquisto e i dati demografici dei clienti.</p>
<p>Per comprendere gli attacchi di ‚Äúinversione del modello‚Äù, si consideri un sistema di riconoscimento facciale utilizzato per concedere l‚Äôaccesso a strutture protette. Il sistema viene addestrato su un set di dati di foto dei dipendenti. Un aggressore potrebbe dedurre le caratteristiche del set di dati originale osservando l‚Äôoutput del modello su vari input. Ad esempio, supponiamo che il livello di confidenza del modello per un particolare volto sia significativamente pi√π alto per un dato set di caratteristiche. In tal caso, un aggressore potrebbe dedurre che qualcuno con quelle caratteristiche √® probabile che sia nel set di dati di addestramento.</p>
<p>La metodologia di ‚Äúinversione del modello‚Äù in genere prevede i seguenti passaggi:</p>
<ul>
<li><p><strong>Accesso agli Output del Modello:</strong> L‚Äôaggressore interroga il modello ML con dati di input e osserva gli output. Ci√≤ avviene spesso tramite un‚Äôinterfaccia legittima, come un‚ÄôAPI pubblica.</p></li>
<li><p><strong>Analisi dei Confidence Score:</strong> Per ogni input, il modello fornisce un ‚Äúpunteggio di confidenza‚Äù che riflette quanto l‚Äôinput sia simile ai dati di training.</p></li>
<li><p><strong>Reverse-Engineering:</strong> Analizzando i punteggi di confidenza o le probabilit√† di output, gli aggressori possono utilizzare tecniche di ottimizzazione per ricostruire ci√≤ che ritengono sia vicino ai dati di input originali.</p></li>
</ul>
<p>Un esempio storico di tale vulnerabilit√† esplorata √® stata la ricerca sugli attacchi di inversione contro il set di dati del premio Netflix degli Stati Uniti, in cui i ricercatori hanno dimostrato che era possibile conoscere le preferenze cinematografiche di un individuo, il che potrebbe portare a violazioni della privacy <span class="citation" data-cites="narayanan2006break">(<a href="../../../references.it.html#ref-narayanan2006break" role="doc-biblioref">Narayanan e Shmatikov 2006</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-narayanan2006break" class="csl-entry" role="listitem">
Narayanan, Arvind, e Vitaly Shmatikov. 2006. <span>¬´How To Break Anonymity of the Netflix Prize Dataset¬ª</span>. <em>CoRR</em>. <a href="http://arxiv.org/abs/cs/0610105">http://arxiv.org/abs/cs/0610105</a>.
</div></div><p>Il furto di modelli implica che potrebbe portare a perdite economiche, minare il vantaggio competitivo e violare la privacy degli utenti. C‚Äô√® anche il rischio di attacchi di inversione del modello, in cui un avversario potrebbe immettere vari dati nel modello rubato per dedurre informazioni sensibili sui dati di addestramento.</p>
<p>In base alla risorsa desiderata, gli attacchi con furto di modelli possono essere suddivisi in due categorie: propriet√† esatte del modello e comportamento approssimativo del modello.</p>
<section id="furto-di-propriet√†-esatte-del-modello" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="furto-di-propriet√†-esatte-del-modello">Furto di Propriet√† Esatte del Modello</h5>
<p>In questi attacchi, l‚Äôobiettivo √® estrarre informazioni su metriche concrete, come i parametri appresi di una rete, gli iperparametri ottimizzati e l‚Äôarchitettura interna dei layer del modello <span class="citation" data-cites="oliynyk2023know">(<a href="../../../references.it.html#ref-oliynyk2023know" role="doc-biblioref">Oliynyk, Mayer, e Rauber 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><ul>
<li><p><strong>Parametri Appresi:</strong> Gli avversari mirano a rubare la conoscenza appresa di un modello (pesi e bias) per replicarla. Il furto di parametri √® generalmente utilizzato con altri attacchi, come il furto di architettura, che non hanno conoscenza dei parametri.</p></li>
<li><p><strong>Iperparametri Ottimizzati:</strong> L‚Äôaddestramento √® costoso e l‚Äôidentificazione della configurazione ottimale degli iperparametri (come velocit√† di apprendimento e regolarizzazione) pu√≤ richiedere molto tempo e risorse. Di conseguenza, rubare gli iperparametri ottimizzati di un modello consente agli avversari di replicare il modello senza sostenere gli stessi costi di sviluppo.</p></li>
<li><p><strong>Architettura del Modello:</strong> Questo attacco riguarda la progettazione e la struttura specifiche del modello, come strati, neuroni e pattern di connettivit√†. Oltre a ridurre i costi di training associati, questo furto rappresenta un grave rischio per la propriet√† intellettuale, potenzialmente compromettendo il vantaggio competitivo di un‚Äôazienda. Il furto di architettura pu√≤ essere ottenuto sfruttando attacchi side-channel (discussi pi√π avanti).</p></li>
</ul>
</section>
<section id="furto-del-comportamento-approssimativo-del-modello" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="furto-del-comportamento-approssimativo-del-modello">Furto del Comportamento Approssimativo del Modello</h5>
<p>Invece di estrarre valori numerici esatti dei parametri del modello, questi attacchi mirano a riprodurre il comportamento del modello (previsioni ed efficacia), il processo decisionale e le caratteristiche di alto livello <span class="citation" data-cites="oliynyk2023know">(<a href="../../../references.it.html#ref-oliynyk2023know" role="doc-biblioref">Oliynyk, Mayer, e Rauber 2023</a>)</span>. Queste tecniche mirano a ottenere risultati simili pur consentendo deviazioni interne nei parametri e nell‚Äôarchitettura. I tipi di furto di comportamento approssimativo includono l‚Äôottenimento dello stesso livello di efficacia e l‚Äôottenimento di coerenza di previsione.</p>
<div class="no-row-height column-margin column-container"><div id="ref-oliynyk2023know" class="csl-entry" role="listitem">
Oliynyk, Daryna, Rudolf Mayer, e Andreas Rauber. 2023. <span>¬´I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences¬ª</span>. <em>ACM Computing Surveys</em> 55 (14s): 1‚Äì41. <a href="https://doi.org/10.1145/3595292">https://doi.org/10.1145/3595292</a>.
</div></div><ul>
<li><p><strong>Livello di efficacia:</strong> Gli aggressori mirano a replicare le capacit√† decisionali del modello piuttosto che concentrarsi sui valori precisi dei parametri. Ci√≤ avviene attraverso la comprensione del comportamento complessivo del modello. Consideriamo uno scenario in cui un aggressore desidera copiare il comportamento di un modello di classificazione delle immagini. Analizzando i limiti decisionali del modello, l‚Äôattacco ottimizza il suo modello per raggiungere un‚Äôefficacia paragonabile al modello originale. Ci√≤ potrebbe comportare l‚Äôanalisi di 1) la matrice di confusione per comprendere l‚Äôequilibrio delle metriche di previsione (vero positivo, vero negativo, falso positivo, falso negativo) e 2) altre metriche di prestazione, come punteggio F1 e precisione, per garantire che i due modelli siano comparabili.</p></li>
<li><p><strong>Coerenza della Previsione:</strong> L‚Äôattaccante cerca di allineare i pattern di previsione del proprio modello con quelli del modello target. Ci√≤ comporta l‚Äôabbinamento degli output di previsione (sia positivi che negativi) sullo stesso set di input e la garanzia della coerenza distributiva tra classi diverse. Ad esempio, prendiamo in considerazione un modello di elaborazione del linguaggio naturale (NLP) che genera un‚Äôanalisi del sentiment per le recensioni di film (etichettando le recensioni come positive, neutre o negative). L‚Äôattaccante cercher√† di mettere a punto il proprio modello per adattarlo alla previsione dei modelli originali sullo stesso set di recensioni di film. Ci√≤ include la garanzia che il modello commetta gli stessi errori (previsioni errate) commessi dal modello target.</p></li>
</ul>
</section>
<section id="caso-di-studio-il-furto-di-propriet√†-intellettuale-di-tesla" class="level4">
<h4 class="anchored" data-anchor-id="caso-di-studio-il-furto-di-propriet√†-intellettuale-di-tesla">Caso di Studio: Il furto di Propriet√† Intellettuale di Tesla</h4>
<p>Nel 2018, Tesla ha intentato una <a href="https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf">causa</a> contro la startup di auto a guida autonoma <a href="https://zoox.com/">Zoox</a>, sostenendo che ex dipendenti avevano rubato dati riservati e segreti commerciali relativi al sistema di assistenza alla guida autonoma di Tesla.</p>
<p>Tesla ha affermato che diversi suoi ex dipendenti hanno sottratto oltre 10 GB di dati proprietari, tra cui modelli di ML e codice sorgente, prima di unirsi a Zoox. Ci√≤ avrebbe incluso uno dei modelli di riconoscimento delle immagini cruciali di Tesla per l‚Äôidentificazione degli oggetti.</p>
<p>Il furto di questo modello proprietario sensibile potrebbe aiutare Zoox ad abbreviare anni di sviluppo ML e duplicare le capacit√† di Tesla. Tesla ha sostenuto che questo furto di propriet√† intellettuale ha causato notevoli danni finanziari e competitivi. C‚Äôerano anche preoccupazioni che potesse consentire attacchi di inversione del modello per dedurre dettagli privati sui dati di test di Tesla.</p>
<p>I dipendenti di Zoox hanno negato di aver rubato informazioni proprietarie. Tuttavia, il caso evidenzia i rischi significativi del furto di modelli, che consente la clonazione di modelli commerciali, causando ripercussioni economiche e aprendo la porta a ulteriori violazioni della privacy dei dati.</p>
</section>
</section>
<section id="avvelenamento-dei-dati" class="level3 page-columns page-full" data-number="14.4.2">
<h3 data-number="14.4.2" class="anchored" data-anchor-id="avvelenamento-dei-dati"><span class="header-section-number">14.4.2</span> Avvelenamento dei Dati</h3>
<p>L‚Äôavvelenamento dei dati √® un attacco in cui i dati di training vengono manomessi, portando a un modello compromesso <span class="citation" data-cites="biggio2012poisoning">(<a href="../../../references.it.html#ref-biggio2012poisoning" role="doc-biblioref">Biggio, Nelson, e Laskov 2012</a>)</span>. Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ci√≤ pu√≤ essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.</p>
<div class="no-row-height column-margin column-container"><div id="ref-biggio2012poisoning" class="csl-entry" role="listitem">
Biggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. <span>¬´Poisoning Attacks against Support Vector Machines.¬ª</span> In <em>Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012</em>. icml.cc / Omnipress. <a href="http://icml.cc/2012/papers/880.pdf">http://icml.cc/2012/papers/880.pdf</a>.
</div></div><p>Il processo di solito prevede i seguenti passaggi:</p>
<ul>
<li><p><strong>Injection:</strong> L‚Äôaggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un‚Äôispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.</p></li>
<li><p><strong>Training:</strong> Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei pattern di dati.</p></li>
<li><p><strong>Deployment:</strong> Una volta distribuito il modello, l‚Äôaddestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilit√† prevedibili che l‚Äôaggressore pu√≤ sfruttare.</p></li>
</ul>
<p>Gli impatti dell‚Äôavvelenamento dei dati vanno oltre i semplici errori di classificazione o cali di accuratezza. Ad esempio, se dati errati o dannosi vengono introdotti nel set di addestramento di un sistema di riconoscimento dei segnali stradali, il modello potrebbe imparare a classificare erroneamente i segnali di stop come segnali di precedenza, il che pu√≤ avere pericolose conseguenze nel mondo reale, specialmente nei sistemi autonomi embedded come i veicoli autonomi.</p>
<p>L‚Äôavvelenamento dei dati pu√≤ degradare l‚Äôaccuratezza di un modello, costringerlo a fare previsioni errate o farlo comportare in modo imprevedibile. In applicazioni critiche come l‚Äôassistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza.</p>
<p>Esistono sei categorie principali di avvelenamento dei dati <span class="citation" data-cites="oprea2022poisoning">(<a href="../../../references.it.html#ref-oprea2022poisoning" role="doc-biblioref">Oprea, Singhal, e Vassilev 2022</a>)</span>:</p>
<div class="no-row-height column-margin column-container"><div id="ref-oprea2022poisoning" class="csl-entry" role="listitem">
Oprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. <span>¬´Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?¬ª</span> <em>Computer</em> 55 (11): 94‚Äì99. <a href="https://doi.org/10.1109/mc.2022.3190787">https://doi.org/10.1109/mc.2022.3190787</a>.
</div></div><ul>
<li><p><strong>Attacchi di Disponibilit√†:</strong> Questi attacchi cercano di compromettere la funzionalit√† complessiva di un modello. Fanno s√¨ che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio √® il ‚Äúlabel flipping‚Äù, in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.</p></li>
<li><p><strong>Attacchi Mirati:</strong> A differenza degli attacchi alla disponibilit√†, gli attacchi mirati mirano a compromettere un numero limitato di campioni di test. Quindi, l‚Äôeffetto √® localizzato su un numero limitato di classi, mentre il modello mantiene lo stesso livello di accuratezza originale sulla maggior parte delle classi. La natura mirata dell‚Äôattacco richiede che l‚Äôaggressore conosca le classi del modello, rendendo pi√π difficile il rilevamento di questi attacchi.</p></li>
<li><p><strong>Attacchi Backdoor:</strong> In questi attacchi, un avversario prende di mira pattern specifici nei dati. L‚Äôaggressore introduce una backdoor (un trigger o pattern nascosto e dannoso) nei dati di training, ad esempio modificando determinate feature nei dati strutturati o un pattern di pixel in una posizione fissa. Ci√≤ fa s√¨ che il modello associ il pattern dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di test che contengono un pattern dannoso, fa false previsioni, evidenziando l‚Äôimportanza della cautela e della prevenzione nel ruolo dei professionisti della sicurezza dei dati.</p></li>
<li><p><strong>Attacchi di Sotto-popolazione:</strong> Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l‚Äôaccuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilit√† e mirati: eseguire attacchi di disponibilit√† (degrado delle prestazioni) nell‚Äôambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:</p></li>
<li><p><strong>Scope:</strong> Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un attore inserisce immagini manipolate di un cartello di avvertimento ‚Äúrallentamenti‚Äù (con perturbazioni o pattern attentamente studiati), che fa s√¨ che un‚Äôauto autonoma non riconosca tale cartello e non rallenti. D‚Äôaltro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica √® un esempio di attacco di sotto-popolazione.</p></li>
<li><p><strong>Conoscenza:</strong> Mentre gli attacchi mirati richiedono un alto grado di familiarit√† con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.</p></li>
</ul>
<section id="caso-di-studio-avvelenamento-dei-sistemi-di-moderazione-dei-contenuti" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="caso-di-studio-avvelenamento-dei-sistemi-di-moderazione-dei-contenuti">Caso di Studio: Avvelenamento dei Sistemi di Moderazione dei Contenuti</h4>
<p>Nel 2017, i ricercatori hanno dimostrato un attacco di avvelenamento dei data contro un modello di classificazione della tossicit√† popolare chiamato Perspective <span class="citation" data-cites="hosseini2017deceiving">(<a href="../../../references.it.html#ref-hosseini2017deceiving" role="doc-biblioref">Hosseini et al. 2017</a>)</span>. Questo modello ML rileva commenti tossici online.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hosseini2017deceiving" class="csl-entry" role="listitem">
Hosseini, Hossein, Sreeram Kannan, Baosen Zhang, e Radha Poovendran. 2017. <span>¬´Deceiving Google‚Äôs Perspective API Built for Detecting Toxic Comments¬ª</span>. <em>ArXiv preprint</em> abs/1702.08138 (febbraio). <a href="http://arxiv.org/abs/1702.08138v1">http://arxiv.org/abs/1702.08138v1</a>.
</div></div><p>I ricercatori hanno aggiunto commenti tossici generati sinteticamente con lievi errori di ortografia e grammaticali ai dati di training del modello. Ci√≤ ha lentamente corrotto il modello, facendogli classificare erroneamente un numero crescente di input gravemente tossici come non tossici nel tempo.</p>
<p>Dopo il ri-addestramento sui dati avvelenati, il tasso di falsi negativi del modello √® aumentato dall‚Äô1,4% al 27%, consentendo ai commenti estremamente tossici di aggirare il rilevamento. I ricercatori hanno avvertito che questo furtivo ‚Äúdata poisoning‚Äù potrebbe consentire la diffusione di discorsi di odio, molestie e abusi se implementato contro sistemi di moderazione reali.</p>
<p>Questo caso evidenzia come l‚Äôavvelenamento dei dati possa degradare l‚Äôaccuratezza e l‚Äôaffidabilit√† del modello. Per le piattaforme di social media, un attacco di avvelenamento che compromette il rilevamento della tossicit√† potrebbe portare alla proliferazione di contenuti dannosi e alla sfiducia nei sistemi di moderazione ML. L‚Äôesempio dimostra perch√© proteggere l‚Äôintegrit√† dei dati di training e monitorare l‚Äôavvelenamento √® fondamentale in tutti i domini applicativi.</p>
</section>
<section id="caso-di-studio-proteggere-larte-attraverso-lavvelenamento-dei-dati" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="caso-di-studio-proteggere-larte-attraverso-lavvelenamento-dei-dati">Caso di Studio: Proteggere l‚ÄôArte Attraverso l‚ÄôAvvelenamento dei Dati</h4>
<p>√à interessante notare che gli attacchi di ‚Äúdata poisoning‚Äù non sono sempre dannosi <span class="citation" data-cites="shan2023prompt">(<a href="../../../references.it.html#ref-shan2023prompt" role="doc-biblioref">Shan et al. 2023</a>)</span>. Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l‚ÄôUniversit√† di Chicago, utilizza l‚Äôavvelenamento dei dati per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di IA generativa. Gli artisti possono utilizzare lo strumento per modificare le proprie immagini in modo sottile prima di caricarle online.</p>
<div class="no-row-height column-margin column-container"></div><p>Sebbene queste modifiche siano impercettibili all‚Äôocchio umano, possono degradare significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando integrate nei dati di addestramento. I modelli generativi possono essere manipolati per produrre output irrealistici o privi di senso. Ad esempio, con solo 300 immagini corrotte, i ricercatori dell‚ÄôUniversit√† di Chicago sono riusciti a ingannare l‚Äôultimo modello ‚ÄúStable Diffusion‚Äù per generare immagini di cani che assomigliano a felini o bovini quando richiesto per le automobili.</p>
<p>Con l‚Äôaumento della quantit√† di immagini corrotte online, l‚Äôefficacia dei modelli addestrati su dati estratti diminuir√† esponenzialmente. Inizialmente, identificare i dati corrotti √® difficile e richiede un intervento manuale. Successivamente, la contaminazione si diffonde rapidamente ai concetti correlati, poich√© i modelli generativi stabiliscono connessioni tra le parole e le loro rappresentazioni visive. Di conseguenza, un‚Äôimmagine corrotta di un‚Äô‚Äúauto‚Äù potrebbe propagarsi in immagini generate collegate a termini come ‚Äúcamion‚Äù, ‚Äútreno‚Äù e ‚Äúautobus‚Äù.</p>
<p>D‚Äôaltro canto, questo strumento pu√≤ essere utilizzato in modo dannoso e influenzare le applicazioni legittime del modello generativo. Ci√≤ dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.</p>
<p><a href="../robust_ai/robust_ai.it.html#fig-poisoning" class="quarto-xref">Figura&nbsp;<span>17.26</span></a> mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in varie categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un‚Äôauto genera una mucca.</p>
<div id="fig-poisoning" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-poisoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Data_poisoning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.2: Avvelenamento dei Dati. Fonte: <span class="citation" data-cites="shan2023prompt">Shan et al. (<a href="../../../references.it.html#ref-shan2023prompt" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-shan2023prompt" class="csl-entry" role="listitem">
Shan, Shawn, Wenxin Ding, Josephine Passananti, Stanley Wu, Haitao Zheng, e Ben Y. Zhao. 2023. <span>¬´Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models¬ª</span>. <em>ArXiv preprint</em> abs/2310.13828 (ottobre). <a href="http://arxiv.org/abs/2310.13828v3">http://arxiv.org/abs/2310.13828v3</a>.
</div></div></figure>
</div>
</section>
</section>
<section id="attacchi-avversari" class="level3 page-columns page-full" data-number="14.4.3">
<h3 data-number="14.4.3" class="anchored" data-anchor-id="attacchi-avversari"><span class="header-section-number">14.4.3</span> Attacchi Avversari</h3>
<p>Gli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) <span class="citation" data-cites="parrish2023adversarial">(<a href="../../../references.it.html#ref-parrish2023adversarial" role="doc-biblioref">Parrish et al. 2023</a>)</span>. Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono ‚Äúhackerare‚Äù il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui lievi, spesso impercettibili alterazioni dei dati di input possono indurre un modello ML a fare una previsione errata.</p>
<div class="no-row-height column-margin column-container"><div id="ref-parrish2023adversarial" class="csl-entry" role="listitem">
Parrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. <span>¬´Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models¬ª</span>. <em>ArXiv preprint</em> abs/2305.14384 (maggio). <a href="http://arxiv.org/abs/2305.14384v1">http://arxiv.org/abs/2305.14384v1</a>.
</div><div id="ref-ramesh2021zero" class="csl-entry" role="listitem">
Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. <span>¬´Zero-Shot Text-to-Image Generation¬ª</span>. In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, a cura di Marina Meila e Tong Zhang, 139:8821‚Äì31. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/ramesh21a.html">http://proceedings.mlr.press/v139/ramesh21a.html</a>.
</div><div id="ref-rombach2022highresolution" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. <span>¬´High-Resolution Image Synthesis with Latent Diffusion Models¬ª</span>. In <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10674‚Äì85. IEEE. <a href="https://doi.org/10.1109/cvpr52688.2022.01042">https://doi.org/10.1109/cvpr52688.2022.01042</a>.
</div></div><p>√à possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE <span class="citation" data-cites="ramesh2021zero">(<a href="../../../references.it.html#ref-ramesh2021zero" role="doc-biblioref">Ramesh et al. 2021</a>)</span> o Stable Diffusion <span class="citation" data-cites="rombach2022highresolution">(<a href="../../../references.it.html#ref-rombach2022highresolution" role="doc-biblioref">Rombach et al. 2022</a>)</span>. Ad esempio, alterando i valori dei pixel di un‚Äôimmagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.</p>
<p>Gli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l‚Äôinferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input dannosi con perturbazioni per fuorviare il riconoscimento di pattern del modello, essenzialmente ‚Äúhackerando‚Äù le percezioni del modello.</p>
<p>Gli attacchi avversari rientrano in diversi scenari:</p>
<ul>
<li><p><strong>Attacchi Whitebox:</strong> L‚Äôattaccante ha una conoscenza completa del funzionamento interno del modello target, inclusi i dati di addestramento, i parametri e l‚Äôarchitettura. Questo ampio accesso facilita lo sfruttamento delle vulnerabilit√† del modello. L‚Äôattaccante pu√≤ sfruttare debolezze specifiche e sottili per costruire esempi avversari altamente efficaci.</p></li>
<li><p><strong>Attacchi Blackbox:</strong> A differenza degli attacchi Whitebox, in quelli Blackbox l‚Äôattaccante ha poca o nessuna conoscenza del modello target. L‚Äôattore avversario deve osservare attentamente il comportamento di output del modello per eseguire l‚Äôattacco.</p></li>
<li><p><strong>Attacchi Greybox:</strong> Questi attacchi occupano uno spettro tra gli attacchi Blackbox e Whitebox. L‚Äôavversario possiede una conoscenza parziale della struttura interna del modello target. Ad esempio, l‚Äôattaccante potrebbe conoscere i dati di training ma non avere informazioni sull‚Äôarchitettura o sui parametri del modello. In scenari pratici, la maggior parte degli attacchi rientra in questa zona grigia.</p></li>
</ul>
<p>Il panorama dei modelli di apprendimento automatico √® complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilit√† all‚Äôinterno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:</p>
<ul>
<li><p><strong>Generative Adversarial Network (GAN):</strong> La natura avversaria delle GAN, in cui un generatore e un discriminatore competono, si allinea perfettamente con la creazione di attacchi avversari <span class="citation" data-cites="goodfellow2020generative">(<a href="../../../references.it.html#ref-goodfellow2020generative" role="doc-biblioref">Goodfellow et al. 2020</a>)</span>. Sfruttando questo framework, la rete del generatore viene addestrata per produrre input che sfruttano le debolezze di un modello target, causandone una classificazione errata. Questo processo dinamico e competitivo rende le GAN particolarmente efficaci nel creare esempi avversari sofisticati e diversificati, sottolineando la loro adattabilit√† nell‚Äôattaccare i modelli di apprendimento automatico.</p></li>
<li><p><strong>Transfer Learning Adversarial Attacks:</strong> Questi attacchi prendono di mira gli estrattori di feature nei modelli di apprendimento per trasferimento introducendo perturbazioni che manipolano le loro rappresentazioni apprese. Gli estrattori di feature, pre-addestrati per identificare modelli generali, sono ottimizzati per attivit√† specifiche nei modelli downstream [a valle]. Gli avversari sfruttano questo trasferimento creando input che distorcono gli output dell‚Äôestrattore di feature, causando classificazioni errate a valle. Gli ‚Äúattacchi headless‚Äù esemplificano questa strategia, in cui gli avversari si concentrano sull‚Äôestrattore di feature senza richiedere l‚Äôaccesso alla classificazione principale o ai dati di addestramento. Ci√≤ evidenzia una vulnerabilit√† critica nelle pipeline di apprendimento per trasferimento, poich√© i componenti fondamentali di molti modelli possono essere sfruttati. Rafforzare le difese √® essenziale, data la diffusa dipendenza dai modelli pre-addestrati <span class="citation" data-cites="ahmed2020headless">(<a href="../../../references.it.html#ref-ahmed2020headless" role="doc-biblioref">Abdelkader et al. 2020</a>)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-goodfellow2020generative" class="csl-entry" role="listitem">
Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. <span>¬´Generative adversarial networks¬ª</span>. <em>Communications of the ACM</em> 63 (11): 139‚Äì44. <a href="https://doi.org/10.1145/3422622">https://doi.org/10.1145/3422622</a>.
</div><div id="ref-ahmed2020headless" class="csl-entry" role="listitem">
Abdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. <span>¬´Headless Horseman: Adversarial Attacks on Transfer Learning Models¬ª</span>. In <em>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 3087‚Äì91. IEEE. <a href="https://doi.org/10.1109/icassp40776.2020.9053181">https://doi.org/10.1109/icassp40776.2020.9053181</a>.
</div></div><section id="caso-di-studio-inganno-dei-modelli-di-rilevamento-dei-segnali-stradali" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="caso-di-studio-inganno-dei-modelli-di-rilevamento-dei-segnali-stradali">Caso di Studio: Inganno dei Modelli di Rilevamento dei Segnali Stradali</h4>
<p>Nel 2017, i ricercatori hanno condotto esperimenti posizionando piccoli adesivi bianchi e neri sui segnali di stop <span class="citation" data-cites="eykholt2018robust">(<a href="../../../references.it.html#ref-eykholt2018robust" role="doc-biblioref">Eykholt et al. 2017</a>)</span>. Quando visti da un occhio umano normale, gli adesivi non oscuravano il segnale n√© ne impedivano l‚Äôinterpretazione. Tuttavia, quando le immagini degli adesivi dei segnali di stop venivano inserite nei modelli ML standard di classificazione dei segnali stradali, venivano classificati erroneamente come segnali di limite di velocit√† nell‚Äô85% dei casi.</p>
<div class="no-row-height column-margin column-container"><div id="ref-eykholt2018robust" class="csl-entry" role="listitem">
Eykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. <span>¬´Robust Physical-World Attacks on Deep Learning Models¬ª</span>. <em>ArXiv preprint</em> abs/1707.08945 (luglio). <a href="http://arxiv.org/abs/1707.08945v5">http://arxiv.org/abs/1707.08945v5</a>.
</div></div><p>Questa dimostrazione ha mostrato come semplici adesivi avversari potrebbero ingannare i sistemi ML facendogli interpretare male i segnali stradali critici. Se implementati in modo realistico, questi attacchi potrebbero mettere a repentaglio la sicurezza pubblica, inducendo i veicoli autonomi a interpretare male i segnali di stop come limiti di velocit√†. I ricercatori hanno avvertito che ci√≤ potrebbe potenzialmente causare pericolosi semplici rallentamenti o accelerazioni negli incroci.</p>
<p>Questo caso di studio fornisce un‚Äôillustrazione concreta di come gli esempi avversari sfruttano i meccanismi di riconoscimento di pattern dei modelli ML. Alterando in modo sottile i dati di input, gli aggressori possono indurre previsioni errate e rappresentare rischi significativi per applicazioni critiche per la sicurezza come le auto a guida autonoma. La semplicit√† dell‚Äôattacco dimostra come anche cambiamenti minori e impercettibili possano sviare i modelli. Di conseguenza, gli sviluppatori devono implementare difese robuste contro tali minacce.</p>
</section>
</section>
</section>
<section id="minacce-alla-sicurezza-per-lhardware-ml" class="level2 page-columns page-full" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="minacce-alla-sicurezza-per-lhardware-ml"><span class="header-section-number">14.5</span> Minacce alla Sicurezza Per l‚ÄôHardware ML</h2>
<p>L‚Äôhardware di apprendimento automatico embedded svolge un ruolo fondamentale nel potenziamento delle moderne applicazioni di IA, ma √® sempre pi√π esposto a una vasta gamma di minacce alla sicurezza. Queste vulnerabilit√† possono derivare da difetti nella progettazione hardware, manomissioni fisiche o persino dai complessi percorsi delle catene di fornitura globali. Per affrontare questi rischi √® necessaria una comprensione completa dei vari modi in cui l‚Äôintegrit√† hardware pu√≤ essere compromessa. Come riassunto in <a href="#tbl-threat_types" class="quarto-xref">Tabella&nbsp;<span>14.1</span></a>, questa sezione esplora le categorie chiave delle minacce hardware, offrendo approfondimenti sulle loro origini, metodi e implicazioni per i sistemi di ML.</p>
<div id="tbl-threat_types" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-threat_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;14.1: Tipi di minaccia alla sicurezza hardware.
</figcaption>
<div aria-describedby="tbl-threat_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 56%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Tipo di minaccia</th>
<th style="text-align: left;">Descrizione</th>
<th style="text-align: left;">Rilevanza per la sicurezza hardware ML</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bug Hardware</td>
<td style="text-align: left;">Difetti intrinseci nelle progettazioni hardware che possono compromettere l‚Äôintegrit√† del sistema.</td>
<td style="text-align: left;">Fondamento della vulnerabilit√† hardware.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Attacchi Fisici</td>
<td style="text-align: left;">Sfruttamento diretto dell‚Äôhardware tramite accesso fisico o manipolazione.</td>
<td style="text-align: left;">Modello di minaccia basilare e palese.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Attacchi di Injection di Guasti</td>
<td style="text-align: left;">Induzione di guasti per causare errori nel funzionamento dell‚Äôhardware, portando a potenziali crash di sistema.</td>
<td style="text-align: left;">Manipolazione sistematica che porta al guasto.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Attacchi a Canale Laterale</td>
<td style="text-align: left;">Sfruttamento di informazioni sul funzionamento dell‚Äôhardware per estrarre dati sensibili.</td>
<td style="text-align: left;">Attacco indiretto tramite osservazione ambientale.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Interfacce con Perdite</td>
<td style="text-align: left;">Vulnerabilit√† derivanti da interfacce che espongono i dati in modo involontario.</td>
<td style="text-align: left;">Esposizione dei dati tramite canali di comunicazione.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hardware Contraffatto</td>
<td style="text-align: left;">Utilizzo di componenti hardware non autorizzati che potrebbero presentare falle di sicurezza.</td>
<td style="text-align: left;">Problemi di vulnerabilit√† aggravati.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Rischi della Catena di Fornitura</td>
<td style="text-align: left;">Rischi introdotti durante il ciclo di vita dell‚Äôhardware, dalla produzione alla distribuzione.</td>
<td style="text-align: left;">Sfide di sicurezza cumulative e multiformi.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="bug-hardware" class="level3 page-columns page-full" data-number="14.5.1">
<h3 data-number="14.5.1" class="anchored" data-anchor-id="bug-hardware"><span class="header-section-number">14.5.1</span> Bug Hardware</h3>
<p>L‚Äôhardware non √® immune al problema pervasivo di difetti di progettazione o bug. Gli aggressori possono sfruttare queste vulnerabilit√† per accedere, manipolare o estrarre dati sensibili, violando la riservatezza e l‚Äôintegrit√† da cui dipendono utenti e servizi. Un esempio di tali vulnerabilit√† √® venuto alla luce con la scoperta di <a href="https://meltdownattack.com/">Meltdown e Spectre</a>, due vulnerabilit√† hardware che sfruttano vulnerabilit√† critiche nei processori moderni. Questi bug consentono agli aggressori di aggirare la barriera hardware che separa le applicazioni, consentendo a un programma dannoso di leggere la memoria di altri programmi e del sistema operativo.</p>
<p>Meltdown <span class="citation" data-cites="Lipp2018meltdown">(<a href="../../../references.it.html#ref-Lipp2018meltdown" role="doc-biblioref">Kocher et al. 2019a</a>)</span> e Spectre <span class="citation" data-cites="Kocher2018spectre">(<a href="../../../references.it.html#ref-Kocher2018spectre" role="doc-biblioref">Kocher et al. 2019b</a>)</span> funzionano sfruttando le ottimizzazioni nelle CPU moderne che consentono loro di eseguire istruzioni speculative fuori ordine prima che i controlli di validit√† siano stati completati. Ci√≤ rivela dati che dovrebbero essere inaccessibili, che l‚Äôattacco cattura tramite canali laterali come le cache. La complessit√† tecnica dimostra la difficolt√† di eliminare le vulnerabilit√† anche con una validazione estesa.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Lipp2018meltdown" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî, et al. 2019a. <span>¬´Spectre Attacks: Exploiting Speculative Execution¬ª</span>. In <em>2019 IEEE Symposium on Security and Privacy (SP)</em>, 1‚Äì19. IEEE. <a href="https://doi.org/10.1109/sp.2019.00002">https://doi.org/10.1109/sp.2019.00002</a>.
</div><div id="ref-Kocher2018spectre" class="csl-entry" role="listitem">
Kocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, et al. 2019b. <span>¬´Spectre Attacks: Exploiting Speculative Execution¬ª</span>. In <em>2019 IEEE Symposium on Security and Privacy (SP)</em>, 1‚Äì19. IEEE. <a href="https://doi.org/10.1109/sp.2019.00002">https://doi.org/10.1109/sp.2019.00002</a>.
</div></div><p>Se un sistema ML elabora dati sensibili, come informazioni personali degli utenti o analisi aziendali proprietarie, Meltdown e Spectre rappresentano un pericolo reale e presente per la sicurezza dei dati. Consideriamo il caso di una scheda acceleratrice ML progettata per velocizzare i processi di apprendimento automatico, come quelli di cui abbiamo parlato nel capitolo <a href="../../../contents/core/hw_acceleration/hw_acceleration.it.html">Accelerazione IA</a>. Questi acceleratori lavorano con la CPU per gestire calcoli complessi, spesso correlati all‚Äôanalisi dei dati, al riconoscimento delle immagini e all‚Äôelaborazione del linguaggio naturale. Se una scheda acceleratrice di questo tipo presenta una vulnerabilit√† simile a Meltdown o Spectre, potrebbe far trapelare i dati che elabora. Un aggressore potrebbe sfruttare questa falla non solo per sottrarre dati, ma anche per ottenere informazioni sul funzionamento del modello ML, incluso potenzialmente il reverse engineering del modello stesso (tornando quindi al problema del <a href="@sec-model_theft">furto di modelli</a>.</p>
<p>Uno scenario reale in cui ci√≤ potrebbe essere devastante sarebbe nel settore sanitario. I sistemi ML elaborano regolarmente dati altamente sensibili dei pazienti per aiutare a diagnosticare, pianificare il trattamento e prevedere i risultati. Un bug nell‚Äôhardware del sistema potrebbe portare alla divulgazione non autorizzata di informazioni sanitarie personali, violando la privacy del paziente e contravvenendo a rigidi standard normativi come l‚Äô<a href="https://www.cdc.gov/phlp/publications/topic/hipaa.html">Health Insurance Portability and Accountability Act (HIPAA)</a></p>
<p>Le vulnerabilit√† Meltdown e Spectre sono un duro promemoria del fatto che la sicurezza hardware non consiste solo nel prevenire l‚Äôaccesso fisico non autorizzato, ma anche nel garantire che l‚Äôarchitettura dell‚Äôhardware non diventi un canale per l‚Äôesposizione dei dati. Difetti di progettazione hardware simili emergono regolarmente in CPU, acceleratori, memoria, bus e altri componenti. Ci√≤ richiede continue mitigazioni retroattive e compromessi sulle prestazioni nei sistemi distribuiti. Soluzioni proattive come le architetture di elaborazione confidenziale potrebbero mitigare intere classi di vulnerabilit√† attraverso una progettazione hardware fondamentalmente pi√π sicura. Contrastare i bug hardware richiede rigore in ogni fase di progettazione, validazione e distribuzione.</p>
</section>
<section id="attacchi-fisici" class="level3" data-number="14.5.2">
<h3 data-number="14.5.2" class="anchored" data-anchor-id="attacchi-fisici"><span class="header-section-number">14.5.2</span> Attacchi Fisici</h3>
<p>La manomissione fisica si riferisce alla manipolazione diretta e non autorizzata di risorse informatiche fisiche per minare l‚Äôintegrit√† dei sistemi di apprendimento automatico. √à un attacco particolarmente insidioso perch√© aggira le tradizionali misure di sicurezza informatica, che spesso si concentrano pi√π sulle vulnerabilit√† del software che sulle minacce hardware.</p>
<p>La manomissione fisica pu√≤ assumere molte forme, da quelle relativamente semplici, come l‚Äôinserimento di un dispositivo USB caricato con software dannoso in un server, a quelle altamente sofisticate, come l‚Äôinclusione di un Trojan hardware durante il processo di produzione di un microchip (discusso pi√π avanti in dettaglio nella sezione Supply Chain). I sistemi ML sono suscettibili a questo attacco perch√© si basano sull‚Äôaccuratezza e l‚Äôintegrit√† del loro hardware per elaborare e analizzare correttamente grandi quantit√† di dati.</p>
<p>Si consideri un drone alimentato da ML utilizzato per la mappatura geografica. Il funzionamento del drone si basa su una serie di sistemi di bordo, tra cui un modulo di navigazione che elabora gli input da vari sensori per determinare il suo percorso. Se un aggressore ottiene l‚Äôaccesso fisico a questo drone, potrebbe sostituire il modulo di navigazione originale con uno compromesso che include una backdoor. Questo modulo manipolato potrebbe quindi alterare la traiettoria di volo del drone per condurre la sorveglianza su aree riservate o persino contrabbandare merci di contrabbando volando su rotte non rilevate.</p>
<p>Un altro esempio √® la manomissione fisica degli scanner biometrici utilizzati per il controllo degli accessi in strutture sicure. Introducendo un sensore modificato che trasmette dati biometrici a un ricevitore non autorizzato, un aggressore pu√≤ accedere ai dati di identificazione personale per autenticare gli individui.</p>
<p>Esistono diversi modi in cui la manomissione fisica pu√≤ verificarsi nell‚Äôhardware ML:</p>
<ul>
<li><p><strong>Manipolazione dei sensori:</strong> Si consideri un veicolo autonomo dotato di telecamere e LiDAR per la percezione ambientale. Un malintenzionato potrebbe manipolare deliberatamente l‚Äôallineamento fisico di questi sensori per creare zone di occlusione o distorcere le misure della distanza. Ci√≤ potrebbe compromettere le capacit√† di rilevamento degli oggetti e potenzialmente mettere in pericolo gli occupanti del veicolo.</p></li>
<li><p><strong>Trojan hardware:</strong> Le modifiche dannose ai circuiti possono introdurre trojan progettati per attivarsi in base a specifiche condizioni di input. Ad esempio, un chip acceleratore ML potrebbe funzionare come previsto fino a quando non incontra un trigger predeterminato, dopodich√© si comporta in modo irregolare.</p></li>
<li><p><strong>Manomissione della memoria:</strong> L‚Äôesposizione fisica e la manipolazione dei chip di memoria potrebbero consentire l‚Äôestrazione di parametri del modello ML crittografati. Le tecniche di iniezione di guasti possono anche corrompere i dati del modello per degradare l‚Äôaccuratezza.</p></li>
<li><p><strong>Introduzione di backdoor:</strong> Ottenendo l‚Äôaccesso fisico ai server, un avversario potrebbe utilizzare keylogger hardware per catturare password e creare account backdoor per l‚Äôaccesso persistente. Questi potrebbero poi essere utilizzati per esfiltrare dati di training ML nel tempo.</p></li>
<li><p><strong>Attacchi alla supply chain:</strong> Manipolare componenti hardware di terze parti o compromettere i canali di produzione e spedizione crea vulnerabilit√† sistemiche difficili da rilevare e correggere.</p></li>
</ul>
</section>
<section id="attacchi-di-fault-injection" class="level3 page-columns page-full" data-number="14.5.3">
<h3 data-number="14.5.3" class="anchored" data-anchor-id="attacchi-di-fault-injection"><span class="header-section-number">14.5.3</span> Attacchi di Fault-injection</h3>
<p>Introducendo intenzionalmente guasti nell‚Äôhardware ML, gli aggressori possono indurre errori nel processo di elaborazione, portando a output non corretti. Questa manipolazione compromette l‚Äôintegrit√† delle operazioni ML e pu√≤ fungere da vettore per ulteriori sfruttamenti, come il reverse engineering del sistema o il bypass del protocollo di sicurezza. L‚Äôiniezione di guasti comporta l‚Äôinterruzione deliberata delle operazioni di elaborazione standard in un sistema tramite interferenze esterne <span class="citation" data-cites="joye2012fault">(<a href="../../../references.it.html#ref-joye2012fault" role="doc-biblioref">Joye e Tunstall 2012</a>)</span>. Attivando con precisione gli errori di elaborazione, gli avversari possono alterare l‚Äôesecuzione del programma in modi che degradano l‚Äôaffidabilit√† o trapelano informazioni sensibili.</p>
<div class="no-row-height column-margin column-container"><div id="ref-joye2012fault" class="csl-entry" role="listitem">
Joye, Marc, e Michael Tunstall. 2012. <em>Fault Analysis in Cryptography</em>. Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-642-29656-7">https://doi.org/10.1007/978-3-642-29656-7</a>.
</div><div id="ref-barenghi2010low" class="csl-entry" role="listitem">
Barenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro Pellicioli, e Gerardo Pelosi. 2010. <span>¬´Low voltage fault attacks to AES¬ª</span>. In <em>2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST)</em>, 7‚Äì12. IEEE; IEEE. <a href="https://doi.org/10.1109/hst.2010.5513121">https://doi.org/10.1109/hst.2010.5513121</a>.
</div><div id="ref-hutter2009contact" class="csl-entry" role="listitem">
Hutter, Michael, Jorn-Marc Schmidt, e Thomas Plos. 2009. <span>¬´Contact-based fault injections and power analysis on RFID tags¬ª</span>. In <em>2009 European Conference on Circuit Theory and Design</em>, 409‚Äì12. IEEE; IEEE. <a href="https://doi.org/10.1109/ecctd.2009.5275012">https://doi.org/10.1109/ecctd.2009.5275012</a>.
</div><div id="ref-amiel2006fault" class="csl-entry" role="listitem">
Amiel, Frederic, Christophe Clavier, e Michael Tunstall. 2006. <span>¬´Fault Analysis of DPA-Resistant Algorithms¬ª</span>. In <em>Fault Diagnosis and Tolerance in Cryptography</em>, 223‚Äì36. Springer; Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/11889700\_20">https://doi.org/10.1007/11889700\_20</a>.
</div><div id="ref-agrawal2003side" class="csl-entry" role="listitem">
Agrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, e Berk Sunar. 2007. <span>¬´Trojan Detection using IC Fingerprinting¬ª</span>. In <em>2007 IEEE Symposium on Security and Privacy (SP ‚Äô07)</em>, 296‚Äì310. Springer; IEEE. <a href="https://doi.org/10.1109/sp.2007.36">https://doi.org/10.1109/sp.2007.36</a>.
</div><div id="ref-skorobogatov2009local" class="csl-entry" role="listitem">
Skorobogatov, Sergei. 2009. <span>¬´Local heating attacks on Flash memory devices¬ª</span>. In <em>2009 IEEE International Workshop on Hardware-Oriented Security and Trust</em>, 1‚Äì6. IEEE; IEEE. <a href="https://doi.org/10.1109/hst.2009.5225028">https://doi.org/10.1109/hst.2009.5225028</a>.
</div><div id="ref-skorobogatov2003optical" class="csl-entry" role="listitem">
Skorobogatov, Sergei P., e Ross J. Anderson. 2002. <span>¬´Optical Fault Induction Attacks.¬ª</span> In <em>Cryptographic Hardware and Embedded Systems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA, August 13‚Äì15, 2002 Revised Papers 4</em>, 2‚Äì12. Springer. <a href="https://doi.org/10.1007/3-540-36400-5\_2">https://doi.org/10.1007/3-540-36400-5\_2</a>.
</div></div><p>Per l‚Äôiniezione di guasti possono essere utilizzate varie tecniche di manomissione fisica. Bassa tensione <span class="citation" data-cites="barenghi2010low">(<a href="../../../references.it.html#ref-barenghi2010low" role="doc-biblioref">Barenghi et al. 2010</a>)</span>, picchi di potenza <span class="citation" data-cites="hutter2009contact">(<a href="../../../references.it.html#ref-hutter2009contact" role="doc-biblioref">Hutter, Schmidt, e Plos 2009</a>)</span>, anomalie di clock <span class="citation" data-cites="amiel2006fault">(<a href="../../../references.it.html#ref-amiel2006fault" role="doc-biblioref">Amiel, Clavier, e Tunstall 2006</a>)</span>, impulsi elettromagnetici <span class="citation" data-cites="agrawal2003side">(<a href="../../../references.it.html#ref-agrawal2003side" role="doc-biblioref">Agrawal et al. 2007</a>)</span>, aumento della temperatura <span class="citation" data-cites="skorobogatov2009local">(<a href="../../../references.it.html#ref-skorobogatov2009local" role="doc-biblioref">S. Skorobogatov 2009</a>)</span> e colpi laser <span class="citation" data-cites="skorobogatov2003optical">(<a href="../../../references.it.html#ref-skorobogatov2003optical" role="doc-biblioref">S. P. Skorobogatov e Anderson 2002</a>)</span> sono comuni vettori di attacco hardware. Sono programmati con precisione per indurre guasti come bit invertiti o istruzioni saltate durante operazioni critiche.</p>
<p>Per i sistemi ML, le conseguenze includono una precisione del modello compromessa, negazione del servizio, estrazione di dati di training privati o parametri del modello e reverse engineering delle architetture del modello. Gli aggressori potrebbero utilizzare l‚Äôiniezione di guasti per forzare classificazioni errate, interrompere sistemi autonomi o rubare propriet√† intellettuale.</p>
<p>Ad esempio, <span class="citation" data-cites="breier2018deeplaser">Breier et al. (<a href="../../../references.it.html#ref-breier2018deeplaser" role="doc-biblioref">2018</a>)</span> ha iniettato con successo un ‚Äúfault attack‚Äù in una rete neurale profonda distribuita su un microcontrollore. Hanno utilizzato un laser per riscaldare transistor specifici, costringendoli a cambiare stato. In un caso, hanno utilizzato questo metodo per attaccare una funzione di attivazione ReLU, con il risultato che la funzione emetteva sempre un valore di 0, indipendentemente dall‚Äôinput. Nel codice assembly mostrato in <a href="#fig-injection" class="quarto-xref">Figura&nbsp;<span>14.3</span></a>, l‚Äôattacco ha fatto s√¨ che il programma in esecuzione saltasse sempre l‚Äôistruzione <code>jmp end</code> alla riga 6. Ci√≤ significa che <code>HiddenLayerOutput[i]</code> √® sempre impostato su 0, sovrascrivendo eventuali valori scritti su di esso nelle righe 4 e 5. Di conseguenza, i neuroni mirati vengono resi inattivi, con conseguenti classificazioni errate.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-injection" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Fault-injection_demonstrated_with_assembly_code.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.3: Iniezione di errore dimostrata con codice assembly. Fonte: <span class="citation" data-cites="breier2018deeplaser">Breier et al. (<a href="../../../references.it.html#ref-breier2018deeplaser" role="doc-biblioref">2018</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-breier2018deeplaser" class="csl-entry" role="listitem">
Breier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, e Yang Liu. 2018. <span>¬´DeepLaser: Practical Fault Attack on Deep Neural Networks¬ª</span>. <em>ArXiv preprint</em> abs/1806.05859 (giugno). <a href="http://arxiv.org/abs/1806.05859v2">http://arxiv.org/abs/1806.05859v2</a>.
</div></div></figure>
</div>
<p>La strategia di un aggressore potrebbe essere quella di dedurre informazioni sulle funzioni di attivazione tramite attacchi side-channel (discussi in seguito). Quindi, l‚Äôaggressore potrebbe tentare di colpire pi√π calcoli di funzioni di attivazione iniettando casualmente guasti nei livelli il pi√π vicino possibile al livello di output, aumentando la probabilit√† e l‚Äôimpatto dell‚Äôattacco.</p>
<p>I dispositivi embedded sono particolarmente vulnerabili a causa di un limitato rafforzamento fisico e di vincoli di risorse che limitano le difese di runtime robuste. Senza un packaging antimanomissione, l‚Äôaccesso dell‚Äôaggressore ai bus di sistema e alla memoria consente di infierire guasti precisi. Anche i modelli ML embedded leggeri mancano di ridondanza per bypassare gli errori.</p>
<p>Questi attacchi possono essere particolarmente insidiosi perch√© aggirano le tradizionali misure di sicurezza basate su software, spesso non tenendo conto delle interruzioni fisiche. Inoltre, poich√© i sistemi ML si basano in larga misura sull‚Äôaccuratezza e l‚Äôaffidabilit√† del loro hardware per attivit√† come il riconoscimento di pattern, il processo decisionale e le risposte automatiche, qualsiasi compromesso nel loro funzionamento dovuto all‚Äôiniezione di guasti pu√≤ avere conseguenze gravi e di vasta portata.</p>
<p>Per mitigare i rischi di iniezione di guasti √® necessario un approccio multi-layer. Il rafforzamento fisico tramite custodie antimanomissione e offuscamento del design aiuta a ridurre l‚Äôaccesso. Il rilevamento di leggere anomalie pu√≤ identificare input di sensori insoliti o output di modelli errati <span class="citation" data-cites="hsiao2023mavfi">(<a href="../../../references.it.html#ref-hsiao2023mavfi" role="doc-biblioref">Hsiao et al. 2023</a>)</span>. Le memorie con correzione degli errori riducono al minimo le interruzioni, mentre la crittografia dei dati salvaguarda le informazioni. Le tecniche emergenti di watermarking dei modelli tracciano i parametri rubati.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hsiao2023mavfi" class="csl-entry" role="listitem">
Hsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. <span>¬´MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles¬ª</span>. In <em>2023 Design, Automation &amp;amp; Test in Europe Conference &amp;amp; Exhibition (DATE)</em>, 1‚Äì6. IEEE; IEEE. <a href="https://doi.org/10.23919/date56975.2023.10137246">https://doi.org/10.23919/date56975.2023.10137246</a>.
</div></div><p>Tuttavia, bilanciare protezioni robuste con i limiti di dimensioni e potenza ristretti dei sistemi embedded rimane una sfida. I limiti della crittografia e la mancanza di coprocessori sicuri su hardware embedded sensibile ai costi limitano le opzioni. In definitiva, la resilienza all‚Äôiniezione di guasti richiede una prospettiva multi-layer che abbraccia i layer di progettazione elettrica, firmware, software e fisica.</p>
</section>
<section id="attacchi-a-canale-laterale" class="level3 page-columns page-full" data-number="14.5.4">
<h3 data-number="14.5.4" class="anchored" data-anchor-id="attacchi-a-canale-laterale"><span class="header-section-number">14.5.4</span> Attacchi a canale laterale</h3>
<p>Gli attacchi side-channel costituiscono una classe di violazioni della sicurezza che sfruttano informazioni rivelate inavvertitamente tramite l‚Äôimplementazione fisica dei sistemi informatici. Contrariamente agli attacchi diretti che prendono di mira vulnerabilit√† software o di rete, questi attacchi sfruttano le caratteristiche hardware intrinseche del sistema per estrarre informazioni sensibili.</p>
<p>La premessa fondamentale di un attacco side-channel √® che il funzionamento di un dispositivo pu√≤ rivelare inavvertitamente informazioni. Tali fughe possono provenire da varie fonti, tra cui l‚Äôenergia elettrica consumata da un dispositivo <span class="citation" data-cites="kocher1999differential">(<a href="../../../references.it.html#ref-kocher1999differential" role="doc-biblioref">Kocher, Jaffe, e Jun 1999</a>)</span>, i campi elettromagnetici che emette <span class="citation" data-cites="gandolfi2001electromagnetic">(<a href="../../../references.it.html#ref-gandolfi2001electromagnetic" role="doc-biblioref">Gandolfi, Mourtel, e Olivier 2001</a>)</span>, il tempo necessario per elaborare determinate operazioni o persino i suoni che produce. Ogni canale pu√≤ intravedere indirettamente i processi interni del sistema, rivelando informazioni che possono compromettere la sicurezza.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kocher1999differential" class="csl-entry" role="listitem">
Kocher, Paul, Joshua Jaffe, e Benjamin Jun. 1999. <span>¬´Differential Power Analysis¬ª</span>. In <em>Advances in Cryptology ‚Äî CRYPTO‚Äô 99</em>, 388‚Äì97. Springer; Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/3-540-48405-1\_25">https://doi.org/10.1007/3-540-48405-1\_25</a>.
</div><div id="ref-gandolfi2001electromagnetic" class="csl-entry" role="listitem">
Gandolfi, Karine, Christophe Mourtel, e Francis Olivier. 2001. <span>¬´Electromagnetic Analysis: Concrete Results¬ª</span>. In <em>Cryptographic Hardware and Embedded Systems ‚Äî CHES 2001</em>, 251‚Äì61. Springer; Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/3-540-44709-1\_21">https://doi.org/10.1007/3-540-44709-1\_21</a>.
</div><div id="ref-Kocher2011Intro" class="csl-entry" role="listitem">
Kocher, Paul, Joshua Jaffe, Benjamin Jun, e Pankaj Rohatgi. 2011. <span>¬´Introduction to differential power analysis¬ª</span>. <em>Journal of Cryptographic Engineering</em> 1 (1): 5‚Äì27. <a href="https://doi.org/10.1007/s13389-011-0006-y">https://doi.org/10.1007/s13389-011-0006-y</a>.
</div></div><p>Si consideri un sistema di apprendimento automatico che esegue transazioni crittografate. Gli algoritmi di crittografia sono progettati per proteggere i dati, ma richiedono un lavoro computazionale per crittografare e de-crittografare le informazioni. Uno standard di crittografia ampiamente utilizzato √® l‚ÄôAdvanced Encryption Standard (AES), che crittografa i dati per impedire l‚Äôaccesso non autorizzato. Tuttavia, gli aggressori possono analizzare i modelli di consumo energetico di un dispositivo che esegue la crittografia per dedurre informazioni sensibili, come la chiave crittografica. Con metodi statistici sofisticati, piccole variazioni nel consumo energetico durante il processo di crittografia possono essere correlate ai dati in fase di elaborazione, rivelando infine la chiave. Alcune tecniche di attacco di analisi differenziale sono ‚ÄúDifferential Power Analysis (DPA)‚Äù <span class="citation" data-cites="Kocher2011Intro">(<a href="../../../references.it.html#ref-Kocher2011Intro" role="doc-biblioref">Kocher et al. 2011</a>)</span>, ‚ÄúDifferential Electromagnetic Analysis (DEMA)‚Äù e ‚ÄúCorrelation Power Analysis (CPA)‚Äù.</p>
<p>Un aggressore che tenta di violare la crittografia AES potrebbe raccogliere tracce di potenza o elettromagnetiche (registrazioni di consumi o emissioni di energia) dal dispositivo mentre esegue la crittografia. Analizzando queste tracce con tecniche statistiche, l‚Äôaggressore potrebbe identificare correlazioni tra le tracce e il testo in chiaro (testo originale non crittografato) o il testo cifrato (testo crittografato). Queste correlazioni potrebbero quindi essere utilizzate per dedurre singoli bit della chiave AES e, alla fine, ricostruire l‚Äôintera chiave. Gli attacchi di analisi differenziale sono particolarmente pericolosi perch√© sono economici, efficaci e non intrusivi, consentendo agli aggressori di aggirare le misure di sicurezza algoritmiche e a livello hardware. Anche le compromissioni tramite questi attacchi sono difficili da rilevare, poich√© non alterano fisicamente il dispositivo n√© violano l‚Äôalgoritmo di crittografia stesso.</p>
<p>Di seguito, una visualizzazione semplificata illustra come l‚Äôanalisi dei pattern di consumo energetico del dispositivo di crittografia pu√≤ aiutare a estrarre informazioni sulle operazioni dell‚Äôalgoritmo e, a sua volta, sui dati segreti. L‚Äôesempio mostra un dispositivo che accetta una password di 5 byte come input. La password immessa in questo scenario √® <code>0x61, 0x52, 0x77, 0x6A, 0x73</code>, che rappresenta la password corretta. I modelli di consumo energetico durante l‚Äôautenticazione forniscono informazioni su come funziona l‚Äôalgoritmo.</p>
<p>In <a href="#fig-encryption" class="quarto-xref">Figura&nbsp;<span>14.4</span></a>, la forma d‚Äôonda rossa rappresenta le linee di dati seriali mentre il bootloader riceve i dati della password in blocchi (ad esempio <code>0x61, 0x52, 0x77, 0x6A, 0x73</code>). Ciascun segmento etichettato (ad esempio, ‚ÄúData: 61‚Äù) corrisponde a un byte della password elaborata dall‚Äôalgoritmo di crittografia. Il grafico blu mostra il consumo energetico del dispositivo di crittografia mentre elabora ogni byte. Quando viene inserita la password corretta, il dispositivo elabora tutti i 5 byte con successo e il grafico della tensione blu mostra modelli coerenti in tutto. Questo grafico fornisce una linea di base per comprendere come appare il consumo energetico del dispositivo quando viene inserita una password corretta. Nelle figure successive, si vedr√† come cambia il profilo energetico con password errate, aiutando a individuare le differenze nel comportamento del dispositivo quando l‚Äôautenticazione fallisce.</p>
<div id="fig-encryption" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encryption-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Power_analysis_of_an_encryption_device_with_a_correct_password.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encryption-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.4: Profilo del consumo energetico del dispositivo durante le normali operazioni con una password valida di 5 byte (0x61, 0x52, 0x77, 0x6A, 0x73). La linea rossa rappresenta i dati seriali ricevuti dal bootloader, che in questa figura riceve i byte corretti. Notare come la linea blu, che rappresenta il consumo energetico durante l‚Äôautenticazione, corrisponda alla ricezione e alla verifica dei byte. Nelle figure successive, questo profilo di consumo energetico blu cambier√†. Fonte: <a href="https://www.youtube.com/watch?v=2iDLfuEBcs8">Colin O‚ÄôFlynn.</a>
</figcaption>
</figure>
</div>
<p>Quando viene inserita una password errata, il grafico dell‚Äôanalisi della potenza √® mostrato in <a href="#fig-encryption2" class="quarto-xref">Figura&nbsp;<span>14.5</span></a>. I primi tre byte della password sono corretti (ad esempio <code>0x61, 0x52, 0x77</code>). Di conseguenza, i pattern di tensione sono molto simili o identici tra i due grafici, fino al quarto byte incluso. Dopo aver elaborato il quarto byte (<code>0x42</code>), il dispositivo rileva una mancata corrispondenza con la password corretta e interrompe l‚Äôulteriore elaborazione. Ci√≤ determina un cambiamento evidente nel pattern di alimentazione, mostrato dal salto improvviso nella linea blu all‚Äôaumentare della tensione.</p>
<div id="fig-encryption2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encryption2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Power_analysis_of_an_encryption_device_with_a_(partially)_wrong_password.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encryption2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.5: Profilo di consumo energetico del dispositivo quando viene inserita una password errata di 5 byte (0x61, 0x52, 0x77, 0x42, 0x42). La linea rossa rappresenta i dati seriali ricevuti dal bootloader, che mostrano i byte di input in fase di elaborazione. I primi tre byte (0x61, 0x52, 0x77) sono corretti e corrispondono alla password prevista, come indicato dalla linea blu coerente del consumo energetico. Tuttavia, durante l‚Äôelaborazione del quarto byte (0x42), viene rilevata una mancata corrispondenza. Il bootloader interrompe l‚Äôulteriore elaborazione, con conseguente salto evidente nella linea blu del consumo energetico, poich√© il dispositivo interrompe l‚Äôautenticazione ed entra in uno stato di errore. Fonte: <a href="https://www.youtube.com/watch?v=2iDLfuEBcs8">Colin O‚ÄôFlynn.</a>
</figcaption>
</figure>
</div>
<p>La <a href="#fig-encryption3" class="quarto-xref">Figura&nbsp;<span>14.6</span></a> mostra un altro esempio, ma in cui la password √® completamente errata (<code>0x30, 0x30, 0x30, 0x30, 0x30</code>), a differenza dell‚Äôesempio precedente con i primi tre byte corretti. Qui, il dispositivo identifica la mancata corrispondenza subito dopo l‚Äôelaborazione del primo byte e interrompe l‚Äôulteriore elaborazione. Ci√≤ si riflette nel profilo di consumo energetico, in cui la linea blu mostra un brusco salto dopo il primo byte, indicando la conclusione anticipata dell‚Äôautenticazione del dispositivo.</p>
<div id="fig-encryption3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encryption3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Power_analysis_of_an_encryption_device_with_a_wrong_password.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encryption3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.6: Profilo di consumo energetico del dispositivo quando viene inserita una password completamente errata (0x30, 0x30, 0x30, 0x30, 0x30). La linea blu mostra un brusco salto dopo l‚Äôelaborazione del primo byte, indicando che il dispositivo ha interrotto il processo di autenticazione. Fonte: <a href="https://www.youtube.com/watch?v=2iDLfuEBcs8">Colin O‚ÄôFlynn.</a>
</figcaption>
</figure>
</div>
<p>L‚Äôesempio sopra dimostra come le informazioni sul processo di crittografia e sulla chiave segreta possono essere dedotte analizzando diversi input e varianti di test di forza bruta di ogni byte della password, in pratica ‚Äúintercettando‚Äù le operazioni del dispositivo. Per una spiegazione pi√π dettagliata, guardare <a href="#vid-powerattack" class="quarto-xref">Video&nbsp;<span>14.3</span></a> di seguito.</p>
<div id="vid-powerattack" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;14.3: Power Attack
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/2iDLfuEBcs8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
<p>Un altro esempio √® un sistema ML per il riconoscimento vocale, che elabora i comandi vocali per eseguire azioni. Misurando la latenza del sistema per rispondere ai comandi o la potenza utilizzata durante l‚Äôelaborazione, un aggressore potrebbe dedurre quali comandi vengono elaborati e quindi apprendere i pattern operativi del sistema. Ancora pi√π sottilmente, il suono emesso dalla ventola o dal disco rigido di un computer potrebbe cambiare in risposta al carico di lavoro, che un microfono sensibile potrebbe captare e analizzare per determinare che tipo di operazioni vengono eseguite.</p>
<p>In scenari reali, gli attacchi side-channel hanno effettivamente estratto chiavi di crittografia e compromesso comunicazioni sicure. Uno dei primi casi registrati di un simile attacco si √® verificato negli anni ‚Äô60, quando l‚Äôagenzia di intelligence britannica MI5 ha affrontato la sfida di decifrare comunicazioni crittografate dall‚Äôambasciata egiziana a Londra. I loro sforzi di decifrazione dei codici sono stati inizialmente ostacolati dalle limitazioni computazionali dell‚Äôepoca, fino a quando un‚Äôingegnosa osservazione dell‚Äôagente MI5 Peter Wright ha alterato il corso dell‚Äôoperazione.</p>
<p>L‚Äôagente dell‚ÄôMI5 Peter Wright propose di usare un microfono per catturare le sottili firme acustiche emesse dalla macchina di cifratura del rotore dell‚Äôambasciata durante la crittografia <span class="citation" data-cites="Burnet1989Spycatcher">(<a href="../../../references.it.html#ref-Burnet1989Spycatcher" role="doc-biblioref">Burnet e Thomas 1989</a>)</span>. I distinti clic meccanici dei rotori mentre gli operatori li configuravano quotidianamente facevano trapelare informazioni critiche sulle impostazioni iniziali. Questo semplice ‚Äúcanale laterale‚Äù del suono ha permesso all‚ÄôMI5 di ridurre drasticamente la complessit√† della decifrazione dei messaggi. Questo primo attacco di ‚Äúperdita‚Äù acustica evidenzia che gli attacchi a canale laterale non sono semplicemente una novit√† dell‚Äôera digitale, ma una continuazione di antichi principi di crittoanalisi. L‚Äôidea che dove c‚Äô√® un segnale, c‚Äô√® un‚Äôopportunit√† di intercettazione rimane fondamentale. Dai clic meccanici alle fluttuazioni elettriche e oltre, i canali laterali consentono agli avversari di estrarre segreti indirettamente attraverso un‚Äôattenta analisi del segnale.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Burnet1989Spycatcher" class="csl-entry" role="listitem">
Burnet, David, e Richard Thomas. 1989. <span>¬´Spycatcher: The Commodification of Truth¬ª</span>. <em>Journal of Law and Society</em> 16 (2): 210. <a href="https://doi.org/10.2307/1410360">https://doi.org/10.2307/1410360</a>.
</div><div id="ref-Asonov2004Keyboard" class="csl-entry" role="listitem">
Asonov, D., e R. Agrawal. s.d. <span>¬´Keyboard acoustic emanations¬ª</span>. In <em>IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004</em>, 3‚Äì11. IEEE; IEEE. <a href="https://doi.org/10.1109/secpri.2004.1301311">https://doi.org/10.1109/secpri.2004.1301311</a>.
</div><div id="ref-gnad2017voltage" class="csl-entry" role="listitem">
Gnad, Dennis R. E., Fabian Oboril, e Mehdi B. Tahoori. 2017. <span>¬´Voltage drop-based fault attacks on FPGAs using valid bitstreams¬ª</span>. In <em>2017 27th International Conference on Field Programmable Logic and Applications (FPL)</em>, 1‚Äì7. IEEE; IEEE. <a href="https://doi.org/10.23919/fpl.2017.8056840">https://doi.org/10.23919/fpl.2017.8056840</a>.
</div><div id="ref-zhao2018fpga" class="csl-entry" role="listitem">
Zhao, Mark, e G. Edward Suh. 2018. <span>¬´FPGA-Based Remote Power Side-Channel Attacks¬ª</span>. In <em>2018 IEEE Symposium on Security and Privacy (SP)</em>, 229‚Äì44. IEEE; IEEE. <a href="https://doi.org/10.1109/sp.2018.00049">https://doi.org/10.1109/sp.2018.00049</a>.
</div></div><p>Oggi, la crittoanalisi acustica si √® evoluta in attacchi come l‚Äôintercettazione della tastiera <span class="citation" data-cites="Asonov2004Keyboard">(<a href="../../../references.it.html#ref-Asonov2004Keyboard" role="doc-biblioref">Asonov e Agrawal, s.d.</a>)</span>. I canali laterali elettrici spaziano dall‚Äôanalisi della potenza su hardware crittografico <span class="citation" data-cites="gnad2017voltage">(<a href="../../../references.it.html#ref-gnad2017voltage" role="doc-biblioref">Gnad, Oboril, e Tahoori 2017</a>)</span> alle fluttuazioni di tensione <span class="citation" data-cites="zhao2018fpga">(<a href="../../../references.it.html#ref-zhao2018fpga" role="doc-biblioref">Zhao e Suh 2018</a>)</span> su acceleratori di machine learning. Anche tempistiche, emissioni elettromagnetiche e persino impronte di calore possono essere sfruttate. Nuovi e inaspettati canali laterali emergono spesso man mano che l‚Äôinformatica diventa pi√π interconnessa e miniaturizzata.</p>
<p>Proprio come la ‚Äúperdita‚Äù acustica analogica dell‚ÄôMI5 ha trasformato la loro decifrazione dei codici, i moderni attacchi ai canali laterali aggirano i confini tradizionali della difesa informatica. Comprendere lo spirito creativo e la persistenza storica degli exploit dei canali laterali √® una conoscenza fondamentale per sviluppatori e difensori che cercano di proteggere in modo completo i moderni sistemi di apprendimento automatico dalle minacce digitali e fisiche.</p>
</section>
<section id="interfacce-con-perdite" class="level3 page-columns page-full" data-number="14.5.5">
<h3 data-number="14.5.5" class="anchored" data-anchor-id="interfacce-con-perdite"><span class="header-section-number">14.5.5</span> Interfacce con Perdite</h3>
<p>Le interfacce ‚Äúleaky‚Äù nei sistemi embedded sono spesso backdoor trascurate che possono trasformarsi in significative vulnerabilit√† di sicurezza. Sebbene progettate per scopi legittimi come comunicazione, manutenzione o debug, queste interfacce possono inavvertitamente fornire agli aggressori una finestra attraverso la quale estrarre informazioni sensibili o iniettare dati dannosi.</p>
<p>Un‚Äôinterfaccia diventa ‚Äúleaky‚Äù quando espone pi√π informazioni del dovuto, spesso a causa della mancanza di rigorosi controlli di accesso o di una schermatura inadeguata dei dati trasmessi. Ecco alcuni esempi concreti di problemi di interfaccia leaky che causano problemi di sicurezza in dispositivi IoT ed embedded:</p>
<ul>
<li><p><strong>Baby Monitor:</strong> Molti baby monitor abilitati al WiFi hanno interfacce non protette per l‚Äôaccesso remoto. Ci√≤ ha consentito agli aggressori di ottenere feed audio e video in tempo reale dalle case delle persone, rappresentando una grave <a href="https://www.fox19.com/story/25310628/hacked-baby-monitor/">violazione della privacy</a>.</p></li>
<li><p><strong>Pacemaker:</strong> Sono state scoperte vulnerabilit√† dell‚Äôinterfaccia in alcuni <a href="https://www.fda.gov/medical-devices/medical-device-recalls/abbott-formally-known-st-jude-medical-recalls-assuritytm-and-enduritytm-pacemakers-potential">pacemaker</a> che potrebbero consentire agli aggressori di manipolare le funzioni cardiache se sfruttate. Ci√≤ presenta uno scenario potenzialmente letale.</p></li>
<li><p><strong>Lampadine Smart:</strong> Un ricercatore ha scoperto di poter accedere a dati non crittografati da lampadine intelligenti tramite un‚Äôinterfaccia di debug, comprese le credenziali WiFi, consentendogli di accedere alla rete connessa <span class="citation" data-cites="greengard2021internet">(<a href="../../../references.it.html#ref-greengard2021internet" role="doc-biblioref">Greengard 2021</a>)</span>.</p></li>
<li><p><strong>Auto Smart:</strong> Se non protetta, la porta di diagnostica OBD-II ha dimostrato di fornire un vettore di attacco ai sistemi automobilistici. Gli aggressori potrebbero usarlo per controllare i freni e altri componenti <span class="citation" data-cites="miller2015remote">(<a href="../../../references.it.html#ref-miller2015remote" role="doc-biblioref">Miller e Valasek 2015</a>)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-greengard2021internet" class="csl-entry" role="listitem">
Greengard, Samuel. 2021. <em>The Internet of Things</em>. The MIT Press. <a href="https://doi.org/10.7551/mitpress/13937.001.0001">https://doi.org/10.7551/mitpress/13937.001.0001</a>.
</div><div id="ref-miller2015remote" class="csl-entry" role="listitem">
Miller, Charlie, e Chris Valasek. 2015. <span>¬´Remote exploitation of an unaltered passenger vehicle¬ª</span>. <em>Black Hat USA</em> 2015 (S 91): 1‚Äì91.
</div></div><p>Sebbene quanto sopra non sia direttamente collegato al ML, si consideri l‚Äôesempio di un sistema di casa intelligente con un componente ML embedded che controlla la sicurezza domestica in base a pattern di comportamento che apprende nel tempo. Il sistema include un‚Äôinterfaccia di manutenzione accessibile tramite la rete locale per aggiornamenti software e controlli di sistema. Se questa interfaccia non richiede un‚Äôautenticazione forte o i dati trasmessi tramite essa non sono crittografati, un aggressore sulla stessa rete potrebbe ottenere l‚Äôaccesso. Potrebbero quindi intercettare le routine quotidiane del proprietario di casa o riprogrammare le impostazioni di sicurezza manipolando il firmware.</p>
<p>Tali ‚Äúfughe‚Äù rappresentano un problema di privacy e un potenziale punto di ingresso per exploit pi√π dannosi. L‚Äôesposizione di dati di training, parametri del modello o output ML da una ‚Äúfuga‚Äù potrebbe aiutare gli avversari a costruire esempi avversari o a sottoporre a reverse engineering i modelli. L‚Äôaccesso tramite un‚Äôinterfaccia con ‚Äúperdite‚Äù potrebbe anche essere utilizzato per modificare il firmware di un dispositivo embedded, caricandolo con codice dannoso che potrebbe spegnerlo, intercettare dati o utilizzarlo in attacchi botnet.</p>
<p>Per mitigare questi rischi, √® necessario un approccio multi-strato, che comprenda controlli tecnici quali autenticazione, crittografia, rilevamento delle anomalie, policy e processi come inventari di interfaccia, controlli di accesso, auditing e pratiche di sviluppo sicure. Disattivare le interfacce non necessarie e compartimentare i rischi tramite un modello zero-trust fornisce una protezione aggiuntiva.</p>
<p>Come progettisti di sistemi ML embedded, dovremmo valutare le interfacce nelle prime fasi dello sviluppo e monitorarle continuamente dopo l‚Äôimplementazione come parte di un ciclo di vita della sicurezza end-to-end. Comprendere e proteggere le interfacce √® fondamentale per garantire la sicurezza complessiva del ML embedded.</p>
</section>
<section id="hardware-contraffatto" class="level3" data-number="14.5.6">
<h3 data-number="14.5.6" class="anchored" data-anchor-id="hardware-contraffatto"><span class="header-section-number">14.5.6</span> Hardware Contraffatto</h3>
<p>I sistemi ML sono affidabili solo quanto l‚Äôhardware sottostante. In un‚Äôepoca in cui i componenti hardware sono beni di consumo globali, l‚Äôaumento di hardware contraffatti o clonati rappresenta una sfida significativa. L‚Äôhardware contraffatto comprende tutti i componenti che sono riproduzioni non autorizzate di parti originali. I componenti contraffatti si infiltrano nei sistemi ML attraverso complesse catene di fornitura che si estendono oltre i confini e coinvolgono numerose fasi dalla produzione alla consegna.</p>
<p>Anche una sola mancanza di integrit√† nella catena di fornitura pu√≤ comportare l‚Äôinserimento di parti contraffatte, progettate per imitare fedelmente le funzioni e l‚Äôaspetto dell‚Äôhardware originale. Ad esempio, un sistema di riconoscimento facciale per il controllo degli accessi ad alta sicurezza potrebbe essere compromesso se dotato di processori contraffatti. Questi processori potrebbero non riuscire a elaborare e verificare accuratamente i dati biometrici, consentendo potenzialmente a persone non autorizzate di accedere ad aree riservate.</p>
<p>La sfida con l‚Äôhardware contraffatto √® multiforme. Compromette la qualit√† e l‚Äôaffidabilit√† dei sistemi ML, poich√© questi componenti potrebbero degradarsi pi√π rapidamente o funzionare in modo imprevedibile a causa di una produzione scadente. Anche i rischi per la sicurezza sono profondi; l‚Äôhardware contraffatto pu√≤ contenere vulnerabilit√† pronte per essere sfruttate da malintenzionati. Ad esempio, un router di rete clonato in un data center ML potrebbe includere una backdoor nascosta, consentendo l‚Äôintercettazione dei dati o l‚Äôintrusione nella rete senza essere rilevati.</p>
<p>Inoltre, l‚Äôhardware contraffatto comporta rischi legali e di conformit√†. Le aziende che utilizzano inavvertitamente parti contraffatte nei loro sistemi ML possono affrontare gravi ripercussioni legali, tra cui multe e sanzioni per il mancato rispetto delle normative e degli standard del settore. Ci√≤ √® particolarmente vero per i settori in cui √® obbligatoria la conformit√† a specifiche normative sulla sicurezza e sulla privacy, come l‚Äôassistenza sanitaria e la finanza.</p>
<p>Le pressioni economiche per ridurre i costi aggravano il problema dell‚Äôhardware contraffatto e costringono le aziende ad approvvigionarsi da fornitori a basso costo, privi di rigorosi processi di verifica. Questa economia pu√≤ introdurre inavvertitamente parti contraffatte in sistemi altrimenti sicuri. Inoltre, rilevare queste contraffazioni √® intrinsecamente complicato poich√© vengono create per passare per componenti originali, il che spesso richiede attrezzature e competenze sofisticate per essere identificate.</p>
<p>Nel campo dell‚Äôapprendimento automatico, dove decisioni in tempo reale e calcoli complessi sono la norma, le implicazioni di un guasto hardware possono essere scomode e potenzialmente pericolose. √à fondamentale che le parti interessate siano pienamente consapevoli di questi rischi. Le sfide poste dall‚Äôhardware contraffatto richiedono una comprensione completa delle attuali minacce all‚Äôintegrit√† del sistema di apprendimento automatico. Ci√≤ sottolinea la necessit√† di una gestione proattiva e informata del ciclo di vita dell‚Äôhardware all‚Äôinterno di questi sistemi avanzati.</p>
</section>
<section id="rischi-della-catena-di-fornitura" class="level3" data-number="14.5.7">
<h3 data-number="14.5.7" class="anchored" data-anchor-id="rischi-della-catena-di-fornitura"><span class="header-section-number">14.5.7</span> Rischi della Catena di Fornitura</h3>
<p>La minaccia dell‚Äôhardware contraffatto √® strettamente legata alle vulnerabilit√† pi√π ampie della supply chain [catena di fornitura]. Le supply chain globalizzate e interconnesse creano molteplici opportunit√† per componenti compromessi di infiltrarsi nel ciclo di vita di un prodotto. Le supply chain coinvolgono numerose entit√†, dalla progettazione alla produzione, all‚Äôassemblaggio, alla distribuzione e all‚Äôintegrazione. Una mancanza di trasparenza e supervisione di ogni partner rende difficile la verifica dell‚Äôintegrit√† a ogni passaggio. Le lacune in qualsiasi punto della catena possono consentire l‚Äôinserimento di parti contraffatte.</p>
<p>Ad esempio, un produttore su contratto potrebbe ricevere e includere inconsapevolmente rifiuti elettronici riciclati contenenti contraffazioni pericolose. Un distributore inaffidabile potrebbe introdurre di nascosto componenti clonati. Le minacce interne a qualsiasi fornitore potrebbero deliberatamente mescolare contraffazioni in spedizioni legittime.</p>
<p>Una volta che le contraffazioni entrano nel flusso di fornitura, passano rapidamente attraverso pi√π mani prima di finire nei sistemi ML in cui il rilevamento √® difficile. Le contraffazioni avanzate come parti ricondizionate o cloni con esterni riconfezionati possono mascherarsi da componenti autentici, superando l‚Äôispezione visiva.</p>
<p>Per identificare i falsi, spesso √® richiesta una profilazione tecnica completa tramite micrografia, screening a raggi X, analisi forense dei componenti e test funzionali. Tuttavia, un‚Äôanalisi cos√¨ costosa non √® pratica per gli acquisti su larga scala.</p>
<p>Strategie come audit della supply chain, screening dei fornitori, convalida della provenienza dei componenti e aggiunta di protezioni antimanomissione possono aiutare a mitigare i rischi. Tuttavia, date le sfide globali alla sicurezza della supply chain, un approccio zero-trust √® prudente. Progettare sistemi ML per utilizzare controlli ridondanti, fail-safe e monitoraggio continuo del runtime fornisce resilienza contro i compromessi dei componenti.</p>
<p>Una rigorosa convalida delle sorgenti hardware abbinata ad architetture di sistema fault-tolerant offre la difesa pi√π solida contro i rischi pervasivi di supply chain globali contorte e opache.</p>
</section>
<section id="caso-di-studio-una-chiamata-di-risveglio-per-la-sicurezza-hardware" class="level3" data-number="14.5.8">
<h3 data-number="14.5.8" class="anchored" data-anchor-id="caso-di-studio-una-chiamata-di-risveglio-per-la-sicurezza-hardware"><span class="header-section-number">14.5.8</span> Caso di Studio: Una Chiamata di Risveglio per la Sicurezza Hardware</h3>
<p>Nel 2018, Bloomberg Businessweek ha pubblicato una <a href="https://www.bloomberg.com/news/features/2018-10-04/the-big-hack-how-china-used-a-tiny-chip-to-infiltrate-america-s-top-companies">storia</a> allarmante che ha attirato molta attenzione nel mondo della tecnologia. L‚Äôarticolo sosteneva che Supermicro aveva segretamente impiantato minuscoli chip spia nell‚Äôhardware del server. I giornalisti hanno affermato che gli hacker statali cinesi che lavoravano con Supermicro potevano infilare questi minuscoli chip nelle schede madri durante la produzione. I minuscoli chip avrebbero presumibilmente dato agli hacker un accesso backdoor ai server utilizzati da oltre 30 grandi aziende, tra cui Apple e Amazon.</p>
<p>Se fosse vero, ci√≤ consentirebbe agli hacker di spiare dati privati o persino manomettere i sistemi. Tuttavia, dopo aver indagato, Apple e Amazon non hanno trovato prove dell‚Äôesistenza di tale hardware Supermicro hackerato. Altri esperti hanno messo in dubbio l‚Äôaccuratezza dell‚Äôarticolo di Bloomberg.</p>
<p>Se la storia sia del tutto accurata o meno non √® una nostra preoccupazione da un punto di vista pedagogico. Tuttavia, questo incidente ha attirato l‚Äôattenzione sui rischi delle catene di fornitura globali per l‚Äôhardware prodotto principalmente in Cina. Quando le aziende esternalizzano e acquistano componenti hardware da fornitori in tutto il mondo, √® necessario che vi sia maggiore visibilit√† nel processo. In questa complessa pipeline globale, si teme che hardware contraffatti o manomessi possano essere introdotti da qualche parte lungo il percorso senza che le aziende tecnologiche se ne accorgano. Le aziende che si affidano troppo a singoli produttori o distributori creano rischi. Ad esempio, a causa dell‚Äôeccessiva dipendenza da <a href="https://www.tsmc.com/english">TSMC</a> per la produzione di semiconduttori, gli Stati Uniti hanno investito 50 miliardi di dollari nel <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2022/08/09/fact-sheet-chips-and-science-act-will-lower-costs-create-jobs-strengthen-supply-chains-and-counter-china/">CHIPS Act</a>.</p>
<p>Man mano che l‚Äôapprendimento automatico si sposta in sistemi pi√π critici, √® fondamentale verificare l‚Äôintegrit√† dell‚Äôhardware dalla progettazione alla produzione e alla consegna. La backdoor Supermicro segnalata ha dimostrato che per la sicurezza dell‚Äôapprendimento automatico non possiamo dare per scontate le catene di fornitura e la produzione globali. Dobbiamo ispezionare e convalidare l‚Äôhardware a ogni collegamento della catena.</p>
</section>
</section>
<section id="sicurezza-hardware-del-ml-embedded" class="level2 page-columns page-full" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="sicurezza-hardware-del-ml-embedded"><span class="header-section-number">14.6</span> Sicurezza Hardware del ML Embedded</h2>
<section id="trusted-execution-environments" class="level3" data-number="14.6.1">
<h3 data-number="14.6.1" class="anchored" data-anchor-id="trusted-execution-environments"><span class="header-section-number">14.6.1</span> Trusted Execution Environments</h3>
<section id="informazioni-su-tee" class="level4">
<h4 class="anchored" data-anchor-id="informazioni-su-tee">Informazioni su TEE</h4>
<p>Un Trusted Execution Environment (TEE) √® un‚Äôarea protetta all‚Äôinterno di un processore host che garantisce l‚Äôesecuzione sicura del codice e la protezione dei dati sensibili. Isolando le attivit√† critiche dal sistema operativo, i TEE resistono agli attacchi software e hardware, fornendo un ambiente protetto per la gestione di calcoli sensibili.</p>
</section>
<section id="vantaggi" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi">Vantaggi</h4>
<p>I TEE sono particolarmente preziosi in scenari in cui devono essere elaborati dati sensibili o in cui l‚Äôintegrit√† delle operazioni di un sistema √® critica. Nel contesto dell‚Äôhardware ML, i TEE assicurano che gli algoritmi e i dati ML siano protetti da manomissioni e ‚Äúperdite‚Äù. Ci√≤ √® essenziale perch√© i modelli ML elaborano spesso informazioni private, segreti commerciali o dati che potrebbero essere sfruttati se esposti.</p>
<p>Ad esempio, un TEE pu√≤ proteggere i parametri del modello ML dall‚Äôestrazione da parte di software dannosi sullo stesso dispositivo. Questa protezione √® fondamentale per la privacy e il mantenimento dell‚Äôintegrit√† del sistema ML, assicurando che i modelli funzionino come previsto e non forniscano output distorti a causa di parametri manipolati. <a href="https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web">Secure Enclave di Apple</a>, presente in iPhone e iPad, √® una forma di TEE che fornisce un ambiente isolato per proteggere i dati sensibili degli utenti e le operazioni crittografiche.</p>
<p>I Trusted Execution Environment (TEE) sono essenziali per i settori che richiedono elevati livelli di sicurezza, tra cui telecomunicazioni, finanza, sanit√† e automotive. I TEE proteggono l‚Äôintegrit√† delle reti 5G nelle telecomunicazioni e supportano applicazioni critiche. Nella finanza, proteggono i pagamenti mobili e i processi di autenticazione. L‚Äôassistenza sanitaria si affida ai TEE per salvaguardare i dati sensibili dei pazienti, mentre il settore automobilistico dipende da loro per la sicurezza e l‚Äôaffidabilit√† dei sistemi autonomi. In tutti i settori, i TEE garantiscono la riservatezza e l‚Äôintegrit√† dei dati e delle operazioni.</p>
<p>Nei sistemi ML, i TEE possono:</p>
<ul>
<li><p>Eseguire in modo sicuro l‚Äôaddestramento e l‚Äôinferenza del modello, assicurando che i risultati del calcolo rimangano riservati.</p></li>
<li><p>Proteggere la riservatezza dei dati di input, come le informazioni biometriche, utilizzati per l‚Äôidentificazione personale o per attivit√† di classificazione sensibili.</p></li>
<li><p>Proteggere i modelli ML impedendo il reverse engineering, che pu√≤ proteggere le informazioni proprietarie e mantenere un vantaggio competitivo.</p></li>
<li><p>Abilitare aggiornamenti sicuri ai modelli ML, assicurando che gli aggiornamenti provengano da una fonte attendibile e non siano stati manomessi durante il transito.</p></li>
<li><p>Rafforzare la sicurezza della rete salvaguardando la trasmissione dei dati tra componenti ML distribuiti tramite crittografia ed elaborazione sicura in-TEE.</p></li>
</ul>
<p>L‚Äôimportanza dei TEE nella sicurezza hardware ML deriva dalla loro capacit√† di proteggere da minacce esterne e interne, tra cui le seguenti:</p>
<ul>
<li><p><strong>Software Dannoso:</strong> I TEE possono impedire al malware ad alto privilegio di accedere alle aree sensibili del sistema ML.</p></li>
<li><p><strong>Manomissione Fisica:</strong> Integrandosi con le misure di sicurezza hardware, i TEE possono proteggere dalla manomissione fisica che tenta di aggirare la sicurezza del software.</p></li>
<li><p><strong>Attacchi Side-Channel:</strong> Sebbene non siano impenetrabili, i TEE possono mitigare specifici attacchi side-channel controllando l‚Äôaccesso a operazioni sensibili e modelli di dati.</p></li>
<li><p><strong>Minacce di Rete:</strong> I TEE migliorano la sicurezza della rete salvaguardando la trasmissione dei dati tra componenti ML distribuiti tramite crittografia ed elaborazione in-TEE sicura. Ci√≤ impedisce efficacemente gli attacchi ‚Äúman-in-the-middle‚Äù e garantisce che i dati vengano trasmessi tramite canali attendibili.</p></li>
</ul>
</section>
<section id="meccanica" class="level4">
<h4 class="anchored" data-anchor-id="meccanica">Meccanica</h4>
<p>I fondamenti dei TEE contengono quattro parti principali:</p>
<ul>
<li><p><strong>Esecuzione Isolata:</strong> Il codice all‚Äôinterno di un TEE viene eseguito in un ambiente separato dal sistema operativo host del dispositivo host. Questo isolamento protegge il codice dall‚Äôaccesso non autorizzato da parte di altre applicazioni.</p></li>
<li><p><strong>Archiviazione Sicura:</strong> I TEE possono archiviare in modo sicuro chiavi crittografiche, token di autenticazione e dati sensibili, impedendo alle applicazioni normali di accedervi al di fuori del TEE.</p></li>
<li><p><strong>Protezione dell‚ÄôIntegrit√†:</strong> I TEE possono verificare l‚Äôintegrit√† del codice e dei dati, assicurando che non siano stati alterati prima dell‚Äôesecuzione o durante l‚Äôarchiviazione.</p></li>
<li><p><strong>Crittografia dei Dati:</strong> I dati gestiti all‚Äôinterno di un TEE possono essere crittografati, rendendoli illeggibili per entit√† senza le chiavi appropriate, che sono anch‚Äôesse gestite all‚Äôinterno del TEE.</p></li>
</ul>
<p>Ecco alcuni esempi di TEE che forniscono sicurezza basata su hardware per applicazioni sensibili:</p>
<ul>
<li><p><strong><a href="https://www.arm.com/technologies/trustzone-for-cortex-m">ARMTrustZone</a>:</strong> Questa tecnologia crea ambienti di esecuzione sicuri e normali isolati tramite controlli hardware e implementati in molti chipset mobili. mobile chipsets.</p></li>
<li><p><strong><a href="https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html">IntelSGX</a>:</strong> Le estensioni Software Guard di Intel forniscono un‚Äôenclave per l‚Äôesecuzione del codice che protegge da varie minacce basate sul software, prendendo di mira in modo specifico le vulnerabilit√† del livello del sistema operativo. Vengono utilizzate per salvaguardare i carichi di lavoro nel cloud.</p></li>
<li><p><strong><a href="https://www.qualcomm.com/products/features/mobile-security-solutions">Qualcomm Secure Execution Environment</a>:</strong> Un sandbox hardware su chipset Qualcomm per app di pagamento e autenticazione mobili.</p></li>
<li><p><strong><a href="https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web">Apple SecureEnclave</a>:</strong> Un TEE per la gestione dei dati biometrici e delle chiavi crittografiche su iPhone e iPad, che facilita i pagamenti mobili sicuri.</p></li>
</ul>
<p><a href="#fig-enclave" class="quarto-xref">Figura&nbsp;<span>14.7</span></a> √® un diagramma che mostra un‚Äôenclave sicura isolata dal processore host per fornire un ulteriore livello di sicurezza. L‚Äôenclave sicura ha una ROM di avvio per stabilire una ‚Äúroot‚Äù hardware di attendibilit√†, un motore AES per operazioni crittografiche efficienti e sicure e memoria protetta. Ha anche un meccanismo per memorizzare le informazioni in modo sicuro su un archivio collegato separato da quello flash NAND utilizzato dal processore applicativo e dal sistema operativo. La NAND flash √® un tipo di storage non volatile utilizzato in dispositivi come SSD, smartphone e tablet per conservare i dati anche quando sono spenti. Isolando i dati sensibili dallo storage NAND a cui accede il sistema principale, questo design garantisce che i dati degli utenti rimangano protetti anche se il kernel del processore applicativo √® compromesso.</p>
<div id="fig-enclave" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-enclave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/System-on-chip_secure_enclave.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-enclave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.7: Enclave sicura System-on-chip. Fonte: <a href="https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web">Apple.</a>
</figcaption>
</figure>
</div>
</section>
<section id="compromessi" class="level4">
<h4 class="anchored" data-anchor-id="compromessi">Compromessi</h4>
<p>Sebbene i ‚ÄúTrusted Execution Environment‚Äù offrano notevoli vantaggi in termini di sicurezza, la loro implementazione comporta dei compromessi. Diversi fattori influenzano se un sistema include un TEE:</p>
<p><strong>Costo:</strong> L‚Äôimplementazione dei TEE comporta costi aggiuntivi. Ci sono costi diretti per l‚Äôhardware e costi indiretti associati allo sviluppo e alla manutenzione di software sicuro per i TEE. Questi costi potrebbero essere giustificabili solo per alcuni dispositivi, in particolare prodotti a basso margine.</p>
<p><strong>Complessit√†:</strong> I TEE aggiungono complessit√† alla progettazione e allo sviluppo del sistema. L‚Äôintegrazione di un TEE con sistemi esistenti richiede una sostanziale ri-progettazione dello stack hardware e software, che pu√≤ rappresentare un ostacolo, in particolare per i sistemi legacy.</p>
<p><strong>Performance Overhead:</strong> I TEE possono introdurre un overhead di prestazioni dovuto ai passaggi aggiuntivi coinvolti nella crittografia e nella verifica dei dati, che potrebbero rallentare le applicazioni sensibili al tempo.</p>
<p><strong>Sfide di Sviluppo:</strong> Lo sviluppo per i TEE richiede conoscenze specialistiche e spesso deve rispettare rigidi protocolli di sviluppo. Ci√≤ pu√≤ estendere i tempi di sviluppo e complicare i processi di debug e test.</p>
<p><strong>Scalabilit√† e Flessibilit√†:</strong> I TEE, a causa della loro natura protetta, possono imporre limitazioni di scalabilit√† e flessibilit√†. L‚Äôaggiornamento dei componenti protetti o il ridimensionamento del sistema per pi√π utenti o dati pu√≤ essere pi√π impegnativo quando tutto deve passare attraverso un ambiente protetto e chiuso.</p>
<p><strong>Consumo Energetico:</strong> L‚Äôelaborazione aumentata richiesta per la crittografia, la decrittografia e i controlli di integrit√† possono portare a un maggiore consumo energetico, una preoccupazione significativa per i dispositivi alimentati a batteria.</p>
<p><strong>Domanda del Mercato:</strong> Non tutti i mercati o le applicazioni richiedono il livello di sicurezza fornito dai TEE. Per molte applicazioni consumer, il rischio percepito potrebbe essere abbastanza basso da indurre i produttori a scegliere di non includere TEE nei loro progetti.</p>
<p><strong>Certificazione e Garanzia di Sicurezza:</strong> I sistemi con TEE potrebbero aver bisogno di rigorose certificazioni di sicurezza con enti come <a href="https://www.commoncriteriaportal.org/ccra/index.cfm">Common Criteria</a> (CC) o <a href="https://www.enisa.europa.eu/">European Union Agency for Cybersecurity</a> (ENISA), che possono essere lunghe e costose. Alcune organizzazioni potrebbero scegliere di astenersi dall‚Äôimplementare TEE per evitare questi ostacoli.</p>
<p><strong>Dispositivi con risorse limitate:</strong> I dispositivi con potenza di elaborazione, memoria o archiviazione limitate potrebbero supportare solo TEE senza compromettere la loro funzionalit√† primaria.</p>
</section>
</section>
<section id="avvio-sicuro" class="level3 page-columns page-full" data-number="14.6.2">
<h3 data-number="14.6.2" class="anchored" data-anchor-id="avvio-sicuro"><span class="header-section-number">14.6.2</span> Avvio Sicuro</h3>
<section id="informazioni" class="level4">
<h4 class="anchored" data-anchor-id="informazioni">Informazioni</h4>
<p>Secure Boot √® uno standard di sicurezza fondamentale che garantisce che un dispositivo si avvii solo tramite software ritenuto attendibile dal produttore del dispositivo. Durante l‚Äôavvio, il firmware controlla la firma digitale di ogni componente software di avvio, inclusi bootloader, kernel e sistema operativo di base. Questo processo verifica che il software non sia stato alterato o manomesso. Se una firma non supera la verifica, il processo di avvio viene interrotto per impedire l‚Äôesecuzione di codice non autorizzato che potrebbe compromettere l‚Äôintegrit√† della sicurezza del sistema.</p>
</section>
<section id="vantaggi-1" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi-1">Vantaggi</h4>
<p>L‚Äôintegrit√† di un sistema ML embedded √® fondamentale fin dal momento dell‚Äôaccensione. Qualsiasi compromissione nel processo di avvio pu√≤ portare all‚Äôesecuzione di software dannoso prima che il sistema operativo e le applicazioni ML inizino, con conseguenti operazioni ML manipolate, accesso ai dati non autorizzato o riutilizzo del dispositivo per attivit√† dannose come botnet o crypto-mining.</p>
<p>Secure Boot offre protezioni vitali per l‚Äôhardware ML embedded tramite i seguenti meccanismi critici:</p>
<ul>
<li><p><strong>Protezione dei Dati ML:</strong> Garantire che i dati utilizzati dai modelli ML, che possono includere informazioni private o sensibili, non siano esposti a manomissioni o furti durante il processo di boot [avvio].</p></li>
<li><p><strong>Protezione dell‚ÄôIntegrit√† del Modello:</strong> Mantenere l‚Äôintegrit√† dei modelli ML √® fondamentale, poich√© la loro manomissione potrebbe portare a risultati errati o dannosi.</p></li>
<li><p><strong>Aggiornamenti Sicuri del Modello:</strong> Abilitare aggiornamenti sicuri per modelli e algoritmi ML, assicurando che gli aggiornamenti siano autenticati e non siano stati alterati.</p></li>
</ul>
</section>
<section id="meccanica-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="meccanica-1">Meccanica</h4>
<p>Secure Boot funziona con i TEE per migliorare ulteriormente la sicurezza del sistema. <a href="#fig-secure-boot" class="quarto-xref">Figura&nbsp;<span>14.8</span></a> illustra un diagramma di flusso di un sistema embedded affidabile. Nella fase di validazione iniziale, Secure Boot verifica che il codice in esecuzione nel TEE sia la versione corretta e non manomessa autorizzata dal produttore del dispositivo. Controllando le firme digitali del firmware e di altri componenti critici del sistema, Secure Boot impedisce modifiche non autorizzate che potrebbero compromettere le capacit√† di sicurezza del TEE. Ci√≤ stabilisce una base di fiducia su cui il TEE pu√≤ eseguire in modo sicuro operazioni sensibili come la gestione delle chiavi crittografiche e l‚Äôelaborazione sicura dei dati. Applicando questi livelli di sicurezza, Secure Boot consente operazioni dei dispositivi sicure e resilienti anche negli ambienti con risorse pi√π limitate.</p>
<div id="fig-secure-boot" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-secure-boot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Secure_Boot_flow.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-secure-boot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.8: Flusso di Secure Boot. Fonte: <span class="citation" data-cites="Rashmi2018Secure">R. V. e A. (<a href="../../../references.it.html#ref-Rashmi2018Secure" role="doc-biblioref">2018</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-Rashmi2018Secure" class="csl-entry" role="listitem">
R. V., Rashmi, e Karthikeyan A. 2018. <span>¬´Secure boot of Embedded Applications - A Review¬ª</span>. In <em>2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)</em>, 291‚Äì98. IEEE. <a href="https://doi.org/10.1109/iceca.2018.8474730">https://doi.org/10.1109/iceca.2018.8474730</a>.
</div></div></figure>
</div>
</section>
<section id="caso-di-studio-face-id-di-apple" class="level4">
<h4 class="anchored" data-anchor-id="caso-di-studio-face-id-di-apple">Caso di Studio: Face ID di Apple</h4>
<p>Un esempio concreto dell‚Äôapplicazione di Secure Boot pu√≤ essere osservato nella tecnologia Face ID di Apple, che utilizza algoritmi di apprendimento automatico avanzati per abilitare il <a href="https://support.apple.com/en-us/102381">riconoscimento facciale</a> su iPhone e iPad. Face ID si basa su una sofisticata integrazione di sensori e software per mappare con precisione la geometria del volto di un utente. Affinch√© Face ID funzioni in modo sicuro e protegga i dati biometrici degli utenti, le operazioni del dispositivo devono essere affidabili fin dall‚Äôinizializzazione. √à qui che Secure Boot svolge un ruolo fondamentale. Di seguito viene descritto come funziona Secure Boot insieme a Face ID:</p>
<ol type="1">
<li><p><strong>Verifica Iniziale:</strong> All‚Äôavvio di un iPhone, il processo Secure Boot inizia all‚Äôinterno di Secure Enclave, un coprocessore specializzato progettato per aggiungere un ulteriore livello di sicurezza. Secure Enclave gestisce i dati biometrici, come le impronte digitali per Touch ID e i dati di riconoscimento facciale per Face ID. Durante il processo di avvio, il sistema verifica rigorosamente che Apple abbia firmato digitalmente il firmware di Secure Enclave, garantendone l‚Äôautenticit√†. Questa fase di verifica assicura che il firmware utilizzato per elaborare i dati biometrici rimanga sicuro e senza essere compromesso.</p></li>
<li><p><strong>Controlli di sicurezza continui:</strong> Dopo l‚Äôinizializzazione e la convalida del sistema da parte di Secure Boot, Secure Enclave comunica con il processore centrale del dispositivo per mantenere una catena di avvio sicura. Durante questo processo, le firme digitali del kernel iOS e di altri componenti di avvio critici vengono meticolosamente verificate per garantirne l‚Äôintegrit√† prima di procedere. Questo modello di ‚Äúcatena di fiducia‚Äù impedisce efficacemente modifiche non autorizzate al bootloader e al sistema operativo, salvaguardando la sicurezza complessiva del dispositivo.</p></li>
<li><p><strong>Elaborazione dei Dati del Viso:</strong> Una volta completata la sequenza di avvio sicura, Secure Enclave interagisce in modo sicuro con gli algoritmi di apprendimento automatico che alimentano Face ID. Il riconoscimento facciale prevede la proiezione e l‚Äôanalisi di oltre 30.000 punti invisibili per creare una mappa di profondit√† del volto dell‚Äôutente e un‚Äôimmagine a infrarossi. Questi dati vengono convertiti in una rappresentazione matematica e confrontati in modo sicuro con i dati del volto registrati e archiviati in Secure Enclave.</p></li>
<li><p><strong>Secure Enclave e Protezione dei Dati:</strong> Secure Enclave √® progettato con precisione per proteggere i dati sensibili e gestire le operazioni crittografiche che salvaguardano tali dati. Anche in caso di kernel del sistema operativo compromesso, i dati del volto elaborati tramite Face ID rimangono inaccessibili ad applicazioni non autorizzate o ad aggressori esterni. √à importante sottolineare che i dati di Face ID non vengono mai trasmessi dal dispositivo e non vengono archiviati su iCloud o altri server esterni.</p></li>
<li><p><strong>Aggiornamenti Firmware:</strong> Apple rilascia frequentemente aggiornamenti per risolvere le vulnerabilit√† della sicurezza e migliorare la funzionalit√† del sistema. Secure Boot garantisce che tutti gli aggiornamenti del firmware siano autenticati, consentendo l‚Äôinstallazione solo di quelli firmati da Apple. Questo processo aiuta a preservare l‚Äôintegrit√† e la sicurezza del sistema di Face ID nel tempo.</p></li>
</ol>
<p>Integrando Secure Boot con hardware dedicato come Secure Enclave, Apple offre solide garanzie di sicurezza per operazioni critiche come il riconoscimento facciale.</p>
</section>
<section id="sfide" class="level4">
<h4 class="anchored" data-anchor-id="sfide">Sfide</h4>
<p>Nonostante i suoi vantaggi, l‚Äôimplementazione di Secure Boot presenta diverse sfide, in particolare in distribuzioni complesse e su larga scala: <strong>Complessit√† della Gestione delle Chiavi:</strong> Generare, archiviare, distribuire, ruotare e revocare chiavi crittografiche in modo dimostrabilmente sicuro √® particolarmente impegnativo ma fondamentale per mantenere la catena di fiducia. Qualsiasi compromissione delle chiavi paralizza le protezioni. Le grandi aziende che gestiscono moltitudini di chiavi di dispositivi affrontano particolari sfide di scala.</p>
<p><strong>Sovraccarico di Prestazioni:</strong> Il controllo delle firme crittografiche durante il Boot pu√≤ aggiungere 50-100ms o pi√π per componente verificato. Questo ritardo pu√≤ essere proibitivo per applicazioni sensibili al tempo o con risorse limitate. Tuttavia, gli impatti sulle prestazioni possono essere ridotti tramite parallelizzazione e accelerazione hardware.</p>
<p><strong>Signing Burden:</strong> [Onere della firma] Gli sviluppatori devono garantire diligentemente che tutti i componenti software coinvolti nel processo di avvio, ovvero bootloader, firmware, kernel del sistema operativo, driver, applicazioni, ecc., siano firmati correttamente da chiavi attendibili. L‚Äôaccettazione della firma del codice di terze parti rimane un problema.</p>
<p><strong>Verifica Crittografica:</strong> Gli algoritmi e i protocolli sicuri devono convalidare la legittimit√† di chiavi e firme, evitare manomissioni o bypass e supportare la revoca. L‚Äôaccettazione di chiavi dubbie mina la fiducia.</p>
<p><strong>Vincoli di Personalizzazione:</strong> Le architetture Secure Boot bloccate dal fornitore limitano il controllo dell‚Äôutente e l‚Äôaggiornabilit√†. I bootloader open source come <a href="https://source.denx.de/u-boot/u-boot">u-boot</a> e <a href="https://www.coreboot.org/">coreboot</a> abilitano la sicurezza supportando al contempo la personalizzazione.</p>
<p><strong>Standard Scalabili:</strong> Standard emergenti come <a href="https://www.microsoft.com/en-us/research/project/dice-device-identifier-composition-engine/">Device Identifier Composition Engine</a> (DICE) e <a href="https://1.ieee802.org/security/802-1ar/">IDevID</a> promettono di fornire e gestire in modo sicuro identit√† e chiavi dei dispositivi su larga scala in tutti gli ecosistemi.</p>
<p>L‚Äôadozione di Secure Boot richiede di seguire le best practice di sicurezza relative alla gestione delle chiavi, alla convalida della crittografia, agli aggiornamenti firmati e al controllo degli accessi. Secure Boot fornisce una solida base per creare integrit√† e affidabilit√† dei dispositivi se implementato con cura.</p>
</section>
</section>
<section id="moduli-di-sicurezza-hardware" class="level3" data-number="14.6.3">
<h3 data-number="14.6.3" class="anchored" data-anchor-id="moduli-di-sicurezza-hardware"><span class="header-section-number">14.6.3</span> Moduli di Sicurezza Hardware</h3>
<section id="hsm" class="level4">
<h4 class="anchored" data-anchor-id="hsm">HSM</h4>
<p>Un ‚ÄúHardware Security Module (HSM)‚Äù √® un dispositivo fisico che gestisce le chiavi digitali per un‚Äôautenticazione avanzata e fornisce l‚Äôelaborazione crittografica. Questi moduli sono progettati per essere resistenti alle manomissioni e fornire un ambiente sicuro per l‚Äôesecuzione di operazioni crittografiche. Gli HSM possono essere dispositivi standalone, schede plug-in o circuiti integrati su un altro dispositivo.</p>
<p>Gli HSM sono fondamentali per varie applicazioni sensibili alla sicurezza perch√© offrono un‚Äôenclave rafforzata e sicura per l‚Äôarchiviazione delle chiavi crittografiche e l‚Äôesecuzione di funzioni crittografiche. Sono particolarmente importanti per garantire la sicurezza delle transazioni, le verifiche dell‚Äôidentit√† e la crittografia dei dati.</p>
</section>
<section id="vantaggi-2" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi-2">Vantaggi</h4>
<p>Gli HSM forniscono diverse funzionalit√† utili per la sicurezza dei sistemi ML:</p>
<p><strong>Protezione dei Dati Sensibili:</strong> Nelle applicazioni di apprendimento automatico, i modelli spesso elaborano dati sensibili che possono essere proprietari o personali. Gli HSM proteggono le chiavi di crittografia utilizzate per proteggere questi dati, sia a riposo che in transito, dall‚Äôesposizione o dal furto.</p>
<p><strong>Garanzia dell‚ÄôIntegrit√† del Modello:</strong> L‚Äôintegrit√† dei modelli ML √® fondamentale per il loro funzionamento affidabile. Gli HSM possono gestire in modo sicuro i processi di firma e verifica per software e firmware ML, assicurando che parti non autorizzate non abbiano alterato i modelli.</p>
<p><strong>Addestramento e Aggiornamenti Sicuri del Modello:</strong> L‚Äôaddestramento e l‚Äôaggiornamento dei modelli ML comportano l‚Äôelaborazione di dati potenzialmente sensibili. Gli HSM garantiscono che questi processi vengano condotti all‚Äôinterno di un confine crittografico sicuro, proteggendo dall‚Äôesposizione dei dati di addestramento e dagli aggiornamenti non autorizzati del modello.</p>
</section>
<section id="compromessi-1" class="level4">
<h4 class="anchored" data-anchor-id="compromessi-1">Compromessi</h4>
<p>Gli HSM comportano diversi compromessi per l‚ÄôML embedded. Questi compromessi sono simili ai TEE, ma per completezza, li discuteremo anche qui attraverso la lente dell‚ÄôHSM.</p>
<p><strong>Costo:</strong> Gli HSM sono dispositivi specializzati che possono essere costosi da procurare e implementare, aumentando il costo complessivo di un progetto ML. Questo pu√≤ essere un fattore significativo per i sistemi embedded, dove i vincoli di costo sono spesso pi√π rigidi.</p>
<p><strong>Sovraccarico di Prestazioni:</strong> Sebbene sicure, le operazioni crittografiche eseguite dagli HSM possono introdurre latenza. Qualsiasi ritardo aggiunto pu√≤ essere critico nelle applicazioni ML embedded ad alte prestazioni in cui l‚Äôinferenza deve avvenire in tempo reale, come nei veicoli autonomi o nei dispositivi di traduzione.</p>
<p><strong>Spazio Fisico:</strong> I sistemi embedded sono spesso limitati dallo spazio fisico e l‚Äôaggiunta di un HSM pu√≤ essere difficile in ambienti con vincoli rigidi. Ci√≤ √® particolarmente vero per l‚Äôelettronica di consumo e la tecnologia indossabile, dove le dimensioni e il fattore di forma sono considerazioni chiave.</p>
<p><strong>Consumo Energetico:</strong> Gli HSM richiedono energia per funzionare, il che pu√≤ rappresentare uno svantaggio per i dispositivi a batteria con una lunga durata della batteria. L‚Äôelaborazione sicura e le operazioni crittografiche possono scaricare la batteria pi√π velocemente, un compromesso significativo per le applicazioni ML embedded mobili o remote.</p>
<p><strong>Complessit√† nell‚ÄôIntegrazione:</strong> L‚Äôintegrazione degli HSM nei sistemi hardware esistenti aggiunge complessit√†. Spesso sono necessarie conoscenze specialistiche per gestire la comunicazione sicura tra l‚ÄôHSM e il processore del sistema e sviluppare software in grado di interfacciarsi con l‚ÄôHSM.</p>
<p><strong>Scalabilit√†:</strong> Il ridimensionamento di una soluzione ML che utilizza gli HSM pu√≤ essere impegnativo. Gestire una flotta di HSM e garantire l‚Äôuniformit√† nelle pratiche di sicurezza tra i dispositivi pu√≤ diventare complesso e costoso quando aumentano le dimensioni della distribuzione, soprattutto quando si ha a che fare con sistemi embedded in cui la comunicazione √® costosa.</p>
<p><strong>Complessit√† Operativa:</strong> Gli HSM possono rendere pi√π complesso l‚Äôaggiornamento del firmware e dei modelli ML. Ogni aggiornamento deve essere firmato e possibilmente crittografato, il che aggiunge passaggi al processo di aggiornamento e potrebbe richiedere meccanismi sicuri per la gestione delle chiavi e la distribuzione degli aggiornamenti.</p>
<p><strong>Sviluppo e Manutenzione:</strong> La natura sicura degli HSM implica che solo personale limitato abbia accesso all‚ÄôHSM per scopi di sviluppo e manutenzione. Ci√≤ pu√≤ rallentare il processo di sviluppo e rendere pi√π difficile la manutenzione di routine.</p>
<p><strong>Certificazione e Conformit√†:</strong> Garantire che un HSM soddisfi specifici standard di settore e requisiti di conformit√† pu√≤ aumentare i tempi e i costi di sviluppo. Ci√≤ potrebbe comportare l‚Äôesecuzione di rigorosi processi di certificazione e audit.</p>
</section>
</section>
<section id="physical-unclonable-functions-puf" class="level3 page-columns page-full" data-number="14.6.4">
<h3 data-number="14.6.4" class="anchored" data-anchor-id="physical-unclonable-functions-puf"><span class="header-section-number">14.6.4</span> Physical Unclonable Functions (PUF)</h3>
<section id="informazioni-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="informazioni-1">Informazioni</h4>
<p>Le ‚ÄúPhysical Unclonable Function (PUF)‚Äù [funzioni fisiche non clonabili] forniscono un mezzo intrinseco all‚Äôhardware per la generazione di chiavi crittografiche e l‚Äôautenticazione del dispositivo sfruttando la variabilit√† di produzione intrinseca nei componenti semiconduttori. Durante la fabbricazione, fattori fisici casuali come variazioni di drogaggio, ruvidit√† del bordo della linea e spessore dielettrico determinano differenze microscopiche tra i semiconduttori, anche quando prodotti dalle stesse maschere. Questi creano variazioni di temporizzazione e potenza rilevabili che agiscono come una ‚Äúimpronta digitale‚Äù unica per ogni chip. Le PUF sfruttano questo fenomeno incorporando circuiti integrati per amplificare piccole differenze di temporizzazione o potenza in uscite digitali misurabili.</p>
<p>Quando stimolato con uno stimolo in input, il circuito PUF produce una risposta di output basata sulle caratteristiche fisiche intrinseche del dispositivo. A causa della loro unicit√† fisica, lo stesso stimolo produrr√† una risposta diversa su altri dispositivi. Questo meccanismo di stimolo-risposta pu√≤ essere utilizzato per generare chiavi in modo sicuro e identificatori legati all‚Äôhardware specifico, eseguire l‚Äôautenticazione del dispositivo o archiviare in modo sicuro i segreti. Ad esempio, una chiave derivata da un PUF funzioner√† solo su quel dispositivo e non potr√† essere clonata o estratta nemmeno con accesso fisico o reverse engineering completo <span class="citation" data-cites="Gao2020Physical">(<a href="../../../references.it.html#ref-Gao2020Physical" role="doc-biblioref">Gao, Al-Sarawi, e Abbott 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="vantaggi-3" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi-3">Vantaggi</h4>
<p>La generazione di chiavi PUF evita l‚Äôarchiviazione esterna delle chiavi, che rischia di essere esposta. Fornisce inoltre una base per altre primitive di sicurezza hardware come Secure Boot. Le sfide di implementazione includono la gestione di affidabilit√† ed entropia variabili tra diverse PUF, sensibilit√† alle condizioni ambientali e suscettibilit√† agli attacchi di modellazione di apprendimento automatico. Se progettate con cura, le PUF consentono applicazioni promettenti nella protezione IP, nel trusted computing e nell‚Äôanticontraffazione.</p>
</section>
<section id="utilit√†" class="level4">
<h4 class="anchored" data-anchor-id="utilit√†">Utilit√†</h4>
<p>I modelli di apprendimento automatico stanno rapidamente diventando una parte fondamentale della funzionalit√† per molti dispositivi embedded, come smartphone, assistenti domestici intelligenti e droni autonomi. Tuttavia, proteggere l‚Äôapprendimento automatico su hardware embedded con risorse limitate pu√≤ essere difficile. Ed √® qui che i PUF si rivelano particolarmente utili. Diamo un‚Äôocchiata ad alcuni esempi di come le PUF possono essere utili.</p>
<p>Le PUF forniscono un modo per generare impronte digitali e chiavi crittografiche univoche legate alle caratteristiche fisiche di ciascun chip sul dispositivo. Facciamo un esempio. Abbiamo un drone con telecamera intelligente che usa ML embedded per tracciare gli oggetti. Un PUF integrato nel processore del drone potrebbe creare una chiave specifica del dispositivo per crittografare il modello ML prima di caricarlo sul drone. In questo modo, anche se un aggressore in qualche modo hackerasse il drone e provasse a rubare il modello, non sarebbe in grado di usarlo su un altro dispositivo!</p>
<p>La stessa chiave PUF potrebbe anche creare una filigrana digitale embedded nel modello ML. Se quel modello dovesse mai trapelare e essere pubblicato online da qualcuno che cercasse di piratarlo, la filigrana potrebbe aiutare a dimostrare che proviene dal drone rubato e non dall‚Äôaggressore. Inoltre, si immagini che la telecamera del drone si colleghi al cloud per scaricare parte della sua elaborazione ML. Il PUF pu√≤ autenticare che la telecamera √® legittima prima che il cloud esegua l‚Äôinferenza su video sensibili. Il cloud potrebbe verificare che il drone non sia stato manomesso fisicamente controllando che le risposte PUF non siano cambiate.</p>
<p>Le PUF consentono tutta questa sicurezza attraverso la casualit√† intrinseca del loro comportamento di stimolo-risposta e il binding hardware. Senza dover memorizzare le chiavi esternamente, le PUF sono ideali per proteggere l‚ÄôML embedded con risorse limitate. Pertanto, offrono un vantaggio unico rispetto ad altri meccanismi.</p>
</section>
<section id="meccanica-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="meccanica-2">Meccanica</h4>
<p>Il principio di funzionamento alla base dei PUF, illustrato in <a href="#fig-pfu" class="quarto-xref">Figura&nbsp;<span>14.9</span></a>, comporta la generazione di una coppia ‚Äústimolo-risposta‚Äù, in cui un input specifico (lo stimolo) al circuito PUF determina un output (la risposta) che √® determinato dalle propriet√† fisiche uniche di quel circuito. Questo processo pu√≤ essere paragonato a un meccanismo di impronte digitali per dispositivi elettronici. I dispositivi che utilizzano ML per elaborare i dati dei sensori possono utilizzare i PUF per proteggere la comunicazione tra dispositivi e impedire l‚Äôesecuzione di modelli ML su hardware contraffatto.</p>
<p><a href="#fig-pfu" class="quarto-xref">Figura&nbsp;<span>14.9</span></a> illustra una panoramica delle basi dei PUF: a) PUF pu√≤ essere pensato come un‚Äôimpronta digitale unica per ogni pezzo di hardware; b) un PUF Ottico √® uno speciale token di plastica che viene illuminato, creando un pattern a macchie unico che viene poi registrato; c) in un APUF (Arbiter PUF), i bit di stimolo selezionano percorsi diversi e un giudice decide quale √® pi√π veloce, dando una risposta di ‚Äò1‚Äô o ‚Äò0‚Äô; d) in un PUF SRAM, la risposta √® determinata dalla mancata corrispondenza nella tensione di soglia dei transistor, dove determinate condizioni portano a una risposta preferita di ‚Äò1‚Äô. Ognuno di questi metodi utilizza caratteristiche specifiche dell‚Äôhardware per creare un identificatore univoco.</p>
<div id="fig-pfu" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-pfu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/PUF_basics.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pfu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.9: Nozioni di base sui PUF. Fonte: <span class="citation" data-cites="Gao2020Physical">Gao, Al-Sarawi, e Abbott (<a href="../../../references.it.html#ref-Gao2020Physical" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-Gao2020Physical" class="csl-entry" role="listitem">
Gao, Yansong, Said F. Al-Sarawi, e Derek Abbott. 2020. <span>¬´Physical unclonable functions¬ª</span>. <em>Nature Electronics</em> 3 (2): 81‚Äì91. <a href="https://doi.org/10.1038/s41928-020-0372-5">https://doi.org/10.1038/s41928-020-0372-5</a>.
</div></div></figure>
</div>
</section>
<section id="sfide-1" class="level4">
<h4 class="anchored" data-anchor-id="sfide-1">Sfide</h4>
<p>Ci sono alcune sfide con i PUF. La risposta PUF pu√≤ essere sensibile alle condizioni ambientali, come fluttuazioni di temperatura e tensione, portando a un comportamento incoerente di cui si deve tenere conto nella progettazione. Inoltre, poich√© i PUF possono generare molte coppie stimolo-risposta uniche, gestire e garantire la coerenza di queste coppie per tutta la durata del dispositivo pu√≤ essere impegnativo. Ultimo ma non meno importante, l‚Äôintegrazione della tecnologia PUF pu√≤ aumentare il costo di produzione complessivo di un dispositivo, sebbene possa far risparmiare sui costi di gestione delle chiavi durante il ciclo di vita del dispositivo.</p>
</section>
</section>
</section>
<section id="problemi-di-privacy-nella-gestione-dei-dati" class="level2 page-columns page-full" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="problemi-di-privacy-nella-gestione-dei-dati"><span class="header-section-number">14.7</span> Problemi di Privacy nella Gestione dei Dati</h2>
<p>La gestione sicura ed etica dei dati personali e sensibili √® fondamentale poich√© l‚Äôapprendimento automatico permea dispositivi come smartphone, dispositivi indossabili ed elettrodomestici intelligenti. Per l‚Äôhardware medico, la gestione sicura ed etica dei dati √® ulteriormente richiesta dalla legge tramite l‚Äô<a href="https://aspe.hhs.gov/report/health-insurance-portability-and-accountability-act-1996">Health Insurance Portability and Accountability Act</a> (HIPAA). Questi sistemi ML embedded presentano rischi unici per la privacy, data la loro intima vicinanza alla vita degli utenti.</p>
<section id="tipi-di-dati-sensibili" class="level3" data-number="14.7.1">
<h3 data-number="14.7.1" class="anchored" data-anchor-id="tipi-di-dati-sensibili"><span class="header-section-number">14.7.1</span> Tipi di Dati Sensibili</h3>
<p>I dispositivi ML embedded come quelli indossabili, assistenti domestici intelligenti e veicoli autonomi elaborano spesso dati altamente personali che richiedono un‚Äôattenta gestione per preservare la privacy dell‚Äôutente e prevenirne l‚Äôuso improprio. Esempi specifici includono referti medici e piani di trattamento elaborati da dispositivi indossabili per la salute, conversazioni private costantemente acquisite da assistenti domestici intelligenti e abitudini di guida dettagliate raccolte da auto connesse. La compromissione di tali dati sensibili pu√≤ portare a gravi conseguenze come furto di identit√†, manipolazione emotiva, umiliazione pubblica ed abuso di sorveglianza di massa.</p>
<p>I dati sensibili assumono molte forme: registri strutturati come elenchi di contatti e contenuti non strutturati come flussi audio e video conversazionali. In ambito medico, le ‚Äúprotected health information (PHI)‚Äù [informazioni sanitarie protette] vengono raccolte dai medici durante ogni interazione e sono fortemente regolamentate da rigide linee guida HIPAA. Anche al di fuori degli ambienti medici, i dati sensibili possono comunque essere raccolti sotto forma di <a href="https://www.dol.gov/general/ppii">Personally Identifiable Information</a> (PII) [Informazioni di identificazione personale], definite come ‚Äúqualsiasi rappresentazione di informazioni che consenta di dedurre ragionevolmente l‚Äôidentit√† di un individuo a cui si applicano le informazioni con mezzi diretti o indiretti‚Äù. Esempi di PII includono indirizzi e-mail, numeri di previdenza sociale e numeri di telefono, tra gli altri campi. Le PII vengono raccolte in ambito medico e in altri contesti (applicazioni finanziarie, ecc.) e sono fortemente regolamentate dalle politiche del Dipartimento del Lavoro.</p>
<p>Anche gli output dei modelli derivati potrebbero far trapelare indirettamente dettagli sugli individui. Oltre ai dati personali, anche algoritmi e set di dati proprietari garantiscono la protezione della riservatezza. Nella sezione Data Engineering, abbiamo trattato diversi argomenti in dettaglio.</p>
<p>Tecniche come la de-identificazione, l‚Äôaggregazione, l‚Äôanonimizzazione e la federazione possono aiutare a trasformare i dati sensibili in forme meno rischiose mantenendo al contempo l‚Äôutilit√† analitica. Tuttavia, controlli diligenti su accesso, crittografia, auditing, consenso, minimizzazione e pratiche di conformit√† sono ancora essenziali durante tutto il ciclo di vita dei dati. Regolamenti come <a href="https://gdpr-info.eu/">GDPR</a> categorizzano diverse classi di dati sensibili e prescrivono responsabilit√† in merito alla loro gestione etica. Standard come <a href="https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final">NIST 800-53</a> forniscono rigorose linee guida per il controllo della sicurezza per la protezione della riservatezza. Con la crescente dipendenza dal ML embedded, comprendere i rischi dei dati sensibili √® fondamentale.</p>
</section>
<section id="regolamenti-applicabili" class="level3" data-number="14.7.2">
<h3 data-number="14.7.2" class="anchored" data-anchor-id="regolamenti-applicabili"><span class="header-section-number">14.7.2</span> Regolamenti Applicabili</h3>
<p>Molte applicazioni ML embedded gestiscono dati sensibili degli utenti in base alle normative HIPAA, GDPR e CCPA. Comprendere le protezioni imposte da queste leggi √® fondamentale per creare sistemi conformi.</p>
<ul>
<li><p>La norma sulla privacy <a href="https://www.hhs.gov/hipaa/for-professionals/privacy/index.html">HIPAA</a> stabilisce che i fornitori di assistenza che svolgono determinate attivit√† regolano la privacy e la sicurezza dei dati medici negli Stati Uniti, con severe sanzioni per le violazioni. Tutti i dispositivi ML embedded correlati alla salute, come dispositivi indossabili diagnostici o robot di assistenza, dovrebbero implementare controlli come audit trail, controlli di accesso e crittografia prescritti da HIPAA.</p></li>
<li><p>Il <a href="https://gdpr-info.eu/">GDPR</a> impone trasparenza, limiti di conservazione e diritti degli utenti sui dati dei cittadini dell‚ÄôUE, anche quando elaborati da aziende al di fuori dell‚ÄôUE. I sistemi per la casa intelligente che catturano conversazioni familiari o pattern di posizione dovrebbero essere conformi al GDPR. I requisiti chiave includono la minimizzazione dei dati, la crittografia e meccanismi per il consenso e la cancellazione.</p></li>
<li><p>Il <a href="https://oag.ca.gov/privacy/ccpa">CCPA</a>, che si applica in California, protegge la privacy dei dati dei consumatori tramite disposizioni quali divulgazioni obbligatorie e diritti di opt-out: i gadget IoT come gli smart speaker e i fitness tracker utilizzati dai californiani rientrano probabilmente nel suo ambito.</p></li>
<li><p>Il CCPA √® stato il primo insieme di regolamenti specifici per stato in merito alle preoccupazioni sulla privacy. Dopo il CCPA, regolamenti simili sono stati emanati anche in <a href="https://pro.bloomberglaw.com/brief/state-privacy-legislation-tracker/">altri 10 stati</a>, con alcuni stati che hanno proposto progetti di legge per la protezione della privacy dei dati dei consumatori.</p></li>
</ul>
<p>Inoltre, quando pertinenti all‚Äôapplicazione, le norme specifiche del settore disciplinano telematica, servizi finanziari, servizi di pubblica utilit√†, ecc. Le best practice come ‚ÄúPrivacy by design‚Äù, valutazioni di impatto e mantenimento di audit trail aiutano a incorporare la conformit√† se non √® gi√† richiesta dalla legge. Date le sanzioni potenzialmente costose, √® consigliabile consultare team legali/di conformit√† quando si sviluppano sistemi ML embedded regolamentati.</p>
</section>
<section id="de-identificazione" class="level3" data-number="14.7.3">
<h3 data-number="14.7.3" class="anchored" data-anchor-id="de-identificazione"><span class="header-section-number">14.7.3</span> De-identificazione</h3>
<p>Se i dati medici vengono completamente de-identificati, le linee guida HIPAA non si applicano direttamente e ci sono molte meno normative. Tuttavia, i dati medici devono essere de-identificati utilizzando <a href="https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html">metodi HIPAA</a> (metodi Safe Harbor o metodi Expert Determination) affinch√© le linee guida HIPAA non siano pi√π applicabili.</p>
<section id="metodi-safe-harbor" class="level4">
<h4 class="anchored" data-anchor-id="metodi-safe-harbor">Metodi Safe Harbor</h4>
<p>I metodi Safe Harbor sono pi√π comunemente utilizzati per de-identificare le informazioni sanitarie protette a causa delle risorse limitate necessarie rispetto ai metodi Expert Determination. La de-identificazione Safe Harbor richiede la pulizia dei set di dati di tutti i dati che rientrano in una delle 18 categorie. Le seguenti categorie sono elencate come informazioni sensibili in base allo standard Safe Harbor:</p>
<ul>
<li>Nome, Localizzatore geografico, Data di nascita, Numero di telefono, Indirizzo e-mail, Indirizzi, Numeri di previdenza sociale, Numeri di cartella clinica, Numeri di beneficiari sanitari, Identificatori di dispositivi e Numeri di serie, Numeri di certificati/patenti (Certificato di nascita, Patente di guida, ecc.), Numeri di conto, Identificatori di veicoli, URL di siti Web, Foto a pieno facciale e Immagini comparabili, Identificatori biometrici, Qualsiasi altro identificatore univoco</li>
</ul>
<p>Per la maggior parte di queste categorie, tutti i dati devono essere rimossi indipendentemente dalle circostanze. Per altre categorie, tra cui informazioni geografiche e data di nascita, i dati possono essere rimossi parzialmente quanto basta per rendere le informazioni difficili da re-identificare. Ad esempio, se un codice postale √® abbastanza grande, le prime 3 cifre possono rimanere poich√© ci sono abbastanza persone nell‚Äôarea geografica da rendere difficile la re-identificazione. Le date di nascita devono essere ripulite da tutti gli elementi tranne l‚Äôanno di nascita e tutte le et√† superiori a 89 anni devono essere aggregate in una categoria 90+.</p>
</section>
<section id="metodi-di-determinazione-degli-esperti" class="level4">
<h4 class="anchored" data-anchor-id="metodi-di-determinazione-degli-esperti">Metodi di Determinazione degli Esperti</h4>
<p>I metodi Safe Harbor funzionano per diversi casi di de-identificazione dei dati medici, sebbene in alcuni casi sia ancora possibile la re-identificazione. Ad esempio, supponiamo che si raccolgano dati su un paziente in una citt√† urbana con un grande codice postale, ma √® stata documentata una malattia rara di cui soffre, una malattia che colpisce solo 25 persone in tutta la citt√†. Dati i dati geografici associati all‚Äôanno di nascita, √® altamente possibile che qualcuno possa re-identificare questo individuo, il che rappresenta una violazione della privacy estremamente dannosa.</p>
<p>In casi unici come questi, sono preferiti metodi di de-identificazione dei dati di determinazione esperta. La de-identificazione di determinazione esperta richiede una ‚Äúpersona con conoscenza ed esperienza appropriate di principi e metodi statistici e scientifici generalmente accettati per rendere le informazioni non identificabili individualmente‚Äù per valutare un set di dati e determinare se il rischio di re-identificazione dei dati individuali in un dato set di dati in combinazione con dati disponibili al pubblico (registri di voto, ecc.), √® estremamente ridotto.</p>
<p>La de-identificazione tramite Expert Determination √® comprensibilmente pi√π difficile da completare rispetto alla de-identificazione tramite Safe Harbor, a causa del costo e della fattibilit√† dell‚Äôaccesso a un esperto per verificare la probabilit√† di re-identificazione di un set di dati. Tuttavia, in molti casi, √® richiesta una Expert Determination per garantire che la re-identificazione dei dati sia estremamente improbabile.</p>
</section>
</section>
<section id="riduzione-al-minimo-dei-dati" class="level3 page-columns page-full" data-number="14.7.4">
<h3 data-number="14.7.4" class="anchored" data-anchor-id="riduzione-al-minimo-dei-dati"><span class="header-section-number">14.7.4</span> Riduzione al Minimo dei Dati</h3>
<p>La riduzione al minimo dei dati comporta la raccolta, la conservazione e l‚Äôelaborazione solo dei dati utente necessari per ridurre i rischi per la privacy derivanti dai sistemi ML embedded. Si inizia limitando i tipi di dati e le istanze raccolte al minimo indispensabile per la funzionalit√† di base del sistema. Ad esempio, un modello di rilevamento degli oggetti raccoglie solo le immagini necessarie per quella specifica attivit√† di visione artificiale. Allo stesso modo, un assistente vocale limiterebbe l‚Äôacquisizione audio a specifici comandi vocali anzich√© registrare in modo persistente i suoni ambientali.</p>
<p>Ove possibile, i dati temporanei che risiedono brevemente nella memoria senza archiviazione persistente forniscono un‚Äôulteriore riduzione al minimo. Dovrebbe essere stabilita una chiara base giuridica, come il consenso dell‚Äôutente, per la raccolta e la conservazione. Il sandboxing e i controlli di accesso impediscono l‚Äôuso non autorizzato oltre le attivit√† previste. I periodi di conservazione dovrebbero essere definiti in base allo scopo, con procedure di eliminazione sicura che rimuovono i dati scaduti.</p>
<p>La riduzione al minimo dei dati pu√≤ essere suddivisa in <a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401034?casa_token=NrOifKo6dPMAAAAA:Gl5NZNpZMiuSRpJblj43c1cNXkXyv7oEOuYlOfX2qvT8e-9mOLoLQQYz29itxVh6xakKm8haWRs">3 categorie</a>:</p>
<ol type="1">
<li><p>‚ÄúI dati devono essere <em>adeguati</em> rispetto allo scopo perseguito‚Äù. L‚Äôomissione di dati pu√≤ limitare l‚Äôaccuratezza dei modelli addestrati sui dati e qualsiasi utilit√† generale di un set di dati. La minimizzazione dei dati richiede che una quantit√† minima di dati venga raccolta dagli utenti durante la creazione di un set di dati che aggiunge valore ad altri.</p></li>
<li><p>I dati raccolti dagli utenti devono essere <em>rilevanti</em> allo scopo della raccolta dati.</p></li>
<li><p>I dati degli utenti dovrebbero essere limitati ai soli dati <em>necessari</em> per soddisfare lo scopo della raccolta dati iniziale. Se √® possibile ottenere risultati altrettanto solidi e accurati da un set di dati pi√π piccolo, non dovrebbero essere raccolti dati aggiuntivi oltre questo set di dati pi√π piccolo.</p></li>
</ol>
<p>Tecniche emergenti come la privacy differenziale, l‚Äôaddestramento federato e la generazione di dati sintetici consentono informazioni utili derivate da dati utente meno grezzi. L‚Äôesecuzione di mappature del flusso di dati e valutazioni di impatto aiutano a identificare le opportunit√† per ridurre al minimo l‚Äôutilizzo di dati grezzi.</p>
<p>Metodologie come Privacy by Design <span class="citation" data-cites="cavoukian2009privacy">(<a href="../../../references.it.html#ref-cavoukian2009privacy" role="doc-biblioref">Cavoukian 2009</a>)</span> considerano tale minimizzazione all‚Äôinizio dell‚Äôarchitettura del sistema. Anche normative come il GDPR impongono principi di minimizzazione dei dati. Con un approccio multilayer nei regni legale, tecnico e di processo, la minimizzazione dei dati limita i rischi nei prodotti ML embedded.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cavoukian2009privacy" class="csl-entry" role="listitem">
Cavoukian, Ann. 2009. <span>¬´Privacy by design¬ª</span>. <em>Office of the Information and Privacy Commissioner</em>.
</div></div><section id="caso-di-studio-minimizzazione-dei-dati-basata-sulle-prestazioni" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="caso-di-studio-minimizzazione-dei-dati-basata-sulle-prestazioni">Caso di Studio: Minimizzazione dei Dati Basata sulle Prestazioni</h4>
<p>La minimizzazione dei dati basata sulle prestazioni <span class="citation" data-cites="Biega2020Oper">(<a href="../../../references.it.html#ref-Biega2020Oper" role="doc-biblioref">Biega et al. 2020</a>)</span> si concentra sull‚Äôespansione della terza categoria di minimizzazione dei dati menzionata sopra, ovvero la <em>limitazione</em>. Definisce specificamente la robustezza dei risultati del modello su un dato set di dati tramite determinate metriche delle prestazioni, in modo che i dati non debbano essere raccolti ulteriormente se non migliorano significativamente le prestazioni. Le metriche delle prestazioni possono essere divise in due categorie:</p>
<div class="no-row-height column-margin column-container"><div id="ref-Biega2020Oper" class="csl-entry" role="listitem">
Biega, Asia J., Peter Potash, Hal Daum√©, Fernando Diaz, e Mich√®le Finck. 2020. <span>¬´Operationalizing the Legal Principle of Data Minimization for Personalization¬ª</span>. In <em>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, a cura di Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, e Yiqun Liu, 399‚Äì408. ACM. <a href="https://doi.org/10.1145/3397271.3401034">https://doi.org/10.1145/3397271.3401034</a>.
</div></div><ol type="1">
<li><p>Prestazioni di minimizzazione dei dati globali: Soddisfatte se un set di dati riduce al minimo la quantit√† di dati per utente mentre le sue prestazioni medie su tutti i dati sono paragonabili alle prestazioni medie del set di dati originale non minimizzato.</p></li>
<li><p>Prestazioni di minimizzazione dei dati per utente: Soddisfatte se un set di dati riduce al minimo la quantit√† di dati per utente mentre le prestazioni minime dei dati utente individuali sono paragonabili a quelle dei dati utente individuali nel set di dati originale non minimizzato.</p></li>
</ol>
<p>La riduzione al minimo dei dati basata sulle prestazioni pu√≤ essere sfruttata in impostazioni di apprendimento automatico, inclusi algoritmi di raccomandazione di film e impostazioni di e-commerce.</p>
<p>La riduzione al minimo dei dati globali √® molto pi√π fattibile della riduzione al minimo dei dati per utente, data la differenza molto pi√π significativa nelle perdite per utente tra i set di dati ridotti al minimo e quelli originali.</p>
</section>
</section>
<section id="consenso-e-trasparenza" class="level3" data-number="14.7.5">
<h3 data-number="14.7.5" class="anchored" data-anchor-id="consenso-e-trasparenza"><span class="header-section-number">14.7.5</span> Consenso e Trasparenza</h3>
<p>Un consenso e una trasparenza significativi sono fondamentali quando si raccolgono dati utente per prodotti ML embedded come smart speaker, dispositivi indossabili e veicoli autonomi. Quando viene configurato per la prima volta. Idealmente, il dispositivo dovrebbe spiegare chiaramente quali tipi di dati vengono raccolti, per quali scopi, come vengono elaborati e le policy di conservazione. Ad esempio, uno smart speaker potrebbe raccogliere campioni vocali per addestrare il riconoscimento vocale e profili vocali personalizzati. Durante l‚Äôuso, promemoria e opzioni della dashboard forniscono una trasparenza continua su come vengono gestiti i dati, come riepiloghi settimanali di frammenti vocali acquisiti. Le opzioni di controllo consentono di revocare o limitare il consenso, come disattivare l‚Äôarchiviazione dei profili vocali.</p>
<p>I flussi di consenso dovrebbero fornire controlli granulari che vadano oltre le semplici scelte binarie s√¨/no. Ad esempio, gli utenti potrebbero acconsentire selettivamente a determinati utilizzi dei dati, come l‚Äôaddestramento al riconoscimento vocale, ma non alla personalizzazione. I focus group e i test di usabilit√† con gli utenti target modellano le interfacce di consenso e la formulazione delle policy sulla privacy per ottimizzare la comprensione e il controllo. Il rispetto dei diritti degli utenti, come l‚Äôeliminazione e la rettifica dei dati, dimostra affidabilit√†. Un gergo legale vago ostacola la trasparenza. Regolamenti come GDPR e CCPA rafforzano i requisiti di consenso. Un consenso ponderato e la trasparenza forniscono agli utenti l‚Äôagenzia sui propri dati, creando al contempo fiducia nei prodotti ML incorporati attraverso una comunicazione e un controllo aperti.</p>
</section>
<section id="problemi-di-privacy-nellapprendimento-automatico" class="level3 page-columns page-full" data-number="14.7.6">
<h3 data-number="14.7.6" class="anchored" data-anchor-id="problemi-di-privacy-nellapprendimento-automatico"><span class="header-section-number">14.7.6</span> Problemi di Privacy nell‚ÄôApprendimento Automatico</h3>
<section id="ia-generativa" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="ia-generativa">IA Generativa</h4>
<p>Sono aumentate anche le preoccupazioni sulla privacy e sulla sicurezza con l‚Äôuso pubblico di modelli di intelligenza artificiale generativa, tra cui GPT4 di OpenAI e altri LLM. ChatGPT, in particolare, √® stato discusso pi√π di recente in merito alla privacy, date tutte le informazioni personali raccolte dagli utenti di ChatGPT. Nel giugno 2023 √® <a href="https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/rIZH4FXwShJE/v0">stata intentata una class action</a> contro ChatGPT a causa del timore che fosse stato addestrato su informazioni mediche e personali proprietarie senza le dovute autorizzazioni o consensi. Come risultato di queste preoccupazioni sulla privacy, <a href="https://www.businessinsider.com/chatgpt-companies-issued-bans-restrictions-openai-ai-amazon-apple-2023-7">molte aziende</a> hanno proibito ai propri dipendenti di accedere a ChatGPT e di caricare informazioni aziendali private sul chatbot. Inoltre, ChatGPT √® suscettibile al ‚Äúprompt injection‚Äù e altri attacchi alla sicurezza che potrebbero compromettere la privacy dei dati proprietari su cui √® stato addestrato.</p>
<section id="caso-di-studio-aggiramento-delle-misure-di-sicurezza-di-chatgpt" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="caso-di-studio-aggiramento-delle-misure-di-sicurezza-di-chatgpt">Caso di Studio: Aggiramento delle Misure di Sicurezza di ChatGPT</h5>
<p>Mentre ChatGPT ha istituito delle protezioni per impedire alle persone di accedere a informazioni private ed eticamente discutibili, diversi individui sono riusciti a bypassare queste protezioni tramite ‚Äúprompt injection‚Äù e altri attacchi alla sicurezza. Come dimostrato in <a href="#fig-role-play" class="quarto-xref">Figura&nbsp;<span>14.10</span></a>, gli utenti possono bypassare le protezioni di ChatGPT per imitare il tono di una ‚Äúnonna defunta‚Äù per imparare come bypassare un firewall per applicazioni Web <span class="citation" data-cites="Gupta2023ChatGPT">(<a href="../../../references.it.html#ref-Gupta2023ChatGPT" role="doc-biblioref">Gupta et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-role-play" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-role-play-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Grandma_role_play_to_bypass_safety_restrictions.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-role-play-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.10: Gioco di ruolo della nonna per bypassare le restrizioni di sicurezza. Fonte: <span class="citation" data-cites="Gupta2023ChatGPT">Gupta et al. (<a href="../../../references.it.html#ref-Gupta2023ChatGPT" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>Inoltre, gli utenti hanno anche utilizzato con successo la psicologia inversa per manipolare ChatGPT e accedere a informazioni inizialmente proibite dal modello. In <a href="#fig-role-play2" class="quarto-xref">Figura&nbsp;<span>14.11</span></a>, a un utente viene inizialmente impedito di scoprire siti Web di pirateria tramite ChatGPT, ma pu√≤ bypassare queste restrizioni utilizzando la psicologia inversa.</p>
<div id="fig-role-play2" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-role-play2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Reverse_psychology_to_bypass_safety_restrictions.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-role-play2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.11: Psicologia inversa per bypassare le restrizioni di sicurezza. Fonte: <span class="citation" data-cites="Gupta2023ChatGPT">Gupta et al. (<a href="../../../references.it.html#ref-Gupta2023ChatGPT" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-Gupta2023ChatGPT" class="csl-entry" role="listitem">
Gupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, e Lopamudra Praharaj. 2023. <span>¬´From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy¬ª</span>. <em>IEEE Access</em> 11: 80218‚Äì45. <a href="https://doi.org/10.1109/access.2023.3300381">https://doi.org/10.1109/access.2023.3300381</a>.
</div></div></figure>
</div>
<p>La facilit√† con cui gli attacchi alla sicurezza possono manipolare ChatGPT √® preoccupante, date le informazioni private su cui √® stato addestrato senza consenso. Ulteriori ricerche sulla privacy dei dati in LLM e sull‚Äôintelligenza artificiale generativa dovrebbero concentrarsi sull‚Äôimpedire al modello di essere cos√¨ ingenuo da indurre attacchi di ‚Äúinjection‚Äù.</p>
</section>
</section>
<section id="cancellazione-dei-dati" class="level4">
<h4 class="anchored" data-anchor-id="cancellazione-dei-dati">Cancellazione dei Dati</h4>
<p>Molte delle normative precedentemente menzionate, incluso il GDPR, includono una clausola sul ‚Äúdiritto all‚Äôoblio‚Äù. Questa <a href="https://gdpr-info.eu/art-17-gdpr/">clausola</a> afferma essenzialmente che ‚Äúl‚Äôinteressato ha il diritto di ottenere dal titolare del trattamento la cancellazione dei dati personali che lo riguardano senza indebito ritardo‚Äù. Tuttavia, in diversi casi, anche se i dati dell‚Äôutente sono stati cancellati da una piattaforma, i dati vengono cancellati solo parzialmente se un modello di apprendimento automatico √® stato addestrato su questi dati per scopi separati. Attraverso metodi simili agli attacchi di inferenza di appartenenza, altri individui possono ancora dedurre i dati di addestramento su cui √® stato addestrato un modello, anche se la presenza dei dati √® stata esplicitamente rimossa online.</p>
<p>Un approccio per affrontare i problemi di privacy con i dati di addestramento dell‚Äôapprendimento automatico √® stato attraverso metodi di privacy differenziali. Ad esempio, aggiungendo rumore laplaciano nel set di addestramento, un modello pu√≤ essere robusto agli attacchi di inferenza di appartenenza, impedendo il recupero dei dati eliminati. Un altro approccio per impedire che i dati eliminati vengano dedotti da attacchi alla sicurezza √® semplicemente riaddestrare il modello da zero sui dati rimanenti. Poich√© questo processo √® dispendioso in termini di tempo e di elaborazione dati, altri ricercatori hanno tentato di affrontare le preoccupazioni sulla privacy relative all‚Äôinferenza dei dati di training del modello tramite un processo chiamato ‚Äúmachine unlearning‚Äù, in cui un modello itera attivamente su se stesso per rimuovere l‚Äôinfluenza dei dati ‚Äúdimenticati‚Äù su cui potrebbe essere stato addestrato, come menzionato di seguito.</p>
</section>
</section>
</section>
<section id="tecniche-ml-per-la-tutela-della-privacy" class="level2 page-columns page-full" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="tecniche-ml-per-la-tutela-della-privacy"><span class="header-section-number">14.8</span> Tecniche ML per la Tutela della Privacy</h2>
<p>Sono state sviluppate molte tecniche per preservare la privacy, ognuna delle quali affronta diversi aspetti e sfide per la sicurezza dei dati. Questi metodi possono essere ampiamente categorizzati in diverse aree chiave: <strong>Differential Privacy</strong>, che si concentra sulla privacy statistica negli output dei dati; <strong>Federated Learning</strong>, che enfatizza l‚Äôelaborazione decentralizzata dei dati; <strong>Homomorphic Encryption e Secure Multi-party Computation (SMC)</strong>, entrambi abilitanti calcoli sicuri su dati crittografati o privati; <strong>Data Anonymization</strong> e <strong>Data Masking and Obfuscation</strong>, che alterano i dati per proteggere le identit√† individuali; <strong>Private Set Intersection</strong> e <strong>Zero-Knowledge Proofs</strong>, che facilitano confronti e convalide di dati sicuri; <strong>Decentralized Identifiers (DID)</strong> per identit√† digitali auto-sovrane; <strong>Privacy-Preserving Record Linkage (PPRL)</strong>, che collega i dati tra le fonti senza esposizione; <strong>Synthetic Data Generation</strong>, che crea set di dati artificiali per analisi sicure; e <strong>Adversarial Learning Techniques</strong>, che migliora la resistenza dei dati o dei modelli agli attacchi alla privacy.</p>
<p>Data l‚Äôampia gamma di queste tecniche, non √® possibile approfondire ciascuna di esse in un singolo corso o discussione, e tanto meno che qualcuno possa conoscerle tutte nei loro gloriosi dettagli. Pertanto, esploreremo alcune tecniche specifiche in modo relativamente dettagliato, fornendo una comprensione pi√π approfondita dei loro principi, applicazioni e delle sfide uniche per la privacy che affrontano nell‚Äôapprendimento automatico. Questo approccio mirato ci fornir√† una comprensione pi√π completa e pratica dei principali metodi di tutela della privacy nei moderni sistemi ML.</p>
<section id="privacy-differenziale" class="level3 page-columns page-full" data-number="14.8.1">
<h3 data-number="14.8.1" class="anchored" data-anchor-id="privacy-differenziale"><span class="header-section-number">14.8.1</span> Privacy Differenziale</h3>
<section id="idea-centrale" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="idea-centrale">Idea Centrale</h4>
<p>La ‚ÄúDifferential Privacy‚Äù <a href="#privacy-differenziale">Privacy Differenziale</a> √® un framework per quantificare e gestire la privacy degli individui in un set di dati <span class="citation" data-cites="Dwork2006Theory">(<a href="../../../references.it.html#ref-Dwork2006Theory" role="doc-biblioref">Dwork et al. 2006</a>)</span>. Fornisce una garanzia matematica che la privacy degli individui nel set di dati non verr√† compromessa, indipendentemente da qualsiasi conoscenza aggiuntiva che un aggressore potrebbe possedere. L‚Äôidea fondamentale della privacy differenziale √® che il risultato di qualsiasi analisi (come una query statistica) dovrebbe essere essenzialmente lo stesso, indipendentemente dal fatto che i dati di un individuo siano inclusi nel set di dati o meno. Ci√≤ significa che osservando il risultato dell‚Äôanalisi, non √® possibile determinare se i dati di un individuo siano stati utilizzati nel calcolo.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Dwork2006Theory" class="csl-entry" role="listitem">
Dwork, Cynthia, Frank McSherry, Kobbi Nissim, e Adam Smith. 2006. <span>¬´Calibrating Noise to Sensitivity in Private Data Analysis¬ª</span>. In <em>Theory of Cryptography</em>, a cura di Shai Halevi e Tal Rabin, 265‚Äì84. Berlin, Heidelberg: Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/11681878\_14">https://doi.org/10.1007/11681878\_14</a>.
</div></div><p>Ad esempio, supponiamo che un database contenga cartelle cliniche di 10 pazienti. Vogliamo pubblicare statistiche sulla prevalenza del diabete in questo campione senza rivelare le condizioni di un paziente. Per fare ci√≤, potremmo aggiungere una piccola quantit√† di rumore casuale al conteggio reale prima di pubblicarlo. Se il numero reale di pazienti diabetici √® 6, potremmo aggiungere rumore da una distribuzione di Laplace per ottenere casualmente 5, 6 o 7, ciascuno con una certa probabilit√†. Un osservatore ora non pu√≤ dire se un singolo paziente ha il diabete basandosi solo sull‚Äôoutput rumoroso. Il risultato della query √® simile a se i dati di ogni paziente sono inclusi o esclusi. Questa √® la privacy differenziale. Pi√π formalmente, un algoritmo randomizzato soddisfa la privacy differenziale Œµ se, per qualsiasi database vicino D e D π che differisce solo per una voce, la probabilit√† di qualsiasi risultato cambia al massimo di un fattore Œµ. Un Œµ inferiore fornisce maggiori garanzie di privacy.</p>
<p>Il meccanismo di Laplace √® uno dei metodi pi√π semplici e comunemente utilizzati per ottenere la privacy differenziale. Comporta l‚Äôaggiunta di rumore che segue una distribuzione di Laplace ai dati o ai risultati delle query. A parte il Meccanismo di Laplace, il principio generale di aggiunta di rumore √® fondamentale per la Privacy differenziale. L‚Äôidea √® di aggiungere rumore casuale ai dati o ai risultati di una query. Il rumore √® calibrato per garantire la necessaria garanzia di privacy mantenendo i dati utili.</p>
<p>Mentre la distribuzione di Laplace √® comune, possono essere utilizzate anche altre distribuzioni come quella gaussiana. Il rumore di Laplace √® utilizzato per la privacy differenziale rigorosa Œµ per query a bassa sensibilit√†. Al contrario, le distribuzioni gaussiane possono essere utilizzate quando la privacy non √® garantita, nota come privacy differenziale (œµ, ùõø). In questa versione rilassata della privacy differenziale, epsilon e delta definiscono la quantit√† di privacy garantita quando si rilasciano informazioni o un modello correlato a un set di dati. Epsilon stabilisce un limite su quanta informazione pu√≤ essere appresa sui dati in base all‚Äôoutput. Allo stesso tempo, delta consente una piccola probabilit√† che la garanzia della privacy venga violata. La scelta tra Laplace, gaussiana e altre distribuzioni dipender√† dai requisiti specifici della query e del set di dati e dal compromesso tra Privacy e accuratezza.</p>
<p>Per illustrare il compromesso tra Privacy e accuratezza nella Privacy differenziale (<span class="math inline">\(\epsilon\)</span>, <span class="math inline">\(\delta\)</span>), i seguenti grafici in <a href="#fig-tradeoffs" class="quarto-xref">Figura&nbsp;<span>14.12</span></a> mostrano i risultati sull‚Äôaccuratezza per diversi livelli di rumore sul set di dati MNIST, un ampio set di dati di cifre scritte a mano <span class="citation" data-cites="abadi2016deep">(<a href="../../../references.it.html#ref-abadi2016deep" role="doc-biblioref">Abadi et al. 2016</a>)</span>. Il valore delta (linea nera; asse y destro) indica il livello di rilassamento della privacy (un valore elevato indica che la Privacy √® meno rigorosa). Man mano che la Privacy diventa pi√π rilassata, aumenta l‚Äôaccuratezza del modello.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-tradeoffs" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-tradeoffs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Privacy-accuracy_tradeoff.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tradeoffs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.12: Compromesso tra privacy e accuratezza. Fonte: <span class="citation" data-cites="abadi2016deep">Abadi et al. (<a href="../../../references.it.html#ref-abadi2016deep" role="doc-biblioref">2016</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-abadi2016deep" class="csl-entry" role="listitem">
Abadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. <span>¬´Deep Learning with Differential Privacy¬ª</span>. In <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>, 308‚Äì18. CCS ‚Äô16. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/2976749.2978318">https://doi.org/10.1145/2976749.2978318</a>.
</div></div></figure>
</div>
<p>I punti chiave da ricordare sulla privacy differenziale sono i seguenti:</p>
<ul>
<li><p><strong>Aggiunta di Rumore:</strong> La tecnica fondamentale nella Privacy differenziale √® l‚Äôaggiunta di rumore casuale controllato ai dati o ai risultati delle query. Questo rumore maschera il contributo dei singoli dati.</p></li>
<li><p><strong>Atto di Bilanciamento:</strong> C‚Äô√® un equilibrio tra Privacy e accuratezza. Pi√π rumore (œµ inferiore) nei dati significa maggiore Privacy ma minore accuratezza nei risultati del modello.</p></li>
<li><p><strong>Universalit√†:</strong> La privacy differenziale non si basa su ipotesi su ci√≤ che sa un aggressore.s. Ci√≤ lo rende robusto contro gli attacchi di re-identificazione, in cui un aggressore cerca di scoprire dati individuali.</p></li>
<li><p><strong>Applicabilit√†:</strong> Pu√≤ essere applicato a vari tipi di dati e query, rendendolo uno strumento versatile per l‚Äôanalisi dei dati che preserva la privacy.</p></li>
</ul>
</section>
<section id="compromessi-2" class="level4">
<h4 class="anchored" data-anchor-id="compromessi-2">Compromessi</h4>
<p>Ci sono diversi compromessi da fare con la Privacy differenziale, come nel caso di qualsiasi algoritmo. Ma concentriamoci sui compromessi specifici computazionali, poich√© ci interessano i sistemi ML. Ci sono alcune considerazioni e compromessi computazionali chiave quando si implementa la privacy differenziale in un sistema di apprendimento automatico:</p>
<p><strong>Generazione di Rumore:</strong> L‚Äôimplementazione della privacy differenziale introduce diversi compromessi computazionali importanti rispetto alle tecniche di apprendimento automatico standard. Una considerazione importante √® la necessit√† di generare in modo sicuro rumore casuale da distribuzioni come Laplace o Gaussiana che vengono aggiunte ai risultati delle query e agli output del modello. La generazione di numeri casuali crittografici di alta qualit√† pu√≤ essere computazionalmente costosa.</p>
<p><strong>Analisi di Sensibilit√†:</strong> Un altro requisito fondamentale √® il monitoraggio rigoroso della sensibilit√† degli algoritmi sottostanti ai singoli punti dati che vengono aggiunti o rimossi. Questa analisi di sensibilit√† globale √® necessaria per calibrare correttamente i livelli di rumore. Tuttavia, l‚Äôanalisi della sensibilit√† del caso peggiore pu√≤ aumentare sostanzialmente la complessit√† computazionale per complesse procedure di addestramento del modello e pipeline di dati.</p>
<p><strong>Gestione del budget per la privacy:</strong> La gestione del budget per la perdita della privacy su pi√π query e iterazioni di apprendimento √® un altro sovraccarico contabile. Il sistema deve tenere traccia dei costi cumulativi per la privacy e comporli per spiegare le garanzie di privacy complessive. Ci√≤ aggiunge un onere computazionale che va oltre la semplice esecuzione di query o modelli di addestramento.</p>
<p><strong>Compromessi tra batch e online:</strong> Per i sistemi di apprendimento online con query continue ad alto volume, gli algoritmi differenzialmente privati richiedono nuovi meccanismi per mantenere l‚Äôutilit√† e prevenire troppe perdite di privacy accumulate poich√© ogni query pu√≤ potenzialmente alterare il budget per la privacy. L‚Äôelaborazione offline in batch √® pi√π semplice da una prospettiva computazionale poich√© elabora i dati in grandi batch, dove ogni batch viene trattato come una singola query. I dati sparsi ad alta dimensionalit√† aumentano anche le difficolt√† dell‚Äôanalisi di sensibilit√†.</p>
<p><strong>Addestramento distribuito:</strong> Quando si addestrano modelli utilizzando approcci <a href="../../../contents/core/training/training.it.html">distribuiti</a> o <a href="../../../contents/core/optimizations/optimizations.it.html">federati</a>, sono necessari nuovi protocolli crittografici per tracciare e limitare le ‚Äúfughe‚Äù di privacy tra i nodi. Il calcolo multi-parti sicuro con dati crittografati per la Privacy differenziale aggiunge un carico computazionale sostanziale.</p>
<p>Mentre la Privacy differenziale fornisce solide garanzie formali di privacy, la sua implementazione rigorosa richiede aggiunte e modifiche alla pipeline di apprendimento automatico a un costo computazionale. La gestione di queste spese generali preservando l‚Äôaccuratezza del modello rimane un‚Äôarea di ricerca attiva.</p>
</section>
<section id="caso-di-studio-privacy-differenziale-in-apple" class="level4">
<h4 class="anchored" data-anchor-id="caso-di-studio-privacy-differenziale-in-apple">Caso di Studio: Privacy Differenziale in Apple</h4>
<p><a href="https://machinelearning.apple.com/research/learning-with-privacy-at-scale#DMNS06">L‚Äôimplementazione della Privacy differenziale da parte di Apple</a> in iOS e MacOS fornisce un importante esempio concreto di <a href="https://docs-assets.developer.apple.com/ml-research/papers/learning-with-privacy-at-scale.pdf">come la Privacy differenziale pu√≤ essere distribuita su larga scala</a>. Apple voleva raccogliere statistiche aggregate sull‚Äôutilizzo nel proprio ecosistema per migliorare prodotti e servizi, ma mirava a farlo senza compromettere la privacy dei singoli utenti.</p>
<p>Per raggiungere questo obiettivo, ha implementato tecniche di privacy differenziale direttamente sui dispositivi degli utenti per rendere anonimi i punti dati prima di inviarli ai server Apple. In particolare, Apple utilizza il meccanismo di Laplace per iniettare rumore casuale attentamente calibrato. Ad esempio, supponiamo che la cronologia delle posizioni di un utente contenga [Lavoro, Casa, Lavoro, Palestra, Lavoro, Casa]. In tal caso, la versione privata differenziale potrebbe sostituire le posizioni esatte con un campione rumoroso come [Palestra, Casa, Lavoro, Lavoro, Casa, Lavoro].</p>
<p>Apple regola la distribuzione del rumore di Laplace per fornire un elevato livello di privacy preservando al contempo l‚Äôutilit√† delle statistiche aggregate. L‚Äôaumento dei livelli di rumore fornisce maggiori garanzie di privacy (valori Œµ inferiori nella terminologia DP) ma pu√≤ ridurre l‚Äôutilit√† dei dati. Gli ingegneri della privacy di Apple hanno ottimizzato empiricamente questo compromesso in base ai loro obiettivi di prodotto.</p>
<p>Apple ottiene statistiche aggregate ad alta fedelt√† aggregando centinaia di milioni di punti dati rumorosi dai dispositivi. Ad esempio, possono analizzare le funzionalit√† delle nuove app iOS mascherando i comportamenti delle app di qualsiasi utente. Il calcolo sul dispositivo evita di inviare dati grezzi ai server Apple.</p>
<p>Il sistema utilizza la generazione di numeri casuali sicuri basata su hardware per campionare in modo efficiente dalla distribuzione di Laplace sui dispositivi. Apple ha anche dovuto ottimizzare i suoi algoritmi e pipeline differenzialmente privati per operare sotto i vincoli computazionali dell‚Äôhardware degli utenti.</p>
<p>Numerosi audit di terze parti hanno verificato che il sistema Apple fornisce rigorose protezioni differenziali della privacy in linea con le loro politiche dichiarate. Naturalmente, le ipotesi sulla composizione nel tempo e sui potenziali rischi di reidentificazione sono ancora valide. L‚Äôimplementazione di Apple mostra come la privacy differenziale pu√≤ essere realizzata in grandi prodotti del mondo reale quando supportata da sufficienti risorse ingegneristiche.</p>
<div id="exr-dptf" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;14.1: Privacy Differenziale - Privacy TensorFlow
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Volete addestrare un modello ML senza compromettere i segreti di nessuno? La Privacy differenziale √® come un superpotere per i dati! In questo Colab, useremo TensorFlow Privacy per aggiungere rumore speciale durante l‚Äôaddestramento. Ci√≤ rende molto pi√π difficile per chiunque determinare se sono stati utilizzati i dati di una singola persona, anche se hanno modi furtivi per sbirciare il modello.</p>
<p><a href="https://colab.research.google.com/github/tensorflow/privacy/blob/master/g3doc/tutorials/classification_privacy.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="il-federated-learning" class="level3 page-columns page-full" data-number="14.8.2">
<h3 data-number="14.8.2" class="anchored" data-anchor-id="il-federated-learning"><span class="header-section-number">14.8.2</span> Il Federated Learning</h3>
<section id="idea-centrale-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="idea-centrale-1">Idea Centrale</h4>
<p>Il ‚ÄúFederated Learning (FL)‚Äù √® un tipo di apprendimento automatico in cui un modello viene creato e distribuito su pi√π dispositivi o server mantenendo localizzati i dati di training. √à stato precedentemente discusso nel capitolo <a href="../../../contents/core/optimizations/optimizations.it.html">Ottimizzazioni del modello</a>. Tuttavia, lo riepilogheremo brevemente qui per completarlo e concentrarci su cose che riguardano questo capitolo.</p>
<p>FL addestra modelli di apprendimento automatico su reti decentralizzate di dispositivi o sistemi, mantenendo tutti i dati di addestramento localizzati. <a href="#fig-fl-lifecycle" class="quarto-xref">Figura&nbsp;<span>14.13</span></a> illustra questo processo: ogni dispositivo partecipante sfrutta i propri dati locali per calcolare gli aggiornamenti del modello, che vengono poi aggregati per creare un modello globale migliorato. Tuttavia, i dati di training grezzi non vengono mai condivisi, trasferiti o compilati direttamente. Questo approccio di tutela della privacy consente lo sviluppo congiunto di modelli ML senza centralizzare i dati di training potenzialmente sensibili in un unico posto.</p>
<div id="fig-fl-lifecycle" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-fl-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Federated_Learning_lifecycle.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fl-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.13: Ciclo di Vita dell‚ÄôApprendimento Federato. Fonte: <span class="citation" data-cites="jin2020towards">Jin et al. (<a href="../../../references.it.html#ref-jin2020towards" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-jin2020towards" class="csl-entry" role="listitem">
Jin, Yilun, Xiguang Wei, Yang Liu, e Qiang Yang. 2020. <span>¬´Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective¬ª</span>. <em>arXiv preprint arXiv:2002.11545</em>, febbraio. <a href="http://arxiv.org/abs/2002.11545v2">http://arxiv.org/abs/2002.11545v2</a>.
</div></div></figure>
</div>
<p>Uno degli algoritmi di aggregazione di modelli pi√π comuni √® Federated Averaging (FedAvg), in cui il modello globale viene creato calcolando la media di tutti i parametri dai parametri locali. Mentre FedAvg funziona bene con dati indipendenti e distribuiti in modo identico (IID), algoritmi alternativi come Federated Proximal (FedProx) sono fondamentali nelle applicazioni del mondo reale in cui i dati sono spesso non IID. FedProx √® progettato per il processo FL quando c‚Äô√® una significativa eterogeneit√† negli aggiornamenti client a causa di diverse distribuzioni di dati tra dispositivi, capacit√† di calcolo o quantit√† variabili di dati.</p>
<p>Lasciando i dati grezzi distribuiti e scambiando solo aggiornamenti temporanei del modello, l‚Äôapprendimento federato fornisce un‚Äôalternativa pi√π sicura e che migliora la privacy alle tradizionali pipeline di apprendimento automatico centralizzate. Ci√≤ consente alle organizzazioni e agli utenti di trarre vantaggio in modo collaborativo da modelli condivisi mantenendo al contempo il controllo e la propriet√† sui dati sensibili. La natura decentralizzata di FL lo rende anche robusto per singoli punti di errore.</p>
<p>Si immagini un gruppo di ospedali che desidera collaborare a uno studio per prevedere i risultati dei pazienti in base ai loro sintomi. Tuttavia, non possono condividere i dati dei pazienti a causa di problemi di privacy e normative come HIPAA. Ecco come il Federated Learning pu√≤ aiutare.</p>
<ul>
<li><p><strong>Addestramento Locale:</strong> Ogni ospedale addestra un modello di apprendimento automatico sui dati dei pazienti. Questo addestramento avviene localmente, il che significa che i dati non lasciano mai i server dell‚Äôospedale.</p></li>
<li><p><strong>Condivisione del Modello:</strong> Dopo l‚Äôaddestramento, ogni ospedale invia solo il modello (in particolare, i suoi parametri o pesi) a un server centrale. Non invia alcun dato del paziente.</p></li>
<li><p><strong>Modelli di Aggregazione:</strong> Il server centrale aggrega questi modelli da tutti gli ospedali in un singolo modello pi√π robusto. Questo processo in genere comporta la media dei parametri del modello.</p></li>
<li><p><strong>Vantaggio:</strong> Il risultato √® un modello di apprendimento automatico che ha appreso da un‚Äôampia gamma di dati dei pazienti senza condividere dati sensibili o rimuoverli dalla loro posizione originale.</p></li>
</ul>
</section>
<section id="compromessi-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="compromessi-3">Compromessi</h4>
<p>Ci sono diversi aspetti correlati alle prestazioni del sistema di FL nei sistemi di apprendimento automatico. Sarebbe saggio comprendere questi compromessi perch√© non esiste un ‚Äúpranzo gratis‚Äù per preservare la privacy tramite FL <span class="citation" data-cites="Li2020Federated">(<a href="../../../references.it.html#ref-Li2020Federated" role="doc-biblioref">Li et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Li2020Federated" class="csl-entry" role="listitem">
Li, Tian, Anit Kumar Sahu, Ameet Talwalkar, e Virginia Smith. 2020. <span>¬´Federated Learning: Challenges, Methods, and Future Directions¬ª</span>. <em>IEEE Signal Processing Magazine</em> 37 (3): 50‚Äì60. <a href="https://doi.org/10.1109/msp.2020.2975749">https://doi.org/10.1109/msp.2020.2975749</a>.
</div></div><p><strong>Sovraccarico di Comunicazione e Vincoli di Rete:</strong> Nel FL, una delle sfide pi√π significative √® la gestione del sovraccarico di comunicazione. Ci√≤ comporta la frequente trasmissione di aggiornamenti del modello tra un server centrale e numerosi dispositivi client, che pu√≤ richiedere molta larghezza di banda. Il numero totale di round di comunicazione e la dimensione dei messaggi trasmessi per round devono essere ridotti per ridurre ulteriormente la comunicazione. Ci√≤ pu√≤ comportare un traffico di rete sostanziale, soprattutto in scenari con molti partecipanti. Inoltre, la latenza diventa un fattore critico: il tempo impiegato per inviare, aggregare e ridistribuire questi aggiornamenti pu√≤ causare ritardi. Ci√≤ influisce sul tempo di training complessivo e ha un impatto sulla reattivit√† del sistema e sulle capacit√† in tempo reale. Gestire questa comunicazione riducendo al minimo l‚Äôutilizzo della larghezza di banda e la latenza √® fondamentale per implementare FL.</p>
<p><strong>Carico di Calcolo sui Dispositivi Locali:</strong> FL si basa su dispositivi client (come smartphone o dispositivi IoT, che sono particolarmente importanti in TinyML) per l‚Äôaddestramento del modello, che spesso hanno una potenza di calcolo e una durata della batteria limitate. L‚Äôesecuzione di algoritmi complessi di apprendimento automatico in locale pu√≤ mettere a dura prova queste risorse, portando a potenziali problemi di prestazioni. Inoltre, le capacit√† di questi dispositivi possono variare in modo significativo, con conseguenti contributi non uniformi al processo di addestramento del modello. Alcuni dispositivi elaborano gli aggiornamenti in modo pi√π rapido ed efficiente di altri, portando a disparit√† nel processo di apprendimento. Bilanciare il carico computazionale per garantire una partecipazione e un‚Äôefficienza coerenti su tutti i dispositivi √® una sfida fondamentale in FL.</p>
<p><strong>Efficienza dell‚ÄôAddestramento del Modello:</strong> La natura decentralizzata di FL pu√≤ influire sull‚Äôefficienza dell‚Äôaddestramento del modello. Raggiungere la convergenza, in cui il modello non migliora pi√π in modo significativo, pu√≤ essere pi√π lento in FL rispetto ai metodi di addestramento centralizzati. Ci√≤ √® particolarmente vero nei casi in cui i dati sono non IID (non indipendenti e distribuiti in modo identico) tra i dispositivi. Inoltre, gli algoritmi utilizzati per aggregare gli aggiornamenti del modello svolgono un ruolo fondamentale nel processo di addestramento. La loro efficienza influisce direttamente sulla velocit√† e l‚Äôefficacia dell‚Äôapprendimento. Sviluppare e implementare algoritmi in grado di gestire le complessit√† di FL garantendo al contempo una convergenza tempestiva √® essenziale per le prestazioni del sistema.</p>
<p><strong>Sfide di Scalabilit√†:</strong> La scalabilit√† √® una preoccupazione significativa in FL, soprattutto con l‚Äôaumento del numero di dispositivi partecipanti. La gestione e il coordinamento degli aggiornamenti del modello da molti dispositivi aggiungono complessit√† e possono mettere a dura prova il sistema. √à fondamentale garantire che l‚Äôarchitettura del sistema possa gestire in modo efficiente questo carico aumentato senza degradare le prestazioni. Ci√≤ implica non solo la gestione degli aspetti computazionali e di comunicazione, ma anche il mantenimento della qualit√† e della coerenza del modello man mano che aumenta la scala dell‚Äôoperazione. Una sfida fondamentale √® la progettazione di sistemi FL che si adattino in modo efficace mantenendo le prestazioni.</p>
<p><strong>Sincronizzazione e Coerenza dei Dati:</strong> Garantire la sincronizzazione dei dati e mantenere la coerenza del modello su tutti i dispositivi partecipanti in FL √® una sfida. Mantenere tutti i dispositivi sincronizzati con l‚Äôultima versione del modello pu√≤ essere difficile in ambienti con connettivit√† intermittente o dispositivi che vanno offline periodicamente. Inoltre, √® fondamentale mantenere la coerenza nel modello addestrato, soprattutto quando si ha a che fare con un‚Äôampia gamma di dispositivi con diverse distribuzioni dei dati e frequenze di aggiornamento. Ci√≤ richiede sofisticate strategie di sincronizzazione e aggregazione per garantire che il modello finale rifletta accuratamente gli addestramenti da tutti i dispositivi.</p>
<p><strong>Consumo Energetico:</strong> Il consumo energetico dei dispositivi client in FL √® un fattore critico, in particolare per i dispositivi alimentati a batteria come smartphone e altri dispositivi TinyML/IoT. Le richieste di elaborazione dei modelli di training a livello locale possono portare a un notevole consumo della batteria, il che potrebbe scoraggiare la partecipazione continua al processo FL. √à essenziale bilanciare i requisiti di elaborazione dei modelli di training con l‚Äôefficienza energetica. Ci√≤ comporta l‚Äôottimizzazione di algoritmi e processi di training per ridurre il consumo energetico e ottenere risultati di addestramento efficaci. Garantire un funzionamento efficiente dal punto di vista energetico √® fondamentale per l‚Äôaccettazione da parte dell‚Äôutente e la sostenibilit√† dei sistemi FL.</p>
</section>
<section id="caso-di-studio-addestramento-federato-per-set-di-dati-sanitari-collaborativi" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="caso-di-studio-addestramento-federato-per-set-di-dati-sanitari-collaborativi">Caso di Studio: Addestramento Federato per Set di Dati Sanitari Collaborativi</h4>
<p>Nel settore sanitario e farmaceutico, le organizzazioni spesso detengono grandi quantit√† di dati preziosi, ma condividerli direttamente √® irto di sfide. Normative severe come GDPR e HIPAA, nonch√© preoccupazioni sulla protezione della propriet√† intellettuale, rendono quasi impossibile combinare set di dati tra aziende. Tuttavia, la collaborazione rimane essenziale per settori in evoluzione come la scoperta di farmaci e l‚Äôassistenza ai pazienti. L‚Äôaddestramento federato offre una soluzione unica consentendo alle aziende di addestrare in modo collaborativo modelli di apprendimento automatico senza mai condividere i propri dati grezzi. Questo approccio garantisce che ogni organizzazione mantenga il pieno controllo dei propri dati, continuando a beneficiare delle intuizioni collettive del gruppo.</p>
<p>Il progetto MELLODDY, un‚Äôiniziativa fondamentale in Europa, esemplifica come l‚Äôaddestramento federato possa superare queste barriere <span class="citation" data-cites="heyndrickx2023melloddy">(<a href="../../../references.it.html#ref-heyndrickx2023melloddy" role="doc-biblioref">Heyndrickx et al. 2023</a>)</span>. MELLODDY ha riunito dieci aziende farmaceutiche per creare la pi√π grande libreria di composti chimici condivisa mai assemblata, che comprende oltre 21 milioni di molecole e 2,6 miliardi di dati sperimentali. Nonostante lavorassero con dati sensibili e proprietari, le aziende hanno collaborato in modo sicuro per migliorare i modelli predittivi per lo sviluppo dei farmaci.</p>
<div class="no-row-height column-margin column-container"><div id="ref-heyndrickx2023melloddy" class="csl-entry" role="listitem">
Heyndrickx, Wouter, Lewis Mervin, Tobias Morawietz, No√© Sturm, Lukas Friedrich, Adam Zalewski, Anastasia Pentina, et al. 2023. <span>¬´Melloddy: Cross-pharma federated learning at unprecedented scale unlocks benefits in qsar without compromising proprietary information¬ª</span>. <em>Journal of chemical information and modeling</em> 64 (7): 2331‚Äì44. <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.3c00799">https://pubs.acs.org/doi/10.1021/acs.jcim.3c00799</a>.
</div></div><p>I risultati sono stati notevoli. Mettendo in comune le informazioni tramite l‚Äôaddestramento federato, ogni azienda ha migliorato significativamente la propria capacit√† di identificare promettenti farmaci candidati. L‚Äôaccuratezza predittiva √® migliorata mentre i modelli hanno anche acquisito una pi√π ampia applicabilit√† a diversi set di dati. MELLODDY ha dimostrato che l‚Äôaddestramento federato non solo preserva la privacy, ma sblocca anche nuove opportunit√† di innovazione consentendo una collaborazione su larga scala basata sui dati. Questo approccio evidenzia un futuro in cui le aziende possono lavorare insieme per risolvere problemi complessi senza sacrificare la sicurezza o la propriet√† dei dati.</p>
</section>
</section>
<section id="machine-unlearning" class="level3 page-columns page-full" data-number="14.8.3">
<h3 data-number="14.8.3" class="anchored" data-anchor-id="machine-unlearning"><span class="header-section-number">14.8.3</span> Machine Unlearning</h3>
<section id="idea-centrale-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="idea-centrale-2">Idea Centrale</h4>
<p>Il ‚ÄúMachine unlearning‚Äù √® un processo abbastanza nuovo che descrive come l‚Äôinfluenza di un sottoinsieme di dati di training pu√≤ essere rimossa dal modello. Sono stati utilizzati diversi metodi per eseguire l‚Äôunlearning automatico e rimuovere l‚Äôinfluenza di un sottoinsieme di dati di training dal modello finale. Un approccio di base potrebbe consistere semplicemente nel perfezionare il modello per pi√π epoche solo sui dati che dovrebbero essere ricordati per ridurre l‚Äôinfluenza dei dati ‚Äúdimenticati‚Äù dal modello. Poich√© questo approccio non rimuove esplicitamente l‚Äôinfluenza dei dati che dovrebbero essere cancellati, sono ancora possibili attacchi di inferenza di appartenenza, quindi i ricercatori hanno adottato altri approcci per disimparare i dati da un modello in modo esplicito. Un tipo di approccio adottato dai ricercatori include l‚Äôadeguamento della funzione di perdita del modello per trattare le perdite del ‚Äúset di dimenticanza esplicito‚Äù (dati da disimparare) e del ‚Äúset di conservazione‚Äù (dati rimanenti che dovrebbero ancora essere ricordati) in modo diverso <span class="citation" data-cites="tarun2023deep khan2021knowledgeadaptation">(<a href="../../../references.it.html#ref-tarun2023deep" role="doc-biblioref">Tarun et al. 2022</a>; <a href="../../../references.it.html#ref-khan2021knowledgeadaptation" role="doc-biblioref">Khan e Swaroop 2021</a>)</span>. <a href="#fig-machine-unlearning" class="quarto-xref">Figura&nbsp;<span>14.14</span></a> illustra alcune delle applicazioni di Machine-unlearning.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tarun2023deep" class="csl-entry" role="listitem">
Tarun, Ayush K, Vikram S Chundawat, Murari Mandal, e Mohan Kankanhalli. 2022. <span>¬´Deep Regression Unlearning¬ª</span>. <em>ArXiv preprint</em> abs/2210.08196 (ottobre). <a href="http://arxiv.org/abs/2210.08196v2">http://arxiv.org/abs/2210.08196v2</a>.
</div><div id="ref-khan2021knowledgeadaptation" class="csl-entry" role="listitem">
Khan, Mohammad Emtiyaz, e Siddharth Swaroop. 2021. <span>¬´Knowledge-Adaptation Priors¬ª</span>. In <em>Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual</em>, a cura di Marc‚ÄôAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 19757‚Äì70. <a href="https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html</a>.
</div></div><div id="fig-machine-unlearning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-machine-unlearning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/machineunlearning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-machine-unlearning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.14: Applicazioni di Machine Unlearning. Fonte: <a href="https://www.bbvaopenmind.com/en/technology/artificial-intelligence/ai-and-machine-unlearning-forgotten-path/">BBVA OpenMind</a>
</figcaption>
</figure>
</div>
</section>
<section id="caso-di-studio-lesperimento-di-harry-potter" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="caso-di-studio-lesperimento-di-harry-potter">Caso di Studio: L‚ÄôEsperimento di Harry Potter</h4>
<p>Alcuni ricercatori hanno dimostrato un esempio concreto di approcci di disapprendimento automatico applicati ai modelli di machine learning SOTA attraverso l‚Äôaddestramento di un LLM, LLaMA2-7b, per disimparare qualsiasi riferimento a Harry Potter <span class="citation" data-cites="eldan2023whos">(<a href="../../../references.it.html#ref-eldan2023whos" role="doc-biblioref">Eldan e Russinovich 2023</a>)</span>. Sebbene questo modello abbia richiesto 184K ore di GPU per il pre-addestramento, √® bastata solo 1 ora di GPU di messa a punto per cancellare la capacit√† del modello di generare o richiamare contenuti correlati a Harry Potter senza compromettere in modo evidente l‚Äôaccuratezza della generazione di contenuti non correlati a Harry Potter. <a href="#fig-hp-prompts" class="quarto-xref">Figura&nbsp;<span>14.15</span></a> mostra come l‚Äôoutput del modello cambia prima (colonna Llama-7b-chat-hf) e dopo (colonna Llama-b messa a punto) che si √® verificato il disapprendimento.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-hp-prompts" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hp-prompts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/Llama_unlearning_Harry_Potter.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hp-prompts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;14.15: Llama disapprendimento di Harry Potter. Fonte: <span class="citation" data-cites="eldan2023whos">Eldan e Russinovich (<a href="../../../references.it.html#ref-eldan2023whos" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-eldan2023whos" class="csl-entry" role="listitem">
Eldan, Ronen, e Mark Russinovich. 2023. <span>¬´Who‚Äôs Harry Potter? Approximate Unlearning in LLMs¬ª</span>. <em>ArXiv preprint</em> abs/2310.02238 (ottobre). <a href="http://arxiv.org/abs/2310.02238v2">http://arxiv.org/abs/2310.02238v2</a>.
</div></div></figure>
</div>
</section>
<section id="altri-utilizzi" class="level4">
<h4 class="anchored" data-anchor-id="altri-utilizzi">Altri Utilizzi</h4>
<section id="rimozione-di-dati-avversari" class="level5">
<h5 class="anchored" data-anchor-id="rimozione-di-dati-avversari">Rimozione di dati avversari</h5>
<p>√à stato precedentemente dimostrato che i modelli di deep learning sono vulnerabili ad attacchi avversari, in cui l‚Äôaggressore genera dati avversari simili ai dati di training originali, in cui un essere umano non riesce a distinguere tra i dati reali e quelli fabbricati. I dati avversari fanno s√¨ che il modello emetta previsioni errate, il che potrebbe avere conseguenze negative in varie applicazioni, tra cui le previsioni di diagnosi sanitaria. Il disapprendimento automatico √® stato utilizzato per <a href="https://arxiv.org/pdf/2209.02299.pdf">disimparare l‚Äôinfluenza dei dati avversari</a>, impedendo cos√¨ che queste previsioni errate si verifichino e causino danni.</p>
</section>
</section>
</section>
<section id="crittografia-omomorfica" class="level3" data-number="14.8.4">
<h3 data-number="14.8.4" class="anchored" data-anchor-id="crittografia-omomorfica"><span class="header-section-number">14.8.4</span> Crittografia Omomorfica</h3>
<section id="idea-centrale-3" class="level4">
<h4 class="anchored" data-anchor-id="idea-centrale-3">Idea Centrale</h4>
<p>La crittografia omomorfica √® una forma di crittografia che consente di eseguire calcoli su testo cifrato, generando un risultato crittografato che, una volta decrittografato, corrisponde al risultato delle operazioni eseguite sul testo in chiaro. Ad esempio, moltiplicando due numeri crittografati con crittografia omomorfica si ottiene un prodotto crittografato che decrittografa il prodotto effettivo dei due numeri. Ci√≤ significa che i dati possono essere elaborati in forma crittografata e solo l‚Äôoutput risultante deve essere decrittografato, migliorando significativamente la sicurezza dei dati, in particolare per le informazioni sensibili.</p>
<p>La crittografia omomorfica consente calcoli esternalizzati su dati crittografati senza esporre i dati stessi esternamente per eseguire le operazioni. Tuttavia, solo determinati calcoli come addizione e moltiplicazione sono supportati negli schemi parzialmente omomorfici. La ‚ÄúFully Homomorphic Encryption (FHE)‚Äù [crittografia completamente omomorfica], in grado di gestire qualsiasi calcolo, √® ancora pi√π complessa. Il numero di possibili operazioni √® limitato prima che l‚Äôaccumulo di rumore corrompa il testo cifrato.</p>
<p>Per utilizzare la crittografia omomorfica su diverse entit√†, le chiavi pubbliche generate con cura devono essere scambiate per le operazioni su dati crittografati separatamente. Questa tecnica di crittografia avanzata consente paradigmi di calcolo sicuri precedentemente impossibili, ma richiede competenze specifiche per essere implementata correttamente nei sistemi del mondo reale.</p>
</section>
<section id="vantaggi-4" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi-4">Vantaggi</h4>
<p>La crittografia omomorfica consente l‚Äôaddestramento del modello di apprendimento automatico e l‚Äôinferenza sui dati crittografati, assicurando che gli input sensibili e i valori intermedi rimangano riservati. Ci√≤ √® fondamentale in ambito sanitario, finanziario, genetico e altri domini, che si affidano sempre di pi√π al ML per analizzare set di dati sensibili e regolamentati contenenti miliardi di dati personali.</p>
<p>La crittografia omomorfica ostacola attacchi come l‚Äôestrazione del modello e l‚Äôinferenza dell‚Äôappartenenza che potrebbero esporre dati privati utilizzati nei flussi di lavoro ML. Fornisce un‚Äôalternativa ai TEE che utilizzano enclave hardware per l‚Äôelaborazione riservata. Tuttavia, gli schemi attuali hanno elevati overhead computazionali e limitazioni algoritmiche che limitano le applicazioni del mondo reale.</p>
<p>La crittografia omomorfica realizza la visione decennale di elaborazione multi-parti sicura consentendo l‚Äôelaborazione su testi cifrati. Concepiti negli anni ‚Äô70, i primi sistemi crittografici completamente omomorfici sono emersi nel 2009, consentendo elaborazioni arbitrarie. La ricerca in corso sta rendendo queste tecniche pi√π efficienti e pratiche.</p>
<p>La crittografia omomorfica mostra grandi promesse nell‚Äôabilitare l‚Äôapprendimento automatico che preserva la privacy in base alle normative emergenti sui dati. Tuttavia, dati i vincoli, si dovrebbe valutare attentamente la sua applicabilit√† rispetto ad altri approcci di elaborazione confidenziale. Esistono ampie risorse per esplorare la crittografia omomorfica e monitorare i progressi nell‚Äôattenuare le barriere all‚Äôadozione.</p>
</section>
<section id="meccanica-3" class="level4">
<h4 class="anchored" data-anchor-id="meccanica-3">Meccanica</h4>
<ol type="1">
<li><p><strong>Crittografia dei Dati:</strong> Prima che i dati vengano elaborati o inviati a un modello ML, vengono crittografati utilizzando uno schema di crittografia omomorfica e una chiave pubblica. Ad esempio, la crittografia dei numeri <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> genera i testi cifrati <span class="math inline">\(E(x)\)</span> e <span class="math inline">\(E(y)\)</span>.</p></li>
<li><p><strong>Calcolo sul Testo Cifrato:</strong> L‚Äôalgoritmo ML elabora direttamente i dati crittografati. Ad esempio, la moltiplicazione dei testi cifrati <span class="math inline">\(E(x)\)</span> e <span class="math inline">\(E(y)\)</span> genera <span class="math inline">\(E(xy)\)</span>. √à possibile eseguire anche un training del modello pi√π complesso sui testi cifrati.</p></li>
<li><p><strong>Crittografia del Risultato:</strong> Il risultato <span class="math inline">\(E(xy)\)</span> rimane crittografato e pu√≤ essere decrittografato solo da qualcuno con la chiave privata corrispondente per rivelare il prodotto effettivo <span class="math inline">\(xy\)</span>.</p></li>
</ol>
<p>Solo le parti autorizzate con la chiave privata possono decrittografare gli output finali, proteggendo lo stato intermedio. Tuttavia, il rumore si accumula con ogni operazione, impedendo ulteriori calcoli senza decrittazione.</p>
<p>Oltre all‚Äôassistenza sanitaria, la crittografia omomorfica consente il calcolo riservato per applicazioni come il rilevamento di frodi finanziarie, analisi assicurative, ricerca genetica e altro ancora. Offre un‚Äôalternativa a tecniche come il calcolo multipartitico e i TEE. La ricerca in corso migliora l‚Äôefficienza e le capacit√†.</p>
<p>Strumenti come HElib, SEAL e TensorFlow HE forniscono librerie per esplorare l‚Äôimplementazione della crittografia omomorfica in pipeline di apprendimento automatico nel mondo reale.</p>
</section>
<section id="compromessi-4" class="level4">
<h4 class="anchored" data-anchor-id="compromessi-4">Compromessi</h4>
<p>Per molte applicazioni in tempo reale ed embedded, la crittografia completamente omomorfica rimane poco pratica per i seguenti motivi.</p>
<p><strong>Sovraccarico Computazionale:</strong> La crittografia omomorfica impone sovraccarichi computazionali molto elevati, spesso con conseguenti rallentamenti di oltre 100 volte per le applicazioni ML del mondo reale. Ci√≤ la rende poco pratica per molti utilizzi sensibili al tempo o con risorse limitate. L‚Äôhardware ottimizzato e la parallelizzazione possono alleviare, ma non eliminare, questo problema.</p>
<p><strong>Complessit√† di Implementazione</strong> Gli algoritmi sofisticati richiedono una profonda competenza in crittografia per essere implementati correttamente. Sfumature come la compatibilit√† del formato con modelli ML in virgola mobile e la gestione scalabile delle chiavi pongono ostacoli. Questa complessit√† ostacola l‚Äôadozione pratica diffusa.</p>
<p><strong>Limitazioni Algoritmiche:</strong> Gli schemi attuali limitano le funzioni e la profondit√† dei calcoli supportati, limitando i modelli e i volumi di dati che possono essere elaborati. La ricerca in corso sta spingendo questi limiti, ma permangono delle restrizioni.</p>
<p><strong>Accelerazione Hardware:</strong> La crittografia omomorfica richiede hardware specializzato, come processori sicuri o coprocessori con TEE, che aggiungono costi di progettazione e infrastruttura.</p>
<p><strong>Progetti Ibridi:</strong> Anzich√© crittografare interi flussi di lavoro, l‚Äôapplicazione selettiva della crittografia omomorfica a sotto-componenti critici pu√≤ ottenere protezione riducendo al minimo i costi generali.</p>
<div id="exr-he" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;14.2: Crittografia Omomorfica
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>La potenza del calcolo crittografato viene sbloccata tramite la crittografia omomorfica, un approccio trasformativo in cui i calcoli vengono eseguiti direttamente sui dati crittografati, garantendo la tutela della privacy durante tutto il processo. Questo Colab esplora i principi del calcolo su numeri crittografati senza esporre i dati sottostanti. Si immagini uno scenario in cui un modello di apprendimento automatico viene addestrato su dati a cui non √® possibile accedere direttamente: tale √® la forza della crittografia omomorfica.</p>
<p><a href="https://colab.research.google.com/drive/1GjKT5Lgh9Madjsyr9UiyeogUgVpTEBMp?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="secure-multiparty-communication" class="level3" data-number="14.8.5">
<h3 data-number="14.8.5" class="anchored" data-anchor-id="secure-multiparty-communication"><span class="header-section-number">14.8.5</span> Secure Multiparty Communication</h3>
<section id="idea-centrale-4" class="level4">
<h4 class="anchored" data-anchor-id="idea-centrale-4">Idea Centrale</h4>
<p>La Multi-Party Communication (MPC) consente a pi√π parti di calcolare congiuntamente una funzione sui propri input, garantendo al contempo la riservatezza degli input di ciascuna parte. Ad esempio, due organizzazioni possono collaborare all‚Äôaddestramento di un modello di apprendimento automatico combinando set di dati senza rivelare reciprocamente informazioni sensibili. I protocolli MPC sono essenziali laddove le normative sulla privacy e sulla riservatezza limitano la condivisione diretta dei dati, come nel settore sanitario o finanziario.</p>
<p>MPC divide il calcolo in parti che ogni partecipante esegue in modo indipendente utilizzando i propri dati privati. Questi risultati vengono quindi combinati per rivelare solo l‚Äôoutput finale, preservando la privacy dei valori intermedi. Vengono utilizzate tecniche crittografiche per garantire che i risultati parziali rimangano privati in modo dimostrabile.</p>
<p>Prendiamo un semplice esempio di protocollo MPC. Uno dei protocolli MPC pi√π basilari √® l‚Äôaddizione sicura di due numeri. Ogni parte suddivide il suo input in quote casuali che vengono distribuite segretamente. Si scambiano le quote e calcolano localmente la somma delle quote, che ricostruisce la somma finale senza rivelare i singoli input. Ad esempio, se Alice ha input x e Bob ha input y:</p>
<ol type="1">
<li><p>Alice genera <span class="math inline">\(x_1\)</span> casuale e imposta <span class="math inline">\(x_2 = x - x_1\)</span></p></li>
<li><p>Bob genera <span class="math inline">\(y_1\)</span> casuale e imposta <span class="math inline">\(y_2 = y - y_1\)</span></p></li>
<li><p>Alice invia <span class="math inline">\(x_1\)</span> a Bob, Bob invia <span class="math inline">\(y_1\)</span> ad Alice (mantenendo segreti <span class="math inline">\(x_2\)</span> e <span class="math inline">\(y_2\)</span>)</p></li>
<li><p>Alice calcola <span class="math inline">\(x_2 + y_1 = s_1\)</span>, Bob calcola <span class="math inline">\(x_1 + y_2 = s_2\)</span></p></li>
<li><p><span class="math inline">\(s_1 + s_2 = x + y\)</span> √® la somma finale, senza rivelare <span class="math inline">\(x\)</span> o <span class="math inline">\(y\)</span>.</p></li>
</ol>
<p>Gli input individuali di Alice e Bob (<span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>) rimangono privati e ciascuna parte rivela solo un numero associato ai propri input originali. Grazie ai risultati casuali, non viene rivelata alcuna informazione sui numeri originali.</p>
<p><strong>Confronto Sicuro:</strong> Un‚Äôaltra operazione di base √® un confronto sicuro di due numeri, per determinare quale √® maggiore dell‚Äôaltro. Questo pu√≤ essere fatto usando tecniche come i ‚ÄúYao‚Äôs Garbled Circuits‚Äù [circuiti distorti di Yao] (https://it.wikipedia.org/wiki/Andrew_Chi-Chih_Yao), dove il circuito di confronto √® crittografato per consentire una valutazione congiunta degli input senza trapelare.</p>
<p><strong>Moltiplicazione Sicura di Matrici:</strong> Le operazioni di matrice come la moltiplicazione sono essenziali per l‚Äôapprendimento automatico. Le tecniche MPC (Multiparty Communication) come la condivisione segreta additiva possono essere usate per dividere le matrici in quote casuali, calcolare i prodotti sulle quote e quindi ricostruire il risultato.</p>
<p><strong>Addestramento Sicuro del Modello:</strong> Gli algoritmi di addestramento dell‚Äôapprendimento automatico distribuito come la media federata possono essere resi sicuri usando MPC. Gli aggiornamenti del modello calcolati su dati partizionati in ogni nodo vengono condivisi segretamente tra i nodi e aggregati per addestrare il modello globale senza esporre aggiornamenti individuali.</p>
<p>L‚Äôidea fondamentale alla base dei protocolli MPC √® quella di dividere il calcolo in passaggi che possono essere eseguiti congiuntamente senza rivelare dati sensibili intermedi. Ci√≤ si ottiene combinando tecniche crittografiche come la condivisione segreta, la crittografia omomorfica, il trasferimento inconsapevole e i circuiti garbled [distorti]. I protocolli MPC consentono il calcolo collaborativo di dati sensibili fornendo al contempo garanzie di privacy dimostrabili. Questa capacit√† di preservazione della privacy √® essenziale per molte applicazioni di apprendimento automatico odierne che coinvolgono pi√π parti che non possono condividere direttamente i propri dati grezzi.</p>
<p>Gli approcci principali utilizzati in MPC includono:</p>
<ul>
<li><p><strong>Crittografia omomorfica:</strong> La crittografia speciale consente di eseguire calcoli su dati crittografati senza decrittografarli.</p></li>
<li><p><strong>Condivisione segreta:</strong> I dati privati vengono suddivisi in quote casuali distribuite a ciascuna parte. I calcoli vengono eseguiti localmente sulle quote e infine ricostruiti.</p></li>
<li><p><strong>Trasferimento inconsapevole:</strong> Un protocollo in cui un ricevitore ottiene un sottoinsieme di dati da un mittente, ma il mittente non sa quali dati specifici sono stati trasferiti.</p></li>
<li><p><strong>Circuiti Garbled:</strong> La funzione da calcolare √® rappresentata come un circuito booleano crittografato (‚Äúdistorto‚Äù) per consentire una valutazione congiunta senza rivelare gli input.</p></li>
</ul>
</section>
<section id="compromessi-5" class="level4">
<h4 class="anchored" data-anchor-id="compromessi-5">Compromessi</h4>
<p>Sebbene i protocolli MPC forniscano solide garanzie di privacy, hanno un costo computazionale elevato rispetto ai calcoli semplici. Ogni operazione sicura, come addizione, moltiplicazione, confronto, ecc., richiede pi√π ordini di elaborazione rispetto all‚Äôoperazione equivalente non crittografata. Questo overhead deriva dalle tecniche crittografiche sottostanti:</p>
<ul>
<li><p>Nella crittografia parzialmente omomorfica, ogni calcolo su testi cifrati richiede costose operazioni a chiave pubblica. La crittografia completamente omomorfica ha overhead ancora pi√π elevati.</p></li>
<li><p>La condivisione segreta divide i dati in pi√π porzioni, quindi anche le operazioni di base richiedono la manipolazione di molte porzioni.</p></li>
<li><p>Il trasferimento inconsapevole e i circuiti distorti aggiungono mascheramento e crittografia per nascondere i pattern di accesso ai dati e i flussi di esecuzione.</p></li>
<li><p>I sistemi MPC richiedono un‚Äôampia comunicazione e interazione tra le parti per elaborare congiuntamente quote/testi cifrati.</p></li>
</ul>
<p>Di conseguenza, i protocolli MPC possono rallentare i calcoli di 3-4 ordini di grandezza rispetto alle implementazioni semplici. Ci√≤ diventa proibitivo per grandi set di dati e modelli. Pertanto, l‚Äôaddestramento di modelli di apprendimento automatico su dati crittografati tramite MPC rimane oggi irrealizzabile per dimensioni di set di dati realistiche a causa del sovraccarico. Sono necessarie ottimizzazioni e approssimazioni intelligenti per rendere pratico l‚ÄôMPC.</p>
<p>La ricerca in corso sull‚ÄôMPC colma questo divario di efficienza attraverso progressi crittografici, nuovi algoritmi, hardware affidabile come le enclave SGX e sfruttando acceleratori come GPU/TPU. Tuttavia, nel prossimo futuro, sar√† necessario un certo grado di approssimazione e compromesso sulle prestazioni per scalare MPC in modo da soddisfare le esigenze dei sistemi di apprendimento automatico del mondo reale.</p>
</section>
</section>
<section id="generazione-di-dati-sintetici" class="level3" data-number="14.8.6">
<h3 data-number="14.8.6" class="anchored" data-anchor-id="generazione-di-dati-sintetici"><span class="header-section-number">14.8.6</span> Generazione di Dati Sintetici</h3>
<section id="idea-centrale-5" class="level4">
<h4 class="anchored" data-anchor-id="idea-centrale-5">Idea Centrale</h4>
<p>La generazione di dati sintetici √® emersa come un importante approccio di apprendimento automatico per la tutela della privacy che consente di sviluppare e testare modelli senza esporre dati utente reali. L‚Äôidea chiave √® quella di addestrare modelli generativi su set di dati del mondo reale e quindi campionare da questi modelli per sintetizzare dati artificiali che statisticamente corrispondono alla distribuzione dei dati originali ma non contengono informazioni effettive dell‚Äôutente. Ad esempio, tecniche come GAN, VAE e aumento dei dati possono essere utilizzate per produrre dati sintetici che imitano set di dati reali preservando al contempo la privacy. Le simulazioni sono anche comunemente impiegate in scenari in cui i dati sintetici devono rappresentare sistemi complessi, come nella ricerca scientifica o nella pianificazione urbana.</p>
<p>La sfida principale della sintesi dei dati √® garantire che gli avversari non possano re-identificare il set di dati originale. Un approccio semplice per ottenere dati sintetici √® aggiungere rumore al set di dati originale, che rischia comunque di far trapelare la privacy. Quando il rumore viene aggiunto ai dati nel contesto della privacy differenziale, vengono utilizzati meccanismi sofisticati basati sulla sensibilit√† dei dati per calibrare la quantit√† e la distribuzione del rumore. Attraverso questi limiti matematicamente rigorosi, la privacy differenziale garantisce generalmente la privacy a un certo livello, che √® l‚Äôobiettivo primario di questa tecnica. Oltre a preservare la privacy, i dati sintetici contrastano molteplici problemi di disponibilit√† dei dati, come set di dati sbilanciati, set di dati scarsi e rilevamento di anomalie.</p>
<p>I ricercatori possono condividere liberamente questi dati sintetici e collaborare alla modellazione senza rivelare informazioni mediche private. I dati sintetici ben costruiti proteggono la privacy e, al tempo stesso, sono utili per lo sviluppo di modelli accurati. Le tecniche chiave per impedire la ricostruzione dei dati originali includono l‚Äôaggiunta di rumore di privacy differenziale durante l‚Äôaddestramento, l‚Äôapplicazione di vincoli di plausibilit√† e l‚Äôutilizzo di pi√π modelli generativi diversi.</p>
</section>
<section id="vantaggi-5" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi-5">Vantaggi</h4>
<p>Sebbene i dati sintetici possano essere necessari a causa di rischi per la privacy o la conformit√†, sono ampiamente utilizzati nei modelli di apprendimento automatico quando i dati disponibili sono di scarsa qualit√†, scarsi o inaccessibili. I dati sintetici offrono uno sviluppo pi√π efficiente ed efficace semplificando i processi di addestramento, test e distribuzione dei modelli robusti. Consentono ai ricercatori di condividere i modelli pi√π ampiamente senza violare le leggi e le normative sulla privacy. La collaborazione tra gli utenti dello stesso set di dati sar√† facilitata, il che aiuter√† ad ampliare le capacit√† e i progressi nella ricerca ML.</p>
<p>Esistono diverse motivazioni per l‚Äôutilizzo di dati sintetici nell‚Äôapprendimento automatico:</p>
<ul>
<li><p><strong>Privacy e Conformit√†:</strong> I dati sintetici evitano di esporre informazioni personali, consentendo una condivisione e una collaborazione pi√π aperte. Ci√≤ √® importante quando si lavora con set di dati sensibili come cartelle cliniche o informazioni finanziarie.</p></li>
<li><p><strong>Scarsit√† di dati:</strong> Quando non sono disponibili dati reali sufficienti, i dati sintetici possono aumentare i set di dati di addestramento. Ci√≤ migliora l‚Äôaccuratezza del modello quando i dati limitati rappresentano un collo di bottiglia.</p></li>
<li><p><strong>Test del modello:</strong> I dati sintetici forniscono sandbox protetti dalla privacy per testare le prestazioni del modello, risolvere i problemi e monitorare i bias.</p></li>
<li><p><strong>Etichettatura dei dati:</strong> I dati di training etichettati di alta qualit√† sono spesso scarsi e costosi. I dati sintetici possono aiutare a generare automaticamente esempi etichettati.</p></li>
</ul>
</section>
<section id="compromessi-6" class="level4">
<h4 class="anchored" data-anchor-id="compromessi-6">Compromessi</h4>
<p>Sebbene i dati sintetici cerchino di rimuovere qualsiasi prova del set di dati originale, la perdita della privacy rappresenta comunque un rischio, poich√© i dati sintetici imitano i dati originali. Le informazioni statistiche e la distribuzione sono simili, se non uguali, tra i dati originali e sintetici. Ricampionando dalla distribuzione, gli avversari potrebbero comunque essere in grado di recuperare i campioni di addestramento originali. A causa dei loro processi di apprendimento e complessit√† intrinseci, le reti neurali potrebbero rivelare accidentalmente informazioni sensibili sui dati di addestramento originali.</p>
<p>Una sfida fondamentale con i dati sintetici √® il potenziale divario tra le distribuzioni dei dati sintetici e quelli del mondo reale. Nonostante i progressi nelle tecniche di modellazione generativa, i dati sintetici potrebbero catturare solo parzialmente la complessit√†, la diversit√† e i pattern sfumati dei dati reali. Ci√≤ pu√≤ limitare l‚Äôutilit√† dei dati sintetici per l‚Äôaddestramento robusto di modelli di apprendimento automatico. Valutare rigorosamente la qualit√† dei dati sintetici tramite metodi avversari e confrontare le prestazioni del modello con i benchmark dei dati reali aiuta a valutare e migliorare la fedelt√†. Tuttavia, intrinsecamente, i dati sintetici rimangono un‚Äôapprossimazione.</p>
<p>Un‚Äôaltra preoccupazione critica sono i rischi per la privacy dei dati sintetici. I modelli generativi possono far trapelare informazioni identificabili sugli individui nei dati di training, il che potrebbe consentire la ricostruzione di informazioni private. Gli attacchi avversari emergenti dimostrano le sfide nel prevenire la perdita di identit√† dalle pipeline di generazione di dati sintetici. Tecniche come la privacy differenziale possono aiutare a salvaguardare la privacy, ma comportano compromessi nell‚Äôutilit√† dei dati. Esiste una tensione intrinseca tra la produzione di dati sintetici validi e la protezione completa dei dati di training sensibili, che deve essere bilanciata.</p>
<p>Ulteriori insidie dei dati sintetici includono distorsioni amplificate, etichettature errate, sovraccarico computazionale per l‚Äôaddestramento di modelli generativi, costi di archiviazione e mancata contabilizzazione di nuovi dati fuori distribuzione. Sebbene questi siano secondari rispetto al divario sintetico-reale e ai rischi per la privacy, rimangono considerazioni importanti quando si valuta l‚Äôidoneit√† dei dati sintetici per particolari attivit√† di apprendimento automatico. Come con qualsiasi tecnica, i vantaggi dei dati sintetici comportano compromessi e limitazioni intrinseche che richiedono strategie di mitigazione ponderate.</p>
</section>
</section>
<section id="riepilogo" class="level3" data-number="14.8.7">
<h3 data-number="14.8.7" class="anchored" data-anchor-id="riepilogo"><span class="header-section-number">14.8.7</span> Riepilogo</h3>
<p>Sebbene tutte le tecniche di cui abbiamo discusso finora mirino a consentire un apprendimento automatico che salvaguardi la privacy, esse implicano meccanismi e compromessi distinti. Fattori come vincoli computazionali, ipotesi di fiducia richieste, modelli di minaccia e caratteristiche dei dati aiutano a guidare il processo di selezione per un caso d‚Äôuso particolare. Tuttavia, trovare il giusto equilibrio tra privacy, accuratezza ed efficienza richiede sperimentazione e valutazione empirica per molte applicazioni. <a href="#tbl-privacy-techniques" class="quarto-xref">Tabella&nbsp;<span>14.2</span></a> √® una tabella di confronto delle principali tecniche di apprendimento automatico che salvaguardano la privacy e dei loro pro e contro:</p>
<div id="tbl-privacy-techniques" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-privacy-techniques-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;14.2: Confronto di tecniche per l‚Äôapprendimento automatico che tutela la privacy.
</figcaption>
<div aria-describedby="tbl-privacy-techniques-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 40%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Tecnica</th>
<th style="text-align: left;">Pro</th>
<th style="text-align: left;">Contro</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Privacy Differenziale</td>
<td style="text-align: left;"><ul>
<li>Forti garanzie formali di privacy</li>
<li>Robusto per attacchi dati ausiliari</li>
<li>Versatile per molti tipi di dati e analisi</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Perdita di accuratezza dovuta all‚Äôaggiunta di rumore</li>
<li>Overhead computazionale per analisi di sensibilit√† e generazione di rumore</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;">Addestramento Federato</td>
<td style="text-align: left;"><ul>
<li>Consente l‚Äôapprendimento collaborativo senza condividere dati grezzi</li>
<li>I dati rimangono decentralizzati migliorando la sicurezza</li>
<li>Nessuna necessit√† di elaborazione crittografata</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Overhead di comunicazione aumentato</li>
<li>Convergenza del modello potenzialmente pi√π lenta</li>
<li>Capacit√† di dispositivi client non uniformi</li>
</ul></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Machine Unlearning</td>
<td style="text-align: left;"><ul>
<li>Consente la rimozione selettiva dell‚Äôinfluenza dei dati dai modelli</li>
<li>Utile per la conformit√† alle normative sulla privacy</li>
<li>Impedisce la conservazione involontaria di dati avversari o obsoleti</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Pu√≤ degradare le prestazioni del modello su attivit√† correlate</li>
<li>Complessit√† di implementazione in modelli su larga scala</li>
<li>Rischio di unlearning incompleto o inefficace</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;">Crittografia Omomorfica</td>
<td style="text-align: left;"><ul>
<li>Consente il calcolo su dati crittografati</li>
<li>Previene l‚Äôesposizione allo stato intermedio</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Costi di calcolo estremamente elevati</li>
<li>Implementazioni crittografiche complesse</li>
<li>Restrizioni sui tipi di funzione</li>
</ul></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Calcolo Multi-Parte Sicuro</td>
<td style="text-align: left;"><ul>
<li>Consente il calcolo congiunto su dati sensibili</li>
<li>Fornisce garanzie di privacy crittografica</li>
<li>Protocolli flessibili per varie funzioni</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Costi di calcolo molto elevati</li>
<li>Complessit√† di implementazione</li>
<li>Vincoli algoritmici sulla profondit√† della funzione</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;">Generazione di Dati Sintetici</td>
<td style="text-align: left;"><ul>
<li>Consente la condivisione dei dati senza perdite</li>
<li>Attenua i problemi di scarsit√† di dati</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Divario sintetico-reale nelle distribuzioni</li>
<li>Potenziale per la ricostruzione di dati privati</li>
<li>Bias e problemi di etichettatura</li>
</ul></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="conclusione" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">14.9</span> Conclusione</h2>
<p>La sicurezza hardware del machine learning √® fondamentale poich√© i sistemi ML embedded vengono sempre pi√π implementati in domini critici per la sicurezza come dispositivi medici, controlli industriali e veicoli autonomi. Abbiamo esplorato varie minacce che spaziano da bug hardware, attacchi fisici, canali laterali, rischi della supply chain, ecc. Difese come TEE, Secure Boot, PUF e moduli di sicurezza hardware forniscono una protezione multi-livello su misura per dispositivi embedded con risorse limitate.</p>
<p>Tuttavia, una vigilanza continua √® essenziale per tracciare i vettori di attacco emergenti e affrontare potenziali vulnerabilit√† tramite pratiche di ingegneria sicure durante l‚Äôintero ciclo di vita dell‚Äôhardware. Man mano che ML e ML embedded si diffondono, il mantenimento di rigorose basi di sicurezza che corrispondano al ritmo accelerato di innovazione del settore rimane imperativo.</p>
</section>
<section id="sec-security-and-privacy-resource" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="sec-security-and-privacy-resource"><span class="header-section-number">14.10</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1jZBi8DS1NUXFdIwNGwofzA4CYRei6lH2e56BOu098-k/edit#slide=id.g1ff987f3d96_0_0">Security.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1Wp-5eO4Bmco2f7ppNKsRkE1utuz22PeLvVoREFSChR8/edit#slide=id.g202a5aaf418_0_0">Privacy.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1WlQdk40zJcW9Bx6ua-vKu3sDrMU_iI89BQGMGk6OEB0/edit?usp=drive_link">Monitoring after Deployment.</a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><p><a href="#vid-jeephack" class="quarto-xref">Video&nbsp;<span>14.1</span></a></p></li>
<li><p><a href="#vid-mirai" class="quarto-xref">Video&nbsp;<span>14.2</span></a></p></li>
<li><p><a href="#vid-powerattack" class="quarto-xref">Video&nbsp;<span>14.3</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-dptf" class="quarto-xref">Esercizio&nbsp;<span>14.1</span></a></p></li>
<li><p><a href="#exr-he" class="quarto-xref">Esercizio&nbsp;<span>14.2</span></a></p></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/ops/ops.it.html" class="pagination-link" aria-label="Operazioni di ML">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/responsible_ai/responsible_ai.it.html" class="pagination-link" aria-label="IA Responsabile">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University). Traduzione di <a href="https://github.com/BravoBaldo">Baldassarre Cesarano</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/privacy_security/privacy_security.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/privacy_security/privacy_security.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>