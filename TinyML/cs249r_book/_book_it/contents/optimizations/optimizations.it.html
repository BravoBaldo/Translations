<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Ottimizzazioni dei Modelli ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/hw_acceleration/hw_acceleration.it.html" rel="next">
<link href="../../contents/efficient_ai/efficient_ai.it.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/training/training.it.html">Training</a></li><li class="breadcrumb-item"><a href="../../contents/optimizations/optimizations.it.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2d6cdf6fc58f1105ce2a5c5c28a11153" class="alert alert-info hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>üåü Aiutaci a raggiungere 1.000 stelle GitHub! üåü Per ogni 25 stelle, Arduino e SEEED doneranno una NiclaVision o una XIAO ESP32S3 per l‚Äôistruzione sull‚Äôintelligenza artificiale. <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per una ‚≠ê</a></p>
</div></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">PREFAZIONE</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">PARTE PRINCIPALE</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Nozioni Fondamentali</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Workflow</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Argomenti Avanzati</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Impatto Sociale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Chiusura</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tool</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Dataset</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Risorse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Le Community</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Casi di Studio</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">9.1</span> Introduzione</a></li>
  <li><a href="#sec-model_ops_representation" id="toc-sec-model_ops_representation" class="nav-link" data-scroll-target="#sec-model_ops_representation"><span class="header-section-number">9.2</span> Rappresentazione Efficiente del Modello</a>
  <ul>
  <li><a href="#sec-pruning" id="toc-sec-pruning" class="nav-link" data-scroll-target="#sec-pruning"><span class="header-section-number">9.2.1</span> Il Pruning</a>
  <ul class="collapse">
  <li><a href="#panoramica" id="toc-panoramica" class="nav-link" data-scroll-target="#panoramica">Panoramica</a></li>
  <li><a href="#potatura-strutturata" id="toc-potatura-strutturata" class="nav-link" data-scroll-target="#potatura-strutturata">Potatura Strutturata</a></li>
  <li><a href="#vantaggi-della-potatura-strutturata" id="toc-vantaggi-della-potatura-strutturata" class="nav-link" data-scroll-target="#vantaggi-della-potatura-strutturata">Vantaggi della Potatura Strutturata</a></li>
  <li><a href="#potatura-non-strutturata" id="toc-potatura-non-strutturata" class="nav-link" data-scroll-target="#potatura-non-strutturata">Potatura non Strutturata</a></li>
  <li><a href="#ipotesi-del-biglietto-della-lotteria" id="toc-ipotesi-del-biglietto-della-lotteria" class="nav-link" data-scroll-target="#ipotesi-del-biglietto-della-lotteria">Ipotesi del Biglietto della Lotteria</a></li>
  <li><a href="#problemi-e-limitazioni" id="toc-problemi-e-limitazioni" class="nav-link" data-scroll-target="#problemi-e-limitazioni">Problemi e Limitazioni</a></li>
  </ul></li>
  <li><a href="#compressione-del-modello" id="toc-compressione-del-modello" class="nav-link" data-scroll-target="#compressione-del-modello"><span class="header-section-number">9.2.2</span> Compressione del Modello</a>
  <ul class="collapse">
  <li><a href="#sec-kd" id="toc-sec-kd" class="nav-link" data-scroll-target="#sec-kd">Distillazione della Conoscenza</a></li>
  <li><a href="#fattorizzazione-di-matrici-di-basso-rango" id="toc-fattorizzazione-di-matrici-di-basso-rango" class="nav-link" data-scroll-target="#fattorizzazione-di-matrici-di-basso-rango">Fattorizzazione di Matrici di Basso Rango</a></li>
  <li><a href="#decomposizione-dei-tensori" id="toc-decomposizione-dei-tensori" class="nav-link" data-scroll-target="#decomposizione-dei-tensori">Decomposizione dei Tensori</a></li>
  </ul></li>
  <li><a href="#modelli-progettati-per-ledge" id="toc-modelli-progettati-per-ledge" class="nav-link" data-scroll-target="#modelli-progettati-per-ledge"><span class="header-section-number">9.2.3</span> Modelli Progettati per l‚ÄôEdge</a>
  <ul class="collapse">
  <li><a href="#tecniche-di-progettazione-del-modello" id="toc-tecniche-di-progettazione-del-modello" class="nav-link" data-scroll-target="#tecniche-di-progettazione-del-modello">Tecniche di Progettazione del Modello</a></li>
  <li><a href="#architetture-di-modello-di-esempio" id="toc-architetture-di-modello-di-esempio" class="nav-link" data-scroll-target="#architetture-di-modello-di-esempio">Architetture di Modello di Esempio</a></li>
  <li><a href="#semplificazione-della-ricerca-di-architetture-di-modelli" id="toc-semplificazione-della-ricerca-di-architetture-di-modelli" class="nav-link" data-scroll-target="#semplificazione-della-ricerca-di-architetture-di-modelli">Semplificazione della Ricerca di Architetture di Modelli</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-model_ops_numerics" id="toc-sec-model_ops_numerics" class="nav-link" data-scroll-target="#sec-model_ops_numerics"><span class="header-section-number">9.3</span> Rappresentazione Numerica Efficiente</a>
  <ul>
  <li><a href="#motivazione" id="toc-motivazione" class="nav-link" data-scroll-target="#motivazione">Motivazione</a></li>
  <li><a href="#le-basi" id="toc-le-basi" class="nav-link" data-scroll-target="#le-basi"><span class="header-section-number">9.3.1</span> Le Basi</a>
  <ul class="collapse">
  <li><a href="#i-tipi" id="toc-i-tipi" class="nav-link" data-scroll-target="#i-tipi">I Tipi</a></li>
  <li><a href="#precisione" id="toc-precisione" class="nav-link" data-scroll-target="#precisione">Precisione</a></li>
  <li><a href="#codifica-e-archiviazione-numerica" id="toc-codifica-e-archiviazione-numerica" class="nav-link" data-scroll-target="#codifica-e-archiviazione-numerica">Codifica e Archiviazione Numerica</a></li>
  </ul></li>
  <li><a href="#vantaggi-dellefficienza" id="toc-vantaggi-dellefficienza" class="nav-link" data-scroll-target="#vantaggi-dellefficienza"><span class="header-section-number">9.3.2</span> Vantaggi dell‚ÄôEfficienza</a></li>
  <li><a href="#sfumature-della-rappresentazione-numerica" id="toc-sfumature-della-rappresentazione-numerica" class="nav-link" data-scroll-target="#sfumature-della-rappresentazione-numerica"><span class="header-section-number">9.3.3</span> Sfumature della Rappresentazione Numerica</a>
  <ul class="collapse">
  <li><a href="#utilizzo-della-memoria" id="toc-utilizzo-della-memoria" class="nav-link" data-scroll-target="#utilizzo-della-memoria">Utilizzo della Memoria</a></li>
  <li><a href="#complessit√†-computazionale" id="toc-complessit√†-computazionale" class="nav-link" data-scroll-target="#complessit√†-computazionale">Complessit√† Computazionale</a></li>
  <li><a href="#compatibilit√†-hardware" id="toc-compatibilit√†-hardware" class="nav-link" data-scroll-target="#compatibilit√†-hardware">Compatibilit√† Hardware</a></li>
  <li><a href="#compromessi-di-precisione-e-accuratezza" id="toc-compromessi-di-precisione-e-accuratezza" class="nav-link" data-scroll-target="#compromessi-di-precisione-e-accuratezza">Compromessi di Precisione e Accuratezza</a></li>
  <li><a href="#esempi-di-compromessi" id="toc-esempi-di-compromessi" class="nav-link" data-scroll-target="#esempi-di-compromessi">Esempi di Compromessi</a></li>
  </ul></li>
  <li><a href="#sec-quant" id="toc-sec-quant" class="nav-link" data-scroll-target="#sec-quant"><span class="header-section-number">9.3.4</span> Quantizzazione</a>
  <ul class="collapse">
  <li><a href="#analisi-iniziale" id="toc-analisi-iniziale" class="nav-link" data-scroll-target="#analisi-iniziale">Analisi Iniziale</a></li>
  </ul></li>
  <li><a href="#i-tipi-1" id="toc-i-tipi-1" class="nav-link" data-scroll-target="#i-tipi-1"><span class="header-section-number">9.3.5</span> I Tipi</a>
  <ul class="collapse">
  <li><a href="#quantizzazione-uniforme" id="toc-quantizzazione-uniforme" class="nav-link" data-scroll-target="#quantizzazione-uniforme">Quantizzazione Uniforme</a></li>
  <li><a href="#quantizzazione-non-uniforme" id="toc-quantizzazione-non-uniforme" class="nav-link" data-scroll-target="#quantizzazione-non-uniforme">Quantizzazione Non-Uniforme</a></li>
  <li><a href="#quantizzazione-stocastica" id="toc-quantizzazione-stocastica" class="nav-link" data-scroll-target="#quantizzazione-stocastica">Quantizzazione Stocastica</a></li>
  <li><a href="#quantizzazione-zero-shot" id="toc-quantizzazione-zero-shot" class="nav-link" data-scroll-target="#quantizzazione-zero-shot">Quantizzazione ‚ÄúZero Shot‚Äù</a></li>
  </ul></li>
  <li><a href="#calibrazione" id="toc-calibrazione" class="nav-link" data-scroll-target="#calibrazione"><span class="header-section-number">9.3.6</span> Calibrazione</a>
  <ul class="collapse">
  <li><a href="#quantizzazione-simmetrica" id="toc-quantizzazione-simmetrica" class="nav-link" data-scroll-target="#quantizzazione-simmetrica">Quantizzazione Simmetrica</a></li>
  <li><a href="#quantizzazione-asimmetrica" id="toc-quantizzazione-asimmetrica" class="nav-link" data-scroll-target="#quantizzazione-asimmetrica">Quantizzazione Asimmetrica</a></li>
  <li><a href="#granularit√†" id="toc-granularit√†" class="nav-link" data-scroll-target="#granularit√†">Granularit√†</a></li>
  <li><a href="#quantizzazione-statica-e-dinamica" id="toc-quantizzazione-statica-e-dinamica" class="nav-link" data-scroll-target="#quantizzazione-statica-e-dinamica">Quantizzazione Statica e Dinamica</a></li>
  </ul></li>
  <li><a href="#tecniche" id="toc-tecniche" class="nav-link" data-scroll-target="#tecniche"><span class="header-section-number">9.3.7</span> Tecniche</a></li>
  <li><a href="#pesi-vs.-attivazioni" id="toc-pesi-vs.-attivazioni" class="nav-link" data-scroll-target="#pesi-vs.-attivazioni"><span class="header-section-number">9.3.8</span> Pesi vs.&nbsp;Attivazioni</a></li>
  <li><a href="#compromessi" id="toc-compromessi" class="nav-link" data-scroll-target="#compromessi"><span class="header-section-number">9.3.9</span> Compromessi</a></li>
  <li><a href="#quantizzazione-e-potatura" id="toc-quantizzazione-e-potatura" class="nav-link" data-scroll-target="#quantizzazione-e-potatura"><span class="header-section-number">9.3.10</span> Quantizzazione e Potatura</a></li>
  <li><a href="#quantizzazione-edge-aware" id="toc-quantizzazione-edge-aware" class="nav-link" data-scroll-target="#quantizzazione-edge-aware"><span class="header-section-number">9.3.11</span> Quantizzazione Edge-aware</a></li>
  </ul></li>
  <li><a href="#sec-model_ops_hw" id="toc-sec-model_ops_hw" class="nav-link" data-scroll-target="#sec-model_ops_hw"><span class="header-section-number">9.4</span> Implementazione Hardware Efficiente</a>
  <ul>
  <li><a href="#ricerca-di-architettura-neurale-basata-sullhardware" id="toc-ricerca-di-architettura-neurale-basata-sullhardware" class="nav-link" data-scroll-target="#ricerca-di-architettura-neurale-basata-sullhardware"><span class="header-section-number">9.4.1</span> Ricerca di Architettura Neurale Basata sull‚ÄôHardware</a>
  <ul class="collapse">
  <li><a href="#configurazione-single-target-fixed-platfrom" id="toc-configurazione-single-target-fixed-platfrom" class="nav-link" data-scroll-target="#configurazione-single-target-fixed-platfrom">Configurazione Single Target, Fixed Platfrom</a></li>
  <li><a href="#configurazioni-single-target-multiple-platform" id="toc-configurazioni-single-target-multiple-platform" class="nav-link" data-scroll-target="#configurazioni-single-target-multiple-platform">Configurazioni Single Target, Multiple Platform</a></li>
  <li><a href="#target-multipli" id="toc-target-multipli" class="nav-link" data-scroll-target="#target-multipli">Target Multipli</a></li>
  <li><a href="#esempi-di-hardware-aware-neural-architecture-search" id="toc-esempi-di-hardware-aware-neural-architecture-search" class="nav-link" data-scroll-target="#esempi-di-hardware-aware-neural-architecture-search">Esempi di ‚ÄúHardware-Aware Neural Architecture Search‚Äù</a></li>
  <li><a href="#topology-aware-nas" id="toc-topology-aware-nas" class="nav-link" data-scroll-target="#topology-aware-nas">Topology-Aware NAS</a></li>
  </ul></li>
  <li><a href="#sfide-nella-hardware-aware-neural-architecture-search" id="toc-sfide-nella-hardware-aware-neural-architecture-search" class="nav-link" data-scroll-target="#sfide-nella-hardware-aware-neural-architecture-search"><span class="header-section-number">9.4.2</span> Sfide nella ‚ÄúHardware-Aware Neural Architecture Search‚Äù</a></li>
  <li><a href="#ottimizzazioni-del-kernel" id="toc-ottimizzazioni-del-kernel" class="nav-link" data-scroll-target="#ottimizzazioni-del-kernel"><span class="header-section-number">9.4.3</span> Ottimizzazioni del Kernel</a>
  <ul class="collapse">
  <li><a href="#ottimizzazioni-del-kernel-generali" id="toc-ottimizzazioni-del-kernel-generali" class="nav-link" data-scroll-target="#ottimizzazioni-del-kernel-generali">Ottimizzazioni del kernel Generali</a></li>
  </ul></li>
  <li><a href="#compute-in-memory-cim" id="toc-compute-in-memory-cim" class="nav-link" data-scroll-target="#compute-in-memory-cim"><span class="header-section-number">9.4.4</span> Compute-in-Memory (CiM)</a></li>
  <li><a href="#ottimizzazione-dellaccesso-alla-memoria" id="toc-ottimizzazione-dellaccesso-alla-memoria" class="nav-link" data-scroll-target="#ottimizzazione-dellaccesso-alla-memoria"><span class="header-section-number">9.4.5</span> Ottimizzazione dell‚ÄôAccesso alla Memoria</a>
  <ul class="collapse">
  <li><a href="#sfruttamento-dei-dati-sparsi" id="toc-sfruttamento-dei-dati-sparsi" class="nav-link" data-scroll-target="#sfruttamento-dei-dati-sparsi">Sfruttamento dei Dati Sparsi</a></li>
  <li><a href="#framework-di-ottimizzazione" id="toc-framework-di-ottimizzazione" class="nav-link" data-scroll-target="#framework-di-ottimizzazione">Framework di Ottimizzazione</a></li>
  <li><a href="#hardware-costruito-attorno-al-software" id="toc-hardware-costruito-attorno-al-software" class="nav-link" data-scroll-target="#hardware-costruito-attorno-al-software">Hardware Costruito Attorno al Software</a></li>
  <li><a href="#splitnet" id="toc-splitnet" class="nav-link" data-scroll-target="#splitnet">SplitNet</a></li>
  <li><a href="#hardware-specifico-per-il-data-augmentation" id="toc-hardware-specifico-per-il-data-augmentation" class="nav-link" data-scroll-target="#hardware-specifico-per-il-data-augmentation">Hardware Specifico per il ‚ÄúData Augmentation‚Äù</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#supporto-software-e-framework" id="toc-supporto-software-e-framework" class="nav-link" data-scroll-target="#supporto-software-e-framework"><span class="header-section-number">9.5</span> Supporto Software e Framework</a>
  <ul>
  <li><a href="#api-native-di-ottimizzazione" id="toc-api-native-di-ottimizzazione" class="nav-link" data-scroll-target="#api-native-di-ottimizzazione"><span class="header-section-number">9.5.1</span> API Native di Ottimizzazione</a></li>
  <li><a href="#strumenti-di-ottimizzazione-automatizzata" id="toc-strumenti-di-ottimizzazione-automatizzata" class="nav-link" data-scroll-target="#strumenti-di-ottimizzazione-automatizzata"><span class="header-section-number">9.5.2</span> Strumenti di Ottimizzazione Automatizzata</a></li>
  <li><a href="#librerie-di-ottimizzazione-hardware" id="toc-librerie-di-ottimizzazione-hardware" class="nav-link" data-scroll-target="#librerie-di-ottimizzazione-hardware"><span class="header-section-number">9.5.3</span> Librerie di Ottimizzazione Hardware</a></li>
  <li><a href="#visualizzazione-delle-ottimizzazioni" id="toc-visualizzazione-delle-ottimizzazioni" class="nav-link" data-scroll-target="#visualizzazione-delle-ottimizzazioni"><span class="header-section-number">9.5.4</span> Visualizzazione delle Ottimizzazioni</a></li>
  <li><a href="#conversione-e-distribuzione-del-modello" id="toc-conversione-e-distribuzione-del-modello" class="nav-link" data-scroll-target="#conversione-e-distribuzione-del-modello"><span class="header-section-number">9.5.5</span> Conversione e Distribuzione del Modello</a></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">9.6</span> Conclusione</a></li>
  <li><a href="#sec-model-optimizations-resource" id="toc-sec-model-optimizations-resource" class="nav-link" data-scroll-target="#sec-model-optimizations-resource"><span class="header-section-number">9.7</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/optimizations/optimizations.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/optimizations/optimizations.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/training/training.it.html">Training</a></li><li class="breadcrumb-item"><a href="../../contents/optimizations/optimizations.it.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-model_optimizations" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-model-optimizations-resource">Slide</a>, <a href="#sec-model-optimizations-resource">Video</a>, <a href="#sec-model-optimizations-resource">Esercizi</a>, <a href="#sec-model-optimizations-resource">Laboratori</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_model_optimizations.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Illustrazione di un modello di rete neurale rappresentato come un cantiere edile affollato, con un gruppo eterogeneo di operai edili, sia uomini che donne, di varie etnie, etichettati come ‚Äòpruning‚Äô, ‚Äòquantization‚Äô e ‚Äòsparsity‚Äô. Stanno lavorando insieme per rendere la rete neurale pi√π efficiente e pi√π piccola, mantenendo un‚Äôelevata precisione. L‚Äôoperaio che si occupa del ‚Äòpruning‚Äô, una donna ispanica, sta tagliando le connessioni non necessarie dal centro della rete. L‚Äôaddetto alla ‚Äòquantization‚Äô, un uomo caucasico, sta regolando o modificando i pesi ovunque. L‚Äôoperaio che si occupa di ‚Äòsparsity‚Äô, una donna africana, sta rimuovendo i nodi non necessari per ridurre il modello. Sullo sfondo, camion e gru edili assistono gli operai nei loro compiti. La rete neurale si sta trasformando visivamente da una struttura complessa e grande a una pi√π snella e piccola.</em></figcaption>
</figure>
</div>
<p>Quando i modelli di apprendimento automatico vengono distribuiti su sistemi, in particolare su sistemi embedded con risorse limitate, l‚Äôottimizzazione dei modelli √® una necessit√†. Mentre il machine learning richiede spesso risorse computazionali sostanziali, i sistemi sono intrinsecamente limitati in termini di memoria, potenza di elaborazione ed energia. Questo capitolo si immerger√† nell‚Äôarte e nella scienza dell‚Äôottimizzazione dei modelli di machine learning per garantire che siano leggeri, efficienti ed efficaci quando distribuiti in scenari TinyML.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Apprendere tecniche come ‚Äúpruning‚Äù, ‚Äúknowledge distillation‚Äù e architetture di modelli specializzate per rappresentare i modelli in modo pi√π efficiente</p></li>
<li><p>Comprendere i metodi di quantizzazione per ridurre le dimensioni del modello e consentire un‚Äôinferenza pi√π rapida tramite numeri con precisione ridotta</p></li>
<li><p>Esplorare approcci di ottimizzazione basati sull‚Äôhardware per abbinare i modelli alle capacit√† del dispositivo target</p></li>
<li><p>Sviluppare un pensiero olistico per bilanciare i compromessi in termini di complessit√† del modello, accuratezza, latenza, potenza ecc. in base ai requisiti dell‚Äôapplicazione</p></li>
<li><p>Scoprire strumenti software come framework e piattaforme di conversione del modello che consentono l‚Äôimplementazione di modelli ottimizzati</p></li>
<li><p>Ottenere informazioni strategiche sulla selezione e l‚Äôapplicazione di ottimizzazioni del modello in base ai vincoli del caso d‚Äôuso e ai target hardware</p></li>
</ul>
</div>
</div>
<section id="introduzione" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="introduzione"><span class="header-section-number">9.1</span> Introduzione</h2>
<p>Abbiamo strutturato questo capitolo in tre livelli. Innanzitutto, in <a href="#sec-model_ops_representation" class="quarto-xref"><span>Sezione 9.2</span></a> esaminiamo l‚Äôimportanza e le metodologie per ridurre la complessit√† dei parametri dei modelli senza compromettere le loro capacit√† di inferenza. Vengono discusse tecniche come il ‚Äúpruning‚Äù [potatura] e la distillazione della conoscenza, offrendo spunti su come i modelli possono essere compressi e semplificati mantenendo, o addirittura migliorando, le loro prestazioni.</p>
<p>Scendendo di un livello, in <a href="#sec-model_ops_numerics" class="quarto-xref"><span>Sezione 9.3</span></a>, studiamo il ruolo della precisione numerica nei calcoli dei modelli e come la sua modifica influisce sulle sue dimensioni, velocit√† e precisione. Esamineremo i vari formati numerici e come l‚Äôaritmetica a precisione ridotta pu√≤ essere sfruttata per ottimizzare i modelli per la distribuzione embedded.</p>
<p>Infine, man mano che scendiamo pi√π in basso e ci avviciniamo all‚Äôhardware, in <a href="#sec-model_ops_hw" class="quarto-xref"><span>Sezione 9.4</span></a>, esploreremo il panorama della progettazione congiunta hardware-software, esplorando come i modelli possono essere ottimizzati adattandoli alle caratteristiche e alle capacit√† specifiche dell‚Äôhardware target. Discuteremo di come i modelli possono essere adattati per sfruttare efficacemente le risorse hardware disponibili.</p>
<div id="fig-3-sections" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-sections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_structure.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-sections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.1: Tre livelli da coprire.
</figcaption>
</figure>
</div>
</section>
<section id="sec-model_ops_representation" class="level2 page-columns page-full" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="sec-model_ops_representation"><span class="header-section-number">9.2</span> Rappresentazione Efficiente del Modello</h2>
<p>Il primo passo per l‚Äôottimizzazione del modello inizia in un territorio familiare per la maggior parte dei professionisti del ML: la rappresentazione efficiente del modello viene spesso affrontata per la prima volta al livello pi√π alto di astrazione della parametrizzazione, ovvero l‚Äôarchitettura stessa del modello.</p>
<p>La maggior parte dei professionisti del ML tradizionali progetta modelli con un obiettivo generale di alto livello in mente, che si tratti di classificazione delle immagini, rilevamento di persone o individuazione di parole chiave come menzionato in precedenza in questo testo. I loro progetti in genere finiscono per adattarsi naturalmente ad alcuni vincoli soft dovuti a risorse di elaborazione limitate durante lo sviluppo, ma in genere questi progetti non sono a conoscenza di vincoli successivi, come quelli richiesti se il modello deve essere distribuito su un dispositivo pi√π limitato anzich√© sul cloud.</p>
<p>In questa sezione, discuteremo di come i professionisti possono sfruttare i principi della progettazione congiunta hardware-software anche nell‚Äôarchitettura di alto livello di un modello per rendere i loro modelli compatibili con i dispositivi edge. Da quelli pi√π consapevoli dell‚Äôhardware a quelli meno consapevoli a questo livello di modifica, discutiamo alcune delle strategie pi√π comuni per una parametrizzazione efficiente del modello: pruning, compressione e architetture edge-friendly. Abbiamo gi√† parlato di pruning e compressione del modello in <a href="../efficient_ai/efficient_ai.it.html#sec-efficient-model-compression" class="quarto-xref"><span>Sezione 8.4</span></a>; questa sezione andr√† oltre le definizioni per fornire una comprensione tecnica del loro funzionamento.</p>
<section id="sec-pruning" class="level3 page-columns page-full" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="sec-pruning"><span class="header-section-number">9.2.1</span> Il Pruning</h3>
<section id="panoramica" class="level4">
<h4 class="anchored" data-anchor-id="panoramica">Panoramica</h4>
<p>Il model pruning [potatura] √® una tecnica di apprendimento automatico che riduce le dimensioni e la complessit√† di un modello di rete neurale, mantenendone il pi√π possibile le capacit√† predittive. L‚Äôobiettivo della potatura √® quello di rimuovere componenti ridondanti o non essenziali del modello, tra cui connessioni tra neuroni, singoli neuroni o persino interi layer della rete.</p>
<p>Questo processo in genere comporta l‚Äôanalisi del modello di machine learning per identificare e rimuovere pesi, nodi o layer che hanno scarso impatto sugli output del modello. Potando selettivamente un modello in questo modo, il numero totale di parametri pu√≤ essere ridotto in modo significativo senza cali sostanziali nell‚Äôaccuratezza del modello. Il modello compresso risultante richiede meno memoria e risorse di calcolo per l‚Äôaddestramento e l‚Äôesecuzione, consentendo tempi di inferenza pi√π rapidi.</p>
<p>Il pruning del modello √® particolarmente utile quando si distribuiscono modelli di apprendimento automatico su dispositivi con risorse di calcolo limitate, come telefoni cellulari o sistemi TinyML. La tecnica facilita la distribuzione di modelli pi√π grandi e complessi su questi dispositivi riducendo le loro richieste di risorse. Inoltre, i modelli pi√π piccoli richiedono meno dati per generalizzare bene e sono meno inclini all‚Äôoverfitting [sovradattamento]. Fornendo un modo efficiente per semplificare i modelli, la potatura dei modelli √® diventata una tecnica fondamentale per ottimizzare le reti neurali nell‚Äôapprendimento automatico.</p>
<p>Esistono diverse tecniche di potatura comuni utilizzate nell‚Äôapprendimento automatico, tra cui la potatura strutturata, la potatura non strutturata, la potatura iterativa, la potatura bayesiana e persino la potatura casuale. Oltre a potare i pesi, si possono anche potare le attivazioni. La potatura di attivazioni prende di mira specificamente neuroni o filtri che si attivano raramente o hanno un‚Äôattivazione complessivamente bassa. Esistono numerosi altri metodi, come la potatura di sensibilit√† e movimento. Per un elenco completo dei metodi, si consiglia al lettore di leggere il seguente articolo: <a href="https://arxiv.org/pdf/2308.06767.pdf">‚ÄúA Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations‚Äù (2023)</a>.</p>
<p>Quindi, come si scelgono i metodi di potatura? Esistono molte varianti di tecniche di potatura, ciascuna delle quali varia l‚Äôeuristica di ci√≤ che dovrebbe essere mantenuto e potato dal modello, nonch√© il numero di volte in cui cui deve essere eseguita. Tradizionalmente, la potatura avviene dopo che il modello √® completamente addestrato, dove il modello potato pu√≤ subire una lieve perdita di accuratezza. Tuttavia, come discuteremo pi√π avanti, recenti scoperte hanno trovato che la potatura pu√≤ essere utilizzata durante l‚Äôaddestramento (ad esempio, in modo iterativo) per identificare rappresentazioni del modello pi√π efficienti e accurate.</p>
</section>
<section id="potatura-strutturata" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="potatura-strutturata">Potatura Strutturata</h4>
<p>Iniziamo con la ‚Äúpotatura strutturata‚Äù, una tecnica che riduce le dimensioni di una rete neurale eliminando intere sotto-strutture specifiche del modello mantenendone la struttura generale. Rimuove interi neuroni/canali o layer in base a criteri di importanza. Ad esempio, per una rete neurale convoluzionale (CNN), potrebbero essere determinate istanze di filtro o canali. Per reti completamente connesse, potrebbero essere i neuroni stessi mantenendo la piena connettivit√† o persino l‚Äôeliminazione di interi layer del modello che sono considerati insignificanti. Questo tipo di potatura spesso porta a reti sparse regolari e strutturate che sono compatibili con l‚Äôhardware.</p>
<p>Sono iniziate a emergere le ‚Äúbest practice‚Äù su come pensare alla potatura strutturata. Ci sono tre componenti principali:</p>
<section id="strutture-candidate-per-il-pruning" class="level5">
<h5 class="anchored" data-anchor-id="strutture-candidate-per-il-pruning">1. Strutture Candidate per il Pruning</h5>
<p>Data la variet√† di approcci, diverse strutture all‚Äôinterno di una rete neurale vengono potate in base a criteri specifici. Le strutture primarie per la potatura includono neuroni, canali e talvolta interi layer, ognuno con le sue implicazioni e metodologie uniche. L‚Äôobiettivo di ogni approccio √® garantire che il modello ridotto mantenga il pi√π possibile la capacit√† predittiva del modello originale, migliorando al contempo l‚Äôefficienza computazionale e riducendo le dimensioni.</p>
<p>Quando i <strong>neuroni</strong> vengono potati, rimuoviamo interi neuroni insieme ai loro pesi e bias associati, riducendo cos√¨ la larghezza del layer. Questo tipo di potatura viene spesso utilizzato in layer completamente connessi.</p>
<p>La potatura del <strong>canale</strong>, che viene applicata prevalentemente nelle reti neurali convoluzionali (CNN), comporta l‚Äôeliminazione di interi canali o filtri, il che a sua volta riduce la profondit√† delle mappe delle feature e influisce sulla capacit√† della rete di estrarre determinate feature dai dati di input. Ci√≤ √® particolarmente cruciale nelle attivit√† di elaborazione delle immagini in cui l‚Äôefficienza computazionale √® fondamentale.</p>
<p>Infine, la potatura dei <strong>layer</strong> adotta un approccio pi√π aggressivo rimuovendo interi layer della rete. Ci√≤ riduce significativamente la profondit√† della rete e quindi la sua capacit√† di plasmare pattern e gerarchie complesse nei dati. Questo approccio richiede un attento equilibrio per garantire che la capacit√† predittiva del modello non venga indebitamente compromessa.</p>
<p><a href="#fig-channel-layer-pruning" class="quarto-xref">Figura&nbsp;<span>9.2</span></a> mostra la differenza tra la potatura di canale/filtro e quella del layer. Quando potiamo un canale, dobbiamo riconfigurare l‚Äôarchitettura del modello per adattarla ai cambiamenti strutturali. Una modifica consiste nel cambiare il numero di canali di input nel layer successivo (qui, il terzo e il layer pi√π profondo): modificando le profondit√† dei filtri applicati al layer con il canale potato. D‚Äôaltra parte, la potatura di un intero layer (rimuovendo tutti i canali nel layer) richiede modifiche pi√π drastiche. Quella principale riguarda la modifica delle connessioni tra i layer rimanenti per sostituire o bypassare il layer potato. Nel nostro caso, riconfiguriamo per connettere il primo e l‚Äôultimo layer. In tutti i casi di potatura, dobbiamo mettere a punto la nuova struttura per regolare i pesi.</p>
<div id="fig-channel-layer-pruning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-channel-layer-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_channel_layer_pruning.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-channel-layer-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.2: Potatura del canale e quella del layer.
</figcaption>
</figure>
</div>
</section>
<section id="stabilire-un-criterio-per-il-pruning" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="stabilire-un-criterio-per-il-pruning">2. Stabilire un criterio per il Pruning</h5>
<p>Stabilire criteri ben definiti per determinare quali strutture specifiche potare da un modello di rete neurale √® una componente cruciale del processo di ‚Äúpruning‚Äù del modello. L‚Äôobiettivo principale qui √® identificare e rimuovere i componenti che contribuiscono meno alle capacit√† predittive del modello, mantenendo al contempo le strutture integrali per preservare l‚Äôaccuratezza.</p>
<p>Una strategia ampiamente adottata ed efficace per potare sistematicamente le strutture si basa sul calcolo di punteggi di importanza per singoli componenti come neuroni, filtri, canali o layer. Questi punteggi servono come metriche quantitative per valutare la significativit√† di ciascuna struttura e il suo effetto sull‚Äôoutput del modello.</p>
<p>Esistono diverse tecniche per assegnare questi punteggi sull‚Äôimportanza:</p>
<ul>
<li><strong>Pruning Basato sulla Magnitudo del peso</strong>: Questo approccio assegna punteggi di importanza a una struttura valutando la magnitudo aggregata dei pesi associati. Le strutture con magnitudo del peso complessivo inferiore sono considerate meno critiche per le prestazioni della rete.</li>
<li><strong>Pruning Basato sul Bradiente</strong>: Questa tecnica utilizza i gradienti della funzione di los [perdita] rispetto ai pesi associati a una struttura. Le strutture con magnitudo del gradiente cumulativo basso, che indica un impatto minimo sulla perdita quando alterato, sono le candidate principali per la potatura.</li>
<li><strong>Pruning Basato sull‚ÄôAttivazione</strong>: Questo metodo tiene traccia della frequenza con cui un neurone o un filtro viene attivato memorizzando queste informazioni in un parametro chiamato contatore delle attivazioni. Ogni volta che la struttura viene attivata, il contatore viene incrementato. Un conteggio di attivazione basso suggerisce che la struttura √® meno rilevante.</li>
<li><strong>Pruning Basato sull‚ÄôEspansione di Taylor</strong>: Questo approccio approssima la modifica nella funzione di perdita derivante dalla rimozione di un dato peso. Valutando la perturbazione della perdita cumulativa derivante dalla rimozione di tutti i pesi associati a una struttura, √® possibile identificare le strutture con un impatto trascurabile sulla perdita, rendendole candidate idonee per la potatura.</li>
</ul>
<p>L‚Äôidea √® di misurare, direttamente o indirettamente, il contributo di ogni componente all‚Äôoutput del modello. Le strutture con un‚Äôinfluenza minima in base ai criteri definiti vengono potate per prime. Ci√≤ consente una potatura selettiva e ottimizzata che comprime al massimo i modelli preservando al contempo la capacit√† predittiva. In generale, √® importante valutare l‚Äôimpatto della rimozione di particolari strutture sull‚Äôoutput del modello, con lavori recenti come <span class="citation" data-cites="rachwan2022winning">(<a href="../../references.it.html#ref-rachwan2022winning" role="doc-biblioref">Rachwan et al. 2022</a>)</span> e <span class="citation" data-cites="lubana2020gradient">(<a href="../../references.it.html#ref-lubana2020gradient" role="doc-biblioref">Lubana e Dick 2020</a>)</span> che studiano combinazioni di tecniche come la potatura basata sulla magnitudine e la potatura basata sul gradiente.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rachwan2022winning" class="csl-entry" role="listitem">
Rachwan, John, Daniel Z√ºgner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, e Stephan G√ºnnemann. 2022. <span>¬´Winning the lottery ahead of time: <span>Efficient</span> early network pruning¬ª</span>. In <em>International Conference on Machine Learning</em>, 18293‚Äì309. PMLR.
</div><div id="ref-lubana2020gradient" class="csl-entry" role="listitem">
Lubana, Ekdeep Singh, e Robert P Dick. 2020. <span>¬´A gradient flow framework for analyzing network pruning¬ª</span>. <em>arXiv preprint arXiv:2009.11839</em>.
</div></div></section>
<section id="selezione-di-una-strategia-di-potatura" class="level5">
<h5 class="anchored" data-anchor-id="selezione-di-una-strategia-di-potatura">3. Selezione di una strategia di potatura</h5>
<p>Ora che abbiamo capito alcune tecniche per determinare l‚Äôimportanza delle strutture all‚Äôinterno di una rete neurale, il passo successivo √® decidere come applicare queste intuizioni. Ci√≤ comporta la selezione di una strategia di potatura appropriata, che stabilisce come e quando le strutture identificate vengono rimosse e come il modello viene messo a punto per mantenere le sue prestazioni. Esistono due principali strategie di potatura strutturata: quella iterativa e la one-shot.</p>
<p>La <strong>potatura iterativa</strong> rimuove gradualmente le strutture attraverso pi√π cicli di potatura seguiti da messa a punto. In ogni ciclo, un piccolo set di strutture viene potato in base a criteri di importanza. Il modello viene poi messo a punto, consentendogli di adattarsi senza problemi ai cambiamenti strutturali prima della successiva iterazione di potatura. Questo approccio graduale e ciclico impedisce bruschi cali di accuratezza. Consente al modello di adattarsi lentamente man mano che le strutture vengono ridotte attraverso le iterazioni.</p>
<p>Consideriamo una situazione in cui desideriamo potare i 6 canali meno efficaci (in base ad alcuni criteri specifici) da una rete neurale convoluzionale. In <a href="#fig-iterative-pruning" class="quarto-xref">Figura&nbsp;<span>9.3</span></a>, mostriamo un processo di potatura semplificato eseguito su 3 iterazioni. In ogni iterazione, eliminiamo solo 2 canali. La rimozione dei canali comporta un degrado della precisione. Nella prima iterazione, la precisione scende da 0.995 a 0.971. Tuttavia, dopo aver perfezionato il modello sulla nuova struttura, siamo in grado di recuperare dalla perdita di prestazioni, portando la precisione a 0.992. Poich√© i cambiamenti strutturali sono minori e graduali, la rete pu√≤ adattarsi pi√π facilmente a essi. Eseguendo lo stesso processo altre 2 volte, finiamo con una precisione finale di 0.991 (una perdita di solo lo 0.4% rispetto all‚Äôoriginale) e una riduzione del 27% nel numero di canali. Pertanto, la potatura iterativa ci consente di mantenere le prestazioni beneficiando di una maggiore efficienza computazionale dovuta alla riduzione delle dimensioni del modello.</p>
<div id="fig-iterative-pruning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iterative-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_iterative_pruning.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iterative-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.3: Potatura iterativa.
</figcaption>
</figure>
</div>
<p>La <strong>potatura one-shot</strong> adotta un approccio pi√π aggressivo, potando una grande porzione di strutture simultaneamente in un‚Äôunica operazione in base a criteri di importanza predefiniti. Segue un‚Äôampia messa a punto per recuperare l‚Äôaccuratezza del modello. Sebbene pi√π rapida, questa strategia aggressiva pu√≤ degradare l‚Äôaccuratezza se il modello non riesce a recuperare durante la messa a punto.</p>
<p>La scelta tra queste strategie comporta la valutazione di fattori quali dimensioni del modello, quanto √® sparso il target, calcolo disponibile e perdite di accuratezza accettabili. La potatura one-shot pu√≤ comprimere rapidamente i modelli, ma quella iterativa pu√≤ consentire una migliore conservazione dell‚Äôaccuratezza per un livello target di potatura. In pratica, la strategia √® personalizzata in base ai vincoli del caso d‚Äôuso. L‚Äôobiettivo generale √® quello di generare una strategia ottimale che rimuova la ridondanza, ottenga guadagni di efficienza tramite la potatura e metta a punto il modello per stabilizzare l‚Äôaccuratezza a un livello accettabile per l‚Äôimplementazione.</p>
<p>Ora si consideri la stessa rete che avevamo nell‚Äôesempio di potatura iterativa. Mentre nel processo iterativo abbiamo potato 2 canali alla volta, nella potatura one-shot poteremo i 6 canali contemporaneamente (<a href="#fig-oneshot-pruning" class="quarto-xref">Figura&nbsp;<span>9.4</span></a>). La rimozione simultanea del 27% del canale della rete altera significativamente la struttura, causando un calo della precisione da 0.995 a 0.914. Date le modifiche principali, la rete non √® in grado di adattarsi correttamente durante la messa a punto e la precisione √® salita a 0.943, un degrado del 5% rispetto alla precisione della rete non potata. Mentre le strutture finali nei processi di potatura iterativa e di potatura one-shot sono identiche, la prima √® in grado di mantenere prestazioni elevate mentre la seconda subisce degradi significativi.</p>
<div id="fig-oneshot-pruning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-oneshot-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_oneshot_pruning.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-oneshot-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.4: Potatura one-shot.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="vantaggi-della-potatura-strutturata" class="level4">
<h4 class="anchored" data-anchor-id="vantaggi-della-potatura-strutturata">Vantaggi della Potatura Strutturata</h4>
<p>La potatura strutturata offre una miriade di vantaggi che soddisfano vari aspetti dell‚Äôimplementazione e dell‚Äôutilizzo del modello, specialmente in ambienti in cui le risorse computazionali sono limitate.</p>
<ul>
<li><p><strong>Efficienza Computazionale:</strong> Eliminando intere strutture, come neuroni o canali, si riduce significativamente il carico computazionale durante le fasi di training e inferenza, consentendo cos√¨ previsioni pi√π rapide del modello e convergenza del training. Inoltre, la rimozione delle strutture riduce intrinsecamente il ‚Äúfootprint‚Äù [impronta] di memoria del modello, assicurando che richieda meno spazio di archiviazione e memoria durante il funzionamento, il che √® particolarmente vantaggioso in ambienti con limiti di memoria come i sistemi TinyML.</p></li>
<li><p><strong>Efficienza Hardware:</strong> La potatura strutturata spesso si traduce in modelli pi√π adatti all‚Äôimplementazione su hardware specializzato, come i Field-Programmable Gate Arrays (FPGA) o Application-Specific Integrated Circuits (ASIC), a causa della regolarit√† e la semplicit√† dell‚Äôarchitettura potata. Con requisiti di elaborazione ridotti, si traduce in un consumo energetico inferiore, fondamentale per i dispositivi alimentati a batteria e i metodi di elaborazione sostenibili.</p></li>
<li><p><strong>Manutenzione e Distribuzione:</strong> Il modello ridotto, sebbene pi√π piccolo, mantiene la sua forma architettonica originale, che pu√≤ semplificare la pipeline di distribuzione e garantire la compatibilit√† con i sistemi e i framework esistenti. Inoltre, con meno parametri e strutture pi√π semplici, il modello potato diventa pi√π facile da gestire e monitorare negli ambienti di produzione, riducendo potenzialmente le spese generali associate alla manutenzione e agli aggiornamenti del modello. Pi√π avanti, quando approfondiremo <a href="../ops/ops.qmd">MLOps</a>, questa necessit√† diventer√† evidente.</p></li>
</ul>
</section>
<section id="potatura-non-strutturata" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="potatura-non-strutturata">Potatura non Strutturata</h4>
<p>Il ‚Äúpruning‚Äù non-strutturato √®, come suggerisce il nome, la potatura del modello senza riguardo alla sotto-struttura specifica del modello. Come accennato in precedenza, offre una maggiore aggressivit√† nella potatura e pu√≤ raggiungere maggiori diradazione del modello mantenendo la precisione, dati meno vincoli su ci√≤ che pu√≤ e non pu√≤ essere potato. In genere, la potatura non-strutturata post-training consiste in un criterio di importanza per i singoli parametri/pesi del modello, potatura/rimozione dei pesi che scendono al di sotto dei criteri e una successiva messa a punto facoltativa per provare a recuperare la precisione persa durante la rimozione dei pesi.</p>
<p>La potatura non-strutturata presenta alcuni vantaggi rispetto a quella strutturata: la rimozione di singoli pesi anzich√© di intere sotto-strutture del modello spesso porta in pratica a minori diminuzioni della precisione del modello. Inoltre, in genere determinare il criterio di importanza per un singolo peso √® molto pi√π semplice che per un‚Äôintera sotto-struttura di parametri nella potatura strutturata, rendendo la prima preferibile nei casi in cui tale overhead √® difficile o poco chiaro da calcolare. Analogamente, il processo effettivo di potatura strutturata √® generalmente meno flessibile, poich√© la rimozione di singoli pesi √® generalmente pi√π semplice della rimozione di intere sotto-strutture e della garanzia che il modello funzioni ancora.</p>
<p>La potatura non strutturata, pur offrendo il potenziale per una significativa riduzione delle dimensioni del modello e una migliore implementabilit√†, porta con s√© problemi legati alla gestione di rappresentazioni sparse e alla garanzia dell‚Äôefficienza computazionale. √à particolarmente utile in scenari in cui √® fondamentale ottenere la massima compressione possibile del modello e in cui l‚Äôambiente di distribuzione pu√≤ gestire in modo efficiente i calcoli sparsi.</p>
<p><a href="#tbl-pruning_methods" class="quarto-xref">Tabella&nbsp;<span>9.1</span></a> fornisce un confronto conciso tra potatura strutturata e la non-strutturata. In questa tabella, gli aspetti relativi alla natura e all‚Äôarchitettura del modello potato (Definizione, Regolarit√† del modello e Livello di compressione) sono raggruppati insieme, seguiti dagli aspetti relativi alle considerazioni computazionali (Efficienza computazionale e Compatibilit√† hardware) e terminando con gli aspetti relativi all‚Äôimplementazione e all‚Äôadattamento del modello potato (Complessit√† di implementazione e Complessit√† di messa a punto). Entrambe le strategie di potatura offrono vantaggi e problemi unici, come mostrato in <a href="#tbl-pruning_methods" class="quarto-xref">Tabella&nbsp;<span>9.1</span></a>, e la selezione tra di esse dovrebbe essere influenzata da requisiti specifici del progetto e della distribuzione.</p>
<div id="tbl-pruning_methods" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pruning_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;9.1: Confronto tra potatura strutturata e non-strutturata.
</figcaption>
<div aria-describedby="tbl-pruning_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 32%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspetto</th>
<th style="text-align: left;">Potatura strutturata</th>
<th style="text-align: left;">Potatura non strutturata</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Definizione</td>
<td style="text-align: left;">Potatura di intere strutture (ad esempio, neuroni, canali, layer) all‚Äôinterno della rete</td>
<td style="text-align: left;">Potatura di singoli pesi o neuroni, con conseguenti matrici sparse o strutture di rete non regolari</td>
</tr>
<tr class="even">
<td style="text-align: left;">Regolarit√† del Modello</td>
<td style="text-align: left;">Mantiene un‚Äôarchitettura di rete regolare e strutturata</td>
<td style="text-align: left;">Si traduce in architetture di rete irregolari e sparse</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Livello di Compressione</td>
<td style="text-align: left;">Pu√≤ offrire una compressione del modello limitata rispetto alla potatura non-strutturata</td>
<td style="text-align: left;">Pu√≤ ottenere una compressione del modello pi√π elevata grazie alla potatura a grana fine</td>
</tr>
<tr class="even">
<td style="text-align: left;">Efficienza Computazionale</td>
<td style="text-align: left;">In genere pi√π efficiente computazionalmente grazie al mantenimento di strutture regolari</td>
<td style="text-align: left;">Pu√≤ essere inefficiente dal punto di vista computazionale a causa di matrici di peso sparse, a meno che non venga utilizzato hardware/software specializzato</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Compatibilit√† Hardware</td>
<td style="text-align: left;">In genere pi√π compatibile con vari hardware grazie alle strutture regolari</td>
<td style="text-align: left;">Potrebbe richiedere hardware che gestisca in modo efficiente i calcoli sparsi per ottenere vantaggi</td>
</tr>
<tr class="even">
<td style="text-align: left;">Complessit√† di Implementazione</td>
<td style="text-align: left;">Spesso pi√π semplice da implementare e gestire grazie al mantenimento della struttura della rete</td>
<td style="text-align: left;">Pu√≤ essere complesso da gestire e calcolare a causa delle rappresentazioni sparse</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Complessit√† di Messa a Punto Fine</td>
<td style="text-align: left;">Potrebbe richiedere strategie di messa a punto fine meno complesse dopo la potatura</td>
<td style="text-align: left;">Potrebbe richiedere strategie di riaddestramento o messa a punto fine pi√π complesse dopo la potatura</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In <a href="#fig-structured-unstructured" class="quarto-xref">Figura&nbsp;<span>9.5</span></a> abbiamo esempi che illustrano le differenze tra potatura non-strutturata e strutturata. Osservare che la potatura non-strutturata pu√≤ portare a modelli che non rispettano pi√π le garanzie strutturali di alto livello delle loro controparti originali non potate: la rete di sinistra non √® pi√π una rete completamente connessa dopo la potatura. La potatura strutturata, d‚Äôaltro canto, mantiene quelle invarianti: al centro, la rete completamente connessa viene potata in modo che resti ancora completamente connessa; allo stesso modo, la CNN mantiene la sua struttura convoluzionale, sebbene con meno filtri.</p>
<div id="fig-structured-unstructured" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-structured-unstructured-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_pruning_comparison.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-structured-unstructured-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.5: Potatura non-strutturata e strutturata. Fonte: <span class="citation" data-cites="qi2021efficient">Qi et al. (<a href="../../references.it.html#ref-qi2021efficient" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-qi2021efficient" class="csl-entry" role="listitem">
Qi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang, e Honggang Zhang. 2021. <span>¬´An efficient pruning scheme of deep neural networks for Internet of Things applications¬ª</span>. <em>EURASIP Journal on Advances in Signal Processing</em> 2021 (1): 31. <a href="https://doi.org/10.1186/s13634-021-00744-4">https://doi.org/10.1186/s13634-021-00744-4</a>.
</div></div></figure>
</div>
</section>
<section id="ipotesi-del-biglietto-della-lotteria" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="ipotesi-del-biglietto-della-lotteria">Ipotesi del Biglietto della Lotteria</h4>
<p>La potatura si √® evoluta da una tecnica puramente post-addestramento che comportava un costo per una certa accuratezza, a un potente approccio di meta-apprendimento applicato durante l‚Äôaddestramento per ridurre la complessit√† del modello. Questo progresso a sua volta migliora l‚Äôefficienza di calcolo, memoria e latenza sia nell‚Äôaddestramento che nell‚Äôinferenza.</p>
<p>Una scoperta rivoluzionaria che ha catalizzato questa evoluzione √® stata l‚Äô<a href="https://arxiv.org/abs/1803.03635">ipotesi del biglietto della lotteria</a> di <span class="citation" data-cites="jonathan2019lottery">Frankle e Carbin (<a href="../../references.it.html#ref-jonathan2019lottery" role="doc-biblioref">2019</a>)</span>. Il loro lavoro afferma che all‚Äôinterno di reti neurali dense esistono sotto-reti sparse, denominate ‚Äúbiglietti vincenti‚Äù, che possono eguagliare o addirittura superare le prestazioni del modello originale quando addestrate in isolamento. In particolare, questi biglietti vincenti, quando inizializzati utilizzando gli stessi pesi della rete originale, possono raggiungere una convergenza e un‚Äôaccuratezza di addestramento altrettanto elevate su un dato compito. Vale la pena sottolineare che hanno scoperto empiricamente l‚Äôipotesi del biglietto della lotteria, che √® stata successivamente formalizzata.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jonathan2019lottery" class="csl-entry" role="listitem">
Frankle, Jonathan, e Michael Carbin. 2019. <span>¬´The Lottery Ticket Hypothesis: <span>Finding</span> Sparse, Trainable Neural Networks¬ª</span>. In <em>7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net. <a href="https://openreview.net/forum?id=rJl-b3RcF7">https://openreview.net/forum?id=rJl-b3RcF7</a>.
</div></div><p>L‚Äôintuizione alla base di questa ipotesi √® che, durante il processo di addestramento di una rete neurale, molti neuroni e connessioni diventano ridondanti o non importanti, in particolare con l‚Äôinclusione di tecniche di addestramento che incoraggiano la ridondanza come il ‚Äúdropout‚Äù [abbandono]. L‚Äôidentificazione, la potatura e l‚Äôinizializzazione di questi ‚Äúbiglietti vincenti‚Äù consentono un addestramento pi√π rapido e modelli pi√π efficienti, poich√© contengono le informazioni essenziali per la decisione del modello per l‚Äôattivit√†. Inoltre, come generalmente noto con la teoria del ‚Äúbias-variance tradeoff‚Äù [compromesso tra bias e varianza], questi biglietti soffrono meno di sovra-parametrizzazione e quindi si generalizzano meglio piuttosto che sovra-adattarsi all‚Äôattivit√†.</p>
<p>In <a href="#fig-lottery-ticket-hypothesis" class="quarto-xref">Figura&nbsp;<span>9.6</span></a> abbiamo un esempio che mostra esperimenti di potatura e addestramento su una LeNet completamente connessa su una variet√† di rapporti di potatura. Nel grafico a sinistra, si nota come una potatura pesante riveli una sotto-rete pi√π efficiente (in verde) che √® il 21,1% delle dimensioni della rete originale (in blu). La sotto-rete raggiunge una maggiore accuratezza e in modo pi√π rapido rispetto alla versione non potata (la linea verde √® sopra la linea blu). Tuttavia, la potatura ha un limite (punto ottimale) e un‚Äôulteriore potatura produrr√† degradi delle prestazioni e alla fine scender√† al di sotto delle prestazioni della versione non potata (nota come le sotto-reti rossa, viola e marrone diminuiscono gradualmente nelle prestazioni di accuratezza) a causa della significativa perdita nel numero di parametri.</p>
<div id="fig-lottery-ticket-hypothesis" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lottery-ticket-hypothesis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_lottery_ticket_hypothesis.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lottery-ticket-hypothesis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.6: Esperimenti sull‚Äôipotesi del biglietto della lotteria.
</figcaption>
</figure>
</div>
<p>Per scoprire questi biglietti vincenti della lotteria all‚Äôinterno di una rete neurale, viene seguito un processo sistematico. Questo processo, illustrato in <a href="#fig-winning-ticket" class="quarto-xref">Figura&nbsp;<span>9.7</span></a> (a sinistra), prevede l‚Äôaddestramento iterativo, la potatura e la reinizializzazione della rete. I passaggi seguenti delineano questo approccio:</p>
<ol type="1">
<li><p>Inizializzare i pesi della rete a valori casuali.</p></li>
<li><p>Addestrare la rete finch√© non converge alle prestazioni desiderate.</p></li>
<li><p>Eliminare una percentuale di rami con i valori di peso pi√π bassi.</p></li>
<li><p>Reinizializzare la rete con gli stessi valori casuali del passaggio 1.</p></li>
<li><p>Ripetere i passaggi 2-4 pi√π volte o finch√© la precisione non peggiora in modo significativo.</p></li>
</ol>
<p>Alla fine, ci si ritrova con una rete potata (<a href="#fig-winning-ticket" class="quarto-xref">Figura&nbsp;<span>9.7</span></a> lato destro), che √® una sotto-rete di quella di partenza. La sotto-rete dovrebbe avere una struttura significativamente pi√π piccola, pur mantenendo un livello di precisione comparabile.</p>
<div id="fig-winning-ticket" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-winning-ticket-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_winning_ticket.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-winning-ticket-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.7: Trovare la sottorete del biglietto vincente.
</figcaption>
</figure>
</div>
</section>
<section id="problemi-e-limitazioni" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="problemi-e-limitazioni">Problemi e Limitazioni</h4>
<p>Non c‚Äô√® niente di gratuito con le ottimizzazioni di potatura, con alcune scelte che comportano sia miglioramenti che costi da considerare. Di seguito, discutiamo alcuni compromessi che gli esperti devono considerare.</p>
<ul>
<li><p><strong>Gestione di Matrici di Peso Sparse:</strong> Una matrice di peso sparsa √® una matrice in cui molti degli elementi sono pari a zero. La potatura non strutturata spesso produce matrici di peso sparse, in cui molti pesi vengono potati a zero. Sebbene ci√≤ riduca le dimensioni del modello, introduce anche diversi problemi. L‚Äôinefficienza computazionale pu√≤ sorgere perch√© l‚Äôhardware standard √® ottimizzato per operazioni di matrice densa. Senza ottimizzazioni che sfruttano la sparsit√†, i risparmi computazionali derivanti dalla potatura possono essere persi. Sebbene le matrici sparse possano essere archiviate senza formati specializzati, sfruttare efficacemente la loro sparsit√† richiede una gestione attenta per evitare di sprecare risorse. Algoritmicamente, la navigazione in strutture sparse richiede di saltare in modo efficiente le voci zero, il che aggiunge complessit√† al calcolo e agli aggiornamenti del modello.</p></li>
<li><p><strong>Qualit√† vs.&nbsp;Riduzione delle Dimensioni:</strong> Una sfida fondamentale sia nella potatura strutturata che in quella non-strutturata √® bilanciare la riduzione delle dimensioni con il mantenimento o il miglioramento delle prestazioni predittive. √à essenziale stabilire criteri di potatura robusti, sia per rimuovere intere strutture (potatura strutturata) sia singoli pesi (potatura non strutturata). Questi criteri di potatura scelti devono identificare accuratamente gli elementi la cui rimozione ha un impatto minimo sulle prestazioni. Spesso √® necessaria un‚Äôattenta sperimentazione per garantire che il modello potato rimanga efficiente mantenendo al contempo le sue prestazioni predittive.</p></li>
<li><p><strong>Fine-Tuning e Riaddestramento:</strong> La messa a punto post-potatura √® fondamentale sia nella potatura strutturata che in quella non-strutturata per recuperare le prestazioni perse e stabilizzare il modello. La sfida comprende la determinazione dell‚Äôestensione, della durata e della natura del processo di messa a punto, che pu√≤ essere influenzato dal metodo di potatura e dal grado di potatura applicato.</p></li>
<li><p><strong>Compatibilit√† ed Efficienza Hardware:</strong> Particolarmente pertinenti alla potatura non-strutturata, la compatibilit√† e l‚Äôefficienza hardware diventano critiche. La potatura non strutturata spesso si traduce in matrici di peso sparse, che potrebbero non essere gestite in modo efficiente da un certo hardware, annullando potenzialmente i vantaggi computazionali della potatura (vedere <a href="#fig-sparse-matrix" class="quarto-xref">Figura&nbsp;<span>9.8</span></a>). Garantire che i modelli potati, in particolare quelli risultanti dall‚Äôeliminazione non-strutturata, siano scalabili, compatibili ed efficienti sull‚Äôhardware target √® una considerazione importante.</p></li>
<li><p><strong>Considerazioni Legali ed Etiche:</strong> Ultimo ma non meno importante, il rispetto delle linee guida legali ed etiche √® importante, soprattutto in ambiti con conseguenze significative. I metodi di potatura devono essere sottoposti a rigorosi processi di validazione, test e potenzialmente certificazione per garantire la conformit√† alle normative e agli standard pertinenti, sebbene al momento non esistano standard formali e ‚Äúbest practice‚Äù che siano esaminati e convalidati da entit√† terze. Ci√≤ √® particolarmente cruciale in applicazioni ad alto rischio come l‚Äôintelligenza artificiale medica e la guida autonoma, dove i cali di qualit√† dovuti a ottimizzazioni simili alla potatura possono essere pericolosi per la vita. Inoltre, le considerazioni etiche si estendono oltre la sicurezza fino all‚Äôequit√† e all‚Äôuguaglianza; un recente lavoro di <span class="citation" data-cites="tran2022pruning">(<a href="../../references.it.html#ref-tran2022pruning" role="doc-biblioref">Tran et al. 2022</a>)</span> ha rivelato che la potatura pu√≤ avere un impatto sproporzionato sulle persone di colore, sottolineando la necessit√† di una valutazione etica completa nel processo di potatura.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-tran2022pruning" class="csl-entry" role="listitem">
Tran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, e Rakshit Naidu. 2022. <span>¬´Pruning has a disparate impact on model accuracy¬ª</span>. <em>Adv Neural Inf Process Syst</em> 35: 17652‚Äì64.
</div></div><div id="fig-sparse-matrix" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sparse-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_sprase_matrix.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sparse-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.8: Matrice dei pesi sparsi.
</figcaption>
</figure>
</div>
<div id="exr-p" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;9.1: Pruning
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Si immagini che la rete neurale sia un cespuglio gigante e troppo cresciuto. La potatura √® come tagliare strategicamente i rami per renderla pi√π forte ed efficiente! Nel Colab, si imparer√† come fare questa potatura in TensorFlow. La comprensione di questi concetti fornir√† le basi per vedere come la potatura rende i modelli abbastanza piccoli da poter essere eseguiti sul telefono!</p>
<p><a href="https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="compressione-del-modello" class="level3 page-columns page-full" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="compressione-del-modello"><span class="header-section-number">9.2.2</span> Compressione del Modello</h3>
<p>Le tecniche di compressione del modello sono fondamentali per distribuire modelli di deep learning su dispositivi con risorse limitate. Queste tecniche mirano a creare modelli pi√π piccoli ed efficienti che preservino le prestazioni predittive dei modelli originali.</p>
<section id="sec-kd" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-kd">Distillazione della Conoscenza</h4>
<p>Una tecnica popolare √® la <strong>knowledge distillation (KD)</strong> <a href="#sec-kd">distillazione della conoscenza</a>, che trasferisce la conoscenza da un modello ‚Äúinsegnante‚Äù ampio e complesso a un modello ‚Äústudente‚Äù pi√π piccolo. L‚Äôidea chiave √® addestrare il modello studente a imitare gli output dell‚Äôinsegnante. Il concetto di KD √® stato reso popolare per la prima volta da <span class="citation" data-cites="hinton2015distilling">Hinton (<a href="../../references.it.html#ref-hinton2015distilling" role="doc-biblioref">2005</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hinton2015distilling" class="csl-entry" role="listitem">
Hinton, Geoffrey. 2005. <span>¬´Van <span>Nostrand‚Äôs</span> Scientific Encyclopedia¬ª</span>. Wiley. <a href="https://doi.org/10.1002/0471743984.vse0673">https://doi.org/10.1002/0471743984.vse0673</a>.
</div></div><section id="panoramica-e-vantaggi" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="panoramica-e-vantaggi">Panoramica e Vantaggi</h5>
<p>La distillazione della conoscenza implica il trasferimento della conoscenza da un modello insegnante ampio e complesso a un modello studente pi√π piccolo. L‚Äôidea di base √® quella di utilizzare gli output dell‚Äôinsegnante, noti come <strong>soft targets</strong>, per guidare il training del modello studente. A differenza dei tradizionali ‚Äúhard targets‚Äù (le vere etichette), quelli soft sono le distribuzioni di probabilit√† sulle classi che il modello insegnante prevede. Queste distribuzioni forniscono informazioni pi√π complete sulle relazioni tra le classi, il che pu√≤ aiutare il modello studente ad apprendere in modo pi√π efficace.</p>
<p>Abbiamo imparato che la funzione softmax converte gli output grezzi di un modello in una distribuzione di probabilit√† sulle classi. Una tecnica chiave in KD √® la <strong>scalatura della temperatura</strong>, che viene applicata alla funzione softmax degli output del modello insegnante. Introducendo un parametro di temperatura, la distribuzione pu√≤ essere regolata: una temperatura pi√π alta produce probabilit√† pi√π soft, il che significa che le differenze tra le probabilit√† di classe diventano meno estreme. Questo effetto di ammorbidimento determina una distribuzione pi√π uniforme, in cui la fiducia del modello nella classe pi√π probabile √® ridotta e altre classi hanno probabilit√† pi√π elevate, diverse da zero. Ci√≤ √® prezioso per il modello studente perch√© gli consente di apprendere non solo dalla classe pi√π probabile, ma anche dalle probabilit√† relative di tutte le classi, catturando pattern sottili che potrebbero essere persi se addestrati solo su obiettivi difficili. Pertanto, la scalabilit√† della temperatura facilita il trasferimento di conoscenze pi√π sfumate dal modello insegnante a quello studente.</p>
<p>La funzione di perdita nella distillazione della conoscenza in genere combina due componenti: una perdita di distillazione e una perdita di classificazione. La perdita di distillazione, spesso calcolata utilizzando la divergenza di Kullback-Leibler (KL), misura la differenza tra gli soft target prodotti dal modello insegnante e gli output del modello studente, incoraggiando lo studente a imitare le previsioni dell‚Äôinsegnante. Nel frattempo, la perdita di classificazione assicura che il modello studente preveda correttamente le etichette vere in base ai dati originali. Insieme, queste due componenti aiutano lo studente modello a conservare le conoscenze dell‚Äôinsegnante, rispettando al contempo le etichette di verit√† di base.</p>
<p>Questi componenti, quando configurati e armonizzati abilmente, consentono al modello studente di assimilare la conoscenza del modello insegnante, creando un percorso verso modelli pi√π piccoli, efficienti e robusti, che mantengono la capacit√† predittiva delle loro controparti pi√π grandi. <a href="#fig-knowledge-distillation" class="quarto-xref">Figura&nbsp;<span>9.9</span></a> visualizza la procedura di training della ‚Äúknowledge distillation‚Äù. Notare come i logit o le soft label del modello insegnante vengono utilizzati per fornire una perdita di distillazione da cui il modello studente pu√≤ imparare.</p>
<div id="fig-knowledge-distillation" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-knowledge-distillation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_knowledge_distillation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knowledge-distillation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.9: Processo di training della distillazione della conoscenza. Fonte: <span class="citation" data-cites="intellabs2023knowledge">IntelLabs (<a href="../../references.it.html#ref-intellabs2023knowledge" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-intellabs2023knowledge" class="csl-entry" role="listitem">
IntelLabs. 2023. <span>¬´Knowledge Distillation - Neural Network Distiller¬ª</span>. <a href="https://intellabs.github.io/distiller/knowledge_distillation.html">https://intellabs.github.io/distiller/knowledge_distillation.html</a>.
</div></div></figure>
</div>
</section>
<section id="sfide" class="level5">
<h5 class="anchored" data-anchor-id="sfide">Sfide</h5>
<p>Tuttavia, KD presenta una serie unica di sfide e considerazioni che ricercatori e professionisti devono affrontare attentamente. Una delle sfide √® nella messa a punto meticolosa degli iperparametri, come il parametro ‚Äútemperatura‚Äù nella funzione softmax e la ponderazione tra la distillazione e la perdita di classificazione nella funzione obiettivo. Raggiungere un equilibrio che sfrutti efficacemente gli output ammorbiditi del modello insegnante mantenendo al contempo la fedelt√† alle etichette dei dati reali non √® banale e pu√≤ avere un impatto significativo sulle prestazioni e sulle capacit√† di generalizzazione del modello studente.</p>
<p>Inoltre, l‚Äôarchitettura del modello studente stesso pone un problema considerevole. Progettare un modello compatto per soddisfare i vincoli di calcolo e memoria, pur essendo in grado di assimilare le conoscenze essenziali dal modello insegnante, richiede una comprensione sfumata della capacit√† del modello e dei compromessi intrinseci coinvolti nella compressione. Il modello studente deve essere attentamente progettato per navigare nella dicotomia di dimensioni e prestazioni, assicurando che la conoscenza distillata venga catturata e utilizzata in modo significativo. Inoltre, la scelta del modello dell‚Äôinsegnante, che influenza intrinsecamente la qualit√† e la natura della conoscenza da trasferire, √® importante e introduce un ulteriore livello di complessit√† nel processo KD.</p>
<p>Queste sfide sottolineano la necessit√† di un approccio completo e sfumato all‚Äôimplementazione di KD, assicurando che i modelli degli studenti risultanti siano sia efficienti che efficaci nei loro contesti operativi.</p>
</section>
</section>
<section id="fattorizzazione-di-matrici-di-basso-rango" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="fattorizzazione-di-matrici-di-basso-rango">Fattorizzazione di Matrici di Basso Rango</h4>
<p>Simile nel tema dell‚Äôapprossimazione, la Low-Rank Matrix Factorization (LRMF) <a href="#fattorizzazione-di-matrici-di-basso-rango">fattorizzazione di matrici di basso rango</a> √® una tecnica matematica utilizzata in algebra lineare e analisi dei dati per approssimare una matrice data scomponendola in due o pi√π matrici di dimensione inferiore. L‚Äôidea fondamentale √® di esprimere una matrice di grandi dimensioni come prodotto di matrici di rango inferiore, il che pu√≤ aiutare a ridurre la complessit√† dei dati preservandone la struttura essenziale. Matematicamente, data una matrice <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, LRMF cercare le matrici <span class="math inline">\(U \in \mathbb{R}^{m \times k}\)</span> e <span class="math inline">\(V \in \mathbb{R}^{k \times n}\)</span> tali che <span class="math inline">\(A \approx UV\)</span>, dove <span class="math inline">\(k\)</span> √® il rango ed √® in genere molto pi√π piccolo di <span class="math inline">\(m\)</span> e <span class="math inline">\(n\)</span>.</p>
<section id="background-e-benefici" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="background-e-benefici">Background e Benefici</h5>
<p>Uno dei primo lavori nel campo della fattorizzazione di matrici, in particolare nel contesto dei sistemi di raccomandazione, √® il documento di <span class="citation" data-cites="koren2009matrix">Koren, Bell, e Volinsky (<a href="../../references.it.html#ref-koren2009matrix" role="doc-biblioref">2009</a>)</span>. Gli autori esaminano vari modelli di fattorizzazione, fornendo approfondimenti sulla loro efficacia nel catturare i pattern sottostanti nei dati e nel migliorare l‚Äôaccuratezza predittiva nel filtraggio collaborativo. LRMF √® stato ampiamente applicato nei sistemi di raccomandazione (come Netflix, Facebook, ecc.), dove la matrice di interazione utente-elemento √® fattorizzata per catturare fattori latenti corrispondenti alle preferenze dell‚Äôutente e agli attributi dell‚Äôelemento.</p>
<div class="no-row-height column-margin column-container"><div id="ref-koren2009matrix" class="csl-entry" role="listitem">
Koren, Yehuda, Robert Bell, e Chris Volinsky. 2009. <span>¬´Matrix Factorization Techniques for Recommender Systems¬ª</span>. <em>Computer</em> 42 (8): 30‚Äì37. <a href="https://doi.org/10.1109/mc.2009.263">https://doi.org/10.1109/mc.2009.263</a>.
</div></div><p>Il vantaggio principale della ‚Äúfattorizzazione di matrici di basso rango‚Äù risiede nella sua capacit√† di ridurre la dimensionalit√† dei dati come mostrato in <a href="#fig-matrix-factorization" class="quarto-xref">Figura&nbsp;<span>9.10</span></a>, dove ci sono meno parametri da memorizzare, rendendola pi√π efficiente dal punto di vista computazionale e riducendo i requisiti di archiviazione a costo di un po‚Äô di elaborazione aggiuntiva. Ci√≤ pu√≤ portare a calcoli pi√π rapidi e rappresentazioni di dati pi√π compatte, il che √® particolarmente prezioso quando si ha a che fare con grandi set di dati. Inoltre, pu√≤ aiutare nella riduzione del rumore e pu√≤ rivelare pattern e relazioni sottostanti nei dati.</p>
<p><a href="#fig-matrix-factorization" class="quarto-xref">Figura&nbsp;<span>9.10</span></a> illustra la diminuzione della parametrizzazione abilitata dalla fattorizzazione di matrici di basso rango. Osservare come la matrice <span class="math inline">\(M\)</span> pu√≤ essere approssimata dal prodotto delle matrici <span class="math inline">\(L_k\)</span> e <span class="math inline">\(R_k^T\)</span>. Per intuizione, la maggior parte dei layer completamente connessi nelle reti sono archiviati come matrice di proiezione <span class="math inline">\(M\)</span>, che richiede il caricamento di <span class="math inline">\(m \times n\)</span> parametri durante il calcolo. Tuttavia, scomponendola e approssimandola come prodotto di due matrici di rango inferiore, abbiamo bisogno di archiviare solo <span class="math inline">\(m \times k + k\times n\)</span> parametri in termini di archiviazione, sostenendo al contempo un costo di calcolo aggiuntivo per la moltiplicazione delle matrici. Finch√© <span class="math inline">\(k &lt; n/2\)</span>, questa fattorizzazione ha meno parametri totali da archiviare, aggiungendo un calcolo di runtime <span class="math inline">\(O(mkn)\)</span> <span class="citation" data-cites="gu2023deep">(<a href="../../references.it.html#ref-gu2023deep" role="doc-biblioref">Gu 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gu2023deep" class="csl-entry" role="listitem">
Gu, Ivy. 2023. <span>¬´Deep Learning Model Compression (ii) by Ivy Gu Medium¬ª</span>. <a href="https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453">https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453</a>.
</div></div><div id="fig-matrix-factorization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-matrix-factorization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_low_rank_matrix_factorization.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-matrix-factorization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.10: Fattorizzazione di matrici di basso rango. Fonte: <a href="https://dustinstansbury.github.io/theclevermachine/svd-data-compression">The Clever Machine.</a>
</figcaption>
</figure>
</div>
</section>
<section id="sfide-1" class="level5">
<h5 class="anchored" data-anchor-id="sfide-1">Sfide</h5>
<p>Ma professionisti e ricercatori incontrano una serie di problemi e considerazioni che richiedono una particolare attenzione e approcci strategici. Come con qualsiasi tecnica di compressione lossy [con perdita], potremmo perdere informazioni durante questo processo di approssimazione: scegliere il rango corretto che bilanci le informazioni perse e i costi computazionali √® altrettanto complicato e aggiunge un ulteriore iperparametro da regolare.</p>
<p>La fattorizzazione di matrici di basso rango √® uno strumento prezioso per la riduzione della dimensionalit√† e per adattare il calcolo ai dispositivi edge ma, come altre tecniche, deve essere attentamente regolata in base al modello e all‚Äôattivit√† da svolgere. Una sfida fondamentale risiede nella gestione della complessit√† computazionale inerente a LRMF, soprattutto quando si hanno a che fare con dati ad alta dimensionalit√† e su larga scala. L‚Äôonere computazionale, in particolare nel contesto di applicazioni in tempo reale e set di dati massicci, rimane un ostacolo significativo per un utilizzo efficace di LRMF.</p>
<p>Inoltre, l‚Äôenigma della scelta del rango ottimale <span class="math inline">\(k\)</span> per la fattorizzazione introduce un ulteriore livello di complessit√†. La selezione di <span class="math inline">\(k\)</span> implica intrinsecamente un compromesso tra accuratezza dell‚Äôapprossimazione e semplicit√† del modello, e l‚Äôidentificazione di un rango che bilanci abilmente questi obiettivi contrastanti spesso richiede una combinazione di competenza di dominio, convalida empirica e, a volte, approcci euristici. La sfida √® ulteriormente amplificata quando i dati comprendono rumore o quando la struttura intrinseca di basso rango non √® pronunciata, rendendo la determinazione di un <span class="math inline">\(k\)</span> adatto ancora pi√π sfuggente.</p>
<p>La gestione di dati mancanti o sparsi, un evento comune in applicazioni come i sistemi di raccomandazione, pone un‚Äôaltra sfida sostanziale. Le tecniche tradizionali di fattorizzazione delle matrici, come la Singular Value Decomposition (SVD), non sono direttamente applicabili alle matrici con voci mancanti, rendendo necessario lo sviluppo e l‚Äôapplicazione di algoritmi specializzati in grado di fattorizzare matrici incomplete mitigando al contempo i rischi di overfitting alle voci osservate. Ci√≤ spesso comporta l‚Äôincorporazione di termini di regolarizzazione o la limitazione della fattorizzazione in modi specifici, il che a sua volta introduce ulteriori iperparametri che devono essere selezionati giudiziosamente.</p>
<p>Inoltre, in scenari in cui i dati evolvono o crescono nel tempo, sviluppare modelli LRMF in grado di adattarsi a nuovi dati senza richiedere una completa rifattorizzazione √® un‚Äôimpresa critica ma impegnativa. Gli algoritmi di fattorizzazione di matrici incrementali e online cercano di risolvere questo problema consentendo l‚Äôaggiornamento delle matrici fattorizzate all‚Äôarrivo di nuovi dati, ma garantire stabilit√†, accuratezza ed efficienza computazionale in queste impostazioni dinamiche rimane un compito intricato. Ci√≤ √® particolarmente impegnativo nello spazio di TinyML, in cui la ridistribuzione dei rami per i modelli aggiornati pu√≤ essere piuttosto impegnativa.</p>
</section>
</section>
<section id="decomposizione-dei-tensori" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="decomposizione-dei-tensori">Decomposizione dei Tensori</h4>
<p>Abbiamo visto in <a href="../frameworks/frameworks.it.html#sec-tensor-data-structures" class="quarto-xref"><span>Sezione 6.4.1</span></a> che i tensori sono strutture flessibili, comunemente utilizzate dai framework ML, che possono rappresentare dati in dimensioni superiori. Similmente alla fattorizzazione di matrici di basso rango, i modelli pi√π complessi possono memorizzare pesi in dimensioni superiori, come i tensori. La decomposizione tensoriale √® l‚Äôanalogo di dimensioni superiori della fattorizzazione di matrici, in cui un tensore modello viene scomposto in componenti di rango inferiore (cfr. <a href="#fig-tensor-decomposition" class="quarto-xref">Figura&nbsp;<span>9.11</span></a>). Questi componenti di rango inferiore sono pi√π facili da calcolare e memorizzare, ma possono soffrire degli stessi problemi menzionati sopra, come la perdita di informazioni e la necessit√† di una messa a punto sfumata degli iperparametri. Matematicamente, dato un tensore <span class="math inline">\(\mathcal{A}\)</span>, la decomposizione tensoriale cerca di rappresentare <span class="math inline">\(\mathcal{A}\)</span> come una combinazione di tensori pi√π semplici, facilitando una rappresentazione compressa che approssima i dati originali riducendo al minimo la perdita di informazioni.</p>
<p>Il lavoro di Tamara G. Kolda e Brett W. Bader, <a href="https://epubs.siam.org/doi/abs/10.1137/07070111X">‚ÄúTensor Decompositions and Applications‚Äù</a> (2009), si distingue come un articolo fondamentale nel campo delle decomposizioni tensoriali. Gli autori forniscono una panoramica completa di vari metodi di decomposizione tensoriale, esplorandone i fondamenti matematici, gli algoritmi e un‚Äôampia gamma di applicazioni, che vanno dall‚Äôelaborazione del segnale al data mining. Naturalmente, il motivo per cui ne stiamo discutendo √® perch√© ha un enorme potenziale per i miglioramenti delle prestazioni del sistema, in particolare nello spazio di TinyML, dove la produttivit√† e i risparmi di memoria sono fondamentali per la fattibilit√† delle distribuzioni.</p>
<div id="fig-tensor-decomposition" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-tensor-decomposition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_tensor_decomposition.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-decomposition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.11: Decomposizione dei Tensori. Fonte: <span class="citation" data-cites="xinyu">Xinyu (<a href="../../references.it.html#ref-xinyu" role="doc-biblioref">s.d.</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-xinyu" class="csl-entry" role="listitem">
Xinyu, Chen. s.d.
</div></div></figure>
</div>
<div id="exr-mc" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;9.2: Compressione di Modelli Scalabili con TensorFlow
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Questo Colab si addentra in una tecnica per comprimere i modelli mantenendo un‚Äôelevata accuratezza. L‚Äôidea chiave √® quella di addestrare un modello con un termine di penalit√† extra che incoraggia il modello a essere pi√π comprimibile. Quindi, il modello viene codificato utilizzando uno schema di codifica speciale che si allinea con questa penalit√†. Questo approccio consente di ottenere modelli compressi che funzionano altrettanto bene dei modelli originali ed √® utile per distribuire modelli su dispositivi con risorse limitate come telefoni cellulari e dispositivi edge.</p>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/optimization/compression.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="modelli-progettati-per-ledge" class="level3 page-columns page-full" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="modelli-progettati-per-ledge"><span class="header-section-number">9.2.3</span> Modelli Progettati per l‚ÄôEdge</h3>
<p>Ora raggiungiamo l‚Äôaltro estremo del gradiente hardware-software, dove prendiamo decisioni specifiche sull‚Äôarchitettura del modello direttamente in base alla conoscenza dei dispositivi edge su cui desideriamo implementare.</p>
<p>Come spiegato nelle sezioni precedenti, i dispositivi edge sono vincolati specificamente da limitazioni di memoria e calcoli parallelizzabili: in quanto tali, se ci sono requisiti critici di velocit√† di inferenza, i calcoli devono essere sufficientemente flessibili da soddisfare i vincoli hardware, qualcosa che pu√≤ essere progettato a livello di architettura del modello. Inoltre, cercare di stipare grandi modelli SOTA ML su dispositivi edge anche dopo potatura e compressione √® generalmente irrealizzabile puramente a causa delle dimensioni: la complessit√† del modello stesso deve essere scelta con pi√π sfumature per adattarsi pi√π fattibilmente al dispositivo. Gli sviluppatori di Edge ML hanno affrontato questa sfida architettonica sia attraverso la progettazione di architetture di modelli edge ML su misura sia attraverso la Neural Architecture Search (NAS) [ricerca di architettura neurale] avente il dispositivo come target, che pu√≤ generare in modo pi√π sistematico architetture fattibili di modelli su dispositivo.</p>
<section id="tecniche-di-progettazione-del-modello" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="tecniche-di-progettazione-del-modello">Tecniche di Progettazione del Modello</h4>
<p>Un design di architettura edge friendly, comunemente utilizzato nel deep learning per l‚Äôelaborazione delle immagini, √® quello delle convoluzioni separabili in profondit√†. Consiste in due fasi distinte: la prima √® la convoluzione in profondit√†, in cui ogni canale di input viene convoluto in modo indipendente con il proprio set di filtri apprendibili, come mostrato in <a href="#fig-depthwise-convolution" class="quarto-xref">Figura&nbsp;<span>9.12</span></a>. Questa fase riduce la complessit√† computazionale in modo significativo rispetto alle convoluzioni standard, poich√© riduce drasticamente il numero di parametri e calcoli coinvolti. La seconda fase √® la convoluzione puntuale, che combina l‚Äôoutput dei canali di convoluzione in profondit√† tramite una convoluzione 1x1, creando interazioni tra canali. Questo approccio offre diversi vantaggi. I vantaggi includono dimensioni ridotte del modello, tempi di inferenza pi√π rapidi e spesso una migliore generalizzazione grazie al minor numero di parametri, rendendolo adatto ad applicazioni mobili ed embedded. Tuttavia, le convoluzioni separabili in profondit√† potrebbero non catturare interazioni spaziali complesse in modo efficace come le convoluzioni standard e potrebbero richiedere pi√π profondit√† (livelli) per raggiungere lo stesso livello di potenza rappresentativa, portando potenzialmente a tempi di addestramento pi√π lunghi. Tuttavia, la loro efficienza in termini di parametri e calcolo le rende una scelta popolare nelle moderne architetture di reti neurali convoluzionali.</p>
<div id="fig-depthwise-convolution" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-depthwise-convolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_depthwise_separable_convolution.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-depthwise-convolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.12: Convoluzioni separabili in profondit√†. Fonte: <span class="citation" data-cites="hegde2023introduction">Hegde (<a href="../../references.it.html#ref-hegde2023introduction" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-hegde2023introduction" class="csl-entry" role="listitem">
Hegde, Sumant. 2023. <span>¬´An Introduction to Separable Convolutions - Analytics Vidhya¬ª</span>. <a href="https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/">https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/</a>.
</div></div></figure>
</div>
</section>
<section id="architetture-di-modello-di-esempio" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="architetture-di-modello-di-esempio">Architetture di Modello di Esempio</h4>
<p>In quest‚Äôottica, diverse architetture recenti sono state, fin dall‚Äôinizio, progettate specificamente per massimizzare la precisione in un‚Äôimplementazione edge, in particolare SqueezeNet, MobileNet ed EfficientNet.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/1602.07360">SqueezeNet</a> di <span class="citation" data-cites="iandola2016squeezenet">Iandola et al. (<a href="../../references.it.html#ref-iandola2016squeezenet" role="doc-biblioref">2016</a>)</span>, ad esempio, utilizza un‚Äôarchitettura compatta con convoluzioni 1x1 e moduli ‚Äúfire‚Äù per ridurre al minimo il numero di parametri mantenendo al contempo una forte accuratezza.</p></li>
<li><p><a href="https://arxiv.org/abs/1704.04861">MobileNet</a> di <span class="citation" data-cites="howard2017mobilenets">Howard et al. (<a href="../../references.it.html#ref-howard2017mobilenets" role="doc-biblioref">2017</a>)</span>, d‚Äôaltra parte, impiega le suddette convoluzioni separabili in profondit√† per ridurre sia il calcolo che le dimensioni del modello.</p></li>
<li><p><a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> di <span class="citation" data-cites="tan2020efficientnet">Tan e Le (<a href="../../references.it.html#ref-tan2020efficientnet" role="doc-biblioref">2023</a>)</span> adotta un approccio diverso ottimizzando il ridimensionamento della rete (ovvero variando la profondit√†, la larghezza e la risoluzione di una rete) e il ridimensionamento composto, una variazione pi√π sfumata del ridimensionamento della rete, per ottenere prestazioni superiori con meno parametri.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-iandola2016squeezenet" class="csl-entry" role="listitem">
Iandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. <span>¬´<span>SqueezeNet:</span> <span>Alexnet-level</span> accuracy with 50x fewer parameters and 0.5 <span>MB</span> model size¬ª</span>. <em>ArXiv preprint</em> abs/1602.07360. <a href="https://arxiv.org/abs/1602.07360">https://arxiv.org/abs/1602.07360</a>.
</div><div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. <span>¬´<span>MobileNets:</span> <span>Efficient</span> Convolutional Neural Networks for Mobile Vision Applications¬ª</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a>.
</div><div id="ref-tan2020efficientnet" class="csl-entry" role="listitem">
Tan, Mingxing, e Quoc V. Le. 2023. <span>¬´Demystifying Deep Learning¬ª</span>. Wiley. <a href="https://doi.org/10.1002/9781394205639.ch6">https://doi.org/10.1002/9781394205639.ch6</a>.
</div></div><p>Questi modelli sono essenziali nel contesto dell‚Äôedge computing in cui la limitazione di potenza di elaborazione e di memoria richiede modelli leggeri ma efficaci in grado di eseguire in modo efficiente attivit√† quali il riconoscimento delle immagini, il rilevamento di oggetti e altro ancora. I loro principi di progettazione mostrano l‚Äôimportanza di un‚Äôarchitettura di modelli intenzionalmente personalizzata per l‚Äôedge computing, in cui prestazioni ed efficienza devono rientrare nei vincoli.</p>
</section>
<section id="semplificazione-della-ricerca-di-architetture-di-modelli" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="semplificazione-della-ricerca-di-architetture-di-modelli">Semplificazione della Ricerca di Architetture di Modelli</h4>
<p>Infine, per affrontare la sfida di trovare architetture di modelli efficienti che siano compatibili con i dispositivi edge, i ricercatori hanno sviluppato pipeline sistematizzate che semplificano la ricerca di progetti performanti. Due framework degni di nota in questo spazio sono <a href="https://arxiv.org/abs/2007.10319">TinyNAS</a> di <span class="citation" data-cites="lin2020mcunet">J. Lin et al. (<a href="../../references.it.html#ref-lin2020mcunet" role="doc-biblioref">2020</a>)</span> e <a href="https://arxiv.org/abs/1711.06798">MorphNet</a> di <span class="citation" data-cites="gordon2018morphnet">Gordon et al. (<a href="../../references.it.html#ref-gordon2018morphnet" role="doc-biblioref">2018</a>)</span>, che automatizzano il processo di ottimizzazione delle architetture di reti neurali per l‚Äôimplementazione edge.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gordon2018morphnet" class="csl-entry" role="listitem">
Gordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, e Edward Choi. 2018. <span>¬´<span>MorphNet:</span> <span>Fast</span> &amp;amp; Simple Resource-Constrained Structure Learning of Deep Networks¬ª</span>. In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 1586‚Äì95. IEEE. <a href="https://doi.org/10.1109/cvpr.2018.00171">https://doi.org/10.1109/cvpr.2018.00171</a>.
</div></div><p>TinyNAS √® un innovativo framework di ricerca di architetture neurali introdotto nel documento MCUNet, progettato per scoprire in modo efficiente architetture di reti neurali leggere per dispositivi edge con risorse computazionali limitate. Sfruttando l‚Äôapprendimento per rinforzo e uno spazio di ricerca compatto di micromoduli neurali, TinyNAS ottimizza sia l‚Äôaccuratezza che la latenza, consentendo l‚Äôimplementazione di modelli di deep learning su microcontrollori, dispositivi IoT e altre piattaforme con risorse limitate. Nello specifico, TinyNAS, in combinazione con un ottimizzatore di rete, TinyEngine, genera diversi spazi di ricerca ridimensionando la risoluzione di input e la larghezza del modello, poi raccoglie la distribuzione FLOP di calcolo delle reti soddisfacenti all‚Äôinterno dello spazio di ricerca per valutarne la priorit√†. TinyNAS si basa sul presupposto che uno spazio di ricerca che ospita FLOP pi√π elevati con vincoli di memoria possa produrre modelli di accuratezza pi√π elevata, cosa che gli autori hanno verificato in pratica nel loro lavoro. In termini di prestazioni empiriche, TinyEngine ha ridotto l‚Äôutilizzo di memoria di picco dei modelli di circa 3.4 volte e ha accelerato l‚Äôinferenza da 1.7 a 3.3 volte rispetto a <a href="https://www.tensorflow.org/lite">TFLite</a> e a <a href="https://www.keil.com/pack/doc/CMSIS/NN/html/index.html">CMSIS-NN</a>.</p>
<p>Analogamente, MorphNet √® un framework di ottimizzazione delle reti neurali progettato per rimodellare e trasformare automaticamente l‚Äôarchitettura delle reti neurali profonde, ottimizzandole per requisiti di distribuzione specifici. Ci√≤ avviene in due fasi: in primo luogo, sfrutta un set di operazioni di morphing della rete personalizzabili, come l‚Äôampliamento o l‚Äôapprofondimento dei layer, per regolare dinamicamente la struttura della rete. Queste operazioni consentono alla rete di adattarsi a vari vincoli computazionali, tra cui dimensioni del modello, latenza e obiettivi di accuratezza, che sono estremamente diffusi nell‚Äôutilizzo dell‚Äôedge computing. Nella seconda fase, MorphNet utilizza un approccio basato sull‚Äôapprendimento di rinforzo per cercare la permutazione ottimale delle operazioni di morphing, bilanciando efficacemente il compromesso tra dimensioni del modello e prestazioni. Questo metodo innovativo consente ai professionisti del deep learning di adattare automaticamente le architetture delle reti neurali a requisiti hardware e applicativi specifici, garantendo un‚Äôimplementazione efficiente ed efficace su diverse piattaforme.</p>
<p>TinyNAS e MorphNet rappresentano alcuni dei numerosi progressi significativi nel campo dell‚Äôottimizzazione sistematica delle reti neurali, consentendo di scegliere e generare sistematicamente architetture per adattarsi perfettamente ai vincoli del problema.</p>
<div id="exr-md" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;9.3: Modelli Progettati per l‚ÄôEdge
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Si Immagini di costruire un piccolo robot in grado di identificare diversi fiori. Deve essere intelligente, ma anche piccolo ed efficiente dal punto di vista energetico! Nel mondo dei ‚ÄúModelli Progettati per l‚ÄôEdge‚Äù, abbiamo appreso tecniche come le convoluzioni separabili in profondit√† e architetture come SqueezeNet, MobileNet ed EfficientNet, tutte progettate per concentrare l‚Äôintelligenza in modelli compatti. Ora, vediamo queste idee in azione con alcuni xColab:</p>
<p><strong>SqueezeNet in Action:</strong> Forse piacerebbe un Colab che mostra come addestrare un modello SqueezeNet su un set di dati di immagini di fiori. Ci√≤ dimostrerebbe le sue piccole dimensioni e come impara a riconoscere i pattern nonostante la sua efficienza.</p>
<p><a href="https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_squeezenet.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
<p><strong>MobileNet Exploration:</strong> Ci si √® mai chiesto se quei piccoli modelli di immagini sono buoni quanto quelli grandi? Scopriamolo! In questo Colab, mettiamo a confronto MobileNet, il campione dei pesi leggeri, con un modello di classificazione delle immagini classico. Li faremo gareggiare per la velocit√†, misureremo le loro esigenze di memoria e vedremo chi vincer√† per accuratezza. Preparatevi per una battaglia di cervelli di immagini!</p>
<p><a href="https://colab.research.google.com/drive/1bOzVaDQo8h6Ngstb7AcfzC35OihpHspt"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-model_ops_numerics" class="level2 page-columns page-full" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="sec-model_ops_numerics"><span class="header-section-number">9.3</span> Rappresentazione Numerica Efficiente</h2>
<p>La rappresentazione numerica implica una miriade di considerazioni, tra cui, ma non solo, la precisione dei numeri, i loro formati di codifica e le operazioni aritmetiche facilitate. Implica invariabilmente una vasta gamma di diversi compromessi, in cui i professionisti sono incaricati di destreggiarsi tra accuratezza numerica ed efficienza computazionale. Ad esempio, mentre i numeri a bassa precisione possono offrire il fascino di un utilizzo di memoria ridotto e calcoli accelerati, presentano contemporaneamente sfide relative alla stabilit√† numerica e al potenziale degrado dell‚Äôaccuratezza del modello.</p>
<section id="motivazione" class="level4">
<h4 class="anchored" data-anchor-id="motivazione">Motivazione</h4>
<p>Emerge l‚Äôimperativo per una rappresentazione numerica efficiente, in particolare perch√© l‚Äôottimizzazione efficiente del modello da sola non √® sufficiente quando si adattano i modelli per l‚Äôimplementazione su dispositivi edge a bassa potenza che operano con vincoli rigorosi.</p>
<p>Oltre a ridurre al minimo le richieste di memoria, l‚Äôenorme potenziale di una rappresentazione numerica efficiente risiede, ma non √® limitato a, queste modalit√† fondamentali. Riducendo l‚Äôintensit√† computazionale, la matematica efficiente pu√≤ amplificare la velocit√† computazionale, consentendo di elaborare modelli pi√π complessi su dispositivi a bassa potenza. Ridurre la precisione in bit di pesi e attivazioni su modelli fortemente sovra-parametrizzati consente la condensazione delle dimensioni del modello per dispositivi edge senza danneggiare significativamente l‚Äôaccuratezza predittiva del modello. Con l‚Äôonnipresenza delle reti neurali nei modelli, la matematica efficiente ha un vantaggio unico nello sfruttare la struttura a layer delle NN per variare la precisione numerica tra i layer, riducendo al minimo la precisione nei layer resistenti e preservando una maggiore precisione in quelli sensibili.</p>
<p>In questa sezione, approfondiremo il modo in cui i professionisti possono sfruttare i principi della progettazione congiunta hardware-software ai livelli pi√π bassi di un modello per facilitare la compatibilit√† con i dispositivi edge. Iniziando con un‚Äôintroduzione ai numeri, esamineremo le sue implicazioni per la memoria del dispositivo e la complessit√† computazionale. Successivamente, intraprenderemo una discussione sui compromessi implicati nell‚Äôadozione di questa strategia, seguita da un‚Äôanalisi approfondita di un metodo fondamentale della matematica efficiente: la quantizzazione.</p>
</section>
<section id="le-basi" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="le-basi"><span class="header-section-number">9.3.1</span> Le Basi</h3>
<section id="i-tipi" class="level4">
<h4 class="anchored" data-anchor-id="i-tipi">I Tipi</h4>
<p>I dati numerici, il fondamento su cui si basano i modelli di apprendimento automatico, si manifestano in due forme principali. Si tratta di numeri interi e numeri in virgola mobile.</p>
<p><strong>Numeri Interi:</strong> Numeri interi, privi di componenti frazionarie, (ad esempio, -3, 0, 42) sono fondamentali negli scenari che richiedono valori discreti. Ad esempio, in ML, le etichette di classe in un‚Äôattivit√† di classificazione potrebbero essere rappresentate come numeri interi, dove ‚Äúgatto‚Äù, ‚Äúcane‚Äù e ‚Äúuccello‚Äù potrebbero essere codificati rispettivamente come 0, 1 e 2.</p>
<p><strong>Numeri in virgola mobile:</strong> Comprendendo numeri reali, (ad esempio, -3.14, 0.01, 2.71828) consentono la rappresentazione di valori con componenti frazionarie. Nei parametri del modello ML, i pesi potrebbero essere inizializzati con piccoli valori a virgola mobile, ad esempio 0.001 o -0.045, per avviare il processo di training. Attualmente, ci sono 4 popolari formati di precisione discussi di seguito.</p>
<p><strong>Larghezze di bit variabili:</strong> Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezza di bit estremamente ridotta possono offrire accelerazioni significative e ridurre ulteriormente il consumo energetico. Sebbene permangano dei problemi nel mantenere l‚Äôaccuratezza del modello con una quantizzazione cos√¨ drastica, si continuano a fare progressi in quest‚Äôarea.</p>
</section>
<section id="precisione" class="level4">
<h4 class="anchored" data-anchor-id="precisione">Precisione</h4>
<p>La precisione, che delinea l‚Äôesattezza con cui un numero √® rappresentato, si biforca tipicamente in singola, doppia, mezza e negli ultimi anni sono emerse numerose altre precisioni per supportare meglio e in modo efficiente le attivit√† di apprendimento automatico sull‚Äôhardware sottostante.</p>
<p><strong>Doppia precisione (Float64):</strong> Allocando 64 bit, la doppia precisione (ad esempio, 3.141592653589793) fornisce una precisione elevata, sebbene richieda pi√π memoria e pi√π risorse di calcolo. Nei calcoli scientifici, dove la precisione √® fondamentale, variabili come œÄ potrebbero essere rappresentate con Float64.</p>
<p><strong>Singola precisione (Float32):</strong> Con 32 bit a disposizione, la singola precisione (ad esempio, 3.1415927) raggiunge un equilibrio tra precisione numerica e risparmio della memoria. In ML, Float32 potrebbe essere impiegato per memorizzare i pesi durante l‚Äôaddestramento per mantenere un livello ragionevole di precisione.</p>
<p><strong>Half Precision (Float16):</strong> Limitata a 16 bit, la half precision (ad esempio, 3.14) riduce l‚Äôutilizzo della memoria e pu√≤ velocizzare i calcoli, sebbene sacrifichi l‚Äôaccuratezza e l‚Äôintervallo numerico. In ML, specialmente durante l‚Äôinferenza su dispositivi con risorse limitate, Float16 potrebbe essere utilizzato per ridurre l‚Äôimpronta di memoria del modello.</p>
<p><strong>Bfloat16:</strong> Brain Floating-Point Format o Bfloat16, impiega anche 16 bit ma li alloca in modo diverso rispetto a FP16: 1 bit per il segno, 8 bit per l‚Äôesponente (che si traduce nello stesso intervallo numerico di float32) e 7 bit per la frazione. Questo formato, sviluppato da Google, d√† priorit√† a un intervallo di esponenti pi√π ampio rispetto alla precisione, rendendolo particolarmente utile nelle applicazioni di apprendimento profondo in cui l‚Äôintervallo dinamico √® cruciale.</p>
<p><a href="#fig-3float" class="quarto-xref">Figura&nbsp;<span>9.13</span></a> illustra le differenze tra i tre formati a virgola mobile: Float32, Float16 e BFloat16.</p>
<div id="fig-3float" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3float-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_3float_types.jpeg" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3float-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.13: Tre formati a virgola mobile.
</figcaption>
</figure>
</div>
<p><strong>Intero:</strong> Le rappresentazioni di numeri interi sono realizzate utilizzando 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocit√† e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attivit√† di inferenza, in particolare su dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione √® spesso accettabile dati i guadagni di efficienza. Una forma estrema di numeri interi √® per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno di due valori: +1 o -1.</p>
<p>√à possibile fare riferimento a <a href="../efficient_ai/efficient_ai.it.html#sec-numerical-formats" class="quarto-xref"><span>Sezione 8.6.1</span></a> per una tabella di confronto tra i compromessi dei diversi tipi numerici.</p>
</section>
<section id="codifica-e-archiviazione-numerica" class="level4">
<h4 class="anchored" data-anchor-id="codifica-e-archiviazione-numerica">Codifica e Archiviazione Numerica</h4>
<p>La codifica numerica, l‚Äôarte di trasformare i numeri in un formato utilizzabile dal computer e la loro successiva memorizzazione sono fondamentali per l‚Äôefficienza computazionale. Ad esempio, i numeri in virgola mobile potrebbero essere codificati utilizzando lo standard IEEE 754, che ripartisce i bit tra i componenti segno, esponente e frazione, consentendo cos√¨ la rappresentazione di una vasta gamma di valori con un singolo formato. Esistono alcuni nuovi formati in virgola mobile IEEE che sono stati definiti specificamente per i carichi di lavoro AI:</p>
<ul>
<li><a href="https://cloud.google.com/tpu/docs/bfloat16">bfloat16</a>- Un formato in virgola mobile a 16 bit introdotto da Google. Ha 8 bit per esponente, 7 bit per mantissa e 1 bit per segno. Offre un compromesso di precisione ridotto tra float a 32 bit e interi a 8 bit. Supportato su molti acceleratori hardware.</li>
<li><a href="https://ieeexplore.ieee.org/document/9399648">posit</a> - Un formato configurabile che pu√≤ rappresentare diversi livelli di precisione in base ai bit esponente. √à pi√π efficiente dei numeri binari in virgola mobile IEEE 754. Ha una gamma dinamica e una precisione regolabili.</li>
<li><a href="https://arxiv.org/abs/1711.02213">Flexpoint</a> - Un formato introdotto da Intel che pu√≤ regolare dinamicamente la precisione tra livelli o all‚Äôinterno di un layer. Consente di adattare la precisione all‚Äôaccuratezza e ai requisiti hardware.</li>
<li><a href="https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/BFMLALB--BFMLALT--vector---BFloat16-floating-point-widening-multiply-add-long--vector--">BF16ALT</a> - Un formato a 16 bit proposto da ARM come alternativa a bfloat16. Utilizza un bit aggiuntivo nell‚Äôesponente per evitare overflow/underflow.</li>
<li><a href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/">TF32</a> - Introdotto da Nvidia per le GPU Ampere. Utilizza 10 bit per l‚Äôesponente invece di 8 bit come FP32. Migliora le prestazioni di training del modello mantenendo l‚Äôaccuratezza.</li>
<li><a href="https://arxiv.org/abs/2209.05433">FP8</a> - Formato a virgola mobile a 8 bit che mantiene 6 bit per la mantissa e 2 bit per l‚Äôesponente. Consente una gamma dinamica migliore rispetto agli interi.</li>
</ul>
<p>Gli obiettivi principali di questi nuovi formati sono di fornire alternative di precisione inferiore ai float a 32 bit per una migliore efficienza computazionale e prestazioni sugli acceleratori AI, mantenendo al contempo l‚Äôaccuratezza del modello. Offrono diversi compromessi in termini di precisione, portata e costo/complessit√† di implementazione.</p>
</section>
</section>
<section id="vantaggi-dellefficienza" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="vantaggi-dellefficienza"><span class="header-section-number">9.3.2</span> Vantaggi dell‚ÄôEfficienza</h3>
<p>Come visto in <a href="../efficient_ai/efficient_ai.it.html#sec-efficiency-benefits" class="quarto-xref"><span>Sezione 8.6.2</span></a>, l‚Äôefficienza numerica √® importante per i carichi di lavoro di apprendimento automatico per una serie di motivi. L‚Äôefficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano pi√π pervasivi, soprattutto in ambienti reali con risorse limitate, l‚Äôattenzione su una numerica efficiente continuer√† a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, √® possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocit√†, memoria ed energia.</p>
</section>
<section id="sfumature-della-rappresentazione-numerica" class="level3 page-columns page-full" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="sfumature-della-rappresentazione-numerica"><span class="header-section-number">9.3.3</span> Sfumature della Rappresentazione Numerica</h3>
<p>Ci sono diverse sfumature con le rappresentazioni numeriche per ML che richiedono di avere una comprensione sia degli aspetti teorici che pratici della rappresentazione numerica, nonch√© una profonda consapevolezza dei requisiti e dei vincoli specifici del dominio applicativo.</p>
<section id="utilizzo-della-memoria" class="level4">
<h4 class="anchored" data-anchor-id="utilizzo-della-memoria">Utilizzo della Memoria</h4>
<p>L‚Äôimpronta di memoria dei modelli ML, in particolare quelli di notevole complessit√† e profondit√†, pu√≤ essere sostanziale, ponendo quindi una sfida significativa sia nelle fasi di training che di deployment. Ad esempio, una rete neurale profonda con 100 milioni di parametri, rappresentata utilizzando Float32 (32 bit o 4 byte per parametro), richiederebbe circa 400 MB di memoria solo per l‚Äôarchiviazione dei pesi del modello. Ci√≤ non tiene conto dei requisiti di memoria aggiuntivi durante il training per l‚Äôarchiviazione di gradienti, stati dell‚Äôottimizzatore e cache di passaggio forward [in avanti], che possono amplificare ulteriormente l‚Äôutilizzo della memoria, potenzialmente mettendo a dura prova le risorse su determinati hardware, in particolare dispositivi edge con capacit√† di memoria limitata.</p>
<p>La scelta della rappresentazione numerica ha un impatto ulteriore sull‚Äôutilizzo della memoria e sull‚Äôefficienza computazionale. Ad esempio, l‚Äôutilizzo di Float64 per i pesi del modello raddoppierebbe i requisiti di memoria rispetto a Float32 e potrebbe potenzialmente aumentare anche il tempo di elaborazione. Per una matrice di peso con dimensioni [1000, 1000], Float64 consumerebbe circa 8 MB di memoria, mentre Float32 la ridurrebbe a circa 4 MB. Pertanto, la selezione di un formato numerico appropriato √® fondamentale per ottimizzare sia la memoria che l‚Äôefficienza computazionale.</p>
</section>
<section id="complessit√†-computazionale" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="complessit√†-computazionale">Complessit√† Computazionale</h4>
<p>La precisione numerica ha un impatto diretto sulla complessit√† computazionale, influenzando il tempo e le risorse necessarie per eseguire operazioni aritmetiche. Ad esempio, le operazioni che utilizzano Float64 generalmente consumano pi√π risorse computazionali rispetto alle loro controparti Float32 o Float16 (vedere <a href="#fig-quantized-energy" class="quarto-xref">Figura&nbsp;<span>9.14</span></a>). Nel regno del ML, dove i modelli potrebbero dover elaborare milioni di operazioni (ad esempio, moltiplicazioni e addizioni in operazioni di matrice durante passaggi in forward e backward), anche piccole differenze nella complessit√† computazionale per operazione possono aggregarsi in un impatto sostanziale sui tempi di training e inferenza. Come mostrato in <a href="#fig-models-speeds" class="quarto-xref">Figura&nbsp;<span>9.15</span></a>, i modelli quantizzati possono essere molte volte pi√π veloci delle loro versioni non-quantizzate.</p>
<div id="fig-quantized-energy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantized-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_horowitz.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.14: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Mark Horowitz, Stanford University.
</figcaption>
</figure>
</div>
<div id="fig-models-speeds" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-models-speeds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_int8vsfloat.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-models-speeds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.15: Velocit√† di tre diversi modelli in forma normale e quantizzata.
</figcaption>
</figure>
</div>
<p>Oltre ai tempi di esecuzione puri, c‚Äô√® anche una preoccupazione per l‚Äôefficienza energetica. Non tutti i calcoli numerici sono creati uguali dal punto di vista dell‚Äôhardware sottostante. Alcune operazioni numeriche sono pi√π efficienti dal punto di vista energetico di altre. Ad esempio, <a href="#fig-operations-energy-comparison" class="quarto-xref">Figura&nbsp;<span>9.16</span></a> di seguito mostra che l‚Äôaddizione di interi √® molto pi√π efficiente dal punto di vista energetico della moltiplicazione di interi.</p>
<div id="fig-operations-energy-comparison" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-operations-energy-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_100x.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-operations-energy-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.16: Utilizzo di energia da parte di operazioni quantizzate. Fonte: <span class="citation" data-cites="isscc2014computings">Isscc (<a href="../../references.it.html#ref-isscc2014computings" role="doc-biblioref">2014</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-isscc2014computings" class="csl-entry" role="listitem">
Isscc. 2014. <span>¬´Computing‚Äôs energy problem (and what we can do about it)¬ª</span>. <a href="https://ieeexplore.ieee.org/document/6757323">https://ieeexplore.ieee.org/document/6757323</a>.
</div></div></figure>
</div>
</section>
<section id="compatibilit√†-hardware" class="level4">
<h4 class="anchored" data-anchor-id="compatibilit√†-hardware">Compatibilit√† Hardware</h4>
<p>Garantire la compatibilit√† e le prestazioni ottimizzate su diverse piattaforme hardware √® un‚Äôaltra sfida nella rappresentazione numerica. Hardware diversi, come CPU, GPU, TPU e FPGA, hanno capacit√† e ottimizzazioni diverse per gestire diverse precisioni numeriche. Ad esempio, alcune GPU potrebbero essere ottimizzate per i calcoli Float32, mentre altre potrebbero fornire accelerazioni per Float16. Sviluppare e ottimizzare modelli ML in grado di sfruttare le capacit√† numeriche specifiche di hardware diversi, garantendo al contempo che il modello mantenga la sua accuratezza e robustezza, richiede un‚Äôattenta considerazione e potenzialmente ulteriori sforzi di sviluppo e test.</p>
</section>
<section id="compromessi-di-precisione-e-accuratezza" class="level4">
<h4 class="anchored" data-anchor-id="compromessi-di-precisione-e-accuratezza">Compromessi di Precisione e Accuratezza</h4>
<p>Il compromesso tra precisione numerica e accuratezza del modello √® una sfida ‚Äúsfumata‚Äù nella rappresentazione numerica. L‚Äôutilizzo di numeri a bassa precisione, come Float16, potrebbe risparmiare memoria e velocizzare i calcoli, ma pu√≤ anche introdurre problemi come errore di quantizzazione e intervallo numerico ridotto. Ad esempio, addestrare un modello con Float16 potrebbe introdurre problemi nella rappresentazione di valori di gradiente molto piccoli, potenzialmente influenzando la convergenza e la stabilit√† del processo di addestramento. Inoltre, in alcune applicazioni, come simulazioni scientifiche o calcoli finanziari, in cui l‚Äôelevata precisione √® fondamentale, l‚Äôuso di numeri a bassa precisione potrebbe non essere consentito a causa del rischio di accumulare errori significativi.</p>
</section>
<section id="esempi-di-compromessi" class="level4">
<h4 class="anchored" data-anchor-id="esempi-di-compromessi">Esempi di Compromessi</h4>
<p>Per comprendere e apprezzare le sfumature, prendiamo in considerazione alcuni esempi di casi d‚Äôuso. Attraverso questi, ci renderemo conto che la scelta della rappresentazione numerica non √® semplicemente una decisione tecnica, ma strategica, che influenza l‚Äôacume predittivo del modello, le sue esigenze computazionali e la sua implementabilit√† in diversi ambienti computazionali. In questa sezione esamineremo un paio di esempi per comprendere meglio i compromessi con i numeri e come si collegano al mondo reale.</p>
<section id="veicoli-autonomi" class="level5">
<h5 class="anchored" data-anchor-id="veicoli-autonomi">Veicoli Autonomi</h5>
<p>Nel dominio dei veicoli autonomi, i modelli ML vengono impiegati per interpretare i dati dei sensori e prendere decisioni in tempo reale. I modelli devono elaborare dati ad alta dimensionalit√† da vari sensori (ad esempio, LiDAR, telecamere, radar) ed eseguire numerosi calcoli entro un intervallo di tempo limitato per garantire un funzionamento sicuro e reattivo del veicolo. Quindi i compromessi qui includerebbero:</p>
<ul>
<li>Utilizzo della Memoria: L‚Äôarchiviazione e l‚Äôelaborazione di dati dei sensori ad alta risoluzione, specialmente in formati a virgola mobile, possono consumare una quantit√† di memoria sostanziale.</li>
<li>Complessit√† Computazionale: L‚Äôelaborazione in tempo reale richiede calcoli efficienti, in cui numeri di precisione pi√π elevata potrebbero impedire l‚Äôesecuzione tempestiva delle azioni di controllo.</li>
</ul>
</section>
<section id="applicazioni-sanitarie-mobili" class="level5">
<h5 class="anchored" data-anchor-id="applicazioni-sanitarie-mobili">Applicazioni Sanitarie Mobili</h5>
<p>Le applicazioni sanitarie mobili spesso utilizzano modelli ML per attivit√† come il riconoscimento delle attivit√†, il monitoraggio della salute o l‚Äôanalisi predittiva, operando nell‚Äôambiente con risorse limitate dei dispositivi mobili. I compromessi in questo caso includerebbero:</p>
<ul>
<li>Compromessi di Precisione e Accuratezza: L‚Äôimpiego di numeri a bassa precisione per conservare risorse potrebbe influire sull‚Äôaccuratezza delle previsioni sanitarie o delle rilevazioni di anomalie, il che potrebbe avere implicazioni significative per la salute e la sicurezza degli utenti.</li>
<li>Compatibilit√† Hardware: I modelli devono essere ottimizzati per diversi hardware mobili, garantendo un funzionamento efficiente su un‚Äôampia gamma di dispositivi con diverse capacit√† di calcolo numerico.</li>
</ul>
</section>
<section id="sistemi-di-trading-ad-alta-frequenza-hft" class="level5">
<h5 class="anchored" data-anchor-id="sistemi-di-trading-ad-alta-frequenza-hft">Sistemi di Trading ad Alta Frequenza (HFT)</h5>
<p>I sistemi HFT sfruttano i modelli ML per prendere decisioni di trading rapide basate su dati di mercato in tempo reale. Questi sistemi richiedono risposte a bassissima latenza per capitalizzare le opportunit√† di trading di breve durata.</p>
<ul>
<li>Complessit√† Computazionale: I modelli devono elaborare e analizzare vasti flussi di dati di mercato con una latenza minima, dove anche lievi ritardi, potenzialmente introdotti da numeri a precisione pi√π elevata, possono comportare opportunit√† perse.</li>
<li>Compromessi di Precisione e Accuratezza: I calcoli finanziari spesso richiedono un‚Äôelevata precisione numerica per garantire valutazioni accurate dei prezzi e dei rischi, ponendo sfide nel bilanciamento tra efficienza computazionale e accuratezza numerica.</li>
</ul>
</section>
<section id="sistemi-di-sorveglianza-basati-su-edge" class="level5">
<h5 class="anchored" data-anchor-id="sistemi-di-sorveglianza-basati-su-edge">Sistemi di Sorveglianza Basati su Edge</h5>
<p>I sistemi di sorveglianza distribuiti su dispositivi edge, come le telecamere di sicurezza, utilizzano modelli ML per attivit√† come rilevamento di oggetti, riconoscimento di attivit√† e rilevamento di anomalie, spesso operando con vincoli di risorse rigorosi.</p>
<ul>
<li>Utilizzo della Memoria: L‚Äôarchiviazione di modelli pre-addestrati e l‚Äôelaborazione di feed video in tempo reale richiedono un utilizzo efficiente della memoria, il che pu√≤ essere impegnativo con numeri ad alta precisione.</li>
<li>Compatibilit√† Hardware: Garantire che i modelli possano funzionare in modo efficiente su dispositivi edge con diverse capacit√† hardware e ottimizzazioni per diverse precisioni numeriche √® fondamentale per una distribuzione diffusa.</li>
</ul>
</section>
<section id="simulazioni-scientifiche" class="level5">
<h5 class="anchored" data-anchor-id="simulazioni-scientifiche">Simulazioni Scientifiche</h5>
<p>I modelli ML vengono sempre pi√π utilizzati nelle simulazioni scientifiche, come la modellazione climatica o le simulazioni di dinamica molecolare, per migliorare le capacit√† predittive e ridurre le richieste di calcolo.</p>
<ul>
<li>Compromessi di Precisione e Accuratezza: Le simulazioni scientifiche spesso richiedono un‚Äôelevata precisione numerica per garantire risultati accurati e affidabili, il che pu√≤ entrare in conflitto con il desiderio di ridurre le richieste di calcolo tramite numeri a bassa precisione.</li>
<li>Complessit√† Computazionale: I modelli devono gestire ed elaborare dati di simulazione complessi e ad alta dimensionalit√† in modo efficiente per garantire risultati tempestivi e consentire simulazioni su larga scala o di lunga durata.</li>
</ul>
<p>Questi esempi illustrano diversi scenari in cui le sfide della rappresentazione numerica nei modelli ML sono palesemente manifestate. Ogni sistema presenta un set unico di requisiti e vincoli, che richiedono strategie e soluzioni personalizzate per affrontare i problemi dell‚Äôutilizzo della memoria, della complessit√† computazionale, dei compromessi tra precisione e accuratezza e della compatibilit√† hardware.</p>
</section>
</section>
</section>
<section id="sec-quant" class="level3" data-number="9.3.4">
<h3 data-number="9.3.4" class="anchored" data-anchor-id="sec-quant"><span class="header-section-number">9.3.4</span> Quantizzazione</h3>
<p>La quantizzazione √® prevalente in vari domini scientifici e tecnologici e comporta essenzialmente la mappatura o la limitazione di un set o intervallo continuo in una controparte discreta per ridurre al minimo il numero di bit richiesti.</p>
<section id="analisi-iniziale" class="level4">
<h4 class="anchored" data-anchor-id="analisi-iniziale">Analisi Iniziale</h4>
<p>Iniziamo la nostra incursione nella quantizzazione con una breve analisi di un importante utilizzo della quantizzazione.</p>
<p>Nel signal processing [elaborazione del segnale], l‚Äôonda sinusoidale continua (mostrata in <a href="#fig-sine-wave" class="quarto-xref">Figura&nbsp;<span>9.17</span></a>) pu√≤ essere quantizzata in valori discreti tramite un processo noto come campionamento. Questo √® un concetto fondamentale nell‚Äôelaborazione del segnale digitale ed √® cruciale per convertire segnali analogici (come l‚Äôonda sinusoidale continua) in una forma digitale che possa essere elaborata dai computer. L‚Äôonda sinusoidale √® un esempio prevalente grazie alla sua natura periodica e regolare, il che la rende uno strumento utile per spiegare concetti come frequenza, ampiezza, fase e, naturalmente, quantizzazione.</p>
<div id="fig-sine-wave" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_sinewave.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.17: Onda Sinusoidale.
</figcaption>
</figure>
</div>
<p>Nella versione quantizzata mostrata in <a href="#fig-quantized-sine-wave" class="quarto-xref">Figura&nbsp;<span>9.18</span></a>, l‚Äôonda sinusoidale continua (<a href="#fig-sine-wave" class="quarto-xref">Figura&nbsp;<span>9.17</span></a>) viene campionata a intervalli regolari (in questo caso, ogni <span class="math inline">\(\frac{\pi}{4}\)</span> radianti) e solo questi valori campionati vengono rappresentati nella versione digitale del segnale. Le linee graduali tra i punti mostrano un modo per rappresentare il segnale quantizzato in una forma costante a tratti. Questo √® un esempio semplificato di come funziona la conversione analogico-digitale, in cui un segnale continuo viene mappato su un set discreto di valori, consentendone la rappresentazione e l‚Äôelaborazione digitale.</p>
<div id="fig-quantized-sine-wave" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantized-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_quantizedsine.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.18: Onda Sinusoidale Quantizzata.
</figcaption>
</figure>
</div>
<p>Tornando al contesto del Machine Learning (ML), la quantizzazione si riferisce al processo di limitazione dei possibili valori che i parametri numerici (come pesi e bias) possono assumere in un set discreto, riducendo cos√¨ la precisione dei parametri e, di conseguenza, l‚Äôingombro di memoria del modello. Se implementata correttamente, la quantizzazione pu√≤ ridurre le dimensioni del modello fino a 4 volte e migliorare la latenza e la produttivit√† dell‚Äôinferenza fino a 2-3 volte. <a href="#fig-quantized-models-size" class="quarto-xref">Figura&nbsp;<span>9.19</span></a> illustra l‚Äôimpatto che la quantizzazione ha sulle dimensioni di modelli diversi: ad esempio, un modello di classificazione delle immagini come ResNet-v2 pu√≤ essere compresso da 180 MB a 45 MB con quantizzazione a 8 bit. In genere, la perdita di accuratezza del modello √® inferiore all‚Äô1% con una quantizzazione ben fatta. L‚Äôaccuratezza pu√≤ spesso essere recuperata riaddestrando il modello quantizzato con tecniche di addestramento consapevoli della quantizzazione. Pertanto, questa tecnica √® emersa come molto importante nell‚Äôimplementazione di modelli ML in ambienti con risorse limitate, come dispositivi mobili, dispositivi IoT e piattaforme di edge computing, dove le risorse computazionali (memoria e potenza di elaborazione) sono limitate.</p>
<div id="fig-quantized-models-size" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantized-models-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_reducedmodelsize.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-models-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.19: Effetto della quantizzazione sulle dimensioni del modello. Fonte: HarvardX.
</figcaption>
</figure>
</div>
<p>Esistono diverse dimensioni della quantizzazione, come uniformit√†, stocasticit√† (o determinismo), simmetria, granularit√† (tra layer/canali/gruppi o persino all‚Äôinterno dei canali), considerazioni sulla calibrazione dell‚Äôintervallo (statico o dinamico) e metodi di messa a punto (QAT, PTQ, ZSQ). Esaminiamo questi di seguito.</p>
</section>
</section>
<section id="i-tipi-1" class="level3 page-columns page-full" data-number="9.3.5">
<h3 data-number="9.3.5" class="anchored" data-anchor-id="i-tipi-1"><span class="header-section-number">9.3.5</span> I Tipi</h3>
<section id="quantizzazione-uniforme" class="level4">
<h4 class="anchored" data-anchor-id="quantizzazione-uniforme">Quantizzazione Uniforme</h4>
<p>La quantizzazione uniforme implica la mappatura di valori continui o ad alta precisione su una rappresentazione a precisione inferiore utilizzando una scala uniforme. Ci√≤ significa che l‚Äôintervallo tra ogni possibile valore quantizzato √® coerente. Ad esempio, se i pesi di un layer di rete neurale sono quantizzati su numeri interi a 8 bit (valori tra 0 e 255), un peso con un valore in virgola mobile di 0.56 potrebbe essere mappato su un valore intero di 143, presupponendo una mappatura lineare tra le scale originale e quantizzata. Grazie all‚Äôuso di pipeline matematiche intere o a virgola fissa, questa forma di quantizzazione consente il calcolo sul dominio quantizzato senza la necessit√† di dequantizzare in anticipo.</p>
<p>Il processo per implementare la quantizzazione uniforme inizia con la scelta di un intervallo di numeri reali da quantizzare. Il passaggio successivo consiste nel selezionare una funzione di quantizzazione e mappare i valori reali sugli interi rappresentabili dalla larghezza di bit della rappresentazione quantizzata. Ad esempio, una scelta popolare per una funzione di quantizzazione √®:</p>
<p><span class="math display">\[
Q(r)=Int(r/S) - Z
\]</span></p>
<p>dove <span class="math inline">\(Q\)</span> √® l‚Äôoperatore di quantizzazione, <span class="math inline">\(r\)</span> √® un input a valore reale (nel nostro caso, un‚Äôattivazione o un peso), <span class="math inline">\(S\)</span> √® un fattore di scala a valore reale e <span class="math inline">\(Z\)</span> √® un punto zero intero. La funzione <code>Int</code> mappa un valore reale in un valore intero tramite un‚Äôoperazione di arrotondamento. Tramite questa funzione, abbiamo mappato in modo efficace i valori reali <span class="math inline">\(r\)</span> in alcuni valori interi, ottenendo livelli quantizzati uniformemente distanziati.</p>
<p>Quando i professionisti hanno la necessit√† di recuperare i valori originali di precisione pi√π elevata, i valori reali <span class="math inline">\(r\)</span> possono essere recuperati dai valori quantizzati tramite un‚Äôoperazione nota come <strong>dequantizzazione</strong>. Nell‚Äôesempio sopra, ci√≤ significherebbe eseguire la seguente operazione sul nostro valore quantizzato:</p>
<p><span class="math display">\[
\bar{r} = S(Q(r) + Z)
\]</span></p>
<p>Come discusso, una certa precisione nel valore reale viene persa dalla quantizzazione. In questo caso, il valore recuperato <span class="math inline">\(\bar{r}\)</span> non corrisponder√† esattamente a <span class="math inline">\(r\)</span> a causa dell‚Äôoperazione di arrotondamento. Questo √® un importante compromesso da notare; tuttavia, in molti utilizzi riusciti della quantizzazione, la perdita di precisione pu√≤ essere trascurabile e l‚Äôaccuratezza del test rimane elevata. Nonostante ci√≤, la quantizzazione uniforme continua a essere la scelta di fatto attuale per la sua semplicit√† e l‚Äôefficiente mappatura all‚Äôhardware.</p>
</section>
<section id="quantizzazione-non-uniforme" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="quantizzazione-non-uniforme">Quantizzazione Non-Uniforme</h4>
<p>La quantizzazione non uniforme, d‚Äôaltro canto, non mantiene un intervallo coerente tra i valori quantizzati. Questo approccio potrebbe essere utilizzato per allocare pi√π possibili valori discreti in regioni in cui i valori dei parametri sono pi√π densamente popolati, preservando cos√¨ maggiori dettagli dove sono pi√π necessari. Ad esempio, nelle distribuzioni a campana di pesi con lunghe code, un set di pesi in un modello si trova prevalentemente all‚Äôinterno di un certo intervallo; quindi, pi√π livelli di quantizzazione potrebbero essere assegnati a tale intervallo per preservare dettagli pi√π fini, consentendoci di acquisire meglio le informazioni. Tuttavia, una delle principali debolezze della quantizzazione non uniforme √® che richiede la dequantizzazione prima di calcoli di precisione pi√π elevata a causa della sua non uniformit√†, limitando la sua capacit√† di accelerare il calcolo rispetto alla quantizzazione uniforme.</p>
<p>In genere, una quantizzazione non uniforme basata su regole utilizza una distribuzione logaritmica di passaggi e livelli esponenzialmente crescenti anzich√© linearmente. Un altra tipologia popolare risiede nella quantizzazione basata su codice binario in cui i vettori di numeri reali vengono quantizzati in vettori binari con un fattore di scala. In particolare, non esiste una soluzione in forma chiusa per minimizzare gli errori tra il valore reale e il valore non uniformemente quantizzato, quindi la maggior parte delle quantizzazioni in questo campo si basa su soluzioni euristiche. Ad esempio, un <a href="https://arxiv.org/abs/1802.00150">lavoro recente</a> di <span class="citation" data-cites="xu2018alternating">Xu et al. (<a href="../../references.it.html#ref-xu2018alternating" role="doc-biblioref">2018</a>)</span> formula la quantizzazione non uniforme come un problema di ottimizzazione in cui i passaggi/livelli di quantizzazione nel quantizzatore <span class="math inline">\(Q\)</span> vengono regolati per ridurre al minimo la differenza tra il tensore originale e la controparte quantizzata.</p>
<div class="no-row-height column-margin column-container"><div id="ref-xu2018alternating" class="csl-entry" role="listitem">
Xu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, e Hongbin Zha. 2018. <span>¬´Alternating Multi-bit Quantization for Recurrent Neural Networks¬ª</span>. In <em>6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings</em>. OpenReview.net. <a href="https://openreview.net/forum?id=S19dR9x0b">https://openreview.net/forum?id=S19dR9x0b</a>.
</div></div><p><span class="math display">\[
\min_Q ||Q(r)-r||^2
\]</span></p>
<p>Inoltre, i quantizzatori addestrabili lo possono essere congiuntamente con parametri di modello e i passaggi/livelli di quantizzazione sono generalmente addestrati con ottimizzazione iterativa o discesa del gradiente. Inoltre, il clustering √® stato utilizzato per alleviare la perdita di informazioni dalla quantizzazione. Sebbene in grado di catturare livelli di dettaglio pi√π elevati, gli schemi di quantizzazione non uniformi possono essere difficili da implementare in modo efficiente su hardware di calcolo generale, rendendoli meno preferiti ai metodi che utilizzano la quantizzazione uniforme.</p>
<div id="fig-quantization-uniformity" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-uniformity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_uniformnonuniform.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-uniformity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.20: Uniformit√† della Quantizzazione. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="../../references.it.html#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
</section>
<section id="quantizzazione-stocastica" class="level4">
<h4 class="anchored" data-anchor-id="quantizzazione-stocastica">Quantizzazione Stocastica</h4>
<p>A differenza dei due approcci precedenti che generano mappature deterministiche, c‚Äô√® un po‚Äô di lavoro che esplora l‚Äôidea della quantizzazione stocastica per l‚Äôaddestramento consapevole della quantizzazione e l‚Äôaddestramento a precisione ridotta. Questo approccio mappa numeri fluttuanti verso l‚Äôalto o verso il basso con una probabilit√† associata alla grandezza dell‚Äôaggiornamento del peso. La speranza generata dall‚Äôintuizione di alto livello √® che un tale approccio probabilistico possa consentire a una rete neurale di esplorare di pi√π, rispetto alla quantizzazione deterministica. Presumibilmente, abilitare un arrotondamento stocastico potrebbe consentire alle reti neurali di sfuggire agli ottimi locali, aggiornando cos√¨ i propri parametri. Di seguito sono riportati due esempi di funzioni di mappatura stocastica:</p>
<p><img src="images/png/efficientnumerics_nonuniform.png" class="img-fluid"></p>
<div id="fig-integer-binary-quantization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-integer-binary-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_binary.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-integer-binary-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.21: Funzioni di quantizzazione Intera e Binaria.
</figcaption>
</figure>
</div>
</section>
<section id="quantizzazione-zero-shot" class="level4">
<h4 class="anchored" data-anchor-id="quantizzazione-zero-shot">Quantizzazione ‚ÄúZero Shot‚Äù</h4>
<p>La quantizzazione Zero-shot si riferisce al processo di conversione di un modello di deep learning a precisione completa direttamente in un modello quantizzato a bassa precisione senza la necessit√† di alcun riaddestramento o messa a punto sul modello quantizzato. Il vantaggio principale di questo approccio √® la sua efficienza, in quanto elimina il processo, spesso dispendioso in termini di tempo e risorse, del riaddestramento post-quantizzazione. Sfruttando tecniche che anticipano e riducono al minimo gli errori di quantizzazione, la quantizzazione zero-shot mantiene l‚Äôaccuratezza originale del modello anche dopo averne ridotto la precisione numerica. √à particolarmente utile per i provider di ‚ÄúMachine Learning as a Service (MLaaS)‚Äù che mirano ad accelerare la distribuzione dei carichi di lavoro dei propri clienti senza dover accedere ai loro set di dati.</p>
</section>
</section>
<section id="calibrazione" class="level3 page-columns page-full" data-number="9.3.6">
<h3 data-number="9.3.6" class="anchored" data-anchor-id="calibrazione"><span class="header-section-number">9.3.6</span> Calibrazione</h3>
<p>La calibrazione √® il processo di selezione dell‚Äôintervallo di clipping [ritaglio] pi√π efficace [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] per pesi e attivazioni da quantizzare. Ad esempio, si consideri la quantizzazione delle attivazioni che originariamente hanno un intervallo in virgola mobile tra -6 e 6 a interi a 8 bit. Prendere solo i valori minimi e massimi possibili di interi a 8 bit (da -128 a 127) come intervallo di quantizzazione, potrebbe non essere il pi√π efficace. Invece, la calibrazione implicherebbe il passaggio di un set di dati rappresentativo e quindi l‚Äôutilizzo di questo intervallo osservato per la quantizzazione.</p>
<p>Esistono molti metodi di calibrazione, ma alcuni comunemente utilizzati includono:</p>
<ul>
<li>Max: Utilizza il valore assoluto massimo visualizzato durante la calibrazione. Tuttavia, questo metodo √® suscettibile di dati anomali. Notare come in <a href="#fig-resnet-activations-histogram" class="quarto-xref">Figura&nbsp;<span>9.22</span></a>, abbiamo un cluster anomalo intorno a 2.1, mentre il resto √® raggruppato attorno a valori pi√π piccoli.</li>
<li>Entropia: Utilizza la divergenza KL per ridurre al minimo la perdita di informazioni tra i valori originali in virgola mobile e i valori che potrebbero essere rappresentati dal formato quantizzato. Questo √® il metodo predefinito utilizzato da TensorRT.</li>
<li>Percentile: Imposta l‚Äôintervallo su un percentile della distribuzione dei valori assoluti osservati durante la calibrazione. Ad esempio, una calibrazione del 99% taglierebbe l‚Äô1% dei valori di magnitudine pi√π grandi.</li>
</ul>
<div id="fig-resnet-activations-histogram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-resnet-activations-histogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_calibrationcopy.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-resnet-activations-histogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.22: Attivazioni di input nel layer 3 in ResNet50. Fonte: @<span class="citation" data-cites="wu2020integer">Wu, Judd, e Isaev (<a href="../../references.it.html#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>√à importante notare che la qualit√† della calibrazione pu√≤ fare la differenza tra un modello quantizzato che conserva la maggior parte della sua accuratezza e uno che si degrada in modo significativo. Quindi, √® un passaggio essenziale nel processo di quantizzazione. Quando si sceglie un intervallo di calibrazione, ci sono due tipi: simmetrico e asimmetrico.</p>
<section id="quantizzazione-simmetrica" class="level4">
<h4 class="anchored" data-anchor-id="quantizzazione-simmetrica">Quantizzazione Simmetrica</h4>
<p>La quantizzazione simmetrica mappa i valori reali su un intervallo di clipping simmetrico centrato su 0. Ci√≤ comporta la scelta di un intervallo [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] dove <span class="math inline">\(\alpha = -\beta\)</span>. Ad esempio, un intervallo simmetrico si baserebbe sui valori min/max dei valori reali in modo tale che:</p>
<p><span class="math display">\[
\alpha = \beta = max(abs(r_{max}), abs(r_{min}))
\]</span></p>
<p>Gli intervalli di clipping simmetrici sono i pi√π ampiamente adottati nella pratica in quanto hanno il vantaggio di un‚Äôimplementazione pi√π semplice. In particolare, la mappatura da zero a zero nell‚Äôintervallo di clipping (talvolta chiamata ‚Äúazzeramento del punto zero‚Äù) pu√≤ portare a una riduzione del costo computazionale durante l‚Äôinferenza <a href="https://arxiv.org/abs/2004.09602"><span class="citation" data-cites="wu2020integer">(<span>Wu, Judd, e Isaev 2020</span>)</span></a>.</p>
</section>
<section id="quantizzazione-asimmetrica" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="quantizzazione-asimmetrica">Quantizzazione Asimmetrica</h4>
<p>La quantizzazione asimmetrica mappa i valori reali in un intervallo di clipping asimmetrico che non √® necessariamente centrato sullo 0, come mostrato in <a href="#fig-quantization-symmetry" class="quarto-xref">Figura&nbsp;<span>9.23</span></a> a destra. Comporta la scelta di un intervallo [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] dove <span class="math inline">\(\alpha \neq -\beta\)</span>. Ad esempio, selezionando un intervallo basato sui valori reali minimi e massimi, o dove <span class="math inline">\(\alpha = r_{min}\)</span> and <span class="math inline">\(\beta = r_{max}\)</span>, si crea un intervallo asimmetrico. In genere, la quantizzazione asimmetrica produce intervalli di clipping pi√π stretti rispetto a quella simmetrica, il che √® importante quando i pesi e le attivazioni target sono sbilanciati, ad esempio, l‚Äôattivazione dopo la ReLU ha sempre valori non negativi. Nonostante produca intervalli di clipping pi√π stretti, la quantizzazione asimmetrica √® meno preferita di quella simmetrica in quanto non azzera sempre il valore dello zero reale.</p>
<div id="fig-quantization-symmetry" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-symmetry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_symmetry.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-symmetry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.23: (a)simmetria della Quantizzazione. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="../../references.it.html#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
</section>
<section id="granularit√†" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="granularit√†">Granularit√†</h4>
<p>Dopo aver deciso il tipo di intervallo di clipping, √® essenziale restringerlo per consentire a un modello di mantenere la massima accuratezza possibile. Daremo un‚Äôocchiata alle reti neurali convoluzionali come nostro modo di esplorare metodi che ottimizzano la granularit√† degli intervalli di clipping per la quantizzazione. L‚Äôattivazione di input di un layer nella nostra CNN subisce una convoluzione con pi√π filtri convoluzionali. Ogni filtro convoluzionale pu√≤ possedere un intervallo di valori univoco. Si noti come in <a href="#fig-quantization-granularity" class="quarto-xref">Figura&nbsp;<span>9.24</span></a> l‚Äôintervallo per il Filtro 1 sia molto pi√π piccolo di quello per il Filtro 3. Di conseguenza, una caratteristica distintiva degli approcci di quantizzazione √® la precisione con cui l‚Äôintervallo di clipping [Œ±,Œ≤] viene determinato per i pesi.</p>
<div id="fig-quantization-granularity" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_granularity.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.24: Granularit√† di quantizzazione: intervalli variabili. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="../../references.it.html#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<ol type="1">
<li><strong>Quantizzazione a Layer:</strong> Questo approccio determina l‚Äôintervallo di clipping considerando tutti i pesi nei filtri convoluzionali di un layer. Quindi, lo stesso intervallo di clipping viene utilizzato per tutti i filtri convoluzionali. √à il pi√π semplice da implementare e, come tale, spesso si traduce in una precisione non ottimale a causa dell‚Äôampia variet√† di intervalli diversi tra i filtri. Ad esempio, un kernel convoluzionale con un intervallo di parametri pi√π ristretto perde la sua risoluzione di quantizzazione a causa di un altro kernel nello stesso layer che ha un intervallo pi√π ampio.</li>
<li><strong>Groupwise Quantization:</strong> Questo approccio raggruppa diversi canali all‚Äôinterno di un layer per calcolare l‚Äôintervallo di clipping. Questo metodo pu√≤ essere utile quando la distribuzione dei parametri su una singola convoluzione/attivazione varia molto. In pratica, questo metodo √® stato utile in Q-BERT <span class="citation" data-cites="sheng2019qbert">(<a href="../../references.it.html#ref-sheng2019qbert" role="doc-biblioref">Shen et al. 2020</a>)</span> per quantizzare i modelli Transformer <span class="citation" data-cites="vaswani2017attention">(<a href="../../references.it.html#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span> costituiti da layer di attenzione completamente connessi. Lo svantaggio di questo approccio √® il costo aggiuntivo di contabilizzazione di diversi fattori di scala.</li>
<li><strong>Channelwise Quantization:</strong> Questo metodo popolare utilizza un intervallo fisso per ogni filtro convoluzionale che √® indipendente dagli altri canali. Poich√© a ogni canale viene assegnato un fattore di scala dedicato, questo metodo garantisce una risoluzione di quantizzazione pi√π elevata e spesso si traduce in una maggiore accuratezza.</li>
<li><strong>Sub-channelwise Quantization:</strong> Portando la quantizzazione canale per canale all‚Äôestremo, questo metodo determina l‚Äôintervallo di clipping rispetto a qualsiasi gruppo di parametri in una convoluzione o in un layer completamente connesso. Potrebbe comportare un overhead considerevole poich√© √® necessario tenere conto di diversi fattori di scala quando si elabora una singola convoluzione o un layer completamente connesso.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-sheng2019qbert" class="csl-entry" role="listitem">
Shen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, e Kurt Keutzer. 2020. <span>¬´Q-<span>BERT:</span> <span>Hessian</span> Based Ultra Low Precision Quantization of <span>BERT</span>¬ª</span>. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 34 (05): 8815‚Äì21. <a href="https://doi.org/10.1609/aaai.v34i05.6409">https://doi.org/10.1609/aaai.v34i05.6409</a>.
</div><div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, e Illia Polosukhin. 2017. <span>¬´Attention is all you need¬ª</span>. <em>Adv Neural Inf Process Syst</em> 30.
</div></div><p>Tra questi, la quantizzazione canale per canale √® lo standard corrente utilizzato per quantizzare i kernel convoluzionali, poich√© consente la regolazione degli intervalli di clipping per ogni singolo kernel con overhead trascurabile.</p>
</section>
<section id="quantizzazione-statica-e-dinamica" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="quantizzazione-statica-e-dinamica">Quantizzazione Statica e Dinamica</h4>
<p>Dopo aver determinato il tipo e la granularit√† dell‚Äôintervallo di clipping, gli esperti devono decidere quando gli intervalli vengono determinati nei loro algoritmi di calibrazione dell‚Äôintervallo. Esistono due approcci per quantizzare le attivazioni: quantizzazione statica e quella dinamica.</p>
<p>La quantizzazione statica √® l‚Äôapproccio pi√π frequentemente utilizzato. In questo, l‚Äôintervallo di clipping √® precalcolato e statico durante l‚Äôinferenza. Non aggiunge alcun sovraccarico computazionale, ma, di conseguenza, comporta una minore accuratezza rispetto alla quantizzazione dinamica. Un metodo popolare per implementarlo √® eseguire una serie di input di calibrazione per calcolare l‚Äôintervallo tipico di attivazioni <span class="citation" data-cites="jacob2018quantization yao2021hawq">(<a href="../../references.it.html#ref-jacob2018quantization" role="doc-biblioref">Jacob et al. 2018</a>; <a href="../../references.it.html#ref-yao2021hawq" role="doc-biblioref">Yao et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jacob2018quantization" class="csl-entry" role="listitem">
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. <span>¬´Quantization and training of neural networks for efficient integer-arithmetic-only inference¬ª</span>. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2704‚Äì13.
</div><div id="ref-yao2021hawq" class="csl-entry" role="listitem">
Yao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, et al. 2021. <span>¬´Hawq-v3: <span>Dyadic</span> neural network quantization¬ª</span>. In <em>International Conference on Machine Learning</em>, 11875‚Äì86. PMLR.
</div></div><p>La quantizzazione dinamica √® un approccio alternativo che calcola dinamicamente l‚Äôintervallo per ogni mappa di attivazione durante il runtime. L‚Äôapproccio richiede calcoli in tempo reale che potrebbero avere un sovraccarico molto elevato. In questo modo, la quantizzazione dinamica spesso raggiunge la massima accuratezza poich√© l‚Äôintervallo viene calcolato specificamente per ogni input.</p>
<p>Tra i due, il calcolo dell‚Äôintervallo in modo dinamico √® solitamente molto costoso, quindi la maggior parte dei professionisti utilizzer√† spesso la quantizzazione statica.</p>
</section>
</section>
<section id="tecniche" class="level3 page-columns page-full" data-number="9.3.7">
<h3 data-number="9.3.7" class="anchored" data-anchor-id="tecniche"><span class="header-section-number">9.3.7</span> Tecniche</h3>
<p>Le due tecniche prevalenti per la quantizzazione dei modelli sono la ‚ÄúPost Training Quantization‚Äù e la ‚ÄúQuantization-Aware Training‚Äù.</p>
<p><strong>Post Training Quantization:</strong> La quantizzazione post-addestramento (PTQ) √® una tecnica di quantizzazione in cui il modello viene quantizzato dopo essere stato addestrato. Il modello viene addestrato in virgola mobile e poi i pesi e le attivazioni vengono quantizzati come fase di post-elaborazione. Questo √® l‚Äôapproccio pi√π semplice e non richiede l‚Äôaccesso ai dati di addestramento. Diversamente la ‚ÄúQuantization-Aware Training (QAT), PTQ‚Äù imposta direttamente i parametri di quantizzazione del peso e dell‚Äôattivazione, rendendolo poco costoso e adatto a situazioni con dati limitati o non etichettati. Tuttavia, non riaggiustare i pesi dopo la quantizzazione, specialmente nella quantizzazione a bassa precisione, pu√≤ portare a un comportamento molto diverso e quindi a una minore accuratezza. Per affrontare questo problema, sono state sviluppate tecniche come la correzione della distorsione, l‚Äôequalizzazione degli intervalli di peso e i metodi di arrotondamento adattivo. PTQ pu√≤ essere applicato anche in scenari zero-shot, in cui non sono disponibili dati di addestramento o di test. Questo metodo √® stato reso ancora pi√π efficiente per avvantaggiare modelli linguistici di grandi dimensioni che richiedono molta elaborazione e memoria. Di recente, √® stata sviluppata SmoothQuant, una soluzione PTQ senza training, che preserva l‚Äôaccuratezza ed √® di uso generale che consente la quantizzazione di peso a 8 bit e attivazione a 8 bit per LLM, dimostrando un‚Äôaccelerazione fino a 1.56x e una riduzione della memoria di 2x per LLM con una perdita trascurabile di accuratezza <a href="https://arxiv.org/abs/2211.10438"><span class="citation" data-cites="xiao2022smoothquant">(<span>Xiao et al. 2022</span>)</span></a>.</p>
<p>In PTQ, un modello pre-addestrato subisce un processo di calibrazione, come mostrato in <a href="#fig-PTQ-diagram" class="quarto-xref">Figura&nbsp;<span>9.25</span></a>. La calibrazione comporta l‚Äôutilizzo di un set di dati separato noto come dati di calibrazione, un sottoinsieme specifico dei dati di training riservato alla quantizzazione per aiutare a trovare gli intervalli di clipping e i fattori di scala appropriati.</p>
<div id="fig-PTQ-diagram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-PTQ-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQ.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-PTQ-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.25: Quantizzazione e Calibrazione Post-Training. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="../../references.it.html#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p><strong>Quantization-Aware Training:</strong> L‚Äôaddestramento consapevole della quantizzazione (QAT) √® una messa a punto del modello PTQ. Il modello viene addestrato in modo consapevole della quantizzazione, consentendogli di adattarsi agli effetti della quantizzazione. Ci√≤ produce una migliore accuratezza con l‚Äôinferenza quantizzata. La quantizzazione di un modello di rete neurale addestrato con metodi come PTQ introduce perturbazioni che possono deviare il modello dal suo punto di convergenza originale. Ad esempio, Krishnamoorthi ha dimostrato che anche con la quantizzazione per canale, reti come MobileNet non raggiungono la precisione di base con int8 ‚ÄúPost Training Quantization (PTQ)‚Äù e richiedono ‚ÄúQuantization-Aware Training (QAT)‚Äù <a href="https://arxiv.org/abs/1806.08342"><span class="citation" data-cites="krishnamoorthi2018quantizing">(<span>Krishnamoorthi 2018</span>)</span></a>. Per risolvere questo problema, QAT riaddestra il modello con parametri quantizzati, impiegando passaggi forward e backward in virgola mobile ma quantizzando i parametri dopo ogni aggiornamento del gradiente. La gestione dell‚Äôoperatore di quantizzazione non differenziabile √® fondamentale; un metodo ampiamente utilizzato √® lo ‚ÄúStraight Through Estimator (STE)‚Äù, che approssima l‚Äôoperazione di arrotondamento come una funzione identit√†. Sebbene esistano altri metodi e varianti, STE rimane il pi√π comunemente utilizzato per la sua efficacia pratica. In QAT, un modello pre-addestrato viene quantizzato e poi messo a punto utilizzando i dati di addestramento per regolare i parametri e recuperare il degrado della precisione, come mostrato in <a href="#fig-QAT-diagram" class="quarto-xref">Figura&nbsp;<span>9.26</span></a>. Il processo di calibrazione viene spesso condotto parallelamente al processo di messa a punto per QAT.</p>
<div id="fig-QAT-diagram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-QAT-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_QAT.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-QAT-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.26: Quantization-Aware Training. Fonte: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="../../references.it.html#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-gholami2021survey" class="csl-entry" role="listitem">
Gholami, Dong Kim, Mahoney Yao, e Keutzer. 2021. <span>¬´A Survey of Quantization Methods for Efficient Neural Network Inference)¬ª</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/2103.13630">https://arxiv.org/abs/2103.13630</a>.
</div></div></figure>
</div>
<p>La ‚ÄúQuantization-Aware Training‚Äù funge da estensione naturale della ‚ÄúPost-Training Quantization‚Äù. Dopo la quantizzazione iniziale eseguita da PTQ, QAT viene utilizzata per perfezionare e mettere a punto ulteriormente i parametri quantizzati: vedere come in <a href="#fig-QAT-PTQ-relation" class="quarto-xref">Figura&nbsp;<span>9.27</span></a>, il modello PTQ subisce un ulteriore passaggio, QAT. Comporta un processo di riqualificazione in cui il modello viene esposto a ulteriori iterazioni di training utilizzando i dati originali. Questo approccio di training dinamico consente al modello di adattare e regolare i suoi parametri, compensando il degrado delle prestazioni causato dalla quantizzazione.</p>
<div id="fig-QAT-PTQ-relation" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-QAT-PTQ-relation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQQAT.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-QAT-PTQ-relation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.27: PTQ e QAT. Fonte: <span class="citation" data-cites="ultimate"><span>¬´The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training¬ª</span> (<a href="../../references.it.html#ref-ultimate" role="doc-biblioref">s.d.</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-ultimate" class="csl-entry" role="listitem">
<span>¬´The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training¬ª</span>. s.d. <a href="https://deci.ai/quantization-and-quantization-aware-training/">https://deci.ai/quantization-and-quantization-aware-training/</a>.
</div></div></figure>
</div>
<p><a href="#fig-quantization-methods-summary" class="quarto-xref">Figura&nbsp;<span>9.28</span></a> mostra l‚Äôaccuratezza relativa di diversi modelli dopo PTQ e QAT. In quasi tutti i casi, QAT produce un‚Äôaccuratezza migliore di PTQ. Si consideri ad esempio EfficientNet b0. Dopo PTQ, l‚Äôaccuratezza scende dal 76.85% a 72.06%. Ma quando applichiamo QAT, l‚Äôaccuratezza rimbalza al 76.95% (con persino un leggero miglioramento rispetto all‚Äôaccuratezza originale).</p>
<div id="fig-quantization-methods-summary" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-methods-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQQATsummary.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-methods-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.28: Accuratezza relativa di PTQ e QAT. Fonte: <span class="citation" data-cites="wu2020integer">Wu, Judd, e Isaev (<a href="../../references.it.html#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Aspetto</strong></th>
<th style="text-align: left;"><strong>Post Training Quantization</strong></th>
<th style="text-align: left;"><strong>Quantization-Aware Training</strong></th>
<th style="text-align: left;"><strong>Dynamic Quantization</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Pro</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Semplicit√†</td>
<td style="text-align: left;">‚úì</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úó</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Preservazione della precisione</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úì</td>
<td style="text-align: left;">‚úì</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adattabilit√†</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úì</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Prestazioni Ottimizzate</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úì</td>
<td style="text-align: left;">Potenzialmente</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Contro</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Degrado della Precisione</td>
<td style="text-align: left;">‚úì</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">Potenzialmente</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sovraccarico Computazionale</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úì</td>
<td style="text-align: left;">‚úì</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Complessit√† di Implementazione</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úì</td>
<td style="text-align: left;">‚úì</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Compromessi</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Velocit√† vs.&nbsp;Precisione</td>
<td style="text-align: left;">‚úì</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úó</td>
</tr>
<tr class="even">
<td style="text-align: left;">Precisione vs.&nbsp;Costo</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úì</td>
<td style="text-align: left;">‚úó</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adattabilit√† vs.&nbsp;Overhead</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úó</td>
<td style="text-align: left;">‚úì</td>
</tr>
</tbody>
</table>
</section>
<section id="pesi-vs.-attivazioni" class="level3" data-number="9.3.8">
<h3 data-number="9.3.8" class="anchored" data-anchor-id="pesi-vs.-attivazioni"><span class="header-section-number">9.3.8</span> Pesi vs.&nbsp;Attivazioni</h3>
<p><strong>Quantizzazione del peso:</strong> Comporta la conversione dei pesi continui o ad alta precisione di un modello in pesi a bassa precisione, come la conversione dei pesi Float32 in pesi INT8 (interi) quantizzati - in <a href="#fig-weight-activations-quantization" class="quarto-xref">Figura&nbsp;<span>9.29</span></a>, la quantizzazione del peso avviene nel secondo passaggio (quadrati rossi) quando moltiplichiamo gli input. Ci√≤ riduce le dimensioni del modello, riducendo cos√¨ la memoria richiesta per archiviare il modello e le risorse computazionali necessarie per eseguire l‚Äôinferenza. Ad esempio, si consideri una matrice di pesi in un layer di rete neurale con pesi Float32 come [0.215, -1.432, 0.902, ‚Ä¶]. Attraverso la quantizzazione del peso, questi potrebbero essere mappati su valori INT8 come [27, -183, 115, ‚Ä¶], riducendo significativamente la memoria richiesta per memorizzarli.</p>
<div id="fig-weight-activations-quantization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weight-activations-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_weightsactivations.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weight-activations-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.29: Quantizzazione del peso e dell‚Äôattivazione. Fonte: HarvardX.
</figcaption>
</figure>
</div>
<p><strong>Quantizzazione dell‚ÄôAttivazione:</strong> Comporta la quantizzazione dei valori di attivazione (output dei livelli) durante l‚Äôinferenza del modello. Ci√≤ pu√≤ ridurre le risorse computazionali richieste durante l‚Äôinferenza, ma introduce ulteriori problemi nel mantenimento dell‚Äôaccuratezza del modello a causa della ridotta precisione dei calcoli intermedi. Ad esempio, in una rete neurale convoluzionale (CNN), le mappe di attivazione (mappe delle feature) prodotte dai layer convoluzionali, originariamente in Float32, potrebbero essere quantizzate su INT8 durante l‚Äôinferenza per accelerare il calcolo, in particolare su hardware ottimizzato per l‚Äôaritmetica degli interi. Inoltre, un lavoro recente ha esplorato l‚Äôuso della quantizzazione del ‚ÄúActivation-aware Weight Quantization‚Äù per la compressione e l‚Äôaccelerazione LLM, che comporta la protezione di solo l‚Äô1% dei pesi salienti pi√π importanti osservando le attivazioni, non i pesi <a href="https://arxiv.org/pdf/2306.00978.pdf"><span class="citation" data-cites="lin2023awq">(<span>Lin et al. 2023</span>)</span></a>.</p>
</section>
<section id="compromessi" class="level3 page-columns page-full" data-number="9.3.9">
<h3 data-number="9.3.9" class="anchored" data-anchor-id="compromessi"><span class="header-section-number">9.3.9</span> Compromessi</h3>
<p>La quantizzazione introduce invariabilmente un compromesso tra dimensioni/prestazioni del modello e accuratezza. Sebbene riduca significativamente l‚Äôingombro della memoria e possa accelerare l‚Äôinferenza, specialmente su hardware ottimizzato per aritmetica a bassa precisione, la precisione ridotta pu√≤ degradare l‚Äôaccuratezza del modello.</p>
<p><strong>Dimensioni del Modello:</strong> Un modello con pesi rappresentati come Float32 quantizzato a INT8 pu√≤ teoricamente ridurre le dimensioni del modello di un fattore 4, consentendone l‚Äôimplementazione su dispositivi con memoria limitata. Le dimensioni di grandi modelli linguistici si stanno sviluppando a un ritmo pi√π veloce della memoria GPU negli ultimi anni, portando a un grande divario tra domanda e offerta di memoria. <a href="#fig-model-size-pace" class="quarto-xref">Figura&nbsp;<span>9.30</span></a> illustra la recente tendenza del divario crescente tra le dimensioni del modello (linea rossa) e la memoria dell‚Äôacceleratore (linea gialla). Le tecniche di quantizzazione e compressione del modello possono aiutare a colmare il divario</p>
<div id="fig-model-size-pace" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-model-size-pace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_modelsizes.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-size-pace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.30: Dimensioni del modello vs.&nbsp;memoria dell‚Äôacceleratore. Fonte: <span class="citation" data-cites="xiao2022smoothquant">Xiao et al. (<a href="../../references.it.html#ref-xiao2022smoothquant" role="doc-biblioref">2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-xiao2022smoothquant" class="csl-entry" role="listitem">
Xiao, Seznec Lin, Demouth Wu, e Han. 2022. <span>¬´<span>SmoothQuant:</span> <span>Accurate</span> and Efficient Post-Training Quantization for Large Language Models¬ª</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/2211.10438">https://arxiv.org/abs/2211.10438</a>.
</div></div></figure>
</div>
<p><strong>Velocit√† di Inferenza:</strong> La quantizzazione pu√≤ anche accelerare l‚Äôinferenza, poich√© l‚Äôaritmetica a precisione inferiore √® computazionalmente meno costosa. Ad esempio, alcuni acceleratori hardware, come Edge TPU di Google, sono ottimizzati per l‚Äôaritmetica INT8 e possono eseguire l‚Äôinferenza in modo significativamente pi√π rapido con modelli quantizzati INT8 rispetto alle loro controparti in virgola mobile. La riduzione della memoria dalla quantizzazione aiuta a ridurre la quantit√† di trasmissione dei dati, risparmiando memoria e velocizzando il processo. <a href="#fig-nvidia-turing" class="quarto-xref">Figura&nbsp;<span>9.31</span></a> confronta l‚Äôaumento della produttivit√† e la riduzione della memoria della larghezza di banda per diversi tipi di dati sulla NVIDIA Turing GPU.</p>
<div id="fig-nvidia-turing" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-nvidia-turing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_benefitsofprecision.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nvidia-turing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.31: Vantaggi dei tipi di dati a precisione inferiore. Fonte: <span class="citation" data-cites="wu2020integer">Wu, Judd, e Isaev (<a href="../../references.it.html#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-wu2020integer" class="csl-entry" role="listitem">
Wu, Zhang Judd, e Micikevicius Isaev. 2020. <span>¬´Integer Quantization for Deep Learning Inference: <span>Principles</span> and Empirical Evaluation)¬ª</span>. <em>ArXiv preprint</em>. <a href="https://arxiv.org/abs/2004.09602">https://arxiv.org/abs/2004.09602</a>.
</div></div></figure>
</div>
<p><strong>Precisione:</strong> La riduzione della precisione numerica post-quantizzazione pu√≤ portare a un degrado della precisione del modello, che potrebbe essere accettabile in alcune applicazioni (ad esempio, classificazione delle immagini) ma non in altre (ad esempio, diagnosi medica). Pertanto, dopo la quantizzazione, il modello richiede in genere una ricalibrazione o una messa a punto per mitigare la perdita di accuratezza. Inoltre, un lavoro recente ha esplorato l‚Äôuso di <a href="https://arxiv.org/pdf/2306.00978.pdf">Activation-aware Weight Quantization <span class="citation" data-cites="lin2023awq">(<span>Lin et al. 2023</span>)</span></a> che si basa sull‚Äôosservazione che proteggere solo l‚Äô1% dei pesi salienti pu√≤ ridurre notevolmente l‚Äôerrore di quantizzazione.</p>
</section>
<section id="quantizzazione-e-potatura" class="level3 page-columns page-full" data-number="9.3.10">
<h3 data-number="9.3.10" class="anchored" data-anchor-id="quantizzazione-e-potatura"><span class="header-section-number">9.3.10</span> Quantizzazione e Potatura</h3>
<p>Pruning [potatura] e quantizzazione funzionano bene insieme ed √® stato scoperto che il pruning non ostacola la quantizzazione. In effetti, il pruning pu√≤ aiutare a ridurre l‚Äôerrore di quantizzazione. Intuitivamente, ci√≤ √® dovuto al pruning che riduce il numero di pesi da quantizzare, riducendo cos√¨ l‚Äôerrore accumulato dalla quantizzazione. Ad esempio, una AlexNet non potata ha 60 milioni di pesi da quantizzare mentre una AlexNet potata ha solo 6.7 milioni di pesi da quantizzare. Questa significativa riduzione dei pesi aiuta a ridurre l‚Äôerrore tra la quantizzazione dell‚ÄôAlexNet non potato rispetto all‚ÄôAlexNet potato. Inoltre, studi recenti hanno scoperto che il pruning consapevole della quantizzazione genera modelli pi√π efficienti dal punto di vista computazionale rispetto al pruning o alla quantizzazione da soli; in genere, ha prestazioni simili o migliori in termini di efficienza computazionale rispetto ad altre tecniche di ricerca dell‚Äôarchitettura neurale come l‚Äôottimizzazione bayesiana <a href="https://arxiv.org/pdf/2102.11289.pdf"><span class="citation" data-cites="hawks2021psandqs">(<span>Hawks et al. 2021</span>)</span></a>.</p>
<div id="fig-compression-methods" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-compression-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_qp1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compression-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.32: Precisione rispetto al tasso di compressione con diversi metodi di compressione. Fonte: <span class="citation" data-cites="han2015deep">Han, Mao, e Dally (<a href="../../references.it.html#ref-han2015deep" role="doc-biblioref">2015</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-han2015deep" class="csl-entry" role="listitem">
Han, Song, Huizi Mao, e William J Dally. 2015. <span>¬´Deep compression: <span>Compressing</span> deep neural networks with pruning, trained quantization and huffman coding¬ª</span>. <em>arXiv preprint arXiv:1510.00149</em>.
</div></div></figure>
</div>
</section>
<section id="quantizzazione-edge-aware" class="level3" data-number="9.3.11">
<h3 data-number="9.3.11" class="anchored" data-anchor-id="quantizzazione-edge-aware"><span class="header-section-number">9.3.11</span> Quantizzazione Edge-aware</h3>
<p>La quantizzazione non solo riduce le dimensioni del modello, ma consente anche calcoli pi√π rapidi e consuma meno energia, rendendola fondamentale per lo sviluppo per edge. I dispositivi edge in genere hanno vincoli di risorse rigidi con elaborazione, memoria e potenza, impossibili da soddisfare per molti dei modelli deep NN profondi odierni. Inoltre, i processori edge non supportano le operazioni in virgola mobile, rendendo la quantizzazione intera particolarmente importante per chip come GAP-8, un SoC RISC-V per l‚Äôinferenza edge con un acceleratore CNN dedicato, che supporta solo l‚Äôaritmetica intera.</p>
<p>Una piattaforma hardware che utilizza la quantizzazione √® il gruppo ARM Cortex-M di core di processori ARM RISC a 32 bit. Sfruttano la quantizzazione a virgola fissa con fattori di scala di potenza di due, in modo che la quantizzazione e la de-quantizzazione possano essere eseguite in modo efficiente tramite spostamento di bit. Inoltre, Google Edge TPU, la soluzione emergente di Google per l‚Äôesecuzione di inferenze in periferia, √® progettata per dispositivi piccoli e a bassa potenza e pu√≤ supportare solo l‚Äôaritmetica a 8 bit. Molti modelli di reti neurali complesse che potevano essere distribuiti solo su server a causa delle loro elevate esigenze di elaborazione possono ora essere eseguiti su dispositivi edge grazie ai recenti progressi (ad esempio metodi di quantizzazione) nel campo dell‚Äôedge computing.</p>
<p>Oltre a essere una tecnica indispensabile per molti processori edge, la quantizzazione ha anche apportato notevoli miglioramenti ai processori non edge, incoraggiando tali processori a soddisfare i requisiti del Service Level Agreement (SLA) come la latenza del 99¬∞ percentile.</p>
<p>Pertanto, la quantizzazione combinata con una logica efficiente a bassa precisione e acceleratori dedicati di deep learning, √® stata una forza trainante cruciale per l‚Äôevoluzione di tali processori edge.</p>
<p><a href="#vid-quant" class="quarto-xref">Video&nbsp;<span>9.1</span></a> √® una lezione sulla quantizzazione e sui diversi metodi di quantizzazione.</p>
<div id="vid-quant" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;9.1: Quantizzazione
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AlASZb93rrc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
</section>
<section id="sec-model_ops_hw" class="level2 page-columns page-full" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="sec-model_ops_hw"><span class="header-section-number">9.4</span> Implementazione Hardware Efficiente</h2>
<p>L‚Äôimplementazione hardware efficiente trascende la selezione di componenti adatti; richiede una comprensione olistica di come il software interagir√† con le architetture sottostanti. L‚Äôessenza del raggiungimento delle massime prestazioni nelle applicazioni TinyML non risiede solo nell‚Äôaffinare gli algoritmi per l‚Äôhardware, ma anche nell‚Äôassicurare che l‚Äôhardware sia strategicamente adattato per supportare questi algoritmi. Questa sinergia tra hardware e software √® fondamentale. Mentre esaminiamo pi√π a fondo le complessit√† dell‚Äôimplementazione hardware efficiente, il significato di un approccio di progettazione congiunta, in cui hardware e software vengono sviluppati in tandem, diventa sempre pi√π evidente. Questa sezione fornisce una panoramica delle tecniche di come l‚Äôhardware e le interazioni tra hardware e software possono essere ottimizzati per migliorare le prestazioni dei modelli.</p>
<section id="ricerca-di-architettura-neurale-basata-sullhardware" class="level3 page-columns page-full" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="ricerca-di-architettura-neurale-basata-sullhardware"><span class="header-section-number">9.4.1</span> Ricerca di Architettura Neurale Basata sull‚ÄôHardware</h3>
<p>Concentrarsi solo sulla precisione durante l‚Äôesecuzione della ricerca di architettura neurale porta a modelli esponenzialmente complessi e che richiedono memoria e capacit√† di elaborazione crescenti. Ci√≤ ha portato a vincoli hardware che limitano lo sfruttamento dei modelli di apprendimento profondo al loro pieno potenziale. Progettare manualmente l‚Äôarchitettura del modello √® ancora pi√π difficile se si considerano la variet√† e le limitazioni dell‚Äôhardware. Ci√≤ ha portato alla creazione di Hardware-aware Neural Architecture Search che incorpora le contrazioni hardware nella loro ricerca e ottimizza lo spazio di ricerca per un hardware e una precisione specifici. HW-NAS pu√≤ essere categorizzato in base a come ottimizza per l‚Äôhardware. Esploreremo brevemente queste categorie e lasceremo dei link a documenti correlati per il lettore interessato.</p>
<section id="configurazione-single-target-fixed-platfrom" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="configurazione-single-target-fixed-platfrom">Configurazione Single Target, Fixed Platfrom</h4>
<p>L‚Äôobiettivo qui √® trovare la migliore architettura in termini di precisione ed efficienza hardware per un hardware target fisso. Per un hardware specifico, ad esempio Arduino Nicla Vision, questa categoria di HW-NAS cercher√† l‚Äôarchitettura che ottimizza precisione, latenza, consumo energetico, ecc.</p>
<section id="strategia-di-ricerca-hardware-aware" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="strategia-di-ricerca-hardware-aware">Strategia di Ricerca Hardware-aware</h5>
<p>Qui, la ricerca √® un problema di ottimizzazione multi-obiettivo, in cui sia l‚Äôaccuratezza che il costo dell‚Äôhardware guidano l‚Äôalgoritmo di ricerca per trovare l‚Äôarchitettura pi√π efficiente <span class="citation" data-cites="tan2019mnasnet cai2018proxylessnas wu2019fbnet">(<a href="../../references.it.html#ref-tan2019mnasnet" role="doc-biblioref">Tan et al. 2019</a>; <a href="../../references.it.html#ref-cai2018proxylessnas" role="doc-biblioref">Cai, Zhu, e Han 2019</a>; <a href="../../references.it.html#ref-wu2019fbnet" role="doc-biblioref">B. Wu et al. 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tan2019mnasnet" class="csl-entry" role="listitem">
Tan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, e Quoc V. Le. 2019. <span>¬´<span>MnasNet:</span> <span>Platform-aware</span> Neural Architecture Search for Mobile¬ª</span>. In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2820‚Äì28. IEEE. <a href="https://doi.org/10.1109/cvpr.2019.00293">https://doi.org/10.1109/cvpr.2019.00293</a>.
</div><div id="ref-cai2018proxylessnas" class="csl-entry" role="listitem">
Cai, Han, Ligeng Zhu, e Song Han. 2019. <span>¬´<span>ProxylessNAS:</span> <span>Direct</span> Neural Architecture Search on Target Task and Hardware¬ª</span>. In <em>7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net. <a href="https://openreview.net/forum?id=HylVB3AqYm">https://openreview.net/forum?id=HylVB3AqYm</a>.
</div><div id="ref-wu2019fbnet" class="csl-entry" role="listitem">
Wu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, e Yangqing Jia. 2019. <span>¬´<span>FBNet:</span> <span>Hardware-aware</span> Efficient <span>ConvNet</span> Design via Differentiable Neural Architecture Search¬ª</span>. In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10734‚Äì42. IEEE. <a href="https://doi.org/10.1109/cvpr.2019.01099">https://doi.org/10.1109/cvpr.2019.01099</a>.
</div></div></section>
<section id="spazio-di-ricerca-hardware-aware" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="spazio-di-ricerca-hardware-aware">Spazio di Ricerca Hardware-aware</h5>
<p>Qui, lo spazio di ricerca √® limitato alle architetture che funzionano bene sull‚Äôhardware specifico. Questo pu√≤ essere ottenuto misurando le prestazioni degli operatori (operatore Conv, operatore Pool, ‚Ä¶) o definendo un set di regole che limitano lo spazio di ricerca. <span class="citation" data-cites="zhang2020fast">(<a href="../../references.it.html#ref-zhang2020fast" role="doc-biblioref">L. L. Zhang et al. 2020</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2020fast" class="csl-entry" role="listitem">
Zhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, e Yunxin Liu. 2020. <span>¬´Fast Hardware-Aware Neural Architecture Search¬ª</span>. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>. IEEE. <a href="https://doi.org/10.1109/cvprw50498.2020.00354">https://doi.org/10.1109/cvprw50498.2020.00354</a>.
</div></div></section>
</section>
<section id="configurazioni-single-target-multiple-platform" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="configurazioni-single-target-multiple-platform">Configurazioni Single Target, Multiple Platform</h4>
<p>Alcuni hardware possono avere configurazioni diverse. Ad esempio, gli FPGA hanno blocchi logici configurabili (CLB) che possono essere configurati dal firmware. Questo metodo consente all‚ÄôHW-NAS di esplorare diverse configurazioni. <span class="citation" data-cites="jiang2019accuracy yang2020coexploration">(<a href="../../references.it.html#ref-jiang2019accuracy" role="doc-biblioref">Hu et al. 2023</a>; <a href="../../references.it.html#ref-yang2020coexploration" role="doc-biblioref">Ho Yoon et al. 2012</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-yang2020coexploration" class="csl-entry" role="listitem">
Ho Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song, Jun Yeong Seok, Kyung Jean Yoon, et al. 2012. <span>¬´Frontiers in Electronic Materials¬ª</span>. Wiley. <a href="https://doi.org/10.1002/9783527667703.ch67">https://doi.org/10.1002/9783527667703.ch67</a>.
</div></div></section>
<section id="target-multipli" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="target-multipli">Target Multipli</h4>
<p>Questa categoria mira a ottimizzare un singolo modello per pi√π hardware. Questo pu√≤ essere utile per lo sviluppo di dispositivi mobili in quanto pu√≤ ottimizzare diversi modelli di telefoni. <span class="citation" data-cites="chu2021discovering jiang2019accuracy">(<a href="../../references.it.html#ref-chu2021discovering" role="doc-biblioref">Chu et al. 2021</a>; <a href="../../references.it.html#ref-jiang2019accuracy" role="doc-biblioref">Hu et al. 2023</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-chu2021discovering" class="csl-entry" role="listitem">
Chu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. <span>¬´Discovering Multi-Hardware Mobile Models via Architecture Search¬ª</span>. In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 3022‚Äì31. IEEE. <a href="https://doi.org/10.1109/cvprw53098.2021.00337">https://doi.org/10.1109/cvprw53098.2021.00337</a>.
</div><div id="ref-jiang2019accuracy" class="csl-entry" role="listitem">
Hu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, e Jian Shi. 2023. <span>¬´Halide Perovskite Semiconductors¬ª</span>. Wiley. <a href="https://doi.org/10.1002/9783527829026.ch13">https://doi.org/10.1002/9783527829026.ch13</a>.
</div></div></section>
<section id="esempi-di-hardware-aware-neural-architecture-search" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="esempi-di-hardware-aware-neural-architecture-search">Esempi di ‚ÄúHardware-Aware Neural Architecture Search‚Äù</h4>
<section id="tinynas" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="tinynas">TinyNAS</h5>
<p>TinyNAS adotta un approccio in due fasi per trovare un‚Äôarchitettura ottimale per il modello tenendo a mente i vincoli del microcontrollore specifico.</p>
<p>Innanzitutto, TinyNAS genera pi√π spazi di ricerca variando la risoluzione di input del modello e il numero di canali dei layer. Quindi, TinyNAS sceglie uno spazio di ricerca in base ai FLOP (operazioni in virgola mobile al secondo) di ogni spazio di ricerca. Gli spazi con una probabilit√† maggiore di contenere architetture con un numero elevato di FLOP producono modelli con maggiore accuratezza: confrontare la linea rossa con la linea nera in <a href="#fig-search-space-flops" class="quarto-xref">Figura&nbsp;<span>9.33</span></a>. Poich√© un numero maggiore di FLOP significa che il modello ha una maggiore capacit√† di calcolo, √® pi√π probabile che il modello abbia una maggiore accuratezza.</p>
<p>Poi, TinyNAS esegue un‚Äôoperazione di ricerca sullo spazio scelto per trovare l‚Äôarchitettura ottimale per i vincoli specifici del microcontrollore. <span class="citation" data-cites="lin2020mcunet">(<a href="../../references.it.html#ref-lin2020mcunet" role="doc-biblioref">J. Lin et al. 2020</a>)</span></p>
<div class="no-row-height column-margin column-container"></div><div id="fig-search-space-flops" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-search-space-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_TinyNAS.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-search-space-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.33: Precisione degli spazi di ricerca. Fonte: <span class="citation" data-cites="lin2020mcunet">J. Lin et al. (<a href="../../references.it.html#ref-lin2020mcunet" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. <span>¬´<span>MCUNet:</span> <span>Tiny</span> Deep Learning on <span>IoT</span> Devices¬ª</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html</a>.
</div></div></figure>
</div>
</section>
</section>
<section id="topology-aware-nas" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="topology-aware-nas">Topology-Aware NAS</h4>
<p>Si concentra sulla creazione e l‚Äôottimizzazione di uno spazio di ricerca allineato alla topologia hardware del dispositivo. <span class="citation" data-cites="zhang2019autoshrink">(<a href="../../references.it.html#ref-zhang2019autoshrink" role="doc-biblioref">T. Zhang et al. 2020</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2019autoshrink" class="csl-entry" role="listitem">
Zhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Helen Li, e Yiran Chen. 2020. <span>¬´<span>AutoShrink:</span> <span>A</span> Topology-Aware <span>NAS</span> for Discovering Efficient Neural Architecture¬ª</span>. In <em>The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, 6829‚Äì36. AAAI Press. <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6163">https://aaai.org/ojs/index.php/AAAI/article/view/6163</a>.
</div></div></section>
</section>
<section id="sfide-nella-hardware-aware-neural-architecture-search" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="sfide-nella-hardware-aware-neural-architecture-search"><span class="header-section-number">9.4.2</span> Sfide nella ‚ÄúHardware-Aware Neural Architecture Search‚Äù</h3>
<p>Sebbene HW-NAS abbia un potenziale elevato per trovare architetture ottimali per TinyML, presenta alcuni problemi. Le metriche hardware come latenza, consumo energetico e utilizzo dell‚Äôhardware sono pi√π difficili da valutare rispetto alle metriche di accuratezza o di perdita. Spesso richiedono strumenti specializzati per misure precise. Inoltre, l‚Äôaggiunta di tutte queste metriche porta a uno spazio di ricerca molto pi√π grande. Ci√≤ fa s√¨ che HW-NAS sia dispendioso in termini di tempo e denaro. Deve essere applicato a ogni hardware per risultati ottimali, tra le altre cose, il che significa che se si deve distribuire il modello su pi√π dispositivi, la ricerca deve essere condotta pi√π volte e produrr√† modelli diversi, a meno che non si ottimizzi per tutti, il che significa una minore accuratezza. Infine, l‚Äôhardware cambia frequentemente e potrebbe essere necessario eseguire HW-NAS su ogni versione.</p>
</section>
<section id="ottimizzazioni-del-kernel" class="level3 page-columns page-full" data-number="9.4.3">
<h3 data-number="9.4.3" class="anchored" data-anchor-id="ottimizzazioni-del-kernel"><span class="header-section-number">9.4.3</span> Ottimizzazioni del Kernel</h3>
<p>Le ottimizzazioni del kernel sono modifiche apportate al kernel per migliorare le prestazioni dei modelli di apprendimento automatico su dispositivi con risorse limitate. Separeremo le ottimizzazioni del kernel in due tipi.</p>
<section id="ottimizzazioni-del-kernel-generali" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="ottimizzazioni-del-kernel-generali">Ottimizzazioni del kernel Generali</h4>
<p>Queste sono ottimizzazioni del kernel da cui tutti i dispositivi possono trarre vantaggio. Forniscono tecniche per convertire il codice in istruzioni pi√π efficienti.</p>
<section id="srotolamento-del-loop" class="level5">
<h5 class="anchored" data-anchor-id="srotolamento-del-loop">‚ÄúSrotolamento‚Äù del Loop</h5>
<p>Invece di avere un loop con ‚Äúloop control‚Äù (incrementando il contatore, si controlla la condizione di terminazione del loop), il loop pu√≤ essere srotolato e il sovraccarico del ‚Äúloop control‚Äù pu√≤ essere omesso. Questo pu√≤ anche fornire ulteriori opportunit√† di parallelismo che potrebbero non essere possibili con la struttura con loop. Questo pu√≤ essere particolarmente utile per loop stretti, in cui il corpo del loop √® un piccolo numero di istruzioni con molte iterazioni.</p>
</section>
<section id="blocking" class="level5">
<h5 class="anchored" data-anchor-id="blocking">Blocking</h5>
<p>Il Blocking viene utilizzato per rendere pi√π efficienti i pattern di accesso alla memoria. Se abbiamo tre calcoli, il primo e l‚Äôultimo devono accedere alla cache A e il secondo deve accedere alla cache B, il ‚Äúblocking‚Äù ferma i primi due calcoli per ridurre il numero di letture di memoria necessarie.</p>
</section>
<section id="tiling" class="level5">
<h5 class="anchored" data-anchor-id="tiling">Tiling</h5>
<p>Analogamente al blocking, il tiling [piastrellatura] divide i dati e il calcolo in blocchi, ma si estende oltre i miglioramenti della cache. Il tiling crea partizioni di calcolo indipendenti che possono essere eseguite in parallelo, il che pu√≤ comportare significativi miglioramenti delle prestazioni.</p>
</section>
<section id="librerie-kernel-ottimizzate" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="librerie-kernel-ottimizzate">Librerie Kernel Ottimizzate</h5>
<p>Questo comprende lo sviluppo di kernel ottimizzati che sfruttano appieno un hardware specifico. Un esempio √® la libreria CMSIS-NN, che √® una raccolta di kernel di reti neurali efficienti sviluppati per ottimizzare le prestazioni e ridurre al minimo l‚Äôingombro di memoria dei modelli sui processori Arm Cortex-M, comuni sui dispositivi edge IoT. Il kernel sfrutta pi√π capacit√† hardware dei processori Cortex-M come Single Instruction Multiple Data (SIMD), Floating Point Unit (FPU) e M-Profile Vector Extensions (MVE). Queste ottimizzazioni rendono pi√π efficienti le operazioni comuni come le moltiplicazioni di matrici, aumentando le prestazioni delle operazioni del modello sui processori Cortex-M. <span class="citation" data-cites="lai2018cmsisnn">(<a href="../../references.it.html#ref-lai2018cmsisnn" role="doc-biblioref">Lai, Suda, e Chandra 2018</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-lai2018cmsisnn" class="csl-entry" role="listitem">
Lai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. <span>¬´<span>CMSIS</span>-<span>NN:</span> <span>Efficient</span> Neural Network Kernels for Arm Cortex-M <span>CPUs</span>¬ª</span>. <a href="https://arxiv.org/abs/1801.06601">https://arxiv.org/abs/1801.06601</a>.
</div></div></section>
</section>
</section>
<section id="compute-in-memory-cim" class="level3 page-columns page-full" data-number="9.4.4">
<h3 data-number="9.4.4" class="anchored" data-anchor-id="compute-in-memory-cim"><span class="header-section-number">9.4.4</span> Compute-in-Memory (CiM)</h3>
<p>Questo √® un esempio di progettazione congiunta di algoritmo e hardware. CiM √® un paradigma di elaborazione che esegue calcoli all‚Äôinterno della memoria. Pertanto, le architetture CiM consentono di eseguire operazioni direttamente sui dati archiviati, senza la necessit√† di spostare i dati avanti e indietro tra unit√† di elaborazione e memoria separate. Questo paradigma di progettazione √® particolarmente utile in scenari in cui lo spostamento dei dati √® una fonte primaria di consumo energetico e latenza, come nelle applicazioni TinyML su dispositivi edge. <a href="#fig-computing-memory" class="quarto-xref">Figura&nbsp;<span>9.34</span></a> √® un esempio di utilizzo di CiM in TinyML: l‚Äôindividuazione delle parole chiave richiede un processo sempre attivo che cerca determinate parole di attivazione (come ‚ÄúHey, Siri‚Äù). Data la natura ad alta intensit√† di risorse di questa attivit√†, l‚Äôintegrazione di CiM per il modello di rilevamento delle parole chiave sempre attivo pu√≤ migliorare l‚Äôefficienza.</p>
<p>Attraverso la progettazione congiunta di algoritmo e hardware, gli algoritmi possono essere ottimizzati per sfruttare le caratteristiche uniche delle architetture CiM e, l‚Äôhardware CiM pu√≤ essere personalizzato o configurato per supportare meglio i requisiti di elaborazione e le caratteristiche degli algoritmi. Ci√≤ si ottiene utilizzando le propriet√† analogiche delle celle di memoria, come l‚Äôaddizione e la moltiplicazione nella DRAM. <span class="citation" data-cites="zhou2021analognets">(<a href="../../references.it.html#ref-zhou2021analognets" role="doc-biblioref">Zhou et al. 2021</a>)</span></p>
<div class="no-row-height column-margin column-container"></div><div id="fig-computing-memory" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-computing-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_CiM.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-computing-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.34: CiM per l‚Äôindividuazione delle parole chiave. Fonte: <span class="citation" data-cites="zhou2021analognets">Zhou et al. (<a href="../../references.it.html#ref-zhou2021analognets" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-zhou2021analognets" class="csl-entry" role="listitem">
Zhou, Chuteng, Fernando Garcia Redondo, Julian B√ºchel, Irem Boybat, Xavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian, Manuel Le Gallo, e Paul N. Whatmough. 2021. <span>¬´<span>AnalogNets:</span> <span>Ml-hw</span> Co-Design of Noise-robust <span>TinyML</span> Models and Always-On Analog Compute-in-Memory Accelerator¬ª</span>. <a href="https://arxiv.org/abs/2111.06503">https://arxiv.org/abs/2111.06503</a>.
</div></div></figure>
</div>
</section>
<section id="ottimizzazione-dellaccesso-alla-memoria" class="level3 page-columns page-full" data-number="9.4.5">
<h3 data-number="9.4.5" class="anchored" data-anchor-id="ottimizzazione-dellaccesso-alla-memoria"><span class="header-section-number">9.4.5</span> Ottimizzazione dell‚ÄôAccesso alla Memoria</h3>
<p>Dispositivi diversi possono avere gerarchie di memorie diverse. L‚Äôottimizzazione per la gerarchia di memoria specifica nell‚Äôhardware specifico pu√≤ portare a grandi miglioramenti delle prestazioni riducendo le costose operazioni di lettura e scrittura nella memoria. L‚Äôottimizzazione del flusso di dati pu√≤ essere ottenuta ottimizzando il riutilizzo dei dati all‚Äôinterno di un singolo layer e tra pi√π layer. Questa ottimizzazione del flusso di dati pu√≤ essere adattata alla gerarchia di memoria specifica dell‚Äôhardware, il che pu√≤ portare a maggiori vantaggi rispetto alle ottimizzazioni generali per diversi hardware.</p>
<section id="sfruttamento-dei-dati-sparsi" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sfruttamento-dei-dati-sparsi">Sfruttamento dei Dati Sparsi</h4>
<p>Il Pruning [potatura] √® un approccio fondamentale per comprimere i modelli e renderli compatibili con dispositivi con risorse limitate. Ci√≤ si traduce in modelli sparsi in cui molti pesi sono 0. Pertanto, sfruttare questa diradazione pu√≤ portare a miglioramenti significativi nelle prestazioni. Sono stati creati degli strumenti per ottenere esattamente questo. RAMAN, √® un acceleratore TinyML sparse progettato per l‚Äôinferenza su dispositivi edge. RAMAN sovrappone le attivazioni di input e output sullo stesso spazio di memoria, riducendo i requisiti di archiviazione fino al 50%. <span class="citation" data-cites="krishna2023raman">(<a href="../../references.it.html#ref-krishna2023raman" role="doc-biblioref">Krishna et al. 2023</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-krishna2023raman" class="csl-entry" role="listitem">
Krishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh Dwivedi, Andr√© van Schaik, Mahesh Mehendale, e Chetan Singh Thakur. 2023. <span>¬´<span>RAMAN:</span> <span>A</span> Re-configurable and Sparse <span>TinyML</span> Accelerator for Inference on Edge¬ª</span>. <a href="https://arxiv.org/abs/2306.06493">https://arxiv.org/abs/2306.06493</a>.
</div></div></section>
<section id="framework-di-ottimizzazione" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="framework-di-ottimizzazione">Framework di Ottimizzazione</h4>
<p>I framework di ottimizzazione sono stati introdotti per sfruttare le capacit√† specifiche dell‚Äôhardware per accelerare il software. Un esempio di tale framework √® hls4ml: <a href="#fig-hls4ml-workflow" class="quarto-xref">Figura&nbsp;<span>9.35</span></a> fornisce una panoramica del flusso di lavoro del framework. Questo flusso di lavoro di co-progettazione software-hardware open source aiuta a interpretare e tradurre algoritmi di machine learning per l‚Äôimplementazione con tecnologie FPGA e ASIC. Funzionalit√† quali ottimizzazione di rete, nuove API Python, potatura consapevole della quantizzazione e flussi di lavoro FPGA end-to-end sono integrate nel framework hls4ml, sfruttando unit√† di elaborazione parallele, gerarchie di memoria e set di istruzioni specializzati per ottimizzare i modelli per hardware edge. Inoltre, hls4ml √® in grado di tradurre algoritmi di apprendimento automatico direttamente nel firmware FPGA.</p>
<div id="fig-hls4ml-workflow" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hls4ml-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_hls4ml.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hls4ml-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.35: workflow del framework hls4ml. Fonte: <span class="citation" data-cites="fahim2021hls4ml">Fahim et al. (<a href="../../references.it.html#ref-fahim2021hls4ml" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-fahim2021hls4ml" class="csl-entry" role="listitem">
Fahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, et al. 2021. <span>¬´hls4ml: <span>An</span> Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices¬ª</span>. <a href="https://arxiv.org/abs/2103.05579">https://arxiv.org/abs/2103.05579</a>.
</div></div></figure>
</div>
<p>Un altro framework per FPGA che si concentra su un approccio olistico √® CFU Playground <span class="citation" data-cites="prakash2022cfu">(<a href="../../references.it.html#ref-prakash2022cfu" role="doc-biblioref">Prakash et al. 2023</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-prakash2022cfu" class="csl-entry" role="listitem">
Prakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. <span>¬´<span>CFU</span> Playground: <span>Full-stack</span> Open-Source Framework for Tiny Machine Learning <span>(TinyML)</span> Acceleration on <span>FPGAs</span>¬ª</span>. In <em>2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</em>. Vol. abs/2201.01863. IEEE. <a href="https://doi.org/10.1109/ispass57527.2023.00024">https://doi.org/10.1109/ispass57527.2023.00024</a>.
</div></div></section>
<section id="hardware-costruito-attorno-al-software" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="hardware-costruito-attorno-al-software">Hardware Costruito Attorno al Software</h4>
<p>In un approccio contrastante, l‚Äôhardware pu√≤ essere progettato su misura attorno ai requisiti software per ottimizzare le prestazioni per un‚Äôapplicazione specifica. Questo paradigma crea hardware specializzato per adattarsi meglio alle specifiche del software, riducendo cos√¨ il sovraccarico computazionale e migliorando l‚Äôefficienza operativa. Un esempio di questo approccio √® un‚Äôapplicazione di riconoscimento vocale di <span class="citation" data-cites="kwon2021hardwaresoftware">(<a href="../../references.it.html#ref-kwon2021hardwaresoftware" role="doc-biblioref">Kwon e Park 2021</a>)</span>. Il documento propone una struttura in cui le operazioni di pre-elaborazione, tradizionalmente gestite dal software, sono assegnate ad un hardware progettato su misura. Questa tecnica √® stata ottenuta introducendo la logica resistore-transistor in un modulo audio a circuito inter-integrato per il windowing e l‚Äôacquisizione di dati audio grezzi nell‚Äôapplicazione di riconoscimento vocale. Di conseguenza, questa ‚Äúdelega‚Äù delle operazioni di pre-elaborazione ha portato a una riduzione del carico computazionale sul software, mostrando un‚Äôapplicazione pratica della creazione di hardware attorno al software per migliorare l‚Äôefficienza e le prestazioni.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-fpga-preprocessing" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-fpga-preprocessing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_preprocessor.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fpga-preprocessing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.36: Delega dell‚Äôelaborazione dei dati a un FPGA. Fonte: <span class="citation" data-cites="kwon2021hardwaresoftware">Kwon e Park (<a href="../../references.it.html#ref-kwon2021hardwaresoftware" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-kwon2021hardwaresoftware" class="csl-entry" role="listitem">
Kwon, Jisu, e Daejin Park. 2021. <span>¬´<span>Hardware/Software</span> Co-Design for <span>TinyML</span> Voice-Recognition Application on Resource Frugal Edge Devices¬ª</span>. <em>Applied Sciences</em> 11 (22): 11073. <a href="https://doi.org/10.3390/app112211073">https://doi.org/10.3390/app112211073</a>.
</div></div></figure>
</div>
</section>
<section id="splitnet" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="splitnet">SplitNet</h4>
<p>Li SplitNet sono state introdotte nel contesto dei sistemi Head-Mounted. Distribuiscono il carico di lavoro delle Deep Neural Network (DNN) tra i sensori della telecamera e un aggregatore. Ci√≤ √® particolarmente interessante nel contesto di TinyML. Il framework SplitNet √® un NAS split-aware per trovare l‚Äôarchitettura di rete neurale ottimale per ottenere una buona accuratezza, dividere il modello tra i sensori e l‚Äôaggregatore e ridurre al minimo la comunicazione tra i sensori e l‚Äôaggregatore. <a href="#fig-splitnet-performance" class="quarto-xref">Figura&nbsp;<span>9.37</span></a> dimostra come le SplitNet (in rosso) ottengano una maggiore accuratezza per una latenza inferiore (in esecuzione su ImageNet) rispetto ad altri approcci, come l‚Äôesecuzione del DNN sul sensore (All-on-sensor; in verde) o sul cellulare (All-on-aggregator; in blu). La comunicazione minima √® importante in TinyML dove la memoria √® fortemente limitata, in questo modo i sensori conducono parte dell‚Äôelaborazione sui loro chip e poi inviano solo le informazioni necessarie all‚Äôaggregatore. Durante i test su ImageNet, SplitNets √® stato in grado di ridurre la latenza di un ordine di grandezza sui dispositivi di visione artificiale montati sulla testa [occhiali o visori]. Ci√≤ pu√≤ essere utile quando il sensore ha il suo chip. <span class="citation" data-cites="dong2022splitnets">(<a href="../../references.it.html#ref-dong2022splitnets" role="doc-biblioref">Dong et al. 2022</a>)</span></p>
<div class="no-row-height column-margin column-container"></div><div id="fig-splitnet-performance" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-splitnet-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_SplitNets.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-splitnet-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.37: Le SplitNet rispetto ad altri approcci. Fonte: <span class="citation" data-cites="dong2022splitnets">Dong et al. (<a href="../../references.it.html#ref-dong2022splitnets" role="doc-biblioref">2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-dong2022splitnets" class="csl-entry" role="listitem">
Dong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T. Kung, e Ziyun Li. 2022. <span>¬´<span>SplitNets:</span> <span>Designing</span> Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems¬ª</span>. In <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 12549‚Äì59. IEEE. <a href="https://doi.org/10.1109/cvpr52688.2022.01223">https://doi.org/10.1109/cvpr52688.2022.01223</a>.
</div></div></figure>
</div>
</section>
<section id="hardware-specifico-per-il-data-augmentation" class="level4">
<h4 class="anchored" data-anchor-id="hardware-specifico-per-il-data-augmentation">Hardware Specifico per il ‚ÄúData Augmentation‚Äù</h4>
<p>Ogni dispositivo edge pu√≤ possedere caratteristiche di sensore uniche, che portano a specifici pattern di rumore che possono influire sulle prestazioni del modello. Un esempio sono i dati audio, in cui sono prevalenti le variazioni derivanti dalla scelta del microfono. Applicazioni come le Keyword Spotting possono sperimentare miglioramenti sostanziali incorporando dati registrati da dispositivi simili a quelli destinati all‚Äôimplementazione. La messa a punto dei modelli esistenti pu√≤ essere impiegata per adattare i dati in modo preciso alle caratteristiche distintive del sensore.</p>
</section>
</section>
</section>
<section id="supporto-software-e-framework" class="level2 page-columns page-full" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="supporto-software-e-framework"><span class="header-section-number">9.5</span> Supporto Software e Framework</h2>
<p>Sebbene tutte le tecniche sopra menzionate come <a href="#sec-pruning">pruning</a>, <a href="#sec-quant">quantizzazione</a> e numeri efficienti siano ben note, rimarrebbero poco pratiche e inaccessibili senza un ampio supporto software. Ad esempio, la quantizzazione diretta di pesi e attivazioni in un modello richiederebbe la modifica manuale della definizione del modello e l‚Äôinserimento di operazioni di quantizzazione. Allo stesso modo, la potatura diretta dei pesi del modello richiede la manipolazione dei tensori dei pesi. Tali approcci noiosi diventano impraticabili su larga scala.</p>
<p>Senza l‚Äôampia innovazione software nei framework, negli strumenti di ottimizzazione e nell‚Äôintegrazione hardware, la maggior parte di queste tecniche rimarrebbe teorica o praticabile solo per gli esperti. Senza API del framework e automazione per semplificare l‚Äôapplicazione di queste ottimizzazioni, non verrebbero adottate. Il supporto software le rende accessibili al pubblico e sblocca vantaggi concreti. Inoltre, problemi come la messa a punto degli iperparametri per la potatura, la gestione del compromesso tra dimensioni del modello e accuratezza e la garanzia della compatibilit√† con i dispositivi target pongono ostacoli che gli sviluppatori devono superare.</p>
<section id="api-native-di-ottimizzazione" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="api-native-di-ottimizzazione"><span class="header-section-number">9.5.1</span> API Native di Ottimizzazione</h3>
<p>I principali framework di machine learning come TensorFlow, PyTorch e MXNet forniscono librerie e API per consentire l‚Äôapplicazione di tecniche comuni di ottimizzazione dei modelli senza richiedere implementazioni personalizzate. Ad esempio, TensorFlow offre il TensorFlow Model Optimization Toolkit che contiene moduli come:</p>
<ul>
<li><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/quantize_model">quantization</a> - Applica un training che tiene conto della quantizzazione per convertire i modelli in virgola mobile in una precisione inferiore come int8 con una perdita di accuratezza minima. Gestisce la quantizzazione del peso e dell‚Äôattivazione.</li>
<li><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras">sparsity</a> - Fornisce API di potatura per indurre la ‚Äúsparsit√†‚Äù e rimuovere connessioni non necessarie in modelli come le reti neurali. Pu√≤ potare pesi, livelli, ecc.</li>
<li><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/clustering">clustering</a> - Supporta la compressione del modello raggruppando i pesi per tassi di compressione pi√π elevati.</li>
</ul>
<p>Queste API consentono agli utenti di abilitare tecniche di ottimizzazione come la quantizzazione e la potatura senza modificare direttamente il codice del modello. √à possibile configurare parametri come i tassi di ‚Äúsparsit√†‚Äù del target, le larghezze di bit di quantizzazione, ecc. Allo stesso modo, PyTorch fornisce torch.quantization per convertire i modelli in rappresentazioni di precisione inferiore. TorchTensor e TorchModule formano le classi di base per il supporto della quantizzazione. Offre inoltre torch.nn.utils.prune per la potatura nativa dei modelli. MXNet offre layer gluon.contrib che aggiungono funzionalit√† di quantizzazione come l‚Äôarrotondamento a punto fisso e l‚Äôarrotondamento stocastico di pesi/attivazioni durante l‚Äôaddestramento. Ci√≤ consente di includere facilmente la quantizzazione nei modelli gluon.</p>
<p>Il vantaggio principale delle ottimizzazioni integrate √® che gli utenti possono applicarle senza dover reimplementare tecniche complesse. Ci√≤ rende i modelli ottimizzati accessibili a un‚Äôampia gamma di professionisti. Garantisce inoltre che le best practice siano seguite basandosi sulla ricerca e sull‚Äôesperienza nell‚Äôimplementazione dei metodi. Man mano che emergono nuove ottimizzazioni, i framework si sforzano di fornire supporto nativo e API ove possibile per abbassare ulteriormente la barriera verso un ML efficiente. La disponibilit√† di questi strumenti √® fondamentale per un‚Äôadozione diffusa.</p>
</section>
<section id="strumenti-di-ottimizzazione-automatizzata" class="level3 page-columns page-full" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="strumenti-di-ottimizzazione-automatizzata"><span class="header-section-number">9.5.2</span> Strumenti di Ottimizzazione Automatizzata</h3>
<p>Gli strumenti di ottimizzazione automatizzati forniti dai framework possono analizzare i modelli e applicare automaticamente ottimizzazioni come quantizzazione, potatura e fusione degli operatori per rendere il processo pi√π semplice e accessibile senza un‚Äôeccessiva messa a punto manuale. In effetti, questo si basa sulla sezione precedente. Ad esempio, TensorFlow fornisce il TensorFlow Model Optimization Toolkit che contiene moduli come:</p>
<ul>
<li><a href="https://www.tensorflow.org/model_optimization/guide/quantization/training">QuantizationAwareTraining</a> - Quantizza automaticamente pesi e attivazioni in un modello per ridurre la precisione come UINT8 o INT8 con una perdita di accuratezza minima. Inserisce nodi di quantizzazione falsi durante l‚Äôaddestramento in modo che il modello possa imparare a essere compatibile con la quantizzazione.</li>
<li><a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras">Pruning</a> - Rimuove automaticamente le connessioni non necessarie in un modello in base all‚Äôanalisi dell‚Äôimportanza del peso. Pu√≤ potare interi filtri in livelli convoluzionali o ‚Äúattention head‚Äù [teste di attenzione] nei trasformatori. Gestisce il ri-addestramento iterativo per recuperare qualsiasi perdita di accuratezza.</li>
<li><a href="https://www.tensorflow.org/guide/graph_optimization">GraphOptimizer</a> - Applica ottimizzazioni grafiche come la fusione degli operatori per consolidare le operazioni e ridurre la latenza di esecuzione, in particolare per l‚Äôinferenza. In <a href="#fig-graph-optimizer" class="quarto-xref">Figura&nbsp;<span>9.38</span></a>, si pu√≤ vedere l‚Äôoriginale (Source Graph) a sinistra e come le sue operazioni vengono trasformate (consolidate) a destra. Notare come Block1 in Source Graph abbia 3 passaggi separati (Convolution, BiasAdd e Activation), che vengono poi consolidati insieme in Block1 su Optimized Graph.</li>
</ul>
<div id="fig-graph-optimizer" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-graph-optimizer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/source_opt.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graph-optimizer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.38: GraphOptimizer. Fonte: <span class="citation" data-cites="annette2020">Wess et al. (<a href="../../references.it.html#ref-annette2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-annette2020" class="csl-entry" role="listitem">
Wess, Matthias, Matvey Ivanov, Christoph Unger, e Anvesh Nookala. 2020. <span>¬´<span>ANNETTE:</span> <span>Accurate</span> Neural Network Execution Time Estimation with Stacked Models¬ª</span>. <em>IEEE</em>. <a href="https://doi.org/10.1109/ACCESS.2020.3047259">https://doi.org/10.1109/ACCESS.2020.3047259</a>.
</div></div></figure>
</div>
<p>Questi moduli automatizzati richiedono solo all‚Äôutente di fornire il modello originale in virgola mobile e di gestire la pipeline di ottimizzazione end-to-end, inclusa qualsiasi riqualificazione per ripristinare la precisione. Anche altri framework come PyTorch offrono un crescente supporto all‚Äôautomazione, ad esempio tramite torch.quantization.quantize_dynamic. L‚Äôottimizzazione automatizzata rende l‚Äôapprendimento automatico efficiente accessibile ai professionisti senza competenze di ottimizzazione.</p>
</section>
<section id="librerie-di-ottimizzazione-hardware" class="level3" data-number="9.5.3">
<h3 data-number="9.5.3" class="anchored" data-anchor-id="librerie-di-ottimizzazione-hardware"><span class="header-section-number">9.5.3</span> Librerie di Ottimizzazione Hardware</h3>
<p>Librerie hardware come TensorRT e TensorFlow XLA consentono di ottimizzare i modelli per l‚Äôhardware target tramite tecniche di cui abbiamo discusso in precedenza.</p>
<ul>
<li><p><strong>Quantizzazione:</strong> Ad esempio, TensorRT e TensorFlow Lite supportano entrambi la quantizzazione dei modelli durante la conversione nel loro formato. Ci√≤ fornisce accelerazioni sui SoC mobili con supporto INT8/INT4.</p></li>
<li><p><strong>Ottimizzazione del Kernel:</strong> ad esempio, TensorRT esegue l‚Äôauto-tuning per ottimizzare i kernel CUDA in base all‚Äôarchitettura GPU per ogni layer nel grafo del modello. Ci√≤ estrae la massima produttivit√†.</p></li>
<li><p><strong>Fusione degli Operatori:</strong> TensorFlow XLA esegue una fusione aggressiva per creare un binario ottimizzato per le TPU. Sui dispositivi mobili, framework come NCNN supportano anche operatori fusi [unificati].</p></li>
<li><p><strong>Codice Specifico per l‚ÄôHardware:</strong> Le librerie vengono utilizzate per generare codice binario ottimizzato specializzato per l‚Äôhardware target. Per esempio, <a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html">TensorRT</a> usa librerie Nvidia CUDA/cuDNN che sono ottimizzate manualmente per ogni architettura GPU. Questa codifica specifica per hardware √® fondamentale per le prestazioni. Sui dispositivi TinyML, questo pu√≤ significare codice assembly ottimizzato per una CPU Cortex M4, ad esempio. I fornitori forniscono CMSIS-NN e altre librerie.</p></li>
<li><p><strong>Ottimizzazioni del Layout dei Dati:</strong> Possiamo sfruttare in modo efficiente la gerarchia della memoria di hardware come cache e registri tramite tecniche come riorganizzazione tensore/peso, tiling e riutilizzo. Ad esempio, TensorFlow XLA ottimizza i layout dei buffer per massimizzare l‚Äôutilizzo della TPU. Questo aiuta qualsiasi sistema con limiti di memoria.</p></li>
<li><p><strong>Ottimizzazione Basata sulla Profilazione:</strong> Possiamo usare strumenti di profilazione per identificare i colli di bottiglia. Ad esempio, regolare i livelli di fusione del kernel in base alla profilazione della latenza. Sui SoC mobili, fornitori come Qualcomm forniscono profiler in SNPE per trovare opportunit√† di ottimizzazione nelle CNN. Questo approccio basato sui dati √® importante per le prestazioni.</p></li>
</ul>
<p>Integrando i modelli di framework con queste librerie hardware tramite pipeline di conversione ed esecuzione, gli sviluppatori di ML possono ottenere significativi incrementi di velocit√† e guadagni di efficienza da ottimizzazioni di basso livello su misura per l‚Äôhardware target. La stretta integrazione tra software e hardware √® fondamentale per consentire un‚Äôimplementazione performante delle applicazioni di ML, in particolare su dispositivi mobili e TinyML.</p>
</section>
<section id="visualizzazione-delle-ottimizzazioni" class="level3 page-columns page-full" data-number="9.5.4">
<h3 data-number="9.5.4" class="anchored" data-anchor-id="visualizzazione-delle-ottimizzazioni"><span class="header-section-number">9.5.4</span> Visualizzazione delle Ottimizzazioni</h3>
<p>L‚Äôimplementazione di tecniche di ottimizzazione del modello senza visibilit√† degli effetti sul modello pu√≤ essere impegnativa. Strumenti dedicati o strumenti di visualizzazione possono fornire informazioni critiche e utili sulle modifiche del modello e aiutano a tracciare il processo di ottimizzazione. Consideriamo le ottimizzazioni che abbiamo considerato in precedenza, come la potatura per la ‚Äúsparsity‚Äù [diradazione] e la quantizzazione.</p>
<section id="sparsit√†" class="level5">
<h5 class="anchored" data-anchor-id="sparsit√†">Sparsit√†</h5>
<p>Ad esempio, si considerino le ottimizzazioni di sparsity. Gli strumenti di visualizzazione di sparsity possono fornire informazioni critiche sui modelli potati, mappando esattamente quali pesi sono stati rimossi. Ad esempio, le mappe di calore di sparsity possono utilizzare gradienti di colore per indicare la percentuale di pesi potati in ogni layer di una rete neurale. I layer con percentuali di potatura pi√π elevate appaiono pi√π scuri (cfr. <a href="#fig-sprase-heat-map" class="quarto-xref">Figura&nbsp;<span>9.39</span></a>). Questo identifica quali layer sono stati semplificati di pi√π tramite potatura (<a href="https://www.numenta.com/blog/2020/10/30/case-for-sparsity-in-neural-networks-part-2-dynamic-sparsity/">Souza 2020</a>).</p>
<div id="fig-sprase-heat-map" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sprase-heat-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://www.numenta.com/wp-content/uploads/2020/10/Picture1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sprase-heat-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.39: Mappa ‚Äútermica‚Äù della rete sparsa. Fonte: <a href="https://www.numenta.com/blog/2020/10/30/case-for-sparsity-in-neural-networks-part-2-dynamic-sparsity/">Numenta</a>.
</figcaption>
</figure>
</div>
<p>I grafici di tendenza possono anche tracciare la scarsit√† nei successivi round di potatura: possono mostrare una rapida potatura iniziale seguita da incrementi pi√π graduali. Il tracciamento della diradazione globale corrente insieme a statistiche come la diradazione media, minima e massima per ogli layer in tabelle o grafici fornisce una panoramica della composizione del modello. Per una rete convoluzionale di esempio, questi strumenti potrebbero rivelare che il primo layer di convoluzione viene potato del 20% mentre quello di classificazione finale viene potato del 70% data la sua ridondanza. La diradazione del modello globale pu√≤ aumentare dal 10% dopo la potatura iniziale al 40% dopo cinque round.</p>
<p>Rendendo i dati di diradazione visivamente accessibili, i professionisti possono comprendere meglio esattamente come il loro modello viene ottimizzato e quali aree vengono interessate. La visibilit√† consente loro di mettere a punto e controllare il processo di potatura per una determinata architettura.</p>
<p>La visualizzazione della diradazione trasforma la potatura in una tecnica trasparente anzich√© in un‚Äôoperazione ‚Äúblack-box‚Äù.</p>
</section>
<section id="quantizzazione-1" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="quantizzazione-1">Quantizzazione</h5>
<p>La conversione di modelli in precisioni numeriche inferiori tramite quantizzazione introduce errori che possono influire sulla precisione del modello se non vengono monitorati e affrontati correttamente. La visualizzazione delle distribuzioni degli errori di quantizzazione fornisce informazioni preziose sugli effetti dei numeri di precisione ridotti applicati a diverse parti di un modello. Per questo, √® possibile generare istogrammi degli errori di quantizzazione per pesi e attivazioni. Questi istogrammi possono rivelare la forma della distribuzione degli errori, se assomigliano a una distribuzione gaussiana o contengono valori anomali e picchi significativi. <a href="#fig-quantization-error" class="quarto-xref">Figura&nbsp;<span>9.40</span></a> mostra le distribuzioni di diversi metodi di quantizzazione. Valori anomali elevati possono indicare problemi con particolari layer che gestiscono la quantizzazione. Il confronto degli istogrammi tra layer evidenzia eventuali aree problematiche che si distinguono con errori anormalmente elevati.</p>
<div id="fig-quantization-error" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-quantization-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_quant_hist.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.40: Errori di Quantizzazione. Fonte: <span class="citation" data-cites="kuzmin2022fp8">Kuzmin et al. (<a href="../../references.it.html#ref-kuzmin2022fp8" role="doc-biblioref">2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-kuzmin2022fp8" class="csl-entry" role="listitem">
Kuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, e Tijmen Blankevoort. 2022. <span>¬´<span>FP8</span> Quantization: <span>The</span> Power of the Exponent¬ª</span>. <a href="https://arxiv.org/abs/2208.09225">https://arxiv.org/abs/2208.09225</a>.
</div></div></figure>
</div>
<p>Le visualizzazioni di attivazione sono importanti anche per rilevare problemi di overflow. Con la mappatura a colori delle attivazioni prima e dopo la quantizzazione, tutti i valori spinti al di fuori degli intervalli previsti diventano visibili. Ci√≤ rivela problemi di saturazione e troncamento che potrebbero alterare le informazioni che fluiscono attraverso il modello. Il rilevamento di questi errori consente di ricalibrare le attivazioni per evitare la perdita di informazioni (<a href="https://medium.com/exemplifyml-ai/visualizing-neural-network-activation-a27caa451ff">Mandal 2022</a>). <a href="#fig-color-mapping" class="quarto-xref">Figura&nbsp;<span>9.41</span></a> √® una mappatura a colori dei kernel convoluzionali AlexNet.</p>
<div id="fig-color-mapping" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-color-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://compsci697l.github.io/assets/cnnvis/filt1.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-color-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.41: Mappatura a colori delle attivazioni. Fonte: <span class="citation" data-cites="alexnet2012">Krizhevsky, Sutskever, e Hinton (<a href="../../references.it.html#ref-alexnet2012" role="doc-biblioref">2017</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-alexnet2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2017. <span>¬´<span>ImageNet</span> classification with deep convolutional neural networks¬ª</span>. A cura di F. Pereira, C. J. Burges, L. Bottou, e K. Q. Weinberger. <em>Commun. ACM</em> 60 (6): 84‚Äì90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div></div></figure>
</div>
<p>Altre tecniche, come il tracciamento dell‚Äôerrore di quantizzazione quadratico medio complessivo a ogni passaggio del processo di addestramento consapevole della quantizzazione, identificano fluttuazioni e divergenze. Picchi improvvisi nel grafico di tracciamento possono indicare punti in cui la quantizzazione sta interrompendo l‚Äôaddestramento del modello. Il monitoraggio di questa metrica crea intuizione sul comportamento del modello in fase di quantizzazione. Insieme, queste tecniche trasformano la quantizzazione in un processo trasparente. Le intuizioni empiriche consentono ai professionisti di valutare correttamente gli effetti della quantizzazione. Individuano le aree dell‚Äôarchitettura del modello o del processo di training da ricalibrare in base ai problemi di quantizzazione osservati. Ci√≤ aiuta a ottenere modelli quantizzati numericamente stabili e accurati.</p>
<p>Fornire questi dati consente ai professionisti di valutare correttamente l‚Äôimpatto della quantizzazione e identificare potenziali aree problematiche del modello da ricalibrare o riprogettare per renderlo pi√π adatto alla quantizzazione. Questa analisi empirica sviluppa l‚Äôintuizione sul raggiungimento di una quantizzazione ottimale.</p>
<p>Gli strumenti di visualizzazione possono fornire approfondimenti che aiutano i professionisti a comprendere meglio gli effetti delle ottimizzazioni sui loro modelli. La visibilit√† consente di correggere i problemi in anticipo prima che l‚Äôaccuratezza o le prestazioni siano influenzate in modo significativo. Aiuta anche ad applicare le ottimizzazioni in modo pi√π efficace per modelli specifici. Queste analisi di ottimizzazione aiutano a sviluppare l‚Äôintuizione quando si trasferiscono i modelli a rappresentazioni pi√π efficienti.</p>
</section>
</section>
<section id="conversione-e-distribuzione-del-modello" class="level3" data-number="9.5.5">
<h3 data-number="9.5.5" class="anchored" data-anchor-id="conversione-e-distribuzione-del-modello"><span class="header-section-number">9.5.5</span> Conversione e Distribuzione del Modello</h3>
<p>Una volta che i modelli sono stati ottimizzati con successo in framework come TensorFlow e PyTorch, sono necessarie piattaforme specializzate di conversione e deployment [distribuzione] del modello per colmare il divario con l‚Äôesecuzione sui dispositivi target.</p>
<p>TensorFlow Lite - La piattaforma di TensorFlow per convertire i modelli in un formato leggero ottimizzato per dispositivi mobili, embedded ed edge. Supporta ottimizzazioni come quantizzazione, fusione del kernel e rimozione di operazioni inutilizzate. I modelli possono essere eseguiti utilizzando kernel TensorFlow Lite ottimizzati sull‚Äôhardware del dispositivo. Fondamentale per la distribuzione mobile e TinyML.</p>
<p>ONNX Runtime - Esegue la conversione e l‚Äôinferenza per i modelli nel formato ‚Äúopen ONNX‚Äù. Fornisce kernel ottimizzati, supporta acceleratori hardware come GPU e distribuzione multipiattaforma dal cloud all‚Äôedge. Consente la distribuzione indipendente dal framework. <a href="#fig-interop" class="quarto-xref">Figura&nbsp;<span>9.42</span></a> √® una mappa di interoperabilit√† ONNX, inclusi i principali framework pi√π diffusi.</p>
<div id="fig-interop" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://miro.medium.com/v2/resize:fit:1400/1*3N6uPaLNEYDjtWBW1vdNoQ.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;9.42: Interoperabilit√† di ONNX. Fonte: <a href="https://towardsdatascience.com/onnx-preventing-framework-lock-in-9a798fb34c92">TowardsDataScience</a>.
</figcaption>
</figure>
</div>
<p>PyTorch Mobile - Consente l‚Äôesecuzione dei modelli PyTorch su iOS e Android convertendoli in rappresentazioni ottimizzate per dispositivi mobili. Fornisce implementazioni mobili efficienti di operazioni come convoluzione e funzioni speciali ottimizzate per hardware mobile.</p>
<p>Queste piattaforme si integrano con driver hardware, sistemi operativi e librerie di acceleratori sui dispositivi per eseguire modelli in modo efficiente utilizzando l‚Äôottimizzazione hardware. Inoltre, delegano le operazioni ad acceleratori ML dedicati, ove presenti. La disponibilit√† di queste piattaforme di distribuzione collaudate e robuste colma il divario tra l‚Äôottimizzazione dei modelli nei framework e la distribuzione effettiva su miliardi di dispositivi. Consentono agli utenti di concentrarsi sullo sviluppo del modello anzich√© sulla creazione di runtime mobili personalizzati. L‚Äôinnovazione continua per supportare nuovi hardware e ottimizzazioni in queste piattaforme √® fondamentale per le ottimizzazioni di ML diffuse.</p>
<p>Fornendo queste pipeline di distribuzione ottimizzate, l‚Äôintero flusso di lavoro, dal training al deployment [distribuzione] del dispositivo, pu√≤ sfruttare le ottimizzazioni del modello per fornire applicazioni ML performanti. Questa infrastruttura software end-to-end ha contribuito a guidare l‚Äôadozione di ML sul dispositivo.</p>
</section>
</section>
<section id="conclusione" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">9.6</span> Conclusione</h2>
<p>In questo capitolo abbiamo discusso l‚Äôottimizzazione del modello nell‚Äôambito software-hardware. Ci siamo immersi in una rappresentazione efficiente del modello, dove abbiamo trattato le sfumature della potatura strutturata e non-strutturata e altre tecniche per la compressione del modello come la distillazione della conoscenza e la decomposizione di matrice e tensore. Ci siamo anche immersi brevemente nella progettazione del modello specifico per l‚Äôedge a livello di parametri e architettura del modello, esplorando argomenti come modelli specifici per l‚Äôedge e NAS basati sull‚Äôhardware.</p>
<p>Abbiamo quindi esplorato rappresentazioni numeriche efficienti, dove abbiamo trattato le basi della matematica, codifiche numeriche e archiviazione, vantaggi della matematica efficiente e le sfumature della rappresentazione numerica con utilizzo della memoria, complessit√† computazionale, compatibilit√† hardware e scenari di compromesso. Abbiamo concluso concentrandoci su un elemento fondamentale della matematica efficiente: la quantizzazione, dove abbiamo esaminato la sua storia, calibrazione, tecniche e interazione con la potatura.</p>
<p>Infine, abbiamo esaminato come possiamo apportare ottimizzazioni specifiche per l‚Äôhardware che abbiamo. Abbiamo esplorato come possiamo trovare architetture modello su misura per l‚Äôhardware, apportare ottimizzazioni nel kernel per gestire meglio il modello e framework creati per sfruttare al meglio l‚Äôhardware. Abbiamo anche esaminato come possiamo fare il contrario e creare hardware attorno al nostro software specifico e abbiamo parlato di come suddividere le reti per l‚Äôesecuzione su pi√π processori disponibili sul dispositivo edge.</p>
<p>Comprendendo il quadro completo dei gradi di libert√† all‚Äôinterno dell‚Äôottimizzazione del modello sia lontano che vicino all‚Äôhardware e i compromessi da considerare quando si implementano questi metodi, i professionisti possono sviluppare una pipeline pi√π ponderata per comprimere i loro carichi di lavoro sui dispositivi edge.</p>
</section>
<section id="sec-model-optimizations-resource" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="sec-model-optimizations-resource"><span class="header-section-number">9.7</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare sia gli studenti che gli insegnanti nel loro percorso di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e aggiungeremo nuovi esercizi nel prossimo futuro.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p>Quantizzazione:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1GOlLUMkd8OTNvrNj7lDSIGricE-569Nk/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Quantization: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/18oLTdwa-dZxbBNpvHzZVyMS8bUbed4ao/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Quantization: Part 2.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1eSOyAOu8Vg_VfIHZ9gWRVjWnmFTOcZ4FavaNMc4reHQ/edit">Post-Training Quantization (PTQ).</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1qvoKLjKadK1abqUuuCCy9gaTynMZivDKLbV2Hjftri8/edit?usp=drive_link">Quantization-Aware Training (QAT).</a></p></li>
</ul></li>
<li><p>Pruning:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1KX_I71smbztdqycPXBDAYjShinrTtQeF/edit#slide=id.p1">Pruning: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1kZGDhnkeRcAw1pz3smO837ftotXQEiO7/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Pruning: Part 2.</a></p></li>
</ul></li>
<li><p><a href="https://docs.google.com/presentation/d/1SXjA3mCSwKmdouuWoxSk7r-Yjd67RG7i/edit#slide=id.g202a77b5f4f_0_110">Knowledge Distillation.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/14K9QFUjiba1NvwG0zobsJdgEklomuM_xeaCP7-5dmY8/edit?usp=drive_link">Clustering.</a></p></li>
<li><p>Neural Architecture Search (NAS):</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1aVGjhj1Q-_JEFHr6CYzPeuMOCiDivzhZCBtg1xV14QM/edit#slide=id.g202a67d8ddf_0_0">NAS overview.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1V-ZD6c8KPrFBrrw8xkAQfkqUu4u53zkX/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">NAS: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1VUf9zyGP9yascD87VSit58S494EPnd8D/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">NAS: Part 2.</a></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><a href="#vid-quant" class="quarto-xref">Video&nbsp;<span>9.1</span></a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-p" class="quarto-xref">Esercizio&nbsp;<span>9.1</span></a></p></li>
<li><p><a href="#exr-mc" class="quarto-xref">Esercizio&nbsp;<span>9.2</span></a></p></li>
<li><p><a href="#exr-md" class="quarto-xref">Esercizio&nbsp;<span>9.3</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laboratori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Oltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l‚Äôesperienza di apprendimento.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/efficient_ai/efficient_ai.it.html" class="pagination-link" aria-label="IA Efficiente">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/hw_acceleration/hw_acceleration.it.html" class="pagination-link" aria-label="Accelerazione IA">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/optimizations/optimizations.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/optimizations/optimizations.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>