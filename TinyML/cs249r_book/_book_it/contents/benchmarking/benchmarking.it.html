<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Benchmarking AI ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/ondevice_learning/ondevice_learning.it.html" rel="next">
<link href="../../contents/hw_acceleration/hw_acceleration.it.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>


</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/benchmarking/benchmarking.it.html">Deployment</a></li><li class="breadcrumb-item"><a href="../../contents/benchmarking/benchmarking.it.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2d6cdf6fc58f1105ce2a5c5c28a11153" class="alert alert-info hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>üåü Aiutaci a raggiungere 1.000 stelle GitHub! üåü Per ogni 25 stelle, Arduino e SEEED doneranno una NiclaVision o una XIAO ESP32S3 per l‚Äôistruzione sull‚Äôintelligenza artificiale. <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per una ‚≠ê</a></p>
</div></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">PREFAZIONE</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">PARTE PRINCIPALE</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Nozioni Fondamentali</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Workflow</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di Intelligenza Artificiale</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Argomenti Avanzati</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Impatto Sociale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Chiusura</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#sec-benchmarking-ai" id="toc-sec-benchmarking-ai" class="nav-link active" data-scroll-target="#sec-benchmarking-ai"><span class="header-section-number">11.1</span> Introduzione</a></li>
  <li><a href="#contesto-storico" id="toc-contesto-storico" class="nav-link" data-scroll-target="#contesto-storico"><span class="header-section-number">11.2</span> Contesto Storico</a>
  <ul>
  <li><a href="#benchmark-standard" id="toc-benchmark-standard" class="nav-link" data-scroll-target="#benchmark-standard"><span class="header-section-number">11.2.1</span> Benchmark Standard</a></li>
  <li><a href="#benchmark-personalizzati" id="toc-benchmark-personalizzati" class="nav-link" data-scroll-target="#benchmark-personalizzati"><span class="header-section-number">11.2.2</span> Benchmark Personalizzati</a></li>
  <li><a href="#consenso-della-comunit√†" id="toc-consenso-della-comunit√†" class="nav-link" data-scroll-target="#consenso-della-comunit√†"><span class="header-section-number">11.2.3</span> Consenso della Comunit√†</a></li>
  </ul></li>
  <li><a href="#benchmark-ai-sistema-modello-e-dati" id="toc-benchmark-ai-sistema-modello-e-dati" class="nav-link" data-scroll-target="#benchmark-ai-sistema-modello-e-dati"><span class="header-section-number">11.3</span> Benchmark AI: Sistema, Modello e Dati</a>
  <ul>
  <li><a href="#benchmark-di-sistema" id="toc-benchmark-di-sistema" class="nav-link" data-scroll-target="#benchmark-di-sistema"><span class="header-section-number">11.3.1</span> Benchmark di Sistema</a></li>
  <li><a href="#benchmark-del-modello" id="toc-benchmark-del-modello" class="nav-link" data-scroll-target="#benchmark-del-modello"><span class="header-section-number">11.3.2</span> Benchmark del Modello</a></li>
  <li><a href="#benchmark-dei-dati" id="toc-benchmark-dei-dati" class="nav-link" data-scroll-target="#benchmark-dei-dati"><span class="header-section-number">11.3.3</span> Benchmark dei Dati</a></li>
  </ul></li>
  <li><a href="#benchmarking-di-sistema" id="toc-benchmarking-di-sistema" class="nav-link" data-scroll-target="#benchmarking-di-sistema"><span class="header-section-number">11.4</span> Benchmarking di Sistema</a>
  <ul>
  <li><a href="#granularit√†" id="toc-granularit√†" class="nav-link" data-scroll-target="#granularit√†"><span class="header-section-number">11.4.1</span> Granularit√†</a>
  <ul class="collapse">
  <li><a href="#micro-benchmark" id="toc-micro-benchmark" class="nav-link" data-scroll-target="#micro-benchmark">Micro Benchmark</a></li>
  <li><a href="#macro-benchmark" id="toc-macro-benchmark" class="nav-link" data-scroll-target="#macro-benchmark">Macro Benchmark</a></li>
  <li><a href="#benchmark-end-to-end" id="toc-benchmark-end-to-end" class="nav-link" data-scroll-target="#benchmark-end-to-end">Benchmark end-to-end</a></li>
  <li><a href="#comprendere-i-compromessi" id="toc-comprendere-i-compromessi" class="nav-link" data-scroll-target="#comprendere-i-compromessi">Comprendere i Compromessi</a></li>
  </ul></li>
  <li><a href="#componenti-dei-benchmark" id="toc-componenti-dei-benchmark" class="nav-link" data-scroll-target="#componenti-dei-benchmark"><span class="header-section-number">11.4.2</span> Componenti dei Benchmark</a>
  <ul class="collapse">
  <li><a href="#dataset-standardizzati" id="toc-dataset-standardizzati" class="nav-link" data-scroll-target="#dataset-standardizzati">Dataset Standardizzati</a></li>
  <li><a href="#attivit√†-predefinite" id="toc-attivit√†-predefinite" class="nav-link" data-scroll-target="#attivit√†-predefinite">Attivit√† Predefinite</a></li>
  <li><a href="#metriche-di-valutazione" id="toc-metriche-di-valutazione" class="nav-link" data-scroll-target="#metriche-di-valutazione">Metriche di Valutazione</a></li>
  <li><a href="#baseline-e-modelli-baseline" id="toc-baseline-e-modelli-baseline" class="nav-link" data-scroll-target="#baseline-e-modelli-baseline">Baseline e Modelli Baseline</a></li>
  <li><a href="#specifiche-hardware-e-software" id="toc-specifiche-hardware-e-software" class="nav-link" data-scroll-target="#specifiche-hardware-e-software">Specifiche Hardware e Software</a></li>
  <li><a href="#condizioni-ambientali" id="toc-condizioni-ambientali" class="nav-link" data-scroll-target="#condizioni-ambientali">Condizioni Ambientali</a></li>
  <li><a href="#regole-di-riproducibilit√†" id="toc-regole-di-riproducibilit√†" class="nav-link" data-scroll-target="#regole-di-riproducibilit√†">Regole di Riproducibilit√†</a></li>
  <li><a href="#linee-guida-per-linterpretazione-dei-risultati" id="toc-linee-guida-per-linterpretazione-dei-risultati" class="nav-link" data-scroll-target="#linee-guida-per-linterpretazione-dei-risultati">Linee Guida per l‚ÄôInterpretazione dei Risultati</a></li>
  </ul></li>
  <li><a href="#training-vs.-inferenza" id="toc-training-vs.-inferenza" class="nav-link" data-scroll-target="#training-vs.-inferenza"><span class="header-section-number">11.4.3</span> Training vs.&nbsp;inferenza</a></li>
  <li><a href="#i-benchmark-del-training" id="toc-i-benchmark-del-training" class="nav-link" data-scroll-target="#i-benchmark-del-training"><span class="header-section-number">11.4.4</span> I Benchmark del Training</a>
  <ul class="collapse">
  <li><a href="#scopo" id="toc-scopo" class="nav-link" data-scroll-target="#scopo">Scopo</a></li>
  <li><a href="#metriche" id="toc-metriche" class="nav-link" data-scroll-target="#metriche">Metriche</a></li>
  <li><a href="#i-task" id="toc-i-task" class="nav-link" data-scroll-target="#i-task">I Task</a></li>
  <li><a href="#i-benchmark" id="toc-i-benchmark" class="nav-link" data-scroll-target="#i-benchmark">I Benchmark</a></li>
  <li><a href="#caso-duso-di-esempio" id="toc-caso-duso-di-esempio" class="nav-link" data-scroll-target="#caso-duso-di-esempio">Caso d‚ÄôUso di Esempio</a></li>
  </ul></li>
  <li><a href="#benchmark-di-inferenza" id="toc-benchmark-di-inferenza" class="nav-link" data-scroll-target="#benchmark-di-inferenza"><span class="header-section-number">11.4.5</span> Benchmark di Inferenza</a>
  <ul class="collapse">
  <li><a href="#scopo-1" id="toc-scopo-1" class="nav-link" data-scroll-target="#scopo-1">Scopo</a></li>
  <li><a href="#metriche-1" id="toc-metriche-1" class="nav-link" data-scroll-target="#metriche-1">Metriche</a></li>
  <li><a href="#i-task-1" id="toc-i-task-1" class="nav-link" data-scroll-target="#i-task-1">I Task</a></li>
  <li><a href="#i-benchmark-1" id="toc-i-benchmark-1" class="nav-link" data-scroll-target="#i-benchmark-1">I Benchmark</a></li>
  <li><a href="#caso-duso-di-esempio-1" id="toc-caso-duso-di-esempio-1" class="nav-link" data-scroll-target="#caso-duso-di-esempio-1">Caso d‚ÄôUso di Esempio</a></li>
  </ul></li>
  <li><a href="#esempio-di-benchmark" id="toc-esempio-di-benchmark" class="nav-link" data-scroll-target="#esempio-di-benchmark"><span class="header-section-number">11.4.6</span> Esempio di Benchmark</a>
  <ul class="collapse">
  <li><a href="#task" id="toc-task" class="nav-link" data-scroll-target="#task">Task</a></li>
  <li><a href="#il-dataset" id="toc-il-dataset" class="nav-link" data-scroll-target="#il-dataset">Il Dataset</a></li>
  <li><a href="#modello" id="toc-modello" class="nav-link" data-scroll-target="#modello">Modello</a></li>
  <li><a href="#metriche-2" id="toc-metriche-2" class="nav-link" data-scroll-target="#metriche-2">Metriche</a></li>
  <li><a href="#benchmark-harness" id="toc-benchmark-harness" class="nav-link" data-scroll-target="#benchmark-harness">Benchmark Harness</a></li>
  <li><a href="#la-baseline" id="toc-la-baseline" class="nav-link" data-scroll-target="#la-baseline">La Baseline</a></li>
  </ul></li>
  <li><a href="#sfide-e-limitazioni" id="toc-sfide-e-limitazioni" class="nav-link" data-scroll-target="#sfide-e-limitazioni"><span class="header-section-number">11.4.7</span> Sfide e Limitazioni</a>
  <ul class="collapse">
  <li><a href="#lotteria-hardware" id="toc-lotteria-hardware" class="nav-link" data-scroll-target="#lotteria-hardware">Lotteria Hardware</a></li>
  <li><a href="#benchmark-engineering" id="toc-benchmark-engineering" class="nav-link" data-scroll-target="#benchmark-engineering">Benchmark Engineering</a></li>
  <li><a href="#problema" id="toc-problema" class="nav-link" data-scroll-target="#problema">Problema</a></li>
  <li><a href="#problemi" id="toc-problemi" class="nav-link" data-scroll-target="#problemi">Problemi</a></li>
  <li><a href="#attenuazione" id="toc-attenuazione" class="nav-link" data-scroll-target="#attenuazione">Attenuazione</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#benchmarking-del-modello" id="toc-benchmarking-del-modello" class="nav-link" data-scroll-target="#benchmarking-del-modello"><span class="header-section-number">11.5</span> Benchmarking del Modello</a>
  <ul>
  <li><a href="#contesto-storico-1" id="toc-contesto-storico-1" class="nav-link" data-scroll-target="#contesto-storico-1"><span class="header-section-number">11.5.1</span> Contesto Storico</a>
  <ul class="collapse">
  <li><a href="#mnist-1998" id="toc-mnist-1998" class="nav-link" data-scroll-target="#mnist-1998">MNIST (1998)</a></li>
  <li><a href="#imagenet-2009" id="toc-imagenet-2009" class="nav-link" data-scroll-target="#imagenet-2009">ImageNet (2009)</a></li>
  <li><a href="#coco-2014" id="toc-coco-2014" class="nav-link" data-scroll-target="#coco-2014">COCO (2014)</a></li>
  <li><a href="#gpt-3-2020" id="toc-gpt-3-2020" class="nav-link" data-scroll-target="#gpt-3-2020">GPT-3 (2020)</a></li>
  <li><a href="#presente-e-futuro" id="toc-presente-e-futuro" class="nav-link" data-scroll-target="#presente-e-futuro">Presente e Futuro</a></li>
  </ul></li>
  <li><a href="#metriche-del-modello" id="toc-metriche-del-modello" class="nav-link" data-scroll-target="#metriche-del-modello"><span class="header-section-number">11.5.2</span> Metriche del Modello</a>
  <ul class="collapse">
  <li><a href="#precisione" id="toc-precisione" class="nav-link" data-scroll-target="#precisione">Precisione</a></li>
  <li><a href="#correttezza" id="toc-correttezza" class="nav-link" data-scroll-target="#correttezza">Correttezza</a></li>
  <li><a href="#complessit√†" id="toc-complessit√†" class="nav-link" data-scroll-target="#complessit√†">Complessit√†</a></li>
  </ul></li>
  <li><a href="#lezioni-apprese" id="toc-lezioni-apprese" class="nav-link" data-scroll-target="#lezioni-apprese"><span class="header-section-number">11.5.3</span> Lezioni Apprese</a>
  <ul class="collapse">
  <li><a href="#tendenze-emergenti" id="toc-tendenze-emergenti" class="nav-link" data-scroll-target="#tendenze-emergenti">Tendenze Emergenti</a></li>
  </ul></li>
  <li><a href="#limitazioni-e-sfide" id="toc-limitazioni-e-sfide" class="nav-link" data-scroll-target="#limitazioni-e-sfide"><span class="header-section-number">11.5.4</span> Limitazioni e Sfide</a></li>
  </ul></li>
  <li><a href="#benchmarking-dei-dati" id="toc-benchmarking-dei-dati" class="nav-link" data-scroll-target="#benchmarking-dei-dati"><span class="header-section-number">11.6</span> Benchmarking dei Dati</a>
  <ul>
  <li><a href="#limitazioni-dellia-incentrata-sul-modello" id="toc-limitazioni-dellia-incentrata-sul-modello" class="nav-link" data-scroll-target="#limitazioni-dellia-incentrata-sul-modello"><span class="header-section-number">11.6.1</span> Limitazioni dell‚ÄôIA Incentrata sul Modello</a></li>
  <li><a href="#verso-unintelligenza-artificiale-incentrata-sui-dati" id="toc-verso-unintelligenza-artificiale-incentrata-sui-dati" class="nav-link" data-scroll-target="#verso-unintelligenza-artificiale-incentrata-sui-dati"><span class="header-section-number">11.6.2</span> Verso un‚ÄôIntelligenza Artificiale Incentrata sui Dati</a></li>
  <li><a href="#benchmarking-dei-dati-1" id="toc-benchmarking-dei-dati-1" class="nav-link" data-scroll-target="#benchmarking-dei-dati-1"><span class="header-section-number">11.6.3</span> Benchmarking dei Dati</a></li>
  <li><a href="#efficienza-dei-dati" id="toc-efficienza-dei-dati" class="nav-link" data-scroll-target="#efficienza-dei-dati"><span class="header-section-number">11.6.4</span> Efficienza dei Dati</a></li>
  </ul></li>
  <li><a href="#la-tripletta" id="toc-la-tripletta" class="nav-link" data-scroll-target="#la-tripletta"><span class="header-section-number">11.7</span> La Tripletta</a></li>
  <li><a href="#benchmark-per-tecnologie-emergenti" id="toc-benchmark-per-tecnologie-emergenti" class="nav-link" data-scroll-target="#benchmark-per-tecnologie-emergenti"><span class="header-section-number">11.8</span> Benchmark per Tecnologie Emergenti</a></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">11.9</span> Conclusione</a></li>
  <li><a href="#sec-benchmarking-ai-resource" id="toc-sec-benchmarking-ai-resource" class="nav-link" data-scroll-target="#sec-benchmarking-ai-resource"><span class="header-section-number">11.10</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/benchmarking/benchmarking.it.html">Deployment</a></li><li class="breadcrumb-item"><a href="../../contents/benchmarking/benchmarking.it.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-benchmarking_ai" class="quarto-section-identifier"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-benchmarking-ai-resource">Slide</a>, <a href="#sec-benchmarking-ai-resource">Video</a>, <a href="#sec-benchmarking-ai-resource">Esercizi</a>, <a href="#sec-benchmarking-ai-resource">Laboratori</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ai_benchmarking.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Foto di un podio su uno sfondo a tema tecnologico. Su ogni piattaforma del podio ci sono chip AI con design intricati. Il chip in alto ha una medaglia d‚Äôoro appesa, il secondo una medaglia d‚Äôargento e il terzo una medaglia di bronzo. Sullo sfondo sono ben visibili striscioni con la scritta ‚ÄúAI Olympics‚Äù.</em></figcaption>
</figure>
</div>
<p>Il benchmarking √® fondamentale per lo sviluppo e la distribuzione di sistemi di machine learning, in particolare applicazioni TinyML. I benchmark consentono agli sviluppatori di misurare e confrontare le prestazioni di diverse architetture di modelli, procedure di training e strategie di distribuzione. Ci√≤ fornisce informazioni chiave su quali approcci funzionano meglio per il problema in questione e sui vincoli dell‚Äôambiente di distribuzione.</p>
<p>Questo capitolo fornir√† una panoramica dei benchmark ML pi√π diffusi, le best practice e come utilizzarli per migliorare lo sviluppo del modello e le prestazioni del sistema. Il suo scopo √® fornire agli sviluppatori gli strumenti e le conoscenze adeguati per effettuare test di benchmark e ottimizzare in modo efficace i propri sistemi, in particolare quelli TinyML.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere lo scopo e gli obiettivi del benchmarking dei sistemi di intelligenza artificiale, tra cui valutazione delle prestazioni, valutazione delle risorse, validazione e altro ancora.</p></li>
<li><p>Scoprire i principali benchmark, metriche e tendenze del modello, tra cui accuratezza, correttezza, complessit√† ed efficienza.</p></li>
<li><p>Acquisire familiarit√† con i componenti chiave di un benchmark di intelligenza artificiale, tra cui set di dati, attivit√†, metriche, linee di base, regole di riproducibilit√† e altro ancora.</p></li>
<li><p>Comprendere la distinzione tra training e inferenza e come ogni fase giustifichi il benchmarking specializzato dei sistemi ML.</p></li>
<li><p>Scoprire i concetti di benchmarking del sistema come produttivit√†, latenza, potenza ed efficienza computazionale.</p></li>
<li><p>Apprezzare l‚Äôevoluzione del benchmarking del modello dall‚Äôaccuratezza a metriche pi√π olistiche come correttezza, robustezza e applicabilit√† nel mondo reale.</p></li>
<li><p>Riconoscere il ruolo crescente del benchmarking dei dati nella valutazione di problemi come bias, rumore, equilibrio e diversit√†.</p></li>
<li><p>Comprendere i limiti della valutazione di modelli, dati e sistemi in isolamento e l‚Äôesigenza emergente di benchmarking integrato.</p></li>
</ul>
</div>
</div>
<section id="sec-benchmarking-ai" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="sec-benchmarking-ai"><span class="header-section-number">11.1</span> Introduzione</h2>
<p>Il benchmarking fornisce le misure essenziali necessarie per guidare il progresso dell‚Äôapprendimento automatico e comprendere veramente le prestazioni del sistema. Come ha affermato il fisico Lord Kelvin, ‚ÄúMisurare √® conoscere‚Äù. I benchmark ci consentono di conoscere quantitativamente le capacit√† di diversi modelli, software e hardware. Consentono agli sviluppatori di ML di misurare il tempo di inferenza, l‚Äôutilizzo della memoria, il consumo energetico e altre metriche che caratterizzano un sistema. Inoltre, i benchmark creano processi standardizzati per la misurazione, consentendo confronti equi tra diverse soluzioni.</p>
<p>Quando i benchmark vengono mantenuti nel tempo, diventano fondamentali per catturare i progressi attraverso generazioni di algoritmi, set di dati e hardware. I modelli e le tecniche che stabiliscono nuovi record sui benchmark di ML da un anno all‚Äôaltro dimostrano miglioramenti tangibili in ci√≤ che √® possibile per l‚Äôapprendimento automatico ‚Äúon-device‚Äù. Utilizzando i benchmark per misurare, i professionisti di ML possono conoscere le capacit√† reali dei loro sistemi e avere la certezza che ogni passaggio rifletta un progresso autentico verso lo stato dell‚Äôarte.</p>
<p>Il benchmarking ha diversi obiettivi e scopi importanti che guidano la sua implementazione per i sistemi di apprendimento automatico.</p>
<ul>
<li><p><strong>Valutazione delle prestazioni.</strong> Ci√≤ comporta la valutazione di parametri chiave come la velocit√†, l‚Äôaccuratezza e l‚Äôefficienza di un dato modello. Ad esempio, in un contesto TinyML, √® fondamentale confrontare la rapidit√† con cui un assistente vocale pu√≤ riconoscere i comandi, poich√© ci√≤ valuta le prestazioni in tempo reale.</p></li>
<li><p><strong>Valutazione delle risorse.</strong> Ci√≤ significa valutare l‚Äôimpatto del modello sulle risorse critiche del sistema, tra cui durata della batteria, utilizzo della memoria e sovraccarico computazionale. Un esempio rilevante √® il confronto del consumo della batteria di due diversi algoritmi di riconoscimento delle immagini in esecuzione su un dispositivo indossabile.</p></li>
<li><p><strong>Validazione e verifica.</strong> Il benchmarking aiuta a garantire che il sistema funzioni correttamente e soddisfi i requisiti specificati. Un modo √® quello di controllare l‚Äôaccuratezza di un algoritmo, come un cardiofrequenzimetro su uno smartwatch, rispetto alle letture di apparecchiature di livello medico come forma di validazione clinica.</p></li>
<li><p><strong>Analisi competitiva.</strong> Ci√≤ consente di confrontare le soluzioni con le offerte concorrenti sul mercato. Ad esempio, il benchmarking di un modello personalizzato di rilevamento di oggetti rispetto ai benchmark TinyML comuni come MobileNet e Tiny-YOLO.</p></li>
<li><p><strong>Credibilit√†.</strong> I benchmark accurati sostengono la credibilit√† delle soluzioni AI e delle organizzazioni che le sviluppano. Dimostrano un impegno verso trasparenza, onest√† e qualit√†, essenziali per creare fiducia con utenti e stakeholder.</p></li>
<li><p><strong>Regolamentazione e Standardizzazione</strong>. Man mano che il settore dell‚ÄôAI continua a crescere, cresce anche la necessit√† di regolamentazione e standardizzazione per garantire che le soluzioni AI siano sicure, etiche ed efficaci. I benchmark accurati e affidabili sono essenziali per questo quadro normativo, poich√© forniscono i dati e le prove necessari per valutare la conformit√† con gli standard del settore e i requisiti legali.</p></li>
</ul>
<p>Questo capitolo tratter√† i 3 tipi di benchmark AI, le metriche standard, gli strumenti e le tecniche che i progettisti utilizzano per ottimizzare i loro sistemi e le sfide e le tendenze nel benchmarking.</p>
</section>
<section id="contesto-storico" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="contesto-storico"><span class="header-section-number">11.2</span> Contesto Storico</h2>
<section id="benchmark-standard" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="benchmark-standard"><span class="header-section-number">11.2.1</span> Benchmark Standard</h3>
<p>L‚Äôevoluzione dei benchmark nell‚Äôinformatica illustra vividamente l‚Äôincessante ricerca dell‚Äôeccellenza e dell‚Äôinnovazione da parte del settore. Nei primi giorni dell‚Äôinformatica, negli anni ‚Äô60 e ‚Äô70, i benchmark erano rudimentali e progettati per i mainframe. Ad esempio, il <a href="https://en.wikipedia.org/wiki/Whetstone_(benchmark)">benchmark Whetstone</a>, che prende il nome dal compilatore Whetstone ALGOL, √® stato uno dei primi test standardizzati per misurare le prestazioni aritmetiche in virgola mobile di una CPU. Questi benchmark pionieristici hanno spinto i produttori a perfezionare le loro architetture e algoritmi per ottenere punteggi di benchmark migliori.</p>
<p>Gli anni ‚Äô80 hanno segnato un cambiamento significativo con l‚Äôascesa dei personal computer. Mentre aziende come IBM, Apple e Commodore gareggiavano per quote di mercato, i benchmark sono diventati strumenti essenziali per consentire una concorrenza leale. I <a href="https://www.spec.org/cpu/">benchmark CPU SPEC</a>, introdotti dalla <a href="https://www.spec.org/">System Performance Evaluation Cooperative (SPEC)</a>, hanno stabilito test standardizzati che consentono confronti oggettivi tra diverse macchine. Questa standardizzazione ha creato un ambiente competitivo, spingendo i produttori di chip e i creatori di sistemi a migliorare continuamente le loro offerte hardware e software.</p>
<p>Gli anni ‚Äô90 hanno portato l‚Äôera delle applicazioni e dei videogiochi ‚Äúgraphics-intensive‚Äù. La necessit√† di benchmark per valutare le prestazioni delle schede grafiche ha portato alla creazione di <a href="https://www.3dmark.com/">3DMark</a> da parte di Futuremark. Mentre i giocatori e i professionisti cercavano schede grafiche ad alte prestazioni, aziende come NVIDIA e AMD sono state spinte a una rapida innovazione, portando a importanti progressi nella tecnologia GPU come gli shader programmabili.</p>
<p>Gli anni 2000 hanno visto un‚Äôimpennata di telefoni cellulari e dispositivi portatili come i tablet. Con la portabilit√† √® arrivata la sfida di bilanciare prestazioni e consumo energetico. Benchmark come <a href="https://bapco.com/products/mobilemark-2014/">MobileMark</a> di BAPCo hanno valutato velocit√† e durata della batteria. Ci√≤ ha spinto le aziende a sviluppare System-on-Chip (SOC) pi√π efficienti dal punto di vista energetico, portando all‚Äôemergere di architetture come ARM che hanno dato priorit√† all‚Äôefficienza energetica.</p>
<p>L‚Äôattenzione dell‚Äôultimo decennio si √® spostata verso il cloud computing, i big data e l‚Äôintelligenza artificiale. I provider di servizi cloud come Amazon Web Services e Google Cloud competono su prestazioni, scalabilit√† e convenienza. I benchmark specifici del cloud come <a href="http://cloudsuite.ch/">CloudSuite</a> sono diventati essenziali, spingendo i provider a ottimizzare la propria infrastruttura per servizi migliori.</p>
</section>
<section id="benchmark-personalizzati" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="benchmark-personalizzati"><span class="header-section-number">11.2.2</span> Benchmark Personalizzati</h3>
<p>Oltre ai benchmark standard del settore, ci sono benchmark personalizzati specificamente progettati per soddisfare i requisiti unici di una particolare applicazione o attivit√†. Sono personalizzati in base alle esigenze specifiche dell‚Äôutente o dello sviluppatore, assicurando che le metriche delle prestazioni siano direttamente pertinenti all‚Äôuso previsto del modello o del sistema di intelligenza artificiale. I benchmark personalizzati possono essere creati da singole organizzazioni, ricercatori o sviluppatori e sono spesso utilizzati insieme ai benchmark standard del settore per fornire una valutazione completa delle prestazioni dell‚Äôintelligenza artificiale.</p>
<p>Ad esempio, un ospedale potrebbe sviluppare un benchmark per valutare un modello di intelligenza artificiale per prevedere la riammissione dei pazienti. Questo benchmark incorporerebbe metriche pertinenti alla popolazione di pazienti dell‚Äôospedale, come dati demografici, anamnesi e fattori sociali. Allo stesso modo, il benchmark di rilevamento delle frodi di un istituto finanziario potrebbe concentrarsi sull‚Äôidentificazione accurata delle transazioni fraudolente riducendo al minimo i falsi positivi. Nel settore automobilistico, un benchmark di veicoli autonomi potrebbe dare priorit√† alle prestazioni in diverse condizioni, alla risposta agli ostacoli e alla sicurezza. I rivenditori potrebbero confrontare i sistemi di raccomandazione utilizzando il tasso di clic, il tasso di conversione e la soddisfazione del cliente. Le aziende manifatturiere potrebbero confrontare i sistemi di controllo qualit√† in base all‚Äôidentificazione dei difetti, all‚Äôefficienza e alla riduzione degli sprechi. In ogni settore, i benchmark personalizzati forniscono alle organizzazioni criteri di valutazione su misura per le loro esigenze e il loro contesto unici. Ci√≤ consente una valutazione pi√π significativa di quanto i sistemi di intelligenza artificiale soddisfino i requisiti.</p>
<p>Il vantaggio dei benchmark personalizzati risiede nella loro flessibilit√† e pertinenza. Possono essere progettati per testare aspetti specifici delle prestazioni critici per il successo della soluzione di intelligenza artificiale nella sua applicazione prevista. Ci√≤ consente una valutazione pi√π mirata e accurata delle capacit√† del modello o del sistema di intelligenza artificiale. I benchmark personalizzati forniscono anche informazioni preziose sulle prestazioni delle soluzioni di intelligenza artificiale in scenari reali, il che pu√≤ essere cruciale per identificare potenziali problemi e aree di miglioramento.</p>
<p>Nell‚Äôintelligenza artificiale, i benchmark svolgono un ruolo cruciale nel guidare il progresso e l‚Äôinnovazione. Sebbene i benchmark siano stati a lungo utilizzati nell‚Äôinformatica, la loro applicazione all‚Äôapprendimento automatico √® relativamente recente. I benchmark incentrati sull‚Äôintelligenza artificiale forniscono metriche standardizzate per valutare e confrontare le prestazioni di diversi algoritmi, architetture di modelli e piattaforme hardware.</p>
</section>
<section id="consenso-della-comunit√†" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="consenso-della-comunit√†"><span class="header-section-number">11.2.3</span> Consenso della Comunit√†</h3>
<p>Una prerogativa fondamentale affinch√© un benchmark abbia un impatto √® che deve riflettere le priorit√† e i valori condivisi della pi√π ampia comunit√† di ricerca. I benchmark progettati in modo isolato rischiano di non ottenere accettazione se trascurano metriche chiave considerate importanti dai gruppi leader. Attraverso uno sviluppo collaborativo con la partecipazione aperta di laboratori accademici, aziende e altri stakeholder, i benchmark possono incorporare un contributo collettivo su capacit√† critiche che vale la pena misurare. Ci√≤ aiuta a garantire che i benchmark valutino aspetti che la comunit√† concorda siano essenziali per far progredire il campo. Il processo di raggiungimento dell‚Äôallineamento su attivit√† e metriche supporta di per s√© la convergenza su ci√≤ che conta di pi√π.</p>
<p>Inoltre, i benchmark pubblicati con ampia co-paternit√† da istituzioni rispettate hanno autorit√† e validit√† che convincono la comunit√† ad adottarli come standard affidabili. I benchmark percepiti come distorti da particolari interessi aziendali o istituzionali generano scetticismo. Anche il coinvolgimento continuo della comunit√† attraverso workshop e sfide √® fondamentale dopo la versione iniziale, ed √® ci√≤ che, ad esempio, ha portato al successo di ImageNet. Col progredire della ricerca, la partecipazione collettiva consente un continuo perfezionamento ed espansione dei benchmark nel tempo.</p>
<p>Infine, i benchmark sviluppati dalla comunit√† rilasciati con accesso aperto accelerano l‚Äôadozione e l‚Äôimplementazione coerente. Abbiamo condiviso codice open source, documentazione, modelli e infrastrutture per ridurre le barriere che impediscono ai gruppi di confrontare le soluzioni su un piano di parit√† utilizzando implementazioni standardizzate. Questa coerenza √® fondamentale per confronti equi. Senza coordinamento, laboratori e aziende potrebbero implementare i benchmark in modo diverso, riducendo la riproducibilit√† dei risultati.</p>
<p>Il consenso della comunit√† conferisce ai benchmark una rilevanza duratura, mentre la frammentazione confonde. Attraverso lo sviluppo collaborativo e un funzionamento trasparente, i benchmark possono diventare standard autorevoli per monitorare i progressi. Molti dei benchmark di cui parliamo in questo capitolo sono stati sviluppati e creati dalla comunit√†, per la comunit√†, ed √® questo che alla fine ha portato al loro successo.</p>
</section>
</section>
<section id="benchmark-ai-sistema-modello-e-dati" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="benchmark-ai-sistema-modello-e-dati"><span class="header-section-number">11.3</span> Benchmark AI: Sistema, Modello e Dati</h2>
<p>La necessit√† di un benchmarking completo diventa fondamentale man mano che i sistemi AI diventano pi√π complessi e onnipresenti. In questo contesto, i benchmark sono spesso classificati in tre categorie principali: Hardware, Modello e Dati. Analizziamo perch√© ognuno di questi gruppi √® essenziale e il significato della valutazione dell‚ÄôAI da queste tre dimensioni distinte:</p>
<section id="benchmark-di-sistema" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="benchmark-di-sistema"><span class="header-section-number">11.3.1</span> Benchmark di Sistema</h3>
<p>I calcoli AI, in particolare quelli nel deep learning, richiedono molte risorse. L‚Äôhardware su cui vengono eseguiti questi calcoli svolge un ruolo importante nel determinare la velocit√†, l‚Äôefficienza e la scalabilit√† delle soluzioni AI. Di conseguenza, i benchmark hardware aiutano a valutare le prestazioni di CPU, GPU, TPU e altri acceleratori nelle attivit√† AI. Comprendendone le prestazioni, gli sviluppatori possono scegliere quali piattaforme hardware si adattano meglio a specifiche applicazioni AI. Inoltre, i produttori di hardware utilizzano questi benchmark per identificare aree di miglioramento, guidando l‚Äôinnovazione nei progetti di chip specifici per AI.</p>
</section>
<section id="benchmark-del-modello" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="benchmark-del-modello"><span class="header-section-number">11.3.2</span> Benchmark del Modello</h3>
<p>L‚Äôarchitettura, le dimensioni e la complessit√† dei modelli AI variano notevolmente. Modelli diversi hanno diverse esigenze di calcolo e offrono diversi livelli di accuratezza ed efficienza. I benchmark dei modelli aiutano a valutare le prestazioni di varie architetture AI su attivit√† standardizzate. Forniscono informazioni sulla velocit√†, l‚Äôaccuratezza e le richieste di risorse di diversi modelli. Eseguendo il benchmarking dei modelli, i ricercatori possono identificare le architetture pi√π performanti per attivit√† specifiche, guidando la comunit√† AI verso soluzioni pi√π efficienti ed efficaci. Inoltre, questi benchmark aiutano a monitorare i progressi della ricerca sull‚Äôintelligenza artificiale, mostrando i progressi nella progettazione e nell‚Äôottimizzazione dei modelli.</p>
</section>
<section id="benchmark-dei-dati" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="benchmark-dei-dati"><span class="header-section-number">11.3.3</span> Benchmark dei Dati</h3>
<p>L‚Äôintelligenza artificiale, in particolare l‚Äôapprendimento automatico, √® intrinsecamente basata sui dati. La qualit√†, le dimensioni e la diversit√† dei dati influenzano l‚Äôefficacia dell‚Äôaddestramento e la capacit√† di generalizzazione dei modelli di intelligenza artificiale. I benchmark dei dati si concentrano sui set di dati utilizzati nell‚Äôaddestramento e nella valutazione dell‚Äôintelligenza artificiale. Forniscono set di dati standardizzati che la comunit√† pu√≤ utilizzare per addestrare e testare i modelli, garantendo parit√† di condizioni per i confronti. Inoltre, questi benchmark evidenziano le sfide relative alla qualit√†, alla diversit√† e alla rappresentazione dei dati, spingendo la comunit√† ad affrontare bias e lacune nei dati di addestramento dell‚Äôintelligenza artificiale. Comprendendo i benchmark dei dati, i ricercatori possono anche valutare come i modelli potrebbero comportarsi in scenari reali, garantendo robustezza e affidabilit√†.</p>
<p>Nelle restanti sezioni, discuteremo ciascuno di questi tipi di benchmark. L‚Äôattenzione sar√† rivolta a un‚Äôesplorazione approfondita dei benchmark di sistema, poich√© sono fondamentali per comprendere e migliorare le prestazioni del sistema di apprendimento automatico. Parleremo brevemente dei benchmark dei modelli e dei dati per una prospettiva completa, ma l‚Äôenfasi e la maggior parte del contenuto saranno dedicati ai benchmark di sistema.</p>
</section>
</section>
<section id="benchmarking-di-sistema" class="level2 page-columns page-full" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="benchmarking-di-sistema"><span class="header-section-number">11.4</span> Benchmarking di Sistema</h2>
<section id="granularit√†" class="level3 page-columns page-full" data-number="11.4.1">
<h3 data-number="11.4.1" class="anchored" data-anchor-id="granularit√†"><span class="header-section-number">11.4.1</span> Granularit√†</h3>
<p>Il benchmarking del sistema di apprendimento automatico fornisce un approccio strutturato e sistematico per valutare le prestazioni di un sistema in diverse dimensioni. Data la complessit√† dei sistemi ML, possiamo analizzare le loro prestazioni attraverso diversi livelli di granularit√† e ottenere una visione completa dell‚Äôefficienza del sistema, identificare potenziali colli di bottiglia e individuare le aree di miglioramento. A tal fine, nel corso degli anni si sono evoluti vari tipi di benchmark che continuano a persistere.</p>
<p><a href="#fig-granularity" class="quarto-xref">Figura&nbsp;<span>11.1</span></a> illustra i diversi livelli di granularit√† di un sistema ML. A livello di applicazione, i benchmark end-to-end valutano le prestazioni complessive del sistema, considerando fattori come la pre-elaborazione dei dati, l‚Äôaddestramento del modello e l‚Äôinferenza. Mentre a livello di modello, i benchmark si concentrano sulla valutazione dell‚Äôefficienza e dell‚Äôaccuratezza di modelli specifici. Ci√≤ include la valutazione di quanto bene i modelli si generalizzano a nuovi dati e della loro efficienza computazionale durante l‚Äôaddestramento e l‚Äôinferenza. Inoltre, il benchmarking pu√≤ estendersi all‚Äôinfrastruttura hardware e software, esaminando le prestazioni di singoli componenti come GPU o TPU.</p>
<div id="fig-granularity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/end2end.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.1: Granularit√† del sistema ML.
</figcaption>
</figure>
</div>
<section id="micro-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="micro-benchmark">Micro Benchmark</h4>
<p>I micro-benchmark nell‚ÄôIA sono specializzati e valutano componenti distinti o operazioni specifiche all‚Äôinterno di un processo di apprendimento automatico pi√π ampio. Questi benchmark si concentrano su singole attivit√†, offrendo approfondimenti sulle richieste computazionali di un particolare layer di rete neurale, l‚Äôefficienza di un‚Äôunica tecnica di ottimizzazione o la produttivit√† di una specifica funzione di attivazione. Ad esempio, i professionisti potrebbero utilizzare i micro-benchmark per misurare il tempo di calcolo richiesto da un layer convoluzionale in un modello di deep learning o per valutare la velocit√† di preelaborazione che alimenta i dati nel modello. Tali valutazioni granulari sono fondamentali per la messa a punto e l‚Äôottimizzazione di aspetti discreti dei modelli di IA, assicurando che ogni componente funzioni al massimo del suo potenziale.</p>
<p>Questi tipi di microbenchmark includono lo zoom su operazioni o componenti molto specifiche della pipeline AI, come le seguenti:</p>
<ul>
<li><p><strong>Operazioni Tensoriali:</strong> Librerie come <a href="https://developer.nvidia.com/cudnn">cuDNN</a> (di NVIDIA) spesso hanno benchmark per misurare le prestazioni di singole operazioni tensoriali, come convoluzioni o moltiplicazioni di matrici, che sono fondamentali per i calcoli del deep learning.</p></li>
<li><p><strong>Funzioni di Attivazione:</strong> Benchmark che misurano la velocit√† e l‚Äôefficienza di varie funzioni di attivazione come ReLU, Sigmoid o Tanh in isolamento.</p></li>
<li><p><strong>Benchmark di Layer:</strong> Valutazioni dell‚Äôefficienza computazionale di distinti layer di rete neurale, come blocchi LSTM o Transformer, quando si opera su dimensioni di input standardizzate.</p></li>
</ul>
<p>Esempio: <a href="https://github.com/baidu-research/DeepBench">DeepBench</a>, introdotto da Baidu, √® un buon esempio di qualcosa che valuta quanto sopra. DeepBench valuta le prestazioni delle operazioni di base nei modelli di deep learning, fornendo informazioni su come diverse piattaforme hardware gestiscono l‚Äôaddestramento e l‚Äôinferenza delle reti neurali.</p>
<div id="exr-cuda" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;11.1: Benchmarking di Sistema - Operazioni Tensoriali
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Ci si √® mai chiesto come mai i filtri immagine diventano cos√¨ veloci? Librerie speciali come cuDNN potenziano quei calcoli su determinati hardware. In questo Colab, useremo cuDNN con PyTorch per velocizzare il filtraggio delle immagini. Lo si consideri un piccolo benchmark, che mostra come il software giusto pu√≤ sbloccare la potenza della GPU!</p>
<p><a href="https://colab.research.google.com/github/RyanHartzell/cudnn-image-filtering/blob/master/notebooks/CuDNN%20Image%20Filtering%20Tutorial%20Using%20PyTorch.ipynb#scrollTo=1sWeXdYsATrr"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="macro-benchmark" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="macro-benchmark">Macro Benchmark</h4>
<p>I macro benchmark forniscono una visione olistica, valutando le prestazioni end-to-end di interi modelli di apprendimento automatico o sistemi di intelligenza artificiale completi. Invece di concentrarsi sulle singole operazioni, i macro benchmark valutano l‚Äôefficacia collettiva dei modelli in scenari o attivit√† del mondo reale. Ad esempio, un macro benchmark potrebbe valutare le prestazioni complete di un modello di apprendimento profondo che esegue la classificazione delle immagini su un set di dati come <a href="https://www.image-net.org/">ImageNet</a>. Ci√≤ include la misura dell‚Äôaccuratezza, della velocit√† di calcolo e del consumo di risorse. Allo stesso modo, si potrebbero misurare il tempo e le risorse cumulativi necessari per addestrare un modello di elaborazione del linguaggio naturale su corpora di testo estesi o valutare le prestazioni di un intero sistema di raccomandazione, dall‚Äôinserimento dei dati agli output finali specifici dell‚Äôutente.</p>
<p>Esempi: Questi benchmark valutano il modello di intelligenza artificiale:</p>
<ul>
<li><p><a href="https://github.com/mlcommons/inference">MLPerf Inference</a> <span class="citation" data-cites="reddi2020mlperf">(<a href="../../references.html#ref-reddi2020mlperf" role="doc-biblioref">Reddi et al. 2020</a>)</span>: Un set di benchmark standard per misurare le prestazioni di software e hardware di apprendimento automatico. MLPerf ha una suite di benchmark dedicati per scale specifiche, come <a href="https://github.com/mlcommons/mobile_app_open">MLPerf Mobile</a> per dispositivi di classe mobile e <a href="https://github.com/mlcommons/tiny">MLPerf Tiny</a>, che si concentra su microcontrollori e altri dispositivi con risorse limitate.</p></li>
<li><p><a href="https://github.com/eembc/mlmark">MLMark di EEMBC</a>: Una suite di benchmarking per valutare le prestazioni e l‚Äôefficienza energetica dei dispositivi embedded che eseguono carichi di lavoro di apprendimento automatico. Questo benchmark fornisce informazioni su come diverse piattaforme hardware gestiscono attivit√† come il riconoscimento delle immagini o l‚Äôelaborazione audio.</p></li>
<li><p><a href="https://ai-benchmark.com/">AI-Benchmark</a> <span class="citation" data-cites="ignatov2018ai">(<a href="../../references.html#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2019</a>)</span>: Uno strumento di benchmarking progettato per dispositivi Android, valuta le prestazioni delle attivit√† di intelligenza artificiale sui dispositivi mobili, comprendendo vari scenari del mondo reale come il riconoscimento delle immagini, l‚Äôanalisi dei volti e il riconoscimento ottico dei caratteri.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-reddi2020mlperf" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. <span>¬´<span>MLPerf</span> Inference Benchmark¬ª</span>. In <em>2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>, 446‚Äì59. IEEE; IEEE. <a href="https://doi.org/10.1109/isca45697.2020.00045">https://doi.org/10.1109/isca45697.2020.00045</a>.
</div><div id="ref-ignatov2018ai" class="csl-entry" role="listitem">
Ignatov, Andrey, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, e Luc Van Gool. 2019. <span>¬´<span>AI</span> Benchmark: <span>All</span> About Deep Learning on Smartphones in 2019¬ª</span>. In <em>2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em>, 0‚Äì0. IEEE. <a href="https://doi.org/10.1109/iccvw.2019.00447">https://doi.org/10.1109/iccvw.2019.00447</a>.
</div></div></section>
<section id="benchmark-end-to-end" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-end-to-end">Benchmark end-to-end</h4>
<p>I benchmark end-to-end forniscono una valutazione completa che si estende oltre i confini del modello di IA stesso. Invece di concentrarsi esclusivamente sull‚Äôefficienza o l‚Äôaccuratezza computazionale di un modello di apprendimento automatico, questi benchmark comprendono l‚Äôintera pipeline di un sistema di IA. Ci√≤ include la pre-elaborazione iniziale dei dati, le prestazioni del modello principale, la post-elaborazione degli output del modello e altri componenti integrali come l‚Äôarchiviazione e le interazioni di rete.</p>
<p>La pre-elaborazione dei dati √® la prima fase in molti sistemi di IA, trasformando i dati grezzi in un formato adatto per l‚Äôaddestramento o l‚Äôinferenza del modello. L‚Äôefficienza, la scalabilit√† e l‚Äôaccuratezza di queste fasi di pre-elaborazione sono vitali per le prestazioni complessive del sistema. I benchmark end-to-end valutano questa fase, assicurando che la pulizia dei dati, la normalizzazione, l‚Äôaumento o qualsiasi altro processo di trasformazione non diventi un collo di bottiglia.</p>
<p>Anche la fase di post-elaborazione √® al centro dell‚Äôattenzione. Ci√≤ comporta l‚Äôinterpretazione degli output grezzi del modello, eventualmente la conversione dei punteggi in categorie significative, il filtraggio dei risultati o persino l‚Äôintegrazione con altri sistemi. Nelle applicazioni del mondo reale, questa fase √® fondamentale per fornire informazioni fruibili e i benchmark end-to-end ne garantiscono l‚Äôefficienza e l‚Äôefficacia.</p>
<p>Oltre alle operazioni di base dell‚ÄôIA, altri componenti del sistema sono importanti per le prestazioni complessive e l‚Äôesperienza utente. Le soluzioni di archiviazione, basate su cloud, on-premise o ibride, possono avere un impatto significativo sui tempi di recupero e archiviazione dei dati, in particolare con vasti set di dati di IA. Allo stesso modo, le interazioni di rete, vitali per le soluzioni di IA basate su cloud o per i sistemi distribuiti, possono diventare colli di bottiglia delle prestazioni se non ottimizzate. I benchmark end-to-end valutano in modo olistico questi componenti, assicurando che l‚Äôintero sistema funzioni senza problemi, dal recupero dei dati alla consegna dell‚Äôoutput finale.</p>
<p>Ad oggi, non esistono benchmark end-to-end pubblici che tengano conto del ruolo dell‚Äôarchiviazione dei dati, della rete e delle prestazioni di elaborazione. Si pu√≤ sostenere che MLPerf Training and Inference si avvicini all‚Äôidea di un benchmark end-to-end, ma si concentrano esclusivamente sulle prestazioni del modello ML e non rappresentano scenari di distribuzione nel mondo reale di come i modelli vengono utilizzati sul campo. Tuttavia, forniscono un segnale molto utile che aiuta a valutare le prestazioni del sistema AI.</p>
<p>Data la specificit√† intrinseca del benchmarking end-to-end, viene in genere eseguito internamente in un‚Äôazienda ‚Äústrumentando‚Äù [inserendo punti di controllo] distribuzioni di produzione reali di AI. Ci√≤ consente agli ingegneri di avere una comprensione e una ripartizione realistiche delle prestazioni, ma data la sensibilit√† e la specificit√† delle informazioni, raramente vengono segnalate all‚Äôesterno dell‚Äôazienda.</p>
</section>
<section id="comprendere-i-compromessi" class="level4">
<h4 class="anchored" data-anchor-id="comprendere-i-compromessi">Comprendere i Compromessi</h4>
<p>Diversi problemi sorgono nelle diverse fasi di un sistema di intelligenza artificiale. I micro-benchmark aiutano a mettere a punto i singoli componenti, i macro-benchmark aiutano a perfezionare le architetture o gli algoritmi del modello e i benchmark end-to-end guidano l‚Äôottimizzazione dell‚Äôintero flusso di lavoro. Comprendendo dove si trova un problema, gli sviluppatori possono applicare ottimizzazioni mirate.</p>
<p>Inoltre, mentre i singoli componenti di un sistema di intelligenza artificiale potrebbero funzionare in modo ottimale in isolamento, possono emergere colli di bottiglia quando interagiscono. I benchmark end-to-end, in particolare, sono fondamentali per garantire che l‚Äôintero sistema, quando funziona collettivamente, soddisfi gli standard di prestazioni ed efficienza desiderati.</p>
<p>Infine, le organizzazioni possono prendere decisioni informate su dove allocare le risorse individuando colli di bottiglia o inefficienze nelle prestazioni. Ad esempio, se i micro-benchmark rivelano inefficienze in specifiche operazioni tensoriali, gli investimenti possono essere indirizzati verso acceleratori hardware specializzati. Al contrario, se i benchmark end-to-end indicano problemi di recupero dei dati, gli investimenti potrebbero essere incanalati verso soluzioni di archiviazione migliori.</p>
</section>
</section>
<section id="componenti-dei-benchmark" class="level3" data-number="11.4.2">
<h3 data-number="11.4.2" class="anchored" data-anchor-id="componenti-dei-benchmark"><span class="header-section-number">11.4.2</span> Componenti dei Benchmark</h3>
<p>In sostanza, un benchmark AI √® pi√π di un semplice test o punteggio; √® un framework di valutazione completo. Per comprenderlo in modo approfondito, analizziamo i componenti tipici che compongono un benchmark AI.</p>
<section id="dataset-standardizzati" class="level4">
<h4 class="anchored" data-anchor-id="dataset-standardizzati">Dataset Standardizzati</h4>
<p>I set di dati fungono da base per la maggior parte dei benchmark AI. Forniscono un set di dati coerente su cui i modelli vengono addestrati e valutati, garantendo parit√† di condizioni per i confronti.</p>
<p>Esempio: ImageNet, un set di dati su larga scala contenente milioni di immagini etichettate che abbracciano migliaia di categorie, √® uno standard di benchmarking popolare per le attivit√† di classificazione delle immagini.</p>
</section>
<section id="attivit√†-predefinite" class="level4">
<h4 class="anchored" data-anchor-id="attivit√†-predefinite">Attivit√† Predefinite</h4>
<p>Un benchmark dovrebbe avere un obiettivo o un compito chiaro che i modelli mirano a raggiungere. Questo compito definisce il problema che il sistema AI sta cercando di risolvere.</p>
<p>Esempio: I compiti per i benchmark di elaborazione del linguaggio naturale potrebbero includere analisi del ‚Äúsentiment‚Äù, riconoscimento di entit√† denominate o traduzione automatica.</p>
</section>
<section id="metriche-di-valutazione" class="level4">
<h4 class="anchored" data-anchor-id="metriche-di-valutazione">Metriche di Valutazione</h4>
<p>Una volta definito un task, i benchmark richiedono parametri per quantificare le prestazioni. Questi parametri offrono misure oggettive per confrontare diversi modelli o sistemi. Nei task di classificazione, parametri come accuratezza, precisione, richiamo e <a href="https://en.wikipedia.org/wiki/F-score">punteggio F1</a> sono comunemente utilizzati. Errori quadratici medi o assoluti potrebbero essere utilizzati per i task di regressione.</p>
</section>
<section id="baseline-e-modelli-baseline" class="level4">
<h4 class="anchored" data-anchor-id="baseline-e-modelli-baseline">Baseline e Modelli Baseline</h4>
<p>I benchmark spesso includono modelli ‚Äúbaseline‚Äù o implementazioni di riferimento. Di solito servono come punti di partenza o standard minimi di prestazione per confrontare nuovi modelli o nuove tecniche. I modelli ‚Äúbaseline‚Äù aiutano i ricercatori a misurare l‚Äôefficacia di nuovi algoritmi.</p>
<p>Nelle suite di benchmark, modelli semplici come la regressione lineare o le reti neurali di base sono spesso le baseline comuni. Queste forniscono un contesto quando si valutano modelli pi√π complessi. Confrontando questi modelli pi√π semplici, i ricercatori possono quantificare i miglioramenti derivanti da approcci avanzati.</p>
<p>Le metriche delle prestazioni variano in base all‚Äôattivit√†, ma ecco alcuni esempi:</p>
<ul>
<li>Le attivit√† di classificazione utilizzano metriche come accuratezza, precisione, richiamo e punteggio F1.</li>
<li>Le attivit√† di regressione utilizzano spesso l‚Äôerrore quadratico medio o l‚Äôerrore assoluto medio.</li>
</ul>
</section>
<section id="specifiche-hardware-e-software" class="level4">
<h4 class="anchored" data-anchor-id="specifiche-hardware-e-software">Specifiche Hardware e Software</h4>
<p>Data la variabilit√† introdotta da diverse configurazioni hardware e software, i benchmark spesso specificano o documentano gli ambienti hardware e software in cui vengono condotti i test.</p>
<p>Esempio: Un benchmark AI potrebbe indicare che le valutazioni sono state condotte su una GPU NVIDIA Tesla V100 utilizzando TensorFlow v2.4.</p>
</section>
<section id="condizioni-ambientali" class="level4">
<h4 class="anchored" data-anchor-id="condizioni-ambientali">Condizioni Ambientali</h4>
<p>Poich√© fattori esterni possono influenzare i risultati del benchmark, √® essenziale controllare o documentare condizioni come temperatura, fonte di alimentazione o processi di background del sistema.</p>
<p>Esempio: I benchmark AI mobili potrebbero specificare che i test sono stati condotti a temperatura ambiente con dispositivi collegati a una fonte di alimentazione per eliminare le variazioni del livello della batteria.</p>
</section>
<section id="regole-di-riproducibilit√†" class="level4">
<h4 class="anchored" data-anchor-id="regole-di-riproducibilit√†">Regole di Riproducibilit√†</h4>
<p>Per garantire che i benchmark siano credibili e possano essere replicati da altri nella comunit√†, spesso includono protocolli dettagliati che coprono tutto, dai ‚Äúrandom seed‚Äù utilizzati agli iperparametri esatti.</p>
<p>Esempio: Un benchmark per un‚Äôattivit√† di learning di rinforzo potrebbe specificare gli episodi esatti dell‚Äôaddestramento, i rapporti di esplorazione-sfruttamento e le strutture di ricompensa utilizzate.</p>
</section>
<section id="linee-guida-per-linterpretazione-dei-risultati" class="level4">
<h4 class="anchored" data-anchor-id="linee-guida-per-linterpretazione-dei-risultati">Linee Guida per l‚ÄôInterpretazione dei Risultati</h4>
<p>Oltre ai punteggi o alle metriche pure, i benchmark spesso forniscono linee guida o contesto per interpretare i risultati, aiutando i professionisti a comprendere le implicazioni pi√π ampie.</p>
<p>Esempio: Un benchmark potrebbe evidenziare che, sebbene il Modello A abbia ottenuto un punteggio pi√π alto del Modello B in termini di accuratezza, offre migliori prestazioni in tempo reale, rendendolo pi√π adatto per applicazioni sensibili al fattore tempo.</p>
</section>
</section>
<section id="training-vs.-inferenza" class="level3" data-number="11.4.3">
<h3 data-number="11.4.3" class="anchored" data-anchor-id="training-vs.-inferenza"><span class="header-section-number">11.4.3</span> Training vs.&nbsp;inferenza</h3>
<p>Il ciclo di vita dello sviluppo di un modello di apprendimento automatico prevede due fasi critiche: addestramento e inferenza. <a href="../training/training.qmd">Training</a>, come forse ricorderete, √® il processo di apprendimento di modelli dai dati per creare il modello. L‚Äôinferenza si riferisce al modello che fa previsioni su nuovi dati non etichettati. Entrambe le fasi svolgono ruoli indispensabili ma distinti. Di conseguenza, ogni fase garantisce un rigoroso benchmarking per valutare metriche delle prestazioni come velocit√†, accuratezza ed efficienza computazionale.</p>
<p>Il benchmarking della fase di training fornisce informazioni su come diverse architetture di modelli, valori di iperparametri e algoritmi di ottimizzazione influiscono sul tempo e sulle risorse necessarie per il training del modello. Ad esempio, il benchmarking mostra come la profondit√† della rete neurale influisce sul tempo di training su un dato set di dati. Il benchmarking rivela anche come acceleratori hardware come GPU e TPU possono accelerare il training.</p>
<p>D‚Äôaltro canto, il benchmark dell‚Äôinferenza valuta le prestazioni del modello in condizioni reali dopo la distribuzione. Le metriche chiave includono latenza, throughput, footprint di memoria e consumo energetico. Questo tipo di benchmarking determina se un modello soddisfa i requisiti della sua applicazione target in termini di tempo di risposta e vincoli del dispositivo. Tuttavia, ne discuteremo ampiamente per garantire una comprensione generale.</p>
</section>
<section id="i-benchmark-del-training" class="level3 page-columns page-full" data-number="11.4.4">
<h3 data-number="11.4.4" class="anchored" data-anchor-id="i-benchmark-del-training"><span class="header-section-number">11.4.4</span> I Benchmark del Training</h3>
<p>Il training [addestramento] rappresenta la fase in cui il sistema elabora e assimila dati grezzi per adattare e perfezionare i propri parametri. Pertanto, √® un‚Äôattivit√† algoritmica e comporta considerazioni a livello di sistema, tra cui pipeline di dati, archiviazione, risorse di elaborazione e meccanismi di orchestrazione. L‚Äôobiettivo √® garantire che il sistema ML possa apprendere in modo efficiente dai dati, ottimizzando sia le prestazioni del modello sia l‚Äôutilizzo delle risorse del sistema.</p>
<section id="scopo" class="level4">
<h4 class="anchored" data-anchor-id="scopo">Scopo</h4>
<p>Dal punto di vista dei sistemi ML, i benchmark del training valutano quanto bene il sistema si adatta all‚Äôaumento dei volumi di dati e delle richieste di elaborazione. Si tratta di comprendere l‚Äôinterazione tra hardware, software e pipeline di dati nel processo di training.</p>
<p>Si consideri un sistema ML distribuito progettato per il training su vasti set di dati, come quelli utilizzati nelle raccomandazioni di prodotti di e-commerce su larga scala. Un benchmark di training valuterebbe l‚Äôefficienza con cui il sistema si adatta a pi√π nodi, gestisce lo sharding [partizionamento] dei dati e gestisce guasti o abbandoni dei nodi durante il training.</p>
<p>I benchmark di training valutano l‚Äôutilizzo di CPU, GPU, memoria e rete durante la fase di training, guidando le ottimizzazioni del sistema. Quando si addestra un modello in un sistema ML basato su cloud, √® fondamentale capire come vengono utilizzate le risorse. Le GPU vengono sfruttate appieno? C‚Äô√® un sovraccarico di memoria non necessario? I benchmark possono evidenziare colli di bottiglia o inefficienze nell‚Äôutilizzo delle risorse, con conseguenti risparmi sui costi e miglioramenti delle prestazioni.</p>
<p>Il training di un modello ML √® subordinato alla consegna tempestiva ed efficiente dei dati. I benchmark in questo contesto valuterebbero anche l‚Äôefficienza delle pipeline di dati, la velocit√† di preelaborazione dei dati e i tempi di recupero dell‚Äôarchiviazione. Per i sistemi di analisi in tempo reale, come quelli utilizzati nel rilevamento delle frodi, la velocit√† con cui i dati di training vengono ingeriti, preelaborati e immessi nel modello pu√≤ essere critica. I benchmark valuterebbero la latenza delle pipeline di dati, l‚Äôefficienza dei sistemi di archiviazione (come SSD rispetto a HDD) e la velocit√† delle attivit√† di aumento o trasformazione dei dati.</p>
</section>
<section id="metriche" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="metriche">Metriche</h4>
<p>Se viste da una prospettiva di sistema, le metriche di training offrono informazioni che trascendono gli indicatori di prestazioni algoritmiche convenzionali. Queste metriche misurano l‚Äôefficacia di apprendimento del modello e misurano l‚Äôefficienza, la scalabilit√† e la robustezza dell‚Äôintero sistema ML durante la fase di training. Analizziamo pi√π a fondo queste metriche e il loro significato.</p>
<p>Le seguenti metriche sono spesso considerate importanti:</p>
<ol type="1">
<li><p><strong>Tempo di training:</strong> Il tempo necessario per addestrare un modello da zero fino a raggiungere un livello di prestazioni soddisfacente. Misura direttamente le risorse di elaborazione necessarie per addestrare un modello. Ad esempio, <a href="https://arxiv.org/abs/1810.04805">il BERT di Google</a> <span class="citation" data-cites="devlin2018bert">(<a href="../../references.html#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span> √® un modello di elaborazione del linguaggio naturale che richiede diversi giorni per l‚Äôaddestramento su un corpus enorme di dati di testo utilizzando pi√π GPU. Il lungo tempo di training √® una sfida significativa in termini di consumo di risorse e costi. In alcuni casi, i benchmark possono invece misurare la produttivit√† del training (campioni di training per unit√† di tempo). La produttivit√† pu√≤ essere calcolata molto pi√π velocemente e facilmente del tempo di addestramento, ma potrebbe oscurare le metriche che ci interessano davvero (ad esempio, il tempo di addestramento).</p></li>
<li><p><strong>Scalabilit√†:</strong> Quanto bene il processo di addestramento pu√≤ gestire gli aumenti delle dimensioni dei dati o della complessit√† del modello. La scalabilit√† pu√≤ essere valutata misurando il tempo di addestramento, l‚Äôutilizzo della memoria e altri consumi di risorse all‚Äôaumentare delle dimensioni dei dati o della complessit√† del modello. Il modello GPT-3 di <a href="https://arxiv.org/abs/2005.14165">OpenAI</a> <span class="citation" data-cites="brown2020language">(<a href="../../references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> ha 175 miliardi di parametri, il che lo rende uno dei pi√π grandi modelli linguistici esistenti. L‚Äôaddestramento di GPT-3 ha richiesto notevoli sforzi ingegneristici per scalare il processo di addestramento in modo da gestire le enormi dimensioni del modello. Ci√≤ ha comportato l‚Äôutilizzo di hardware specializzato, addestramento distribuito e altre tecniche per garantire che il modello potesse essere addestrato in modo efficiente.</p></li>
<li><p><strong>Utilizzo delle Risorse:</strong> La misura in cui il processo di addestramento utilizza le risorse di calcolo disponibili come CPU, GPU, memoria e I/O del disco. Un elevato utilizzo delle risorse pu√≤ indicare un processo di training efficiente, mentre un basso utilizzo pu√≤ suggerire colli di bottiglia o inefficienze. Ad esempio, il training di una rete neurale convoluzionale (CNN) per la classificazione delle immagini richiede notevoli risorse GPU. L‚Äôutilizzo di configurazioni multi-GPU e l‚Äôottimizzazione del codice di training per l‚Äôaccelerazione GPU possono migliorare notevolmente l‚Äôutilizzo delle risorse e l‚Äôefficienza del training.</p></li>
<li><p><strong>Consumo di Memoria:</strong> La quantit√† di memoria utilizzata dal processo di training. Il consumo di memoria pu√≤ essere un fattore limitante per il training di modelli o set di dati di grandi dimensioni. Ad esempio, i ricercatori di Google hanno dovuto affrontare notevoli sfide di consumo di memoria durante il training di BERT. Il modello ha centinaia di milioni di parametri, che richiedono grandi quantit√† di memoria. I ricercatori hanno dovuto sviluppare tecniche per ridurre il consumo di memoria, come il checkpointing del gradiente e il parallelismo del modello.</p></li>
<li><p><strong>Consumo Energetico:</strong> L‚Äôenergia consumata durante il training. Man mano che i modelli di apprendimento automatico diventano pi√π complessi, il consumo energetico √® diventato un fattore importante da considerare. Il training di grandi modelli di apprendimento automatico pu√≤ consumare molta energia, e quindi molto carbonio. Ad esempio, si √® stimato che l‚Äôaddestramento di GPT-3 di OpenAI abbia un‚Äôimpronta di carbonio equivalente a un viaggio in auto di 700.000 chilometri.</p></li>
<li><p><strong>Throughput:</strong> l numero di campioni di addestramento elaborati per unit√† di tempo. Un throughput [produttivit√†] pi√π elevato indica generalmente un processo di addestramento pi√π efficiente. La produttivit√† √® una metrica importante da considerare quando si addestra un sistema di raccomandazione per una piattaforma di e-commerce. Una produttivit√† elevata assicura che il modello possa elaborare rapidamente grandi volumi di dati di interazione dell‚Äôutente, il che √® fondamentale per mantenere la pertinenza e l‚Äôaccuratezza delle raccomandazioni. Ma √® anche importante capire come bilanciare la produttivit√† con i limiti di latenza. Pertanto, un vincolo di produttivit√† limitato dalla latenza viene spesso imposto agli accordi sul livello di servizio per le distribuzioni di applicazioni del data center.</p></li>
<li><p><strong>Costo:</strong> Il costo della training di un modello pu√≤ includere sia risorse computazionali che umane. Il costo √® importante quando si considera la praticit√† e la fattibilit√† del training di modelli grandi o complessi. Si stima che l‚Äôaddestramento di modelli di linguaggio grandi come GPT-3 costi milioni di dollari. Questo costo include risorse computazionali, elettriche e umane necessarie per lo sviluppo e l‚Äôaddestramento del modello.</p></li>
<li><p><strong>Tolleranza agli Errori e Robustezza:</strong> La capacit√† del processo di training di gestire guasti o errori senza bloccarsi o produrre risultati errati. Questo √® importante per garantire l‚Äôaffidabilit√† del processo di addestramento. Errori di rete o malfunzionamenti hardware possono verificarsi in uno scenario reale in cui un modello di apprendimento automatico viene addestrato su un sistema distribuito. Negli ultimi anni, √® diventato abbondantemente chiaro che gli errori derivanti dalla corruzione ‚Äúsilenziosa‚Äù dei dati sono emersi come un problema importante. Un processo di addestramento affidabile e tollerante agli errori pu√≤ recuperare da tali errori senza compromettere l‚Äôintegrit√† del modello.</p></li>
<li><p><strong>Facilit√† d‚ÄôUso e Flessibilit√†:</strong> La facilit√† con cui il processo di addestramento pu√≤ essere impostato e utilizzato e la sua flessibilit√† nella gestione di diversi tipi di dati e modelli. In aziende come Google, l‚Äôefficienza pu√≤ talvolta essere misurata dal numero di anni di ‚ÄúSoftware Engineer (SWE)‚Äù risparmiati poich√© ci√≤ si traduce direttamente in impatto. La facilit√† d‚Äôuso e la flessibilit√† possono ridurre il tempo e lo sforzo necessari per addestrare un modello. TensorFlow e PyTorch sono popolari framework di apprendimento automatico che forniscono interfacce intuitive e API flessibili per la creazione e l‚Äôaddestramento di modelli di machine-learning. Questi framework supportano molte architetture di modelli e sono dotati di strumenti che semplificano il processo di addestramento.</p></li>
<li><p><strong>Riproducibilit√†:</strong> La capacit√† di riprodurre i risultati del processo di training. La riproducibilit√† √® importante per verificare la correttezza e la validit√† di un modello. Tuttavia, le variazioni dovute alle caratteristiche stocastiche della rete spesso rendono difficile riprodurre il comportamento preciso delle applicazioni in fase di addestramento, il che pu√≤ rappresentare una sfida per il benchmarking.</p></li>
</ol>
<div class="no-row-height column-margin column-container"></div><p>Eseguendo il benchmarking per questi tipi di metriche, possiamo ottenere una visione completa delle prestazioni e dell‚Äôefficienza del processo di training da una prospettiva di sistema. Ci√≤ pu√≤ aiutare a identificare le aree di miglioramento e garantire che le risorse siano utilizzate in modo efficace.</p>
</section>
<section id="i-task" class="level4">
<h4 class="anchored" data-anchor-id="i-task">I Task</h4>
<p>Selezionare una manciata di task [attivit√†] rappresentative per il benchmarking dei sistemi di machine learning √® una sfida, perch√© l‚Äôapprendimento automatico viene applicato a vari domini con caratteristiche e requisiti unici. Ecco alcune delle sfide affrontate nella selezione di attivit√† rappresentative:</p>
<ol type="1">
<li><strong>Diversit√† di Applicazioni:</strong> L‚Äôapprendimento automatico viene utilizzato in numerosi campi, come sanit√†, finanza, elaborazione del linguaggio naturale, visione artificiale e molti altri. Ogni campo ha attivit√† specifiche che potrebbero non essere rappresentative di altri campi. Ad esempio, le attivit√† di classificazione delle immagini nella visione artificiale potrebbero non essere importanti per il rilevamento delle frodi finanziarie.</li>
<li><strong>Variabilit√† nei Tipi di Dati e nella Qualit√†:</strong> Diverse attivit√† richiedono tipi di dati diversi, come testo, immagini, video o dati numerici. La qualit√† e la disponibilit√† dei dati possono variare notevolmente tra le attivit√†, rendendo difficile selezionare attivit√† rappresentative delle sfide generali affrontate nell‚Äôapprendimento automatico.</li>
<li><strong>Complessit√† e Difficolt√† delle Attivit√†:</strong> La complessit√† delle attivit√† varia notevolmente. Alcune sono relativamente semplici, mentre altre sono molto complesse e richiedono modelli e tecniche sofisticate. Selezionare attivit√† rappresentative che coprano le complessit√† riscontrate nell‚Äôapprendimento automatico √® difficile.</li>
<li><strong>Problemi Etici e di Privacy:</strong> Alcune attivit√† possono riguardare dati sensibili o privati, come cartelle cliniche o informazioni personali. Queste attivit√† possono presentare problemi etici e di privacy che devono essere affrontati, rendendole meno adatte come attivit√† rappresentative per il benchmarking.</li>
<li><strong>Requisiti di Scalabilit√† e Risorse:</strong> Attivit√† diverse possono avere requisiti di scalabilit√† e risorse diverse. Alcune attivit√† possono richiedere ampie risorse di calcolo, mentre altre possono essere eseguite con risorse minime. Selezionare attivit√† che rappresentino i requisiti di risorse generali nell‚Äôapprendimento automatico √® difficile.</li>
<li><strong>Metriche di Valutazione:</strong> Le metriche utilizzate per valutare le prestazioni dei modelli di apprendimento automatico variano tra le attivit√†. Alcune attivit√† possono avere metriche di valutazione consolidate, mentre altre non hanno metriche chiare o standardizzate. Ci√≤ pu√≤ rendere difficile confrontare le prestazioni tra diverse attivit√†.</li>
<li><strong>Generalizzabilit√† dei Risultati:</strong> I risultati ottenuti dal benchmarking su un‚Äôattivit√† specifica potrebbero non essere generalizzabili ad altre attivit√†. Ci√≤ significa che le prestazioni di un sistema di apprendimento automatico su un‚Äôattivit√† selezionata potrebbero non essere indicative delle sue prestazioni su altre attivit√†.</li>
</ol>
<p>√à importante considerare attentamente questi fattori quando si progettano benchmark per garantire che siano significativi e pertinenti per la vasta gamma di attivit√† incontrate nel machine learning.</p>
</section>
<section id="i-benchmark" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="i-benchmark">I Benchmark</h4>
<p>Ecco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per l‚Äôaddestramento di sistemi di apprendimento automatico.</p>
<p><em><a href="https://github.com/mlcommons/training">MLPerf Training Benchmark</a></em></p>
<p>MLPerf √® una suite di benchmark progettata per misurare le prestazioni di hardware, software e servizi di apprendimento automatico. Il benchmark di MLPerf Training <span class="citation" data-cites="mattson2020mlperf">(<a href="../../references.html#ref-mattson2020mlperf" role="doc-biblioref">Mattson et al. 2020a</a>)</span> si concentra sul tempo necessario per addestrare i modelli a una metrica di qualit√† target. Include carichi di lavoro diversi, come classificazione delle immagini, rilevamento di oggetti, traduzione e apprendimento per rinforzo.</p>
<div class="no-row-height column-margin column-container"></div><p>Metriche:</p>
<ul>
<li>Tempo di training per la qualit√† target</li>
<li>Throughput (esempi al secondo)</li>
<li>Utilizzo delle risorse (CPU, GPU, memoria, I/O del disco)</li>
</ul>
<p><em><a href="https://dawn.cs.stanford.edu/benchmark/">DAWNBench</a></em></p>
<p>DAWNBench <span class="citation" data-cites="coleman2017dawnbench">(<a href="../../references.html#ref-coleman2017dawnbench" role="doc-biblioref">Coleman et al. 2019</a>)</span> √® una suite di benchmark incentrata sul tempo di training end-to-end del deep learning e sulle prestazioni dell‚Äôinferenza. Include attivit√† comuni come la classificazione delle immagini e la risposta alle domande.</p>
<div class="no-row-height column-margin column-container"><div id="ref-coleman2017dawnbench" class="csl-entry" role="listitem">
Coleman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris R√©, e Matei Zaharia. 2019. <span>¬´Analysis of <span>DAWNBench,</span> a Time-to-Accuracy Machine Learning Performance Benchmark¬ª</span>. <em>ACM SIGOPS Operating Systems Review</em> 53 (1): 14‚Äì25. <a href="https://doi.org/10.1145/3352020.3352024">https://doi.org/10.1145/3352020.3352024</a>.
</div></div><p>Metriche:</p>
<ul>
<li>Tempo di training per la precisione target</li>
<li>Latenza dell‚Äôinferenza</li>
<li>Costo (in termini di risorse di cloud computing e storage)</li>
</ul>
<p><em><a href="https://github.com/rdadolf/fathom">Fathom</a></em></p>
<p>Fathom <span class="citation" data-cites="adolf2016fathom">(<a href="../../references.html#ref-adolf2016fathom" role="doc-biblioref">Adolf et al. 2016</a>)</span> √® un benchmark dell‚ÄôUniversit√† di Harvard che valuta le prestazioni dei modelli di deep learning utilizzando un set diversificato di carichi di lavoro. Questi includono attivit√† comuni come la classificazione delle immagini, il riconoscimento vocale e la modellazione del linguaggio.</p>
<div class="no-row-height column-margin column-container"><div id="ref-adolf2016fathom" class="csl-entry" role="listitem">
Adolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. <span>¬´Fathom: <span>Reference</span> workloads for modern deep learning methods¬ª</span>. In <em>2016 IEEE International Symposium on Workload Characterization (IISWC)</em>, 1‚Äì10. IEEE; IEEE. <a href="https://doi.org/10.1109/iiswc.2016.7581275">https://doi.org/10.1109/iiswc.2016.7581275</a>.
</div></div><p>Metriche:</p>
<ul>
<li>Operazioni al secondo (per misurare l‚Äôefficienza computazionale)</li>
<li>Tempo di completamento per ogni carico di lavoro</li>
<li>Larghezza di banda della memoria</li>
</ul>
</section>
<section id="caso-duso-di-esempio" class="level4">
<h4 class="anchored" data-anchor-id="caso-duso-di-esempio">Caso d‚ÄôUso di Esempio</h4>
<p>Consideriamo uno scenario in cui vogliamo eseguire il benchmark dell‚Äôaddestramento di un modello di classificazione delle immagini su una piattaforma hardware specifica.</p>
<ol type="1">
<li><strong>Task:</strong> L‚Äôattivit√† consiste nell‚Äôaddestrare una rete neurale convoluzionale (CNN) per la classificazione delle immagini sul set di dati CIFAR-10.</li>
<li><strong>Benchmark:</strong> Possiamo usare il benchmark di addestramento MLPerf per questa attivit√†. Include un carico di lavoro di classificazione delle immagini pertinente alla nostra attivit√†.</li>
<li><strong>Metriche:</strong> Misureremo le seguenti metriche:</li>
</ol>
<ul>
<li>Tempo di addestramento per raggiungere una precisione target del 90%.</li>
<li>Throughput in termini di immagini elaborate al secondo.</li>
<li>Utilizzo di GPU e CPU durante l‚Äôaddestramento.</li>
</ul>
<p>Misurando queste metriche, possiamo valutare le prestazioni e l‚Äôefficienza del processo di addestramento sulla piattaforma hardware selezionata. Queste informazioni possono quindi essere utilizzate per identificare potenziali colli di bottiglia o aree di miglioramento.</p>
</section>
</section>
<section id="benchmark-di-inferenza" class="level3" data-number="11.4.5">
<h3 data-number="11.4.5" class="anchored" data-anchor-id="benchmark-di-inferenza"><span class="header-section-number">11.4.5</span> Benchmark di Inferenza</h3>
<p>L‚Äôinferenza nell‚Äôapprendimento automatico si riferisce all‚Äôuso di un modello addestrato per fare previsioni su dati nuovi e mai visti prima. √à la fase in cui il modello applica le conoscenze apprese per risolvere il problema per cui √® stato progettato, come la classificazione di immagini, il riconoscimento vocale o la traduzione di testo.</p>
<section id="scopo-1" class="level4">
<h4 class="anchored" data-anchor-id="scopo-1">Scopo</h4>
<p>Quando creiamo modelli di machine learning, il nostro obiettivo finale √® di distribuirli in applicazioni del mondo reale in cui possano fornire previsioni accurate e affidabili su dati nuovi e mai visti. Questo processo di utilizzo di un modello addestrato per fare previsioni √® noto come inferenza. Le prestazioni reali di un modello di apprendimento automatico possono differire in modo significativo dalle sue prestazioni su set di dati di addestramento o validazione, il che rende l‚Äôinferenza di benchmarking un passaggio cruciale nello sviluppo e nell‚Äôimplementazione di modelli di machine learning.</p>
<p>Il benchmarking dell‚Äôinferenza ci consente di valutare quanto bene un modello di apprendimento automatico funziona in scenari del mondo reale. Questa valutazione garantisce che il modello sia pratico e affidabile quando distribuito in applicazioni, fornendo una comprensione pi√π completa del comportamento del modello con dati reali. Inoltre, il benchmarking pu√≤ aiutare a identificare potenziali colli di bottiglia o limitazioni nelle prestazioni del modello. Ad esempio, se un modello impiega troppo tempo per dedurre, potrebbe non essere pratico per applicazioni in tempo reale come la guida autonoma o gli assistenti vocali.</p>
<p>L‚Äôefficienza delle risorse √® un altro aspetto critico dell‚Äôinferenza, poich√© pu√≤ essere computazionalmente intensiva e richiedere memoria e potenza di elaborazione significative. Il benchmarking aiuta a garantire che il modello sia efficiente per quanto riguarda l‚Äôutilizzo delle risorse, il che √® particolarmente importante per i dispositivi edge con capacit√† computazionali limitate, come smartphone o dispositivi IoT. Inoltre, il benchmarking ci consente di confrontare le prestazioni del nostro modello con quelli concorrenti o versioni precedenti dello stesso modello. Questo confronto √® essenziale per prendere decisioni informate su quale modello implementare in un‚Äôapplicazione specifica.</p>
<p>Infine, √® fondamentale garantire che le previsioni del modello non siano solo accurate, ma anche coerenti tra diversi dati. Il benchmarking aiuta a verificare l‚Äôaccuratezza e la coerenza del modello, assicurando che soddisfi i requisiti dell‚Äôapplicazione. Valuta inoltre la robustezza del modello, assicurando che possa gestire la variabilit√† dei dati del mondo reale e comunque fare previsioni accurate.</p>
</section>
<section id="metriche-1" class="level4">
<h4 class="anchored" data-anchor-id="metriche-1">Metriche</h4>
<ol type="1">
<li><p><strong>Precisione:</strong> La precisione √® una delle metriche pi√π importanti quando si confrontano i modelli di machine learning. Quantifica la percentuale di previsioni corrette effettuate dal modello rispetto ai valori o alle etichette reali. Ad esempio, se un modello di rilevamento dello spam riesce a classificare correttamente 95 messaggi e-mail su 100, la sua precisione verrebbe calcolata al 95%.</p></li>
<li><p><strong>Latenza:</strong> La latenza √® una metrica delle prestazioni che calcola il ritardo o l‚Äôintervallo di tempo tra la ricezione dell‚Äôinput e la produzione dell‚Äôoutput corrispondente da parte del sistema di apprendimento automatico. Un esempio che descrive chiaramente la latenza √® un‚Äôapplicazione di traduzione in tempo reale; se esiste un ritardo di mezzo secondo dal momento in cui un utente inserisce una frase al momento in cui l‚Äôapp visualizza il testo tradotto, la latenza del sistema √® di 0.5 secondi.</p></li>
<li><p><strong>Latency-Bounded Throughput:</strong> Il throughput limitato dalla latenza √® una metrica preziosa che combina gli aspetti di latenza e throughput, misurando il throughput massimo di un sistema pur rispettando un vincolo di latenza specificato. Ad esempio, in un‚Äôapplicazione di streaming video che utilizza un modello di apprendimento automatico per generare e visualizzare automaticamente i sottotitoli, il throughput limitato dalla latenza misurerebbe quanti frame video il sistema pu√≤ elaborare al secondo (throughput) garantendo al contempo che i sottotitoli vengano visualizzati con un ritardo non superiore a 1 secondo (latenza). Questa metrica √® particolarmente importante nelle applicazioni in tempo reale in cui soddisfare i requisiti di latenza √® fondamentale per l‚Äôesperienza utente.</p></li>
<li><p><strong>Throughput:</strong> Il throughput valuta la capacit√† del sistema misurando il numero di inferenze o previsioni che un modello di apprendimento automatico pu√≤ gestire entro un‚Äôunit√† di tempo specifica. Si consideri un sistema di riconoscimento vocale che utilizza una Recurrent Neural Network (RNN) come modello sottostante; se questo sistema riesce a elaborare e comprendere 50 diverse clip audio in un minuto, allora la sua velocit√† di elaborazione √® di 50 clip al minuto.</p></li>
<li><p><strong>Efficienza energetica:</strong> L‚Äôefficienza energetica √® una metrica che determina la quantit√† di energia consumata dal modello di apprendimento automatico per eseguire una singola inferenza. Un esempio lampante di ci√≤ sarebbe un modello di elaborazione del linguaggio naturale basato su un‚Äôarchitettura di rete Transformer; se utilizza 0,1 Joule di energia per tradurre una frase dall‚Äôinglese al francese, la sua efficienza energetica √® misurata a 0,1 Joule per inferenza.</p></li>
<li><p><strong>Utilizzo della memoria:</strong> L‚Äôutilizzo della memoria quantifica il volume di RAM necessario a un modello di apprendimento automatico per svolgere attivit√† di inferenza. Un esempio rilevante per illustrare questo sarebbe un sistema di riconoscimento facciale basato su una CNN; se un tale sistema richiede 150 MB di RAM per elaborare e riconoscere i volti all‚Äôinterno di un‚Äôimmagine, il suo utilizzo della memoria √® di 150 MB.</p></li>
</ol>
</section>
<section id="i-task-1" class="level4">
<h4 class="anchored" data-anchor-id="i-task-1">I Task</h4>
<p>Le sfide nella scelta di attivit√† rappresentative per il benchmarking dei sistemi di apprendimento automatico inferenziale sono, in generale, piuttosto simili alla tassonomia che abbiamo fornito per la il training. Tuttavia, per essere pignoli, discutiamone nel contesto dei sistemi di machine learning inferenziale.</p>
<ol type="1">
<li><p><strong>Diversit√† di Applicazioni:</strong> L‚Äôapprendimento automatico inferenziale √® impiegato in numerosi domini come sanit√†, finanza, intrattenimento, sicurezza e altro. Ogni dominio ha attivit√† uniche e ci√≤ che √® rappresentativo in un dominio potrebbe non esserlo in un altro. Ad esempio, un‚Äôattivit√† di inferenza per prevedere i prezzi delle azioni nel dominio finanziario potrebbe differire dalle attivit√† di riconoscimento delle immagini nel dominio medico.</p></li>
<li><p><strong>Variabilit√† nei Tipi di Dati:</strong> Diverse attivit√† di inferenza richiedono diversi tipi di dati: testo, immagini, video, dati numerici, ecc. Assicurarsi che i benchmark affrontino l‚Äôampia variet√† di tipi di dati utilizzati nelle applicazioni del mondo reale √® una sfida. Ad esempio, i sistemi di riconoscimento vocale elaborano dati audio, che sono molto diversi dai dati visivi elaborati dai sistemi di riconoscimento facciale.</p></li>
<li><p><strong>Complessit√† delle Attivit√†:</strong> La complessit√† delle attivit√† di inferenza pu√≤ variare enormemente, da attivit√† di classificazione di base ad attivit√† complesse che richiedono modelli all‚Äôavanguardia. Ad esempio, distinguere tra due categorie (classificazione binaria) √® in genere pi√π semplice che rilevare centinaia di tipi di oggetti in una scena affollata.</p></li>
<li><p><strong>Requisiti in Tempo Reale:</strong> Alcune applicazioni richiedono risposte immediate o in tempo reale, mentre altre possono consentire un certo ritardo. Nella guida autonoma, il rilevamento degli oggetti in tempo reale e il processo decisionale sono fondamentali, mentre un motore di raccomandazione per un sito Web di shopping potrebbe tollerare lievi ritardi.</p></li>
<li><p><strong>Problemi di Scalabilit√†:</strong> Data la variabilit√† della scala delle applicazioni, dai dispositivi edge ai server basati su cloud, le attivit√† devono rappresentare i diversi ambienti di elaborazione in cui si verifica l‚Äôinferenza. Ad esempio, un‚Äôattivit√† di inferenza in esecuzione sulle risorse limitate di uno smartphone √® diversa da un potente server cloud.</p></li>
<li><p><strong>Diversit√† delle Metriche di Valutazione:</strong> Le metriche utilizzate per valutare le prestazioni possono variare in modo significativo a seconda dell‚Äôattivit√†. Trovare un terreno comune o una metrica universalmente accettata per attivit√† diverse √® una sfida. Ad esempio, la precisione e il ‚Äúrecall‚Äù [richiamo] potrebbero essere vitali per un‚Äôattivit√† di diagnosi medica, mentre la produttivit√† (inferenze al secondo) potrebbero essere pi√π cruciali per le attivit√† di elaborazione video.</p></li>
<li><p><strong>Problemi Etici e di Privacy:</strong> Esistono problemi relativi all‚Äôetica e alla privacy, soprattutto in aree sensibili come il riconoscimento facciale o l‚Äôelaborazione dei dati personali. Questi problemi possono influire sulla selezione e sulla natura delle attivit√† utilizzate per il benchmarking. Ad esempio, l‚Äôutilizzo di dati facciali reali per il benchmarking pu√≤ sollevare problemi di privacy, mentre i dati sintetici potrebbero non replicare le sfide del mondo reale.</p></li>
<li><p><strong>Diversit√† Hardware:</strong> Con un‚Äôampia gamma di dispositivi, da GPU, CPU e TPU ad ASIC personalizzati utilizzati per l‚Äôinferenza, garantire che le attivit√† siano rappresentative su hardware diversi √® una sfida. Ad esempio, un‚Äôattivit√† ottimizzata per l‚Äôinferenza su una GPU potrebbe avere prestazioni non ottimali su un dispositivo edge.</p></li>
</ol>
</section>
<section id="i-benchmark-1" class="level4">
<h4 class="anchored" data-anchor-id="i-benchmark-1">I Benchmark</h4>
<p>Ecco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per sistemi di apprendimento automatico inferenziale.</p>
<p><strong><a href="https://github.com/mlcommons/inference">MLPerf Inference Benchmark</a>:</strong> MLPerf Inference √® una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una variet√† di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l‚Äôobiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza. Le sue metriche includono:</p>
<p>MLPerf Inference √® una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una variet√† di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l‚Äôobiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza.</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Latenza</li>
<li>Throughput [Produttivit√†]</li>
<li>Precisione</li>
<li>Consumo energetico</li>
</ul>
<p><strong><a href="https://ai-benchmark.com/">AI Benchmark</a>:</strong> AI Benchmark √® uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attivit√† di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un‚Äôanalisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware. Le sue metriche includono:</p>
<p>AI Benchmark √® uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e di apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attivit√† di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un‚Äôanalisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware.</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Latenza</li>
<li>Consumo energetico</li>
<li>Utilizzo della memoria</li>
<li>Throughput [Produttivit√†]</li>
</ul>
<p><strong><a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">Toolkit OpenVINO</a>:</strong> Il toolkit OpenVINO fornisce uno strumento di benchmark per misurare le prestazioni dei modelli di apprendimento profondo per varie attivit√†, come la classificazione delle immagini, il rilevamento degli oggetti e il riconoscimento facciale, su hardware Intel. Offre approfondimenti dettagliati sulle prestazioni di inferenza dei modelli su diverse configurazioni hardware. Le sue metriche includono:</p>
<p>Metriche:</p>
<ul>
<li>Tempo di inferenza</li>
<li>Throughput [Produttivit√†]</li>
<li>Latenza</li>
<li>Utilizzo di CPU e GPU</li>
</ul>
</section>
<section id="caso-duso-di-esempio-1" class="level4">
<h4 class="anchored" data-anchor-id="caso-duso-di-esempio-1">Caso d‚ÄôUso di Esempio</h4>
<p>Consideriamo uno scenario in cui vogliamo valutare le prestazioni di inferenza di un modello di rilevamento di oggetti su uno specifico dispositivo edge.</p>
<p>Task: L‚Äôattivit√† consiste nell‚Äôeseguire il rilevamento di oggetti in tempo reale su flussi video, rilevando e identificando oggetti quali veicoli, pedoni e segnali stradali.</p>
<p>Benchmark: Possiamo utilizzare AI Benchmark per questa attivit√† in quanto valuta le prestazioni di inferenza sui dispositivi edge, il che si adatta al nostro scenario.</p>
<p>Metriche: Misureremo le seguenti metriche:</p>
<ul>
<li>Tempo di inferenza per elaborare ogni fotogramma video</li>
<li>Latenza per generare i ‚Äúbounding box‚Äù per gli oggetti rilevati</li>
<li>Consumo energetico durante il processo di inferenza</li>
<li>Produttivit√† in termini di fotogrammi video elaborati al secondo</li>
</ul>
<p>Misurando queste metriche, possiamo valutare le prestazioni del modello di rilevamento di oggetti sul dispositivo edge e identificare eventuali colli di bottiglia o aree di ottimizzazione per migliorare le capacit√† di elaborazione in tempo reale.</p>
<div id="exr-perf" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;11.2: Benchmark di Inferenza - MLPerf
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Prepararsi a mettere alla prova i propri modelli di intelligenza artificiale! MLPerf √® come le Olimpiadi per le prestazioni del machine learning. In questo Colab, utilizzeremo un toolkit chiamato CK per eseguire benchmark MLPerf ufficiali, misurare la velocit√† e l‚Äôaccuratezza di un proprio modello e persino utilizzare TVM per dargli una spinta super veloce. Pronti a vedere il modello vincere la sua medaglia?</p>
<p><a href="https://colab.research.google.com/drive/1aywGlyD1ZRDtQTrQARVgL1882JcvmFK-?usp=sharing#scrollTo=tnyHAdErL72u"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="esempio-di-benchmark" class="level3 page-columns page-full" data-number="11.4.6">
<h3 data-number="11.4.6" class="anchored" data-anchor-id="esempio-di-benchmark"><span class="header-section-number">11.4.6</span> Esempio di Benchmark</h3>
<p>Per illustrare correttamente i componenti di un benchmark di sistema, possiamo esaminare il benchmark di individuazione delle parole chiave in MLPerf Tiny e spiegare la motivazione alla base di ogni decisione.</p>
<section id="task" class="level4">
<h4 class="anchored" data-anchor-id="task">Task</h4>
<p>L‚Äôindividuazione delle parole chiave √® stata selezionata come attivit√† perch√© √® un caso d‚Äôuso comune in TinyML che √® stato ben consolidato per anni. Inoltre, l‚Äôhardware tipico utilizzato per l‚Äôindividuazione delle parole chiave differisce sostanzialmente dalle offerte di altri benchmark, come l‚Äôattivit√† di riconoscimento vocale di MLPerf Inference.</p>
</section>
<section id="il-dataset" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="il-dataset">Il Dataset</h4>
<p><a href="https://www.tensorflow.org/datasets/catalog/speech_commands">Google Speech Commands</a> <span class="citation" data-cites="warden2018speech">(<a href="../../references.html#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span> √® stato selezionato come il miglior dataset per rappresentare l‚Äôattivit√†. Il dataset √® ben consolidato nella comunit√† di ricerca e ha una licenza permissiva, che consente di utilizzarlo facilmente in un benchmark.</p>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>¬´Speech commands: <span>A</span> dataset for limited-vocabulary speech recognition¬ª</span>. <em>ArXiv preprint</em> abs/1804.03209. <a href="https://arxiv.org/abs/1804.03209">https://arxiv.org/abs/1804.03209</a>.
</div></div></section>
<section id="modello" class="level4">
<h4 class="anchored" data-anchor-id="modello">Modello</h4>
<p>Il componente principale successivo √® il modello, che funger√† da carico di lavoro primario per il benchmark. Il modello dovrebbe essere ben consolidato come soluzione per l‚Äôattivit√† selezionata piuttosto che una soluzione all‚Äôavanguardia. Il modello selezionato √® un semplice modello di convoluzione separabile in profondit√†. Questa architettura non √® la soluzione all‚Äôavanguardia per l‚Äôattivit√†, ma √® ben consolidata e non progettata per una piattaforma hardware specifica come molte soluzioni all‚Äôavanguardia. Nonostante sia un benchmark di inferenza, stabilisce anche una ricetta di training di riferimento per essere completamente riproducibile e trasparente.</p>
</section>
<section id="metriche-2" class="level4">
<h4 class="anchored" data-anchor-id="metriche-2">Metriche</h4>
<p>La latenza √® stata selezionata come metrica primaria per il benchmark, poich√© i sistemi di individuazione delle parole chiave devono reagire rapidamente per mantenere la soddisfazione dell‚Äôutente. Inoltre, dato che i sistemi TinyML sono spesso alimentati a batteria, il consumo energetico viene misurato per garantire l‚Äôefficienza della piattaforma hardware. L‚Äôaccuratezza del modello viene misurata anche per garantire che le ottimizzazioni applicate da un submitter, come la quantizzazione, non degradino l‚Äôaccuratezza oltre una soglia.</p>
</section>
<section id="benchmark-harness" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-harness">Benchmark Harness</h4>
<p>MLPerf Tiny utilizza <a href="https://github.com/eembc/energyrunner">EEMBCs EnergyRunner benchmark harness</a> per caricare gli input nel modello e isolare e misurare il consumo energetico del dispositivo. Quando si misura il consumo energetico, √® fondamentale selezionare un ‚Äúharness‚Äù [imbracatura] che sia accurato ai livelli di potenza previsti dei dispositivi sottoposti a test e sufficientemente semplice da non diventare un peso per i partecipanti al benchmark.</p>
</section>
<section id="la-baseline" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="la-baseline">La Baseline</h4>
<p>Gli invii di baseline sono fondamentali per contestualizzare i risultati e come punto di riferimento per aiutare i partecipanti a iniziare. L‚Äôinvio di base dovrebbe dare priorit√† alla semplicit√† e alla leggibilit√† rispetto alle prestazioni avanzate. L‚Äôindividuazione della parola chiave della baseline utilizza un <a href="https://www.st.com/en/microcontrollers-microprocessors.html">microcontrollore STM</a> standard come hardware e <a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite come microcontrollore</a> <span class="citation" data-cites="david2021tensorflow">(<a href="../../references.html#ref-david2021tensorflow" role="doc-biblioref">David et al. 2021</a>)</span> come framework di inferenza.</p>
<div class="no-row-height column-margin column-container"><div id="ref-david2021tensorflow" class="csl-entry" role="listitem">
David, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. <span>¬´Tensorflow lite micro: <span>Embedded</span> machine learning for tinyml systems¬ª</span>. <em>Proceedings of Machine Learning and Systems</em> 3: 800‚Äì811.
</div></div></section>
</section>
<section id="sfide-e-limitazioni" class="level3 page-columns page-full" data-number="11.4.7">
<h3 data-number="11.4.7" class="anchored" data-anchor-id="sfide-e-limitazioni"><span class="header-section-number">11.4.7</span> Sfide e Limitazioni</h3>
<p>Sebbene il benchmarking fornisca una metodologia strutturata per la valutazione delle prestazioni in domini complessi come l‚Äôintelligenza artificiale e l‚Äôinformatica, il processo pone anche diverse sfide. Se non affrontati correttamente, questi ostacoli possono minare la credibilit√† e l‚Äôaccuratezza dei risultati del benchmarking. Alcune delle difficolt√† predominanti affrontate nel benchmarking includono quanto segue:</p>
<ul>
<li><p><strong>Copertura incompleta del problema:</strong> Le attivit√† di benchmarking potrebbero non rappresentare completamente lo spazio del problema. Ad esempio, i set di dati di classificazione delle immagini comuni come <a href="https://www.cs.toronto.edu/kriz/cifar.html">CIFAR-10</a> hanno una diversit√† limitata nei tipi di immagini. Gli algoritmi ottimizzati per tali benchmark potrebbero non riuscire a generalizzare bene con i set di dati del mondo reale.</p></li>
<li><p><strong>Insignificanza statistica:</strong> I benchmark devono avere prove e campioni di dati sufficienti per produrre risultati statisticamente significativi. Ad esempio, il benchmarking di un modello OCR su solo poche scansioni di testo potrebbe non catturare adeguatamente i suoi veri tassi di errore.</p></li>
<li><p><strong>Riproducibilit√† limitata:</strong> Variazioni di hardware, versioni software, basi di codice e altri fattori possono ridurre la riproducibilit√† dei risultati di benchmark. MLPerf affronta questo problema fornendo implementazioni di riferimento e specifiche ambientali.</p></li>
<li><p><strong>Disallineamento con gli obiettivi finali:</strong> I benchmark che si concentrano solo su metriche di velocit√† o accuratezza possono disallineare gli obiettivi reali come costi ed efficienza energetica. I benchmark devono riflettere tutti gli assi prestazionali critici.</p></li>
<li><p><strong>Rapida obsolescenza:</strong> A causa del rapido ritmo dei progressi nell‚Äôintelligenza artificiale e nell‚Äôinformatica, i benchmark e i loro set di dati possono rapidamente diventare obsoleti. Mantenere benchmark aggiornati √® quindi una sfida persistente.</p></li>
</ul>
<p>Ma di tutte queste, la sfida pi√π importante √® l‚Äôingegneria dei benchmark.</p>
<section id="lotteria-hardware" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="lotteria-hardware">Lotteria Hardware</h4>
<p>La <a href="https://arxiv.org/abs/2009.06489">‚Äúhardware lottery‚Äù</a> nel benchmarking dei sistemi di machine learning si riferisce alla situazione in cui il successo o l‚Äôefficienza di un modello di apprendimento automatico sono significativamente influenzati dalla compatibilit√† del modello con l‚Äôhardware sottostante <span class="citation" data-cites="chu2021discovering">(<a href="../../references.html#ref-chu2021discovering" role="doc-biblioref">Chu et al. 2021</a>)</span>. In altre parole, alcuni modelli hanno prestazioni eccezionali perch√© sono adatti alle caratteristiche o alle capacit√† specifiche dell‚Äôhardware su cui vengono eseguiti, piuttosto che perch√© sono modelli intrinsecamente superiori.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chu2021discovering" class="csl-entry" role="listitem">
Chu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. <span>¬´Discovering Multi-Hardware Mobile Models via Architecture Search¬ª</span>. In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 3022‚Äì31. IEEE. <a href="https://doi.org/10.1109/cvprw53098.2021.00337">https://doi.org/10.1109/cvprw53098.2021.00337</a>.
</div></div><div id="fig-hardware-lottery" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hardware-lottery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hardware_lottery.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-lottery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.2: Hardware Lottery.
</figcaption>
</figure>
</div>
<p>Ad esempio, alcuni modelli di apprendimento automatico possono essere progettati e ottimizzati per sfruttare le capacit√† di elaborazione parallela di acceleratori hardware specifici, come le unit√† di elaborazione grafica (GPU) o le unit√† di elaborazione tensore (TPU). Di conseguenza, questi modelli potrebbero mostrare prestazioni superiori quando vengono sottoposti a benchmark su tale hardware rispetto ad altri modelli che non sono ottimizzati per l‚Äôhardware.</p>
<p>Ad esempio, un articolo del 2018 ha introdotto una nuova architettura di rete neurale convoluzionale per la classificazione delle immagini che ha raggiunto un‚Äôaccuratezza all‚Äôavanguardia su ImageNet. Tuttavia, l‚Äôarticolo menzionava solo che il modello era stato addestrato su 8 GPU senza specificare il modello, la dimensione della memoria o altri dettagli rilevanti. Uno studio di follow-up ha cercato di riprodurre i risultati, ma ha scoperto che addestrare lo stesso modello su GPU comunemente disponibili ha ottenuto un‚Äôaccuratezza inferiore del 10%, anche dopo l‚Äôottimizzazione degli iperparametri. L‚Äôhardware originale probabilmente aveva una larghezza di banda di memoria e una potenza di calcolo molto pi√π elevate. Come altro esempio, i tempi di addestramento per modelli linguistici di grandi dimensioni possono variare drasticamente in base alle GPU utilizzate.</p>
<p>La ‚Äúhardware lottery‚Äù pu√≤ introdurre sfide e bias nel benchmarking dei sistemi di apprendimento automatico, poich√© le prestazioni del modello non dipendono esclusivamente dall‚Äôarchitettura o dall‚Äôalgoritmo del modello, ma anche dalla compatibilit√† e dalle sinergie con l‚Äôhardware sottostante. Ci√≤ pu√≤ rendere difficile confrontare equamente diversi modelli e identificare il modello migliore in base ai suoi meriti intrinseci. Pu√≤ anche portare a una situazione in cui la comunit√† converge su modelli che sono adatti all‚Äôhardware pi√π diffuso del momento, trascurando potenzialmente altri modelli che potrebbero essere superiori ma incompatibili con le attuali tendenze hardware.</p>
</section>
<section id="benchmark-engineering" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-engineering">Benchmark Engineering</h4>
<p>La lotteria hardware si verifica quando un modello di apprendimento automatico funziona in modo eccezionalmente bene o male su una configurazione hardware specifica a causa di compatibilit√† o incompatibilit√† impreviste. Il modello non √® esplicitamente progettato o ottimizzato per quell‚Äôhardware specifico dagli sviluppatori o dagli ingegneri; piuttosto, capita che si allinei o (non si allinei) con le capacit√† o le limitazioni dell‚Äôhardware. In questo caso, le prestazioni del modello sull‚Äôhardware sono un prodotto della coincidenza piuttosto che della progettazione.</p>
<p>Contrariamente alla lotteria hardware accidentale, il benchmark engineering implica l‚Äôottimizzazione o la progettazione deliberata di un modello di apprendimento automatico per funzionare eccezionalmente bene su hardware specifico, spesso per vincere benchmark o competizioni. Questa ottimizzazione intenzionale potrebbe includere la modifica dell‚Äôarchitettura, degli algoritmi o dei parametri del modello per sfruttare appieno le funzionalit√† e le capacit√† dell‚Äôhardware.</p>
</section>
<section id="problema" class="level4">
<h4 class="anchored" data-anchor-id="problema">Problema</h4>
<p>Il benchmark engineering si riferisce alla modifica o all‚Äôottimizzazione di un sistema di intelligenza artificiale per ottimizzare le prestazioni su test di benchmark specifici, spesso a scapito della generalizzabilit√† o delle prestazioni nel mondo reale. Ci√≤ pu√≤ includere la regolazione di iperparametri, dati di training o altri aspetti del sistema specificamente per ottenere punteggi elevati sulle metriche di benchmark senza necessariamente migliorare la funzionalit√† o l‚Äôutilit√† complessiva del sistema.</p>
<p>La motivazione alla base dell‚Äôingegneria dei benchmark spesso deriva dal desiderio di ottenere punteggi di prestazioni elevate per scopi di marketing o competitivi. Punteggi di benchmark elevati possono dimostrare la superiorit√† di un sistema di intelligenza artificiale rispetto ai concorrenti e possono essere un argomento chiave per la vendita per potenziali utenti o investitori. Questa pressione per ottenere buoni risultati nei benchmark a volte porta a dare priorit√† alle ottimizzazioni specifiche del benchmark rispetto a miglioramenti pi√π olistici del sistema.</p>
<p>Pu√≤ comportare diversi rischi e sfide. Uno dei rischi principali √® che il sistema di intelligenza artificiale possa funzionare meglio nelle applicazioni del mondo reale rispetto a quanto suggeriscono i punteggi di benchmark. Ci√≤ pu√≤ portare a insoddisfazione dell‚Äôutente, danni alla reputazione e potenziali problemi di sicurezza o etici. Inoltre, l‚Äôingegneria dei benchmark pu√≤ contribuire a una mancanza di trasparenza e responsabilit√† nella comunit√† dell‚Äôintelligenza artificiale, poich√© pu√≤ essere difficile discernere quanta parte delle prestazioni di un sistema di intelligenza artificiale sia dovuta a miglioramenti genuini rispetto a ottimizzazioni specifiche del benchmark.</p>
<p>La comunit√† AI deve dare priorit√† alla trasparenza e alla responsabilit√† per mitigare i rischi associati all‚Äôingegneria dei benchmark. Ci√≤ pu√≤ includere la divulgazione di eventuali ottimizzazioni o modifiche apportate specificamente per i test di benchmark e la fornitura di valutazioni pi√π complete dei sistemi AI che includono metriche delle prestazioni del mondo reale e punteggi di benchmark. I ricercatori e gli sviluppatori devono dare priorit√† a miglioramenti olistici dei sistemi AI che ne migliorino la generalizzabilit√† e la funzionalit√† in varie applicazioni anzich√© concentrarsi esclusivamente su ottimizzazioni specifiche del benchmark.</p>
</section>
<section id="problemi" class="level4">
<h4 class="anchored" data-anchor-id="problemi">Problemi</h4>
<p>Uno dei problemi principali dell‚Äôingegneria del benchmark √® che pu√≤ compromettere le prestazioni reali dei sistemi di intelligenza artificiale. Quando gli sviluppatori si concentrano sull‚Äôottimizzazione dei loro sistemi per ottenere punteggi elevati in specifici test di benchmark, potrebbero trascurare altri importanti aspetti delle prestazioni del sistema, cruciali nelle applicazioni del mondo reale. Ad esempio, un sistema di intelligenza artificiale progettato per il riconoscimento delle immagini potrebbe essere progettato per funzionare eccezionalmente bene in un test di benchmark che include un set specifico di immagini, ma necessita di aiuto per riconoscere accuratamente immagini leggermente diverse da quelle nel set di test.</p>
<p>Un‚Äôaltra area di miglioramento con l‚Äôingegneria di benchmark √® che pu√≤ comportare sistemi di intelligenza artificiale privi di generalizzabilit√†. In altre parole, mentre il sistema pu√≤ funzionare bene nel test di benchmark, potrebbe aver bisogno di aiuto per gestire una vasta gamma di input o scenari. Ad esempio, un modello di intelligenza artificiale sviluppato per l‚Äôelaborazione del linguaggio naturale potrebbe essere progettato per ottenere punteggi elevati in un test di benchmark che include un tipo specifico di testo, ma non riesce a elaborare accuratamente il testo che non rientra in quel tipo specifico.</p>
<p>Pu√≤ anche portare a risultati fuorvianti. Quando i sistemi di intelligenza artificiale sono progettati per funzionare bene nei test di benchmark, i risultati potrebbero non riflettere accuratamente le reali capacit√† del sistema. Questo pu√≤ essere problematico per gli utenti o gli investitori che si affidano ai punteggi di benchmark per prendere decisioni informate su quali sistemi di intelligenza artificiale utilizzare o in cui investire. Ad esempio, un sistema di intelligenza artificiale progettato per ottenere punteggi elevati in un test di benchmark per il riconoscimento vocale potrebbe dover essere pi√π in grado di riconoscere accuratamente il parlato in situazioni reali, portando gli utenti o gli investitori a prendere decisioni basate su informazioni imprecise.</p>
</section>
<section id="attenuazione" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="attenuazione">Attenuazione</h4>
<p>Esistono diversi modi per mitigare l‚Äôingegneria dei benchmark. La trasparenza nel processo di benchmarking √® fondamentale per mantenere l‚Äôaccuratezza e l‚Äôaffidabilit√† dei benchmark. Ci√≤ implica la divulgazione chiara delle metodologie, dei set di dati e dei criteri di valutazione utilizzati nei test di benchmark, nonch√© di eventuali ottimizzazioni o modifiche apportate al sistema di intelligenza artificiale ai fini del benchmark.</p>
<p>Un modo per ottenere trasparenza √® attraverso l‚Äôuso di benchmark open source. I benchmark open source vengono resi disponibili al pubblico, consentendo a ricercatori, sviluppatori e altre parti interessate di esaminarli, criticarli e contribuire, garantendone cos√¨ l‚Äôaccuratezza e l‚Äôaffidabilit√†. Questo approccio collaborativo facilita anche la condivisione delle ‚Äúbest practice‚Äù e lo sviluppo di benchmark pi√π solidi e completi.</p>
<p>Un esempio √® MLPerf Tiny. √à un framework open source progettato per semplificare il confronto di diverse soluzioni nel mondo di TinyML. Il suo design modulare consente di sostituire i componenti per il confronto o il miglioramento. Le implementazioni di riferimento, mostrate in verde e arancione in <a href="#fig-ml-perf" class="quarto-xref">Figura&nbsp;<span>11.3</span></a>, fungono da base per i risultati. TinyML spesso necessita di ottimizzazione nell‚Äôintero sistema e gli utenti possono contribuire concentrandosi su parti specifiche, come la quantizzazione. Il design modulare del benchmark consente agli utenti di mostrare i propri contributi e il vantaggio competitivo modificando un‚Äôimplementazione di riferimento. In breve, MLPerf Tiny offre un modo flessibile e modulare per valutare e migliorare le applicazioni TinyML, semplificando il confronto e il miglioramento di diversi aspetti della tecnologia.</p>
<div id="fig-ml-perf" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-ml-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlperf_tiny.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.3: Design modulare di MLPerf Tiny. Fonte: <span class="citation" data-cites="mattson2020mlperf">Mattson et al. (<a href="../../references.html#ref-mattson2020mlperf" role="doc-biblioref">2020a</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-mattson2020mlperf" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî, et al. 2020a. <span>¬´<span>MLPerf:</span> <span>An</span> Industry Standard Benchmark Suite for Machine Learning Performance¬ª</span>. <em>IEEE Micro</em> 40 (2): 8‚Äì16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div></div></figure>
</div>
<p>Un altro metodo per ottenere trasparenza √® attraverso la revisione paritaria dei benchmark. Ci√≤ comporta che esperti indipendenti esaminino e convalidino la metodologia, i set di dati e i risultati del benchmark per garantirne la credibilit√† e l‚Äôaffidabilit√†. La revisione paritaria pu√≤ fornire un mezzo prezioso per verificare l‚Äôaccuratezza dei test di benchmark e contribuire a creare fiducia nei risultati.</p>
<p>La standardizzazione dei benchmark √® un‚Äôaltra importante soluzione per mitigare l‚Äôingegneria dei benchmark. I benchmark standardizzati forniscono un quadro comune per la valutazione dei sistemi di intelligenza artificiale, garantendo coerenza e comparabilit√† tra diversi sistemi e applicazioni. Ci√≤ pu√≤ essere ottenuto sviluppando standard e ‚Äúbest practice‚Äù per l‚Äôintero settore per il benchmarking e tramite metriche e criteri di valutazione comuni.</p>
<p>Anche la verifica da parte di terze parti dei risultati pu√≤ essere preziosa per mitigare l‚Äôingegneria dei benchmark. Ci√≤ comporta che una terza parte indipendente verifichi i risultati di un test di benchmark per garantirne la credibilit√† e l‚Äôaffidabilit√†. La verifica di terze parti pu√≤ creare fiducia nei risultati e fornire un mezzo prezioso per convalidare le prestazioni e le capacit√† dei sistemi di intelligenza artificiale.</p>
</section>
</section>
</section>
<section id="benchmarking-del-modello" class="level2 page-columns page-full" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="benchmarking-del-modello"><span class="header-section-number">11.5</span> Benchmarking del Modello</h2>
<p>Il benchmarking dei modelli di machine learning √® importante per determinare l‚Äôefficacia e l‚Äôefficienza di vari algoritmi di apprendimento automatico nella risoluzione di compiti o problemi specifici. Analizzando i risultati ottenuti dal benchmarking, sviluppatori e ricercatori possono identificare i punti di forza e di debolezza dei loro modelli, portando a decisioni pi√π informate sulla selezione del modello e su un‚Äôulteriore ottimizzazione.</p>
<p>L‚Äôevoluzione e il progresso dei modelli di apprendimento automatico sono intrinsecamente collegati alla disponibilit√† e alla qualit√† dei set di dati. Nell‚Äôapprendimento automatico, i dati fungono da materia prima che alimenta gli algoritmi, consentendo loro di apprendere, adattarsi e, in definitiva, eseguire compiti che erano tradizionalmente di dominio degli esseri umani. Pertanto, √® importante comprendere questa storia.</p>
<section id="contesto-storico-1" class="level3 page-columns page-full" data-number="11.5.1">
<h3 data-number="11.5.1" class="anchored" data-anchor-id="contesto-storico-1"><span class="header-section-number">11.5.1</span> Contesto Storico</h3>
<p>I dataset di apprendimento automatico hanno una storia ricca e si sono evoluti in modo significativo nel corso degli anni, crescendo in dimensioni, complessit√† e diversit√† per soddisfare le richieste sempre crescenti del settore. Diamo un‚Äôocchiata pi√π da vicino a questa evoluzione, partendo da uno dei primi e pi√π iconici set di dati: MNIST.</p>
<section id="mnist-1998" class="level4">
<h4 class="anchored" data-anchor-id="mnist-1998">MNIST (1998)</h4>
<p>Il <a href="https://www.tensorflow.org/datasets/catalog/mnist">dataset MNIST</a>, creato da Yann LeCun, Corinna Cortes e Christopher J.C. Burges nel 1998, pu√≤ essere considerato una pietra miliare nella storia dei dataset di machine learning. Comprende 70.000 immagini in scala di grigi da 28x28 pixel etichettate di cifre scritte a mano (0-9). MNIST √® stato ampiamente utilizzato per il benchmarking degli algoritmi nell‚Äôelaborazione delle immagini e nell‚Äôapprendimento automatico come punto di partenza per molti ricercatori e professionisti. <a href="#fig-mnist" class="quarto-xref">Figura&nbsp;<span>11.4</span></a> mostra alcuni esempi di cifre scritte a mano.</p>
<div id="fig-mnist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mnist.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.4: Cifre scritte a mano in MNIST. Fonte: <a href="https://en.wikipedia.org/wiki/File:MnistExamplesModified.png">Suvanjanprasai.</a>
</figcaption>
</figure>
</div>
</section>
<section id="imagenet-2009" class="level4">
<h4 class="anchored" data-anchor-id="imagenet-2009">ImageNet (2009)</h4>
<p>Facciamo un salto al 2009 e vediamo l‚Äôintroduzione di <a href="https://www.tensorflow.org/datasets/catalog/imagenet2012">ImageNet</a>, che ha segnato un balzo significativo nella scala e nella complessit√† dei dataset. ImageNet √® composto da oltre 14 milioni di immagini etichettate che abbracciano pi√π di 20.000 categorie. Fei-Fei Li e il suo team lo hanno sviluppato per far progredire il riconoscimento degli oggetti e la ricerca sulla visione artificiale. Il dataset √® diventato sinonimo della ImageNet Large Scale Visual Recognition Challenge (ILSVRC), una competizione annuale cruciale nello sviluppo di modelli di deep learning, tra cui il famoso AlexNet nel 2012.</p>
</section>
<section id="coco-2014" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="coco-2014">COCO (2014)</h4>
<p>Il <a href="https://cocodataset.org/">Common Objects in Context (COCO) dataset</a> <span class="citation" data-cites="lin2014microsoft">(<a href="../../references.html#ref-lin2014microsoft" role="doc-biblioref">Lin et al. 2014</a>)</span>, rilasciato nel 2014, ha ulteriormente ampliato il panorama dei set di dati di apprendimento automatico introducendo un set pi√π ricco di annotazioni. COCO √® costituito da immagini contenenti scene complesse con pi√π oggetti e ogni immagine √® annotata con riquadri di delimitazione degli oggetti, maschere di segmentazione e didascalie. Questo set di dati √® stato determinante nel far progredire la ricerca nel rilevamento degli oggetti, nella segmentazione e nella didascalia delle immagini.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2014microsoft" class="csl-entry" role="listitem">
Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, e C Lawrence Zitnick. 2014. <span>¬´Microsoft coco: <span>Common</span> objects in context¬ª</span>. In <em>Computer Vision<span></span>ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, 740‚Äì55. Springer.
</div></div><p><img src="images/png/coco.png" class="img-fluid" alt="Il dataset Coco. Fonte: Coco."><a href="https://cocodataset.org/images/jpg/coco-examples.jpg" id="fig-coco">https://cocodataset.org/images/jpg/coco-examples.jpg</a></p>
</section>
<section id="gpt-3-2020" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="gpt-3-2020">GPT-3 (2020)</h4>
<p>Sebbene gli esempi sopra riportati si concentrino principalmente sui dataset di immagini, si sono verificati anche sviluppi significativi nei dataset di testo. Un esempio degno di nota √® GPT-3 <span class="citation" data-cites="brown2020language">(<a href="../../references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>, sviluppato da OpenAI. GPT-3 √® un modello linguistico addestrato su testo Internet eterogeneo. Sebbene il dataset utilizzato per addestrare GPT-3 non sia disponibile al pubblico, il modello stesso, costituito da 175 miliardi di parametri, √® una testimonianza della scala e della complessit√† dei moderni dataset e modelli di apprendimento automatico.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>¬´Language Models are Few-Shot Learners¬ª</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div></div></section>
<section id="presente-e-futuro" class="level4">
<h4 class="anchored" data-anchor-id="presente-e-futuro">Presente e Futuro</h4>
<p>Oggi disponiamo di una pletora di dataset che abbracciano vari domini, tra cui sanit√†, finanza, scienze sociali e altro ancora. Le seguenti caratteristiche ci aiutano a classificare lo spazio e la crescita dei dataset di apprendimento automatico che alimentano lo sviluppo del modello.</p>
<ol type="1">
<li><p><strong>Diversit√† dei Set di Dati:</strong> La variet√† di set di dati disponibili per ricercatori e ingegneri si √® ampliata notevolmente, coprendo molti campi, tra cui l‚Äôelaborazione del linguaggio naturale, il riconoscimento delle immagini e altro ancora. Questa diversit√† ha alimentato lo sviluppo di modelli di apprendimento automatico specializzati, su misura per attivit√† specifiche, come la traduzione, il riconoscimento vocale e il riconoscimento facciale.</p></li>
<li><p><strong>Volume di Dati:</strong> L‚Äôenorme volume di dati che √® diventato disponibile nell‚Äôera digitale ha anche svolto un ruolo cruciale nel progresso dei modelli di apprendimento automatico. I grandi set di dati consentono ai modelli di catturare la complessit√† e le sfumature dei fenomeni del mondo reale, portando a previsioni pi√π accurate e affidabili.</p></li>
<li><p><strong>Qualit√† e Pulizia dei Dati:</strong> La qualit√† dei dati √® un altro fattore critico che influenza le prestazioni dei modelli di apprendimento automatico. Set di dati puliti, ben etichettati e imparziali sono essenziali per modelli di addestramento solidi ed equi.</p></li>
<li><p><strong>Accesso Aperto ai Dati:</strong> La disponibilit√† di set di dati ‚Äúopen-access‚Äù ha contribuito in modo significativo anche al progresso dell‚Äôapprendimento automatico. I dati ‚Äúaperti‚Äù consentono ai ricercatori di tutto il mondo di collaborare, condividere approfondimenti e basarsi sul lavoro degli altri, portando a un‚Äôinnovazione pi√π rapida e allo sviluppo di modelli pi√π avanzati.</p></li>
<li><p><strong>Problemi di Etica e Privacy:</strong> Man mano che i set di dati crescono in dimensioni e complessit√†, le considerazioni etiche e i problemi di privacy diventano sempre pi√π importanti. √à in corso un dibattito sull‚Äôequilibrio tra lo sfruttamento dei dati per i progressi dell‚Äôapprendimento automatico e la protezione dei diritti alla privacy degli individui.</p></li>
</ol>
<p>Lo sviluppo di modelli di apprendimento automatico si basa in larga misura sulla disponibilit√† di set di dati diversificati, grandi, di alta qualit√† e ad accesso libero. Mentre andiamo avanti, affrontare le considerazioni etiche e le preoccupazioni sulla privacy associate all‚Äôuso di grandi set di dati √® fondamentale per garantire che le tecnologie di apprendimento automatico siano vantaggiose per la societ√†. C‚Äô√® una crescente consapevolezza che i dati agiscono come carburante per l‚Äôapprendimento automatico, guidando e alimentando lo sviluppo di modelli di apprendimento automatico. Di conseguenza, si sta ponendo maggiore attenzione sullo sviluppo dei set di dati stessi. Esploreremo questo aspetto in modo pi√π dettagliato nella sezione del benchmarking dei dati.</p>
</section>
</section>
<section id="metriche-del-modello" class="level3 page-columns page-full" data-number="11.5.2">
<h3 data-number="11.5.2" class="anchored" data-anchor-id="metriche-del-modello"><span class="header-section-number">11.5.2</span> Metriche del Modello</h3>
<p>La valutazione del modello di machine learning si √® evoluta da un focus ristretto sulla precisione a un approccio pi√π completo che considera una serie di fattori, da considerazioni etiche e applicabilit√† nel mondo reale a vincoli pratici come dimensioni ed efficienza del modello. Questo cambiamento riflette la maturazione del campo poich√© i modelli di apprendimento automatico vengono sempre pi√π applicati in scenari reali diversi e complessi.</p>
<section id="precisione" class="level4">
<h4 class="anchored" data-anchor-id="precisione">Precisione</h4>
<p>La precisione √® una delle metriche pi√π intuitive e comunemente utilizzate per valutare i modelli di apprendimento automatico. In sostanza, la precisione misura la percentuale di previsioni corrette effettuate dal modello rispetto a tutte le previsioni. Ad esempio, immaginiamo di aver sviluppato un modello di apprendimento automatico per classificare le immagini come contenenti o meno un gatto. Se testiamo questo modello su un set di dati di 100 immagini e ne identifica correttamente 90, calcoleremmo la sua precisione al 90%.</p>
<p>Nelle fasi iniziali dell‚Äôapprendimento automatico, la precisione era spesso la metrica principale, se non l‚Äôunica, considerata quando si valutavano le prestazioni del modello. Ci√≤ √® comprensibile, data la sua natura semplice e la facilit√† di interpretazione. Tuttavia, con il progredire del settore, i limiti del fare affidamento esclusivamente sulla precisione sono diventati pi√π evidenti.</p>
<p>Si consideri l‚Äôesempio di un modello di diagnosi medica con una precisione del 95%. Sebbene a prima vista possa sembrare impressionante, dobbiamo guardare pi√π a fondo per valutare appieno le prestazioni del modello. Supponiamo che il modello non riesca a diagnosticare accuratamente condizioni gravi che, sebbene rare, possono avere gravi conseguenze; la sua elevata precisione potrebbe non essere cos√¨ significativa. Un esempio pertinente di ci√≤ √® il <a href="https://about.google/intl/ALL_us/stories/seeingpotential/">modello di apprendimento automatico della retinopatia di Google</a>, progettato per diagnosticare la retinopatia diabetica e l‚Äôedema maculare diabetico da fotografie della retina.</p>
<p>Il modello di Google ha dimostrato livelli di precisione impressionanti in contesti di laboratorio. Tuttavia, quando √® stato distribuito in ambienti clinici reali in Thailandia, <a href="https://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-clinic-covid-diabetes-retina-disease/">ha dovuto affrontare sfide significative</a>. Nel contesto reale, il modello ha incontrato popolazioni di pazienti diverse, qualit√† delle immagini variabili e una gamma di diverse condizioni mediche a cui non era stato esposto durante il suo training. Di conseguenza, le sue prestazioni avrebbero potuto essere migliori e ha fatto fatica a mantenere gli stessi livelli di accuratezza osservati in laboratorio. Questo esempio serve come un chiaro promemoria del fatto che, sebbene un‚Äôelevata accuratezza sia un attributo importante e desiderabile per un modello di diagnosi medica, deve essere valutata insieme ad altri fattori, come la capacit√† del modello di generalizzare a diverse popolazioni e gestire condizioni reali diverse e imprevedibili, per comprenderne veramente il valore e il potenziale impatto sull‚Äôassistenza ai pazienti.</p>
<p>Allo stesso modo, se il modello funziona bene in media ma mostra significative disparit√† nelle prestazioni tra diversi gruppi demografici, anche questo sarebbe motivo di preoccupazione.</p>
<p>L‚Äôevoluzione dell‚Äôapprendimento automatico ha quindi visto uno spostamento verso un approccio pi√π olistico alla valutazione del modello, tenendo conto non solo dell‚Äôaccuratezza, ma anche di altri fattori cruciali come la correttezza, trasparenza e applicabilit√† nel mondo reale. Un esempio lampante √® il progetto <a href="http://gendershades.org/">Gender Shades</a> del MIT Media Lab, guidato da Joy Buolamwini, che evidenzia significativi pregiudizi razziali e di genere nei sistemi commerciali di riconoscimento facciale. Il progetto ha valutato le prestazioni di tre tecnologie di riconoscimento facciale sviluppate da IBM, Microsoft e Face++. Ha scoperto che tutte presentavano dei pregiudizi, con prestazioni migliori su volti maschili e dalla pelle pi√π chiara rispetto a volti femminili e dalla pelle pi√π scura.</p>
<p>Sebbene l‚Äôaccuratezza rimanga una metrica fondamentale e preziosa per la valutazione dei modelli di apprendimento automatico, √® necessario un approccio pi√π completo per valutare appieno le prestazioni di un modello. Ci√≤ significa considerare metriche aggiuntive che tengano conto di correttezza, trasparenza e applicabilit√† nel mondo reale, nonch√© condurre test rigorosi su diversi set di dati per scoprire e mitigare eventuali potenziali pregiudizi. Il passaggio a un approccio pi√π olistico alla valutazione del modello riflette la maturazione del campo e il suo crescente riconoscimento delle implicazioni nel mondo reale e delle considerazioni etiche associate all‚Äôimplementazione di modelli di apprendimento automatico.</p>
</section>
<section id="correttezza" class="level4">
<h4 class="anchored" data-anchor-id="correttezza">Correttezza</h4>
<p>La correttezza nei modelli di apprendimento automatico √® un aspetto multiforme e critico che richiede un‚Äôattenzione particolare, in particolare nelle applicazioni ad alto rischio che influenzano significativamente la vita delle persone, come nei processi di approvazione dei prestiti, nelle assunzioni e nella giustizia penale. Si riferisce al trattamento equo di tutti gli individui, indipendentemente dai loro attributi demografici o sociali come razza, genere, et√† o stato socioeconomico.</p>
<p>Affidarsi semplicemente all‚Äôaccuratezza pu√≤ essere insufficiente e potenzialmente fuorviante quando si valutano i modelli. Ad esempio, si consideri un modello di approvazione dei prestiti con un tasso di accuratezza del 95%. Sebbene questa cifra possa sembrare impressionante a prima vista, non rivela come il modello si comporta nei diversi gruppi demografici. Se questo modello discrimina costantemente un gruppo particolare, la sua accuratezza √® meno encomiabile e la sua correttezza viene messa in discussione.</p>
<p>La discriminazione pu√≤ manifestarsi in varie forme, come la discriminazione diretta, in cui un modello utilizza esplicitamente attributi sensibili come razza o genere nel suo processo decisionale, o discriminazione indiretta, in cui variabili apparentemente neutre sono correlate ad attributi sensibili, influenzando indirettamente i risultati del modello. Un esempio infame di quest‚Äôultimo √® lo strumento COMPAS utilizzato nel sistema di giustizia penale degli Stati Uniti, che ha mostrato pregiudizi razziali nel prevedere i tassi di recidiva nonostante non utilizzasse esplicitamente la razza come variabile.</p>
<p>Affrontare l‚Äôequit√† implica un attento esame delle prestazioni del modello tra gruppi diversi, identificando potenziali pregiudizi e rettificando le disparit√† attraverso misure correttive come il ribilanciamento dei set di dati, l‚Äôadeguamento dei parametri del modello e l‚Äôimplementazione di algoritmi consapevoli dell‚Äôequit√† e della correttezza. Ricercatori e professionisti sviluppano continuamente metriche e metodologie su misura per casi d‚Äôuso specifici per valutare la correttezza e l‚Äôequit√† in scenari del mondo reale. Ad esempio, l‚Äôanalisi di impatto disparato, la parit√† demografica e le pari opportunit√† sono alcune delle metriche impiegate per valutare l‚Äôequit√†/correttezza.</p>
<p>Inoltre, la trasparenza e l‚Äôinterpretabilit√† dei modelli sono fondamentali per raggiungere la correttezza. Comprendere come un modello prende decisioni pu√≤ rivelare potenziali pregiudizi e consentire alle parti interessate di ritenere responsabili gli sviluppatori. Strumenti open source come <a href="https://ai-fairness-360.org/">AI Fairness 360</a> di IBM e <a href="https://www.tensorflow.org/tfx/guide/fairness_indicators">Fairness Indicators</a> di TensorFlow sono in fase di sviluppo per facilitare le valutazioni dell‚Äôequit√†/correttezza e l‚Äôattenuazione dei pregiudizi nei modelli di apprendimento automatico.</p>
<p>Garantire l‚Äôequit√†/correttezza nei modelli di apprendimento automatico, in particolare nelle applicazioni che hanno un impatto significativo sulla vita delle persone, richiede una rigorosa valutazione delle prestazioni del modello in gruppi diversi, un‚Äôattenta identificazione e attenuazione dei pregiudizi e l‚Äôimplementazione di misure di trasparenza e interpretabilit√†. Affrontando la correttezza in modo completo, possiamo lavorare per sviluppare modelli di apprendimento automatico equi, giusti e vantaggiosi per la societ√†.</p>
</section>
<section id="complessit√†" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="complessit√†">Complessit√†</h4>
<section id="parameteri" class="level5">
<h5 class="anchored" data-anchor-id="parameteri">Parameteri*</h5>
<p>Nelle fasi iniziali del machine learning, il benchmarking dei modelli si basava spesso sui conteggi dei parametri come proxy [sostituto] per la complessit√† del modello. La logica era che pi√π parametri in genere portano a un modello pi√π complesso, che dovrebbe, a sua volta, fornire prestazioni migliori. Tuttavia, questo approccio si √® dimostrato inadeguato in quanto deve tenere conto del costo computazionale associato all‚Äôelaborazione di molti parametri.</p>
<p>Ad esempio, GPT-3, sviluppato da OpenAI, √® un modello linguistico che vanta ben 175 miliardi di parametri. Sebbene raggiunga prestazioni all‚Äôavanguardia in varie attivit√† di elaborazione del linguaggio naturale, le sue dimensioni e le risorse computazionali necessarie per eseguirlo lo rendono poco pratico per l‚Äôimplementazione in molti scenari del mondo reale, in particolare quelli con capacit√† computazionali limitate.</p>
<p>Affidarsi ai conteggi dei parametri come proxy per la complessit√† del modello non riesce a considerare anche l‚Äôefficienza del modello. Se ottimizzato per l‚Äôefficienza, un modello con meno parametri potrebbe essere altrettanto efficace, se non di pi√π, di un modello con un conteggio di parametri pi√π elevato. Ad esempio, MobileNets, sviluppato da Google, √® una famiglia di modelli progettati specificamente per dispositivi mobili ed edge. Utilizzano convoluzioni separabili in base alla profondit√† per ridurre il numero di parametri e i costi computazionali, pur mantenendo prestazioni competitive.</p>
<p>Alla luce di queste limitazioni, il settore si √® spostato verso un approccio pi√π olistico al benchmarking dei modelli che considera i conteggi dei parametri e altri fattori cruciali come le operazioni in virgola mobile al secondo (FLOP), il consumo di memoria e la latenza. I FLOP, in particolare, sono emersi come una metrica importante in quanto forniscono una rappresentazione pi√π accurata del carico computazionale imposto da un modello. Questo passaggio a un approccio pi√π completo al benchmarking dei modelli riflette il riconoscimento della necessit√† di bilanciare prestazioni e praticit√†, assicurando che i modelli siano efficaci, efficienti e implementabili in scenari reali.</p>
</section>
<section id="flop" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="flop">FLOP</h5>
<p>La dimensione di un modello di apprendimento automatico √® un aspetto essenziale che influisce direttamente sulla sua usabilit√† in scenari pratici, soprattutto quando le risorse computazionali sono limitate. Tradizionalmente, il numero di parametri in un modello veniva spesso utilizzato come proxy per le sue dimensioni, con l‚Äôipotesi di base che pi√π parametri si sarebbero tradotti in prestazioni migliori. Tuttavia, questa visione semplicistica non considera il costo computazionale dell‚Äôelaborazione di questi parametri. √à qui che entra in gioco il concetto di ‚Äúfloating-point operations per second (FLOP)‚Äù [operazioni in virgola mobile al secondo], che fornisce una rappresentazione pi√π accurata del carico computazionale imposto da un modello.</p>
<p>I FLOP misurano il numero di operazioni in virgola mobile eseguite da un modello per generare una previsione. Un modello con molti FLOP richiede risorse computazionali sostanziali per elaborare il vasto numero di operazioni, il che potrebbe renderlo poco pratico per alcune applicazioni. Al contrario, un modello con un conteggio di FLOP inferiore √® pi√π leggero e pu√≤ essere facilmente distribuito in scenari in cui le risorse computazionali sono limitate.</p>
<p><a href="#fig-flops" class="quarto-xref">Figura&nbsp;<span>11.5</span></a>, da <span class="citation" data-cites="bianco2018benchmark">(<a href="../../references.html#ref-bianco2018benchmark" role="doc-biblioref">Bianco et al. 2018</a>)</span>, mostra la relazione tra la Top-1 Accuracy su ImageNet (asse y), i G-FLOP del modello (asse x) e il conteggio dei parametri del modello (dimensione del cerchio).</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-flops" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/model_FLOPS_VS_TOP_1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.5: Un grafico che raffigura la top-1 Imagenet Accuracy rispetto al conteggio FLOP di un modello insieme al conteggio dei parametri del modello. La figura mostra un compromesso complessivo tra complessit√† e accuratezza del modello, sebbene alcune architetture del modello siano pi√π efficienti di altre. Fonte: <span class="citation" data-cites="bianco2018benchmark">Bianco et al. (<a href="../../references.html#ref-bianco2018benchmark" role="doc-biblioref">2018</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-bianco2018benchmark" class="csl-entry" role="listitem">
Bianco, Simone, Remi Cadene, Luigi Celona, e Paolo Napoletano. 2018. <span>¬´Benchmark analysis of representative deep neural network architectures¬ª</span>. <em>IEEE access</em> 6: 64270‚Äì77.
</div></div></figure>
</div>
<p>Consideriamo un esempio. BERT [Bidirectional Encoder Representations from Transformers] <span class="citation" data-cites="devlin2018bert">(<a href="../../references.html#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span>, un popolare modello di elaborazione del linguaggio naturale, ha oltre 340 milioni di parametri, il che lo rende un modello di grandi dimensioni con elevata accuratezza e prestazioni impressionanti in varie attivit√†. Tuttavia, le dimensioni di BERT, unite al suo elevato numero di FLOP, lo rendono un modello computazionalmente intensivo che potrebbe non essere adatto per applicazioni in tempo reale o per l‚Äôimplementazione su dispositivi edge con capacit√† computazionali limitate.</p>
<div class="no-row-height column-margin column-container"><div id="ref-devlin2018bert" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, e Kristina Toutanova. 2019. <span>¬´<span>BERT:</span> <span>Pre-training</span> of Deep Bidirectional Transformers for Language Understanding¬ª</span>. In <em>Proceedings of the 2019 Conference of the North</em>, 4171‚Äì86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/n19-1423">https://doi.org/10.18653/v1/n19-1423</a>.
</div></div><p>Alla luce di ci√≤, c‚Äô√® stato un crescente interesse nello sviluppo di modelli pi√π piccoli in grado di raggiungere livelli di prestazioni simili alle loro controparti pi√π grandi, pur essendo pi√π efficienti nel carico computazionale. DistilBERT, ad esempio, √® una versione pi√π piccola di BERT che mantiene il 97% delle sue prestazioni, pur essendo il 40% pi√π piccola in termini di numero di parametri. La riduzione delle dimensioni si traduce anche in un numero di FLOP inferiore, rendendo DistilBERT una scelta pi√π pratica per scenari con risorse limitate.</p>
<p>In sintesi, mentre il conteggio dei parametri fornisce un‚Äôindicazione utile della dimensione del modello, non √® una metrica completa in quanto deve considerare il costo computazionale associato all‚Äôelaborazione di questi parametri. I FLOP, d‚Äôaltro canto, offrono una rappresentazione pi√π accurata del carico computazionale di un modello e sono quindi una considerazione essenziale quando si distribuiscono modelli di apprendimento automatico in scenari reali, in particolare quando le risorse computazionali sono limitate. L‚Äôevoluzione dal basarsi esclusivamente sul conteggio dei parametri alla considerazione dei FLOP indica una maturazione nel campo, che riflette una maggiore consapevolezza dei vincoli pratici e delle sfide dell‚Äôimplementazione di modelli di apprendimento automatico in contesti diversi.</p>
</section>
<section id="efficienza" class="level5">
<h5 class="anchored" data-anchor-id="efficienza">Efficienza</h5>
<p>Anche le metriche di efficienza, come il consumo di memoria e la latenza/capacit√† di elaborazione, hanno acquisito importanza. Queste metriche sono particolarmente cruciali quando si distribuiscono modelli su dispositivi edge o in applicazioni in tempo reale, poich√© misurano la velocit√† con cui un modello pu√≤ elaborare i dati e la quantit√† di memoria richiesta. In questo contesto, le curve di Pareto vengono spesso utilizzate per visualizzare il compromesso tra diverse metriche, aiutando le parti interessate a decidere quale modello si adatta meglio alle loro esigenze.</p>
</section>
</section>
</section>
<section id="lezioni-apprese" class="level3 page-columns page-full" data-number="11.5.3">
<h3 data-number="11.5.3" class="anchored" data-anchor-id="lezioni-apprese"><span class="header-section-number">11.5.3</span> Lezioni Apprese</h3>
<p>Il benchmarking dei modelli ci ha offerto diverse preziose intuizioni che possono essere sfruttate per guidare l‚Äôinnovazione nei benchmark di sistema. La progressione dei modelli di apprendimento automatico √® stata profondamente influenzata dall‚Äôavvento delle classifiche e dalla disponibilit√† open source di modelli e set di dati. Questi elementi hanno svolto il ruolo di catalizzatori significativi, spingendo l‚Äôinnovazione e accelerando l‚Äôintegrazione di modelli all‚Äôavanguardia negli ambienti di produzione. Tuttavia, come approfondiremo ulteriormente, questi non sono gli unici fattori che contribuiscono allo sviluppo dei benchmark di apprendimento automatico.</p>
<p>Le classifiche svolgono un ruolo fondamentale nel fornire un metodo oggettivo e trasparente per ricercatori e professionisti per valutare l‚Äôefficacia di diversi modelli, classificandoli in base alle loro prestazioni nei benchmark. Questo sistema promuove un ambiente competitivo, incoraggiando lo sviluppo di modelli che non siano solo accurati ma anche efficienti. L‚ÄôImageNet Large Scale Visual Recognition Challenge (ILSVRC) ne √® un ottimo esempio, con la sua classifica annuale che contribuisce in modo significativo allo sviluppo di modelli innovativi come AlexNet.</p>
<p>L‚Äôaccesso open source a modelli e set di dati all‚Äôavanguardia diffonde ulteriormente l‚Äôapprendimento automatico, facilitando la collaborazione tra ricercatori e professionisti in tutto il mondo. Questo accesso aperto accelera il processo di test, convalida e distribuzione di nuovi modelli in ambienti di produzione, come dimostrato dall‚Äôadozione diffusa di modelli come BERT e GPT-3 in varie applicazioni, dall‚Äôelaborazione del linguaggio naturale a compiti multimodali pi√π complessi.</p>
<p>Piattaforme di collaborazione della comunit√† come Kaggle hanno rivoluzionato il settore ospitando competizioni che uniscono data scientist da tutto il mondo per risolvere problemi intricati. Benchmark specifici fungono da paletti per l‚Äôinnovazione e lo sviluppo di modelli.</p>
<p>Inoltre, la disponibilit√† di set di dati diversi e di alta qualit√† √® fondamentale per l‚Äôaddestramento e il test dei modelli di apprendimento automatico. Set di dati come ImageNet hanno svolto un ruolo fondamentale nell‚Äôevoluzione dei modelli di riconoscimento delle immagini, mentre ampi set di dati di testo hanno facilitato i progressi nei modelli di elaborazione del linguaggio naturale.</p>
<p>Infine, √® necessario supportare i contributi di istituti accademici e di ricerca. Il loro ruolo nella pubblicazione di articoli di ricerca, nella condivisione di risultati in conferenze e nella promozione della collaborazione tra varie istituzioni ha contribuito in modo significativo al progresso dei modelli e dei benchmark di apprendimento automatico.</p>
<section id="tendenze-emergenti" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="tendenze-emergenti">Tendenze Emergenti</h4>
<p>Man mano che i modelli di apprendimento automatico diventano pi√π sofisticati, lo diventano anche i benchmark necessari per valutarli in modo accurato. Ci sono diversi benchmark e dataset emergenti che stanno guadagnando popolarit√† grazie alla loro capacit√† di valutare i modelli in scenari pi√π complessi e realistici:</p>
<p><strong>Dataset Multimodali:</strong> Questi set di dati contengono pi√π tipi di dati, come testo, immagini e audio, per rappresentare meglio le situazioni del mondo reale. Un esempio √® VQA (Visual Question Answering) <span class="citation" data-cites="antol2015vqa">(<a href="../../references.html#ref-antol2015vqa" role="doc-biblioref">Antol et al. 2015</a>)</span>, in cui viene testata la capacit√† dei modelli di rispondere a domande basate su testo sulle immagini.</p>
<div class="no-row-height column-margin column-container"><div id="ref-antol2015vqa" class="csl-entry" role="listitem">
Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, e Devi Parikh. 2015. <span>¬´<span>VQA:</span> <span>Visual</span> Question Answering¬ª</span>. In <em>2015 IEEE International Conference on Computer Vision (ICCV)</em>, 2425‚Äì33. IEEE. <a href="https://doi.org/10.1109/iccv.2015.279">https://doi.org/10.1109/iccv.2015.279</a>.
</div></div><p><strong>Valutazione di Correttezza e Bias:</strong> C‚Äô√® una crescente attenzione alla creazione di benchmark che valutino l‚Äôequit√†/Correttezza e i bias [pregiudizi] dei modelli di apprendimento automatico. Esempi includono il toolkit <a href="https://ai-fairness-360.org/">AI Fairness 360</a>, che offre un set completo di metriche e set di dati per valutare il bias nei modelli.</p>
<p><strong>Generalizzazione Out-of-Distribution:</strong> Test di quanto bene i modelli funzionano su dati diversi dalla distribuzione di training originale. Questo valuta la capacit√† del modello di generalizzare a dati nuovi e inediti. Esempi di benchmark sono Wilds <span class="citation" data-cites="koh2021wilds">(<a href="../../references.html#ref-koh2021wilds" role="doc-biblioref">Koh et al. 2021</a>)</span>, RxRx e ANC-Bench.</p>
<div class="no-row-height column-margin column-container"><div id="ref-koh2021wilds" class="csl-entry" role="listitem">
Koh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. <span>¬´<span>WILDS:</span> <span>A</span> Benchmark of in-the-Wild Distribution Shifts¬ª</span>. In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, a cura di Marina Meila e Tong Zhang, 139:5637‚Äì64. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/koh21a.html">http://proceedings.mlr.press/v139/koh21a.html</a>.
</div><div id="ref-hendrycks2021natural" class="csl-entry" role="listitem">
Hendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, e Dawn Song. 2021. <span>¬´Natural Adversarial Examples¬ª</span>. In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 15262‚Äì71. IEEE. <a href="https://doi.org/10.1109/cvpr46437.2021.01501">https://doi.org/10.1109/cvpr46437.2021.01501</a>.
</div><div id="ref-xie2020adversarial" class="csl-entry" role="listitem">
Xie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, e Quoc V. Le. 2020. <span>¬´Adversarial Examples Improve Image Recognition¬ª</span>. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 816‚Äì25. IEEE. <a href="https://doi.org/10.1109/cvpr42600.2020.00090">https://doi.org/10.1109/cvpr42600.2020.00090</a>.
</div></div><p><strong>Robustezza Avversaria:</strong> Valutazione delle prestazioni del modello in caso di attacchi avversari o perturbazioni ai dati di input. Questo testa la robustezza del modello. Esempi di benchmark sono ImageNet-A <span class="citation" data-cites="hendrycks2021natural">(<a href="../../references.html#ref-hendrycks2021natural" role="doc-biblioref">Hendrycks et al. 2021</a>)</span>, ImageNet-C <span class="citation" data-cites="xie2020adversarial">(<a href="../../references.html#ref-xie2020adversarial" role="doc-biblioref">Xie et al. 2020</a>)</span> e CIFAR-10.1.</p>
<p><strong>Prestazioni nel Mondo Reale:</strong> Test di modelli su set di dati del mondo reale che corrispondono da vicino alle attivit√† finali anzich√© solo su set di dati di benchmark predefiniti. Esempi sono set di dati di imaging medico per attivit√† sanitarie o log di chat di assistenza clienti per sistemi di dialogo.</p>
<p><strong>Efficienza Energetica e di Calcolo:</strong> Benchmark che misurano le risorse di calcolo necessarie per ottenere una particolare accuratezza. Questo valuta l‚Äôefficienza del modello. Esempi sono MLPerf e Greenbench, gi√† discussi nella sezione Benchmarking dei sistemi.</p>
<p><strong>Interpretabilit√† e Spiegabilit√†:</strong> Benchmark che valutano quanto sia facile comprendere e spiegare la logica interna e le previsioni di un modello. Esempi di parametri sono la fedelt√† ai gradienti di input e la coerenza delle spiegazioni.</p>
</section>
</section>
<section id="limitazioni-e-sfide" class="level3" data-number="11.5.4">
<h3 data-number="11.5.4" class="anchored" data-anchor-id="limitazioni-e-sfide"><span class="header-section-number">11.5.4</span> Limitazioni e Sfide</h3>
<p>Sebbene i benchmark dei modelli siano uno strumento essenziale per valutare i modelli di machine learning, √® necessario affrontare diverse limitazioni e sfide per garantire che riflettano accuratamente le prestazioni in scenari reali.</p>
<p><strong>Il dataset non corrisponde a scenari reali:</strong> Spesso, i dati utilizzati nei benchmark dei modelli vengono puliti e preelaborati a tal punto che potrebbe essere necessario rappresentare accuratamente i dati che un modello incontrerebbe in applicazioni reali. Questa versione idealizzata dei dati pu√≤ portare a una sovrastima delle prestazioni di un modello. Nel caso del set di dati ImageNet, le immagini sono ben etichettate e categorizzate. Tuttavia, in uno scenario reale, un modello potrebbe dover gestire immagini sfocate che potrebbero essere meglio illuminate o scattate da angolazioni scomode. Questa discrepanza pu√≤ influire in modo significativo sulle prestazioni del modello.</p>
<p><strong>Sim2Real Gap:</strong> Il Sim2Real Gap si riferisce alla differenza nelle prestazioni di un modello quando si passa da un ambiente simulato a un ambiente reale. Questo gap √® spesso osservato nella robotica, dove un robot addestrato in un ambiente simulato ha difficolt√† a svolgere compiti nel mondo reale a causa della complessit√† e dell‚Äôimprevedibilit√† degli ambienti reali. Un robot addestrato a raccogliere oggetti in un ambiente simulato potrebbe aver bisogno di aiuto per svolgere lo stesso compito nel mondo reale perch√© l‚Äôambiente simulato non rappresenta accuratamente le complessit√† della fisica, dell‚Äôilluminazione e della variabilit√† degli oggetti del mondo reale.</p>
<p><strong>Sfide nella Creazione di Dataset:</strong> La creazione di un set di dati per il benchmarking del modello √® un‚Äôattivit√† impegnativa che richiede un‚Äôattenta considerazione di vari fattori come qualit√† dei dati, diversit√† e rappresentazione. Come discusso nella sezione di ingegneria dei dati, garantire che i dati siano puliti, imparziali e rappresentativi dello scenario del mondo reale √® fondamentale per l‚Äôaccuratezza e l‚Äôaffidabilit√† del benchmark. Ad esempio, quando si crea un set di dati per un‚Äôattivit√† correlata all‚Äôassistenza sanitaria, √® importante assicurarsi che i dati siano rappresentativi dell‚Äôintera popolazione e non distorti verso un particolare gruppo demografico. Ci√≤ garantisce che il modello funzioni bene in diverse popolazioni di pazienti.</p>
<p>I benchmark del modello sono essenziali per misurare la capacit√† di un‚Äôarchitettura di modello di risolvere un‚Äôattivit√† fissa, ma √® importante affrontare le limitazioni e le sfide ad essi associate. Ci√≤ include il garantire che il set di dati rappresenti accuratamente scenari del mondo reale, affrontare il divario Sim2Real e superare le sfide della creazione di set di dati imparziali e rappresentativi. Affrontando queste sfide e molte altre, possiamo garantire che i benchmark del modello forniscano una valutazione pi√π accurata e affidabile delle prestazioni di un modello in applicazioni del mondo reale.</p>
<p>Lo <a href="https://arxiv.org/pdf/1804.03209.pdf">Speech Commands dataset</a> e il suo successore <a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/fe131d7f5a6b38b23cc967316c13dae2-Paper-round2.pdf">MSWC</a> sono benchmark comuni per una delle applicazioni TinyML per eccellenza, l‚Äôindividuazione delle parole chiave. I comandi vocali stabiliscono metriche di errore di streaming oltre la precisione di classificazione standard top-1 pi√π pertinenti al caso d‚Äôuso di individuazione delle parole chiave. L‚Äôutilizzo di metriche pertinenti ai casi √® ci√≤ che eleva un dataset a un benchmark del modello.</p>
</section>
</section>
<section id="benchmarking-dei-dati" class="level2 page-columns page-full" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="benchmarking-dei-dati"><span class="header-section-number">11.6</span> Benchmarking dei Dati</h2>
<p>Negli ultimi anni, l‚Äôintelligenza artificiale si √® concentrata sullo sviluppo di modelli di apprendimento automatico sempre pi√π sofisticati, come i grandi modelli linguistici. L‚Äôobiettivo √® stato quello di creare modelli in grado di prestazioni di livello umano o sovrumane su un‚Äôampia gamma di attivit√†, addestrandoli su enormi set di dati. Questo approccio incentrato sul modello ha prodotto rapidi progressi, con modelli che hanno ottenuto risultati all‚Äôavanguardia su molti benchmark consolidati. <a href="#fig-superhuman-perf" class="quarto-xref">Figura&nbsp;<span>11.6</span></a> mostra le prestazioni dei sistemi di intelligenza artificiale rispetto alle prestazioni umane (contrassegnate dalla linea orizzontale a 0) in cinque applicazioni: riconoscimento della scrittura a mano, riconoscimento vocale, riconoscimento delle immagini, comprensione della lettura e comprensione del linguaggio. Negli ultimi dieci anni, le prestazioni dell‚Äôintelligenza artificiale hanno superato quelle degli esseri umani.</p>
<p>Tuttavia, le crescenti preoccupazioni su questioni come pregiudizi, sicurezza e robustezza persistono anche nei modelli che raggiungono un‚Äôelevata accuratezza sui benchmark standard. Inoltre, alcuni set di dati popolari utilizzati per la valutazione dei modelli stanno iniziando a saturarsi, con modelli che raggiungono prestazioni quasi perfette su divisioni di test esistenti <span class="citation" data-cites="kiela2021dynabench">(<a href="../../references.html#ref-kiela2021dynabench" role="doc-biblioref">Kiela et al. 2021</a>)</span>. Come semplice esempio, ci sono immagini di test nel classico dataset di cifre scritte a mano MNIST che potrebbero sembrare indecifrabili per la maggior parte dei valutatori umani, ma a cui √® stata assegnata un‚Äôetichetta quando √® stato creato il set di dati: i modelli che concordano con quelle etichette potrebbero sembrare esibire prestazioni sovrumane, ma potrebbero invece catturare solo idiosincrasie del processo di etichettatura e acquisizione dalla creazione del set di dati nel 1994. Con lo stesso spirito, i ricercatori di visione artificiale ora chiedono: ‚ÄúAbbiamo finito con ImageNet?‚Äù <span class="citation" data-cites="beyer2020we">(<a href="../../references.html#ref-beyer2020we" role="doc-biblioref">Beyer et al. 2020</a>)</span>. Ci√≤ evidenzia i limiti nell‚Äôapproccio convenzionale incentrato sul modello di ottimizzazione dell‚Äôaccuratezza su set di dati fissi tramite innovazioni architettoniche.</p>
<div class="no-row-height column-margin column-container"><div id="ref-beyer2020we" class="csl-entry" role="listitem">
Beyer, Lucas, Olivier J H√©naff, Alexander Kolesnikov, Xiaohua Zhai, e A√§ron van den Oord. 2020. <span>¬´Are we done with imagenet?¬ª</span> <em>ArXiv preprint</em> abs/2006.07159. <a href="https://arxiv.org/abs/2006.07159">https://arxiv.org/abs/2006.07159</a>.
</div></div><div id="fig-superhuman-perf" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-superhuman-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/dynabench.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-superhuman-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.6: IA e prestazioni umane. Fonte: <span class="citation" data-cites="kiela2021dynabench">Kiela et al. (<a href="../../references.html#ref-kiela2021dynabench" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-kiela2021dynabench" class="csl-entry" role="listitem">
Kiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. <span>¬´Dynabench: <span>Rethinking</span> Benchmarking in <span>NLP</span>¬ª</span>. In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 4110‚Äì24. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.naacl-main.324">https://doi.org/10.18653/v1/2021.naacl-main.324</a>.
</div></div></figure>
</div>
<p>Sta emergendo un paradigma alternativo chiamato IA incentrata sui dati. Invece di trattare i dati come statici e concentrarsi strettamente sulle prestazioni del modello, questo approccio riconosce che i modelli sono validi solo quanto i loro dati di training. Quindi, l‚Äôenfasi si sposta sulla cura di dataset di alta qualit√† che riflettano meglio la complessit√† del mondo reale, sviluppando benchmark di valutazione pi√π informativi e considerando attentamente come i dati vengono campionati, preelaborati e aumentati. L‚Äôobiettivo √® ottimizzare il comportamento del modello migliorando i dati anzich√© semplicemente ottimizzando le metriche su set di dati imperfetti. L‚Äôintelligenza artificiale incentrata sui dati esamina e migliora criticamente i dati stessi per produrre un‚Äôintelligenza artificiale utile. Ci√≤ riflette un‚Äôimportante evoluzione nella mentalit√†, poich√© il campo affronta le carenze di un benchmarking ristretto.</p>
<p>Questa sezione esplorer√† le principali differenze tra gli approcci all‚Äôintelligenza artificiale incentrati sui modelli e sui dati. Questa distinzione ha importanti implicazioni sul modo in cui eseguiamo il benchmarking dei sistemi di intelligenza artificiale. In particolare, vedremo come concentrarsi sulla qualit√† dei dati e sull‚Äôefficienza pu√≤ migliorare direttamente le prestazioni dell‚Äôapprendimento automatico come alternativa all‚Äôottimizzazione delle sole architetture dei modelli. L‚Äôapproccio incentrato sui dati riconosce che i modelli sono validi solo quanto i loro dati di addestramento. Quindi, migliorare la cura dei dati, i benchmark di valutazione e i processi di gestione dei dati pu√≤ produrre sistemi di intelligenza artificiale pi√π sicuri, pi√π equi e pi√π robusti. Ripensare al benchmarking per dare priorit√† ai dati insieme ai modelli rappresenta un‚Äôimportante evoluzione, poich√© il campo mira a fornire un impatto affidabile nel mondo reale.</p>
<section id="limitazioni-dellia-incentrata-sul-modello" class="level3" data-number="11.6.1">
<h3 data-number="11.6.1" class="anchored" data-anchor-id="limitazioni-dellia-incentrata-sul-modello"><span class="header-section-number">11.6.1</span> Limitazioni dell‚ÄôIA Incentrata sul Modello</h3>
<p>Nell‚Äôera dell‚ÄôIA incentrata sul modello, una caratteristica importante era lo sviluppo di architetture di modelli complesse. Ricercatori e professionisti hanno dedicato notevoli sforzi alla progettazione di modelli sofisticati e intricati nella ricerca di prestazioni superiori. Ci√≤ ha spesso comportato l‚Äôincorporazione di livelli aggiuntivi e la messa a punto di una moltitudine di iperparametri per ottenere miglioramenti nell‚Äôaccuratezza. Contemporaneamente, c‚Äôera una notevole enfasi sullo sfruttamento di algoritmi avanzati. Questi algoritmi, spesso in prima linea nelle ultime ricerche, sono stati impiegati per migliorare le prestazioni dei modelli di IA. L‚Äôobiettivo principale di questi algoritmi era ottimizzare il processo di apprendimento dei modelli, estraendo cos√¨ il massimo delle informazioni dai dati di addestramento.</p>
<p>Sebbene l‚Äôapproccio incentrato sul modello sia stato centrale per molti progressi nell‚ÄôIA, ha diverse aree di miglioramento. Innanzitutto, lo sviluppo di architetture di modelli complesse pu√≤ spesso portare a un overfitting. Questo √® quando il modello funziona bene sui dati di addestramento ma deve generalizzare a nuovi dati mai visti. I layer aggiuntivi e la complessit√† possono catturare il rumore nei dati di training come se fosse un pattern reale, danneggiando le prestazioni del modello su nuovi dati.</p>
<p>In secondo luogo, affidarsi ad algoritmi avanzati pu√≤ a volte oscurare la reale comprensione del funzionamento di un modello. Questi algoritmi spesso agiscono come una scatola nera, rendendo difficile interpretare il modo in cui il modello prende decisioni. Questa mancanza di trasparenza pu√≤ essere un ostacolo significativo, specialmente in applicazioni critiche come sanit√† e finanza, dove la comprensione del processo decisionale del modello √® fondamentale.</p>
<p>In terzo luogo, l‚Äôenfasi sul raggiungimento di risultati all‚Äôavanguardia su set di dati di riferimento pu√≤ a volte essere fuorviante. Questi dataset devono rappresentare in modo pi√π completo le complessit√† e la variabilit√† dei dati del mondo reale. Un modello che funziona bene su un set di dati di riferimento potrebbe non essere necessariamente generalizzato bene a dati nuovi e mai visti in un‚Äôapplicazione del mondo reale. Questa discrepanza pu√≤ portare a una falsa fiducia nelle capacit√† del modello e ostacolarne l‚Äôapplicabilit√† pratica.</p>
<p>Infine, l‚Äôapproccio incentrato sul modello spesso si basa su grandi set di dati etichettati per l‚Äôaddestramento. Tuttavia, ottenere tali set di dati richiede tempo e impegno in molti scenari del mondo reale. Questa dipendenza da grandi dataset limita anche l‚Äôapplicabilit√† dell‚ÄôIA in domini in cui i dati sono scarsi o costosi da etichettare.</p>
<p>Come risultato delle ragioni di cui sopra, e di molte altre, la comunit√† dell‚ÄôIA sta passando a un approccio pi√π incentrato sui dati. Invece di concentrarsi solo sull‚Äôarchitettura del modello, i ricercatori stanno ora dando priorit√† alla cura di set di dati di alta qualit√†, allo sviluppo di migliori benchmark di valutazione e alla considerazione di come i dati vengono campionati e preelaborati. L‚Äôidea chiave √® che i modelli sono validi solo quanto i loro dati di training. Quindi, concentrandoci sull‚Äôottenimento dei dati giusti, potremo sviluppare sistemi di intelligenza artificiale pi√π equi, sicuri e allineati con i valori umani. Questo cambiamento incentrato sui dati rappresenta un importante cambiamento di mentalit√† man mano che l‚Äôintelligenza artificiale progredisce.</p>
</section>
<section id="verso-unintelligenza-artificiale-incentrata-sui-dati" class="level3 page-columns page-full" data-number="11.6.2">
<h3 data-number="11.6.2" class="anchored" data-anchor-id="verso-unintelligenza-artificiale-incentrata-sui-dati"><span class="header-section-number">11.6.2</span> Verso un‚ÄôIntelligenza Artificiale Incentrata sui Dati</h3>
<p>L‚Äôintelligenza artificiale incentrata sui dati √® un paradigma che sottolinea l‚Äôimportanza di dataset di alta qualit√†, ben etichettati e diversificati nello sviluppo di modelli di intelligenza artificiale. Contrariamente all‚Äôapproccio incentrato sul modello, che si concentra sulla rifinitura e l‚Äôiterazione dell‚Äôarchitettura e dell‚Äôalgoritmo del modello per migliorare le prestazioni, l‚Äôintelligenza artificiale incentrata sui dati d√† priorit√† alla qualit√† dei dati di input come motore principale per migliorare le prestazioni del modello. I dati di alta qualit√† sono <a href="https://landing.ai/blog/tips-for-a-data-centric-ai-approach/">puliti, ben etichettati</a> e rappresentativi degli scenari del mondo reale che il modello incontrer√†. Al contrario, i dati di bassa qualit√† possono portare a scarse prestazioni del modello, indipendentemente dalla complessit√† o dalla sofisticatezza dell‚Äôarchitettura del modello.</p>
<p>L‚Äôintelligenza artificiale incentrata sui dati pone una forte enfasi sulla pulizia e l‚Äôetichettatura dei dati. La pulizia comporta la rimozione di valori anomali, la gestione dei valori mancanti e la risoluzione di altre incongruenze nei dati. L‚Äôetichettatura, d‚Äôaltro canto, comporta l‚Äôassegnazione di etichette significative e accurate ai dati. Entrambi questi processi sono fondamentali per garantire che il modello di intelligenza artificiale venga addestrato su dati accurati e pertinenti. Un altro aspetto importante dell‚Äôapproccio incentrato sui dati √® il ‚Äúdata augmentation‚Äù [l‚Äôaumento dei dati]. Ci√≤ comporta l‚Äôaumento artificiale delle dimensioni e della diversit√† del set di dati applicando varie trasformazioni ai dati, come rotazione, ridimensionamento e capovolgimento delle immagini di addestramento. L‚Äôaumento dei dati aiuta a migliorare la robustezza del modello e le capacit√† di generalizzazione.</p>
<p>Ci sono diversi vantaggi nell‚Äôadottare un approccio incentrato sui dati per lo sviluppo dell‚Äôintelligenza artificiale. Innanzitutto, porta a prestazioni del modello migliorate e capacit√† di generalizzazione. Assicurandosi che il modello venga addestrato su dati diversi e di alta qualit√†, il modello pu√≤ generalizzare meglio a dati nuovi e mai visti <span class="citation" data-cites="gaviria2022dollar">(<a href="../../references.html#ref-gaviria2022dollar" role="doc-biblioref">Mattson et al. 2020b</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>Inoltre, un approccio incentrato sui dati pu√≤ spesso portare a modelli pi√π semplici che sono pi√π facili da interpretare e gestire. Questo perch√© l‚Äôenfasi √® sui dati piuttosto che sull‚Äôarchitettura del modello, il che significa che i modelli pi√π semplici possono raggiungere prestazioni elevate quando addestrati su dati di alta qualit√†.</p>
<p>Il passaggio all‚ÄôIA incentrata sui dati rappresenta un significativo cambiamento di paradigma. Dando priorit√† alla qualit√† dei dati di input, questo approccio mira a migliorare le prestazioni del modello e le capacit√† di generalizzazione, portando infine a sistemi di IA pi√π robusti e affidabili. Mentre continuiamo ad avanzare nella nostra comprensione e applicazione dell‚ÄôIA, √® probabile che l‚Äôapproccio incentrato sui dati svolga un ruolo importante nel plasmare il futuro di questo campo.</p>
</section>
<section id="benchmarking-dei-dati-1" class="level3 page-columns page-full" data-number="11.6.3">
<h3 data-number="11.6.3" class="anchored" data-anchor-id="benchmarking-dei-dati-1"><span class="header-section-number">11.6.3</span> Benchmarking dei Dati</h3>
<p>Il benchmarking dei dati mira a valutare problemi comuni nei set di dati, come l‚Äôidentificazione di errori di etichetta, caratteristiche rumorose, squilibrio di rappresentazione (ad esempio, su 1000 classi in Imagenet-1K, ci sono oltre 100 categorie che sono solo tipi di cani), squilibrio di classe (dove alcune classi hanno molti pi√π campioni di altre), se i modelli addestrati su un dato set di dati possono generalizzare a caratteristiche fuori distribuzione o quali tipi di bias potrebbero esistere in un dato set di dati <span class="citation" data-cites="gaviria2022dollar">(<a href="../../references.html#ref-gaviria2022dollar" role="doc-biblioref">Mattson et al. 2020b</a>)</span>. Nella sua forma pi√π semplice, il benchmarking dei dati mira a migliorare l‚Äôaccuratezza su un set di test rimuovendo campioni di addestramento rumorosi o etichettati in modo errato mantenendo fissa l‚Äôarchitettura del modello. Recenti competizioni nel benchmarking dei dati hanno invitato i partecipanti a presentare nuove strategie di ‚Äúaugmentation‚Äù e tecniche di apprendimento attivo.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gaviria2022dollar" class="csl-entry" role="listitem">
Mattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020b. <span>¬´<span>MLPerf:</span> <span>An</span> Industry Standard Benchmark Suite for Machine Learning Performance¬ª</span>. <em>IEEE Micro</em> 40 (2): 8‚Äì16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div></div><p>Le tecniche incentrate sui dati continuano a guadagnare attenzione nel benchmarking, soprattutto perch√© i modelli di base sono sempre pi√π addestrati su obiettivi auto-supervisionati. Rispetto ai set di dati pi√π piccoli come Imagenet-1K, i set di dati pi√π grandi comunemente usati nell‚Äôapprendimento auto-supervisionato, come Common Crawl, OpenImages e LAION-5B, contengono quantit√† maggiori di rumore, duplicati, bias e dati potenzialmente offensivi.</p>
<p><a href="https://www.datacomp.ai/">DataComp</a> √® una competizione di dataset lanciata di recente che ha come obiettivo la valutazione di grandi corpora. DataComp si concentra sulle coppie linguaggio-immagine usate per addestrare i modelli CLIP. Il documento introduttivo rileva che quando il budget di elaborazione totale per l‚Äôaddestramento √® costante, i modelli CLIP pi√π performanti nelle attivit√† downstream, come la classificazione ImageNet, vengono addestrati solo sul 30% del pool di campioni disponibile. Ci√≤ suggerisce che un corretto filtraggio di grandi corpora √® fondamentale per migliorare l‚Äôaccuratezza dei modelli di base. Analogamente, Demystifying CLIP Data <span class="citation" data-cites="xu2023demystifying">(<a href="../../references.html#ref-xu2023demystifying" role="doc-biblioref">Xu et al. 2023</a>)</span> chiede se il successo di CLIP sia attribuibile all‚Äôarchitettura o al set di dati.</p>
<div class="no-row-height column-margin column-container"><div id="ref-xu2023demystifying" class="csl-entry" role="listitem">
Xu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, e Christoph Feichtenhofer. 2023. <span>¬´Demystifying <span>CLIP</span> Data¬ª</span>. <em>ArXiv preprint</em> abs/2309.16671. <a href="https://arxiv.org/abs/2309.16671">https://arxiv.org/abs/2309.16671</a>.
</div></div><p><a href="https://www.dataperf.org/">DataPerf</a> √® un altro recente lavoro incentrato sul benchmarking dei dati in varie modalit√†. DataPerf offre round di competizione online per stimolare il miglioramento dei dataset. L‚Äôofferta inaugurale √® stata lanciata con sfide in termini di visione, parlato, acquisizione, debug e prompt di testo per la generazione di immagini.</p>
</section>
<section id="efficienza-dei-dati" class="level3 page-columns page-full" data-number="11.6.4">
<h3 data-number="11.6.4" class="anchored" data-anchor-id="efficienza-dei-dati"><span class="header-section-number">11.6.4</span> Efficienza dei Dati</h3>
<p>Man mano che i modelli di apprendimento automatico diventano pi√π grandi e complessi e le risorse di elaborazione diventano pi√π scarse di fronte alla crescente domanda, diventa difficile soddisfare i requisiti di elaborazione anche con le flotte di machine learning pi√π grandi. Per superare queste sfide e garantire la scalabilit√† del sistema di apprendimento automatico, √® necessario esplorare nuove opportunit√† che aumentino gli approcci convenzionali alla scalabilit√† delle risorse.</p>
<p>Migliorare la qualit√† dei dati pu√≤ essere un metodo utile per avere un impatto significativo sulle prestazioni del sistema di apprendimento automatico. Uno dei principali vantaggi del miglioramento della qualit√† dei dati √® il potenziale di poter ridurre le dimensioni del set di dati di addestramento mantenendo o addirittura migliorando le prestazioni del modello. Questa riduzione delle dimensioni dei dati √® direttamente correlata alla quantit√† di tempo di addestramento richiesto, consentendo cos√¨ ai modelli di convergere in modo pi√π rapido ed efficiente. Raggiungere questo equilibrio tra qualit√† dei dati e dimensioni del set di dati √® un compito impegnativo che richiede lo sviluppo di metodi, algoritmi e tecniche sofisticati.</p>
<p>Possono essere adottati diversi approcci per migliorare la qualit√† dei dati. Questi metodi includono e non sono limitati a quanto segue:</p>
<ul>
<li><strong>Pulizia dei Dati:</strong> Ci√≤ comporta la gestione dei valori mancanti, la correzione degli errori e la rimozione dei valori anomali. I dati puliti assicurano che il modello non stia imparando da rumore o imprecisioni.</li>
<li><strong>Interpretabilit√† e Spiegabilit√† dei Dati:</strong> Le tecniche comuni includono LIME <span class="citation" data-cites="ribeiro2016should">(<a href="../../references.html#ref-ribeiro2016should" role="doc-biblioref">Ribeiro, Singh, e Guestrin 2016</a>)</span>, che fornisce informazioni sui limiti decisionali dei classificatori, e valori Shapley <span class="citation" data-cites="lundberg2017unified">(<a href="../../references.html#ref-lundberg2017unified" role="doc-biblioref">Lundberg e Lee 2017</a>)</span>, che stimano l‚Äôimportanza dei singoli campioni nel contribuire alle previsioni di un modello.</li>
<li><strong>Feature Engineering:</strong> Trasformare o creare nuove funzionalit√† pu√≤ migliorare significativamente le prestazioni del modello fornendo informazioni pi√π pertinenti per l‚Äôapprendimento.</li>
<li><strong>Data Augmentation:</strong> Aumentare i dati creando nuovi campioni tramite varie trasformazioni pu√≤ aiutare a migliorare la robustezza e la generalizzazione del modello.</li>
<li><strong>Active Learning:</strong> Questo √® un approccio di apprendimento semi-supervisionato in cui il modello interroga attivamente un ‚Äúoracolo‚Äù umano per etichettare i campioni pi√π informativi <span class="citation" data-cites="coleman2022similarity">(<a href="../../references.html#ref-coleman2022similarity" role="doc-biblioref">Coleman et al. 2022</a>)</span>. Ci√≤ garantisce che il modello venga addestrato sui dati pi√π rilevanti.</li>
<li>Riduzione della Dimensionalit√†: Tecniche come PCA possono ridurre il numero di feature in un set di dati, riducendo cos√¨ la complessit√† e il tempo di addestramento.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-ribeiro2016should" class="csl-entry" role="listitem">
Ribeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. <span>¬´<span>‚Äù</span> Why should i trust you?<span>‚Äù</span> Explaining the predictions of any classifier¬ª</span>. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>, 1135‚Äì44.
</div><div id="ref-lundberg2017unified" class="csl-entry" role="listitem">
Lundberg, Scott M., e Su-In Lee. 2017. <span>¬´A Unified Approach to Interpreting Model Predictions¬ª</span>. In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765‚Äì74. <a href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</a>.
</div><div id="ref-coleman2022similarity" class="csl-entry" role="listitem">
Coleman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert D. Nowak, Roshan Sumbaly, Matei Zaharia, e I. Zeki Yalniz. 2022. <span>¬´Similarity Search for Efficient Active Learning and Search of Rare Concepts¬ª</span>. In <em>Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022</em>, 6402‚Äì10. AAAI Press. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20591">https://ojs.aaai.org/index.php/AAAI/article/view/20591</a>.
</div></div><p>Esistono molti altri metodi in circolazione. Ma l‚Äôobiettivo √® lo stesso. Affinare il set di dati e garantire che sia della massima qualit√† pu√≤ ridurre il tempo di addestramento necessario per la convergenza dei modelli. Tuttavia, per raggiungere questo obiettivo √® necessario sviluppare e implementare metodi, algoritmi e tecniche sofisticati in grado di pulire, preelaborare e aumentare i dati, mantenendo al contempo i campioni pi√π informativi. Questa √® una sfida continua che richieder√† una continua ricerca e innovazione nel campo dell‚Äôapprendimento automatico.</p>
</section>
</section>
<section id="la-tripletta" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="la-tripletta"><span class="header-section-number">11.7</span> La Tripletta</h2>
<p>Mentre i benchmark di sistema, modello e dati sono stati tradizionalmente studiati in modo isolato, si sta diffondendo la consapevolezza che per comprendere e far progredire completamente l‚ÄôIA, dobbiamo adottare una visione pi√π olistica. Iterando tra sistemi di benchmarking, modelli e dataset insieme, potrebbero emergere nuove intuizioni che non sono evidenti quando questi componenti vengono analizzati separatamente. Le prestazioni del sistema influiscono sulla precisione del modello, le capacit√† del modello determinano le esigenze dei dati e le caratteristiche dei dati determinano i requisiti del sistema.</p>
<p>Il benchmarking della triade di sistema, modello e dati in modo integrato porter√† probabilmente a scoperte sulla progettazione congiunta dei sistemi di IA, sulle propriet√† di generalizzazione dei modelli e sul ruolo della cura e della qualit√† dei dati nel consentire le prestazioni. Piuttosto che benchmark ristretti di singoli componenti, il futuro dell‚ÄôIA richiede benchmark che valutino la relazione simbiotica tra piattaforme di elaborazione, algoritmi e dati di training. Questa prospettiva a livello di sistema sar√† fondamentale per superare le attuali limitazioni e sbloccare il prossimo livello di capacit√† dell‚ÄôIA.</p>
<p><a href="#fig-benchmarking-trifecta" class="quarto-xref">Figura&nbsp;<span>11.7</span></a> illustra i molti modi potenziali per far interagire tra loro il benchmarking dei dati, quello dei modelli e quello dell‚Äôinfrastruttura di sistema. L‚Äôesplorazione di queste complesse interazioni probabilmente porter√† alla scoperta di nuove opportunit√† di ottimizzazione e capacit√† di miglioramento. La tripletta di benchmark di dati, modelli e sistemi offre un ricco spazio per la progettazione congiunta e la co-ottimizzazione.</p>
<div id="fig-benchmarking-trifecta" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-benchmarking-trifecta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/trifecta.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-benchmarking-trifecta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;11.7: La tripletta del Benchmarking.
</figcaption>
</figure>
</div>
<p>Sebbene questa prospettiva integrata rappresenti una tendenza emergente, il settore ha ancora molto da scoprire sulle sinergie e i compromessi tra questi componenti. Mentre eseguiamo il benchmarking iterativo di combinazioni di dati, modelli e sistemi, emergeranno nuove intuizioni che rimangono nascoste quando questi elementi vengono studiati separatamente. Questo approccio di benchmarking multiforme che traccia le intersezioni di dati, algoritmi e hardware promette di essere una strada fruttuosa per importanti progressi nell‚Äôintelligenza artificiale, anche se √® ancora nelle sue fasi iniziali.</p>
</section>
<section id="benchmark-per-tecnologie-emergenti" class="level2 page-columns page-full" data-number="11.8">
<h2 data-number="11.8" class="anchored" data-anchor-id="benchmark-per-tecnologie-emergenti"><span class="header-section-number">11.8</span> Benchmark per Tecnologie Emergenti</h2>
<p>Date le loro significative differenze rispetto alle tecniche esistenti, le tecnologie emergenti possono essere particolarmente difficili da progettare per i benchmark. I benchmark standard utilizzati per le tecnologie esistenti potrebbero non evidenziare le caratteristiche chiave del nuovo approccio. Al contrario, i nuovi benchmark potrebbero essere visti come artificiosi per favorire la tecnologia emergente rispetto ad altre. Potrebbero essere cos√¨ diversi dai benchmark esistenti da non poter essere compresi e perdere significato. Pertanto, i benchmark per le tecnologie emergenti devono bilanciare equit√†, applicabilit√† e facilit√† di confronto con quelli esistenti.</p>
<p>Un esempio di tecnologia emergente in cui il benchmarking si √® dimostrato particolarmente difficile √® nel <a href="@sec-neuromorphic">Neuromorphic Computing</a>. Utilizzando il cervello come fonte di ispirazione per un‚Äôintelligenza generale scalabile, robusta ed efficiente dal punto di vista energetico, il calcolo neuromorfico <span class="citation" data-cites="schuman2022opportunities">(<a href="../../references.html#ref-schuman2022opportunities" role="doc-biblioref">Schuman et al. 2022</a>)</span> incorpora direttamente meccanismi biologicamente realistici sia negli algoritmi di calcolo che nell‚Äôhardware, come le reti neurali spiking <span class="citation" data-cites="maass1997networks">(<a href="../../references.html#ref-maass1997networks" role="doc-biblioref">Maass 1997</a>)</span> e le architetture non-von Neumann architectures per eseguirle <span class="citation" data-cites="davies2018loihi modha2023neural">(<a href="../../references.html#ref-davies2018loihi" role="doc-biblioref">Davies et al. 2018</a>; <a href="../../references.html#ref-modha2023neural" role="doc-biblioref">Modha et al. 2023</a>)</span>. Da una prospettiva full-stack di modelli, tecniche di training e sistemi hardware, il calcolo neuromorfico differisce dall‚Äôhardware e dall‚Äôintelligenza artificiale convenzionali. Pertanto, esiste una sfida fondamentale nello sviluppo di benchmark equi e utili per guidare la tecnologia.</p>
<div class="no-row-height column-margin column-container"><div id="ref-schuman2022opportunities" class="csl-entry" role="listitem">
Schuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. <span>¬´Opportunities for neuromorphic computing algorithms and applications¬ª</span>. <em>Nature Computational Science</em> 2 (1): 10‚Äì19. <a href="https://doi.org/10.1038/s43588-021-00184-y">https://doi.org/10.1038/s43588-021-00184-y</a>.
</div><div id="ref-maass1997networks" class="csl-entry" role="listitem">
Maass, Wolfgang. 1997. <span>¬´Networks of spiking neurons: <span>The</span> third generation of neural network models¬ª</span>. <em>Neural Networks</em> 10 (9): 1659‚Äì71. <a href="https://doi.org/10.1016/s0893-6080(97)00011-7">https://doi.org/10.1016/s0893-6080(97)00011-7</a>.
</div><div id="ref-davies2018loihi" class="csl-entry" role="listitem">
Davies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. <span>¬´Loihi: <span>A</span> Neuromorphic Manycore Processor with On-Chip Learning¬ª</span>. <em>IEEE Micro</em> 38 (1): 82‚Äì99. <a href="https://doi.org/10.1109/mm.2018.112130359">https://doi.org/10.1109/mm.2018.112130359</a>.
</div><div id="ref-modha2023neural" class="csl-entry" role="listitem">
Modha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. <span>¬´Neural inference at the frontier of energy, space, and time¬ª</span>. <em>Science</em> 382 (6668): 329‚Äì35. <a href="https://doi.org/10.1126/science.adh1174">https://doi.org/10.1126/science.adh1174</a>.
</div><div id="ref-yik2023neurobench" class="csl-entry" role="listitem">
Yik, Jason, Soikat Hasan Ahmed, Zergham Ahmed, Brian Anderson, Andreas G. Andreou, Chiara Bartolozzi, Arindam Basu, et al. 2023. <span>¬´<span>NeuroBench:</span> <span>Advancing</span> Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking¬ª</span>. <a href="https://arxiv.org/abs/2304.04640">https://arxiv.org/abs/2304.04640</a>.
</div></div><p>Un‚Äôiniziativa in corso per sviluppare benchmark neuromorfici standard √® NeuroBench <span class="citation" data-cites="yik2023neurobench">(<a href="../../references.html#ref-yik2023neurobench" role="doc-biblioref">Yik et al. 2023</a>)</span>. Per un benchmarking adeguato del neuromorfico, NeuroBench segue principi di alto livello di <em>inclusivit√†</em> attraverso l‚Äôapplicabilit√† di attivit√† e metriche sia alle soluzioni neuromorfiche che non neuromorfiche, <em>attuabilit√†</em> dell‚Äôimplementazione utilizzando strumenti comuni e aggiornamenti <em>iterativi</em> per continuare a garantire la pertinenza man mano che il campo cresce rapidamente. NeuroBench e altri benchmark per le tecnologie emergenti forniscono una guida critica per le tecniche future, che potrebbero essere necessarie man mano che i limiti di scalabilit√† degli approcci esistenti si avvicinano.</p>
</section>
<section id="conclusione" class="level2" data-number="11.9">
<h2 data-number="11.9" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">11.9</span> Conclusione</h2>
<p>Ci√≤ che viene misurato viene migliorato. Questo capitolo ha esplorato la natura multiforme del benchmarking che abbraccia sistemi, modelli e dati. Il benchmarking √® importante per far progredire l‚ÄôIA in quanto fornisce le misurazioni essenziali per monitorare i progressi.</p>
<p>I benchmark del sistema ML consentono l‚Äôottimizzazione attraverso metriche di velocit√†, efficienza e scalabilit√†. I benchmark del modello guidano l‚Äôinnovazione attraverso attivit√† e metriche standardizzate oltre l‚Äôaccuratezza. I benchmark dei dati evidenziano problemi di qualit√†, equilibrio e rappresentazione.</p>
<p>√à importante notare che la valutazione di questi componenti in modo isolato presenta dei limiti. In futuro, sar√† probabilmente utilizzato un benchmarking pi√π integrato per esplorare l‚Äôinterazione tra benchmark di sistema, modello e dati. Questa visione promette nuove intuizioni sulla progettazione congiunta di dati, algoritmi e infrastrutture.</p>
<p>Man mano che l‚ÄôIA diventa pi√π complessa, il benchmarking completo diventa ancora pi√π critico. Gli standard devono evolversi continuamente per misurare nuove capacit√† e rivelare limitazioni. Una stretta collaborazione tra settore, mondo accademico, etichette nazionali, ecc. √® essenziale per sviluppare benchmark rigorosi, trasparenti e socialmente utili.</p>
<p>Il benchmarking fornisce la bussola per guidare il progresso nell‚ÄôIA. Misurando costantemente e condividendo apertamente i risultati, possiamo orientarci verso sistemi performanti, robusti e affidabili. Se l‚ÄôIA deve soddisfare adeguatamente le esigenze sociali e umane, deve essere sottoposta a benchmarking tenendo a mente gli interessi dell‚Äôumanit√†. A tal fine, ci sono aree emergenti, come il benchmarking della sicurezza dei sistemi di IA, ma questo √® per un altro giorno e qualcosa di cui possiamo discutere ulteriormente in ‚ÄúGenerative AI‚Äù!</p>
<p>Il benchmarking √® un argomento in continua evoluzione. L‚Äôarticolo <a href="https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b">The Olympics of AI: Benchmarking Machine Learning Systems</a> copre diversi sottocampi emergenti nel benchmarking dell‚ÄôIA, tra cui robotica, realt√† estesa e calcolo neuromorfico che incoraggiamo il lettore ad approfondire.</p>
</section>
<section id="sec-benchmarking-ai-resource" class="level2" data-number="11.10">
<h2 data-number="11.10" class="anchored" data-anchor-id="sec-benchmarking-ai-resource"><span class="header-section-number">11.10</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/17udz3gxeYF3r3X1r4ePwu1I9H8ljb53W3ktFSmuDlGs/edit?usp=drive_link&amp;resourcekey=0-Espn0a0x81kl2txL_jIWjw">Perch√© il benchmarking √® importante?</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/18PI_0xmcW1xwwfcjmj25PikqBM_92vQfOXFV4hah-6I/edit?resourcekey=0-KO3HQcDAsR--jgbKd5cp4w#slide=id.g94db9f9f78_0_2">Benchmarking di inferenza embedded.</a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-cuda" class="quarto-xref">Esercizio&nbsp;<span>11.1</span></a></p></li>
<li><p><a href="#exr-perf" class="quarto-xref">Esercizio&nbsp;<span>11.2</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laboratori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Oltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l‚Äôesperienza di apprendimento.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/hw_acceleration/hw_acceleration.it.html" class="pagination-link" aria-label="Accelerazione IA">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/ondevice_learning/ondevice_learning.it.html" class="pagination-link" aria-label="Apprendimento On-Device">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/benchmarking/benchmarking.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>