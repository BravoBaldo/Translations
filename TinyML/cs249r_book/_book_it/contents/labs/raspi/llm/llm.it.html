<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Small Language Models (SLM) ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../contents/labs/shared/shared.it.html" rel="next">
<link href="../../../../contents/labs/raspi/object_detection/object_detection.it.html" rel="prev">
<link href="../../../../favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../../scripts/ai_menu/dist/bundle.js" defer=""></script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.it.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/llm/llm.it.html">Small Language Models (SLM)</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2b8d2ba3a08f8b4ab16660e3d0aa1206" class="alert alert-primary hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>‚≠ê [18 Ott] <b>Abbiamo raggiunto 1.000 stelle GitHub</b> üéâ Grazie a voi, Arduino e SEEED hanno donato kit hardware di IA per <a href="https://tinyml.seas.harvard.edu/4D/pastEvents">per i workshop TinyML</a> nei paesi in via di sviluppo <br> üéì [15 Nov] La <a href="https://www.edgeaifoundation.org/">EDGE AI Foundation</a> <strong>equipara i fondi per borse di studio accademiche</strong> per ogni nuovo GitHub ‚≠ê (fino a 10.000 stelle). <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per supportare!</a> üôè <br> üöÄ <b>La nostra missione. 1 ‚≠ê = 1 üë©‚Äçüéì Studente</b>. Ogni stella racconta una storia: studenti che acquisiscono conoscenze e sostenitori che guidano la missione. Insieme, stiamo facendo la differenza.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/about/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/ai/socratiq.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABORATORI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/part_LABS.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">LABORATORI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/overview.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/part_nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">part_nicla_vision.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/part_xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">part_xiao_esp32s3.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/part_raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">part_raspi.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/part_shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">part_shared.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#panoramica" id="toc-panoramica" class="nav-link active" data-scroll-target="#panoramica">Panoramica</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a>
  <ul>
  <li><a href="#raspberry-pi-active-cooler" id="toc-raspberry-pi-active-cooler" class="nav-link" data-scroll-target="#raspberry-pi-active-cooler">Raspberry Pi Active Cooler</a></li>
  </ul></li>
  <li><a href="#generative-ai-genai" id="toc-generative-ai-genai" class="nav-link" data-scroll-target="#generative-ai-genai">Generative AI (GenAI)</a>
  <ul>
  <li><a href="#large-language-models-llm" id="toc-large-language-models-llm" class="nav-link" data-scroll-target="#large-language-models-llm">Large Language Models (LLM)</a></li>
  <li><a href="#modelli-chiusi-e-aperti" id="toc-modelli-chiusi-e-aperti" class="nav-link" data-scroll-target="#modelli-chiusi-e-aperti">Modelli Chiusi e Aperti:</a></li>
  <li><a href="#small-language-models-slm" id="toc-small-language-models-slm" class="nav-link" data-scroll-target="#small-language-models-slm">Small Language Models (SLM)</a></li>
  </ul></li>
  <li><a href="#ollama" id="toc-ollama" class="nav-link" data-scroll-target="#ollama">Ollama</a>
  <ul>
  <li><a href="#installazione-di-ollama" id="toc-installazione-di-ollama" class="nav-link" data-scroll-target="#installazione-di-ollama">Installazione di Ollama</a></li>
  <li><a href="#meta-llama-3.2-1b3b" id="toc-meta-llama-3.2-1b3b" class="nav-link" data-scroll-target="#meta-llama-3.2-1b3b">Meta Llama 3.2 1B/3B</a></li>
  <li><a href="#google-gemma-2-2b" id="toc-google-gemma-2-2b" class="nav-link" data-scroll-target="#google-gemma-2-2b">Google Gemma 2 2B</a></li>
  <li><a href="#microsoft-phi3.5-3.8b" id="toc-microsoft-phi3.5-3.8b" class="nav-link" data-scroll-target="#microsoft-phi3.5-3.8b">Microsoft Phi3.5 3.8B</a></li>
  <li><a href="#modelli-multimodali" id="toc-modelli-multimodali" class="nav-link" data-scroll-target="#modelli-multimodali">Modelli Multimodali</a></li>
  <li><a href="#ispezione-delle-risorse-locali" id="toc-ispezione-delle-risorse-locali" class="nav-link" data-scroll-target="#ispezione-delle-risorse-locali">Ispezione delle risorse locali</a></li>
  </ul></li>
  <li><a href="#libreria-python-ollama" id="toc-libreria-python-ollama" class="nav-link" data-scroll-target="#libreria-python-ollama">Libreria Python Ollama</a>
  <ul>
  <li><a href="#chiamata-di-funzione" id="toc-chiamata-di-funzione" class="nav-link" data-scroll-target="#chiamata-di-funzione">Chiamata di Funzione</a>
  <ul class="collapse">
  <li><a href="#ma-cos√®-esattamente-la-chiamata-di-funzione" id="toc-ma-cos√®-esattamente-la-chiamata-di-funzione" class="nav-link" data-scroll-target="#ma-cos√®-esattamente-la-chiamata-di-funzione">Ma cos‚Äô√® esattamente la ‚Äúchiamata di funzione‚Äù?</a></li>
  <li><a href="#creiamo-un-progetto." id="toc-creiamo-un-progetto." class="nav-link" data-scroll-target="#creiamo-un-progetto.">Creiamo un progetto.</a></li>
  </ul></li>
  <li><a href="#importazione-delle-librerie" id="toc-importazione-delle-librerie" class="nav-link" data-scroll-target="#importazione-delle-librerie">1. Importazione delle Librerie</a></li>
  <li><a href="#definizione-di-input-e-modello" id="toc-definizione-di-input-e-modello" class="nav-link" data-scroll-target="#definizione-di-input-e-modello">2. Definizione di Input e Modello</a></li>
  <li><a href="#definizione-della-struttura-dei-dati-di-risposta" id="toc-definizione-della-struttura-dei-dati-di-risposta" class="nav-link" data-scroll-target="#definizione-della-struttura-dei-dati-di-risposta">3. Definizione della Struttura dei Dati di Risposta</a></li>
  <li><a href="#impostazione-del-client-openai" id="toc-impostazione-del-client-openai" class="nav-link" data-scroll-target="#impostazione-del-client-openai">4. Impostazione del Client OpenAI</a></li>
  <li><a href="#generazione-della-risposta" id="toc-generazione-della-risposta" class="nav-link" data-scroll-target="#generazione-della-risposta">5. Generazione della Risposta</a></li>
  <li><a href="#calcolo-della-distanza" id="toc-calcolo-della-distanza" class="nav-link" data-scroll-target="#calcolo-della-distanza">6. Calcolo della Distanza</a></li>
  <li><a href="#aggiunta-di-immagini" id="toc-aggiunta-di-immagini" class="nav-link" data-scroll-target="#aggiunta-di-immagini">Aggiunta di immagini</a></li>
  </ul></li>
  <li><a href="#slm-tecniche-di-ottimizzazione" id="toc-slm-tecniche-di-ottimizzazione" class="nav-link" data-scroll-target="#slm-tecniche-di-ottimizzazione">SLM: Tecniche di Ottimizzazione</a></li>
  <li><a href="#implementazione-del-rag" id="toc-implementazione-del-rag" class="nav-link" data-scroll-target="#implementazione-del-rag">Implementazione del RAG</a>
  <ul>
  <li><a href="#un-semplice-progetto-rag" id="toc-un-semplice-progetto-rag" class="nav-link" data-scroll-target="#un-semplice-progetto-rag">Un semplice progetto RAG</a></li>
  <li><a href="#andando-oltre" id="toc-andando-oltre" class="nav-link" data-scroll-target="#andando-oltre">Andando Oltre</a></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione">Conclusione</a></li>
  <li><a href="#risorse" id="toc-risorse" class="nav-link" data-scroll-target="#risorse">Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/labs/raspi/llm/llm.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/labs/raspi/llm/llm.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.it.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/llm/llm.it.html">Small Language Models (SLM)</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Small Language Models (SLM)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/jpeg/cover.jpg" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E prompt - Un‚Äôillustrazione in stile cartoon anni ‚Äô50 che mostra un Raspberry Pi che esegue un piccolo modello di linguaggio all‚Äôedge. Il Raspberry Pi √® stilizzato in modo retro-futuristico con bordi arrotondati e cromature, collegato a sensori e dispositivi da cartone animato giocosi. I fumetti fluttuano in giro, rappresentando l‚Äôelaborazione del linguaggio, e lo sfondo ha un paesaggio stravagante di dispositivi interconnessi con fili e piccoli gadget, tutti disegnati in uno stile cartoon vintage. La tavolozza dei colori utilizza tenui colori pastello e contorni audaci tipici dei cartoni animati degli anni ‚Äô50, conferendo un‚Äôatmosfera divertente e nostalgica alla scena.</em></figcaption>
</figure>
</div>
<section id="panoramica" class="level2">
<h2 class="anchored" data-anchor-id="panoramica">Panoramica</h2>
<p>Nell‚Äôarea in rapida crescita dell‚Äôintelligenza artificiale, l‚Äôedge computing offre l‚Äôopportunit√† di decentralizzare le capacit√† tradizionalmente riservate a server potenti e centralizzati. Questo laboratorio esplora l‚Äôintegrazione pratica di piccole versioni di modelli linguistici tradizionali di grandi dimensioni (LLM) in un Raspberry Pi 5, trasformando questo dispositivo edge in un hub di IA in grado di elaborare dati in tempo reale e in loco.</p>
<p>Man mano che i modelli linguistici di grandi dimensioni crescono in dimensioni e complessit√†, gli Small Language Model (SLM) offrono un‚Äôalternativa interessante per i dispositivi edge, raggiungendo un equilibrio tra prestazioni ed efficienza delle risorse. Eseguendo questi modelli direttamente su Raspberry Pi, possiamo creare applicazioni reattive e rispettose della privacy che funzionano anche in ambienti con connettivit√† Internet limitata o assente.</p>
<p>Questo laboratorio guider√† attraverso la configurazione, l‚Äôottimizzazione e lo sfruttamento degli SLM su Raspberry Pi. Esploreremo l‚Äôinstallazione e l‚Äôutilizzo di <a href="https://ollama.com/">Ollama</a>. Questo framework open source ci consente di eseguire LLM localmente sulle nostre macchine (i nostri desktop o dispositivi edge come Raspberry Pi o NVidia Jetson). Ollama √® progettato per essere efficiente, scalabile e facile da usare, il che lo rende una buona opzione per distribuire modelli di IA come Microsoft Phi, Google Gemma, Meta Llama e LLaVa (Multimodal). Integreremo alcuni di questi modelli in progetti utilizzando l‚Äôecosistema Python, esplorandone il potenziale in scenari del mondo reale (o almeno indicheremo questa direzione).</p>
<p><img src="images/jpeg/slm-example.jpg" class="img-fluid"></p>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>Nei laboratori precedenti avremmo potuto usare qualsiasi modello Raspi, ma qui la scelta deve ricadere sul Raspberry Pi 5 (Raspi-5). √à una piattaforma robusta che aggiorna sostanzialmente l‚Äôultima versione 4, dotata di Broadcom BCM2712, una CPU Arm Cortex-A76 quad-core a 64 bit da 2,4 GHz con Cryptographic Extension e capacit√† di caching migliorate. Vanta una GPU VideoCore VII, due uscite HDMI¬Æ 4Kp60 con HDR e un decoder HEVC 4Kp60. Le opzioni di memoria includono 4 GB e 8 GB di SDRAM LPDDR4X ad alta velocit√†, con 8 GB come nostra scelta per eseguire SLM. Presenta inoltre un‚Äôarchiviazione espandibile tramite uno slot per schede microSD e un‚Äôinterfaccia PCIe 2.0 per periferiche veloci come gli SSD M.2 (Solid State Drive).</p>
<blockquote class="blockquote">
<p>Per applicazioni SSL reali, gli SSD sono un‚Äôopzione migliore delle schede SD.</p>
</blockquote>
<p>A proposito, come ha spiegato <a href="https://www.hackster.io/aallan">Alasdair Allan</a>, l‚Äôinferenza diretta sulla CPU Raspberry Pi 5, senza accelerazione GPU, √® ora alla pari con le prestazioni del Coral TPU.</p>
<p><img src="images/png/bench.png" class="img-fluid"></p>
<p>Per maggiori informazioni, consultare l‚Äôarticolo completo: <a href="https://www.hackster.io/news/benchmarking-tensorflow-and-tensorflow-lite-on-raspberry-pi-5-b9156d58a6a2?mc_cid=0cab3d08f4&amp;mc_eid=e96256ccba">Benchmarking TensorFlow e TensorFlow Lite su Raspberry Pi 5</a>.</p>
<section id="raspberry-pi-active-cooler" class="level3">
<h3 class="anchored" data-anchor-id="raspberry-pi-active-cooler">Raspberry Pi Active Cooler</h3>
<p>Suggeriamo di installare un Active Cooler, una soluzione di raffreddamento clip-on dedicata per Raspberry Pi 5 (Raspi-5), per questo laboratorio. Combina un dissipatore di calore in alluminio con una ventola di raffreddamento a temperatura controllata per mantenere il Raspi-5 in funzione comodamente sotto carichi pesanti, come l‚Äôesecuzione di SLM.</p>
<p><img src="images/png/cooler.png" class="img-fluid"></p>
<p>L‚ÄôActive Cooler ha dei pad termici pre-applicati per il trasferimento del calore ed √® montato direttamente sulla scheda Raspberry Pi 5 tramite perni a molla. Il firmware Raspberry Pi lo gestisce attivamente: a 60 ¬∞C, la ventola del soffiatore viene accesa; a 67,5 ¬∞C, la velocit√† della ventola verr√† aumentata; e infine, a 75 ¬∞C, la ventola aumenter√† fino alla massima velocit√†. La ventola del soffiatore rallenter√† automaticamente quando la temperatura scender√† al di sotto di questi limiti.</p>
<p><img src="images/png/temp_comp.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Per evitare il surriscaldamento, tutte le schede Raspberry Pi iniziano a limitare il processore quando la temperatura raggiunge gli 80 ¬∞C e a limitarlo ulteriormente quando raggiunge la temperatura massima di 85 ¬∞C (maggiori dettagli <a href="https://www.raspberrypi.com/news/heating-and-cooling-raspberry-pi-5/">qui</a>).</p>
</blockquote>
</section>
</section>
<section id="generative-ai-genai" class="level2">
<h2 class="anchored" data-anchor-id="generative-ai-genai">Generative AI (GenAI)</h2>
<p>Generative AI √® un sistema di IA in grado di creare nuovi contenuti originali su vari supporti come <strong>testo, immagini, audio e video</strong>. Questi sistemi apprendono modelli da dati esistenti e utilizzano tale conoscenza per generare nuovi output che in precedenza non esistevano. <strong>Large Language Models (LLM)</strong>, <strong>Small Language Models (SLM)</strong> e <strong>modelli multimodali</strong> possono essere tutti considerati tipi di GenAI quando utilizzati per attivit√† generative.</p>
<p>GenAI fornisce il framework concettuale per la creazione di contenuti basati sull‚Äôintelligenza artificiale, con gli LLM che fungono da potenti generatori di testo per uso generale. Gli SLM adattano questa tecnologia all‚Äôedge computing, mentre i modelli multimodali estendono le capacit√† di GenAI a diversi tipi di dati. Insieme, rappresentano uno spettro di tecnologie di intelligenza artificiale generativa, ciascuna con i suoi punti di forza e applicazioni, che guidano collettivamente la creazione e la comprensione di contenuti basati sull‚ÄôIA.</p>
<section id="large-language-models-llm" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models-llm">Large Language Models (LLM)</h3>
<p>I ‚ÄúLarge Language Model (LLM)‚Äù sono sistemi avanzati di intelligenza artificiale che comprendono, elaborano e generano testi simili a quelli umani. Questi modelli sono caratterizzati dalla loro enorme scala in termini di quantit√† di dati su cui vengono addestrati e numero di parametri che contengono. Gli aspetti critici degli LLM includono:</p>
<ol type="1">
<li><p><strong>Dimensioni</strong>: Gli LLM contengono in genere miliardi di parametri. Ad esempio, GPT-3 ha 175 miliardi di parametri, mentre alcuni modelli pi√π recenti superano un trilione di parametri.</p></li>
<li><p><strong>Dati di Addestramento</strong>: Vengono addestrati su grandi quantit√† di dati di testo, spesso inclusi libri, siti Web e altre fonti diverse, che ammontano a centinaia di gigabyte o persino terabyte di testo.</p></li>
<li><p><strong>Architettura</strong>: La maggior parte degli LLM utilizza <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">architetture basate su trasformatori</a>, che consentono loro di elaborare e generare testo prestando attenzione a diverse parti dell‚Äôinput contemporaneamente.</p></li>
<li><p><strong>Capacit√†</strong>: Gli LLM possono eseguire un‚Äôampia gamma di attivit√† linguistiche senza una messa a punto specifica, tra cui:</p>
<ul>
<li>Generazione di testo</li>
<li>Traduzione</li>
<li>Riepilogo</li>
<li>Risposte a domande</li>
<li>Generazione di codice</li>
<li>Ragionamento logico</li>
</ul></li>
<li><p><strong>Apprendimento a Intervalli</strong>: Spesso possono comprendere ed eseguire nuove attivit√† con esempi o istruzioni minimi.</p></li>
<li><p><strong>Richiede molte Risorse</strong>: A causa delle loro dimensioni, gli LLM richiedono in genere risorse di elaborazione significative per funzionare, spesso necessitando di potenti GPU o TPU.</p></li>
<li><p><strong>Sviluppo Continuo</strong>: Il campo degli LLM √® in rapida evoluzione, con nuovi modelli e tecniche che emergono costantemente.</p></li>
<li><p><strong>Considerazioni Etiche</strong>: L‚Äôuso degli LLM solleva importanti questioni su pregiudizi, disinformazione e impatto ambientale della formazione di modelli cos√¨ grandi.</p></li>
<li><p><strong>Applicazioni</strong>: Gli LLM sono utilizzati in vari campi, tra cui la creazione di contenuti, il servizio clienti, l‚Äôassistenza alla ricerca e lo sviluppo di software.</p></li>
<li><p><strong>Limitazioni</strong>: Nonostante la loro potenza, gli LLM possono produrre informazioni errate o distorte e non hanno capacit√† di vera comprensione o ragionamento.</p></li>
</ol>
<p>Dobbiamo notare che utilizziamo modelli di grandi dimensioni oltre al testo, chiamandoli <em>modelli multimodali</em>. Questi modelli integrano ed elaborano informazioni da pi√π tipi di input contemporaneamente. Sono progettati per comprendere e generare contenuti in varie forme di dati, come testo, immagini, audio e video.</p>
<p>Certamente. Definiamo modelli ‚Äúopen‚Äù [aperti] e ‚Äúclosed‚Äù [chiusi] nel contesto di modelli di linguaggio e IA:</p>
</section>
<section id="modelli-chiusi-e-aperti" class="level3">
<h3 class="anchored" data-anchor-id="modelli-chiusi-e-aperti">Modelli Chiusi e Aperti:</h3>
<p><strong>Modelli Chiusi</strong>, detti anche modelli proprietari, sono modelli di IA il cui funzionamento interno, codice e dati di addestramento non sono divulgati pubblicamente. Esempi: GPT-4 (di OpenAI), Claude (di Anthropic), Gemini (di Google).</p>
<p><strong>Modelli Aperti</strong>, noti anche come modelli open source, sono modelli di intelligenza artificiale il cui codice sottostante, architettura e spesso dati di addestramento sono disponibili e accessibili al pubblico. Esempi: Gemma (di Google), LLaMA (di Meta) e Phi (di Microsoft).</p>
<p>I modelli aperti sono particolarmente rilevanti per l‚Äôesecuzione di modelli su dispositivi edge come Raspberry Pi in quanto possono essere pi√π facilmente adattati, ottimizzati e distribuiti in ambienti con risorse limitate. Tuttavia, √® fondamentale verificare le loro Licenze. I modelli aperti sono dotati di varie licenze open source che possono influenzare il loro utilizzo in applicazioni commerciali, mentre i modelli chiusi hanno termini di servizio chiari, seppur restrittivi.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/llms-slm.png" class="img-fluid figure-img"></p>
<figcaption>Adattato da https://arxiv.org/pdf/2304.13712</figcaption>
</figure>
</div>
</section>
<section id="small-language-models-slm" class="level3">
<h3 class="anchored" data-anchor-id="small-language-models-slm">Small Language Models (SLM)</h3>
<p>Nel contesto dell‚Äôedge computing su dispositivi come Raspberry Pi, gli LLM su larga scala sono in genere troppo grandi e dispendiosi in termini di risorse per essere eseguiti direttamente. Questa limitazione ha spinto lo sviluppo di modelli pi√π piccoli ed efficienti, come gli Small Language Models (SLM).</p>
<p>Gli SLM sono versioni compatte degli LLM progettate per essere eseguite in modo efficiente su dispositivi con risorse limitate come smartphone, dispositivi IoT e computer a scheda singola come Raspberry Pi. Questi modelli sono significativamente pi√π piccoli in termini di dimensioni e requisiti di calcolo rispetto alle loro controparti pi√π grandi, pur mantenendo impressionanti capacit√† di comprensione e generazione del linguaggio.</p>
<p>Le caratteristiche principali degli SLM includono:</p>
<ol type="1">
<li><p><strong>Conteggio ridotto dei parametri</strong>: In genere vanno da poche centinaia di milioni a qualche miliardo di parametri, rispetto ai miliardi a due cifre nei modelli pi√π grandi.</p></li>
<li><p><strong>Meno ingombro di memoria</strong>: Richiedono, al massimo, pochi gigabyte di memoria anzich√© decine o centinaia di gigabyte.</p></li>
<li><p><strong>Tempo di inferenza pi√π rapido</strong>: Possono generare risposte in millisecondi o secondi su dispositivi edge.</p></li>
<li><p><strong>Efficienza energetica</strong>: Consumano meno energia, rendendoli adatti per dispositivi alimentati a batteria.</p></li>
<li><p><strong>Tutela della privacy</strong>: Abilitano l‚Äôelaborazione sul dispositivo senza inviare dati ai server cloud.</p></li>
<li><p><strong>Funzionalit√† offline</strong>: Funzionano senza una connessione Internet.</p></li>
</ol>
<p>Gli SLM raggiungono le loro dimensioni compatte attraverso varie tecniche come la distillazione della conoscenza, la potatura del modello e la quantizzazione. Sebbene non possano eguagliare le ampie capacit√† dei modelli pi√π grandi, gli SLM eccellono in compiti e domini specifici, il che li rende ideali per applicazioni mirate su dispositivi edge.</p>
<blockquote class="blockquote">
<p>In genere prenderemo in considerazione gli SLM, modelli linguistici con meno di 5 miliardi di parametri quantizzati a 4 bit.</p>
</blockquote>
<p>Esempi di SLM includono versioni compresse di modelli come Meta Llama, Microsoft PHI e Google Gemma. Questi modelli consentono un‚Äôampia gamma di attivit√† di elaborazione del linguaggio naturale direttamente sui dispositivi edge, dalla classificazione del testo e analisi del sentiment alle risposte alle domande e alla generazione di testo limitato.</p>
<p>Per maggiori informazioni sugli SLM, il documento, <a href="https://arxiv.org/pdf/2408.11796">LLM Pruning and Distillation in Practice: The Minitron Approach</a>, fornisce un approccio che applica il pruning e la distillazione per ottenere SLM da LLM. E, <a href="https://arxiv.org/pdf/2409.15790">SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS</a>, presenta un‚Äôindagine e un‚Äôanalisi complete di Small Language Models (SLM), che sono modelli linguistici con da 100 milioni a 5 miliardi di parametri progettati per dispositivi con risorse limitate.</p>
</section>
</section>
<section id="ollama" class="level2">
<h2 class="anchored" data-anchor-id="ollama">Ollama</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://ollama.com/public/ollama.png" class="img-fluid figure-img"></p>
<figcaption>logo di ollama</figcaption>
</figure>
</div>
<p><a href="https://ollama.com/">Ollama</a> √® un framework open source che ci consente di eseguire modelli linguistici (LM), grandi o piccoli, localmente sulle nostre macchine. Ecco alcuni punti critici su Ollama:</p>
<ol type="1">
<li><p><strong>Esecuzione del Modello Locale</strong>: Ollama consente di eseguire LM su personal computer o dispositivi edge come Raspi-5, eliminando la necessit√† di chiamate API basate su cloud.</p></li>
<li><p><strong>Facilit√† d‚ÄôUso</strong>: Fornisce una semplice interfaccia a riga di comando per scaricare, eseguire e gestire diversi modelli linguistici.</p></li>
<li><p><strong>Variet√† di Modelli</strong>: Ollama supporta vari LLM, tra cui Phi, Gemma, Llama, Mistral e altri modelli open source.</p></li>
<li><p><strong>Personalizzazione</strong>: Gli utenti possono creare e condividere modelli personalizzati su misura per esigenze o domini specifici.</p></li>
<li><p><strong>Leggero</strong>: Progettato per essere efficiente e funzionare su hardware di livello consumer.</p></li>
<li><p><strong>Integrazione API</strong>: Offre un‚ÄôAPI che consente l‚Äôintegrazione con altre applicazioni e servizi.</p></li>
<li><p><strong>Incentrato sulla Privacy</strong>: Eseguendo i modelli localmente, affronta i problemi di privacy associati all‚Äôinvio di dati a server esterni.</p></li>
<li><p><strong>Multipiattaforma</strong>: Disponibile per sistemi macOS, Windows e Linux (il nostro caso, qui).</p></li>
<li><p><strong>Sviluppo Attivo</strong>: Aggiornato regolarmente con nuove funzionalit√† e supporto per i modelli.</p></li>
<li><p><strong>Guidato dalla Community</strong>: Trae vantaggio dai contributi della community e dalla condivisione dei modelli.</p></li>
</ol>
<p>Per saperne di pi√π su cosa sia Ollama e come funziona sotto internamente, si pu√≤ guardare questo breve video di <a href="https://www.youtube.com/@technovangelist">Matt Williams</a>, uno dei fondatori di Ollama:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/90ozfdsQOKo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<blockquote class="blockquote">
<p>Matt ha un corso completamente gratuito su Ollama che consigliamo: </p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/9KEUFe4KQAI?si=D_-q3CMbHiT-twuy" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</blockquote>
<section id="installazione-di-ollama" class="level3">
<h3 class="anchored" data-anchor-id="installazione-di-ollama">Installazione di Ollama</h3>
<p>Impostiamo e attiviamo un ambiente virtuale per lavorare con Ollama:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv ~/ollama</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/ollama/bin/activate</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ed eseguiamo il comando per installare Ollama:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://ollama.com/install.sh <span class="kw">|</span> <span class="fu">sh</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Di conseguenza, un‚ÄôAPI verr√† eseguita in background su <code>127.0.0.1:11434</code>. D‚Äôora in poi, possiamo eseguire Ollama tramite il terminale. Per iniziare, verifichiamo la versione di Ollama, che ci dir√† anche se √® installata correttamente:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> <span class="at">-v</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/install-ollama-rpi5.png" class="img-fluid"></p>
<p>Nella <a href="https://ollama.com/library">pagina della Libreria Ollama</a>, possiamo trovare i modelli supportati da Ollama. Ad esempio, filtrando per <code>Most popular</code>, possiamo vedere Meta Llama, Google Gemma, Microsoft Phi, LLaVa, ecc.</p>
</section>
<section id="meta-llama-3.2-1b3b" class="level3">
<h3 class="anchored" data-anchor-id="meta-llama-3.2-1b3b">Meta Llama 3.2 1B/3B</h3>
<p><img src="images/png/small_and_multimodal.png" class="img-fluid"></p>
<p>Installiamo ed eseguiamo il nostro primo piccolo modello linguistico, <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama 3.2</a> 1B (e 3B). La serie Meta Llama 3.2 comprende un set di modelli linguistici generativi multilingue disponibili in dimensioni di parametri pari a 1 miliardo e 3 miliardi. Questi modelli sono progettati per elaborare input di testo e generare output di testo. Le varianti sintonizzate sulle istruzioni all‚Äôinterno di questa raccolta sono specificamente ottimizzate per applicazioni conversazionali multilingue, tra cui attivit√† che comportano il recupero e la sintesi delle informazioni con un approccio agentico. Rispetto a molti modelli di chat open source e proprietari esistenti, i modelli sintonizzati sulle istruzioni Llama 3.2 dimostrano prestazioni superiori su benchmark di settore ampiamente utilizzati.</p>
<p>I modelli 1B e 3B sono stati potati da Llama 8B, quindi i logit dai modelli 8B e 70B sono stati utilizzati come target a livello di token (distillazione a livello di token). La distillazione della conoscenza √® stata utilizzata per recuperare le prestazioni (sono stati addestrati con 9 trilioni di token). Il modello 1B ha 1,24B, quantizzati a intero (Q8_0), e i parametri 3B, 3,12B, con una quantizzazione Q4_0, che termina con una dimensione di 1,3 GB e 2 GB, rispettivamente. La sua finestra di contesto √® di 131.072 token.</p>
<p><img src="images/jpeg/llama3_2.jpg" class="img-fluid"></p>
<p><strong>Installare ed eseguire il</strong> <strong>Model</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llama3.2:1b</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Eseguendo il modello col comando precedente, dovremmo avere il prompt di Ollama disponibile per inserire una domanda e iniziare a chattare con il modello LLM; ad esempio,</p>
<p><code>&gt;&gt;&gt; What is the capital of France?</code></p>
<p>Quasi immediatamente, otteniamo la risposta corretta:</p>
<p><code>The capital of France is Paris.</code></p>
<p>Utilizzando l‚Äôopzione <code>--verbose</code> quando si richiama il modello, verranno generate diverse statistiche sulle sue prestazioni (Il modello eseguir√† il polling solo la prima volta che eseguiamo il comando).</p>
<p><img src="images/png/llama3_2_1b_performance.png" class="img-fluid"></p>
<p>Ogni metrica fornisce informazioni su come il modello elabora gli input e genera gli output. Ecco una ripartizione del significato di ogni metrica:</p>
<ul>
<li><strong>Total Duration (2.620170326s)</strong>: Questo √® il tempo completo impiegato dall‚Äôinizio del comando al completamento della risposta. Comprende il caricamento del modello, l‚Äôelaborazione del prompt di input e la generazione della risposta.</li>
<li><strong>Load Duration (39.947908ms)</strong>: Questa durata indica il tempo necessario per caricare il modello o i componenti necessari nella memoria. Se questo valore √® minimo, pu√≤ suggerire che il modello √® stato precaricato o che √® stata richiesta solo una configurazione minima.</li>
<li><strong>Prompt Eval Count (32 tokens)</strong>: Il numero di token nel prompt di input. In NLP, i token sono in genere parole o sotto-parole, quindi questo conteggio include tutti i token che il modello ha valutato per comprendere e rispondere alla query.</li>
<li><strong>Prompt Eval Duration (1.644773s)</strong>: Misura il tempo impiegato dal modello per valutare o elaborare il prompt di input. Rappresenta la maggior parte della durata totale, il che implica che comprendere la query e preparare una risposta √® la parte pi√π dispendiosa in termini di tempo del processo.</li>
<li><strong>Prompt Eval Rate (19.46 tokens/s)</strong>: Questa frequenza indica la rapidit√† con cui il modello elabora i token dal prompt di input. Riflette la velocit√† del modello in termini di comprensione del linguaggio naturale.</li>
<li><strong>Eval Count (8 token(s))</strong>: Questo √® il numero di token nella risposta del modello, che in questo caso era ‚ÄúThe capital of France is Paris.‚Äù</li>
<li><strong>Eval Duration (889.941ms)</strong>: Questo √® il tempo impiegato per generare l‚Äôoutput in base all‚Äôinput valutato. √à molto pi√π breve della valutazione del prompt, il che suggerisce che generare la risposta √® meno complesso o computazionalmente intensivo rispetto alla comprensione del prompt.</li>
<li><strong>Eval Rate (8.99 tokens/s)</strong>: Simile alla frequenza di valutazione del prompt, indica la velocit√† con cui il modello genera token di output. √à una metrica fondamentale per comprendere l‚Äôefficienza del modello nella generazione di output.</li>
</ul>
<p>Questa ripartizione dettagliata pu√≤ aiutare a comprendere le richieste computazionali e le caratteristiche delle prestazioni dell‚Äôesecuzione di SLM come Llama su dispositivi edge come Raspberry Pi 5. Mostra che mentre la valutazione del prompt richiede pi√π tempo, la generazione effettiva delle risposte √® relativamente pi√π rapida. Questa analisi √® fondamentale per ottimizzare le prestazioni e diagnosticare potenziali colli di bottiglia nelle applicazioni in tempo reale.</p>
<p>Caricando ed eseguendo il modello 3B, possiamo vedere la differenza nelle prestazioni per lo stesso prompt;</p>
<p><img src="images/png/llama3_2_3b_performance.png" class="img-fluid"></p>
<p>Il tasso di valutazione √® inferiore, 5,3 token/s rispetto ai 9 token/s del modello pi√π piccolo.</p>
<p>Quando si chiede</p>
<p><code>&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?</code></p>
<p>Il modello 1B ha risposto <code>9,841 kilometers (6,093 miles)</code>, il che √® impreciso, e il modello 3B ha risposto <code>7,300 miles (11,700 km)</code>, il che √® vicino alla distanza corretta (11,642 km).</p>
<p>Chiediamo le coordinate di Parigi:</p>
<p><code>&gt;&gt;&gt; what is the latitude and longitude of Paris?</code></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> latitude and longitude of Paris are 48.8567¬∞ N <span class="er">(</span><span class="ex">48¬∞55</span><span class="st">' </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="st">42" N) and 2.3510¬∞ E (2¬∞22'</span> 8<span class="st">" E), respectively.</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/paris-lat-lon.png" class="img-fluid"></p>
<p>Sia i modelli 1B che 3B hanno dato risposte corrette.</p>
</section>
<section id="google-gemma-2-2b" class="level3">
<h3 class="anchored" data-anchor-id="google-gemma-2-2b">Google Gemma 2 2B</h3>
<p>Installiamo <a href="https://ollama.com/library/gemma2:2b">Gemma 2</a>, un modello efficiente e ad alte prestazioni disponibile in tre dimensioni: 2B, 9B e 27B. Installeremo <a href="https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f"><strong>Gemma 2 2B</strong></a>, un modello leggero addestrato con 2 trilioni di token che produce risultati sproporzionati imparando da modelli pi√π grandi tramite distillazione. Il modello ha 2,6 miliardi di parametri e una quantizzazione Q4_0, che termina con una dimensione di 1,6 GB. La sua finestra di contesto √® di 8.192 token.</p>
<p><img src="images/png/gemma_2.png" class="img-fluid"></p>
<p><strong>Installare ed eseguire il</strong> <strong>Model</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run gemma2:2b <span class="at">--verbose</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Eseguendo il modello col comando precedente, dovremmo avere il prompt di Ollama disponibile per inserire una domanda e iniziare a chattare con il modello LLM; ad esempio,</p>
<p><code>&gt;&gt;&gt; What is the capital of France?</code></p>
<p>Quasi immediatamente, otteniamo la risposta corretta:</p>
<p><code>The capital of France is **Paris**. üóº</code></p>
<p>E le statistiche.</p>
<p><img src="images/png/gemma.png" class="img-fluid"></p>
<p>Possiamo vedere che Gemma 2:2B ha pi√π o meno le stesse prestazioni di Lama 3.2:3B, ma con meno parametri.</p>
<p><strong>Altri esempi:</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> What <span class="ex">is</span> the distance between Paris and Santiago, Chile<span class="pp">?</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> distance between Paris, France and Santiago, Chile is </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="ex">approximately</span> <span class="pp">**</span>7,000 miles <span class="er">(</span><span class="ex">11,267</span> kilometers<span class="kw">)</span><span class="ex">**.</span> </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Keep</span> in mind that this is a straight-line distance, and actual </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="ex">travel</span> distance can vary depending on the chosen routes and any </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="ex">stops</span> along the way. ‚úàÔ∏è<span class="kw">`</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Inoltre, una buona risposta ma meno precisa di Llama3.2:3B.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> what <span class="ex">is</span> the latitude and longitude of Paris<span class="pp">?</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="ex">You</span> got it! Here are the latitudes and longitudes of Paris, </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="ex">France:</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="ex">*</span> <span class="pp">**</span>Latitude:<span class="pp">**</span> 48.8566¬∞ N <span class="er">(</span><span class="ex">north</span><span class="kw">)</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="ex">*</span> <span class="pp">**</span>Longitude:<span class="pp">**</span> 2.3522¬∞ E <span class="er">(</span><span class="ex">east</span><span class="kw">)</span> </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Let</span> me know if you<span class="st">'d like to explore more about Paris or its </span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="st">location! üóºüá´üá∑</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Una risposta buona e precisa (un po‚Äô pi√π prolissa delle risposte del lama).</p>
</section>
<section id="microsoft-phi3.5-3.8b" class="level3">
<h3 class="anchored" data-anchor-id="microsoft-phi3.5-3.8b">Microsoft Phi3.5 3.8B</h3>
<p>Prendiamo un modello pi√π grande (ma comunque ‚Äútiny‚Äù), il <a href="https://ollama.com/library/phi3.5">PHI3.5</a>, un modello aperto all‚Äôavanguardia leggero da 3,8B di Microsoft. Il modello appartiene alla famiglia di modelli Phi-3 e supporta la lunghezza del contesto <code>token 128K</code> e le lingue: arabo, cinese, ceco, danese, olandese, inglese, finlandese, francese, tedesco, ebraico, ungherese, italiano, giapponese, coreano, norvegese, polacco, portoghese, russo, spagnolo, svedese, tailandese, turco e ucraino.</p>
<p>La dimensione del modello, in termini di byte, dipender√† dal formato di quantizzazione specifico utilizzato. La dimensione pu√≤ variare dalla quantizzazione a 2 bit (<code>q2_k</code>) di 1,4 GB (prestazioni pi√π elevate/qualit√† inferiore) alla quantizzazione a 16 bit (fp-16) di 7,6 GB (prestazioni pi√π basse/qualit√† pi√π elevata).</p>
<p>Eseguiamo la quantizzazione a 4 bit (<code>Q4_0</code>), che richieder√† 2,2 GB di RAM, con un compromesso intermedio per quanto riguarda la qualit√† dell‚Äôoutput e le prestazioni.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run phi3.5:3.8b <span class="at">--verbose</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Si pu√≤ usare <code>run</code> o <code>pull</code> per scaricare il modello. Ci√≤ che accade √® che Ollama tiene nota dei modelli estratti e, una volta che PHI3 non esiste, prima di eseguirlo, Ollama lo estrae.</p>
</blockquote>
<p>Immettiamo lo stesso prompt usato prima:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> What <span class="ex">is</span> the capital of France<span class="pp">?</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of France is Paris. It<span class="st">' extradites significant </span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="st">historical, cultural, and political importance to the country as </span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="st">well as being a major European city known for its art, fashion, </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="st">gastronomy, and culture. Its influence extends beyond national </span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="st">borders, with millions of tourists visiting each year from around </span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="st">the globe. The Seine River flows through Paris before it reaches </span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="st">the broader English Channel at Le Havre. Moreover, France is one </span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="st">of Europe'</span>s leading economies with its capital playing a key role </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>La risposta √® stata molto ‚Äúverbosa‚Äù, specifichiamo un prompt migliore:</p>
<p><img src="images/png/paris-2.png" class="img-fluid"></p>
<p>In questo caso, la risposta √® stata comunque pi√π lunga di quanto ci aspettassimo, con una velocit√† di valutazione di 2,25 token/s, pi√π del doppio di quella di Gemma e Llama.</p>
<blockquote class="blockquote">
<p>Scegliere il prompt pi√π appropriato √® una delle competenze pi√π importanti da utilizzare con gli LLM, indipendentemente dalle dimensioni.</p>
</blockquote>
<p>Quando abbiamo posto le stesse domande su distanza e latitudine/longitudine, non abbiamo ottenuto una buona risposta per una distanza di <code>13,507 kilometers (8,429 miles)</code>, ma andava bene per le coordinate. Di nuovo, avrebbe potuto essere meno verbosa (pi√π di 200 token per ogni risposta).</p>
<p>Possiamo usare qualsiasi modello come assistente poich√© la loro velocit√† √® relativamente decente, ma al 24 settembre (2023), Llama2:3B √® una scelta migliore. Si dovrebbero provare altri modelli, a seconda delle esigenze. <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard">ü§ó Open LLM Leaderboard</a> pu√≤ dare un‚Äôidea sui migliori modelli in termini di dimensioni, benchmark, licenza, ecc.</p>
<blockquote class="blockquote">
<p>Il miglior modello da usare √® quello adatto alle specifiche necessit√†. Inoltre, si tenga presente che questo campo si evolve con nuovi modelli ogni giorno,</p>
</blockquote>
</section>
<section id="modelli-multimodali" class="level3">
<h3 class="anchored" data-anchor-id="modelli-multimodali">Modelli Multimodali</h3>
<p>I modelli multimodali sono sistemi di intelligenza artificiale (IA) in grado di elaborare e comprendere informazioni provenienti da pi√π fonti, come immagini, testo, audio e video. Nel nostro contesto, gli LLM multimodali possono elaborare vari input, tra cui testo, immagini e audio, come prompt e convertire tali prompt in vari output, non solo il tipo di sorgente.</p>
<p>Qui lavoreremo con <a href="https://ollama.com/library/llava-phi3:3.8b">LLaVA-Phi-3</a>, un modello LLaVA ottimizzato da Phi 3 Mini 4k. Ha solidi benchmark di prestazioni che sono alla pari con il modello originale <a href="https://llava-vl.github.io/">LLaVA</a> (Large Language and Vision Assistant).</p>
<p>LLaVA-Phi-3 √® un modello multimodale di grandi dimensioni addestrato end-to-end progettato per comprendere e generare contenuti in base a input visivi (immagini) e istruzioni testuali. Combina le capacit√† di un codificatore visivo e di un modello linguistico per elaborare e rispondere a input multimodali.</p>
<p>Installiamo il modello:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llava-phi3:3.8b <span class="at">--verbose</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Cominciamo con un input di testo:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> You <span class="ex">are</span> a helpful AI assistant. What is the capital of France<span class="pp">?</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="ex">As</span> an AI language model, I can tell you that the capital of France </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="ex">is</span> Paris. It<span class="st">'s not only the largest city in the country but also </span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="st">serves as its political and administrative center. Paris is known </span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="st">for its iconic landmarks such as the Eiffel Tower, Notre-Dame </span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="st">Cathedral, and the Louvre Museum. The city has a rich history, </span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="st">beautiful architecture, and is widely considered to be one of the </span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="st">most romantic cities in the world.</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>La risposta ha richiesto circa 30 secondi, con una velocit√† di valutazione di 3,93 token/s! Niente male!</p>
<p>Ma vediamo di immettere un‚Äôimmagine come input. Per questo, creiamo una directory per lavorare:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> OLLAMA</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> OLLAMA</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Scarichiamo un‚Äôimmagine 640x320 da Internet, per esempio (Wikipedia: <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg/640px-La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg">Paris, France)</a>:</p>
<p><img src="images/jpeg/paris.jpg" class="img-fluid"></p>
<p>Utilizzando FileZilla, ad esempio, carichiamo l‚Äôimmagine nella cartella OLLAMA sul Raspi-5 e chiamiamola <code>image_test_1.jpg</code>. Dovremmo avere l‚Äôintero path dell‚Äôimmagine (possiamo usare <code>pwd</code> per ottenerlo).</p>
<p><code>/home/mjrovai/Documents/OLLAMA/image_test_1.jpg</code></p>
<p>Se si usa un desktop, si pu√≤ copiare il path dell‚Äôimmagine cliccando sull‚Äôimmagine con il tasto destro del mouse.</p>
<p><img src="images/png/image_test-path.png" class="img-fluid"></p>
<p>Immettiamo questo prompt:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> Describe <span class="ex">the</span> image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Il risultato √® stato ottimo, ma la latenza complessiva √® stata significativa: quasi 4 minuti per eseguire l‚Äôinferenza.</p>
<p><img src="images/png/paris-infer-1.png" class="img-fluid"></p>
</section>
<section id="ispezione-delle-risorse-locali" class="level3">
<h3 class="anchored" data-anchor-id="ispezione-delle-risorse-locali">Ispezione delle risorse locali</h3>
<p>Utilizzando htop, possiamo monitorare le risorse in esecuzione sul nostro dispositivo.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">htop</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Durante il periodo in cui il modello √® in esecuzione, possiamo ispezionare le risorse:</p>
<p><img src="images/png/htop.png" class="img-fluid"></p>
<p>Tutte e quattro le CPU funzionano a quasi il 100% della loro capacit√† e la memoria utilizzata con il modello caricato √® di <code>3.24GB</code>. Uscendo da Ollama, la memoria scende a circa <code>377 MB</code> (senza desktop).</p>
<p>√à anche essenziale monitorare la temperatura. Quando si esegue Raspberry con un desktop, √® possibile visualizzare la temperatura sulla barra delle applicazioni:</p>
<p><img src="images/png/resourses-temp.png" class="img-fluid"></p>
<p>Se si √® ‚Äúheadless‚Äù, la temperatura pu√≤ essere monitorata col comando:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">vcgencmd</span> measure_temp</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Se non si fa nulla, la temperatura √® di circa <code>50¬∞C</code> per le CPU in esecuzione all‚Äô1%. Durante l‚Äôinferenza, con le CPU al 100%, la temperatura pu√≤ salire fino a quasi <code>70¬∞C</code>. Questo √® OK e significa che il dissipatore attivo funziona, mantenendo la temperatura al di sotto di 80¬∞C / 85¬∞C (il suo limite).</p>
</section>
</section>
<section id="libreria-python-ollama" class="level2">
<h2 class="anchored" data-anchor-id="libreria-python-ollama">Libreria Python Ollama</h2>
<p>Finora, abbiamo esplorato la capacit√† di chat degli SLM utilizzando la riga di comando su un terminale. Tuttavia, vogliamo integrare quei modelli nei nostri progetti, quindi Python sembra essere la strada giusta. La buona notizia √® che Ollama ha una libreria del genere.</p>
<p>La <a href="https://github.com/ollama/ollama-python">libreria Python Ollama</a> semplifica l‚Äôinterazione con modelli LLM avanzati, consentendo risposte e capacit√† pi√π sofisticate, oltre a fornire il modo pi√π semplice per integrare progetti Python 3.8+ con <a href="https://github.com/ollama/ollama">Ollama.</a></p>
<p>Per una migliore comprensione di come creare app usando Ollama con Python, possiamo seguire i <a href="https://www.youtube.com/@technovangelist">video di Matt Williams</a>, come quello qui sotto:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/_4K20tOsXK8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><strong>Installazione:</strong></p>
<p>Nel terminale, eseguire il comando:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ollama</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Avremo bisogno di un editor di testo o di un IDE per creare uno script Python. Se si esegue Raspberry OS su un desktop, diverse opzioni, come Thonny e Geany, sono gi√† state installate di default (accessibili tramite <code>[Menu][Programming]</code>). Si possono scaricare altri IDE, come Visual Studio Code, da <code>[Menu][Recommended Software]</code>. Quando si apre la finestra, si va su <code>[Programming]</code>, e si seleziona l‚Äôopzione che si preferisce e si preme <code>[Apply]</code>.</p>
<p><img src="images/png/menu.png" class="img-fluid"></p>
<p>Se si preferisce usare Jupyter Notebook per lo sviluppo:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install jupyter</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--generate-config</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Per eseguire Jupyter Notebook, si lancia il comando (cambiare l‚Äôindirizzo IP per il proprio):</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--ip</span><span class="op">=</span>192.168.4.209 <span class="at">--no-browser</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Sul terminale, si pu√≤ vedere l‚Äôindirizzo URL locale per aprire il notebook:</p>
<p><img src="images/png/jupyter.png" class="img-fluid"></p>
<p>Possiamo accedervi da un altro computer inserendo l‚Äôindirizzo IP del Raspberry Pi e il token fornito in un browser web (dovremmo copiarlo dal terminale).</p>
<p>Nella nostra directory di lavoro nel Raspi, creeremo un nuovo notebook Python 3.</p>
<p>Entriamo con uno script molto semplice per verificare i modelli installati:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>ollama.<span class="bu">list</span>()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Tutti i modelli verranno stampati come un dizionario, ad esempio:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>{<span class="st">'name'</span>: <span class="st">'gemma2:2b'</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>   <span class="st">'model'</span>: <span class="st">'gemma2:2b'</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>   <span class="st">'modified_at'</span>: <span class="st">'2024-09-24T19:30:40.053898094+01:00'</span>,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>   <span class="st">'size'</span>: <span class="dv">1629518495</span>,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>   <span class="st">'digest'</span>: <span class="st">'8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7'</span>,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>   <span class="st">'details'</span>: {<span class="st">'parent_model'</span>: <span class="st">''</span>,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'format'</span>: <span class="st">'gguf'</span>,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'family'</span>: <span class="st">'gemma2'</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'families'</span>: [<span class="st">'gemma2'</span>],</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'parameter_size'</span>: <span class="st">'2.6B'</span>,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quantization_level'</span>: <span class="st">'Q4_0'</span>}}]}</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ripetiamo una delle domande che abbiamo fatto prima, ma ora usando <code>ollama.generate()</code> dalla libreria Python Ollama. Questa API generer√† una risposta per il prompt specificato con il modello fornito. Questo √® un endpoint di streaming, quindi ci saranno una serie di risposte. L‚Äôoggetto di risposta finale includer√† statistiche e dati aggiuntivi dalla richiesta.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'gemma2:2b'</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>PROMPT <span class="op">=</span> <span class="st">'What is the capital of France?'</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> ollama.generate(model<span class="op">=</span>MODEL, prompt<span class="op">=</span>PROMPT)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (res)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Nel caso in cui si stia eseguendo il codice come script Python, lo si deve salvare, ad esempio, test_ollama.py. Si pu√≤ usare l‚ÄôIDE per eseguirlo o farlo direttamente sul terminale. Da ricordare, inoltre, che si deve sempre chiamare il modello e definirlo quando si esegue uno script stand-alone.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> test_ollama.py</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Di conseguenza, avremo la risposta del modello in un formato JSON:</p>
<pre><code>{'model': 'gemma2:2b', 'created_at': '2024-09-25T14:43:31.869633807Z', 
'response': 'The capital of France is **Paris**. üá´üá∑ \n', 'done': True, 
'done_reason': 'stop', 'context': [106, 1645, 108, 1841, 603, 573, 6037, 576,
6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 
168428, 235248, 244304, 241035, 235248, 108], 'total_duration': 24259469458, 
'load_duration': 19830013859, 'prompt_eval_count': 16, 'prompt_eval_duration': 
1908757000, 'eval_count': 14, 'eval_duration': 2475410000}</code></pre>
<p>Come possiamo vedere, vengono generate diverse informazioni, come:</p>
<ul>
<li><strong>response</strong>: il testo di output principale generato dal modello in risposta al nostro prompt.
<ul>
<li><code>The capital of France is **Paris**. üá´üá∑</code></li>
</ul></li>
<li><strong>context</strong>: gli ID token che rappresentano l‚Äôinput e il contesto utilizzati dal modello. I token sono rappresentazioni numeriche del testo utilizzate per l‚Äôelaborazione dal modello linguistico.
<ul>
<li><code>[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108,</code> <code>106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428,</code> <code>235248, 244304, 241035, 235248, 108]</code></li>
</ul></li>
</ul>
<p>Le Metriche delle Prestazioni:</p>
<ul>
<li><strong>total_duration</strong>: Tempo totale impiegato per l‚Äôoperazione in nanosecondi. In questo caso, circa 24,26 secondi.</li>
<li><strong>load_duration</strong>: Tempo impiegato per caricare il modello o i componenti in nanosecondi. Circa 19,83 secondi.</li>
<li><strong>prompt_eval_duration</strong>: Tempo impiegato per valutare il prompt in nanosecondi. Circa 16 nanosecondi.</li>
<li><strong>eval_count</strong>: Numero di token valutati durante la generazione. Qui, 14 token.</li>
<li><strong>eval_duration</strong>: Tempo impiegato dal modello per generare la risposta in nanosecondi. Circa 2,5 secondi.</li>
</ul>
<p>Ma ci√≤ che vogliamo √® la semplice ‚Äòresponse‚Äô e, forse per l‚Äôanalisi, la durata totale dell‚Äôinferenza, quindi modifichiamo il codice per estrarlo dal dizionario:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>res[<span class="st">'response'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(res[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ora, abbiamo:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of France is <span class="pp">**</span>Paris<span class="pp">**</span>. üá´üá∑ </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 24.26 seconds</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Utilizzo di Ollama.chat()</strong></p>
<p>Un altro modo per ottenere la nostra risposta √® utilizzare <code>ollama.chat()</code>, che genera il messaggio successivo in una chat con un modello fornito. Questo √® un endpoint di streaming, quindi si verificheranno una serie di risposte. Lo streaming pu√≤ essere disabilitato utilizzando <code>"stream": false</code>. L‚Äôoggetto di risposta finale includer√† anche statistiche e dati aggiuntivi dalla richiesta.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>PROMPT_1 <span class="op">=</span> <span class="st">'What is the capital of France?'</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},])</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>resp_1 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(res[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>La risposta √® la stessa di prima.</p>
<p>Una considerazione importante √® che usando <code>ollama.generate()</code>, la risposta √® ‚Äúchiara‚Äù dalla ‚Äúmemoria‚Äù del modello dopo la fine dell‚Äôinferenza (usata solo una volta), ma se vogliamo mantenere una conversazione, dobbiamo usare <code>ollama.chat()</code>. Vediamolo in azione:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>PROMPT_1 <span class="op">=</span> <span class="st">'What is the capital of France?'</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},])</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>resp_1 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(response[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>PROMPT_2 <span class="op">=</span> <span class="st">'and of Italy?'</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'assistant'</span>,<span class="st">'content'</span>: resp_1,},</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_2,},])</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>resp_2 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(response[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Nel codice sopra, stiamo eseguendo due query e il secondo prompt considera il risultato del primo.</p>
<p>Ecco come ha risposto il modello:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of France is <span class="pp">**</span>Paris<span class="pp">**</span>. üá´üá∑ </span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 2.82 seconds</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of Italy is <span class="pp">**</span>Rome<span class="pp">**</span>. üáÆüáπ </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 4.46 seconds</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Ottenere una descrizione dell‚Äôimmagine</strong>:</p>
<p>Allo stesso modo in cui abbiamo utilizzato il modello <code>LlaVa-PHI-3</code> con la riga di comando per analizzare un‚Äôimmagine, lo stesso pu√≤ essere fatto qui con Python. Usiamo la stessa immagine di Parigi, ma ora con <code>ollama.generate()</code>:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'llava-phi3:3.8b'</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>PROMPT <span class="op">=</span> <span class="st">"Describe this picture"</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'image_test_1.jpg'</span>, <span class="st">'rb'</span>) <span class="im">as</span> image_file:</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> image_file.read()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>MODEL,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    prompt<span class="op">=</span>PROMPT,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    images<span class="op">=</span> [img]</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>response[<span class="st">'response'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(res[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ecco il risultato:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">This</span> image captures the iconic cityscape of Paris, France. The vantage point </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="ex">is</span> high, providing a panoramic view of the Seine River that meanders through </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="ex">the</span> heart of the city. Several bridges arch gracefully over the river, </span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="ex">connecting</span> different parts of the city. The Eiffel Tower, an iron lattice </span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="ex">structure</span> with a pointed top and two antennas on its summit, stands tall in the </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="ex">background,</span> piercing the sky. It is painted in a light gray color, contrasting </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="ex">against</span> the blue sky speckled with white clouds.</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> buildings that line the river are predominantly white or beige, their uniform</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="ex">color</span> palette broken occasionally by red roofs peeking through. The Seine River </span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="ex">itself</span> appears calm and wide, reflecting the city<span class="st">'s architectural beauty in its </span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="st">surface. On either side of the river, trees add a touch of green to the urban </span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="st">landscape.</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="st">The image is taken from an elevated perspective, looking down on the city. This </span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="st">viewpoint allows for a comprehensive view of Paris'</span>s beautiful architecture and </span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="ex">layout.</span> The relative positions of the buildings, bridges, and other structures </span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="ex">create</span> a harmonious composition that showcases the city<span class="st">'s charm.</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a><span class="st">In summary, this image presents a serene day in Paris, with its architectural </span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="st">marvels - from the Eiffel Tower to the river-side buildings - all bathed in soft </span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="st">colors under a clear sky.</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="st"> [INFO] Total Duration: 256.45 seconds</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Il modello ha impiegato circa 4 minuti (256,45 s) per restituire una descrizione dettagliata dell‚Äôimmagine.</p>
<blockquote class="blockquote">
<p>Nel notebook <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb">10-Ollama_Python_Library</a>, √® possibile trovare gli esperimenti con la libreria Python Ollama.</p>
</blockquote>
<section id="chiamata-di-funzione" class="level3">
<h3 class="anchored" data-anchor-id="chiamata-di-funzione">Chiamata di Funzione</h3>
<p>Finora, possiamo osservare che utilizzando la risposta del modello in una variabile, possiamo incorporarla efficacemente in progetti reali. Tuttavia, sorge un problema importante quando il modello fornisce risposte diverse allo stesso input. Ad esempio, supponiamo di aver bisogno solo del nome della capitale di un paese e delle sue coordinate come risposta del modello negli esempi precedenti, senza ulteriori informazioni, anche quando si utilizzano modelli dettagliati come Microsoft Phi. Per garantire risposte coerenti, possiamo utilizzare la ‚Äúchiamata di funzione Ollama‚Äù, che √® completamente compatibile con l‚ÄôAPI OpenAI.</p>
<section id="ma-cos√®-esattamente-la-chiamata-di-funzione" class="level4">
<h4 class="anchored" data-anchor-id="ma-cos√®-esattamente-la-chiamata-di-funzione">Ma cos‚Äô√® esattamente la ‚Äúchiamata di funzione‚Äù?</h4>
<p>Nell‚Äôintelligenza artificiale moderna, la chiamata di funzione con Large Language Models (LLM) consente a questi modelli di eseguire azioni che vanno oltre la generazione di testo. Integrandosi con funzioni o API esterne, gli LLM possono accedere a dati in tempo reale, automatizzare attivit√† e interagire con vari sistemi.</p>
<p>Ad esempio, invece di rispondere semplicemente a una query sul meteo, un LLM pu√≤ chiamare un‚ÄôAPI meteo per recuperare le condizioni attuali e fornire informazioni accurate e aggiornate. Questa capacit√† migliora la pertinenza e l‚Äôaccuratezza delle risposte del modello e lo rende uno strumento potente per guidare flussi di lavoro e automatizzare i processi, trasformandolo in un partecipante attivo nelle applicazioni del mondo reale.</p>
<p>Per maggiori dettagli sulla ‚ÄúFunction Calling‚Äù <a href="#chiamata-di-funzione">chiamata di funzione</a>, c‚Äô√® questo video realizzato da <a href="https://www.youtube.com/@MervinPraison">Marvin Prison</a>:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/eHfMCtlsb1o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="creiamo-un-progetto." class="level4">
<h4 class="anchored" data-anchor-id="creiamo-un-progetto.">Creiamo un progetto.</h4>
<p>Vogliamo creare un‚Äô<em>app</em> in cui l‚Äôutente inserisce il nome di un Paese e ottiene, come output, la distanza in km dalla capitale di tale Paese e la posizione dell‚Äôapp (per semplicit√†, useremo Santiago del Cile come posizione dell‚Äôapp).</p>
<p><img src="images/png/block-fc-proj.png" class="img-fluid"></p>
<p>Una volta che l‚Äôutente inserisce il nome di un paese, il modello restituir√† il nome della sua capitale (come stringa) e la latitudine e la longitudine di tale citt√† (in float). Utilizzando queste coordinate, possiamo usare una semplice libreria Python (<a href="https://pypi.org/project/haversine/">haversine</a>) per calcolare la distanza tra quei 2 punti.</p>
<p>L‚Äôidea di questo progetto √® dimostrare una combinazione di interazione del modello linguistico, gestione dei dati strutturati con Pydantic e calcoli geospaziali utilizzando la formula di Haversine (informatica tradizionale).</p>
<p>Per prima cosa, installiamo alcune librerie. Oltre a <em>Haversine</em>, la principale √® la <a href="https://github.com/openai/openai-python">libreria Python OpenAI</a>, che fornisce un comodo accesso all‚ÄôAPI REST OpenAI da qualsiasi applicazione Python 3.7+. L‚Äôaltra √® <a href="https://docs.pydantic.dev/latest/">Pydantic</a> (e instructor), una libreria robusta di convalida dei dati e gestione delle impostazioni progettata da Python per migliorare la robustezza e l‚Äôaffidabilit√† della nostra base di codice. In breve, <em>Pydantic</em> ci aiuter√† a garantire che la risposta del nostro modello sia sempre coerente.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install haversine</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install pydantic </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install instructor</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ora, dovremmo creare uno script Python progettato per interagire con il nostro modello (LLM) per determinare le coordinate della capitale di un paese e calcolare la distanza da Santiago del Cile a quella capitale.</p>
<p>Diamo un‚Äôocchiata al codice:</p>
</section>
</section>
<section id="importazione-delle-librerie" class="level3">
<h3 class="anchored" data-anchor-id="importazione-delle-librerie">1. Importazione delle Librerie</h3>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> haversine <span class="im">import</span> haversine</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> instructor</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>sys</strong>: Fornisce accesso a parametri e funzioni specifici del sistema. Viene utilizzato per ottenere argomenti dalla riga di comando.</li>
<li><strong>haversine</strong>: Una funzione della libreria haversine che calcola la distanza tra due punti geografici utilizzando la formula Haversine.</li>
<li><strong>openAI</strong>: Un modulo per interagire con l‚ÄôAPI OpenAI (anche se viene utilizzato insieme a una configurazione locale, Ollama). Qui tutto √® offline.</li>
<li><strong>pydantic</strong>: Fornisce la convalida dei dati e la gestione delle impostazioni utilizzando annotazioni di tipo Python. Viene utilizzato per definire la struttura dei dati di risposta previsti.</li>
<li><strong>instructor</strong>: Un modulo viene utilizzato per applicare patch al client OpenAI per funzionare in una modalit√† specifica (probabilmente correlata alla gestione dei dati strutturati).</li>
</ul>
</section>
<section id="definizione-di-input-e-modello" class="level3">
<h3 class="anchored" data-anchor-id="definizione-di-input-e-modello">2. Definizione di Input e Modello</h3>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>country <span class="op">=</span> sys.argv[<span class="dv">1</span>]       <span class="co"># Get the country from command-line arguments</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'phi3.5:3.8b'</span>     <span class="co"># The name of the model to be used</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>mylat <span class="op">=</span> <span class="op">-</span><span class="fl">33.33</span>              <span class="co"># Latitude of Santiago de Chile</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>mylon <span class="op">=</span> <span class="op">-</span><span class="fl">70.51</span>              <span class="co"># Longitude of Santiago de Chile</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>country</strong>: In uno script Python, √® possibile ottenere il nome del paese dagli argomenti della riga di comando. In un notebook Jupyter, possiamo immettere il suo nome, ad esempio,
<ul>
<li><code>country = "France"</code></li>
</ul></li>
<li><strong>MODEL</strong>: Specifica il modello utilizzato, che √®, in questo esempio, phi3.5.</li>
<li><strong>mylat</strong> <strong>e</strong> <strong>mylon</strong>: Coordinate di Santiago del Cile, utilizzate come punto di partenza per il calcolo della distanza.</li>
</ul>
</section>
<section id="definizione-della-struttura-dei-dati-di-risposta" class="level3">
<h3 class="anchored" data-anchor-id="definizione-della-struttura-dei-dati-di-risposta">3. Definizione della Struttura dei Dati di Risposta</h3>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CityCoord(BaseModel):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    city: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"Name of the city"</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    lat: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"Decimal Latitude of the city"</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    lon: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"Decimal Longitude of the city"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>CityCoord</strong>: Un modello Pydantic che definisce la struttura prevista della risposta dal LLM. Si aspetta tre campi: city (nome della citt√†), lat (latitudine) e lon (longitudine).</li>
</ul>
</section>
<section id="impostazione-del-client-openai" class="level3">
<h3 class="anchored" data-anchor-id="impostazione-del-client-openai">4. Impostazione del Client OpenAI</h3>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> instructor.patch(</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    OpenAI(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        base_url<span class="op">=</span><span class="st">"http://localhost:11434/v1"</span>,  <span class="co"># Local API base URL (Ollama)</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        api_key<span class="op">=</span><span class="st">"ollama"</span>,                      <span class="co"># API key (not used)</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span>instructor.Mode.JSON,                 <span class="co"># Mode for structured JSON output</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>OpenAI</strong>: Questa configurazione inizializza un client OpenAI con un URL di base locale e una chiave API (ollama). Utilizza un server locale.</li>
<li><strong>instructor.patch</strong>: Applica patch al client OpenAI per funzionare in modalit√† JSON, abilitando un output strutturato che corrisponde al modello Pydantic.</li>
</ul>
</section>
<section id="generazione-della-risposta" class="level3">
<h3 class="anchored" data-anchor-id="generazione-della-risposta">5. Generazione della Risposta</h3>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>resp <span class="op">=</span> client.chat.completions.create(</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>MODEL,</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"content"</span>: <span class="ss">f"return the decimal latitude and decimal longitude </span><span class="ch">\</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="ss">            of the capital of the </span><span class="sc">{</span>country<span class="sc">}</span><span class="ss">."</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    response_model<span class="op">=</span>CityCoord,</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    max_retries<span class="op">=</span><span class="dv">10</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>client.chat.completions.create</strong>: Chiama l‚ÄôLLM per generare una risposta.</li>
<li><strong>model</strong>: Specifica il modello da utilizzare (llava-phi3).</li>
<li><strong>messages</strong>: Contiene il prompt per l‚ÄôLLM, che chiede latitudine e longitudine della capitale del paese specificato.</li>
<li><strong>response_model</strong>: Indica che la risposta deve essere conforme al modello CityCoord.</li>
<li><strong>max_retries</strong>: Numero massimo di tentativi di ripetizione se la richiesta fallisce.</li>
</ul>
</section>
<section id="calcolo-della-distanza" class="level3">
<h3 class="anchored" data-anchor-id="calcolo-della-distanza">6. Calcolo della Distanza</h3>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>distance <span class="op">=</span> haversine((mylat, mylon), (resp.lat, resp.lon), unit<span class="op">=</span><span class="st">'km'</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Santiago de Chile is about </span><span class="sc">{</span><span class="bu">int</span>(<span class="bu">round</span>(distance, <span class="op">-</span><span class="dv">1</span>))<span class="sc">:,}</span><span class="ss"> </span><span class="ch">\</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="ss">        kilometers away from </span><span class="sc">{</span>resp<span class="sc">.</span>city<span class="sc">}</span><span class="ss">."</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>haversine</strong>: Calcola la distanza tra Santiago del Cile e la capitale restituita dall‚ÄôLLM utilizzando le rispettive coordinate.</li>
<li><strong>(mylat, mylon)</strong>: Coordinate di Santiago del Cile.</li>
<li><strong>resp.city</strong>: Nome della capitale del paese</li>
<li><strong>(resp.lat, resp.lon)</strong>: Le coordinate della capitale sono fornite dalla risposta dell‚ÄôLLM.</li>
<li><strong>unit=‚Äòkm‚Äô</strong>: Specifica che la distanza deve essere calcolata in chilometri.</li>
<li><strong>print</strong>: Restituisce la distanza, arrotondata ai 10 chilometri pi√π vicini, con migliaia di separatori per una migliore leggibilit√†.</li>
</ul>
<p><strong>Esecuzione del codice</strong></p>
<p>Se inseriamo paesi diversi, ad esempio Francia, Colombia e Stati Uniti, possiamo notare che riceviamo sempre le stesse informazioni strutturate:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Santiago</span> de Chile is about 8,060 kilometers away from Washington, D.C..</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Santiago</span> de Chile is about 4,250 kilometers away from Bogot√°.</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Santiago</span> de Chile is about 11,630 kilometers away from Paris.</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Eseguendo il codice come script, il risultato verr√† stampato sul terminale:</p>
<p><img src="images/png/script-fc.png" class="img-fluid"></p>
<p>E i calcoli sono piuttosto buoni!</p>
<p><img src="images/png/calc_real.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Nel notebook <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb">20-Ollama_Function_Calling</a>, √® possibile trovare esperimenti con tutti i modelli installati.</p>
</blockquote>
</section>
<section id="aggiunta-di-immagini" class="level3">
<h3 class="anchored" data-anchor-id="aggiunta-di-immagini">Aggiunta di immagini</h3>
<p>Ora √® il momento raccogliere il tutto! Modifichiamo lo script in modo che invece di immettere il nome del paese (come testo), l‚Äôutente immetta un‚Äôimmagine e l‚Äôapplicazione (basata su SLM) restituisca la citt√† nell‚Äôimmagine e la sua posizione geografica. Con quei dati, possiamo calcolare la distanza come prima.</p>
<p><img src="images/png/block-2.png" class="img-fluid"></p>
<p>Per semplicit√†, implementeremo questo nuovo codice in due passaggi. Innanzitutto, LLM analizzer√† l‚Äôimmagine e creer√† una descrizione (testo). Questo testo verr√† passato a un‚Äôaltra istanza, dove il modello estrarr√† le informazioni necessarie per il passaggio.</p>
<p>Inizieremo ad importare le librerie</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> haversine <span class="im">import</span> haversine</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> instructor</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Possiamo vedere l‚Äôimmagine se si esegue il codice su Jupyter Notebook. Per questo dobbiamo anche importare:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Tali librerie non sono necessarie se eseguiamo il codice come uno script.</p>
</blockquote>
<p>Ora, definiamo il modello e le coordinate locali:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'llava-phi3:3.8b'</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>mylat <span class="op">=</span> <span class="op">-</span><span class="fl">33.33</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>mylon <span class="op">=</span> <span class="op">-</span><span class="fl">70.51</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Possiamo effettuare il download di una nuova immagine, ad esempio Machu Picchu da Wikipedia. Sul Notebook possiamo vederla:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"image_test_3.jpg"</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.title("Image")</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/jpeg/image_test_3.jpg" class="img-fluid"></p>
<p>Ora, definiamo una funzione che ricever√† l‚Äôimmagine e che <code>return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located</code> [restituir√† la latitudine decimale e la longitudine decimale della citt√† nell‚Äôimmagine, il suo nome e il paese in cui si trova].</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> image_description(img_path):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(img_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> ollama.chat(</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span>MODEL,</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>              {</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>                <span class="st">'role'</span>: <span class="st">'user'</span>,</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>                <span class="st">'content'</span>: <span class="st">'''return the decimal latitude and decimal longitude </span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="st">                              of the city in the image, its name, and </span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="st">                              what country it is located'''</span>,</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>                <span class="st">'images'</span>: [<span class="bu">file</span>.read()],</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>              },</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>            options <span class="op">=</span> {</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>              <span class="st">'temperature'</span>: <span class="dv">0</span>,</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>              }</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(response['message']['content'])</span></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Possiamo stampare l‚Äôintera risposta per debug.</p>
</blockquote>
<p>La descrizione dell‚Äôimmagine generata per la funzione verr√† passata di nuovo come prompt per il modello.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.perf_counter()  <span class="co"># Start timing</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CityCoord(BaseModel):</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    city: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"Name of the city in the image"</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    country: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"""Name of the country where"</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="st">                                             the city in the image is located</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="st">                                             """</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    lat: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"""Decimal Latitude of the city in"</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="st">                                            the image"""</span>)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    lon: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"""Decimal Longitude of the city in"</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="st">                                           the image"""</span>)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="co"># enables `response_model` in create call</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> instructor.patch(</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    OpenAI(</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>        base_url<span class="op">=</span><span class="st">"http://localhost:11434/v1"</span>,</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>        api_key<span class="op">=</span><span class="st">"ollama"</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span>instructor.Mode.JSON,</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>image_description <span class="op">=</span> image_description(img_path)</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Send this description to the model</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>resp <span class="op">=</span> client.chat.completions.create(</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>MODEL,</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"content"</span>: image_description,</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>    response_model<span class="op">=</span>CityCoord,</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>    max_retries<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Se stampiamo la descrizione dell‚Äôimmagine, otterremo:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> image shows the ancient city of Machu Picchu, located in Peru. The city is</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="ex">perched</span> on a steep hillside and consists of various structures made of stone. It </span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="ex">is</span> surrounded by lush greenery and towering mountains. The sky above is blue with</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="ex">scattered</span> clouds. </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Machu</span> Picchu<span class="st">'s latitude is approximately 13.5086¬∞ S, and its longitude is around</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="st">72.5494¬∞ W.</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>E la seconda risposta del modello (<code>resp</code>) sar√†:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="ex">CityCoord</span><span class="er">(</span><span class="va">city</span><span class="op">=</span><span class="st">'Machu Picchu'</span>, <span class="va">country</span><span class="op">=</span><span class="st">'Peru'</span>, <span class="va">lat</span><span class="op">=</span>-13.5086, <span class="va">lon</span><span class="op">=</span>-72.5494<span class="kw">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ora possiamo effettuare un ‚ÄúPost-Processing‚Äù, calcolando la distanza e preparando la risposta finale:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>distance <span class="op">=</span> haversine((mylat, mylon), (resp.lat, resp.lon), unit<span class="op">=</span><span class="st">'km'</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> The image shows </span><span class="sc">{</span>resp<span class="sc">.</span>city<span class="sc">}</span><span class="ss">, with lat:</span><span class="sc">{</span><span class="bu">round</span>(resp.lat, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> and </span><span class="ch">\</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="ss">      long: </span><span class="sc">{</span><span class="bu">round</span>(resp.lon, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">, located in </span><span class="sc">{</span>resp<span class="sc">.</span>country<span class="sc">}</span><span class="ss"> and about </span><span class="ch">\</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="ss">            </span><span class="sc">{</span><span class="bu">int</span>(<span class="bu">round</span>(distance, <span class="op">-</span><span class="dv">1</span>))<span class="sc">:,}</span><span class="ss"> kilometers away from </span><span class="ch">\</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="ss">            Santiago, Chile.</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.perf_counter()  <span class="co"># End timing</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>elapsed_time <span class="op">=</span> end_time <span class="op">-</span> start_time  <span class="co"># Calculate elapsed time</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" [INFO] ==&gt; The code (running </span><span class="sc">{</span>MODEL<span class="sc">}</span><span class="ss">), took </span><span class="sc">{</span>elapsed_time<span class="sc">:.1f}</span><span class="ss"> </span><span class="ch">\</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="ss">      seconds to execute.</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>E otterremo:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> image shows Machu Picchu, with lat:-13.16 and long: <span class="at">-72.54,</span> located in Peru</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a> <span class="ex">and</span> about 2,250 kilometers away from Santiago, Chile.</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> ==<span class="op">&gt;</span> The code <span class="er">(</span><span class="ex">running</span> llava-phi3:3.8b<span class="kw">)</span><span class="ex">,</span> took 491.3 seconds to execute.</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Nel notebook <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb">30-Function_Calling_with_images</a> √® possibile trovare gli esperimenti con pi√π immagini.</p>
<p>Scarichiamo ora lo script <code>calc_distance_image.py</code> da <a href="">GitHub</a> ed eseguiamolo sul terminale con il comando:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> calc_distance_image.py /home/mjrovai/Documents/OLLAMA/image_test_3.jpg</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Immettere la patch completa dell‚Äôimmagine di Machu Picchu come argomento. Otterremo lo stesso risultato precedente.</p>
<p><img src="images/png/app-machu-picchu.png" class="img-fluid"></p>
<p><em>Per wuanto riguarda</em> Parigi?</p>
<p><img src="images/png/paris-app.png" class="img-fluid"></p>
<p>Naturalmente, ci sono molti modi per ottimizzare il codice utilizzato qui. Tuttavia, l‚Äôidea √® di esplorare il notevole potenziale della <em>function calling</em> con SLM nell‚Äôedge, consentendo a tali modelli di integrarsi con funzioni o API esterne. Andando oltre la generazione di testo, gli SLM possono accedere a dati in tempo reale, automatizzare attivit√† e interagire con vari sistemi.</p>
</section>
</section>
<section id="slm-tecniche-di-ottimizzazione" class="level2">
<h2 class="anchored" data-anchor-id="slm-tecniche-di-ottimizzazione">SLM: Tecniche di Ottimizzazione</h2>
<p>I Large Language Model (LLM) hanno rivoluzionato l‚Äôelaborazione del linguaggio naturale, ma la loro distribuzione e ottimizzazione presentano sfide uniche. Un problema significativo √® la tendenza degli LLM (e pi√π in particolare degli SLM) a generare informazioni che sembrano plausibili ma di fatto errate, un fenomeno noto come <strong>allucinazione</strong>. Ci√≤ si verifica quando i modelli producono contenuti che sembrano coerenti ma non sono basati sulla verit√† o sui fatti del mondo reale.</p>
<p>Altre sfide includono le immense risorse computazionali richieste per l‚Äôaddestramento e l‚Äôesecuzione di questi modelli, la difficolt√† nel mantenere aggiornate le conoscenze all‚Äôinterno del modello e la necessit√† di adattamenti specifici per dominio. Problemi di privacy sorgono anche quando si gestiscono dati sensibili durante l‚Äôaddestramento o l‚Äôinferenza. Inoltre, garantire prestazioni coerenti in diverse attivit√† e mantenere un uso etico di questi potenti strumenti presentano sfide continue. Affrontare questi problemi √® fondamentale per l‚Äôimplementazione efficace e responsabile di LLM in applicazioni del mondo reale.</p>
<p>Le tecniche fondamentali per migliorare le prestazioni e l‚Äôefficienza di LLM (e SLM) sono Fine-tuning, Prompt engineering e Retrieval-Augmented Generation (RAG).</p>
<ul>
<li><p><strong>Fine-tuning</strong>, sebbene richieda pi√π risorse, offre un modo per specializzare LLM per domini o attivit√† particolari. Questo processo comporta un ulteriore addestramento del modello su set di dati attentamente curati, consentendogli di adattare la sua vasta conoscenza generale ad applicazioni specifiche. Il Fine-tuning pu√≤ portare a miglioramenti sostanziali nelle prestazioni, specialmente in campi specializzati o per casi d‚Äôuso unici.</p></li>
<li><p><strong>Prompt engineering</strong> √® in prima linea nell‚Äôottimizzazione LLM. Creando attentamente prompt di input, possiamo guidare i modelli per produrre output pi√π accurati e pertinenti. Questa tecnica comporta la strutturazione di query che sfruttano le conoscenze e le capacit√† pre-addestrate del modello, spesso incorporando esempi o istruzioni specifiche per modellare la risposta desiderata.</p></li>
<li><p><strong>Retrieval-Augmented Generation (RAG)</strong> rappresenta un altro approccio potente per migliorare le prestazioni LLM. Questo metodo combina la vasta conoscenza incorporata nei modelli pre-addestrati con la capacit√† di accedere e incorporare informazioni esterne aggiornate. Recuperando dati pertinenti per integrare il processo decisionale del modello, RAG pu√≤ migliorare significativamente l‚Äôaccuratezza e ridurre la probabilit√† di generare informazioni obsolete o false.</p></li>
</ul>
<p>Per le applicazioni edge, √® pi√π utile concentrarsi su tecniche come RAG che possono migliorare le prestazioni del modello senza dover effettuare un ‚Äúfine-tuning‚Äù sul dispositivo. Esploriamolo.</p>
</section>
<section id="implementazione-del-rag" class="level2">
<h2 class="anchored" data-anchor-id="implementazione-del-rag">Implementazione del RAG</h2>
<p>In un‚Äôinterazione di base tra un utente e un modello linguistico, l‚Äôutente pone una domanda, che viene inviata come prompt al modello. Il modello genera una risposta basata esclusivamente sulla sua conoscenza pre-addestrata. In un processo RAG, c‚Äô√® un passaggio aggiuntivo tra la domanda dell‚Äôutente e la risposta del modello. La domanda dell‚Äôutente innesca un processo di recupero da una ‚Äúknowledge base‚Äù.</p>
<p><img src="images/png/rag-1.png" class="img-fluid"></p>
<section id="un-semplice-progetto-rag" class="level3">
<h3 class="anchored" data-anchor-id="un-semplice-progetto-rag">Un semplice progetto RAG</h3>
<p>Ecco i passaggi per implementare una Retrieval Augmented Generation (RAG) di base:</p>
<ul>
<li><p><strong>Determinare il tipo di documenti che si utilizzeranno:</strong> I tipi migliori sono documenti da cui possiamo ottenere testo pulito e non oscurato. I PDF possono essere problematici perch√© sono progettati per la stampa, non per estrarre testo sensato. Per lavorare con i PDF, dovremmo ottenere il documento di origine o utilizzare strumenti per gestirli.</p></li>
<li><p><strong>Suddividere il testo in blocchi:</strong> Non possiamo archiviare il testo come un unico lungo flusso a causa delle limitazioni delle dimensioni del contesto e della potenziale confusione. La suddivisione in blocchi comporta la suddivisione del testo in parti pi√π piccole. Il testo in blocchi ha molti modi, come conteggio dei caratteri, token, parole, paragrafi o sezioni. √à anche possibile sovrapporre i blocchi.</p></li>
<li><p><strong>Crea embedding:</strong> Gli ‚Äúembedding‚Äù [incorporamenti] sono rappresentazioni numeriche del testo che catturano il significato semantico. Creiamo incorporamenti passando ogni blocco di testo attraverso un particolare modello di embedding. Il modello genera un vettore, la cui lunghezza dipende dal modello di embedding utilizzato. Dovremmo estrarre uno (o pi√π) <a href="https://ollama.com/blog/embedding-models">modelli di embedding</a> da Ollama, per eseguire questa attivit√†. Ecco alcuni esempi di modelli di embedding disponibili su Ollama.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Modello</th>
<th>Dimensione del parametro</th>
<th>Dimensione dell‚ÄôEmbedding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>mxbai-embed-large</td>
<td>334M</td>
<td>1024</td>
</tr>
<tr class="even">
<td>nomic-embed-text</td>
<td>137M</td>
<td>768</td>
</tr>
<tr class="odd">
<td>all-minilm</td>
<td>23M</td>
<td>384</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>In genere, dimensioni dell‚Äôembedding maggiori catturano informazioni pi√π sfumate sull‚Äôinput. Tuttavia, richiedono anche pi√π risorse per l‚Äôelaborazione e un numero maggiore di parametri dovrebbe aumentare la latenza (ma anche la qualit√† della risposta).</p>
</blockquote></li>
<li><p><strong>Memorizzare i blocchi e gli embedding in un database vettoriale:</strong> Avremo bisogno di un modo per trovare in modo efficiente i blocchi di testo pi√π rilevanti per un dato prompt, ed √® qui che entra in gioco un database vettoriale. Useremo <a href="https://www.trychroma.com/">Chromadb</a>, un database vettoriale open source nativo IA, che semplifica la creazione di RAG creando conoscenze, fatti e competenze collegabili per LLM. Vengono memorizzati sia embedding che il testo sorgente per ogni blocco.</p></li>
<li><p><strong>Creare il prompt:</strong> Quando abbiamo una domanda, creiamo un embedding e interroghiamo il database vettoriale per i blocchi pi√π simili. Poi, selezioniamo i primi risultati e includiamo il loro testo nel prompt.</p></li>
</ul>
<p>L‚Äôobiettivo di RAG √® fornire al modello le informazioni pi√π rilevanti dai nostri documenti, consentendogli di generare risposte pi√π accurate e informative. Quindi, implementiamo un semplice esempio di un SLM che incorpora un set particolare di fatti sulle api (‚ÄúBee Facts‚Äù).</p>
<p>All‚Äôinterno dell‚Äôambiente <code>ollama</code>, si inserisce il comando nel terminale per l‚Äôinstallazione di Chromadb:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ollama chromadb</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Tiriamo fuori un modello di embedding intermedio, <code>nomic-embed-text</code></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull nomic-embed-text</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>E creiamo una directory di lavoro:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>cd Documents<span class="op">/</span>OLLAMA<span class="op">/</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>mkdir RAG<span class="op">-</span>simple<span class="op">-</span>bee</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>cd RAG<span class="op">-</span>simple<span class="op">-</span>bee<span class="op">/</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Creiamo un nuovo notebook Jupyter, <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/40-RAG-simple-bee.ipynb">40-RAG-simple-bee</a> per qualche esplorazione:</p>
<p>Importare le librerie necessarie:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> chromadb</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>E definiamo i modelli aor:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>EMB_MODEL <span class="op">=</span> <span class="st">"nomic-embed-text"</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'llama3.2:3B'</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Inizialmente, dovrebbe essere creata una ‚Äúknowledge base‚Äù sui fatti sulle api. Ci√≤ comporta la raccolta di documenti rilevanti e la loro conversione in embedding vettoriali. Questi embedding vengono poi archiviati in un database vettoriale, consentendo in seguito ricerche di similarit√† efficienti. Si immette il ‚Äúdocument‚Äù, una base di ‚Äúfatti sulle api‚Äù come un elenco:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> [</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Bee-keeping, also known as apiculture, involves the maintenance of bee </span><span class="ch">\</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="st">    colonies, typically in hives, by humans."</span>,</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The most commonly kept species of bees is the European honey bee (Apis </span><span class="ch">\</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="st">    mellifera)."</span>,</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"There are another 20,000 different bee species in the world."</span>,  </span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Brazil alone has more than 300 different bee species, and the </span><span class="ch">\</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="st">    vast majority, unlike western honey bees, don‚Äôt sting."</span>, </span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Reports written in 1577 by Hans Staden, mention three native bees </span><span class="ch">\</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="st">    used by indigenous people in Brazil."</span>,</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The indigenous people in Brazil used bees for medicine and food purposes"</span>,</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"From Hans Staden report: probable species: manda√ßaia (Melipona </span><span class="ch">\</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="st">    quadrifasciata), mandaguari (Scaptotrigona postica) and jata√≠-amarela </span><span class="ch">\</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a><span class="st">    (Tetragonisca angustula)."</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Non abbiamo bisogno di ‚Äúsuddividere‚Äù il documento qui perch√© useremo ogni elemento dell‚Äôelenco e un blocco.</p>
</blockquote>
<p>Ora creeremo il nostro database di embedding vettoriale <code>bee_facts</code> e memorizzeremo il documento in esso:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> chromadb.Client()</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>collection <span class="op">=</span> client.create_collection(name<span class="op">=</span><span class="st">"bee_facts"</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># store each document in a vector embedding database</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, d <span class="kw">in</span> <span class="bu">enumerate</span>(documents):</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>  response <span class="op">=</span> ollama.embeddings(model<span class="op">=</span>EMB_MODEL, prompt<span class="op">=</span>d)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>  embedding <span class="op">=</span> response[<span class="st">"embedding"</span>]</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>  collection.add(</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    ids<span class="op">=</span>[<span class="bu">str</span>(i)],</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    embeddings<span class="op">=</span>[embedding],</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    documents<span class="op">=</span>[d]</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>ra che abbiamo creato la nostra ‚ÄúKnowledge Base‚Äù, possiamo iniziare a fare query, recuperando dati da essa:</p>
<p><img src="images/png/rag-2-2.png" class="img-fluid"></p>
<p><strong>Query utente:</strong> Il processo inizia quando un utente pone una domanda, come ‚ÄúQuante api ci sono in una colonia? Chi depone le uova e in che quantit√†? E per quanto riguarda parassiti e malattie comuni?‚Äù</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"How many bees are in a colony? Who lays eggs and how much? How about</span><span class="ch">\</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="st">          common pests and diseases?"</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Query Embedding:</strong> La domanda dell‚Äôutente viene convertita in un embedding vettoriale utilizzando <strong>lo stesso modello di embedding </strong> utilizzato per la knowledge base.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.embeddings(</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>  prompt<span class="op">=</span>prompt,</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>  model<span class="op">=</span>EMB_MODEL</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Recupero di Documenti Pertinenti:</strong> Il sistema esegue una ricerca nella knowledge base utilizzando il ‚Äúquery embedding‚Äù per trovare i documenti pi√π pertinenti (in questo caso, i 5 pi√π probabili). Ci√≤ avviene tramite una ricerca di similarit√†, che confronta la ‚Äúquery embedding‚Äù con gli embedding di documenti nel database.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> collection.query(</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>  query_embeddings<span class="op">=</span>[response[<span class="st">"embedding"</span>]],</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  n_results<span class="op">=</span><span class="dv">5</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> results[<span class="st">'documents'</span>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Prompt Augmentation:</strong> Le informazioni rilevanti recuperate vengono combinate con la query utente originale per creare un prompt ‚Äúaumentato‚Äù. Questo prompt ora contiene la domanda dell‚Äôutente e i fatti pertinenti dalla knowledge base.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>prompt<span class="op">=</span><span class="ss">f"Using this data: </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. Respond to this prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>,</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Generazione di Risposte:</strong> Il prompt aumentato viene poi immesso in un modello linguistico, in questo caso il modello <code>llama3.2:3b</code>. Il modello utilizza questo contesto arricchito per generare una risposta completa. Parametri come temperatura, top_k e top_p vengono impostati per controllare la casualit√† e la qualit√† della risposta generata.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> ollama.generate(</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>  model<span class="op">=</span>MODEL,</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>  prompt<span class="op">=</span><span class="ss">f"Using this data: </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. Respond to this prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>  options<span class="op">=</span>{</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"temperature"</span>: <span class="fl">0.0</span>,</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"top_k"</span>:<span class="dv">10</span>,</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"top_p"</span>:<span class="fl">0.5</span>                          }</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Response Delivery:</strong> Infine, il sistema restituisce all‚Äôutente la risposta generata.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output[<span class="st">'response'</span>])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb64"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Based</span> on the provided data, here are the answers to your questions:</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="ex">1.</span> How many bees are in a colony<span class="pp">?</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="ex">A</span> typical bee colony can contain between 20,000 and 80,000 bees.</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="ex">2.</span> Who lays eggs and how much<span class="pp">?</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> queen bee lays up to 2,000 eggs per day during peak seasons.</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="ex">3.</span> What about common pests and diseases<span class="pp">?</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Common</span> pests and diseases that affect bees include varroa mites, hive beetles,</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="ex">and</span> foulbrood.</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Creiamo una funzione che ci aiuti a rispondere a nuove domande:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rag_bees(prompt, n_results<span class="op">=</span><span class="dv">5</span>, temp<span class="op">=</span><span class="fl">0.0</span>, top_k<span class="op">=</span><span class="dv">10</span>, top_p<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.perf_counter()  <span class="co"># Start timing</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate an embedding for the prompt and retrieve the data </span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> ollama.embeddings(</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>      prompt<span class="op">=</span>prompt,</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>      model<span class="op">=</span>EMB_MODEL</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> collection.query(</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>      query_embeddings<span class="op">=</span>[response[<span class="st">"embedding"</span>]],</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>      n_results<span class="op">=</span>n_results</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> results[<span class="st">'documents'</span>]</span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate a response combining the prompt and data retrieved</span></span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> ollama.generate(</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>      model<span class="op">=</span>MODEL,</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>      prompt<span class="op">=</span><span class="ss">f"Using this data: </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. Respond to this prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>      options<span class="op">=</span>{</span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">"temperature"</span>: temp,</span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">"top_k"</span>: top_k,</span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">"top_p"</span>: top_p                          }</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(output[<span class="st">'response'</span>])</span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.perf_counter()  <span class="co"># End timing</span></span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>    elapsed_time <span class="op">=</span> <span class="bu">round</span>((end_time <span class="op">-</span> start_time), <span class="dv">1</span>)  <span class="co"># Calculate elapsed time</span></span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] ==&gt; The code for model: </span><span class="sc">{</span>MODEL<span class="sc">}</span><span class="ss">, took </span><span class="sc">{</span>elapsed_time<span class="sc">}</span><span class="ss">s </span><span class="ch">\</span></span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a><span class="ss">          to generate the answer.</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ora possiamo creare delle query e chiamare la funzione:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Are bees in Brazil?"</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>rag_bees(prompt)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb67"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Yes,</span> bees are found in Brazil. According to the data, Brazil has more than 300</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="ex">different</span> bee species, and indigenous people in Brazil used bees for medicine and</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="ex">food</span> purposes. Additionally, reports from 1577 mention three native bees used by</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="ex">indigenous</span> people in Brazil.</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> ==<span class="op">&gt;</span> The code for model: llama3.2:3b, took 22.7s to generate the answer.</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A proposito, se il modello utilizzato supporta pi√π lingue, possiamo utilizzarlo (ad esempio, il portoghese), anche se il set di dati √® stato creato in inglese:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Existem abelhas no Brazil?"</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>rag_bees(prompt)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Sim,</span> existem abelhas no Brasil! De acordo com o relato de Hans Staden, h√° tr√™s </span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="ex">esp√©cies</span> de abelhas nativas do Brasil que foram mencionadas: manda√ßaia <span class="er">(</span><span class="ex">Melipona</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="ex">quadrifasciata</span><span class="kw">)</span><span class="ex">,</span> mandaguari <span class="er">(</span><span class="ex">Scaptotrigona</span> postica<span class="kw">)</span> <span class="ex">e</span> jata√≠-amarela <span class="er">(</span><span class="ex">Tetragonisca</span></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="ex">angustula</span><span class="kw">)</span><span class="bu">.</span> Al√©m disso, o Brasil √© conhecido por ter mais de 300 esp√©cies diferentes de abelhas, a maioria das quais n√£o √© agressiva e n√£o p√µe veneno.</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> ==<span class="op">&gt;</span> The code for model: llama3.2:3b, took 54.6s to generate the answer.</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="andando-oltre" class="level3">
<h3 class="anchored" data-anchor-id="andando-oltre">Andando Oltre</h3>
<p>I piccoli modelli LLM testati hanno funzionato bene sull‚Äôedge, sia nel testo che con le immagini, ma ovviamente avevano un‚Äôelevata latenza per quanto riguarda quest‚Äôultima. Una combinazione di modelli specifici e dedicati pu√≤ portare a risultati migliori; ad esempio, in casi reali, un modello di rilevamento degli oggetti (come YOLO) pu√≤ ottenere una descrizione generale e un conteggio degli oggetti su un‚Äôimmagine che, una volta passati a un LLM, possono aiutare a estrarre informazioni e azioni essenziali.</p>
<p>Secondo Avi Baum, CTO di Hailo,</p>
<blockquote class="blockquote">
<p>Nel vasto panorama dell‚Äôintelligenza artificiale (IA), uno dei viaggi pi√π intriganti √® stata l‚Äôevoluzione dell‚ÄôIA nell‚Äôedge. Questo viaggio ci ha portato dalla classica visione artificiale ai regni dell‚ÄôIA discriminativa, dell‚ÄôIA potenziata e ora, alla rivoluzionaria frontiera dell‚ÄôIA generativa. Ogni passo ci ha avvicinato a un futuro in cui i sistemi intelligenti si integrano perfettamente con la nostra vita quotidiana, offrendo un‚Äôesperienza immersiva non solo di percezione ma anche di creazione nel palmo della nostra mano.</p>
</blockquote>
<p><img src="images/jpeg/halo.jpg" class="img-fluid"></p>
</section>
</section>
<section id="conclusione" class="level2">
<h2 class="anchored" data-anchor-id="conclusione">Conclusione</h2>
<p>Questo laboratorio ha dimostrato come un Raspberry Pi 5 pu√≤ essere trasformato in un potente hub AI in grado di eseguire ‚Äúlarge language model (LLM)‚Äù per analisi e approfondimenti dei dati in tempo reale e in loco utilizzando Ollama e Python. La versatilit√† e la potenza del Raspberry Pi, unite alle capacit√† di LLM leggeri come Llama 3.2 e LLaVa-Phi-3-mini, lo rendono un‚Äôeccellente piattaforma per applicazioni di edge computing.</p>
<p>Il potenziale di esecuzione di LLM sull‚Äôedge si estende ben oltre la semplice elaborazione dei dati, come negli esempi di questo laboratorio. Ecco alcuni suggerimenti innovativi per l‚Äôutilizzo di questo progetto:</p>
<p><strong>1. Smart Home Automation:</strong></p>
<ul>
<li>Integrare gli SLM per interpretare i comandi vocali o analizzare i dati dei sensori per l‚Äôautomazione domestica intelligente. Ci√≤ potrebbe includere il monitoraggio e il controllo in tempo reale di dispositivi domestici, sistemi di sicurezza e gestione energetica, tutti elaborati localmente senza fare affidamento sui servizi cloud.</li>
</ul>
<p><strong>2. Raccolta e Analisi dei Dati sul Campo:</strong></p>
<ul>
<li>Distribuire gli SLM su Raspberry Pi in configurazioni remote o mobili per la raccolta e l‚Äôanalisi dei dati in tempo reale. Pu√≤ essere utilizzato in agricoltura per monitorare la salute delle colture, negli studi ambientali per il monitoraggio della fauna selvatica o nella risposta ai disastri per la consapevolezza della situazione e la gestione delle risorse.</li>
</ul>
<p><strong>3. Strumenti Didattici:</strong></p>
<ul>
<li>Creare strumenti didattici interattivi che sfruttano gli SLM per fornire feedback immediato, traduzione linguistica e tutoraggio. Pu√≤ essere particolarmente utile nelle regioni in via di sviluppo con accesso limitato a tecnologie avanzate e connettivit√† Internet.</li>
</ul>
<p><strong>4. Applicazioni Sanitarie:</strong></p>
<ul>
<li>Utilizzare gli SLM per la diagnosi medica e il monitoraggio dei pazienti. Possono fornire analisi in tempo reale dei sintomi e suggerire potenziali trattamenti. Pu√≤ essere integrato in piattaforme di telemedicina o dispositivi sanitari portatili.</li>
</ul>
<p><strong>5. Intelligence Aziendale Locale:</strong></p>
<ul>
<li>Implementare gli SLM in ambienti di vendita al dettaglio o di piccole imprese per analizzare il comportamento dei clienti, gestire l‚Äôinventario e ottimizzare le operazioni. La capacit√† di elaborare i dati localmente garantisce la privacy e riduce la dipendenza dai servizi esterni.</li>
</ul>
<p><strong>6. IoT Industriale:</strong></p>
<ul>
<li>Integrare gli SLM nei sistemi IoT industriali per manutenzione predittiva, controllo qualit√† e ottimizzazione dei processi. Raspberry Pi pu√≤ fungere da unit√† di elaborazione dati localizzata, riducendo la latenza e migliorando l‚Äôaffidabilit√† dei sistemi automatizzati.</li>
</ul>
<p><strong>7. Veicoli Autonomi:</strong></p>
<ul>
<li>Utilizzare gli SLM per elaborare dati sensoriali da veicoli autonomi, consentendo decisioni e navigazione in tempo reale. Questo pu√≤ essere applicato a droni, robot e auto a guida autonoma per una maggiore autonomia e sicurezza.</li>
</ul>
<p><strong>8. Patrimonio Culturale e Turismo:</strong></p>
<ul>
<li>Implementare gli SLM per fornire siti del patrimonio culturale e guide museali interattive e informative. I visitatori possono utilizzare questi sistemi per ottenere informazioni e approfondimenti in tempo reale, migliorando la loro esperienza senza connettivit√† Internet.</li>
</ul>
<p><strong>9. Progetti Artistici e Creativi:</strong></p>
<ul>
<li>Utilizzare SLM per analizzare e generare contenuti creativi, come musica, arte e letteratura. Questo pu√≤ promuovere progetti innovativi nei settori creativi e consentire esperienze interattive uniche in mostre e spettacoli.</li>
</ul>
<p><strong>10. Tecnologie Assistenziali Personalizzate:</strong></p>
<ul>
<li>Sviluppare tecnologie assistenziali per persone con disabilit√†, fornendo supporto personalizzato e adattivo tramite testo in tempo reale, traduzione di lingue e altri strumenti accessibili.</li>
</ul>
</section>
<section id="risorse" class="level2">
<h2 class="anchored" data-anchor-id="risorse">Risorse</h2>
<ul>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb">10-Ollama_Python_Library notebook</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb">20-Ollama_Function_Calling notebook</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb">30-Function_Calling_with_images notebook</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/40-RAG-simple-bee.ipynb">40-RAG-simple-bee notebook</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/calc_distance_image.py">calc_distance_image python script</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../contents/labs/raspi/object_detection/object_detection.it.html" class="pagination-link" aria-label="Rilevamento degli Oggetti">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Rilevamento degli Oggetti</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../contents/labs/shared/shared.it.html" class="pagination-link" aria-label="Lab Condivisi">
        <span class="nav-page-text">Lab Condivisi</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University). Traduzione di <a href="https://github.com/BravoBaldo">Baldassarre Cesarano</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/labs/raspi/llm/llm.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/labs/raspi/llm/llm.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>