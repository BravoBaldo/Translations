<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Rilevamento degli Oggetti – Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../contents/labs/raspi/llm/llm.it.html" rel="next">
<link href="../../../../contents/labs/raspi/image_classification/image_classification.it.html" rel="prev">
<link href="../../../../favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../../scripts/ai_menu/dist/bundle.js" defer=""></script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalità oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.it.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/object_detection/object_detection.it.html">Rilevamento degli Oggetti</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell’IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABORATORI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/part_LABS.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">LABORATORI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/overview.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/part_nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">part_nicla_vision.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/part_xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">part_xiao_esp32s3.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/part_raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">part_raspi.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/part_shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">part_shared.it.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione">Introduzione</a>
  <ul>
  <li><a href="#fondamenti-dellobject-detection" id="toc-fondamenti-dellobject-detection" class="nav-link" data-scroll-target="#fondamenti-dellobject-detection">Fondamenti dell’Object Detection</a>
  <ul class="collapse">
  <li><a href="#classificazione-delle-immagini-vs.-rilevamento-degli-oggetti" id="toc-classificazione-delle-immagini-vs.-rilevamento-degli-oggetti" class="nav-link" data-scroll-target="#classificazione-delle-immagini-vs.-rilevamento-degli-oggetti">Classificazione delle Immagini vs.&nbsp;Rilevamento degli Oggetti</a></li>
  <li><a href="#componenti-chiave-del-rilevamento-degli-oggetti" id="toc-componenti-chiave-del-rilevamento-degli-oggetti" class="nav-link" data-scroll-target="#componenti-chiave-del-rilevamento-degli-oggetti">Componenti Chiave del Rilevamento degli Oggetti</a></li>
  <li><a href="#sfide-nel-rilevamento-degli-oggetti" id="toc-sfide-nel-rilevamento-degli-oggetti" class="nav-link" data-scroll-target="#sfide-nel-rilevamento-degli-oggetti">Sfide nel Rilevamento degli Oggetti</a></li>
  <li><a href="#approcci-al-rilevamento-di-oggetti" id="toc-approcci-al-rilevamento-di-oggetti" class="nav-link" data-scroll-target="#approcci-al-rilevamento-di-oggetti">Approcci al Rilevamento di Oggetti</a></li>
  <li><a href="#metriche-di-valutazione" id="toc-metriche-di-valutazione" class="nav-link" data-scroll-target="#metriche-di-valutazione">Metriche di Valutazione</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati" id="toc-panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati" class="nav-link" data-scroll-target="#panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati">Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati</a>
  <ul>
  <li><a href="#impostazione-dellambiente-tflite" id="toc-impostazione-dellambiente-tflite" class="nav-link" data-scroll-target="#impostazione-dellambiente-tflite">Impostazione dell’Ambiente TFLite</a></li>
  <li><a href="#creazione-di-una-directory-di-lavoro" id="toc-creazione-di-una-directory-di-lavoro" class="nav-link" data-scroll-target="#creazione-di-una-directory-di-lavoro">Creazione di una Directory di Lavoro:</a></li>
  <li><a href="#inferenza-e-post-elaborazione" id="toc-inferenza-e-post-elaborazione" class="nav-link" data-scroll-target="#inferenza-e-post-elaborazione">Inferenza e Post-Elaborazione</a></li>
  <li><a href="#efficientdet" id="toc-efficientdet" class="nav-link" data-scroll-target="#efficientdet">EfficientDet</a></li>
  </ul></li>
  <li><a href="#progetto-di-rilevamento-di-oggetti" id="toc-progetto-di-rilevamento-di-oggetti" class="nav-link" data-scroll-target="#progetto-di-rilevamento-di-oggetti">Progetto di Rilevamento di Oggetti</a>
  <ul>
  <li><a href="#lobiettivo" id="toc-lobiettivo" class="nav-link" data-scroll-target="#lobiettivo">L’Obiettivo</a></li>
  <li><a href="#raccolta-dati-grezzi" id="toc-raccolta-dati-grezzi" class="nav-link" data-scroll-target="#raccolta-dati-grezzi">Raccolta Dati Grezzi</a></li>
  <li><a href="#etichettatura-dei-dati" id="toc-etichettatura-dei-dati" class="nav-link" data-scroll-target="#etichettatura-dei-dati">Etichettatura dei Dati</a>
  <ul class="collapse">
  <li><a href="#annotazione" id="toc-annotazione" class="nav-link" data-scroll-target="#annotazione">Annotazione</a></li>
  <li><a href="#pre-elaborazione-dei-dati" id="toc-pre-elaborazione-dei-dati" class="nav-link" data-scroll-target="#pre-elaborazione-dei-dati">Pre-elaborazione dei Dati</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio" id="toc-addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio" class="nav-link" data-scroll-target="#addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio">Addestramento di un Modello SSD MobileNet su Edge Impulse Studio</a>
  <ul>
  <li><a href="#caricamento-dei-dati-annotati" id="toc-caricamento-dei-dati-annotati" class="nav-link" data-scroll-target="#caricamento-dei-dati-annotati">Caricamento dei dati annotati</a></li>
  <li><a href="#impulse-design" id="toc-impulse-design" class="nav-link" data-scroll-target="#impulse-design">Impulse Design</a></li>
  <li><a href="#pre-elaborazione-di-tutti-i-dataset" id="toc-pre-elaborazione-di-tutti-i-dataset" class="nav-link" data-scroll-target="#pre-elaborazione-di-tutti-i-dataset">Pre-elaborazione di tutti i dataset</a></li>
  <li><a href="#progettazione-addestramento-e-test-del-modello" id="toc-progettazione-addestramento-e-test-del-modello" class="nav-link" data-scroll-target="#progettazione-addestramento-e-test-del-modello">Progettazione, Addestramento e Test del Modello</a></li>
  <li><a href="#distribuzione-del-modello" id="toc-distribuzione-del-modello" class="nav-link" data-scroll-target="#distribuzione-del-modello">Distribuzione del modello</a></li>
  <li><a href="#inferenza-e-post-elaborazione-1" id="toc-inferenza-e-post-elaborazione-1" class="nav-link" data-scroll-target="#inferenza-e-post-elaborazione-1">Inferenza e Post-Elaborazione</a></li>
  </ul></li>
  <li><a href="#training-di-un-modello-fomo-su-edge-impulse-studio" id="toc-training-di-un-modello-fomo-su-edge-impulse-studio" class="nav-link" data-scroll-target="#training-di-un-modello-fomo-su-edge-impulse-studio">Training di un modello FOMO su Edge Impulse Studio</a>
  <ul>
  <li><a href="#come-funziona-fomo" id="toc-come-funziona-fomo" class="nav-link" data-scroll-target="#come-funziona-fomo">Come funziona FOMO?</a></li>
  <li><a href="#impulse-design-nuovo-training-e-test" id="toc-impulse-design-nuovo-training-e-test" class="nav-link" data-scroll-target="#impulse-design-nuovo-training-e-test">Impulse Design, nuovo Training e Test</a></li>
  <li><a href="#distribuzione-del-modello-1" id="toc-distribuzione-del-modello-1" class="nav-link" data-scroll-target="#distribuzione-del-modello-1">Distribuzione del modello</a></li>
  <li><a href="#inferenza-e-post-elaborazione-2" id="toc-inferenza-e-post-elaborazione-2" class="nav-link" data-scroll-target="#inferenza-e-post-elaborazione-2">Inferenza e Post-Elaborazione</a></li>
  </ul></li>
  <li><a href="#esplorazione-di-un-modello-yolo-tramite-ultralitics" id="toc-esplorazione-di-un-modello-yolo-tramite-ultralitics" class="nav-link" data-scroll-target="#esplorazione-di-un-modello-yolo-tramite-ultralitics">Esplorazione di un Modello YOLO tramite Ultralitics</a>
  <ul>
  <li><a href="#a-proposito-del-modello-yolo" id="toc-a-proposito-del-modello-yolo" class="nav-link" data-scroll-target="#a-proposito-del-modello-yolo">A proposito del modello YOLO</a>
  <ul class="collapse">
  <li><a href="#caratteristiche-principali" id="toc-caratteristiche-principali" class="nav-link" data-scroll-target="#caratteristiche-principali">Caratteristiche Principali:</a></li>
  </ul></li>
  <li><a href="#installazione" id="toc-installazione" class="nav-link" data-scroll-target="#installazione">Installazione</a></li>
  <li><a href="#test-di-yolo" id="toc-test-di-yolo" class="nav-link" data-scroll-target="#test-di-yolo">Test di YOLO</a></li>
  <li><a href="#esportazione-del-modello-in-formato-ncnn" id="toc-esportazione-del-modello-in-formato-ncnn" class="nav-link" data-scroll-target="#esportazione-del-modello-in-formato-ncnn">Esportazione del Modello in formato NCNN</a></li>
  <li><a href="#esplorare-yolo-con-python" id="toc-esplorare-yolo-con-python" class="nav-link" data-scroll-target="#esplorare-yolo-con-python">Esplorare YOLO con Python</a></li>
  <li><a href="#addestramento-di-yolov8-su-un-dataset-personalizzato" id="toc-addestramento-di-yolov8-su-un-dataset-personalizzato" class="nav-link" data-scroll-target="#addestramento-di-yolov8-su-un-dataset-personalizzato">Addestramento di YOLOv8 su un Dataset Personalizzato</a>
  <ul class="collapse">
  <li><a href="#punti-critici-sul-notebook" id="toc-punti-critici-sul-notebook" class="nav-link" data-scroll-target="#punti-critici-sul-notebook">Punti critici sul Notebook:</a></li>
  </ul></li>
  <li><a href="#inferenza-col-modello-addestrato-usando-raspi" id="toc-inferenza-col-modello-addestrato-usando-raspi" class="nav-link" data-scroll-target="#inferenza-col-modello-addestrato-usando-raspi">Inferenza col modello addestrato, usando Raspi</a></li>
  </ul></li>
  <li><a href="#rilevamento-di-oggetti-su-un-live-streaming" id="toc-rilevamento-di-oggetti-su-un-live-streaming" class="nav-link" data-scroll-target="#rilevamento-di-oggetti-su-un-live-streaming">Rilevamento di Oggetti su un Live Streaming</a></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione">Conclusione</a></li>
  <li><a href="#risorse" id="toc-risorse" class="nav-link" data-scroll-target="#risorse">Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/labs/raspi/object_detection/object_detection.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/labs/raspi/object_detection/object_detection.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.it.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/object_detection/object_detection.it.html">Rilevamento degli Oggetti</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Rilevamento degli Oggetti</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/jpeg/cover.jpg" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E prompt - Immagine di copertina per un capitolo “Rilevamento oggetti” in un tutorial su Raspberry Pi, progettata nello stesso stile da laboratorio di elettronica degli anni ’50 delle copertine precedenti. La scena dovrebbe presentare in primo piano ruote e cubi, simili a quelli forniti dall’utente, posizionati su un banco da lavoro. Un Raspberry Pi con un modulo fotocamera collegato dovrebbe catturare un’immagine di questi oggetti. Circondare la scena con classici strumenti da laboratorio come saldatori, resistori e fili. Lo sfondo del laboratorio dovrebbe includere apparecchiature d’epoca come oscilloscopi e radio a valvole, mantenendo il tocco dettagliato e nostalgico dell’epoca. Non dovrebbero essere inclusi testo o loghi.</em></figcaption>
</figure>
</div>
<section id="introduzione" class="level2">
<h2 class="anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>Sulla base della nostra esplorazione della classificazione delle immagini, ora rivolgiamo la nostra attenzione a un’attività di visione artificiale più avanzata: il rilevamento degli oggetti. Mentre la classificazione delle immagini assegna una singola etichetta a un’intera immagine, il rilevamento degli oggetti va oltre identificando e localizzando più oggetti all’interno di una singola immagine. Questa capacità apre molte nuove applicazioni e sfide, in particolare nell’edge computing e nei dispositivi IoT come Raspberry Pi.</p>
<p>Il rilevamento degli oggetti combina le attività di classificazione e localizzazione. Non solo determina quali oggetti sono presenti in un’immagine, ma individua anche le loro posizioni, ad esempio disegnando riquadri di delimitazione attorno a essi. Questa complessità aggiunta rende il rilevamento degli oggetti uno strumento più potente per comprendere le scene visive, ma richiede anche modelli e tecniche di training più sofisticati.</p>
<p>Nell’IA edge, dove lavoriamo con risorse computazionali limitate, l’implementazione di modelli di rilevamento degli oggetti efficienti diventa cruciale. Le sfide che abbiamo affrontato con la classificazione delle immagini, ovvero bilanciare le dimensioni del modello, la velocità di inferenza e l’accuratezza, sono amplificate nel rilevamento degli oggetti. Tuttavia, i vantaggi sono anche più significativi, poiché il rilevamento degli oggetti consente un’analisi dei dati visivi più sfumata e dettagliata.</p>
<p>Alcune applicazioni del rilevamento di oggetti su dispositivi edge includono:</p>
<ol type="1">
<li>Sistemi di sorveglianza e sicurezza</li>
<li>Veicoli autonomi e droni</li>
<li>Controllo industriale della qualità</li>
<li>Monitoraggio della fauna selvatica</li>
<li>Applicazioni di realtà aumentata</li>
</ol>
<p>Mettendo le mani nel rilevamento di oggetti, ci baseremo sui concetti e sulle tecniche esplorate nella classificazione delle immagini. Esamineremo le architetture di rilevamento di oggetti più diffuse progettate per l’efficienza, come:</p>
<ul>
<li>“Single Stage Detectors” [Rilevatori a stadio singolo], come MobileNet ed EfficientDet,</li>
<li>FOMO (Faster Objects, More Objects), e</li>
<li>YOLO (You Only Look Once).</li>
</ul>
<blockquote class="blockquote">
<p>Per saperne di più sui modelli di rilevamento di oggetti, seguire il tutorial <a href="https://machinelearningmastery.com/object-recognition-with-deep-learning/">A Gentle Introduction to Object Recognition With Deep Learning</a>.</p>
</blockquote>
<p>Esploreremo quei modelli di rilevamento di oggetti utilizzando</p>
<ul>
<li>TensorFlow Lite Runtime (ora modificato in <a href="https://ai.google.dev/edge/litert">LiteRT</a>),</li>
<li>Edge Impulse Linux Python SDK e</li>
<li>Ultralitics</li>
</ul>
<p><img src="images/png/block.png" class="img-fluid"></p>
<p>In questo laboratorio, tratteremo i fondamenti del rilevamento degli oggetti e le sue differenze rispetto alla classificazione delle immagini. Impareremo anche come addestrare, mettere a punto, testare, ottimizzare e distribuire le architetture di rilevamento degli oggetti più diffuse utilizzando un dataset creato da zero.</p>
<section id="fondamenti-dellobject-detection" class="level3">
<h3 class="anchored" data-anchor-id="fondamenti-dellobject-detection">Fondamenti dell’Object Detection</h3>
<p>Il rilevamento degli oggetti si basa sulle fondamenta della classificazione delle immagini, ma ne amplia notevolmente le capacità. Per comprendere il rilevamento degli oggetti, è fondamentale innanzitutto riconoscere le sue principali differenze rispetto alla classificazione delle immagini:</p>
<section id="classificazione-delle-immagini-vs.-rilevamento-degli-oggetti" class="level4">
<h4 class="anchored" data-anchor-id="classificazione-delle-immagini-vs.-rilevamento-degli-oggetti">Classificazione delle Immagini vs.&nbsp;Rilevamento degli Oggetti</h4>
<p><strong>Classificazione delle Immagini:</strong></p>
<ul>
<li>Assegna una singola etichetta a un’intera immagine</li>
<li>Risponde alla domanda: “Qual è l’oggetto o la scena principale di questa immagine?”</li>
<li>Genera una singola previsione di classe per l’intera immagine</li>
</ul>
<p><strong>Rilevamento degli oggetti:</strong></p>
<ul>
<li>Identifica e individua più oggetti all’interno di un’immagine</li>
<li>Risponde alle domande: “Quali oggetti sono presenti in questa immagine e dove si trovano?”</li>
<li>Genera più previsioni, ciascuna composta da un’etichetta di classe e un riquadro di delimitazione</li>
</ul>
<p>Per visualizzare questa differenza, prendiamo in considerazione un esempio: <img src="images/jpeg/objxclas.jpg" class="img-fluid"></p>
<p>Questo diagramma illustra la differenza critica: la classificazione delle immagini fornisce un’unica etichetta per l’intera immagine, mentre il rilevamento degli oggetti identifica più oggetti, le loro classi e le loro posizioni all’interno dell’immagine.</p>
</section>
<section id="componenti-chiave-del-rilevamento-degli-oggetti" class="level4">
<h4 class="anchored" data-anchor-id="componenti-chiave-del-rilevamento-degli-oggetti">Componenti Chiave del Rilevamento degli Oggetti</h4>
<p>I sistemi di rilevamento degli oggetti sono in genere costituiti da due componenti principali:</p>
<ol type="1">
<li><p>Localizzazione degli Oggetti: Questo componente identifica dove si trovano gli oggetti nell’immagine. In genere genera riquadri di delimitazione, regioni rettangolari che racchiudono ogni oggetto rilevato.</p></li>
<li><p>Classificazione degli Oggetti: Questo componente determina la classe o la categoria di ogni oggetto rilevato, in modo simile alla classificazione delle immagini ma applicato a ogni regione localizzata.</p></li>
</ol>
</section>
<section id="sfide-nel-rilevamento-degli-oggetti" class="level4">
<h4 class="anchored" data-anchor-id="sfide-nel-rilevamento-degli-oggetti">Sfide nel Rilevamento degli Oggetti</h4>
<p>Il rilevamento degli oggetti presenta diverse sfide oltre a quelle della classificazione delle immagini:</p>
<ul>
<li>Oggetti multipli: Un’immagine può contenere più oggetti di varie classi, dimensioni e posizioni.</li>
<li>Scale variabili: Gli oggetti possono apparire a diverse dimensioni all’interno dell’immagine.</li>
<li>Occlusione: Gli oggetti possono essere parzialmente nascosti o sovrapposti.</li>
<li>Disordine di sfondo: Distinguere gli oggetti da sfondi complessi può essere difficile.</li>
<li>Prestazioni in tempo reale: Molte applicazioni richiedono tempi di inferenza rapidi, soprattutto su dispositivi edge.</li>
</ul>
</section>
<section id="approcci-al-rilevamento-di-oggetti" class="level4">
<h4 class="anchored" data-anchor-id="approcci-al-rilevamento-di-oggetti">Approcci al Rilevamento di Oggetti</h4>
<p>Esistono due approcci principali al rilevamento di oggetti:</p>
<ol type="1">
<li><p>Rilevatori a due stadi: Prima propongono le regioni di interesse e poi classificano ciascuna regione. Esempi includono R-CNN e le sue varianti (Fast R-CNN, Faster R-CNN).</p></li>
<li><p>Rilevatori a stadio singolo: Questi prevedono bounding box (o centroidi) e probabilità di classe in un passaggio “forward” [in avanti] della rete. Esempi includono YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector) e FOMO (Faster Objects, More Objects). Questi sono spesso più veloci e più adatti per dispositivi edge come Raspberry Pi.</p></li>
</ol>
</section>
<section id="metriche-di-valutazione" class="level4">
<h4 class="anchored" data-anchor-id="metriche-di-valutazione">Metriche di Valutazione</h4>
<p>Il rilevamento degli oggetti utilizza metriche diverse rispetto alla classificazione delle immagini:</p>
<ul>
<li><strong>Intersection over Union (IoU)</strong>: Misura la sovrapposizione tra bounding box previsti e “ground truth” [reali].</li>
<li><strong>Mean Average Precision (mAP)</strong>: Combina precisione e richiamo in tutte le classi e soglie IoU.</li>
<li><strong>Frames Per Second (FPS)</strong>: Misura la velocità di rilevamento, fondamentale per le applicazioni in tempo reale su dispositivi edge.</li>
</ul>
</section>
</section>
</section>
<section id="panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati" class="level2">
<h2 class="anchored" data-anchor-id="panoramica-dei-modelli-di-rilevamento-degli-oggetti-pre-addestrati">Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati</h2>
<p>Come abbiamo visto nell’introduzione, data un’immagine o un flusso video, un modello di rilevamento degli oggetti può identificare quale, di un set noto di oggetti, potrebbe essere presente e fornire informazioni sulle loro posizioni all’interno dell’immagine.</p>
<blockquote class="blockquote">
<p>Si possono testare alcuni modelli comuni online visitando <a href="https://mediapipe-studio.webapps.google.com/studio/demo/object_detector">Object Detection - MediaPipe Studio</a></p>
</blockquote>
<p>Su <a href="https://www.kaggle.com/models?id=298,130,299">Kaggle</a>, possiamo trovare i modelli tflite pre-addestrati più comuni da usare con Raspi, <a href="https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v1/tfLite">ssd_mobilenet_v1</a> e <a href="https://www.kaggle.com/models/tensorflow/efficientdet/tfLite">EfficientDet</a>. Quei modelli sono stati addestrati sul dataset COCO (Common Objects in Context), con oltre 200.000 immagini etichettate in 91 categorie. Scaricare i modelli e caricali nella cartella <code>./models</code> in Raspi.</p>
<blockquote class="blockquote">
<p>In alternativa<a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models">,</a> si possono trovare i modelli e le etichette COCO su <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models">GitHub</a>.</p>
</blockquote>
<p>Per la prima parte di questo laboratorio, ci concentreremo su un modello SSD-Mobilenet V1 300x300 pre-addestrato e lo confronteremo con EfficientDet-lite0 320x320, anch’esso addestrato utilizzando il set di dati COCO 2017. Entrambi i modelli sono stati convertiti in un formato TensorFlow Lite (4,2 MB per SSD Mobilenet e 4,6 MB per EfficientDet).</p>
<blockquote class="blockquote">
<p>SSD-Mobilenet V2 o V3 è consigliato per progetti di “transfer learning”, ma una volta che il modello TFLite V1 sarà disponibile al pubblico, lo useremo per questa panoramica.</p>
</blockquote>
<p><img src="images/png/model-deploy.png" class="img-fluid"></p>
<section id="impostazione-dellambiente-tflite" class="level3">
<h3 class="anchored" data-anchor-id="impostazione-dellambiente-tflite">Impostazione dell’Ambiente TFLite</h3>
<p>Dovremmo confermare i passaggi eseguiti nell’ultimo laboratorio pratico, Image Classification, come segue:</p>
<ul>
<li><p>Aggiornamento di Raspberry Pi</p></li>
<li><p>Installazione delle Librerie Richieste</p></li>
<li><p>Impostazione di un Ambiente Virtuale (Facoltativo ma Consigliato)</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/tflite/bin/activate</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>Installazione di TensorFlow Lite Runtime</p></li>
<li><p>Installazione di Additional Python Libraries (all’interno dell’ambiente)</p></li>
</ul>
</section>
<section id="creazione-di-una-directory-di-lavoro" class="level3">
<h3 class="anchored" data-anchor-id="creazione-di-una-directory-di-lavoro">Creazione di una Directory di Lavoro:</h3>
<p>Considerando che abbiamo creato la cartella <code>Documents/TFLITE</code> nell’ultimo Lab, creiamo ora le cartelle specifiche per questo lab di rilevamento degli oggetti:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/TFLITE/</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> OBJ_DETECT</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> OBJ_DETECT</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> images</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> models</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> models</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="inferenza-e-post-elaborazione" class="level3">
<h3 class="anchored" data-anchor-id="inferenza-e-post-elaborazione">Inferenza e Post-Elaborazione</h3>
<p>Apriamo un nuovo <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_MobileNetV1.ipynb">notebook</a> per seguire tutti i passaggi per rilevare oggetti su un’immagine:</p>
<p>Importare le librerie necessarie:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tflite_runtime.interpreter <span class="im">as</span> tflite</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Caricare il modello TFLite e allocare i tensori:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"./models/ssd-mobilenet-v1-tflite-default-v1.tflite"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>interpreter <span class="op">=</span> tflite.Interpreter(model_path<span class="op">=</span>model_path)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>interpreter.allocate_tensors()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ottenere i tensori di input e output.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>input_details <span class="op">=</span> interpreter.get_input_details()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>output_details <span class="op">=</span> interpreter.get_output_details()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Input details</strong> ci informeranno su come il modello dovrebbe essere alimentato con un’immagine. La forma di <code>(1, 300, 300, 3)</code> con un dtype di <code>uint8</code> ci dice che un’immagine non normalizzata (intervallo di valori pixel da 0 a 255) con dimensioni (300x300x3) dovrebbe essere inserita una alla volta (Batch Dimension: 1).</p>
<p>Gli <strong>output details</strong> includono non solo le etichette (“classes”) e le probabilità (“scores”), ma anche la posizione relativa della finestra dei bounding box (“box”) su dove si trova l’oggetto nell’immagine e il numero di oggetti rilevati (“num_detections”). I dettagli di output ci dicono anche che il modello può rilevare un <strong>massimo di 10 oggetti</strong> nell’immagine.</p>
<p><img src="images/png/inference result.png" class="img-fluid"></p>
<p>Quindi, per l’esempio precedente, utilizzando la stessa immagine di gatto utilizzata col <em>Lab di Classificazione Immagine</em> alla ricerca dell’output, abbiamo una <strong>probabilità del 76%</strong> di aver trovato un oggetto con un <strong>ID classe di 16</strong> su un’area delimitata da un <strong>bounding box di [0.028011084, 0.020121813, 0.9886069, 0.802299]</strong>. Questi quattro numeri sono correlati a <code>ymin</code>, <code>xmin</code>, <code>ymax</code> e <code>xmax</code>, le coordinate del box.</p>
<p>Considerando che <strong>y</strong> va dall’alto <code>(ymin</code>) al basso (<code>ymax</code>) e <strong>x</strong> va da sinistra (<code>xmin</code>) a destra (<code>xmax</code>), abbiamo, di fatto, le coordinate dell’angolo superiore/sinistro e di quello inferiore/destro. Con entrambi i bordi e conoscendo la forma dell’immagine, è possibile disegnare un rettangolo attorno all’oggetto, come mostrato nella figura sottostante:</p>
<p><img src="images/png/boulding-boxes.png" class="img-fluid"></p>
<p>Successivamente, dovremmo scoprire cosa significa un ID di classe uguale a 16. Aprendo il file <code>coco_labels.txt</code>, come un elenco, ogni elemento ha un indice associato e ispezionando l’indice 16, otteniamo, come previsto, <code>cat</code>. La probabilità è il valore restituito dal punteggio.</p>
<p>Carichiamo ora alcune immagini con più oggetti su di esse per il test.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"./images/cat_dog.jpeg"</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>orig_img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(orig_img)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/cat-dog.png" class="img-fluid"></p>
<p>In base ai dettagli di input, pre-elaboriamo l’immagine, modificandone la forma ed espandendone le dimensioni:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> orig_img.resize((input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">1</span>], </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                  input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">2</span>]))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>input_data <span class="op">=</span> np.expand_dims(img, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>input_data.shape, input_data.dtype</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>La nuova forma input_data è<code>(1, 300, 300, 3)</code> con un dtype di <code>uint8</code>, che è compatibile con quanto previsto dal modello.</p>
<p>Utilizzando input_data, eseguiamo l’interprete, misuriamo la latenza e otteniamo l’output:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>interpreter.set_tensor(input_details[<span class="dv">0</span>][<span class="st">'index'</span>], input_data)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>interpreter.invoke()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> (end_time <span class="op">-</span> start_time) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Convert to milliseconds</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"Inference time: </span><span class="sc">{:.1f}</span><span class="st">ms"</span>.<span class="bu">format</span>(inference_time))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Con una latenza di circa 800 ms, possiamo ottenere 4 output distinti:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>boxes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">0</span>][<span class="st">'index'</span>])[<span class="dv">0</span>] </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">1</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">2</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]   </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>num_detections <span class="op">=</span> <span class="bu">int</span>(interpreter.get_tensor(output_details[<span class="dv">3</span>][<span class="st">'index'</span>])[<span class="dv">0</span>])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Da una rapida occhiata, possiamo vedere che il modello ha rilevato 2 oggetti con un punteggio superiore a 0,5:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_detections):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> scores[i] <span class="op">&gt;</span> <span class="fl">0.5</span>:  <span class="co"># Confidence threshold</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Object </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Bounding Box: </span><span class="sc">{</span>boxes[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Confidence: </span><span class="sc">{</span>scores[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Class: </span><span class="sc">{</span>classes[i]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/infer-mobv1.png" class="img-fluid"></p>
<p>E possiamo anche visualizzare i risultati:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(orig_img)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_detections):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> scores[i] <span class="op">&gt;</span> <span class="fl">0.5</span>:  <span class="co"># Adjust threshold as needed</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        ymin, xmin, ymax, xmax <span class="op">=</span> boxes[i]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        (left, right, top, bottom) <span class="op">=</span> (xmin <span class="op">*</span> orig_img.width, </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>                                      xmax <span class="op">*</span> orig_img.width, </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>                                      ymin <span class="op">*</span> orig_img.height, </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                                      ymax <span class="op">*</span> orig_img.height)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        rect <span class="op">=</span> plt.Rectangle((left, top), right<span class="op">-</span>left, bottom<span class="op">-</span>top, </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>                             fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        plt.gca().add_patch(rect)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        class_id <span class="op">=</span> <span class="bu">int</span>(classes[i])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> labels[class_id]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        plt.text(left, top<span class="op">-</span><span class="dv">10</span>, <span class="ss">f'</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>scores[i]<span class="sc">:.2f}</span><span class="ss">'</span>, </span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'red'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, backgroundcolor<span class="op">=</span><span class="st">'white'</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/visual_inf.png" class="img-fluid"></p>
</section>
<section id="efficientdet" class="level3">
<h3 class="anchored" data-anchor-id="efficientdet">EfficientDet</h3>
<p>EfficientDet non è tecnicamente un modello SSD (Single Shot Detector), ma condivide alcune somiglianze e si basa su idee provenienti da SSD e altre architetture di rilevamento degli oggetti:</p>
<ol type="1">
<li>EfficientDet:
<ul>
<li>Sviluppato dai ricercatori di Google nel 2019</li>
<li>Utilizza EfficientNet come rete backbone</li>
<li>Utilizza una nuova “bi-directional feature pyramid network (BiFPN)” [rete piramidale di feature bidirezionale]</li>
<li>Utilizza il ridimensionamento composto per ridimensionare in modo efficiente la rete backbone e i componenti di rilevamento degli oggetti.</li>
</ul></li>
<li>Somiglianze con SSD:
<ul>
<li>Entrambi sono rilevatori a stadio singolo, il che significa che eseguono la localizzazione e la classificazione degli oggetti in un singolo passaggio “forward” [in avanti].</li>
<li>Entrambi utilizzano mappe di feature multiscala per rilevare oggetti a diverse scale.</li>
</ul></li>
<li>Differenze principali:
<ul>
<li>Backbone: SSD in genere utilizza VGG o MobileNet, mentre EfficientDet utilizza EfficientNet.</li>
<li>Fusione di funzionalità: SSD utilizza una semplice piramide di funzionalità, mentre EfficientDet utilizza il più avanzato BiFPN.</li>
<li>Metodo di ridimensionamento: EfficientDet introduce il ridimensionamento composto per tutti i componenti della rete</li>
</ul></li>
<li>Vantaggi di EfficientDet:
<ul>
<li>In genere, raggiunge migliori compromessi tra accuratezza ed efficienza rispetto a SSD e molti altri modelli di rilevamento di oggetti.</li>
<li>Un ridimensionamento più flessibile consente una famiglia di modelli con diversi compromessi tra dimensioni e prestazioni.</li>
</ul></li>
</ol>
<p>Sebbene EfficientDet non sia un modello SSD, può essere visto come un’evoluzione delle architetture di rilevamento a fase singola, che incorpora tecniche più avanzate per migliorare efficienza e precisione. Quando si utilizza EfficientDet, possiamo aspettarci strutture di output simili a SSD (ad esempio, bounding box e punteggi di classe).</p>
<blockquote class="blockquote">
<p>Su GitHub, si trova un altro <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_EfficientDet.ipynb">notebook</a> che esplora il modello EfficientDet che abbiamo realizzato con SSD MobileNet.</p>
</blockquote>
</section>
</section>
<section id="progetto-di-rilevamento-di-oggetti" class="level2">
<h2 class="anchored" data-anchor-id="progetto-di-rilevamento-di-oggetti">Progetto di Rilevamento di Oggetti</h2>
<p>Ora, svilupperemo un progetto completo di classificazione delle immagini dalla raccolta dei dati all’addestramento e all’implementazione. Come abbiamo fatto con il progetto di classificazione delle immagini, il modello addestrato e convertito verrà utilizzato per l’inferenza.</p>
<p>Utilizzeremo lo stesso set di dati per addestrare 3 modelli: SSD-MobileNet V2, FOMO e YOLO.</p>
<section id="lobiettivo" class="level3">
<h3 class="anchored" data-anchor-id="lobiettivo">L’Obiettivo</h3>
<p>Tutti i progetti di Machine Learning devono iniziare con un obiettivo. Supponiamo di trovarci in una struttura industriale e di dover ordinare e contare <strong>ruote</strong> e <strong>scatole</strong> speciali.</p>
<p><img src="images/jpeg/proj_goal.jpg" class="img-fluid"></p>
<p>In altre parole, dovremmo eseguire una classificazione multi-etichetta, in cui ogni immagine può avere tre classi:</p>
<ul>
<li><p>Background (nessun oggetto)</p></li>
<li><p>Box [Scatola]</p></li>
<li><p>Wheel [Ruota]</p></li>
</ul>
</section>
<section id="raccolta-dati-grezzi" class="level3">
<h3 class="anchored" data-anchor-id="raccolta-dati-grezzi">Raccolta Dati Grezzi</h3>
<p>Una volta definito l’obiettivo del nostro progetto di apprendimento automatico, il passaggio successivo, e più cruciale, è la raccolta del dataset. Possiamo utilizzare un telefono, il Raspi o un mix per creare il set di dati grezzi (senza etichette). Usiamo la semplice app Web sul nostro Raspberry Pi per visualizzare le immagini <code>QVGA (320 x 240)</code> catturate in un browser.</p>
<p>Da GitHub, si prende lo script Python <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/python_scripts/get_img_data.py">get_img_data.py</a> e lo si apre nel terminale:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> get_img_data.py</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Accedere all’interfaccia web:</p>
<ul>
<li>Sul Raspberry Pi stesso (se si ha una GUI): si apre un browser web e si va su<code>http://localhost:5000</code></li>
<li>Da un altro dispositivo sulla stessa rete: aprire un browser web e andare su <code>http://&lt;raspberry_pi_ip&gt;:5000</code> (Sostituire <code>&lt;raspberry_pi_ip&gt;</code> con l’indirizzo IP del Raspberry Pi). Per esempio: <code>http://192.168.4.210:5000/</code></li>
</ul>
<p><img src="images/png/app.png" class="img-fluid">Lo script Python crea un’interfaccia basata sul Web per catturare e organizzare set di dati di immagini utilizzando un Raspberry Pi e la sua fotocamera. È utile per progetti di apprendimento automatico che richiedono dati di immagini etichettati o meno, come nel nostro caso.</p>
<p>Accedere all’interfaccia Web da un browser, inserire un’etichetta generica per le immagini da catturare e premere <code>Start Capture</code>.</p>
<p><img src="images/png/cap-img.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Notare che le immagini da catturare avranno più etichette che dovranno essere definite in seguito.</p>
</blockquote>
<p>Utilizzare l’anteprima live per posizionare la fotocamera e cliccare su <code>Capture Image</code> per salvare le immagini sotto l’etichetta corrente (in questo caso, <code>box-wheel</code>.</p>
<p><img src="images/png/img_cap-1.png" class="img-fluid"></p>
<p>Quando abbiamo abbastanza immagini, possiamo premere <code>Stop Capture</code>. Le immagini catturate vengono salvate nella cartella dataset/box-wheel:</p>
<p><img src="images/png/dataset.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Acquisire circa 60 immagini. Provare ad acquisire con diverse angolazioni, sfondi e condizioni di luce. Filezilla può trasferire il dataset raw creato sul computer principale.</p>
</blockquote>
</section>
<section id="etichettatura-dei-dati" class="level3">
<h3 class="anchored" data-anchor-id="etichettatura-dei-dati">Etichettatura dei Dati</h3>
<p>Il passaggio successivo in un progetto di Object Detect è creare un dataset etichettato. Dovremmo etichettare le immagini del dataset raw, creando riquadri di delimitazione attorno agli oggetti di ogni immagine (box e ruota). Possiamo usare strumenti di etichettatura come <a href="https://pypi.org/project/labelImg/">LabelImg</a>, <a href="https://www.cvat.ai/">CVAT</a>, <a href="https://roboflow.com/annotate">Roboflow</a>, o persino <a href="https://edgeimpulse.com/">Edge Impulse Studio</a>. Dopo aver esplorato lo strumento Edge Impulse in altri laboratori, usiamo Roboflow qui.</p>
<blockquote class="blockquote">
<p>Stiamo usando Roboflow (versione gratuita) qui per due motivi principali. 1) Possiamo avere un auto-labeler e 2) Il dataset annotato è disponibile in diversi formati e può essere utilizzato sia su Edge Impulse Studio (lo useremo per MobileNet V2 e FOMO train) sia su CoLab (YOLOv8 train), ad esempio. Avendo il dataset annotato su Edge Impulse (account gratuito), non è possibile utilizzarlo per il training su altre piattaforme.</p>
</blockquote>
<p>Dovremmo caricare il dataset grezzo su <a href="https://roboflow.com/">Roboflow</a>. Creare un account gratuito lì e avviare un nuovo progetto, ad esempio (“box-versus-wheel”).</p>
<p><img src="images/png/create-project-rf.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Non entreremo nei dettagli del processo Roboflow dato che sono disponibili molti tutorial.</p>
</blockquote>
<section id="annotazione" class="level4">
<h4 class="anchored" data-anchor-id="annotazione">Annotazione</h4>
<p>Una volta creato il progetto e caricato il dataset, si devono effettuare le annotazioni utilizzando lo strumento “Auto-Label”. Notare che si può anche caricare immagini con solo uno sfondo, che dovrebbero essere salvate senza annotazioni.</p>
<p><img src="images/png/annotation.png" class="img-fluid"></p>
<p>Una volta annotate tutte le immagini, si devono dividere in training, validation e testing.</p>
<p><img src="images/png/dataset_rf.png" class="img-fluid"></p>
</section>
<section id="pre-elaborazione-dei-dati" class="level4">
<h4 class="anchored" data-anchor-id="pre-elaborazione-dei-dati">Pre-elaborazione dei Dati</h4>
<p>L’ultimo passaggio con il dataset è la pre-elaborazione per generare una versione finale per il training. Ridimensioniamo tutte le immagini a 320x320 e generiamo versioni aumentate di ogni immagine (augmentation) per creare nuovi esempi di training da cui il nostro modello può imparare.</p>
<p>Per l’augmentation, ruoteremo le immagini (+/-15<sup>o</sup>), ritaglieremo e varieremo la luminosità e l’esposizione.</p>
<p><img src="images/png/pre-proc.png" class="img-fluid"></p>
<p>Alla fine del processo avremo 153 immagini.</p>
<p><img src="images/png/final-dataset.png" class="img-fluid"></p>
<p>Ora si deve esportare il dataset annotato in un formato che Edge Impulse, Ultralitics e altri framework/strumenti possano comprendere, ad esempio <code>YOLOv8</code>. Scarichiamo una versione compressa del dataset sul nostro desktop.</p>
<p><img src="images/png/download-dataset.png" class="img-fluid"></p>
<p>Qui è possibile rivedere come è stato strutturato il dataset</p>
<p><img src="images/png/dataset-struct.png" class="img-fluid"></p>
<p>Ci sono 3 cartelle separate, una per ogni suddivisione (<code>train</code>/<code>test</code>/<code>valid</code>). Per ognuna di esse ci sono 2 sottocartelle, <code>images</code> e <code>labels</code>. Le immagini sono archiviate come <strong>image_id.jpg</strong> e <strong>images_id.txt</strong>, dove “image_id” è univoco per ogni immagine.</p>
<p>Il formato del file delle etichette sarà <code>class_id</code> <code>coordinate del riquadro di delimitazione</code>, dove nel nostro caso class_id sarà <code>0</code> per <code>box</code> e <code>1</code> per <code>wheel</code>. L’ID numerico (o, 1, 2…) seguirà l’ordine alfabetico del nome della classe.</p>
<p>Il file <code>data.yaml</code> contiene informazioni sul set di dati come i nomi delle classi (<code>names: ['box', 'wheel']</code>) seguendo il formato YOLO.</p>
<p>E questo è tutto! Siamo pronti per iniziare l’addestramento utilizzando Edge Impulse Studio (come faremo nel passaggio successivo), Ultralytics (come faremo quando discuteremo di YOLO) o persino l’addestramento da zero su CoLab (come abbiamo fatto col dataset Cifar-10 nel laboratorio di classificazione delle immagini).</p>
<blockquote class="blockquote">
<p>Il dataset pre-elaborato si trova sul <a href="https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset">sito Roboflow</a>, o qui:<a href="https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset"><img src="https://app.roboflow.com/images/download-dataset-badge.svg"> </a></p>
</blockquote>
</section>
</section>
</section>
<section id="addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio" class="level2">
<h2 class="anchored" data-anchor-id="addestramento-di-un-modello-ssd-mobilenet-su-edge-impulse-studio">Addestramento di un Modello SSD MobileNet su Edge Impulse Studio</h2>
<p>Si va su <a href="https://www.edgeimpulse.com/">Edge Impulse Studio</a>, si inseriscono le proprie credenziali in <strong>Login</strong> (o si crea un account) e si avvia un nuovo progetto.</p>
<blockquote class="blockquote">
<p>Qui, è possibile clonare il progetto sviluppato per questo laboratorio pratico: <a href="https://studio.edgeimpulse.com/public/515477/live">Raspi - Object Detection</a>.</p>
</blockquote>
<p>Nella scheda <code>Dashboard</code> del progetto, si va in basso e su <strong>Project info</strong> e per “Labeling method” si seleziona <code>Bounding boxes (object detection)</code></p>
<section id="caricamento-dei-dati-annotati" class="level3">
<h3 class="anchored" data-anchor-id="caricamento-dei-dati-annotati">Caricamento dei dati annotati</h3>
<p>Su Studio, si va alla scheda <code>Data acquisition</code> e nella sezione <code>UPLOAD DATA</code> si carica dal computer il set di dati non elaborato.</p>
<p>Possiamo usare l’opzione <code>Select a folder</code>, scegliendo, ad esempio, la cartella <code>train</code> nel computer, che contiene due sottocartelle, <code>images</code> e <code>labels</code>. Selezionare <code>Image label format</code>, “YOLO TXT”, caricare nella categoria <code>Training</code> e premere <code>Upload data</code>.</p>
<p><img src="images/png/upload-data.png" class="img-fluid"></p>
<p>Ripetere il processo per i dati di test (caricare entrambe le cartelle, test e convalida). Alla fine del processo di caricamento, si ottiene il set di dati annotato di 153 immagini suddivise in train/test (84%/16%).</p>
<blockquote class="blockquote">
<p>Notare che le etichette saranno archiviate nei file di etichette <code>0</code> e <code>1</code>, che sono equivalenti a <code>box</code> e <code>wheel</code>.</p>
</blockquote>
<p><img src="images/png/ei-dataset.png" class="img-fluid"></p>
</section>
<section id="impulse-design" class="level3">
<h3 class="anchored" data-anchor-id="impulse-design">Impulse Design</h3>
<p>La prima cosa da definire quando entriamo nella fase <code>Create impulse</code> è descrivere il dispositivo target per la distribuzione. Apparirà una finestra pop-up. Selezioneremo Raspberry 4, un dispositivo intermedio tra Raspi-Zero e Raspi-5.</p>
<blockquote class="blockquote">
<p>Questa scelta non interferirà con l’addestramento; ci darà solo un’idea della latenza del modello su quel target specifico.</p>
</blockquote>
<p><img src="images/png/target-device.png" class="img-fluid"></p>
<p>In questa fase, si deve definire come:</p>
<ul>
<li><p>Il <strong>pre-processing</strong> consiste nel ridimensionare le singole immagini. Nel nostro caso, le immagini sono state pre-elaborate su Roboflow, a <code>320x320</code>, quindi teniamole. La modifica delle dimensioni non avrà importanza qui perché le immagini sono già quadrate. Se si carica un’immagine rettangolare, la si deve schiacciare (forma quadrata, senza ritagliarla). In seguito, si potrebbe definire se le immagini vengono convertite da RGB a scala di grigi o meno.</p></li>
<li><p><strong>Design a Model</strong>, in questo caso, “Object Detection”.</p></li>
</ul>
<p><img src="images/png/impulse-design.png" class="img-fluid"></p>
</section>
<section id="pre-elaborazione-di-tutti-i-dataset" class="level3">
<h3 class="anchored" data-anchor-id="pre-elaborazione-di-tutti-i-dataset">Pre-elaborazione di tutti i dataset</h3>
<p>Nella sezione <code>Image</code>, selezionare <strong>Color depth</strong> come <code>RGB</code> e premere <code>Save parameters</code>.</p>
<p><img src="images/png/ei-image-pre.png" class="img-fluid"></p>
<p>Lo Studio passa automaticamente alla sezione successiva, <code>Generate features</code>, dove tutti i campioni verranno pre-elaborati, ottenendo 480 oggetti: 207 box e 273 ruote.</p>
<p><img src="images/png/ei-features.png" class="img-fluid"></p>
<p>L’esploratore di feature mostra che tutti i campioni evidenziano una buona separazione dopo la generazione delle feature.</p>
</section>
<section id="progettazione-addestramento-e-test-del-modello" class="level3">
<h3 class="anchored" data-anchor-id="progettazione-addestramento-e-test-del-modello">Progettazione, Addestramento e Test del Modello</h3>
<p>Per l’addestramento, dovremmo selezionare un modello pre-addestrato. Usiamo <strong>MobileNetV2 SSD FPN-Lite (solo 320x320)</strong> . È un modello di rilevamento oggetti pre-addestrato progettato per individuare fino a 10 oggetti all’interno di un’immagine, generando un riquadro di delimitazione per ogni oggetto rilevato. Il modello è di circa 3,7 MB. Supporta un input RGB a 320x320px.</p>
<p>Per quanto riguarda gli iperparametri di training, il modello verrà addestrato con:</p>
<ul>
<li>Epochs: 25</li>
<li>Batch size: 32</li>
<li>Learning Rate: 0.15.</li>
</ul>
<p>Per la convalida durante l’addestramento, il 20% del set di dati (<em>validation_dataset</em>) verrà risparmiato.</p>
<p><img src="images/png/ei-train-result.png" class="img-fluid"></p>
<p>Di conseguenza, il modello termina con un punteggio di precisione complessivo (basato su COCO mAP) dell’88,8%, superiore al risultato ottenuto utilizzando i dati di test (83,3%).</p>
</section>
<section id="distribuzione-del-modello" class="level3">
<h3 class="anchored" data-anchor-id="distribuzione-del-modello">Distribuzione del modello</h3>
<p>Abbiamo due modi per distribuire il modello:</p>
<ul>
<li><strong>Modello TFLite</strong>, che consente di distribuire il modello addestrato come <code>.tflite</code> affinché Raspi lo esegua tramite Python.</li>
<li><strong>Linux (AARCH64)</strong>, un binario per Linux (AARCH64), implementa il protocollo Edge Impulse Linux, che consente di eseguire i modelli su qualsiasi scheda di sviluppo basata su Linux, ad esempio con SDK per Python. Consulta la documentazione per maggiori informazioni e le <a href="https://docs.edgeimpulse.com/docs/edge-impulse-for-linux">istruzioni di configurazione</a>.</li>
</ul>
<p>Distribuiamo il <strong>modello TFLite</strong>. Nella scheda <code>Dashboard</code>, si va su Transfer learning model (int8 quantized) e si clicca sull’icona di download:</p>
<p><img src="images/png/ei-deploy-int8.png" class="img-fluid"></p>
<p>Trasferire il modello dal computer alla cartella Raspi <code>./models</code> e catturare o ottenere alcune immagini per l’inferenza e salvale nella cartella <code>./images</code>.</p>
</section>
<section id="inferenza-e-post-elaborazione-1" class="level3">
<h3 class="anchored" data-anchor-id="inferenza-e-post-elaborazione-1">Inferenza e Post-Elaborazione</h3>
<p>L’inferenza può essere fatta come discusso nella <em>Panoramica dei Modelli di Rilevamento degli Oggetti Pre-Addestrati</em>. Iniziamo un nuovo <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-SSD-MobileNetV2.ipynb">notebook</a> per seguire tutti i passaggi per rilevare cubi e ruote su un’immagine.</p>
<p>Importare le librerie necessarie:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tflite_runtime.interpreter <span class="im">as</span> tflite</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Definire il percorso del modello e delle etichette:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"./models/ei-raspi-object-detection-SSD-MobileNetv2-320x0320-</span><span class="ch">\</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="st">int8.lite"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'box'</span>, <span class="st">'wheel'</span>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Ricordare che il modello restituirà l’ID della classe come valori (0 e 1), seguendo un ordine alfabetico in base ai nomi delle classi.</p>
</blockquote>
<p>Caricare il modello, allocare i tensori e ottenere i dettagli dei tensori di input e output:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the TFLite model</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>interpreter <span class="op">=</span> tflite.Interpreter(model_path<span class="op">=</span>model_path)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>interpreter.allocate_tensors()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get input and output tensors</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>input_details <span class="op">=</span> interpreter.get_input_details()</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>output_details <span class="op">=</span> interpreter.get_output_details()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Una differenza fondamentale da notare è che il <code>dtype</code> dei dettagli di input del modello è ora <code>int8</code>, il che significa che i valori di input vanno da -128 a +127, mentre ogni pixel della nostra immagine raw va da 0 a 256. Ciò significa che dovremmo pre-elaborare l’immagine per farla corrispondere. Possiamo controllare qui:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>input_dtype <span class="op">=</span> input_details[<span class="dv">0</span>][<span class="st">'dtype'</span>]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>input_dtype</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>numpy.int8</code></pre>
<p>Quindi, apriamo l’immagine e mostriamola:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"./images/box_2_wheel_2.jpg"</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>orig_img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(orig_img)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/orig-img.png" class="img-fluid"></p>
<p>Ed eseguiamo la pre-elaborazione:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>scale, zero_point <span class="op">=</span> input_details[<span class="dv">0</span>][<span class="st">'quantization'</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> orig_img.resize((input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">1</span>], </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                  input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">2</span>]))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>img_array <span class="op">=</span> np.array(img, dtype<span class="op">=</span>np.float32) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>img_array <span class="op">=</span> (img_array <span class="op">/</span> scale <span class="op">+</span> zero_point).clip(<span class="op">-</span><span class="dv">128</span>, <span class="dv">127</span>).astype(np.int8)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>input_data <span class="op">=</span> np.expand_dims(img_array, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Controllando i dati di input, possiamo verificare che il tensore di input è compatibile con quanto previsto dal modello:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>input_data.shape, input_data.dtype</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>((1, 320, 320, 3), dtype('int8'))</code></pre>
<p>Adesso è il momento di effettuare l’inferenza. Calcoliamo anche la latenza del modello:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inference on Raspi-Zero</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>interpreter.set_tensor(input_details[<span class="dv">0</span>][<span class="st">'index'</span>], input_data)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>interpreter.invoke()</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> (end_time <span class="op">-</span> start_time) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Convert to milliseconds</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"Inference time: </span><span class="sc">{:.1f}</span><span class="st">ms"</span>.<span class="bu">format</span>(inference_time))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Il modello impiegherà circa 600 ms per eseguire l’inferenza nel Raspi-Zero, che è circa 5 volte più lungo di un Raspi-5.</p>
<p>Ora possiamo ottenere le classi di output degli oggetti rilevati, le coordinate dei suoi bounding box e le probabilità.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>boxes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">1</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">3</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">0</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]        </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>num_detections <span class="op">=</span> <span class="bu">int</span>(interpreter.get_tensor(output_details[<span class="dv">2</span>][<span class="st">'index'</span>])[<span class="dv">0</span>])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_detections):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> scores[i] <span class="op">&gt;</span> <span class="fl">0.5</span>:  <span class="co"># Confidence threshold</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Object </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Bounding Box: </span><span class="sc">{</span>boxes[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Confidence: </span><span class="sc">{</span>scores[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Class: </span><span class="sc">{</span>classes[i]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/infer-text.png" class="img-fluid"></p>
<p>Dai risultati, possiamo vedere che sono stati rilevati 4 oggetti: due con ID classe 0 (<code>box</code>) e due con ID classe 1 (<code>wheel</code>), cosa è corretto!</p>
<p>Visualizziamo il risultato per un <code>threshold</code> [soglia] di 0,5</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>threshold <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(orig_img)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_detections):</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> scores[i] <span class="op">&gt;</span> threshold:  </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        ymin, xmin, ymax, xmax <span class="op">=</span> boxes[i]</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        (left, right, top, bottom) <span class="op">=</span> (xmin <span class="op">*</span> orig_img.width, </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>                                      xmax <span class="op">*</span> orig_img.width, </span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>                                      ymin <span class="op">*</span> orig_img.height, </span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>                                      ymax <span class="op">*</span> orig_img.height)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        rect <span class="op">=</span> plt.Rectangle((left, top), right<span class="op">-</span>left, bottom<span class="op">-</span>top, </span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>                             fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        plt.gca().add_patch(rect)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        class_id <span class="op">=</span> <span class="bu">int</span>(classes[i])</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> labels[class_id]</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        plt.text(left, top<span class="op">-</span><span class="dv">10</span>, <span class="ss">f'</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>scores[i]<span class="sc">:.2f}</span><span class="ss">'</span>, </span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'red'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, backgroundcolor<span class="op">=</span><span class="st">'white'</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/infer-visual.png" class="img-fluid"></p>
<p>Ma cosa succede se riduciamo la soglia a 0.3, ad esempio?</p>
<p><img src="images/png/infer-mult.png" class="img-fluid"></p>
<p>Cominciamo a vedere falsi positivi e <strong>rilevazioni multiple</strong>, in cui il modello rileva lo stesso oggetto più volte con diversi livelli di confidenza e bounding box leggermente diversi.</p>
<p>Di solito, a volte, dobbiamo regolare la soglia su valori più piccoli per catturare tutti gli oggetti, evitando falsi negativi, che porterebbero a rilevazioni multiple.</p>
<p>Per migliorare i risultati di rilevamento, dovremmo implementare <strong>Non-Maximum Suppression (NMS)</strong>, che aiuta a eliminare le bounding box sovrapposte e mantiene solo il rilevamento più affidabile.</p>
<p>Per questo, creiamo una funzione generale denominata <code>non_max_suppression()</code>, con il ruolo di perfezionare i risultati di rilevamento degli oggetti eliminando le bounding box ridondanti e sovrapposte. Ciò è possibile selezionando in modo iterativo la rilevazione con il punteggio di confidenza più elevato e rimuovendo altre rilevazioni significativamente sovrapposte in base a una soglia di “Intersection over Union (IoU)” [intersezione su unione].</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> non_max_suppression(boxes, scores, threshold):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to corner coordinates</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> boxes[:, <span class="dv">0</span>]</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    y1 <span class="op">=</span> boxes[:, <span class="dv">1</span>]</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> boxes[:, <span class="dv">2</span>]</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    y2 <span class="op">=</span> boxes[:, <span class="dv">3</span>]</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    areas <span class="op">=</span> (x2 <span class="op">-</span> x1 <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> (y2 <span class="op">-</span> y1 <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    order <span class="op">=</span> scores.argsort()[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    keep <span class="op">=</span> []</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> order.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> order[<span class="dv">0</span>]</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        keep.append(i)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        xx1 <span class="op">=</span> np.maximum(x1[i], x1[order[<span class="dv">1</span>:]])</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        yy1 <span class="op">=</span> np.maximum(y1[i], y1[order[<span class="dv">1</span>:]])</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        xx2 <span class="op">=</span> np.minimum(x2[i], x2[order[<span class="dv">1</span>:]])</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        yy2 <span class="op">=</span> np.minimum(y2[i], y2[order[<span class="dv">1</span>:]])</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> np.maximum(<span class="fl">0.0</span>, xx2 <span class="op">-</span> xx1 <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> np.maximum(<span class="fl">0.0</span>, yy2 <span class="op">-</span> yy1 <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        inter <span class="op">=</span> w <span class="op">*</span> h</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        ovr <span class="op">=</span> inter <span class="op">/</span> (areas[i] <span class="op">+</span> areas[order[<span class="dv">1</span>:]] <span class="op">-</span> inter)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        inds <span class="op">=</span> np.where(ovr <span class="op">&lt;=</span> threshold)[<span class="dv">0</span>]</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>        order <span class="op">=</span> order[inds <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> keep</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Come funziona:</p>
<ol type="1">
<li><p>Sorting: Inizia ordinando tutte le rilevazioni in base ai loro punteggi di confidenza, dal più alto al più basso.</p></li>
<li><p>Selection: Seleziona la casella con il punteggio più alto e la aggiunge all’elenco finale delle rilevazioni.</p></li>
<li><p>Comparison: Questa casella selezionata viene confrontata con tutte le restanti caselle con punteggio più basso.</p></li>
<li><p>Elimination: Qualsiasi casella che si sovrapponga in modo significativo (oltre la soglia IoU) alla casella selezionata viene eliminata.</p></li>
<li><p>Iteration: Questo processo si ripete con la casella con il punteggio più alto successivo finché tutte le caselle non vengono elaborate.</p></li>
</ol>
<p>Ora, possiamo definire una funzione di visualizzazione più precisa che prenderà in considerazione una soglia IoU, rilevando solo gli oggetti selezionati dalla funzione <code>non_max_suppression</code>:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_detections(image, boxes, classes, scores, </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>                         labels, threshold, iou_threshold):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(image, Image.Image):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        image_np <span class="op">=</span> np.array(image)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        image_np <span class="op">=</span> image</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    height, width <span class="op">=</span> image_np.shape[:<span class="dv">2</span>]</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert normalized coordinates to pixel coordinates</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    boxes_pixel <span class="op">=</span> boxes <span class="op">*</span> np.array([height, width, height, width])</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply NMS</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    keep <span class="op">=</span> non_max_suppression(boxes_pixel, scores, iou_threshold)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the figure size to 12x8 inches</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    ax.imshow(image_np)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> keep:</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[i] <span class="op">&gt;</span> threshold:</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>            ymin, xmin, ymax, xmax <span class="op">=</span> boxes[i]</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>            rect <span class="op">=</span> patches.Rectangle((xmin <span class="op">*</span> width, ymin <span class="op">*</span> height),</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>                                     (xmax <span class="op">-</span> xmin) <span class="op">*</span> width,</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>                                     (ymax <span class="op">-</span> ymin) <span class="op">*</span> height,</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>                                     linewidth<span class="op">=</span><span class="dv">2</span>, edgecolor<span class="op">=</span><span class="st">'r'</span>, facecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>            ax.add_patch(rect)</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>            class_name <span class="op">=</span> labels[<span class="bu">int</span>(classes[i])]</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>            ax.text(xmin <span class="op">*</span> width, ymin <span class="op">*</span> height <span class="op">-</span> <span class="dv">10</span>,</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>                    <span class="ss">f'</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>scores[i]<span class="sc">:.2f}</span><span class="ss">'</span>, color<span class="op">=</span><span class="st">'red'</span>,</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>                    fontsize<span class="op">=</span><span class="dv">12</span>, backgroundcolor<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ora possiamo creare una funzione che chiamerà le altre, eseguendo l’inferenza su qualsiasi immagine:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detect_objects(img_path, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    orig_img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    scale, zero_point <span class="op">=</span> input_details[<span class="dv">0</span>][<span class="st">'quantization'</span>]</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> orig_img.resize((input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">1</span>], </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>                      input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">2</span>]))</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    img_array <span class="op">=</span> np.array(img, dtype<span class="op">=</span>np.float32) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    img_array <span class="op">=</span> (img_array <span class="op">/</span> scale <span class="op">+</span> zero_point).clip(<span class="op">-</span><span class="dv">128</span>, <span class="dv">127</span>).<span class="op">\</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    astype(np.int8)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    input_data <span class="op">=</span> np.expand_dims(img_array, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference on Raspi-Zero</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    interpreter.set_tensor(input_details[<span class="dv">0</span>][<span class="st">'index'</span>], input_data)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    interpreter.invoke()</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    inference_time <span class="op">=</span> (end_time <span class="op">-</span> start_time) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Convert to ms</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">"Inference time: </span><span class="sc">{:.1f}</span><span class="st">ms"</span>.<span class="bu">format</span>(inference_time))</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the outputs</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    boxes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">1</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    classes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">3</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">0</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]        </span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    num_detections <span class="op">=</span> <span class="bu">int</span>(interpreter.get_tensor(output_details[<span class="dv">2</span>][<span class="st">'index'</span>])[<span class="dv">0</span>])</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    visualize_detections(orig_img, boxes, classes, scores, labels, </span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>                         threshold<span class="op">=</span>conf, </span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>                         iou_threshold<span class="op">=</span>iou)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ora, eseguendo il codice, ottenendo di nuovo la stessa immagine con una soglia di confidenza di 0.3, ma con un piccolo IoU:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"./images/box_2_wheel_2.jpg"</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>detect_objects(img_path, conf<span class="op">=</span><span class="fl">0.3</span>,iou<span class="op">=</span><span class="fl">0.05</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/infer-iou.png" class="img-fluid"></p>
</section>
</section>
<section id="training-di-un-modello-fomo-su-edge-impulse-studio" class="level2">
<h2 class="anchored" data-anchor-id="training-di-un-modello-fomo-su-edge-impulse-studio">Training di un modello FOMO su Edge Impulse Studio</h2>
<p>L’inferenza con il modello SSD MobileNet ha funzionato bene, ma la latenza era significativamente alta. L’inferenza variava da 0.5 a 1.3 secondi su un Raspi-Zero, il che significa circa o meno di 1 FPS (1 frame al secondo). Un’alternativa per accelerare il processo è usare FOMO (Faster Objects, More Objects).</p>
<p>Questo nuovo algoritmo di apprendimento automatico consente di contare più oggetti e di trovare la loro posizione in un’immagine in tempo reale utilizzando fino a 30 volte meno potenza di elaborazione e memoria rispetto a MobileNet SSD o YOLO. Il motivo principale per cui ciò è possibile è che mentre altri modelli calcolano le dimensioni dell’oggetto disegnando un quadrato attorno ad esso (bounding box), FOMO ignora le dimensioni dell’immagine, fornendo solo le informazioni sulla posizione dell’oggetto nell’immagine tramite le sue coordinate del centroide.</p>
<section id="come-funziona-fomo" class="level3">
<h3 class="anchored" data-anchor-id="come-funziona-fomo">Come funziona FOMO?</h3>
<p>In una tipica pipeline di rilevamento degli oggetti, la prima fase consiste nell’estrarre le feature dall’immagine di input. <strong>FOMO sfrutta MobileNetV2 per eseguire questa attività</strong>. MobileNetV2 elabora l’immagine di input per produrre una feature map che cattura le caratteristiche essenziali, come texture, forme e bordi degli oggetti, in modo computazionalmente efficiente.</p>
<p><img src="images/png/fomo-1.png" class="img-fluid"></p>
<p>Una volta estratte queste feature, l’architettura più semplice di FOMO, focalizzata sul rilevamento del punto centrale, interpreta la feature map per determinare dove si trovano gli oggetti nell’immagine. L’output è una griglia di celle, in cui ogni cella rappresenta se è stato rilevato o meno un centro dell’oggetto. Il modello restituisce uno o più punteggi di confidenza per ogni cella, indicando la probabilità che un oggetto sia presente.</p>
<p>Vediamo come funziona su un’immagine.</p>
<p>FOMO divide l’immagine in blocchi di pixel usando un fattore di 8. Per l’input di 96x96, la griglia è 12x12 (96/8=12). Per un 160x160, la griglia sarà 20x20 e così via. Successivamente, FOMO eseguirà un classificatore attraverso ogni blocco di pixel per calcolare la probabilità che ci sia un box o una ruota in ognuno di essi e, successivamente, determinerà le regioni che hanno la più alta probabilità di contenere l’oggetto (se un blocco di pixel non ha oggetti, verrà classificato come <em>background</em>). Dalla sovrapposizione della regione finale, FOMO fornisce le coordinate (relative alle dimensioni dell’immagine) del centroide di questa regione.</p>
<p><img src="images/png/fomo-works.png" class="img-fluid"></p>
<p><strong>Compromesso Tra Velocità e Precisione</strong>:</p>
<ul>
<li><strong>Risoluzione della Griglia</strong>: FOMO utilizza una griglia di risoluzione fissa, il che significa che ogni cella può rilevare se un oggetto è presente in quella parte dell’immagine. Sebbene non fornisca un’elevata precisione di localizzazione, fa un compromesso essendo veloce e computazionalmente leggero, il che è fondamentale per i dispositivi edge.</li>
<li><strong>Rilevamento Multi-Oggetto</strong>: Poiché ogni cella è indipendente, FOMO può rilevare più oggetti contemporaneamente in un’immagine identificando più centri.</li>
</ul>
</section>
<section id="impulse-design-nuovo-training-e-test" class="level3">
<h3 class="anchored" data-anchor-id="impulse-design-nuovo-training-e-test">Impulse Design, nuovo Training e Test</h3>
<p>Tornare a Edge Impulse Studio e nella scheda <code>Experiments</code>, creare un altro impulso. Ora, le immagini di input dovrebbero essere 160x160 (questa è la dimensione di input prevista per MobilenetV2).</p>
<p><img src="images/png/impulse-2.png" class="img-fluid"></p>
<p>Nella scheda <code>Image</code>, generare le feature e spostarsi nella scheda <code>Object detection</code>.</p>
<p>Dovremmo selezionare un modello pre-addestrato per l’addestramento. Usiamo <strong>FOMO (Faster Objects, More Objects) MobileNetV2 0.35.</strong></p>
<p><img src="images/png/model-choice.png" class="img-fluid"></p>
<p>Per quanto riguarda gli iperparametri di training, il modello verrà addestrato con:</p>
<ul>
<li>Epochs: 30</li>
<li>Batch size: 32</li>
<li>Learning Rate: 0.001.</li>
</ul>
<p>Per la convalida durante l’addestramento, il 20% del set di dati (<em>validation_dataset</em>) verrà risparmiato. Non applicheremo il “Data Augmentation” per il restante 80% (<em>train_dataset</em>) perché il dataset è già stato aumentato durante la fase di etichettatura su Roboflow.</p>
<p>Di conseguenza, il modello termina con un punteggio F1 complessivo del 93,3% con una latenza impressionante di 8 ms (Raspi-4), circa 60 volte inferiore a quella ottenuta con SSD MovileNetV2.</p>
<p><img src="images/png/fomo-train-result.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Notare che FOMO ha aggiunto automaticamente una terza etichetta di background ai due <em>box</em> (0) e <em>ruote</em> (1) definite in precedenza.</p>
</blockquote>
<p>Nella scheda <code>Model testing</code>, possiamo vedere che l’accuratezza era del 94%. Ecco uno dei risultati del campione di test:</p>
<p><img src="images/png/fomo-test.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Nelle attività di rilevamento di oggetti, l’accuratezza non è in genere la <a href="https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/">metrica di valutazione primaria</a>. Il rilevamento di oggetti comporta la classificazione degli oggetti e la definizione di riquadri di delimitazione attorno a essi, il che lo rende un problema più complesso della semplice classificazione. Il problema è che non abbiamo il riquadro di delimitazione, solo i centroidi. In breve, usare l’accuratezza come metrica potrebbe essere fuorviante e potrebbe non fornire una comprensione completa delle prestazioni del modello.</p>
</blockquote>
</section>
<section id="distribuzione-del-modello-1" class="level3">
<h3 class="anchored" data-anchor-id="distribuzione-del-modello-1">Distribuzione del modello</h3>
<p>Come abbiamo fatto nella sezione precedente, possiamo distribuire il modello addestrato come TFLite o Linux (AARCH64). Ora facciamolo come <strong>Linux (AARCH64)</strong>, un binario che implementa il protocollo <a href="https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux">Edge Impulse Linux</a>.</p>
<p>Edge Impulse per i modelli Linux viene fornito in formato <code>.eim</code>. Questo <a href="https://docs.edgeimpulse.com/docs/run-inference/linux-eim-executable">eseguibile</a> contiene il nostro “impulso completo” creato in Edge Impulse Studio. L’impulso è costituito dai blocchi di elaborazione del segnale e da qualsiasi blocco di apprendimento e anomalia che abbiamo aggiunto e addestrato. È compilato con ottimizzazioni per il nostro processore o GPU (ad esempio, istruzioni NEON su core ARM), più un semplice layer IPC (su un socket Unix).</p>
<p>Nella scheda <code>Deploy</code>, selezionare l’opzione <code>Linux (AARCH64)</code>, il modello <code>int8</code> e premere <code>Build</code>.</p>
<p><img src="images/png/deploy-linux.png" class="img-fluid"></p>
<p>Il modello verrà scaricato automaticamente sul computer.</p>
<p>Sul nostro Raspi, creiamo una nuova area di lavoro:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ~</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> EI_Linux</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> EI_Linux</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> models</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> images</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Rinominare il modello per facilitarne l’identificazione:</p>
<p>Ad esempio, <code>raspi-object-detection-linux-aarch64-FOMO-int8.eim</code> e trasferirlo nella nuova cartella Raspi <code>./models</code> e catturare o ottenere alcune immagini per l’inferenza e salvarle nella cartella <code>./images</code>.</p>
</section>
<section id="inferenza-e-post-elaborazione-2" class="level3">
<h3 class="anchored" data-anchor-id="inferenza-e-post-elaborazione-2">Inferenza e Post-Elaborazione</h3>
<p>L’inferenza verrà effettuata utilizzando <a href="https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux/linux-python-sdk">Linux Python SDK</a>. Questa libreria consente di eseguire modelli di apprendimento automatico e raccogliere dati dei sensori su macchine <a href="https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux">Linux</a> utilizzando Python. L’SDK è open source e ospitato su GitHub: <a href="https://github.com/edgeimpulse/linux-sdk-python">edgeimpulse/linux-sdk-python</a>.</p>
<p>Impostiamo un ambiente virtuale per lavorare con Linux Python SDK</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv ~/eilinux</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/eilinux/bin/activate</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>E installare tutte le librerie necessarie:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get update</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get install libatlas-base-dev libportaudio0 libportaudio2</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get installlibportaudiocpp0 portaudio19-dev</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install edge_impulse_linux <span class="at">-i</span> https://pypi.python.org/simple</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install Pillow matplotlib pyaudio opencv-contrib-python</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get install portaudio19-dev</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install pyaudio </span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install opencv-contrib-python</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Consentire al modello di essere eseguibile.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chmod</span> +x raspi-object-detection-linux-aarch64-FOMO-int8.eim</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Installare Jupiter Notebook nel nuovo ambiente</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install jupyter</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Eseguire un notebook in locale (su Raspi-4 o 5 con desktop)</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>o sul browser del computer:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--ip</span><span class="op">=</span>192.168.4.210 <span class="at">--no-browser</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Avviamo un nuovo <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-Linux-FOMO.ipynb">notebook</a> seguendo tutti i passaggi per rilevare cubi e ruote su un’immagine utilizzando il modello FOMO e l’SDK Python di Edge Impulse Linux.</p>
<p>Importare le librerie necessarie:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys, time</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> edge_impulse_linux.image <span class="im">import</span> ImageImpulseRunner</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Definire il percorso del modello e delle etichette:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>model_file <span class="op">=</span> <span class="st">"raspi-object-detection-linux-aarch64-int8.eim"</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"models/"</span><span class="op">+</span> model_file <span class="co"># Trained ML model from Edge Impulse</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'box'</span>, <span class="st">'wheel'</span>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Ricordare che il modello restituirà l’ID della classe come valori (0 e 1), seguendo un ordine alfabetico in base ai nomi delle classi.</p>
</blockquote>
<p>Caricare e inizializzare il modello:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model file</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>runner <span class="op">=</span> ImageImpulseRunner(model_path)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>model_info <span class="op">=</span> runner.init()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Il <code>model_info</code> conterrà informazioni critiche sul modello. Tuttavia, a differenza dell’interprete TFLite, la libreria EI Linux Python SDK preparerà ora il modello per l’inferenza.</p>
<p>Quindi, apriamo l’immagine e mostriamola (ora, per compatibilità, useremo OpenCV, la libreria CV utilizzata internamente da EI. OpenCV legge l’immagine come BGR, quindi dovremo convertirla in RGB:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"./images/1_box_1_wheel.jpg"</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>orig_img <span class="op">=</span> cv2.imread(img_path)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>img_rgb <span class="op">=</span> cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_rgb)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/orig-fomo-img.png" class="img-fluid"></p>
<p>Ora otterremo le feature e l’immagine preelaborata (<code>ritagliata</code>) utilizzando <code>runner</code>:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>features, cropped <span class="op">=</span> runner.get_features_from_image_auto_studio_setings(img_rgb)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ed eseguiamo l’inferenza. Calcoliamo anche la latenza del modello:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> runner.classify(features)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Otteniamo le classi di output degli oggetti rilevati, i loro centroidi dei bounding box e le probabilità.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Found </span><span class="sc">%d</span><span class="st"> bounding boxes (</span><span class="sc">%d</span><span class="st"> ms.)'</span> <span class="op">%</span> (</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">len</span>(res[<span class="st">"result"</span>][<span class="st">"bounding_boxes"</span>]), </span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  res[<span class="st">'timing'</span>][<span class="st">'dsp'</span>] <span class="op">+</span> res[<span class="st">'timing'</span>][<span class="st">'classification'</span>]))</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bb <span class="kw">in</span> res[<span class="st">"result"</span>][<span class="st">"bounding_boxes"</span>]:</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="sc">%s</span><span class="st"> (</span><span class="sc">%.2f</span><span class="st">): x=</span><span class="sc">%d</span><span class="st"> y=</span><span class="sc">%d</span><span class="st"> w=</span><span class="sc">%d</span><span class="st"> h=</span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> (</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>      bb[<span class="st">'label'</span>], bb[<span class="st">'value'</span>], bb[<span class="st">'x'</span>], </span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>      bb[<span class="st">'y'</span>], bb[<span class="st">'width'</span>], bb[<span class="st">'height'</span>]))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Found 2 bounding boxes (29 ms.)
    1 (0.91): x=112 y=40 w=16 h=16
    0 (0.75): x=48 y=56 w=8 h=8</code></pre>
<p>I risultati mostrano che sono stati rilevati due oggetti: uno con ID classe 0 (<code>box</code>) e uno con ID classe 1 (<code>wheel</code>), il che è corretto!</p>
<p>Visualizziamo il risultato (Il <code>threshold</code> [soglia] è 0.5, il valore predefinito impostato durante il test del modello su Edge Impulse Studio).</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="st">Found </span><span class="sc">%d</span><span class="st"> bounding boxes (latency: </span><span class="sc">%d</span><span class="st"> ms)'</span> <span class="op">%</span> (</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">len</span>(res[<span class="st">"result"</span>][<span class="st">"bounding_boxes"</span>]), </span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  res[<span class="st">'timing'</span>][<span class="st">'dsp'</span>] <span class="op">+</span> res[<span class="st">'timing'</span>][<span class="st">'classification'</span>]))</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(cropped)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Go through each of the returned bounding boxes</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>bboxes <span class="op">=</span> res[<span class="st">'result'</span>][<span class="st">'bounding_boxes'</span>]</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bbox <span class="kw">in</span> bboxes:</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the corners of the bounding box</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    left <span class="op">=</span> bbox[<span class="st">'x'</span>]</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    top <span class="op">=</span> bbox[<span class="st">'y'</span>]</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    width <span class="op">=</span> bbox[<span class="st">'width'</span>]</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    height <span class="op">=</span> bbox[<span class="st">'height'</span>]</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw a circle centered on the detection</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>    circ <span class="op">=</span> plt.Circle((left<span class="op">+</span>width<span class="op">//</span><span class="dv">2</span>, top<span class="op">+</span>height<span class="op">//</span><span class="dv">2</span>), <span class="dv">5</span>, </span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>                     fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    plt.gca().add_patch(circ)</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    class_id <span class="op">=</span> <span class="bu">int</span>(bbox[<span class="st">'label'</span>])</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    class_name <span class="op">=</span> labels[class_id]</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    plt.text(left, top<span class="op">-</span><span class="dv">10</span>, <span class="ss">f'</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>bbox[<span class="st">"value"</span>]<span class="sc">:.2f}</span><span class="ss">'</span>, </span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>              color<span class="op">=</span><span class="st">'red'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, backgroundcolor<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/infer-fomo-result.png" class="img-fluid"></p>
</section>
</section>
<section id="esplorazione-di-un-modello-yolo-tramite-ultralitics" class="level2">
<h2 class="anchored" data-anchor-id="esplorazione-di-un-modello-yolo-tramite-ultralitics">Esplorazione di un Modello YOLO tramite Ultralitics</h2>
<p>Per questo laboratorio, esploreremo YOLOv8. <a href="https://ultralytics.com/">Ultralytics</a> <a href="https://github.com/ultralytics/ultralytics">YOLOv8</a> è una versione dell’acclamato modello di rilevamento di oggetti in tempo reale e segmentazione delle immagini, YOLO. YOLOv8 è basato su progressi all’avanguardia nel deep learning e nella visione artificiale, offrendo prestazioni senza pari in termini di velocità e precisione. Il suo design semplificato lo rende adatto a varie applicazioni e facilmente adattabile a diverse piattaforme hardware, dai dispositivi edge alle API cloud.</p>
<section id="a-proposito-del-modello-yolo" class="level3">
<h3 class="anchored" data-anchor-id="a-proposito-del-modello-yolo">A proposito del modello YOLO</h3>
<p>Il modello YOLO (You Only Look Once) è un algoritmo di rilevamento di oggetti altamente efficiente e ampiamente utilizzato, noto per le sue capacità di elaborazione in tempo reale. A differenza dei tradizionali sistemi di rilevamento di oggetti che riutilizzano classificatori o localizzatori per eseguire il rilevamento, YOLO inquadra il problema del rilevamento come un singolo compito di regressione. Questo approccio innovativo consente a YOLO di prevedere simultaneamente più bounding box e le relative probabilità di classe da immagini complete in un’unica valutazione, aumentando notevolmente la sua velocità.</p>
<section id="caratteristiche-principali" class="level4">
<h4 class="anchored" data-anchor-id="caratteristiche-principali">Caratteristiche Principali:</h4>
<ol type="1">
<li><p><strong>Architettura “ingle Network”</strong>:</p>
<ul>
<li>YOLO impiega una singola rete neurale per elaborare l’intera immagine. Questa rete divide l’immagine in una griglia e, per ogni cella della griglia, prevede direttamente i bounding box e le relative probabilità di classe. Questa formazione end-to-end migliora la velocità e semplifica l’architettura del modello.</li>
</ul></li>
<li><p><strong>Elaborazione in Tempo Reale</strong>:</p>
<ul>
<li>Una delle caratteristiche distintive di YOLO è la sua capacità di eseguire il rilevamento di oggetti in tempo reale. A seconda della versione e dell’hardware, YOLO può elaborare immagini con frame al secondo (FPS) elevati. Ciò lo rende ideale per applicazioni che richiedono un rilevamento di oggetti rapido e accurato, come videosorveglianza, guida autonoma e analisi di eventi sportivi in diretta.</li>
</ul></li>
<li><p><strong>Evoluzione delle Versioni</strong>:</p>
<ul>
<li>Nel corso degli anni, YOLO ha subito miglioramenti significativi, da YOLOv1 all’ultimo YOLOv10. Ogni iterazione ha introdotto miglioramenti in termini di accuratezza, velocità ed efficienza. YOLOv8, ad esempio, incorpora progressi nell’architettura di rete, metodologie di training migliorate e un supporto migliore per vari hardware, garantendo prestazioni più robuste.</li>
<li>Sebbene YOLOv10 sia il membro più recente della famiglia con prestazioni incoraggianti in base alla sua documentazione, è stato appena rilasciato (maggio 2024) e non è completamente integrato con la libreria Ultralitycs. Al contrario, l’analisi della curva di precisione-richiamo suggerisce che YOLOv8 generalmente supera YOLOv9, catturando una percentuale maggiore di veri positivi e riducendo al minimo i falsi positivi in modo più efficace (per maggiori dettagli, vedere questo <a href="https://encord.com/blog/performanceyolov9-vs-yolov8-custom-dataset/">articolo</a>). Quindi, questo laboratorio si basa su YOLOv8n.</li>
</ul>
<p><img src="images/jpeg/versions.jpg" class="img-fluid"></p></li>
<li><p><strong>Precisione ed Efficienza</strong>:</p>
<ul>
<li>Mentre le prime versioni di YOLO barattavano un po’ di precisione per la velocità, le versioni recenti hanno fatto notevoli progressi nell’equilibrare entrambi. I modelli più recenti sono più veloci e precisi, rilevano piccoli oggetti (come le api) e funzionano bene su set di dati complessi.</li>
</ul></li>
<li><p><strong>Ampia Gamma di Applicazioni</strong>:</p>
<ul>
<li>La versatilità di YOLO ha portato alla sua adozione in numerosi campi. Viene utilizzato nei sistemi di monitoraggio del traffico per rilevare e contare i veicoli, nelle applicazioni di sicurezza per identificare potenziali minacce e nella tecnologia agricola per monitorare raccolti e bestiame. La sua applicazione si estende a qualsiasi dominio che richieda un rilevamento efficiente e accurato degli oggetti.</li>
</ul></li>
<li><p><strong>Comunità e Sviluppo</strong>:</p>
<ul>
<li>YOLO continua a evolversi ed è supportato da una solida comunità di sviluppatori e ricercatori (essendo YOLOv8 molto forte). Le implementazioni open source e l’ampia documentazione lo hanno reso accessibile per la personalizzazione e l’integrazione in vari progetti. Framework di deep learning popolari come Darknet, TensorFlow e PyTorch supportano YOLO, ampliandone ulteriormente l’applicabilità.</li>
<li><a href="https://github.com/ultralytics/ultralytics?tab=readme-ov-file">Ultralytics YOLOv8</a> non solo può <a href="https://docs.ultralytics.com/tasks/detect">Detect</a> [rilevare] (il nostro caso qui) ma anche <a href="https://docs.ultralytics.com/tasks/segment">Segment</a>are e mettere in posa con <a href="https://docs.ultralytics.com/tasks/pose">Pose</a> modelli pre-addestrati sul set di dati <a href="https://docs.ultralytics.com/datasets/detect/coco">COCO</a> e YOLOv8 <a href="https://docs.ultralytics.com/tasks/classify">Classifica</a> modelli pre-addestrati sul set di dati <a href="https://docs.ultralytics.com/datasets/classify/imagenet">ImageNet</a>. La modalità <a href="https://docs.ultralytics.com/modes/track">Track</a> è disponibile per tutti i modelli Detect, Segment e Pose.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png" class="img-fluid figure-img"></p>
<figcaption>Attività supportate da Ultralytics YOLO</figcaption>
</figure>
</div></li>
</ol>
</section>
</section>
<section id="installazione" class="level3">
<h3 class="anchored" data-anchor-id="installazione">Installazione</h3>
<p>Sul nostro Raspi, disattiviamo l’ambiente corrente per creare una nuova area di lavoro:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ~</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> YOLO</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> YOLO</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> models</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> images</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Impostiamo un Virtual Environment per lavorare con Ultralytics YOLOv8</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv ~/yolo</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/yolo/bin/activate</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>E installiamo i pacchetti Ultralytics per l’inferenza locale sul Raspi</p>
<ol type="1">
<li>Aggiorniamo l’elenco dei pacchetti, installiamo pip ed eseguiamo l’aggiornamento all’ultima versione:</li>
</ol>
<div class="sourceCode" id="cb48"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt update</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install python3-pip <span class="at">-y</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> pip</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li>Installiamo il pacchetto pip <code>ultralytics</code> con le dipendenze opzionali:</li>
</ol>
<div class="sourceCode" id="cb49"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ultralytics<span class="pp">[</span><span class="ss">export</span><span class="pp">]</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="3" type="1">
<li>Riavviamo il dispositivo:</li>
</ol>
<div class="sourceCode" id="cb50"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> reboot</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="test-di-yolo" class="level3">
<h3 class="anchored" data-anchor-id="test-di-yolo">Test di YOLO</h3>
<p>Dopo l’avvio di Raspi-Zero, attiviamo l’ambiente <code>yolo</code>, andiamo alla directory di lavoro,</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/yolo/bin/activate</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /Documents/YOLO</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>ed eseguiamo l’inferenza su un’immagine che verrà scaricata dal sito web di Ultralytics, che utilizza il modello YOLOV8n (il più piccolo della famiglia) nel Terminal (CLI):</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> predict model=<span class="st">'yolov8n'</span> source=<span class="st">'https://ultralytics.com/images/bus.jpg'</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>La famiglia di modelli YOLO è pre-addestrata con il dataset COCO.</p>
</blockquote>
<p>Il risultato dell’inferenza apparirà nel terminale. Nell’immagine (bus.jpg), sono state rilevate 4 <code>persone</code>, 1 <code>autobus</code> e 1 <code>segnale di stop</code>:</p>
<p><img src="images/png/yolo-infer-bus.png" class="img-fluid"></p>
<p>Inoltre, abbiamo ricevuto un messaggio che indica <code>Results saved to runs/detect/predict</code>. Ispezionando quella directory, possiamo vedere una nuova immagine salvata (bus.jpg). Scarichiamola dal Raspi-Zero sul desktop per l’ispezione:</p>
<p><img src="images/png/yolo-bus.png" class="img-fluid"></p>
<p>Quindi, Ultrayitics YOLO è installato correttamente sul nostro Raspi. Ma, su Raspi-Zero, un problema è l’elevata latenza per questa inferenza, circa 18 secondi, anche con il modello più in miniatura della famiglia (YOLOv8n).</p>
</section>
<section id="esportazione-del-modello-in-formato-ncnn" class="level3">
<h3 class="anchored" data-anchor-id="esportazione-del-modello-in-formato-ncnn">Esportazione del Modello in formato NCNN</h3>
<p>L’implementazione di modelli di visione artificiale su dispositivi edge con potenza di calcolo limitata, come Raspi-Zero, può causare problemi di latenza. Un’alternativa è quella di utilizzare un formato ottimizzato per prestazioni ottimali. Ciò garantisce che anche i dispositivi con potenza di elaborazione limitata possano gestire bene attività di visione artificiale avanzate.</p>
<p>Di tutti i formati di esportazione del modello supportati da Ultralytics, <a href="https://docs.ultralytics.com/integrations/ncnn">NCNN</a> è un framework di elaborazione inferenziale di reti neurali ad alte prestazioni ottimizzato per piattaforme mobili. Fin dall’inizio della progettazione, NCNN è stato profondamente attento all’implementazione e all’uso su telefoni cellulari e non aveva dipendenze di terze parti. È multipiattaforma e funziona più velocemente di tutti i framework open source noti (come TFLite).</p>
<p>NCNN offre le migliori prestazioni di inferenza quando si lavora con dispositivi Raspberry Pi. NCNN è altamente ottimizzato per piattaforme mobili embedded (come l’architettura ARM).</p>
<p>Quindi, convertiamo il nostro modello e rieseguiamo l’inferenza:</p>
<ol type="1">
<li>Esportiamo un modello PyTorch YOLOv8n in formato NCNN, creando: ‘/yolov8n_ncnn_model’</li>
</ol>
<div class="sourceCode" id="cb53"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> export model=yolov8n.pt format=ncnn</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li>Eseguiamo l’inferenza con il modello esportato (ora la sorgente potrebbe essere l’immagine bus.jpg scaricata dal sito Web nella directory corrente nell’ultima inferenza):</li>
</ol>
<div class="sourceCode" id="cb54"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> predict model=<span class="st">'./yolov8n_ncnn_model'</span> source=<span class="st">'bus.jpg'</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>La prima inferenza, quando il modello viene caricato, di solito ha una latenza elevata (circa 17 s), ma dalla seconda, è possibile notare che l’inferenza scende a circa 2 s.</p>
</blockquote>
</section>
<section id="esplorare-yolo-con-python" class="level3">
<h3 class="anchored" data-anchor-id="esplorare-yolo-con-python">Esplorare YOLO con Python</h3>
<p>Per iniziare, chiamiamo l’interprete Python in modo da poter esplorare il funzionamento del modello YOLO, riga per riga:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ora, dovremmo chiamare la libreria YOLO da Ultralitics e caricare il modello:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'yolov8n_ncnn_model'</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Quindi, eseguire l’inferenza su un’immagine (usiamo di nuovo <code>bus.jpg</code>):</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'bus.jpg'</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">True</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/python-infer-bus.png" class="img-fluid"></p>
<p>Possiamo verificare che il risultato è quasi identico a quello che otteniamo eseguendo l’inferenza a livello di terminale (CLI), tranne per il fatto che la fermata dell’autobus non è stata rilevata con il modello NCNN ridotto. Notare che la latenza è stata ridotta.</p>
<p>Analizziamo il contenuto di “result”.</p>
<p>Ad esempio, possiamo vedere <code>result[0].boxes.data</code>, che mostra il risultato principale dell’inferenza, che è un profilo tensoriale (4, 6). Ogni riga è uno degli oggetti rilevati, ovvero le prime 4 colonne, le coordinate delle bounding box, la quinta, la confidenza e la sesta, la classe (in questo caso, <code>0: person</code> e <code>5: bus</code>):</p>
<p><img src="images/png/result-bus.png" class="img-fluid"></p>
<p>Possiamo accedere a diversi risultati di inferenza separatamente, come il tempo di inferenza, e stamparli in un formato migliore:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> <span class="bu">int</span>(result[<span class="dv">0</span>].speed[<span class="st">'inference'</span>])</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">}</span><span class="ss"> ms"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Oppure possiamo avere il numero totale di oggetti rilevati:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of objects: </span><span class="sc">{</span><span class="bu">len</span> (result[<span class="dv">0</span>].boxes.cls)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/data-bus.png" class="img-fluid"></p>
<p>Con Python, possiamo creare un output dettagliato che soddisfi le nostre esigenze (vedere <a href="https://docs.ultralytics.com/modes/predict/">Model Prediction with Ultralytics YOLO</a> per maggiori dettagli). Eseguiamo uno script Python invece di inserirlo manualmente riga per riga nell’interprete, come mostrato di seguito. Usiamo <code>nano</code> come editor di testo. Per prima cosa, dovremmo creare uno script Python vuoto denominato, ad esempio, <code>yolov8_tests.py</code>:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>nano yolov8_tests.py</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Si inseriscono le righe di codice:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the YOLOv8 model</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'yolov8n_ncnn_model'</span>)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Run inference</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'bus.jpg'</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">False</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print the results</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> <span class="bu">int</span>(result[<span class="dv">0</span>].speed[<span class="st">'inference'</span>])</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">}</span><span class="ss"> ms"</span>)</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of objects: </span><span class="sc">{</span><span class="bu">len</span> (result[<span class="dv">0</span>].boxes.cls)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/png/yolo-py-script.png" class="img-fluid"></p>
<p>E si inseriscono i comandi: <code>[CTRL+O]</code> + <code>[INVIO]</code> +<code>[CTRL+X]</code> per salvare lo script Python.</p>
<p>Eseguire lo script:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> yolov8_tests.py</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Il risultato è lo stesso dell’esecuzione dell’inferenza a livello di terminale (CLI) e con l’interprete Python nativo.</p>
<blockquote class="blockquote">
<p>Chiamare la libreria YOLO e caricare il modello per l’inferenza per la prima volta richiede molto tempo, ma le inferenze successive saranno molto più veloci. Ad esempio, la prima singola inferenza può richiedere diversi secondi, ma in seguito il tempo di inferenza dovrebbe essere ridotto a meno di 1 secondo.</p>
</blockquote>
</section>
<section id="addestramento-di-yolov8-su-un-dataset-personalizzato" class="level3">
<h3 class="anchored" data-anchor-id="addestramento-di-yolov8-su-un-dataset-personalizzato">Addestramento di YOLOv8 su un Dataset Personalizzato</h3>
<p>Torniamo al nostro set di dati “Boxe versus Wheel”, etichettato su <a href="https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset">Roboflow</a>. Nell’opzione <code>Download Dataset</code>, invece dell’opzione <code>Download a zip to computer</code> eseguita per l’addestramento su Edge Impulse Studio, opteremo per <code>Show download code</code>. Questa opzione aprirà una finestra pop-up con un frammento di codice che dovrebbe essere incollato nel nostro notebook di training.</p>
<p><img src="images/png/dataset_code.png" class="img-fluid"></p>
<p>Per l’addestramento, adattiamo uno degli esempi pubblici disponibili da Ultralitytics ed eseguiamolo su Google Colab. Di seguito, si può trovare il mio da adattare al progetto:</p>
<ul>
<li>YOLOv8 Box versus Wheel Dataset Training <a href="https://colab.research.google.com/github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/yolov8_box_vs_wheel.ipynb">[Open In Colab]</a></li>
</ul>
<section id="punti-critici-sul-notebook" class="level4">
<h4 class="anchored" data-anchor-id="punti-critici-sul-notebook">Punti critici sul Notebook:</h4>
<ol type="1">
<li><p>Eseguirlo con GPU (NVidia T4 è gratuito)</p></li>
<li><p>Installare Ultralytics tramite PIP.</p>
<p><img src="images/png/yolo-train-lib.png" class="img-fluid"></p></li>
<li><p>Ora, si può importare YOLO e caricare il dataset sul CoLab, incollando il codice di download che riceviamo da Roboflow. Notare che il dataset verrà montato in <code>/content/datasets/</code>:</p></li>
</ol>
<p><img src="images/png/yolo-dataset-upload.png" class="img-fluid"></p>
<ol start="4" type="1">
<li>È essenziale verificare e modificare il file <code>data.yaml</code> con il path corretto per le immagini (copiare il percorso su ogni cartella <code>images</code>).</li>
</ol>
<div class="sourceCode" id="cb63"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="ex">names:</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> box</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> wheel</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="ex">nc:</span> 2</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="ex">roboflow:</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">license:</span> CC BY 4.0</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">project:</span> box-versus-wheel-auto-dataset</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">url:</span> https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset/dataset/5</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">version:</span> 5</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>  <span class="ex">workspace:</span> marcelo-rovai-riila</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="ex">test:</span> /content/datasets/Box-versus-Wheel-auto-dataset-5/test/images</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="ex">train:</span> /content/datasets/Box-versus-Wheel-auto-dataset-5/train/images</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="ex">val:</span> /content/datasets/Box-versus-Wheel-auto-dataset-5/valid/images</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="5" type="1">
<li><p>Definire i principali iperparametri da modificare rispetto ai valori di default, ad esempio:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="ex">MODEL</span> = <span class="st">'yolov8n.pt'</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="ex">IMG_SIZE</span> = 640</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="ex">EPOCHS</span> = 25 <span class="co"># For a final project, you should consider at least 100 epochs</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Eseguire il training (utilizzando la CLI):</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/train-result.png" class="img-fluid figure-img"></p>
<figcaption>image-20240910111319804</figcaption>
</figure>
</div>
<p>L’addestramento del modello ha richiesto alcuni minuti e ha prodotto risultati eccellenti (mAP50 pari a 0,995). Al termine del training, tutti i risultati vengono salvati nella cartella elencata, ad esempio: <code>/runs/detect/train/</code>. Lì si trova, ad esempio, la matrice di confusione.</p></li>
</ol>
<p><img src="images/png/matrix.png" class="img-fluid"></p>
<ol start="7" type="1">
<li>Notare che il modello addestrato (<code>best.pt</code>) viene salvato nella cartella <code>/runs/detect/train/weights/</code>. Ora, si deve convalidare il modello addestrato con <code>valid/images</code>.</li>
</ol>
<div class="sourceCode" id="cb66"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>I risultati erano simili al training.</code></pre>
<ol start="8" type="1">
<li>Ora, si deve eseguire l’inferenza sulle immagini lasciate a parte per il test</li>
</ol>
<div class="sourceCode" id="cb68"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I risultati dell’inferenza vengono salvati nella cartella <code>runs/detect/predict</code>. Vediamone alcuni:</p>
<p><img src="images/png/test-infer-yolo.png" class="img-fluid"></p>
<ol start="9" type="1">
<li><p>Si consiglia di esportare i risultati di train, validation e test per un Drive su Google. Per farlo, si deve montare l’unità.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>drive.mount(<span class="st">'/content/gdrive'</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>e copiare il contenuto della cartella <code>/runs</code> in una cartella che si deve creare nel Drive, ad esempio:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!scp</span> <span class="at">-r</span> /content/runs <span class="st">'/content/gdrive/MyDrive/10_UNIFEI/Box_vs_Wheel_Project'</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
</section>
<section id="inferenza-col-modello-addestrato-usando-raspi" class="level3">
<h3 class="anchored" data-anchor-id="inferenza-col-modello-addestrato-usando-raspi">Inferenza col modello addestrato, usando Raspi</h3>
<p>Scaricare il modello addestrato <code>/runs/detect/train/weights/best.pt</code> nel computer. Utilizzando l’FTP FileZilla, trasferiamo <code>best.pt</code> nella cartella dei modelli Raspi (prima del trasferimento, si può cambiare il nome del modello, ad esempio, <code>box_wheel_320_yolo.pt</code>).</p>
<p>Con FileZilla FTP, trasferiamo alcune immagini dal set di dati di prova a <code>.\YOLO\images</code>:</p>
<p>Torniamo alla cartella YOLO e utilizziamo l’interprete Python:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ..</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Come prima, importeremo la libreria YOLO e definiremo il nostro modello convertito per rilevare le api:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'./models/box_wheel_320_yolo.pt'</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ora, definiamo un’immagine e chiamiamo l’inferenza (stavolta salveremo il risultato dell’immagine per la verifica esterna):</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'./images/1_box_1_wheel.jpg'</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">True</span>, imgsz<span class="op">=</span><span class="dv">320</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ripetiamo per diverse immagini. Il risultato dell’inferenza viene salvato sulla variabile <code>result</code> e l’immagine elaborata su <code>runs/detect/predict8</code></p>
<p><img src="images/png/infer-yolo.png" class="img-fluid"></p>
<p>Con FileZilla FTP, possiamo inviare il risultato dell’inferenza al Desktop per la verifica:</p>
<p><img src="images/png/yolo-infer-raspi.png" class="img-fluid"></p>
<p>Possiamo vedere che il risultato dell’inferenza è eccellente! Il modello è stato addestrato in base al modello base più piccolo della famiglia YOLOv8 (YOLOv8n). Il problema è la latenza, circa 1 secondo (o 1 FPS su Raspi-Zero). Naturalmente, possiamo ridurre questa latenza e convertire il modello in TFLite o NCNN.</p>
</section>
</section>
<section id="rilevamento-di-oggetti-su-un-live-streaming" class="level2">
<h2 class="anchored" data-anchor-id="rilevamento-di-oggetti-su-un-live-streaming">Rilevamento di Oggetti su un Live Streaming</h2>
<p>Tutti i modelli esplorati in questo laboratorio possono rilevare oggetti in tempo reale utilizzando una telecamera. L’immagine catturata dovrebbe essere l’input per il modello addestrato e convertito. Per Raspi-4 o 5 con un desktop, OpenCV può catturare i frame e visualizzare il risultato dell’inferenza.</p>
<p>Tuttavia, è anche possibile creare un live streaming con una webcam per rilevare oggetti in tempo reale. Ad esempio, iniziamo con lo script sviluppato per l’app Image Classification e adattiamolo per un’<em>Applicazione Web di Rilevamento di Oggetti in Tempo Reale Utilizzando TensorFlow Lite e Flask</em>.</p>
<p>Questa versione dell’app funzionerà per tutti i modelli TFLite. Verificare che il modello sia nella cartella corretta, ad esempio:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"./models/ssd-mobilenet-v1-tflite-default-v1.tflite"</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Scaricare lo script Python <code>object_detection_app.py</code> da <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/python_scripts/object_detection_app.py">GitHub</a>.</p>
<p>E sul terminale, si esegue:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> object_detection_app.py</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>E accedere all’interfaccia web:</p>
<ul>
<li>Sul Raspberry Pi stesso (se si ha una GUI): si apre un browser web e si va su<code>http://localhost:5000</code></li>
<li>Da un altro dispositivo sulla stessa rete: aprire un browser web e andare su <code>http://&lt;raspberry_pi_ip&gt;:5000</code> (Sostituire <code>&lt;raspberry_pi_ip&gt;</code> con l’indirizzo IP del Raspberry Pi). Per esempio: <code>http://192.168.4.210:5000/</code></li>
</ul>
<p>Ecco alcuni screenshot dell’app in esecuzione su un desktop esterno</p>
<p><img src="images/png/app-running.png" class="img-fluid"></p>
<p>Diamo un’occhiata a una descrizione tecnica dei moduli chiave utilizzati nell’applicazione di rilevamento degli oggetti:</p>
<ol type="1">
<li><strong>TensorFlow Lite (tflite_runtime)</strong>:
<ul>
<li>Scopo: Inferenza efficiente di modelli di machine learning su dispositivi edge.</li>
<li>Perché: TFLite offre dimensioni ridotte del modello e prestazioni ottimizzate rispetto a TensorFlow completo, il che è fondamentale per dispositivi con risorse limitate come Raspberry Pi. Supporta l’accelerazione hardware e la quantizzazione, migliorando ulteriormente l’efficienza.</li>
<li>Funzioni chiave: <code>Interpreter</code> per caricare ed eseguire il modello, <code>get_input_details()</code> e <code>get_output_details()</code> per l’interfaccia col modello.</li>
</ul></li>
<li><strong>Flask:</strong>
<ul>
<li>Scopo: Framework web leggero per la creazione del server backend.</li>
<li>Perché: La semplicità e la flessibilità di Flask lo rendono ideale per lo sviluppo e la distribuzione rapidi di applicazioni web. Richiede meno risorse rispetto a framework più grandi, adatto a dispositivi edge.</li>
<li>Componenti chiave: decoratori di route per definire endpoint API, oggetti <code>Response</code> per lo streaming video, <code>render_template_string</code> per servire HTML dinamico.</li>
</ul></li>
<li><strong>Picamera2:</strong>
<ul>
<li>Scopo: Interfaccia con il modulo fotocamera Raspberry Pi.</li>
<li>Perché: Picamera2 è la libreria più recente per il controllo delle fotocamere Raspberry Pi, che offre prestazioni e funzionalità migliorate rispetto alla libreria Picamera originale.</li>
<li>Funzioni chiave: <code>create_preview_configuration()</code> per impostare la fotocamera, <code>capture_file()</code> per catturare i fotogrammi.</li>
</ul></li>
<li><strong>PIL (Python Imaging Library):</strong>
<ul>
<li>Scopo: Elaborazione e manipolazione delle immagini.</li>
<li>Perché: PIL fornisce un’ampia gamma di funzionalità di elaborazione delle immagini. Viene utilizzato qui per ridimensionare le immagini, disegnare riquadri di delimitazione e convertire tra formati di immagine.</li>
<li>Classi chiave: <code>Image</code> per caricare e manipolare immagini, <code>ImageDraw</code> per disegnare forme e testo sulle immagini.</li>
</ul></li>
<li><strong>NumPy:</strong>
<ul>
<li>Scopo: Operazioni array efficienti e calcolo numerico.</li>
<li>Perché: le operazioni su array di NumPy sono molto più veloci delle liste Python pure, il che è fondamentale per elaborare in modo efficiente i dati delle immagini e gli input/output del modello.</li>
<li>Funzioni chiave: <code>array()</code> per creare array, <code>expand_dims()</code> per aggiungere dimensioni agli array.</li>
</ul></li>
<li><strong>Threading:</strong>
<ul>
<li>Scopo: Esecuzione simultanea di attività.</li>
<li>Perché: Il threading consente l’acquisizione simultanea di frame, il rilevamento di oggetti e il funzionamento del server Web, cruciali per il mantenimento delle prestazioni in tempo reale.</li>
<li>Componenti chiave: La classe <code>Thread</code> crea thread di esecuzione separati e Lock viene utilizzato per la sincronizzazione dei thread.</li>
</ul></li>
<li><strong>io.BytesIO:</strong>
<ul>
<li>Scopo: Stream binari in memoria.</li>
<li>Perché: Consente una gestione efficiente dei dati delle immagini in memoria senza bisogno di file temporanei, migliorando la velocità e riducendo le operazioni di I/O.</li>
</ul></li>
<li><strong>time:</strong>
<ul>
<li>Scopo: Funzioni correlate al tempo.</li>
<li>Perché: Utilizzato per aggiungere ritardi (<code>time.sleep()</code>) per controllare la frequenza dei fotogrammi e per le misure delle prestazioni.</li>
</ul></li>
<li><strong>jQuery (client-side)</strong>:
<ul>
<li>Scopo: Manipolazione DOM semplificata e richieste AJAX.</li>
<li>Perché: Semplifica l’aggiornamento dinamico dell’interfaccia web e la comunicazione con il server senza ricaricamenti di pagina.</li>
<li>Funzioni chiave: <code>.get()</code> e <code>.post()</code> per richieste AJAX, metodi di manipolazione DOM per l’aggiornamento dell’interfaccia utente.</li>
</ul></li>
</ol>
<p>Per quanto riguarda l’architettura del sistema dell’app principale:</p>
<ol type="1">
<li><strong>Main Thread</strong>: Esegue il server Flask, gestisce le richieste HTTP e serve l’interfaccia web.</li>
<li><strong>Camera Thread</strong>: Cattura continuamente i frame dalla fotocamera.</li>
<li><strong>Detection Thread</strong>: Elabora i frame tramite il modello TFLite per il rilevamento degli oggetti.</li>
<li><strong>Frame Buffer</strong>: Spazio di memoria condiviso (protetto da blocchi) che memorizza i frame più recenti e i risultati del rilevamento.</li>
</ol>
<p>E il flusso di dati dell’app, possiamo descriverlo in breve:</p>
<ol type="1">
<li>La fotocamera cattura il frame → Frame Buffer</li>
<li>Il thread di rilevamento legge dal Frame Buffer → Elabora tramite il modello TFLite → Aggiorna i risultati del rilevamento nel Frame Buffer</li>
<li>Flask indirizza l’accesso al Frame Buffer per fornire i risultati più recenti del frame e del rilevamento</li>
<li>Il client Web riceve gli aggiornamenti tramite AJAX e aggiorna l’interfaccia utente</li>
</ol>
<p>Questa architettura consente un rilevamento efficiente degli oggetti in tempo reale, mantenendo al contempo un’interfaccia Web reattiva in esecuzione su un dispositivo edge con risorse limitate come un Raspberry Pi. Il threading e le librerie efficienti come TFLite e PIL consentono al sistema di elaborare i frame video in tempo reale, mentre Flask e jQuery forniscono un modo intuitivo per interagire con essi.</p>
<p>Si può testare l’app con un altro modello pre-elaborato, come EfficientDet, modificando la riga dell’app:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"./models/lite-model_efficientdet_lite0_detection_metadata_1.tflite"</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Per usare l’app per il modello SSD-MobileNetV2, addestrato su Edge Impulse Studio con il set di dati “Box versus Wheel”, il codice dovrebbe essere adattato anche in base ai dettagli di input, come abbiamo esplorato sul suo <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-SSD-MobileNetV2.ipynb">notebook</a>.</p>
</blockquote>
</section>
<section id="conclusione" class="level2">
<h2 class="anchored" data-anchor-id="conclusione">Conclusione</h2>
<p>Questo laboratorio ha esplorato l’implementazione del rilevamento di oggetti su dispositivi edge come Raspberry Pi, dimostrando la potenza e il potenziale dell’esecuzione di attività avanzate di computer vision su hardware con risorse limitate. Abbiamo trattato diversi aspetti essenziali:</p>
<ol type="1">
<li><p><strong>Confronto tra Modelli</strong>: Abbiamo esaminato diversi modelli di rilevamento oggetti, tra cui SSD-MobileNet, EfficientDet, FOMO e YOLO, confrontandone le prestazioni e i compromessi sui dispositivi edge.</p></li>
<li><p><strong>Training e Deployment</strong>: Utilizzando un set di dati personalizzato di scatole e ruote (etichettato su Roboflow), abbiamo esaminato il processo di addestramento dei modelli utilizzando Edge Impulse Studio e Ultralytics e la loro distribuzione su Raspberry Pi.</p></li>
<li><p><strong>Tecniche di Ottimizzazione</strong>: Per migliorare la velocità di inferenza sui dispositivi edge, abbiamo esplorato vari metodi di ottimizzazione, come la quantizzazione del modello (TFLite int8) e la conversione del formato (ad esempio, in NCNN).</p></li>
<li><p><strong>Applicazioni in Tempo Reale</strong>: Il laboratorio ha esemplificato un’applicazione Web di rilevamento oggetti in tempo reale, dimostrando come questi modelli possono essere integrati in sistemi pratici e interattivi.</p></li>
<li><p><strong>Considerazioni sulle Prestazioni</strong>: Durante il laboratorio abbiamo discusso l’equilibrio tra accuratezza del modello e velocità di inferenza, un aspetto fondamentale per le applicazioni di IA edge.</p></li>
</ol>
<p>La capacità di eseguire il rilevamento di oggetti su dispositivi edge apre numerose possibilità in vari ambiti, dall’agricoltura di precisione, all’automazione industriale e al controllo di qualità, alle applicazioni per la casa intelligente e al monitoraggio ambientale. Elaborando i dati localmente, questi sistemi possono offrire latenza ridotta, privacy migliorata e funzionamento in ambienti con connettività limitata.</p>
<p>Guardando al futuro, le potenziali aree di ulteriore esplorazione includono: - Implementazione di pipeline multi-modello per attività più complesse - Esplorazione di opzioni di accelerazione hardware per Raspberry Pi - Integrazione del rilevamento di oggetti con altri sensori per sistemi AI edge più completi - Sviluppo di soluzioni edge-to-cloud che sfruttano sia l’elaborazione locale che le risorse cloud</p>
<p>Il rilevamento di oggetti su dispositivi edge può creare sistemi intelligenti e reattivi che portano la potenza dell’IA direttamente nel mondo fisico, aprendo nuove frontiere nel modo in cui interagiamo e comprendiamo il nostro ambiente.</p>
</section>
<section id="risorse" class="level2">
<h2 class="anchored" data-anchor-id="risorse">Risorse</h2>
<ul>
<li><p><a href="https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset">Dataset (“Box versus Wheel”)</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_MobileNetV1.ipynb">SSD-MobileNet Notebook on a Raspi</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_EfficientDet.ipynb">EfficientDet Notebook on a Raspi</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-Linux-FOMO.ipynb">FOMO - EI Linux Notebook on a Raspi</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/yolov8_box_vs_wheel.ipynb">YOLOv8 Box versus Wheel Dataset Training on CoLab</a></p></li>
<li><p><a href="https://studio.edgeimpulse.com/public/515477/live">Edge Impulse Project - SSD MobileNet and FOMO</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/python_scripts">Script Python</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models">Modelli</a></p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../contents/labs/raspi/image_classification/image_classification.it.html" class="pagination-link" aria-label="Classificazione delle Immagini">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Classificazione delle Immagini</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../contents/labs/raspi/llm/llm.it.html" class="pagination-link" aria-label="Small Language Models (SLM)">
        <span class="nav-page-text">Small Language Models (SLM)</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/labs/raspi/object_detection/object_detection.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/labs/raspi/object_detection/object_detection.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro è stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>