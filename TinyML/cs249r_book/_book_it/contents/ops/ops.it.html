<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>13&nbsp; Operazioni di ML ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/privacy_security/privacy_security.it.html" rel="next">
<link href="../../contents/ondevice_learning/ondevice_learning.it.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/benchmarking/benchmarking.it.html">Deployment</a></li><li class="breadcrumb-item"><a href="../../contents/ops/ops.it.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2d6cdf6fc58f1105ce2a5c5c28a11153" class="alert alert-info hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>üåü Aiutaci a raggiungere 1.000 stelle GitHub! üåü Per ogni 25 stelle, Arduino e SEEED doneranno una NiclaVision o una XIAO ESP32S3 per l‚Äôistruzione sull‚Äôintelligenza artificiale. <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per una ‚≠ê</a></p>
</div></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">PREFAZIONE</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">PARTE PRINCIPALE</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Nozioni Fondamentali</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Workflow</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di Intelligenza Artificiale</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Argomenti Avanzati</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Impatto Sociale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Chiusura</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">13.1</span> Introduzione</a></li>
  <li><a href="#contesto-storico" id="toc-contesto-storico" class="nav-link" data-scroll-target="#contesto-storico"><span class="header-section-number">13.2</span> Contesto Storico</a>
  <ul>
  <li><a href="#devops" id="toc-devops" class="nav-link" data-scroll-target="#devops"><span class="header-section-number">13.2.1</span> DevOps</a></li>
  <li><a href="#mlops" id="toc-mlops" class="nav-link" data-scroll-target="#mlops"><span class="header-section-number">13.2.2</span> MLOps</a></li>
  </ul></li>
  <li><a href="#componenti-chiave-di-mlops" id="toc-componenti-chiave-di-mlops" class="nav-link" data-scroll-target="#componenti-chiave-di-mlops"><span class="header-section-number">13.3</span> Componenti Chiave di MLOps</a>
  <ul>
  <li><a href="#sec-ops-data-mgmt" id="toc-sec-ops-data-mgmt" class="nav-link" data-scroll-target="#sec-ops-data-mgmt"><span class="header-section-number">13.3.1</span> Gestione dei Dati</a></li>
  <li><a href="#pipeline-cicd" id="toc-pipeline-cicd" class="nav-link" data-scroll-target="#pipeline-cicd"><span class="header-section-number">13.3.2</span> Pipeline CI/CD</a></li>
  <li><a href="#addestramento-del-modello" id="toc-addestramento-del-modello" class="nav-link" data-scroll-target="#addestramento-del-modello"><span class="header-section-number">13.3.3</span> Addestramento del Modello</a></li>
  <li><a href="#valutazione-del-modello" id="toc-valutazione-del-modello" class="nav-link" data-scroll-target="#valutazione-del-modello"><span class="header-section-number">13.3.4</span> Valutazione del Modello</a></li>
  <li><a href="#distribuzione-del-modello" id="toc-distribuzione-del-modello" class="nav-link" data-scroll-target="#distribuzione-del-modello"><span class="header-section-number">13.3.5</span> Distribuzione del Modello</a></li>
  <li><a href="#model-serving" id="toc-model-serving" class="nav-link" data-scroll-target="#model-serving"><span class="header-section-number">13.3.6</span> Model Serving</a></li>
  <li><a href="#gestione-dellinfrastruttura" id="toc-gestione-dellinfrastruttura" class="nav-link" data-scroll-target="#gestione-dellinfrastruttura"><span class="header-section-number">13.3.7</span> Gestione dell‚ÄôInfrastruttura</a></li>
  <li><a href="#monitoraggio" id="toc-monitoraggio" class="nav-link" data-scroll-target="#monitoraggio"><span class="header-section-number">13.3.8</span> Monitoraggio</a></li>
  <li><a href="#governance" id="toc-governance" class="nav-link" data-scroll-target="#governance"><span class="header-section-number">13.3.9</span> Governance</a></li>
  <li><a href="#comunicazione-e-collaborazione" id="toc-comunicazione-e-collaborazione" class="nav-link" data-scroll-target="#comunicazione-e-collaborazione"><span class="header-section-number">13.3.10</span> Comunicazione e Collaborazione</a></li>
  </ul></li>
  <li><a href="#debito-tecnico-nascosto-nei-sistemi-ml" id="toc-debito-tecnico-nascosto-nei-sistemi-ml" class="nav-link" data-scroll-target="#debito-tecnico-nascosto-nei-sistemi-ml"><span class="header-section-number">13.4</span> Debito Tecnico Nascosto nei Sistemi ML</a>
  <ul>
  <li><a href="#erosione-dei-confini-del-modello" id="toc-erosione-dei-confini-del-modello" class="nav-link" data-scroll-target="#erosione-dei-confini-del-modello"><span class="header-section-number">13.4.1</span> Erosione dei Confini del Modello</a></li>
  <li><a href="#intreccio" id="toc-intreccio" class="nav-link" data-scroll-target="#intreccio"><span class="header-section-number">13.4.2</span> Intreccio</a></li>
  <li><a href="#cascate-di-correzione" id="toc-cascate-di-correzione" class="nav-link" data-scroll-target="#cascate-di-correzione"><span class="header-section-number">13.4.3</span> Cascate di Correzione</a></li>
  <li><a href="#consumatori-non-dichiarati" id="toc-consumatori-non-dichiarati" class="nav-link" data-scroll-target="#consumatori-non-dichiarati"><span class="header-section-number">13.4.4</span> Consumatori Non Dichiarati</a></li>
  <li><a href="#debito-di-dipendenza-dai-dati" id="toc-debito-di-dipendenza-dai-dati" class="nav-link" data-scroll-target="#debito-di-dipendenza-dai-dati"><span class="header-section-number">13.4.5</span> Debito di Dipendenza dai Dati</a></li>
  <li><a href="#debito-di-analisi-dai-cicli-di-feedback" id="toc-debito-di-analisi-dai-cicli-di-feedback" class="nav-link" data-scroll-target="#debito-di-analisi-dai-cicli-di-feedback"><span class="header-section-number">13.4.6</span> Debito di Analisi dai Cicli di Feedback</a></li>
  <li><a href="#le-giungle-di-pipeline" id="toc-le-giungle-di-pipeline" class="nav-link" data-scroll-target="#le-giungle-di-pipeline"><span class="header-section-number">13.4.7</span> Le Giungle di Pipeline</a></li>
  <li><a href="#debito-di-configurazione" id="toc-debito-di-configurazione" class="nav-link" data-scroll-target="#debito-di-configurazione"><span class="header-section-number">13.4.8</span> Debito di Configurazione</a></li>
  <li><a href="#il-mondo-che-cambia" id="toc-il-mondo-che-cambia" class="nav-link" data-scroll-target="#il-mondo-che-cambia"><span class="header-section-number">13.4.9</span> Il Mondo che Cambia</a></li>
  <li><a href="#gestire-il-debito-tecnico-nelle-fasi-iniziali" id="toc-gestire-il-debito-tecnico-nelle-fasi-iniziali" class="nav-link" data-scroll-target="#gestire-il-debito-tecnico-nelle-fasi-iniziali"><span class="header-section-number">13.4.10</span> Gestire il Debito Tecnico nelle Fasi Iniziali</a></li>
  <li><a href="#riepilogo" id="toc-riepilogo" class="nav-link" data-scroll-target="#riepilogo"><span class="header-section-number">13.4.11</span> Riepilogo</a></li>
  </ul></li>
  <li><a href="#ruoli-e-responsabilit√†" id="toc-ruoli-e-responsabilit√†" class="nav-link" data-scroll-target="#ruoli-e-responsabilit√†"><span class="header-section-number">13.5</span> Ruoli e Responsabilit√†</a>
  <ul>
  <li><a href="#ingegneri-dei-dati" id="toc-ingegneri-dei-dati" class="nav-link" data-scroll-target="#ingegneri-dei-dati"><span class="header-section-number">13.5.1</span> Ingegneri dei Dati</a></li>
  <li><a href="#data-scientist" id="toc-data-scientist" class="nav-link" data-scroll-target="#data-scientist"><span class="header-section-number">13.5.2</span> Data Scientist</a></li>
  <li><a href="#ml-engineer" id="toc-ml-engineer" class="nav-link" data-scroll-target="#ml-engineer"><span class="header-section-number">13.5.3</span> ML Engineer</a></li>
  <li><a href="#devops-engineer" id="toc-devops-engineer" class="nav-link" data-scroll-target="#devops-engineer"><span class="header-section-number">13.5.4</span> DevOps Engineer</a></li>
  <li><a href="#project-manager" id="toc-project-manager" class="nav-link" data-scroll-target="#project-manager"><span class="header-section-number">13.5.5</span> Project Manager</a></li>
  </ul></li>
  <li><a href="#sfide-dei-sistemi-embedded" id="toc-sfide-dei-sistemi-embedded" class="nav-link" data-scroll-target="#sfide-dei-sistemi-embedded"><span class="header-section-number">13.6</span> Sfide dei Sistemi Embedded</a>
  <ul>
  <li><a href="#risorse-di-elaborazione-limitate" id="toc-risorse-di-elaborazione-limitate" class="nav-link" data-scroll-target="#risorse-di-elaborazione-limitate"><span class="header-section-number">13.6.1</span> Risorse di Elaborazione Limitate</a></li>
  <li><a href="#memoria-limitata" id="toc-memoria-limitata" class="nav-link" data-scroll-target="#memoria-limitata"><span class="header-section-number">13.6.2</span> Memoria Limitata</a></li>
  <li><a href="#connettivit√†-intermittente" id="toc-connettivit√†-intermittente" class="nav-link" data-scroll-target="#connettivit√†-intermittente"><span class="header-section-number">13.6.3</span> Connettivit√† Intermittente</a></li>
  <li><a href="#limitazioni-della-potenza" id="toc-limitazioni-della-potenza" class="nav-link" data-scroll-target="#limitazioni-della-potenza"><span class="header-section-number">13.6.4</span> Limitazioni della Potenza</a></li>
  <li><a href="#gestione-della-flotta" id="toc-gestione-della-flotta" class="nav-link" data-scroll-target="#gestione-della-flotta"><span class="header-section-number">13.6.5</span> Gestione della Flotta</a></li>
  <li><a href="#raccolta-dati-on-device" id="toc-raccolta-dati-on-device" class="nav-link" data-scroll-target="#raccolta-dati-on-device"><span class="header-section-number">13.6.6</span> Raccolta Dati On-Device</a></li>
  <li><a href="#personalizzazione-specifica-del-dispositivo" id="toc-personalizzazione-specifica-del-dispositivo" class="nav-link" data-scroll-target="#personalizzazione-specifica-del-dispositivo"><span class="header-section-number">13.6.7</span> Personalizzazione Specifica del Dispositivo</a></li>
  <li><a href="#considerazioni-sulla-sicurezza" id="toc-considerazioni-sulla-sicurezza" class="nav-link" data-scroll-target="#considerazioni-sulla-sicurezza"><span class="header-section-number">13.6.8</span> Considerazioni sulla Sicurezza</a></li>
  <li><a href="#diversi-target-hardware" id="toc-diversi-target-hardware" class="nav-link" data-scroll-target="#diversi-target-hardware"><span class="header-section-number">13.6.9</span> Diversi Target Hardware</a></li>
  <li><a href="#copertura-dei-test" id="toc-copertura-dei-test" class="nav-link" data-scroll-target="#copertura-dei-test"><span class="header-section-number">13.6.10</span> Copertura dei Test</a></li>
  <li><a href="#rilevamento-della-deriva-del-concetto" id="toc-rilevamento-della-deriva-del-concetto" class="nav-link" data-scroll-target="#rilevamento-della-deriva-del-concetto"><span class="header-section-number">13.6.11</span> Rilevamento della Deriva del Concetto</a></li>
  </ul></li>
  <li><a href="#mlops-tradizionali-e-mlops-embedded" id="toc-mlops-tradizionali-e-mlops-embedded" class="nav-link" data-scroll-target="#mlops-tradizionali-e-mlops-embedded"><span class="header-section-number">13.7</span> MLOps Tradizionali e MLOps Embedded</a>
  <ul>
  <li><a href="#gestione-del-ciclo-di-vita-del-modello" id="toc-gestione-del-ciclo-di-vita-del-modello" class="nav-link" data-scroll-target="#gestione-del-ciclo-di-vita-del-modello"><span class="header-section-number">13.7.1</span> Gestione del Ciclo di Vita del Modello</a>
  <ul class="collapse">
  <li><a href="#gestione-dei-dati" id="toc-gestione-dei-dati" class="nav-link" data-scroll-target="#gestione-dei-dati">Gestione dei Dati</a></li>
  <li><a href="#addestramento-del-modello-1" id="toc-addestramento-del-modello-1" class="nav-link" data-scroll-target="#addestramento-del-modello-1">Addestramento del Modello</a></li>
  <li><a href="#valutazione-del-modello-1" id="toc-valutazione-del-modello-1" class="nav-link" data-scroll-target="#valutazione-del-modello-1">Valutazione del Modello</a></li>
  <li><a href="#distribuzione-del-modello-1" id="toc-distribuzione-del-modello-1" class="nav-link" data-scroll-target="#distribuzione-del-modello-1">Distribuzione del Modello</a></li>
  </ul></li>
  <li><a href="#integrazione-di-sviluppo-e-operazioni" id="toc-integrazione-di-sviluppo-e-operazioni" class="nav-link" data-scroll-target="#integrazione-di-sviluppo-e-operazioni"><span class="header-section-number">13.7.2</span> Integrazione di Sviluppo e Operazioni</a>
  <ul class="collapse">
  <li><a href="#pipeline-cicd-1" id="toc-pipeline-cicd-1" class="nav-link" data-scroll-target="#pipeline-cicd-1">Pipeline CI/CD</a></li>
  <li><a href="#gestione-dellinfrastruttura-1" id="toc-gestione-dellinfrastruttura-1" class="nav-link" data-scroll-target="#gestione-dellinfrastruttura-1">Gestione dell‚ÄôInfrastruttura</a></li>
  <li><a href="#comunicazione-e-collaborazione-1" id="toc-comunicazione-e-collaborazione-1" class="nav-link" data-scroll-target="#comunicazione-e-collaborazione-1">Comunicazione e Collaborazione</a></li>
  </ul></li>
  <li><a href="#eccellenza-operativa" id="toc-eccellenza-operativa" class="nav-link" data-scroll-target="#eccellenza-operativa"><span class="header-section-number">13.7.3</span> Eccellenza operativa</a>
  <ul class="collapse">
  <li><a href="#monitoraggio-1" id="toc-monitoraggio-1" class="nav-link" data-scroll-target="#monitoraggio-1">Monitoraggio</a></li>
  <li><a href="#governance-1" id="toc-governance-1" class="nav-link" data-scroll-target="#governance-1">Governance</a></li>
  </ul></li>
  <li><a href="#confronto" id="toc-confronto" class="nav-link" data-scroll-target="#confronto"><span class="header-section-number">13.7.4</span> Confronto</a></li>
  <li><a href="#mlops-tradizionali" id="toc-mlops-tradizionali" class="nav-link" data-scroll-target="#mlops-tradizionali"><span class="header-section-number">13.7.5</span> MLOps Tradizionali</a>
  <ul class="collapse">
  <li><a href="#gestione-dei-dati-1" id="toc-gestione-dei-dati-1" class="nav-link" data-scroll-target="#gestione-dei-dati-1">Gestione dei Dati</a></li>
  <li><a href="#addestramento-del-modello-2" id="toc-addestramento-del-modello-2" class="nav-link" data-scroll-target="#addestramento-del-modello-2">Addestramento del Modello</a></li>
  <li><a href="#valutazione-del-modello-2" id="toc-valutazione-del-modello-2" class="nav-link" data-scroll-target="#valutazione-del-modello-2">Valutazione del Modello</a></li>
  <li><a href="#distribuzione-del-modello-2" id="toc-distribuzione-del-modello-2" class="nav-link" data-scroll-target="#distribuzione-del-modello-2">Distribuzione del Modello</a></li>
  </ul></li>
  <li><a href="#mlops-embedded" id="toc-mlops-embedded" class="nav-link" data-scroll-target="#mlops-embedded"><span class="header-section-number">13.7.6</span> MLOps Embedded</a>
  <ul class="collapse">
  <li><a href="#edge-impulse" id="toc-edge-impulse" class="nav-link" data-scroll-target="#edge-impulse">Edge Impulse</a></li>
  <li><a href="#limitazioni" id="toc-limitazioni" class="nav-link" data-scroll-target="#limitazioni">Limitazioni</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#casi-di-studio" id="toc-casi-di-studio" class="nav-link" data-scroll-target="#casi-di-studio"><span class="header-section-number">13.8</span> Casi di Studio</a>
  <ul>
  <li><a href="#oura-ring" id="toc-oura-ring" class="nav-link" data-scroll-target="#oura-ring"><span class="header-section-number">13.8.1</span> Oura Ring</a></li>
  <li><a href="#clinaiops" id="toc-clinaiops" class="nav-link" data-scroll-target="#clinaiops"><span class="header-section-number">13.8.2</span> ClinAIOps</a>
  <ul class="collapse">
  <li><a href="#cicli-di-feedback" id="toc-cicli-di-feedback" class="nav-link" data-scroll-target="#cicli-di-feedback">Cicli di Feedback</a></li>
  <li><a href="#esempio-di-ipertensione" id="toc-esempio-di-ipertensione" class="nav-link" data-scroll-target="#esempio-di-ipertensione">Esempio di Ipertensione</a></li>
  <li><a href="#mlops-vs.-clinaiops" id="toc-mlops-vs.-clinaiops" class="nav-link" data-scroll-target="#mlops-vs.-clinaiops">MLOps vs.&nbsp;ClinAIOps</a></li>
  <li><a href="#riepilogo-1" id="toc-riepilogo-1" class="nav-link" data-scroll-target="#riepilogo-1">Riepilogo</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">13.9</span> Conclusione</a></li>
  <li><a href="#sec-embedded-aiops-resource" id="toc-sec-embedded-aiops-resource" class="nav-link" data-scroll-target="#sec-embedded-aiops-resource"><span class="header-section-number">13.10</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/ops/ops.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/ops/ops.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/benchmarking/benchmarking.it.html">Deployment</a></li><li class="breadcrumb-item"><a href="../../contents/ops/ops.it.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-mlops" class="quarto-section-identifier"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-embedded-aiops-resource">Slide</a>, <a href="#sec-embedded-aiops-resource">Video</a>, <a href="#sec-embedded-aiops-resource">Esercizi</a>, <a href="#sec-embedded-aiops-resource">Laboratori</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ml_ops.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Crea un‚Äôillustrazione rettangolare ampia e dettagliata di un flusso di lavoro di IA. L‚Äôimmagine dovrebbe mostrare il processo in sei fasi, con un flusso da sinistra a destra: 1. Raccolta dati, con individui diversi come sesso e discendenza che utilizzano una variet√† di dispositivi come laptop, smartphone e sensori per raccogliere dati. 2. Elaborazione dati, che mostra un data center con server attivi e database con luci luminose. 3. Training del modello, rappresentata da uno schermo di computer con codice, diagrammi di reti neurali e indicatori di avanzamento. 4. Valutazione del modello, con persone che esaminano l‚Äôanalisi dei dati su grandi monitor. 5. Distribuzione, in cui l‚ÄôIA √® integrata in robotica, app mobili e apparecchiature industriali. 6. Monitoraggio, che mostra professionisti che monitorano le metriche delle prestazioni dell‚ÄôIA su dashboard per verificare l‚Äôaccuratezza e la deriva del concetto nel tempo. Ogni fase dovrebbe essere contrassegnata in modo distinto e lo stile dovrebbe essere pulito, elegante e moderno con una combinazione di colori dinamica e informativa.</em></figcaption>
</figure>
</div>
<p>Questo capitolo esplora le pratiche e le architetture necessarie per sviluppare, distribuire e gestire efficacemente i modelli ML durante il loro intero ciclo di vita. Esaminiamo le varie fasi del processo ML, tra cui raccolta dati, addestramento del modello, valutazione, distribuzione e monitoraggio. Anche l‚Äôimportanza dell‚Äôautomazione, della collaborazione e del miglioramento continuo √® un argomento di cui discutiamo. Mettiamo a confronto diversi ambienti per la distribuzione del modello ML, dai server cloud ai dispositivi edge embedded, e analizziamo i loro vincoli distinti. Mostriamo come personalizzare la progettazione e le operazioni del sistema ML attraverso esempi concreti per prestazioni del modello affidabili e ottimizzate in qualsiasi ambiente target. L‚Äôobiettivo √® fornire ai lettori una comprensione completa della gestione del modello ML in modo che possano creare ed eseguire con successo applicazioni ML che forniscano valore in modo sostenibile.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Capire cos‚Äô√® MLOps e perch√© √® necessario</p></li>
<li><p>Imparare i modelli architetturali per MLOps tradizionali</p></li>
<li><p>Confrontare MLOps tradizionali con MLOps embedded nel ciclo di vita ML</p></li>
<li><p>Identificare i vincoli chiave degli ambienti embedded</p></li>
<li><p>Imparare strategie per mitigare i problemi del ML embedded</p></li>
<li><p>Esaminare casi di studio del mondo reale che dimostrano i principi MLOps embedded</p></li>
<li><p>Apprezzare la necessit√† di approcci olistici tecnici e umani</p></li>
</ul>
</div>
</div>
<section id="introduzione" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="introduzione"><span class="header-section-number">13.1</span> Introduzione</h2>
<p>Machine Learning Operations (MLOps) √® un approccio sistematico che combina machine learning (ML), data science e ingegneria del software per automatizzare il ciclo di vita end-to-end di ML. Ci√≤ include tutto, dalla preparazione dei dati e dal training del modello alla distribuzione e alla manutenzione. MLOps garantisce che i modelli ML siano sviluppati, distribuiti e mantenuti in modo efficiente ed efficace.</p>
<p>Cominciamo prendendo un caso di esempio generale (ad esempio, ML non edge). Prendiamo in considerazione un‚Äôazienda di ‚Äúride sharing‚Äù che desidera distribuire un modello di machine learning per prevedere la domanda dei passeggeri in tempo reale. Il team di data science impiega mesi per sviluppare un modello, ma quando √® il momento di distribuirlo, si rende conto che deve essere compatibile con l‚Äôambiente di produzione del team di ingegneria. La distribuzione del modello richiede la ricostruzione da zero, il che comporta settimane di lavoro aggiuntivo. √à qui che entra in gioco MLOps.</p>
<p>Con MLOps, protocolli e strumenti, il modello sviluppato dal team di data science pu√≤ essere distribuito e integrato senza problemi nell‚Äôambiente di produzione. In sostanza, MLOps elimina gli attriti durante lo sviluppo, la distribuzione e la manutenzione dei sistemi ML. Migliora la collaborazione tra i team tramite flussi di lavoro e interfacce definiti. MLOps accelera anche la velocit√† di iterazione consentendo la distribuzione continua per i modelli ML.</p>
<p>Per l‚Äôazienda di ride sharing, implementare MLOps significa che il loro modello di previsione della domanda pu√≤ essere frequentemente riqualificato e distribuito in base ai nuovi dati in arrivo. Ci√≤ mantiene il modello accurato nonostante il cambiamento del comportamento del passeggero. MLOps consente inoltre all‚Äôazienda di sperimentare nuove tecniche di modellazione poich√© i modelli possono essere rapidamente testati e aggiornati.</p>
<p>Altri vantaggi di MLOps includono il monitoraggio avanzato della discendenza del modello, la riproducibilit√† e l‚Äôauditing. La catalogazione dei flussi di lavoro ML e la standardizzazione degli artefatti, come il logging delle versioni del modello, il monitoraggio della discendenza dei dati e il confezionamento di modelli e parametri, consente una visione pi√π approfondita della provenienza del modello. La standardizzazione di questi artefatti facilita la tracciabilit√† di un modello fino alle sue origini, la replica del processo di sviluppo del modello e l‚Äôesame di come una versione del modello √® cambiata nel tempo. Ci√≤ facilita anche la conformit√† alle normative, che √® particolarmente critica in settori regolamentati come sanit√† e finanza, dove √® importante essere in grado di verificare e spiegare i modelli.</p>
<p>Le principali organizzazioni adottano MLOps per aumentare la produttivit√†, aumentare la collaborazione e accelerare i risultati ML. Fornisce i framework, gli strumenti e le best practice per gestire efficacemente i sistemi ML durante il loro ciclo di vita. Ci√≤ si traduce in modelli pi√π performanti, tempi di realizzazione pi√π rapidi e un vantaggio competitivo duraturo. Mentre esploriamo ulteriormente MLOps, si consideri come l‚Äôimplementazione di queste pratiche pu√≤ aiutare ad affrontare le sfide ML embedded oggi e in futuro.</p>
</section>
<section id="contesto-storico" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="contesto-storico"><span class="header-section-number">13.2</span> Contesto Storico</h2>
<p>MLOps affonda le sue radici in DevOps, un insieme di pratiche che combinano sviluppo software (Dev) e operazioni IT (Ops) per accorciare il ciclo di vita dello sviluppo e fornire una distribuzione ‚Äúcontinua‚Äù di software di alta qualit√†. I parallelismi tra MLOps e DevOps sono evidenti nella loro attenzione all‚Äôautomazione, alla collaborazione e al miglioramento continuo. In entrambi i casi, l‚Äôobiettivo √® quello di abbattere i ‚Äúsilos‚Äù tra i diversi team (sviluppatori, operazioni e, nel caso di MLOps, data scientist e ingegneri ML) e creare un processo pi√π snello ed efficiente. √à utile comprendere meglio la storia di questa evoluzione per comprendere MLOps nel contesto dei sistemi tradizionali.</p>
<section id="devops" class="level3" data-number="13.2.1">
<h3 data-number="13.2.1" class="anchored" data-anchor-id="devops"><span class="header-section-number">13.2.1</span> DevOps</h3>
<p>Il termine ‚ÄúDevOps‚Äù √® stato coniato per la prima volta nel 2009 da <a href="https://www.jedi.be/">Patrick Debois</a>, un consulente e professionista Agile. Debois ha organizzato la prima conferenza <a href="https://www.devopsdays.org/">DevOpsDays</a> a Ghent, in Belgio, nel 2009. La conferenza ha riunito professionisti dello sviluppo e delle operazioni per discutere di modi per migliorare la collaborazione e automatizzare i processi.</p>
<p>DevOps ha le sue radici nel movimento <a href="https://agilemanifesto.org/">Agile</a>, iniziato nei primi anni 2000. Agile ha fornito le basi per un approccio pi√π collaborativo allo sviluppo software e ha enfatizzato le piccole release iterative. Tuttavia, Agile si concentra principalmente sulla collaborazione tra team di sviluppo. Man mano che le metodologie Agile diventavano pi√π popolari, le organizzazioni si sono rese conto della necessit√† di estendere questa collaborazione ai team operativi.</p>
<p>La natura isolata dei team di sviluppo e delle operazioni ha spesso portato a inefficienze, conflitti e ritardi nella distribuzione del software. Questa necessit√† di una migliore collaborazione e integrazione tra questi team ha portato al movimento <a href="https://www.atlassian.com/devops">DevOps</a>. DevOps pu√≤ essere visto come un‚Äôestensione dei principi Agile, inclusi i team operativi.</p>
<p>I principi chiave di DevOps includono collaborazione, automazione, integrazione continua, distribuzione e feedback. DevOps si concentra sull‚Äôautomazione dell‚Äôintera pipeline di distribuzione del software, dallo sviluppo alla distribuzione. Migliora la collaborazione tra i team di sviluppo e operativi, utilizzando strumenti come <a href="https://www.jenkins.io/">Jenkins</a>, <a href="https://www.docker.com/">Docker</a> e <a href="https://kubernetes.io/">Kubernetes</a> per semplificare il ciclo di vita dello sviluppo.</p>
<p>Mentre Agile e DevOps condividono principi comuni in materia di collaborazione e feedback, DevOps mira specificamente all‚Äôintegrazione di sviluppo e operazioni IT, espandendo Agile oltre i soli team di sviluppo. Introduce pratiche e strumenti per automatizzare la distribuzione del software e migliorare la velocit√† e la qualit√† delle release del software.</p>
</section>
<section id="mlops" class="level3" data-number="13.2.2">
<h3 data-number="13.2.2" class="anchored" data-anchor-id="mlops"><span class="header-section-number">13.2.2</span> MLOps</h3>
<p><a href="https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">MLOps</a>, d‚Äôaltro canto, sta per Machine Learning Operations ed estende i principi di DevOps al ciclo di vita ML. MLOps automatizza e semplifica l‚Äôintero ciclo di vita dell‚Äôapprendimento automatico, dalla preparazione dei dati allo sviluppo del modello, fino all‚Äôimplementazione e al monitoraggio. L‚Äôobiettivo principale di MLOps √® facilitare la collaborazione tra data scientist, data engineer e operazioni IT e automatizzare la distribuzione, il monitoraggio e la gestione dei modelli ML. Alcuni fattori chiave hanno portato all‚Äôascesa di MLOps.</p>
<ul>
<li><strong>Data drift:</strong> La deriva dei dati degrada le prestazioni del modello nel tempo, motivando la necessit√† di rigorosi monitoraggi e procedure di riqualificazione automatizzate fornite da MLOps.</li>
<li><strong>Riproducibilit√†:</strong> La mancanza di riproducibilit√† negli esperimenti di machine learning ha motivato i sistemi MLOps a tracciare codice, dati e variabili di ambiente per abilitare flussi di lavoro ML riproducibili.</li>
<li><strong>Spiegabilit√†:</strong> La natura di ‚Äúscatola nera‚Äù e la mancanza di spiegabilit√† di modelli complessi hanno motivato la necessit√† di funzionalit√† MLOps per aumentare la trasparenza e la spiegabilit√† del modello.</li>
<li><strong>Monitoraggio:</strong> L‚Äôincapacit√† di monitorare in modo affidabile le prestazioni del modello dopo la distribuzione ha evidenziato la necessit√† di soluzioni MLOps con una solida strumentazione delle prestazioni del modello e avvisi.</li>
<li><strong>Attrito:</strong> L‚Äôattrito nel riaddestramento e nella distribuzione manuale dei modelli ha motivato la necessit√† di sistemi MLOps che automatizzano le pipeline di distribuzione dell‚Äôapprendimento automatico.</li>
<li><strong>Ottimizzazione:</strong> La complessit√† della configurazione dell‚Äôinfrastruttura di apprendimento automatico ha motivato la necessit√† di piattaforme MLOps con un‚Äôinfrastruttura ML ottimizzata e pronta all‚Äôuso.</li>
</ul>
<p>Sebbene DevOps e MLOps condividano l‚Äôobiettivo comune di automatizzare e semplificare i processi, differiscono significativamente in termini di attenzione e sfide. DevOps si occupa principalmente di sviluppo software e operazioni IT. Consente la collaborazione tra questi team e automatizza la distribuzione del software. Al contrario, MLOps si concentra sul ciclo di vita dell‚Äôapprendimento automatico. Affronta complessit√† aggiuntive come <a href="https://dvc.org/">versioning dei dati</a>, <a href="https://dvc.org/">versioning dei modelli</a> e <a href="https://www.fiddler.ai/">monitoraggio dei modelli</a>. MLOps richiede la collaborazione tra una gamma pi√π ampia di stakeholder, tra cui data scientist, data engineer e IT operations. Va oltre l‚Äôambito del DevOps tradizionale incorporando le sfide uniche della gestione dei modelli ML durante il loro ciclo di vita. <a href="#tbl-mlops" class="quarto-xref">Tabella&nbsp;<span>13.1</span></a> fornisce un confronto affiancato di DevOps e MLOps, evidenziandone le principali differenze e somiglianze.</p>
<div id="tbl-mlops" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-mlops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;13.1: Confronto tra DevOps e MLOps.
</figcaption>
<div aria-describedby="tbl-mlops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 42%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">DevOps</th>
<th style="text-align: left;">MLOps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Obiettivo</td>
<td style="text-align: left;">Semplificazione dei processi di sviluppo software e operativi</td>
<td style="text-align: left;">Ottimizzazione del ciclo di vita dei modelli di apprendimento automatico</td>
</tr>
<tr class="even">
<td style="text-align: left;">Metodologia</td>
<td style="text-align: left;">Integrazione continua e distribuzione continua (CI/CD) per lo sviluppo software</td>
<td style="text-align: left;">Simile a CI/CD ma incentrato sui flussi di lavoro di apprendimento automatico</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Strumenti Principali</td>
<td style="text-align: left;">Controllo delle versioni (Git), strumenti CI/CD (Jenkins, Travis CI), gestione della configurazione (Ansible, Puppet)</td>
<td style="text-align: left;">Strumenti di versioning dei dati, strumenti di training e deployment dei modelli, pipeline CI/CD su misura per ML</td>
</tr>
<tr class="even">
<td style="text-align: left;">Problemi Principali</td>
<td style="text-align: left;">Integrazione del codice, test, gestione delle release, automazione, infrastruttura come codice</td>
<td style="text-align: left;">Gestione dei dati, versioning dei modelli, monitoraggio degli esperimenti, deployment dei modelli, scalabilit√† dei flussi di lavoro ML</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Risultati Tipici</td>
<td style="text-align: left;">Release software pi√π rapide e affidabili, collaborazione migliorata tra team di sviluppo e operativi</td>
<td style="text-align: left;">Gestione e deployment efficienti dei modelli di apprendimento automatico, collaborazione migliorata tra data scientist e ingegneri</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Scoprire di pi√π sui cicli di vita ML tramite un ‚Äúcase study‚Äù che presenta il riconoscimento vocale in <a href="#vid-mlops" class="quarto-xref">Video&nbsp;<span>13.1</span></a>.</p>
<div id="vid-mlops" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;13.1: MLOps
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/YJsRD_hU4tc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
</section>
<section id="componenti-chiave-di-mlops" class="level2 page-columns page-full" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="componenti-chiave-di-mlops"><span class="header-section-number">13.3</span> Componenti Chiave di MLOps</h2>
<p>In questo capitolo, forniremo una panoramica dei componenti principali di MLOps, un set emergente di pratiche che consente una distribuzione solida e una gestione del ciclo di vita dei modelli ML in produzione. Sebbene alcuni elementi MLOps, come l‚Äôautomazione e il monitoraggio, siano stati trattati nei capitoli precedenti, li integreremo in un framework e approfondiremo funzionalit√† aggiuntive, come la governance. Inoltre, descriveremo e collegheremo gli strumenti pi√π diffusi utilizzati in ogni componente, come <a href="https://labelstud.io/">LabelStudio</a> per l‚Äôetichettatura dei dati. Alla fine, speriamo che abbiate compreso la metodologia MLOps end-to-end che porta i modelli dall‚Äôideazione alla creazione di valore sostenibile all‚Äôinterno delle organizzazioni.</p>
<p><a href="#fig-ops-layers" class="quarto-xref">Figura&nbsp;<span>13.1</span></a> mostra lo stack di sistema MLOps. Il ciclo di vita MLOps inizia dalla gestione dei dati e dalle pipeline CI/CD per lo sviluppo del modello. I modelli sviluppati passano attraverso il training e la valutazione del modello. Una volta addestrati alla convergenza, la distribuzione del modello porta i modelli in produzione e pronti per essere serviti. Dopo la distribuzione, il servizio del modello reagisce alle modifiche del carico di lavoro e soddisfa gli accordi sul livello di servizio in modo economicamente conveniente quando serve milioni di utenti finali o applicazioni AI. La gestione dell‚Äôinfrastruttura garantisce che le risorse necessarie siano disponibili e ottimizzate durante l‚Äôintero ciclo di vita. Monitoraggio continuo, governance, comunicazione e collaborazione sono i restanti elementi di MLOps per garantire uno sviluppo e un funzionamento senza interruzioni dei modelli ML.</p>
<div id="fig-ops-layers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ops-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlops_overview_layers.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ops-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.1: Lo stack MLOps, inclusi Modelli ML, Framework, Orchestrazione del Modello, Infrastruttura e Hardware, illustra il flusso di lavoro end-to-end di MLOps.
</figcaption>
</figure>
</div>
<section id="sec-ops-data-mgmt" class="level3" data-number="13.3.1">
<h3 data-number="13.3.1" class="anchored" data-anchor-id="sec-ops-data-mgmt"><span class="header-section-number">13.3.1</span> Gestione dei Dati</h3>
<p>Una gestione dei dati solida e l‚Äôingegneria dei dati potenziano attivamente implementazioni <a href="https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">MLOps</a> di successo. I team acquisiscono, archiviano e preparano correttamente i dati grezzi da sensori, database, app e altri sistemi per il training e la distribuzione dei modelli.</p>
<p>I team monitorano attivamente le modifiche ai set di dati nel tempo utilizzando il controllo delle versioni con <a href="https://git-scm.com/">Git</a> e strumenti come <a href="https://github.com/">GitHub</a> o <a href="https://about.gitlab.com/">GitLab</a>. Gli scienziati dei dati collaborano alla cura dei set di dati unendo le modifiche di pi√π collaboratori. I team possono rivedere o ripristinare ogni iterazione di un set di dati, se necessario.</p>
<p>I team etichettano e annotano meticolosamente i dati utilizzando un software di etichettatura come <a href="https://labelstud.io/">LabelStudio</a>, che consente ai team distribuiti di lavorare insieme all‚Äôetichettatura dei set di dati. Man mano che le variabili target e le convenzioni di etichettatura si evolvono, i team mantengono l‚Äôaccessibilit√† alle versioni precedenti.</p>
<p>I team archiviano il set di dati non elaborato e tutte le risorse derivate su servizi cloud come <a href="https://aws.amazon.com/s3/">Amazon S3</a> o <a href="https://cloud.google.com/storage">Google Cloud Storage</a>. Questi servizi forniscono un‚Äôarchiviazione scalabile e resiliente con funzionalit√† di controllo delle versioni. I team possono impostare autorizzazioni di accesso granulari.</p>
<p>Le pipeline di dati robuste create dai team automatizzano l‚Äôestrazione, l‚Äôunione, la pulizia e la trasformazione dei dati grezzi in set di dati pronti per l‚Äôanalisi. <a href="https://www.prefect.io/">Prefect</a>, <a href="https://airflow.apache.org/">Apache Airflow</a> e <a href="https://www.getdbt.com/">dbt</a> sono orchestratori di flussi di lavoro che consentono agli ingegneri di sviluppare pipeline di elaborazione dati flessibili e riutilizzabili.</p>
<p>Ad esempio, una pipeline pu√≤ acquisire dati da database <a href="https://www.postgresql.org/">PostgreSQL</a>, API REST e CSV archiviati su S3 [Simple Storage Service]. Pu√≤ filtrare, deduplicare e aggregare i dati, gestire gli errori e salvare l‚Äôoutput su S3. La pipeline pu√≤ anche spingere i dati trasformati in un feature store come <a href="https://www.tecton.ai/">Tecton</a> o <a href="https://feast.dev/">Feast</a> per un accesso a bassa latenza.</p>
<p>In un caso d‚Äôuso di manutenzione predittiva industriale, i dati dei sensori vengono acquisiti dai dispositivi in S3. Una pipeline perfetta elabora i dati dei sensori, unendoli ai record di manutenzione. Il set di dati arricchito √® archiviato in Feast in modo che i modelli possano recuperare facilmente i dati pi√π recenti per l‚Äôaddestramento e le previsioni.</p>
<p><a href="#vid-datapipe" class="quarto-xref">Video&nbsp;<span>13.2</span></a> di seguito riporta una breve panoramica delle pipeline di dati.</p>
<div id="vid-datapipe" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;13.2: Pipeline di Dati
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/gz-44N3MMOA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
<section id="pipeline-cicd" class="level3" data-number="13.3.2">
<h3 data-number="13.3.2" class="anchored" data-anchor-id="pipeline-cicd"><span class="header-section-number">13.3.2</span> Pipeline CI/CD</h3>
<p>Le pipeline di integrazione continua e distribuzione continua (CI/CD) automatizzano attivamente la progressione dei modelli ML dallo sviluppo iniziale alla distribuzione in produzione. Adattati per i sistemi ML, i principi CI/CD consentono ai team di distribuire rapidamente e in modo robusto nuovi modelli con errori manuali ridotti al minimo.</p>
<p>Le pipeline CI/CD orchestrano i passaggi chiave, tra cui il controllo delle nuove modifiche al codice, la trasformazione dei dati, il training e la registrazione di nuovi modelli, i test di convalida, la containerizzazione, la distribuzione in ambienti come cluster di staging e la promozione in produzione. I team sfruttano le soluzioni CI/CD pi√π diffuse come <a href="https://www.jenkins.io/">Jenkins</a>, <a href="https://circleci.com/">CircleCI</a> e <a href="https://github.com/features/actions">GitHub Actions</a> per eseguire queste pipeline MLOps, mentre <a href="https://www.prefect.io/">Prefect</a>, <a href="https://metaflow.org/">Metaflow</a> e <a href="https://www.kubeflow.org/">Kubeflow</a> offrono opzioni incentrate su ML.</p>
<p><a href="#fig-ops-cicd" class="quarto-xref">Figura&nbsp;<span>13.2</span></a> illustra una pipeline CI/CD specificamente pensata per MLOps. Il processo inizia con un dataset e un repository di feature (a sinistra), che alimenta una fase di ingestione del dataset. Dopo l‚Äôingestione, i dati vengono sottoposti a convalida per garantirne la qualit√† prima di essere trasformati per l‚Äôaddestramento. Parallelamente, un trigger di riaddestramento pu√≤ avviare la pipeline in base a criteri specificati. I dati passano poi attraverso una fase di addestramento/ottimizzazione del modello all‚Äôinterno di un motore di elaborazione dati, seguita dalla valutazione e convalida del modello. Una volta convalidato, il modello viene registrato e archiviato in un repository di metadati e artefatti di apprendimento automatico. La fase finale prevede la distribuzione del modello addestrato nuovamente nel dataset e nel repository di feature, creando cos√¨ un processo ciclico per il miglioramento continuo e la distribuzione di modelli di apprendimento automatico.</p>
<div id="fig-ops-cicd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ops-cicd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/cicd_pipelines.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ops-cicd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.2: Diagramma CI/CD MLOps. Fonte: HarvardX.
</figcaption>
</figure>
</div>
<p>Ad esempio, quando uno scienziato dei dati verifica i miglioramenti a un modello di classificazione delle immagini in un repository <a href="https://github.com/">GitHub</a>, questo attiva attivamente una pipeline CI/CD Jenkins. La pipeline riesegue le trasformazioni dei dati e l‚Äôaddestramento del modello sui dati pi√π recenti, monitorando gli esperimenti con <a href="https://mlflow.org/">MLflow</a>. Dopo i test di validazione automatizzati, i team distribuiscono il contenitore del modello in un cluster di staging <a href="https://kubernetes.io/">Kubernetes</a> per un ulteriore controllo qualit√†. Una volta approvato, Jenkins facilita un rollout graduale del modello in produzione con <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments">distribuzioni canary</a> per rilevare eventuali problemi. Se vengono rilevate anomalie, la pipeline consente ai team di tornare alla versione precedente del modello in modo fluido.</p>
<p>Le pipeline CI/CD consentono ai team di iterare e distribuire rapidamente modelli ML collegando i diversi passaggi dallo sviluppo alla distribuzione con automazione continua. L‚Äôintegrazione di strumenti MLOps come MLflow migliora il packaging del modello, il controllo delle versioni e la tracciabilit√† della pipeline. CI/CD √® fondamentale per far progredire i modelli oltre i prototipi in sistemi aziendali sostenibili.</p>
</section>
<section id="addestramento-del-modello" class="level3" data-number="13.3.3">
<h3 data-number="13.3.3" class="anchored" data-anchor-id="addestramento-del-modello"><span class="header-section-number">13.3.3</span> Addestramento del Modello</h3>
<p>Nella fase di training del modello, gli scienziati dei dati sperimentano attivamente diverse architetture e algoritmi ML per creare modelli ottimizzati che estraggono informazioni e modelli dai dati. MLOps introduce best practice e automazione per rendere questo processo iterativo pi√π efficiente e riproducibile.</p>
<p>I moderni framework ML come <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://pytorch.org/">PyTorch</a> e <a href="https://keras.io/">Keras</a> forniscono componenti predefiniti che semplificano la progettazione di reti neurali e altre architetture di modelli. Gli scienziati dei dati sfruttano moduli integrati per layer, attivazioni, perdite, ecc. e API di alto livello come Keras per concentrarsi maggiormente sull‚Äôarchitettura del modello.</p>
<p>MLOps consente ai team di impacchettare il codice di training del modello in script e notebook riutilizzabili e tracciati. Man mano che i modelli vengono sviluppati, funzionalit√† come <a href="https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview">ottimizzazione degli iperparametri</a>, <a href="https://arxiv.org/abs/1808.05377">ricerca dell‚Äôarchitettura neurale</a> e <a href="https://scikit-learn.org/stable/modules/feature_selection.html">selezione automatica delle funzionalit√†</a> si ripetono rapidamente per trovare le configurazioni pi√π performanti.</p>
<p>I team utilizzano Git per controllare le versioni del codice di training e ospitarlo in repository come GitHub per tenere traccia delle modifiche nel tempo. Ci√≤ consente una collaborazione fluida tra i data scientist.</p>
<p>Notebook come <a href="https://jupyter.org/">Jupyter</a> creano un eccellente ambiente di sviluppo di modelli interattivi. I notebook contengono l‚Äôinserimento dei dati, la pre-elaborazione, la dichiarazione del modello, il ciclo di training, la valutazione e il codice di esportazione in un documento riproducibile.</p>
<p>Infine, i team orchestrano il training del modello come parte di una pipeline CI/CD per l‚Äôautomazione. Ad esempio, una pipeline Jenkins pu√≤ attivare uno script Python per caricare nuovi dati di training, riaddestrare un classificatore TensorFlow, valutare le metriche del modello e registrare automaticamente il modello se vengono raggiunte le soglie di prestazione.</p>
<p>Un esempio di flusso di lavoro prevede che uno scienziato dei dati utilizzi un notebook PyTorch per sviluppare un modello CNN per la classificazione delle immagini. La libreria <a href="https://www.fast.ai/">fastai</a> fornisce API di alto livello per semplificare l‚Äôaddestramento delle CNN sui set di dati delle immagini. Il notebook addestra il modello sui dati campione, valuta le metriche di accuratezza e ottimizza gli iperparametri come la velocit√† di apprendimento e i layer per ottimizzare le prestazioni. Questo notebook riproducibile √® controllato dalla versione e integrato in una pipeline di riaddestramento.</p>
<p>L‚Äôautomazione e la standardizzazione dell‚Äôaddestramento del modello consentono ai team di accelerare la sperimentazione e raggiungere il rigore necessario per produrre sistemi ML.</p>
</section>
<section id="valutazione-del-modello" class="level3" data-number="13.3.4">
<h3 data-number="13.3.4" class="anchored" data-anchor-id="valutazione-del-modello"><span class="header-section-number">13.3.4</span> Valutazione del Modello</h3>
<p>Prima di distribuire i modelli, i team eseguono una valutazione e dei test rigorosi per convalidare i benchmark delle prestazioni e la prontezza per il rilascio. MLOps introduce le best practice relative alla convalida, all‚Äôaudit e ai <a href="https://martinfowler.com/bliki/CanaryRelease.html">test canary</a> dei modelli.</p>
<p>In genere, i team valutano i modelli rispetto ai <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">dataset di test</a> di holdout che non vengono utilizzati durante il training. I dati di test provengono dalla stessa distribuzione dei dati di produzione. I team calcolano metriche come <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuratezza</a>, <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">AUC</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precisione</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">richiamo</a> e <a href="https://en.wikipedia.org/wiki/F1_score">punteggio F1</a>.</p>
<p>I team monitorano inoltre le stesse metriche nel tempo rispetto ai campioni di dati di test. Se i dati di valutazione provengono da flussi di produzione live, questo rileva le <a href="https://www.ibm.com/cloud/learn/data-drift">derive dei dati</a> che degradano le prestazioni del modello nel tempo.</p>
<p>La supervisione umana per il rilascio del modello rimane importante. Gli scienziati dei dati esaminano le prestazioni nei segmenti e nelle sezioni chiave. L‚Äôanalisi degli errori aiuta a identificare i punti deboli del modello per guidare il miglioramento. team applicano tecniche di <a href="https://developers.google.com/machine-learning/fairness-overview">equit√†</a> e <a href="https://developers.google.com/machine-learning/fairness-overview">rilevamento di bias</a>.</p>
<p>Il test Canary rilascia un modello a un piccolo sottoinsieme di utenti per valutare le prestazioni nel mondo reale prima di un‚Äôampia distribuzione. I team indirizzano gradualmente il traffico alla versione canary monitorando i problemi.</p>
<p>Ad esempio, un rivenditore valuta un modello di raccomandazione di prodotto personalizzato rispetto ai dati di test storici, esaminando le metriche di accuratezza e diversit√†. I team calcolano anche le metriche sui dati dei clienti in tempo reale nel tempo, rilevando una riduzione dell‚Äôaccuratezza nelle ultime 2 settimane. Prima dell‚Äôimplementazione completa, il nuovo modello viene rilasciato al 5% del traffico web per garantire che non vi sia alcun degrado.</p>
<p>L‚Äôautomazione della valutazione e delle versioni canary riduce i rischi di distribuzione. Tuttavia, la revisione umana deve ancora essere pi√π critica per valutare le dinamiche meno quantificabili del comportamento del modello. Una rigorosa convalida pre-distribuzione fornisce sicurezza nell‚Äôimmissione dei modelli in produzione.</p>
</section>
<section id="distribuzione-del-modello" class="level3" data-number="13.3.5">
<h3 data-number="13.3.5" class="anchored" data-anchor-id="distribuzione-del-modello"><span class="header-section-number">13.3.5</span> Distribuzione del Modello</h3>
<p>I team devono confezionare, testare e tracciare correttamente i modelli ML per distribuirli in modo affidabile in produzione. MLOps introduce framework e procedure per il versioning attivo, la distribuzione, il monitoraggio e l‚Äôaggiornamento dei modelli in modi sostenibili.</p>
<p>I team containerizzano i modelli utilizzando <a href="https://www.docker.com/">Docker</a>, che raggruppa codice, librerie e dipendenze in un‚Äôunit√† standardizzata. I container consentono una portabilit√† fluida tra gli ambienti.</p>
<p>Framework come <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> e <a href="https://bentoml.org/">BentoML</a> aiutano a servire le previsioni dai modelli distribuiti tramite API ottimizzate per le prestazioni. Questi framework gestiscono il versioning, il ridimensionamento e il monitoraggio.</p>
<p>I team distribuiscono prima i modelli aggiornati in ambienti di staging o QA per i test prima del rollout completo in produzione. Le distribuzioni shadow o canary instradano un campione di traffico per testare le varianti del modello. I team aumentano gradualmente l‚Äôaccesso ai nuovi modelli.</p>
<p>I team creano solide procedure di rollback nel caso in cui emergano problemi. I rollback ripristinano l‚Äôultima versione valida del modello. L‚Äôintegrazione con pipeline CI/CD semplifica la ridistribuzione, se necessario.</p>
<p>I team monitorano attentamente gli artefatti del modello, come script, pesi, log e metriche, per ogni versione con strumenti di metadati ML come <a href="https://mlflow.org/">MLflow</a>. Ci√≤ mantiene la discendenza e la verificabilit√†.</p>
<p>Ad esempio, un rivenditore inserisce in un ‚Äúcontenitore‚Äù un modello di raccomandazione di prodotto in <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> e lo distribuisce in un cluster di staging <a href="https://kubernetes.io/">Kubernetes</a>. Dopo aver monitorato e approvato le prestazioni sul traffico di esempio, Kubernetes sposta il 10% del traffico di produzione al nuovo modello. Se non vengono rilevati problemi dopo alcuni giorni, il nuovo modello occupa il 100% del traffico. Tuttavia, i team dovrebbero mantenere la versione precedente accessibile per il rollback, se necessario.</p>
<p>I processi di distribuzione del modello consentono ai team di rendere i sistemi ML resilienti in produzione tenendo conto di tutti gli stati di transizione.</p>
</section>
<section id="model-serving" class="level3 page-columns page-full" data-number="13.3.6">
<h3 data-number="13.3.6" class="anchored" data-anchor-id="model-serving"><span class="header-section-number">13.3.6</span> Model Serving</h3>
<p>Dopo il ‚Äúdeployment‚Äù [distribuzione] del modello, ML-as-a-Service diventa un componente fondamentale nel ciclo di vita di MLOps. I servizi online come Facebook/Meta gestiscono decine di trilioni di query di inferenza al giorno <span class="citation" data-cites="wu2019machine">(<a href="../../references.html#ref-wu2019machine" role="doc-biblioref">Wu et al. 2019</a>)</span>. Il ‚Äúmodel serving‚Äù colma il divario tra i modelli sviluppati e le applicazioni ML o gli utenti finali, assicurando che i modelli distribuiti siano accessibili, performanti e scalabili negli ambienti di produzione.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wu2019machine" class="csl-entry" role="listitem">
Wu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan, Kim Hazelwood, et al. 2019. <span>¬´Machine Learning at Facebook: Understanding Inference at the Edge¬ª</span>. In <em>2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</em>, 331‚Äì44. IEEE; IEEE. <a href="https://doi.org/10.1109/hpca.2019.00048">https://doi.org/10.1109/hpca.2019.00048</a>.
</div></div><p>Diversi framework facilitano il model serving, tra cui <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a>, <a href="https://developer.nvidia.com/triton-inference-server">NVIDIA Triton Inference Server</a> e <a href="https://kserve.github.io/website/latest/">KServe</a> (in precedenza KFServing). Questi strumenti forniscono interfacce standardizzate per la distribuzione di modelli distribuiti su varie piattaforme e gestiscono molte complessit√† dell‚Äôinferenza del modello su larga scala.</p>
<p>Il model serving pu√≤ essere categorizzato in tre tipi principali:</p>
<ol type="1">
<li><strong>Online Serving:</strong> Fornisce previsioni in tempo reale con bassa latenza, il che √® fondamentale per applicazioni come sistemi di raccomandazione o rilevamento frodi.</li>
<li><strong>Offline Serving:</strong> Elabora grandi batch di dati in modo asincrono, adatto per attivit√† come la generazione periodica di report.</li>
<li><strong>Near-Online Serving (semi-sincrono):</strong> Bilancia tra online e offline, offrendo risposte relativamente rapide per applicazioni meno sensibili al tempo come i chatbot.</li>
</ol>
<p>Una delle sfide principali per i sistemi di model serving √® operare secondo requisiti di prestazioni definiti da Service Level Agreement (SLA) e Service Level Objective (SLO). Gli SLA sono contratti formali che specificano i livelli di servizio previsti. Questi livelli di servizio si basano su parametri quali tempo di risposta, disponibilit√† e produttivit√†. Gli SLO sono obiettivi interni che i team si prefiggono di soddisfare o superare i propri SLA.</p>
<p>Per il model serving ML, gli accordi e gli obiettivi SLA e SLO hanno un impatto diretto sull‚Äôesperienza utente, sull‚Äôaffidabilit√† del sistema e sui risultati aziendali. Pertanto, i team ottimizzano attentamente la propria piattaforma di servizio. I serving system ML impiegano varie tecniche per ottimizzare le prestazioni e l‚Äôutilizzo delle risorse, come le seguenti:</p>
<ol type="1">
<li><strong>Pianificazione e batch delle richieste:</strong> Gestisce in modo efficiente le richieste di inferenza ML in arrivo, ottimizzando le prestazioni tramite strategie di accodamento e raggruppamento intelligenti. Sistemi come Clipper <span class="citation" data-cites="crankshaw2017clipper">(<a href="../../references.html#ref-crankshaw2017clipper" role="doc-biblioref">Crankshaw et al. 2017</a>)</span> introducono il servizio di previsione online a bassa latenza con tecniche di caching e batch.</li>
<li><strong>Selezione e routing delle istanze del modello:</strong> Algoritmi intelligenti indirizzano le richieste alle versioni o alle istanze del modello appropriate. INFaaS <span class="citation" data-cites="romero2021infaas">(<a href="../../references.html#ref-romero2021infaas" role="doc-biblioref">Romero et al. 2021</a>)</span> esplora questo aspetto generando varianti del modello e navigando in modo efficiente nello spazio di compromesso in base ai requisiti di prestazioni e accuratezza.</li>
<li><strong>Bilanciamento del carico:</strong> Distribuisce i carichi di lavoro in modo uniforme su pi√π istanze di servizio. MArk (Model Ark) <span class="citation" data-cites="zhang2019mark">(<a href="../../references.html#ref-zhang2019mark" role="doc-biblioref">C. Zhang et al. 2019</a>)</span> dimostra tecniche efficaci di bilanciamento del carico per sistemi di servizio ML.</li>
<li><strong>Autoscaling delle istanze del modello:</strong> Regola dinamicamente la capacit√† in base alla domanda. Sia INFaaS <span class="citation" data-cites="romero2021infaas">(<a href="../../references.html#ref-romero2021infaas" role="doc-biblioref">Romero et al. 2021</a>)</span> che MArk <span class="citation" data-cites="zhang2019mark">(<a href="../../references.html#ref-zhang2019mark" role="doc-biblioref">C. Zhang et al. 2019</a>)</span> incorporano funzionalit√† di autoscaling per gestire in modo efficiente le fluttuazioni del carico di lavoro.</li>
<li><strong>Orchestration del modello:</strong> Gestisce l‚Äôesecuzione del modello, abilitando l‚Äôelaborazione parallela e l‚Äôallocazione strategica delle risorse. AlpaServe <span class="citation" data-cites="li2023alpaserve">(<a href="../../references.html#ref-li2023alpaserve" role="doc-biblioref">Z. Li et al. 2023</a>)</span> dimostra tecniche avanzate per la gestione di modelli di grandi dimensioni e scenari di servizio complessi.</li>
<li><strong>Previsione del tempo di esecuzione:</strong> Sistemi come Clockwork <span class="citation" data-cites="gujarati2020serving">(<a href="../../references.html#ref-gujarati2020serving" role="doc-biblioref">Gujarati et al. 2020</a>)</span> si concentrano sul servizio ad alte prestazioni prevedendo i tempi di esecuzione delle singole inferenze e utilizzando in modo efficiente gli acceleratori hardware.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-crankshaw2017clipper" class="csl-entry" role="listitem">
Crankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, e Ion Stoica. 2017. <span>¬´Clipper: A <span class="math inline">\(\{\)</span>Low-Latency<span class="math inline">\(\}\)</span> online prediction serving system¬ª</span>. In <em>14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)</em>, 613‚Äì27.
</div><div id="ref-romero2021infaas" class="csl-entry" role="listitem">
Romero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, e Christos Kozyrakis. 2021. <span>¬´INFaaS: Automated Model-less Inference Serving.¬ª</span> In <em>2021 USENIX Annual Technical Conference (USENIX ATC 21)</em>, 397‚Äì411. <a href="https://www.usenix.org/conference/atc21/presentation/romero">https://www.usenix.org/conference/atc21/presentation/romero</a>.
</div><div id="ref-zhang2019mark" class="csl-entry" role="listitem">
Zhang, Chengliang, Minchen Yu, Wei Wang 0030, e Feng Yan 0001. 2019. <span>¬´MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving.¬ª</span> In <em>2019 USENIX Annual Technical Conference (USENIX ATC 19)</em>, 1049‚Äì62. <a href="https://www.usenix.org/conference/atc19/presentation/zhang-chengliang">https://www.usenix.org/conference/atc19/presentation/zhang-chengliang</a>.
</div><div id="ref-li2023alpaserve" class="csl-entry" role="listitem">
Li, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, et al. 2023. <span>¬´<span class="math inline">\(\{\)</span>AlpaServe<span class="math inline">\(\}\)</span>: Statistical multiplexing with model parallelism for deep learning serving¬ª</span>. In <em>17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)</em>, 663‚Äì79.
</div><div id="ref-gujarati2020serving" class="csl-entry" role="listitem">
Gujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, e Jonathan Mace. 2020. <span>¬´Serving DNNs like Clockwork: Performance Predictability from the Bottom Up.¬ª</span> In <em>14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)</em>, 443‚Äì62. <a href="https://www.usenix.org/conference/osdi20/presentation/gujarati">https://www.usenix.org/conference/osdi20/presentation/gujarati</a>.
</div></div><p>I serving system ML che eccellono in queste aree consentono alle organizzazioni di distribuire modelli che funzionano in modo affidabile sotto pressione. Il risultato sono applicazioni AI scalabili e reattive in grado di gestire le richieste del mondo reale e fornire valore in modo coerente.</p>
</section>
<section id="gestione-dellinfrastruttura" class="level3" data-number="13.3.7">
<h3 data-number="13.3.7" class="anchored" data-anchor-id="gestione-dellinfrastruttura"><span class="header-section-number">13.3.7</span> Gestione dell‚ÄôInfrastruttura</h3>
<p>I team MLOps sfruttano ampiamente gli strumenti ‚Äú<a href="https://www.infoworld.com/article/3271126/what-is-iac-infrastructure-as-code-explained.html">infrastructure as code (IaC)</a>‚Äù e le solide architetture cloud per gestire attivamente le risorse necessarie per lo sviluppo, il training e la distribuzione dei sistemi ML.</p>
<p>I team utilizzano strumenti IaC come <a href="https://www.terraform.io/">Terraform</a>, <a href="https://aws.amazon.com/cloudformation/">CloudFormation</a> e <a href="https://www.ansible.com/">Ansible</a> per definire, fornire e aggiornare a livello di programmazione l‚Äôinfrastruttura in modo controllato dalla versione. Per MLOps, i team utilizzano ampiamente Terraform per avviare risorse su <a href="https://aws.amazon.com/">AWS</a>, <a href="https://cloud.google.com/">GCP</a> e <a href="https://azure.microsoft.com/">Azure</a>.</p>
<p>Per la creazione e il training dei modelli, i team forniscono dinamicamente risorse di elaborazione come server GPU, cluster di container, storage e database tramite Terraform in base alle esigenze degli scienziati dei dati. Il codice incapsula e preserva le definizioni dell‚Äôinfrastruttura.</p>
<p>I container e gli orchestratori come Docker e Kubernetes consentono ai team di impacchettare modelli e distribuirli in modo affidabile in diversi ambienti. I contenitori possono essere attivati o disattivati automaticamente in base alla domanda.</p>
<p>Sfruttando l‚Äôelasticit√† del cloud, i team aumentano o diminuiscono le risorse per soddisfare i picchi nei carichi di lavoro come i lavori di ottimizzazione degli iperparametri o i picchi nelle richieste di previsione. <a href="https://aws.amazon.com/autoscaling/">Auto-scaling</a> consente un‚Äôefficienza dei costi ottimizzata.</p>
<p>L‚Äôinfrastruttura si estende su dispositivi on-prem, cloud ed edge. Uno stack tecnologico robusto offre flessibilit√† e resilienza. Gli strumenti di monitoraggio consentono ai team di osservare l‚Äôutilizzo delle risorse.</p>
<p>Ad esempio, una configurazione Terraform pu√≤ distribuire un cluster GCP Kubernetes per ospitare modelli TensorFlow addestrati esposti come microservizi di previsione. Il cluster aumenta i pod per gestire un traffico maggiore. L‚Äôintegrazione CI/CD distribuisce senza problemi nuovi contenitori di modelli.</p>
<p>La gestione attenta dell‚Äôinfrastruttura tramite IaC e monitoraggio consente ai team di prevenire i colli di bottiglia nell‚Äôoperativit√† dei sistemi ML su larga scala.</p>
</section>
<section id="monitoraggio" class="level3" data-number="13.3.8">
<h3 data-number="13.3.8" class="anchored" data-anchor-id="monitoraggio"><span class="header-section-number">13.3.8</span> Monitoraggio</h3>
<p>I team MLOps mantengono attivamente un monitoraggio robusto per mantenere la visibilit√† nei modelli ML distribuiti in produzione. Il monitoraggio continuo fornisce informazioni sulle prestazioni del modello e del sistema in modo che i team possano rilevare e risolvere rapidamente i problemi per ridurre al minimo le interruzioni.</p>
<p>I team monitorano attivamente gli aspetti chiave del modello, inclusa l‚Äôanalisi di campioni di previsioni live per tracciare metriche come accuratezza e <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html">matrice di confusione</a> nel tempo.</p>
<p>Quando monitorano le prestazioni, i team devono profilare i dati in arrivo per verificare la deriva del modello, un calo costante dell‚Äôaccuratezza del modello dopo l‚Äôimplementazione in produzione. La deriva del modello pu√≤ verificarsi in due modi: <a href="https://en.wikipedia.org/wiki/Concept_drift">deriva del concetto</a> e deriva dei dati. La deriva del concetto si riferisce a un cambiamento fondamentale osservato nella relazione tra i dati di input e quelli target. Ad esempio, con l‚Äôavanzare della pandemia di COVID-19, i siti di e-commerce e vendita al dettaglio hanno dovuto correggere le raccomandazioni del modello poich√© i dati di acquisto erano ampiamente distorti verso articoli come il disinfettante per le mani. La deriva dei dati descrive i cambiamenti nella distribuzione dei dati nel tempo. Ad esempio, gli algoritmi di riconoscimento delle immagini utilizzati nelle auto a guida autonoma devono tenere conto della stagionalit√† nell‚Äôosservazione dell‚Äôambiente circostante. I team monitorano anche le metriche delle prestazioni delle applicazioni come latenza ed errori per le integrazioni dei modelli.</p>
<p>Da una prospettiva infrastrutturale, i team monitorano i problemi di capacit√† come elevato utilizzo di CPU, memoria e disco e interruzioni del sistema. Strumenti come <a href="https://prometheus.io/">Prometheus</a>, <a href="https://grafana.com/">Grafana</a> ed <a href="https://www.elastic.co/">Elastic</a> consentono ai team di raccogliere, analizzare, interrogare e visualizzare attivamente diverse metriche di monitoraggio. Le dashboard rendono le dinamiche altamente visibili.</p>
<p>I team configurano gli allarmi per le metriche di monitoraggio chiave come cali di accuratezza e guasti del sistema per consentire una risposta proattiva agli eventi che minacciano l‚Äôaffidabilit√†. Ad esempio, i cali di accuratezza del modello attivano avvisi per i team per esaminare potenziali deviazioni dei dati e riaddestrare i modelli utilizzando campioni di dati aggiornati e rappresentativi.</p>
<p>Dopo la distribuzione, il monitoraggio completo consente ai team di mantenere la fiducia nello stato del modello e del sistema. Consente ai team di rilevare e risolvere preventivamente le deviazioni tramite allarmi e dashboard basati sui dati. Il monitoraggio attivo √® essenziale per mantenere sistemi ML altamente disponibili e affidabili.</p>
<p>Guardare il video qui sotto per saperne di pi√π sul monitoraggio.</p>
<div id="vid-monitoring" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;13.3: Monitoraggio del Modello
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/hq_XyP9y0xg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
<section id="governance" class="level3" data-number="13.3.9">
<h3 data-number="13.3.9" class="anchored" data-anchor-id="governance"><span class="header-section-number">13.3.9</span> Governance</h3>
<p>I team MLOps stabiliscono attivamente pratiche di governance appropriate come componente fondamentale. La governance fornisce una supervisione sui modelli ML per garantire che siano affidabili, etici e conformi. Senza governance, sussistono rischi significativi di modelli che si comportano in modi pericolosi o proibiti quando vengono distribuiti in applicazioni e processi aziendali.</p>
<p>La governance MLOps impiega tecniche per fornire trasparenza sulle previsioni, sulle prestazioni e sul comportamento del modello durante l‚Äôintero ciclo di vita ML. Metodi di spiegabilit√† come <a href="https://github.com/slundberg/shap">SHAP</a> e <a href="https://github.com/marcotcr/lime">LIME</a> aiutano gli auditor a comprendere perch√© i modelli effettuano determinate previsioni evidenziando le caratteristiche di input influenti alla base delle decisioni. <a href="https://developers.google.com/machine-learning/fairness-overview">Bias detection</a> analizza le prestazioni del modello in diversi gruppi demografici definiti da attributi come et√†, sesso ed etnia per rilevare eventuali distorsioni sistematiche. I team eseguono rigorose procedure di test su set di dati rappresentativi per convalidare le prestazioni del modello prima della distribuzione.</p>
<p>Una volta in produzione, i team monitorano la <a href="https://en.wikipedia.org/wiki/Concept_drift">concept drift</a> [deriva del concetto] per determinare se le relazioni predittive cambiano nel tempo in modi che degradano l‚Äôaccuratezza del modello. I team analizzano anche i registri di produzione per scoprire modelli nei tipi di errori generati dai modelli. La documentazione sulla provenienza dei dati, le procedure di sviluppo e le metriche di valutazione fornisce ulteriore visibilit√†.</p>
<p>Piattaforme come <a href="https://www.ibm.com/cloud/watson-openscale">Watson OpenScale</a> incorporano funzionalit√† di governance come il monitoraggio dei bias e la spiegabilit√† direttamente nella creazione di modelli, nei test e nel monitoraggio della produzione. Le aree di interesse principali della governance sono trasparenza, correttezza e conformit√†. Ci√≤ riduce al minimo i rischi che i modelli si comportino in modo errato o pericoloso quando integrati nei processi aziendali. L‚Äôintegrazione di pratiche di governance nei flussi di lavoro MLOps consente ai team di garantire un‚ÄôIA affidabile.</p>
</section>
<section id="comunicazione-e-collaborazione" class="level3" data-number="13.3.10">
<h3 data-number="13.3.10" class="anchored" data-anchor-id="comunicazione-e-collaborazione"><span class="header-section-number">13.3.10</span> Comunicazione e Collaborazione</h3>
<p>MLOps abbatte attivamente i ‚Äúsilos‚Äù e consente il libero flusso di informazioni e approfondimenti tra i team in tutte le fasi del ciclo di vita ML. Strumenti come <a href="https://mlflow.org/">MLflow</a>, <a href="https://wandb.ai/">Weights &amp; Biases</a> e contesti di dati forniscono tracciabilit√† e visibilit√† per migliorare la collaborazione.</p>
<p>I team utilizzano MLflow per sistematizzare il monitoraggio di esperimenti, versioni e artefatti del modello. Gli esperimenti possono essere loggati a livello di programmazione da notebook di data science e job di training. Il registro dei modelli fornisce un hub centrale per i team per archiviare modelli pronti per la produzione prima della distribuzione, con metadati come descrizioni, metriche, tag e discendenza. Le integrazioni con <a href="https://github.com/">Github</a>, <a href="https://about.gitlab.com/">GitLab</a> facilitano i trigger per la modifica del codice.</p>
<p>‚ÄúWeights &amp; Biases‚Äù fornisce strumenti collaborativi su misura per i team ML. Gli scienziati dei dati registrano gli esperimenti, visualizzano metriche come curve di perdita e condividono approfondimenti sulla sperimentazione con i colleghi. Le dashboard di confronto evidenziano le differenze del modello. I team discutono dei progressi e dei passaggi successivi.</p>
<p>La definizione di contesti di dati condivisi, ovvero glossari, <a href="https://en.wikipedia.org/wiki/Data_dictionary">dizionari di dati</a> e riferimenti di schemi, garantisce l‚Äôallineamento del significato e dell‚Äôutilizzo dei dati tra i ruoli. La documentazione aiuta a comprendere chi non ha accesso diretto ai dati.</p>
<p>Ad esempio, uno scienziato dei dati pu√≤ utilizzare ‚ÄúWeights &amp; Biases‚Äù per analizzare un esperimento con un modello di rilevamento delle anomalie e condividere i risultati della valutazione con altri membri del team per discutere dei miglioramenti. Il modello finale pu√≤ quindi essere registrato con MLflow prima di essere consegnato per la distribuzione.</p>
<p>L‚Äôabilitazione della trasparenza, della tracciabilit√† e della comunicazione tramite MLOps consente ai team di rimuovere i colli di bottiglia e accelerare la distribuzione di sistemi ML di impatto.</p>
<p><a href="#vid-deploy" class="quarto-xref">Video&nbsp;<span>13.4</span></a> affronta le sfide chiave nella distribuzione del modello, tra cui la deriva del concetto, la deriva del modello e i problemi di ingegneria del software.</p>
<div id="vid-deploy" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video&nbsp;13.4: Sfide della Distribuzione
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/UyEtTyeahus" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
</section>
<section id="debito-tecnico-nascosto-nei-sistemi-ml" class="level2 page-columns page-full" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="debito-tecnico-nascosto-nei-sistemi-ml"><span class="header-section-number">13.4</span> Debito Tecnico Nascosto nei Sistemi ML</h2>
<p>Il debito tecnico [https://it.wikipedia.org/wiki/Debito_tecnico] √® sempre pi√π pressante per i sistemi di apprendimento automatico. Questa metafora, originariamente proposta negli anni ‚Äô90, paragona i costi a lungo termine dello sviluppo rapido del software al debito finanziario. Proprio come un debito finanziario alimenta una crescita vantaggiosa, un debito tecnico gestito con attenzione consente una rapida iterazione. Tuttavia, se non controllato, l‚Äôaccumulo di debito tecnico pu√≤ superare qualsiasi guadagno.</p>
<p><a href="#fig-technical-debt" class="quarto-xref">Figura&nbsp;<span>13.3</span></a> illustra i vari componenti che contribuiscono al debito tecnico nascosto dei sistemi ML. Mostra la natura interconnessa di configurazione, raccolta dati ed estrazione di funzionalit√†, che √® fondamentale per la base di codice ML. Le dimensioni delle caselle indicano la proporzione dell‚Äôintero sistema rappresentata da ciascun componente. Nei sistemi ML industriali, il codice per l‚Äôalgoritmo del modello costituisce solo una piccola frazione (vedere la piccola casella nera al centro rispetto a tutte le altre caselle grandi). La complessit√† dei sistemi ML e la natura frenetica del settore rendono molto facile l‚Äôaccumulo di debito tecnico.</p>
<div id="fig-technical-debt" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-technical-debt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hidden_debt.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-technical-debt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.3: Componenti del sistema ML. Fonte: <span class="citation" data-cites="sculley2015hidden">Sambasivan et al. (<a href="../../references.html#ref-sculley2015hidden" role="doc-biblioref">2021</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<section id="erosione-dei-confini-del-modello" class="level3" data-number="13.4.1">
<h3 data-number="13.4.1" class="anchored" data-anchor-id="erosione-dei-confini-del-modello"><span class="header-section-number">13.4.1</span> Erosione dei Confini del Modello</h3>
<p>A differenza del software tradizionale, ML non ha confini chiari tra i componenti, come si vede nel diagramma sopra. Questa erosione dell‚Äôastrazione crea intrecci che esacerbano il debito tecnico in diversi modi:</p>
</section>
<section id="intreccio" class="level3" data-number="13.4.2">
<h3 data-number="13.4.2" class="anchored" data-anchor-id="intreccio"><span class="header-section-number">13.4.2</span> Intreccio</h3>
<p>Un accoppiamento stretto tra i componenti del modello ML rende difficile isolare le modifiche. La modifica di una parte provoca effetti a catena imprevedibili in tutto il sistema. ‚ÄúChanging Anything Changes Everything (noto anche come CACE)‚Äù [Cambiare qualcosa cambia tutto] √® un fenomeno che si applica a qualsiasi modifica apportata al sistema. Le potenziali mitigazioni includono la scomposizione del problema quando possibile o il monitoraggio ravvicinato delle modifiche nel comportamento per contenerne l‚Äôimpatto.</p>
</section>
<section id="cascate-di-correzione" class="level3 page-columns page-full" data-number="13.4.3">
<h3 data-number="13.4.3" class="anchored" data-anchor-id="cascate-di-correzione"><span class="header-section-number">13.4.3</span> Cascate di Correzione</h3>
<p><a href="#fig-correction-cascades-flowchart" class="quarto-xref">Figura&nbsp;<span>13.4</span></a> illustra il concetto di cascate di correzione nel flusso di lavoro ML, dalla definizione del problema all‚Äôimplementazione del modello. Gli archi rappresentano le potenziali correzioni iterative necessarie in ogni fase del flusso di lavoro, con colori diversi corrispondenti a problemi distinti come l‚Äôinterazione con la fragilit√† del mondo fisico, competenze inadeguate nel dominio dell‚Äôapplicazione, sistemi di ricompensa in conflitto e scarsa documentazione inter-organizzativa.</p>
<p>Le frecce rosse indicano l‚Äôimpatto delle cascate, che possono portare a revisioni significative nel processo di sviluppo del modello. Al contrario, la linea rossa tratteggiata rappresenta la misura drastica di abbandono del processo per riavviarlo. Questa immagine sottolinea la natura complessa e interconnessa dello sviluppo del sistema ML e l‚Äôimportanza di affrontare questi problemi all‚Äôinizio del ciclo di sviluppo per mitigare i loro effetti di amplificazione a valle.</p>
<div id="fig-correction-cascades-flowchart" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-correction-cascades-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/data_cascades.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-correction-cascades-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.4: Diagramma di flusso delle cascate di correzione. Fonte: <span class="citation" data-cites="sculley2015hidden">Sambasivan et al. (<a href="../../references.html#ref-sculley2015hidden" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-sculley2015hidden" class="csl-entry" role="listitem">
Sambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. <span>¬´<span>‚ÄúEveryone wants to do the model work, not the data work‚Äù</span>: Data Cascades in High-Stakes AI¬ª</span>. In <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>. ACM. <a href="https://doi.org/10.1145/3411764.3445518">https://doi.org/10.1145/3411764.3445518</a>.
</div></div></figure>
</div>
<p>La creazione di modelli in sequenza crea dipendenze rischiose in cui i modelli successivi si basano su quelli precedenti. Ad esempio, prendere un modello esistente e perfezionarlo per un nuovo caso d‚Äôuso sembra efficiente. Tuttavia, questo incorpora ipotesi dal modello originale che potrebbero eventualmente richiedere una correzione.</p>
<p>Diversi fattori influenzano la decisione di creare modelli in sequenza o meno:</p>
<ul>
<li><strong>Dimensioni del dataset e tasso di crescita:</strong> Con set di dati statici e di piccole dimensioni, la messa a punto dei modelli esistenti ha spesso senso. Per set di dati di grandi dimensioni e in crescita, l‚Äôaddestramento di modelli personalizzati da zero consente una maggiore flessibilit√† per tenere conto dei nuovi dati.</li>
<li><strong>Risorse di elaborazione disponibili:</strong> La messa a punto richiede meno risorse rispetto all‚Äôaddestramento di modelli di grandi dimensioni da zero. Con risorse limitate, sfruttare i modelli esistenti potrebbe essere l‚Äôunico approccio fattibile.</li>
</ul>
<p>Mentre la messa a punto dei modelli esistenti pu√≤ essere efficiente, la modifica dei componenti fondamentali in seguito diventa estremamente costosa a causa di questi effetti a cascata. Pertanto, si dovrebbe considerare attentamente l‚Äôintroduzione di nuove architetture di modelli, anche se ad alta intensit√† di risorse, per evitare cascate di correzioni in futuro. Questo approccio pu√≤ aiutare ad attenuare gli effetti di amplificazione dei problemi a valle e a ridurre il debito tecnico. Tuttavia, ci sono ancora scenari in cui la creazione di modelli sequenziali ha senso, il che richiede un attento equilibrio tra efficienza, flessibilit√† e manutenibilit√† a lungo termine nel processo di sviluppo ML.</p>
</section>
<section id="consumatori-non-dichiarati" class="level3" data-number="13.4.4">
<h3 data-number="13.4.4" class="anchored" data-anchor-id="consumatori-non-dichiarati"><span class="header-section-number">13.4.4</span> Consumatori Non Dichiarati</h3>
<p>Una volta che le previsioni del modello ML sono rese disponibili, molti sistemi downstream [derivati] potrebbero utilizzarle silenziosamente come input per un‚Äôulteriore elaborazione. Tuttavia, il modello originale non √® stato progettato per adattarsi a questo ampio riutilizzo. A causa dell‚Äôopacit√† intrinseca dei sistemi ML, diventa impossibile analizzare completamente l‚Äôimpatto degli output del modello come input altrove. Le modifiche al modello possono quindi avere conseguenze costose e pericolose interrompendo dipendenze non rilevate.</p>
<p>I ‚Äúconsumatori‚Äù non dichiarati possono anche abilitare loop di feedback nascosti se i loro output influenzano indirettamente i dati di training del modello originale. Le mitigazioni includono la limitazione dell‚Äôaccesso alle previsioni, la definizione di contratti di servizio rigorosi e il monitoraggio di segnali di influenze non-modellate. Architettare sistemi ML per incapsulare e isolare i loro effetti limita i rischi di propagazione imprevista.</p>
</section>
<section id="debito-di-dipendenza-dai-dati" class="level3" data-number="13.4.5">
<h3 data-number="13.4.5" class="anchored" data-anchor-id="debito-di-dipendenza-dai-dati"><span class="header-section-number">13.4.5</span> Debito di Dipendenza dai Dati</h3>
<p>Il debito di dipendenza dei dati si riferisce a dipendenze di dati instabili e sottoutilizzate, che possono avere ripercussioni dannose e difficili da rilevare. Sebbene questo sia un fattore chiave del debito tecnologico per il software tradizionale, tali sistemi possono trarre vantaggio dall‚Äôuso di strumenti ampiamente disponibili per l‚Äôanalisi statica da parte di compilatori e linker per identificare dipendenze di questo tipo. I sistemi ML necessitano di strumenti simili.</p>
<p>Una mitigazione per le dipendenze di dati instabili √® l‚Äôuso del versioning, che garantisce la stabilit√† degli input ma comporta il costo della gestione di pi√π set di dati e il potenziale della obsolescenza. Un‚Äôaltra mitigazione per le dipendenze di dati sottoutilizzate √® quella di condurre una valutazione esaustiva ‚Äúleave-one-feature-out‚Äù.</p>
</section>
<section id="debito-di-analisi-dai-cicli-di-feedback" class="level3" data-number="13.4.6">
<h3 data-number="13.4.6" class="anchored" data-anchor-id="debito-di-analisi-dai-cicli-di-feedback"><span class="header-section-number">13.4.6</span> Debito di Analisi dai Cicli di Feedback</h3>
<p>A differenza del software tradizionale, i sistemi ML possono cambiare il loro comportamento nel tempo, rendendo difficile l‚Äôanalisi pre-distribuzione. Questo debito si manifesta nei cicli di feedback, sia diretti che nascosti.</p>
<p>I cicli di feedback diretti si verificano quando un modello influenza i suoi input futuri, ad esempio consigliando prodotti agli utenti che, a loro volta, modellano i dati di training futuri. I cicli nascosti sorgono indirettamente tra modelli, ad esempio due sistemi che interagiscono tramite ambienti del mondo reale. I cicli di feedback graduali sono particolarmente difficili da rilevare. Questi cicli portano al debito di analisi, ovvero l‚Äôincapacit√† di prevedere come un modello agir√† completamente dopo il rilascio. Essi compromettono la validazione pre-distribuzione consentendo un‚Äôautoinfluenza non modellata.</p>
<p>Un attento monitoraggio e distribuzioni ‚Äúcanary‚Äù aiutano a rilevare il feedback. Tuttavia, permangono sfide fondamentali nella comprensione delle interazioni complesse del modello. Le scelte architettoniche che riducono l‚Äôintreccio e l‚Äôaccoppiamento mitigano l‚Äôeffetto composto del debito di analisi.</p>
</section>
<section id="le-giungle-di-pipeline" class="level3" data-number="13.4.7">
<h3 data-number="13.4.7" class="anchored" data-anchor-id="le-giungle-di-pipeline"><span class="header-section-number">13.4.7</span> Le Giungle di Pipeline</h3>
<p>I workflow [flussi di lavoro] ML spesso necessitano di interfacce pi√π standardizzate tra i componenti. Ci√≤ porta i team a ‚Äúincollare‚Äù gradualmente le pipeline con codice personalizzato. Ci√≤ che emerge sono ‚Äúgiungle di pipeline‚Äù, ovvero passaggi di pre-elaborazione aggrovigliati che sono fragili e resistono al cambiamento. Evitare modifiche a queste pipeline disordinate fa s√¨ che i team sperimentino attraverso prototipi alternativi. Presto, proliferano molteplici modi di fare. La necessit√† di astrazioni e interfacce impedisce quindi la condivisione, il riutilizzo e l‚Äôefficienza.</p>
<p>Il debito tecnico si accumula man mano che le pipeline si solidificano in vincoli legacy. I team sprecano tempo nella gestione di codice idiosincratico anzich√© massimizzare le prestazioni del modello. Principi architettonici come modularit√† e incapsulamento sono necessari per stabilire interfacce pulite. Le astrazioni condivise consentono componenti intercambiabili, impediscono il lock-in e promuovono la diffusione delle ‚Äúbest practice‚Äù tra i team. Liberarsi dalle ‚Äúgiungle di pipeline‚Äù richiede in definitiva l‚Äôapplicazione di standard che impediscano l‚Äôaccumulo di debito di astrazione. I vantaggi delle interfacce e delle API che domano la complessit√† superano i costi di transizione.</p>
</section>
<section id="debito-di-configurazione" class="level3" data-number="13.4.8">
<h3 data-number="13.4.8" class="anchored" data-anchor-id="debito-di-configurazione"><span class="header-section-number">13.4.8</span> Debito di Configurazione</h3>
<p>I sistemi ML comportano una configurazione estesa di iperparametri, architetture e altri parametri di ottimizzazione. Tuttavia, la configurazione √® spesso un ripensamento, che necessita di pi√π rigore e test: aumentano le configurazioni ad hoc, amplificate dalle numerose ‚Äúmanopole‚Äù disponibili per l‚Äôottimizzazione di modelli ML complessi.</p>
<p>Questo accumulo di debito tecnico ha diverse conseguenze. Configurazioni fragili e obsolete portano a dipendenze nascoste e bug che causano guasti di produzione. La conoscenza sulle configurazioni ottimali √® isolata anzich√© condivisa, portando a un lavoro ridondante. Riprodurre e confrontare i risultati diventa difficile quando le configurazioni mancano di documentazione. I vincoli legacy si accumulano poich√© i team temono di modificare configurazioni poco comprese.</p>
<p>Per affrontare il debito di configurazione √® necessario stabilire standard per documentare, testare, convalidare e archiviare centralmente le configurazioni. Investire in approcci pi√π automatizzati, come l‚Äôottimizzazione degli iperparametri e la ricerca dell‚Äôarchitettura, riduce la dipendenza dall‚Äôottimizzazione manuale. Una migliore igiene della configurazione rende il miglioramento iterativo pi√π gestibile impedendo alla complessit√† di aumentare all‚Äôinfinito. La chiave √® riconoscere la configurazione come parte integrante del ciclo di vita del sistema ML piuttosto che come un ripensamento ad hoc.</p>
</section>
<section id="il-mondo-che-cambia" class="level3" data-number="13.4.9">
<h3 data-number="13.4.9" class="anchored" data-anchor-id="il-mondo-che-cambia"><span class="header-section-number">13.4.9</span> Il Mondo che Cambia</h3>
<p>I sistemi ML operano in ambienti dinamici del mondo reale. Le soglie e le decisioni inizialmente efficaci diventano obsolete man mano che il mondo si evolve. Tuttavia, i vincoli legacy rendono difficile adattare i sistemi a popolazioni, modelli di utilizzo e altri fattori contestuali mutevoli.</p>
<p>Questo debito si manifesta in due modi principali. In primo luogo, le soglie preimpostate e le euristiche richiedono una rivalutazione e una messa a punto costanti man mano che i loro valori ottimali si spostano. In secondo luogo, la convalida dei sistemi tramite test statici di unit√† e integrazione fallisce quando input e comportamenti sono obiettivi in movimento.</p>
<p>Rispondere a un mondo in continua evoluzione in tempo reale con sistemi ML legacy √® impegnativo. Il debito tecnico si accumula man mano che le ipotesi decadono. La mancanza di architettura modulare e la capacit√† di aggiornare dinamicamente i componenti senza effetti collaterali esacerbano questi problemi.</p>
<p>Per mitigare questo problema √® necessario integrare configurabilit√†, monitoraggio e aggiornabilit√† modulare. L‚Äôapprendimento online, in cui i modelli si adattano continuamente e solidi cicli di feedback alle pipeline di training, aiutano a sintonizzarsi automaticamente sul mondo. Tuttavia, anticipare e progettare il cambiamento √® essenziale per prevenire l‚Äôerosione delle prestazioni nel mondo reale nel tempo.</p>
</section>
<section id="gestire-il-debito-tecnico-nelle-fasi-iniziali" class="level3" data-number="13.4.10">
<h3 data-number="13.4.10" class="anchored" data-anchor-id="gestire-il-debito-tecnico-nelle-fasi-iniziali"><span class="header-section-number">13.4.10</span> Gestire il Debito Tecnico nelle Fasi Iniziali</h3>
<p>√à comprensibile che il debito tecnico si accumuli naturalmente nelle prime fasi di sviluppo del modello. Quando si punta a creare rapidamente modelli MVP, i team spesso hanno bisogno di informazioni pi√π complete su quali componenti raggiungeranno la scala o richiederanno modifiche. √à previsto un po‚Äô di lavoro differito.</p>
<p>Tuttavia, anche i sistemi iniziali frammentati dovrebbero seguire principi come ‚ÄúFlexible Foundations‚Äù per evitare di mettersi nei guai:</p>
<ul>
<li>Il codice modulare e le librerie riutilizzabili consentono di scambiare i componenti in un secondo momento</li>
<li>L‚Äôaccoppiamento debole tra modelli, archivi dati e logica aziendale facilita il cambiamento</li>
<li>I layer di astrazione nascondono i dettagli di implementazione che potrebbero cambiare nel tempo</li>
<li>Il servizio di modelli containerizzati mantiene aperte le opzioni sui requisiti di distribuzione</li>
</ul>
<p>Le decisioni che sembrano ragionevoli al momento possono limitare seriamente la flessibilit√† futura. Ad esempio, incorporare la logica aziendale chiave nel codice modello anzich√© tenerla separata rende estremamente difficili le modifiche successive al modello.</p>
<p>Con una progettazione ponderata, tuttavia, √® possibile creare rapidamente all‚Äôinizio mantenendo gradi di libert√† per migliorare. Man mano che il sistema matura, emergono prudenti punti di interruzione in cui l‚Äôintroduzione di nuove architetture in modo proattivo evita massicce rilavorazioni in futuro. In questo modo si bilanciano le urgenti tempistiche con la riduzione delle future cascate di correzione.</p>
</section>
<section id="riepilogo" class="level3" data-number="13.4.11">
<h3 data-number="13.4.11" class="anchored" data-anchor-id="riepilogo"><span class="header-section-number">13.4.11</span> Riepilogo</h3>
<p>Sebbene il debito finanziario sia una buona metafora per comprendere i compromessi, differisce dalla misurabilit√† del debito tecnico. Il debito tecnico deve essere completamente monitorato e quantificato. Ci√≤ rende difficile per i team gestire i compromessi tra muoversi rapidamente e introdurre intrinsecamente pi√π debito rispetto al prendersi il tempo per ripagare tale debito.</p>
<p>Il documento <a href="https://papers.nips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">Hidden Technical Debt of Machine Learning Systems</a> diffonde la consapevolezza delle sfumature del debito tecnologico specifico del sistema ML. Incoraggia un ulteriore sviluppo nell‚Äôampia area del ML manutenibile.</p>
</section>
</section>
<section id="ruoli-e-responsabilit√†" class="level2" data-number="13.5">
<h2 data-number="13.5" class="anchored" data-anchor-id="ruoli-e-responsabilit√†"><span class="header-section-number">13.5</span> Ruoli e Responsabilit√†</h2>
<p>Data la vastit√† di MLOps, l‚Äôimplementazione di successo di sistemi ML richiede competenze diversificate e una stretta collaborazione tra persone con diverse aree di competenza. Mentre gli scienziati dei dati creano i modelli ML di base, √® necessario un lavoro di squadra interfunzionale per distribuire con successo questi modelli in ambienti di produzione e consentire loro di fornire un valore aziendale sostenibile.</p>
<p>MLOps fornisce il framework e le pratiche per coordinare gli sforzi di vari ruoli coinvolti nello sviluppo, nella distribuzione e nell‚Äôesecuzione di sistemi MLG. Collegare i ‚Äúsilos‚Äù tradizionali tra i team di dati, ingegneria e operazioni √® fondamentale per il successo di MLOps. Abilitare una collaborazione senza soluzione di continuit√† attraverso il ciclo di vita dell‚Äôapprendimento automatico accelera la realizzazione dei vantaggi garantendo al contempo l‚Äôaffidabilit√† e le prestazioni a lungo termine dei modelli ML.</p>
<p>Esamineremo alcuni ruoli chiave coinvolti in MLOps e le loro responsabilit√† principali. Comprendere l‚Äôampiezza delle competenze necessarie per rendere operativi i modelli ML guida l‚Äôassemblaggio dei team MLOps. Chiarisce inoltre come i flussi di lavoro tra i ruoli si adattano alla metodologia MLOps sovraordinata.</p>
<section id="ingegneri-dei-dati" class="level3" data-number="13.5.1">
<h3 data-number="13.5.1" class="anchored" data-anchor-id="ingegneri-dei-dati"><span class="header-section-number">13.5.1</span> Ingegneri dei Dati</h3>
<p>Gli ingegneri dei dati sono responsabili della creazione e della manutenzione dell‚Äôinfrastruttura dati e delle pipeline che alimentano i dati nei modelli ML. Garantiscono che i dati vengano trasferiti senza problemi dai sistemi di origine agli ambienti di archiviazione, elaborazione e progettazione delle funzionalit√† necessari per lo sviluppo e la distribuzione dei modelli ML. Le loro principali responsabilit√† includono:</p>
<ul>
<li>Migrare dati grezzi da database, sensori e app ‚Äúon-prem‚Äù [in azienda], in data lake basati su cloud, come Amazon S3 o Google Cloud Storage. Ci√≤ fornisce un‚Äôarchiviazione economica e scalabile.</li>
<li>Creare pipeline di dati con ‚Äúscheduler‚Äù [pianificatori] di flussi di lavoro come Apache Airflow, Prefect e dbt. Questi estraggono i dati dalle sorgenti, li trasformano e li convalidano, e li caricano direttamente in destinazioni come data warehouse, feature store o per l‚Äôaddestramento del modello.</li>
<li>Trasformare dati grezzi e disordinati in set di dati strutturati e pronti per l‚Äôanalisi. Ci√≤ include la gestione di valori nulli o malformati, la deduplicazione, l‚Äôunione di origini dati disparate, l‚Äôaggregazione dei dati e la progettazione di nuove feature.</li>
<li>Manutenere componenti dell‚Äôinfrastruttura dati come data warehouse cloud (<a href="https://www.snowflake.com/en/data-cloud/workloads/data-warehouse/">Snowflake</a>, <a href="https://aws.amazon.com/redshift/">Redshift</a>, <a href="https://cloud.google.com/bigquery?hl=en">BigQuery</a>), data lake e sistemi di gestione dei metadati. Provisioning e ottimizzazione dei sistemi di elaborazione dati.</li>
<li>Fornire e ottimizzare sistemi di elaborazione dati per una gestione e un‚Äôanalisi dei dati efficiente e scalabile.</li>
<li>Definire i processi di versioning, backup e archiviazione dei dati per i set di dati e funzionalit√† ML e applicare policy di governance dei dati.</li>
</ul>
<p>Ad esempio, un‚Äôazienda manifatturiera pu√≤ utilizzare pipeline Apache Airflow per estrarre dati dei sensori dai PLC in fabbrica e trasferirli in un data lake Amazon S3. Gli ingegneri dei dati elaborerebbero poi questi dati grezzi per filtrarli, pulirli e unirli ai metadati del prodotto. Questi output della pipeline verrebbero quindi caricati in un data warehouse Snowflake da cui √® possibile leggere le feature per l‚Äôaddestramento e la previsione del modello.</p>
<p>Il team di ingegneria dei dati crea e sostiene la base dati per uno sviluppo e un funzionamento affidabili del modello. Il loro lavoro consente agli scienziati dei dati e agli ingegneri ML di concentrarsi sulla creazione, l‚Äôaddestramento e l‚Äôimplementazione di modelli ML su larga scala.</p>
</section>
<section id="data-scientist" class="level3" data-number="13.5.2">
<h3 data-number="13.5.2" class="anchored" data-anchor-id="data-scientist"><span class="header-section-number">13.5.2</span> Data Scientist</h3>
<p>Il lavoro dei ‚Äúdata scientist‚Äù [scienziato dei dati] √® concentrarsi sulla ricerca, sperimentazione, sviluppo e miglioramento continuo dei modelli ML. Sfruttano la loro competenza in statistica, modellazione e algoritmi per creare modelli ad alte prestazioni. Le loro principali responsabilit√† includono:</p>
<ul>
<li>Collaborare con team aziendali e di dati per identificare opportunit√† in cui ML pu√≤ aggiungere valore, inquadrare il problema e definire metriche di successo.</li>
<li>Eseguire analisi esplorative dei dati per comprendere le relazioni nei dati, ricavare informazioni e identificare funzionalit√† rilevanti per la modellazione.</li>
<li>Ricercare e sperimentare diversi algoritmi ML e architetture di modelli in base al problema e alle caratteristiche dei dati e sfruttare librerie come TensorFlow, PyTorch e Keras.</li>
<li>Massimizzare le prestazioni, addestrare e perfezionare i modelli regolando gli iperparametri, regolando le architetture delle reti neurali, l‚Äôingegneria delle funzionalit√†, ecc.</li>
<li>Valutare le prestazioni del modello tramite metriche come accuratezza, AUC e punteggi F1 ed eseguire analisi degli errori per identificare aree di miglioramento.</li>
<li>Sviluppare nuove versioni del modello mediante l‚Äôintegrazione di nuovi dati, test di diversi approcci, ottimizzazione del comportamento del modello e mantenimento della documentazione e della discendenza per i modelli.</li>
</ul>
<p>Ad esempio, uno scienziato dei dati pu√≤ sfruttare TensorFlow e <a href="https://www.tensorflow.org/probability">TensorFlow Probability</a> per sviluppare un modello di previsione della domanda per la pianificazione dell‚Äôinventario ala vendita al dettaglio. Itereranno su diversi modelli di sequenza come LSTM e sperimenteranno funzionalit√† derivate da dati di prodotto, vendite e stagionali. Il modello verr√† valutato in base a metriche di errore rispetto alla domanda effettiva prima dell‚Äôimplementazione. Lo scienziato dei dati monitora le prestazioni e riqualifica/migliora il modello man mano che arrivano nuovi dati.</p>
<p>I data scientist guidano la creazione, il miglioramento e l‚Äôinnovazione del modello attraverso la loro competenza nelle tecniche di ML. Collaborano strettamente con altri ruoli per garantire che i modelli creino il massimo impatto aziendale.</p>
</section>
<section id="ml-engineer" class="level3" data-number="13.5.3">
<h3 data-number="13.5.3" class="anchored" data-anchor-id="ml-engineer"><span class="header-section-number">13.5.3</span> ML Engineer</h3>
<p>Gli ‚Äúingegneri ML‚Äù consentono ai modelli sviluppati dagli scienziati dei dati di essere prodotti e distribuiti su larga scala. La loro competenza fa s√¨ che i modelli servano in modo affidabile alle previsioni nelle applicazioni e nei processi aziendali. Le loro principali responsabilit√† includono:</p>
<ul>
<li>Prendere modelli prototipo dagli scienziati dei dati e rafforzarli per gli ambienti di produzione tramite best practice di codifica.</li>
<li>Creare API e microservizi per la distribuzione dei modelli utilizzando strumenti come <a href="https://flask.palletsprojects.com/en/3.0.x/">Flask</a>, <a href="https://fastapi.tiangolo.com/">FastAPI</a>. Containerizzare i modelli con Docker.</li>
<li>Gestire le versioni dei modelli, sincronizzarli in produzione utilizzando pipeline CI/CD e implementare release canary, test A/B e procedure di rollback.</li>
<li>Ottimizzare le prestazioni dei modelli per elevata scalabilit√†, bassa latenza ed efficienza dei costi. Sfruttare compressione, quantizzazione e servizio multi-modello.</li>
<li>Monitorare i modelli una volta in produzione e garantire affidabilit√† e precisione continue. Riqualificare periodicamente i modelli.</li>
</ul>
<p>Ad esempio, un ingegnere ML pu√≤ prendere un modello di rilevamento delle frodi TensorFlow sviluppato da data scientist e containerizzarlo utilizzando TensorFlow Serving per una distribuzione scalabile. Il modello verrebbe integrato nella pipeline di elaborazione delle transazioni dell‚Äôazienda tramite API. L‚Äôingegnere ML implementa un registro dei modelli e una pipeline CI/CD utilizzando MLFlow e Jenkins per distribuire gli aggiornamenti del modello in modo affidabile. Gli ingegneri ML monitorano quindi il modello in esecuzione per prestazioni continue utilizzando strumenti come Prometheus e Grafana. Se l‚Äôaccuratezza del modello diminuisce, avviano la riqualificazione e la distribuzione di una nuova versione del modello.</p>
<p>Il team di ingegneria ML consente ai modelli di data science di progredire senza problemi in sistemi di produzione sostenibili e robusti. La loro competenza nella creazione di sistemi modulari e monitorati offre un valore aziendale continuo.</p>
</section>
<section id="devops-engineer" class="level3" data-number="13.5.4">
<h3 data-number="13.5.4" class="anchored" data-anchor-id="devops-engineer"><span class="header-section-number">13.5.4</span> DevOps Engineer</h3>
<p>Gli ‚Äúingegneri DevOps‚Äù abilitano MLOps creando e gestendo l‚Äôinfrastruttura sottostante per lo sviluppo, la distribuzione e il monitoraggio dei modelli ML. Forniscono l‚Äôarchitettura cloud e le pipeline di automazione. Le loro principali responsabilit√† includono:</p>
<ul>
<li>Approvvigionare e gestire l‚Äôinfrastruttura cloud per i flussi di lavoro ML utilizzando strumenti IaC come Terraform, Docker e Kubernetes.</li>
<li>Sviluppare pipeline CI/CD per il riaddestramento, la convalida e la distribuzione del modello. Integrare strumenti ML nella pipeline, come MLflow e Kubeflow.</li>
<li>Monitorare le prestazioni del modello e dell‚Äôinfrastruttura tramite strumenti come <a href="https://prometheus.io/">Prometheus</a>, <a href="https://grafana.com/">Grafana</a>, <a href="https://aws.amazon.com/what-is/elk-stack/">stack ELK</a>. Creare allarmi e dashboard.</li>
<li>Implementare pratiche di governance relative allo sviluppo, al test e alla promozione del modello per consentire riproducibilit√† e tracciabilit√†.</li>
<li>Embedding dei modelli ML nelle applicazioni. Espongono i modelli tramite API e microservizi per l‚Äôintegrazione.</li>
<li>Ottimizzazione delle prestazioni e dei costi dell‚Äôinfrastruttura e sfruttamento dell‚Äôautoscaling, delle istanze spot e della disponibilit√† in tutte le regioni.</li>
</ul>
<p>Ad esempio, un ingegnere DevOps esegue il provisioning di un cluster Kubernetes su AWS utilizzando Terraform per eseguire lavori di training ML e distribuzione online. L‚Äôingegnere crea una pipeline CI/CD in Jenkins, che attiva il riaddestramento del modello quando sono disponibili nuovi dati. Dopo il test automatizzato, il modello viene registrato con MLflow e distribuito nel cluster Kubernetes. L‚Äôingegnere monitora quindi lo stato del cluster, l‚Äôutilizzo delle risorse del contenitore e la latenza dell‚ÄôAPI utilizzando Prometheus e Grafana.</p>
<p>Il team DevOps consente una rapida sperimentazione e distribuzioni affidabili per ML tramite competenze cloud, automazione e monitoraggio. Il loro lavoro massimizza l‚Äôimpatto del modello riducendo al minimo il debito tecnico.</p>
</section>
<section id="project-manager" class="level3" data-number="13.5.5">
<h3 data-number="13.5.5" class="anchored" data-anchor-id="project-manager"><span class="header-section-number">13.5.5</span> Project Manager</h3>
<p>I project manager svolgono un ruolo fondamentale in MLOps coordinando le attivit√† tra i team coinvolti nella distribuzione dei progetti ML. Aiutano a guidare l‚Äôallineamento, la ‚Äúaccountability‚Äù [affidabilit√†] ed accelerano i risultati. Le loro principali responsabilit√† includono:</p>
<ul>
<li>Collaborare con le parti interessate per definire obiettivi di progetto, metriche di successo, tempistiche e budget; delineare specifiche e ‚Äúscope‚Äù.</li>
<li>Creare un piano di progetto che comprenda acquisizione dati, sviluppo modello, configurazione infrastrutturale, distribuzione e monitoraggio.</li>
<li>Coordinare i lavori di progettazione, sviluppo e test tra ingegneri dei dati, scienziati dei dati, ingegneri ML e ruoli DevOps.</li>
<li>Monitorare i progressi e le milestone, identificare gli ostacoli e risolverli tramite azioni correttive e gestire rischi e problemi.</li>
<li>Facilitare la comunicazione tramite report di stato, riunioni, workshop e documentazione e consentire una collaborazione senza interruzioni.</li>
<li>Guidare l‚Äôaderenza alle tempistiche e al budget e aumentare i superamenti o le carenze previsti per la mitigazione.</li>
</ul>
<p>Ad esempio, un project manager creerebbe un piano di progetto per sviluppare e migliorare un modello di previsione dell‚Äôabbandono dei clienti. Coordinare data engineer che creano pipeline di dati, data scientist che sperimentano modelli, ML engineer che producono modelli e DevOps che impostano l‚Äôinfrastruttura di distribuzione. Il project manager monitora i progressi tramite milestone come preparazione del set di dati, prototipazione del modello, distribuzione e monitoraggio. Per attuare soluzioni preventive, evidenziano eventuali rischi, ritardi o problemi di budget.</p>
<p>I project manager qualificati consentono ai team MLOps di lavorare in sinergia per fornire rapidamente il massimo valore aziendale dagli investimenti ML. La loro leadership e organizzazione si allineano con team diversi.</p>
</section>
</section>
<section id="sfide-dei-sistemi-embedded" class="level2 page-columns page-full" data-number="13.6">
<h2 data-number="13.6" class="anchored" data-anchor-id="sfide-dei-sistemi-embedded"><span class="header-section-number">13.6</span> Sfide dei Sistemi Embedded</h2>
<p>Esamineremo brevemente le sfide dei sistemi embedded in modo da definire il contesto per quelle specifiche che emergono con gli MLOps embedded, di cui parleremo nella sezione seguente.</p>
<section id="risorse-di-elaborazione-limitate" class="level3 page-columns page-full" data-number="13.6.1">
<h3 data-number="13.6.1" class="anchored" data-anchor-id="risorse-di-elaborazione-limitate"><span class="header-section-number">13.6.1</span> Risorse di Elaborazione Limitate</h3>
<p>I dispositivi embedded come i microcontrollori e i telefoni cellulari hanno una potenza di elaborazione molto pi√π limitata rispetto alle macchine dei data center o alle GPU. Un tipico microcontrollore pu√≤ avere solo KB di RAM, velocit√† della CPU MHz e nessuna GPU. Ad esempio, un microcontrollore in uno smartwatch pu√≤ avere solo un processore a 32 bit in esecuzione a 120 MHz con 320 KB di RAM <span class="citation" data-cites="stm2021l4">(<a href="../../references.html#ref-stm2021l4" role="doc-biblioref"><span>¬´EuroSoil 2021 (O205)¬ª</span> 2021</a>)</span>. Ci√≤ consente modelli ML semplici come piccole regressioni lineari o foreste casuali, ma reti neurali profonde pi√π complesse sarebbero irrealizzabili. Le strategie per mitigare questo includono quantizzazione, potatura, architetture di modelli efficienti e scaricamento di determinati calcoli sul cloud quando la connettivit√† lo consente.</p>
<div class="no-row-height column-margin column-container"><div id="ref-stm2021l4" class="csl-entry" role="listitem">
<span>¬´EuroSoil 2021 (O205)¬ª</span>. 2021. In <em>EuroSoil 2021 (O205)</em>. DS12902. STMicroelectronics; Frontiers Media SA. <a href="https://doi.org/10.3389/978-2-88966-997-4">https://doi.org/10.3389/978-2-88966-997-4</a>.
</div></div></section>
<section id="memoria-limitata" class="level3" data-number="13.6.2">
<h3 data-number="13.6.2" class="anchored" data-anchor-id="memoria-limitata"><span class="header-section-number">13.6.2</span> Memoria Limitata</h3>
<p>Memorizzare grandi modelli ML e set di dati direttamente su dispositivi embedded √® spesso impossibile con una memoria limitata. Ad esempio, un modello di rete neurale profonda pu√≤ facilmente occupare centinaia di MB, il che supera la capacit√† di archiviazione di molti sistemi embedded. Si consideri questo esempio. Una fotocamera per la fauna selvatica che cattura immagini per rilevare animali pu√≤ avere solo una scheda di memoria da 2 GB. Ne serve di pi√π per memorizzare un modello di deep learning per la classificazione delle immagini che spesso ha una dimensione di centinaia di MB. Di conseguenza, ci√≤ richiede l‚Äôottimizzazione dell‚Äôutilizzo della memoria tramite compressione dei pesi, numeri di precisione inferiore e pipeline di inferenza in streaming.</p>
</section>
<section id="connettivit√†-intermittente" class="level3" data-number="13.6.3">
<h3 data-number="13.6.3" class="anchored" data-anchor-id="connettivit√†-intermittente"><span class="header-section-number">13.6.3</span> Connettivit√† Intermittente</h3>
<p>Molti dispositivi embedded operano in ambienti remoti senza una connettivit√† Internet affidabile. Dobbiamo fare affidamento su qualcosa di diverso dall‚Äôaccesso cloud costante per un comodo riaddestramento, monitoraggio e distribuzione. Al contrario, abbiamo bisogno di strategie intelligenti sia di pianificazione che si memorizzazione nella cache per ottimizzare le connessioni intermittenti. Ad esempio, un modello che prevede la resa del raccolto in una fattoria remota potrebbe dover fare previsioni giornaliere ma avere connettivit√† al cloud solo una volta alla settimana quando l‚Äôagricoltore si reca in citt√†. Il modello deve funzionare in modo indipendente tra una connessione e l‚Äôaltra.</p>
</section>
<section id="limitazioni-della-potenza" class="level3" data-number="13.6.4">
<h3 data-number="13.6.4" class="anchored" data-anchor-id="limitazioni-della-potenza"><span class="header-section-number">13.6.4</span> Limitazioni della Potenza</h3>
<p>I dispositivi embedded come telefoni, dispositivi indossabili e sensori remoti sono alimentati a batteria. L‚Äôinferenza e la comunicazione continue possono esaurire rapidamente le batterie, limitandone la funzionalit√†. Ad esempio, un collare intelligente che contrassegna gli animali in via di estinzione funziona con una piccola batteria. L‚Äôesecuzione continua di un modello di tracciamento GPS scaricherebbe la batteria nel giro di pochi giorni. Il collare deve pianificare con attenzione quando attivare il modello. Pertanto, l‚ÄôML integrato deve gestire attentamente le attivit√† per risparmiare energia. Le tecniche includono acceleratori hardware ottimizzati, caching delle previsioni ed esecuzione di modelli adattivi.</p>
</section>
<section id="gestione-della-flotta" class="level3" data-number="13.6.5">
<h3 data-number="13.6.5" class="anchored" data-anchor-id="gestione-della-flotta"><span class="header-section-number">13.6.5</span> Gestione della Flotta</h3>
<p>Per i dispositivi embedded prodotti in serie, milioni di unit√† possono essere distribuite sul campo per orchestrare gli aggiornamenti. Ipoteticamente, l‚Äôaggiornamento di un modello di rilevamento delle frodi su 100 milioni di carte di credito (future intelligenti) richiede l‚Äôinvio sicuro degli aggiornamenti a ciascun dispositivo distribuito anzich√© a un data center centralizzato. Una scala cos√¨ distribuita rende la gestione dell‚Äôintera flotta molto pi√π difficile di un cluster di server centralizzato. Richiede protocolli intelligenti per aggiornamenti ‚Äúover-the-air‚Äù, gestione dei problemi di connettivit√† e monitoraggio dei vincoli di risorse tra i dispositivi.</p>
</section>
<section id="raccolta-dati-on-device" class="level3" data-number="13.6.6">
<h3 data-number="13.6.6" class="anchored" data-anchor-id="raccolta-dati-on-device"><span class="header-section-number">13.6.6</span> Raccolta Dati On-Device</h3>
<p>La raccolta di dati di training utili richiede la progettazione sia dei sensori sul dispositivo sia delle pipeline software. Questo √® diverso dai server, dove possiamo estrarre dati da fonti esterne. Le sfide includono la gestione del rumore dei sensori. I sensori su una macchina industriale rilevano vibrazioni e temperatura per prevedere le esigenze di manutenzione. Ci√≤ richiede la messa a punto dei sensori e delle frequenze di campionamento per acquisire dati utili.</p>
</section>
<section id="personalizzazione-specifica-del-dispositivo" class="level3" data-number="13.6.7">
<h3 data-number="13.6.7" class="anchored" data-anchor-id="personalizzazione-specifica-del-dispositivo"><span class="header-section-number">13.6.7</span> Personalizzazione Specifica del Dispositivo</h3>
<p>Uno smart speaker impara i modelli vocali e la cadenza del parlato di un singolo utente per migliorare la precisione del riconoscimento proteggendo al contempo la privacy. Adattare i modelli ML a dispositivi e utenti specifici √® importante, ma ci√≤ pone sfide per la privacy. L‚Äôapprendimento sul dispositivo consente la personalizzazione senza trasmettere cos√¨ tanti dati privati. Tuttavia, bilanciare il miglioramento del modello, la tutela della privacy e i vincoli richiede nuove tecniche.</p>
</section>
<section id="considerazioni-sulla-sicurezza" class="level3" data-number="13.6.8">
<h3 data-number="13.6.8" class="anchored" data-anchor-id="considerazioni-sulla-sicurezza"><span class="header-section-number">13.6.8</span> Considerazioni sulla Sicurezza</h3>
<p>Se un ML embedded estremamente grande in sistemi come i veicoli a guida autonoma non viene progettato con attenzione, ci sono seri rischi per la sicurezza. Per garantire un funzionamento sicuro prima dell‚Äôimplementazione, le auto a guida autonoma devono essere sottoposte a test approfonditi in pista in scenari simulati di pioggia, neve e ostacoli. Ci√≤ richiede una convalida approfondita, dispositivi di sicurezza, simulatori e conformit√† agli standard prima dell‚Äôimplementazione.</p>
</section>
<section id="diversi-target-hardware" class="level3" data-number="13.6.9">
<h3 data-number="13.6.9" class="anchored" data-anchor-id="diversi-target-hardware"><span class="header-section-number">13.6.9</span> Diversi Target Hardware</h3>
<p>Esiste una vasta gamma di processori embedded, tra cui ARM, x86, acceleratori AI specializzati, FPGA, ecc. Supportare questa eterogeneit√† rende difficile l‚Äôimplementazione. Abbiamo bisogno di strategie come framework standardizzati, test approfonditi e messa a punto del modello per ogni piattaforma. Ad esempio, un modello di rilevamento degli oggetti necessita di implementazioni efficienti su dispositivi embedded come Raspberry Pi, Nvidia Jetson e Google Edge TPU.</p>
</section>
<section id="copertura-dei-test" class="level3" data-number="13.6.10">
<h3 data-number="13.6.10" class="anchored" data-anchor-id="copertura-dei-test"><span class="header-section-number">13.6.10</span> Copertura dei Test</h3>
<p>Testare rigorosamente i casi limite √® difficile con risorse di simulazione embedded limitate, ma test esaustivi sono fondamentali in sistemi come le auto a guida autonoma. Testare esaustivamente un modello di pilota automatico richiede milioni di chilometri simulati, esponendolo a eventi rari come guasti dei sensori. Pertanto, strategie come la generazione di dati sintetici, la simulazione distribuita e l‚Äôingegneria del caos aiutano a migliorare la copertura.</p>
</section>
<section id="rilevamento-della-deriva-del-concetto" class="level3" data-number="13.6.11">
<h3 data-number="13.6.11" class="anchored" data-anchor-id="rilevamento-della-deriva-del-concetto"><span class="header-section-number">13.6.11</span> Rilevamento della Deriva del Concetto</h3>
<p>Con dati di monitoraggio limitati da ogni dispositivo remoto, rilevare cambiamenti nei dati di input nel tempo √® molto pi√π difficile. La deriva pu√≤ portare a degradazioni delle prestazioni del modello. Sono necessari metodi ‚Äúleggeri‚Äù per identificare quando √® necessario un riaddestramento. Un modello che prevede i carichi della rete elettrica mostra un calo delle prestazioni man mano che i modelli di utilizzo cambiano nel tempo. Con i soli dati locali sui dispositivi, questa tendenza √® difficile da individuare.</p>
</section>
</section>
<section id="mlops-tradizionali-e-mlops-embedded" class="level2 page-columns page-full" data-number="13.7">
<h2 data-number="13.7" class="anchored" data-anchor-id="mlops-tradizionali-e-mlops-embedded"><span class="header-section-number">13.7</span> MLOps Tradizionali e MLOps Embedded</h2>
<p>Negli MLOps tradizionali, i modelli ML vengono in genere distribuiti in ambienti basati su cloud o server, con risorse abbondanti come potenza di calcolo e memoria. Questi ambienti facilitano il funzionamento regolare di modelli complessi che richiedono risorse di calcolo significative. Ad esempio, un modello di riconoscimento delle immagini basato su cloud potrebbe essere utilizzato da una piattaforma di social media per contrassegnare automaticamente le foto con etichette pertinenti. In questo caso, il modello pu√≤ sfruttare le vaste risorse disponibili nel cloud per elaborare in modo efficiente grandi quantit√† di dati.</p>
<p>D‚Äôaltro canto, i MLOps embedded comportano la distribuzione di modelli ML su sistemi embedded, sistemi di calcolo specializzati progettati per eseguire funzioni specifiche all‚Äôinterno di sistemi pi√π grandi. I sistemi embedded sono in genere caratterizzati dalle loro risorse di calcolo e potenza limitate. Ad esempio, un modello ML potrebbe essere ‚Äúembedded‚Äù in un termostato intelligente per ottimizzare il riscaldamento e il raffreddamento in base alle preferenze e alle abitudini dell‚Äôutente. Il modello deve essere ottimizzato per funzionare in modo efficiente sull‚Äôhardware limitato del termostato senza comprometterne le prestazioni o la precisione.</p>
<p>La differenza fondamentale tra i MLOps tradizionali e quelli embedded risiede nei vincoli di risorse del sistema embedded. Mentre gli MLOps tradizionali possono sfruttare abbondanti risorse cloud o server, gli MLOps embedded devono fare i conti con le limitazioni hardware su cui viene distribuito il modello. Ci√≤ richiede un‚Äôattenta ottimizzazione e messa a punto del modello per garantire che possa fornire informazioni accurate e preziose entro i vincoli del sistema embedded.</p>
<p>Inoltre, gli MLOps embedded devono considerare le sfide uniche poste dall‚Äôintegrazione dei modelli ML con altri componenti del sistema embedded. Ad esempio, il modello deve essere compatibile con il software e l‚Äôhardware del sistema e deve essere in grado di interfacciarsi senza problemi con altri componenti, come sensori o attuatori. Ci√≤ richiede una profonda comprensione sia dei sistemi ML che di quelli integrati e una stretta collaborazione tra data scientist, ingegneri e altre parti interessate.</p>
<p>Quindi, mentre gli MLOps tradizionali e gli MLOps embedded condividono l‚Äôobiettivo comune di distribuire e mantenere modelli ML in ambienti di produzione, le sfide uniche poste dai sistemi embedded richiedono un approccio specializzato. Gli MLOps embedded devono bilanciare attentamente la necessit√† di accuratezza e prestazioni del modello con i vincoli dell‚Äôhardware su cui viene distribuito il modello. Ci√≤ richiede una profonda comprensione sia dei sistemi ML che di quelli embedded e una stretta collaborazione tra i vari stakeholder per garantire l‚Äôintegrazione di successo dei modelli ML nei sistemi embedded.</p>
<p>Questa volta, raggrupperemo i sottoargomenti in categorie pi√π ampie per semplificare la struttura del nostro processo di pensiero su MLOps. Questa struttura aiuter√† a comprendere come i diversi aspetti di MLOps siano interconnessi e perch√© ciascuno sia importante per il funzionamento efficiente dei sistemi ML mentre discutiamo le sfide nel contesto dei sistemi embedded.</p>
<ul>
<li>Gestione del Ciclo di Vita del Modello
<ul>
<li>Gestione dei Dati: Gestione dell‚Äôingestione dei dati, convalida e controllo delle versioni.</li>
<li>Addestramento dei Modelli: Tecniche e pratiche per un addestramento dei modelli efficace e scalabile.</li>
<li>Valutazione dei Modelli: Strategie per testare e convalidare le prestazioni dei modelli.</li>
<li>Distribuzione dei modelli: Approcci per la distribuzione dei modelli in ambienti di produzione.</li>
</ul></li>
<li>Integrazione di Sviluppo e Operazioni
<ul>
<li>Pipeline CI/CD: Integrazione dei modelli ML in pipeline di integrazione e distribuzione continue.</li>
<li>Gestione dell‚Äôinfrastruttura: Impostazione e manutenzione dell‚Äôinfrastruttura necessaria per il training e la distribuzione dei modelli.</li>
<li>Comunicazione e Collaborazione: Garanzia di una comunicazione e collaborazione fluide tra data scientist, ingegneri ML e team operativi.</li>
</ul></li>
<li>Eccellenza operativa
<ul>
<li>Monitoraggio: Tecniche per il monitoraggio delle prestazioni dei modelli, della deriva dei dati e dello stato operativo.</li>
<li>Governance: Implementazione di policy per la verificabilit√†, la conformit√† e le considerazioni etiche dei modelli.</li>
</ul></li>
</ul>
<section id="gestione-del-ciclo-di-vita-del-modello" class="level3" data-number="13.7.1">
<h3 data-number="13.7.1" class="anchored" data-anchor-id="gestione-del-ciclo-di-vita-del-modello"><span class="header-section-number">13.7.1</span> Gestione del Ciclo di Vita del Modello</h3>
<section id="gestione-dei-dati" class="level4">
<h4 class="anchored" data-anchor-id="gestione-dei-dati">Gestione dei Dati</h4>
<p>Nei tradizionali MLOps centralizzati, i dati vengono aggregati in grandi dataset e data lake, poi elaborati su server cloud o ‚Äúon-prem‚Äù [in sede]. Tuttavia, MLOps embedded si basa su dati decentralizzati da sensori locali sui dispositivi. I dispositivi raccolgono batch pi√π piccoli di dati incrementali, spesso rumorosi e non strutturati. Con vincoli di connettivit√†, questi dati non possono sempre essere trasmessi istantaneamente al cloud e devono essere memorizzati nella cache in modo intelligente ed elaborati all‚Äôedge.</p>
<p>A causa della potenza di calcolo limitata sui dispositivi embedded, i dati si possono solo preelaborare e pulire in modo minimo prima della trasmissione. Il filtraggio e l‚Äôelaborazione anticipati avvengono nei gateway edge per ridurre i carichi di trasmissione. Mentre si sfrutta l‚Äôarchiviazione cloud, altre elaborazioni e archiviazioni avvengono all‚Äôedge per tenere conto della connettivit√† intermittente. I dispositivi identificano e trasmettono solo i sottoinsiemi di dati pi√π critici al cloud.</p>
<p>Anche l‚Äôetichettatura richiede un accesso centralizzato ai dati, che richiede tecniche pi√π automatizzate come l‚Äôapprendimento federato, in cui i dispositivi etichettano in modo collaborativo i dati dei peer. Con i dispositivi edge personali, la privacy dei dati e le normative sono preoccupazioni critiche. La raccolta, la trasmissione e l‚Äôarchiviazione dei dati devono essere sicure e conformi.</p>
<p>Ad esempio, uno smartwatch pu√≤ raccogliere il conteggio dei passi giornalieri, la frequenza cardiaca e le coordinate GPS. Questi dati vengono memorizzati nella cache locale e trasmessi a un gateway edge quando √® disponibile il WiFi: il gateway elabora e filtra i dati prima di sincronizzare i sottoinsiemi rilevanti con la piattaforma cloud per riaddestrare i modelli.</p>
</section>
<section id="addestramento-del-modello-1" class="level4">
<h4 class="anchored" data-anchor-id="addestramento-del-modello-1">Addestramento del Modello</h4>
<p>Nei tradizionali MLOps centralizzati, i modelli vengono addestrati utilizzando dati abbondanti tramite deep learning su server GPU cloud ad alta potenza. Tuttavia, glii MLOps embedded necessitano di maggiore supporto in termini di complessit√† del modello, disponibilit√† dei dati e risorse di elaborazione per l‚Äôaddestramento.</p>
<p>Il volume di dati aggregati √® molto pi√π basso, spesso richiedendo tecniche come l‚Äôapprendimento federato tra dispositivi per creare set di addestramento. La natura specializzata dei dati edge limita anche i set di dati pubblici per il pre-addestramento. Per questioni di privacy, i campioni di dati devono essere strettamente controllati e resi anonimi ove possibile.</p>
<p>Inoltre, i modelli devono utilizzare architetture semplificate ottimizzate per hardware edge a bassa potenza. Date le limitazioni di elaborazione, le GPU di fascia alta sono inaccessibili per un deep learning intensivo. L‚Äôaddestramento sfrutta server edge e cluster a bassa potenza con approcci distribuiti per spartire il carico.</p>
<p>Il ‚Äútransfer learning‚Äù emerge come una strategia cruciale per affrontare la scarsit√† di dati e l‚Äôirregolarit√† nell‚Äôapprendimento automatico, in particolare negli scenari di edge computing. Come illustrato in <a href="#fig-transfer-learning-mlops" class="quarto-xref">Figura&nbsp;<span>13.5</span></a>, questo approccio prevede il pre-training di modelli su grandi set di dati pubblici e la loro successiva messa a punto su dati edge specifici del dominio. La figura raffigura una rete neurale in cui i layer iniziali (da W_{A1} a W_{A4}), responsabili dell‚Äôestrazione delle feature generali, sono congelati (indicati da una linea tratteggiata verde). Questi layer conservano la conoscenza delle attivit√† precedenti, accelerando l‚Äôapprendimento e riducendo i requisiti di risorse. Gli ultimi livelli (da W_{A5} a W_{A7}), oltre la linea tratteggiata blu, sono messi a punto per l‚Äôattivit√† specifica, concentrandosi sull‚Äôapprendimento delle feature specifiche dell‚Äôattivit√†.</p>
<div id="fig-transfer-learning-mlops" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-learning-mlops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/transfer_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-learning-mlops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.5: Trasferimento dell‚Äôapprendimento in MLOps. Fonte: HarvardX.
</figcaption>
</figure>
</div>
<p>Questo metodo non solo mitiga la scarsit√† di dati, ma si adatta anche alla natura decentralizzata dei dati embedded. Inoltre, tecniche come l‚Äôapprendimento incrementale sul dispositivo possono personalizzare ulteriormente i modelli in base a casi d‚Äôuso specifici. La mancanza di dati ampiamente etichettati in molti domini motiva anche l‚Äôuso di tecniche semi-supervisionate, che completano l‚Äôapproccio di apprendimento per trasferimento. Sfruttando le conoscenze preesistenti e adattandole a compiti specializzati, l‚Äôapprendimento per trasferimento all‚Äôinterno di un framework MLOps consente ai modelli di ottenere prestazioni pi√π elevate con meno risorse, anche in ambienti con vincoli di dati.</p>
<p>Ad esempio, un assistente domestico intelligente pu√≤ pre-addestrare un modello di riconoscimento audio su clip YouTube pubbliche, il che aiuta a eseguire il bootstrap con conoscenze generali. Quindi trasferisce l‚Äôapprendimento a un piccolo campione di dati domestici per classificare elettrodomestici ed eventi personalizzati, specializzandosi nel modello. Il modello si trasforma in una rete neurale leggera ottimizzata per dispositivi abilitati al microfono in tutta la casa.</p>
<p>Pertanto, gli MLOps embedded affrontano sfide acute nella costruzione di set di dati di training, nella progettazione di modelli efficienti e nella distribuzione del calcolo per lo sviluppo del modello rispetto alle impostazioni tradizionali. Dati i vincoli embedded, √® necessario un attento adattamento, come l‚Äôapprendimento tramite trasferimento e il training distribuito, per addestrare i modelli.</p>
</section>
<section id="valutazione-del-modello-1" class="level4">
<h4 class="anchored" data-anchor-id="valutazione-del-modello-1">Valutazione del Modello</h4>
<p>Nei tradizionali MLOps centralizzati, i modelli vengono valutati principalmente utilizzando metriche di accuratezza e dataset di test di holdout. Tuttavia, gli MLOps embedded richiedono una valutazione pi√π olistica che tenga conto dei vincoli di sistema oltre all‚Äôaccuratezza.</p>
<p>I modelli devono essere testati in anticipo e spesso su hardware edge distribuito che copre diverse configurazioni. Oltre all‚Äôaccuratezza, fattori come latenza, utilizzo della CPU, ingombro di memoria e consumo energetico sono criteri di valutazione critici. I modelli vengono selezionati in base a compromessi tra queste metriche per soddisfare i vincoli dei dispositivi edge.</p>
<p>Anche la deriva dei dati deve essere monitorata, dove i modelli addestrati sui dati cloud degradano in accuratezza nel tempo sui dati edge locali. I dati embedded hanno spesso una maggiore variabilit√† rispetto ai set di addestramento centralizzati. Valutare i modelli su diversi campioni di dati edge operativi √® fondamentale. Ma a volte, ottenere i dati per monitorare la deriva pu√≤ essere difficile se questi dispositivi sono in circolazione e la comunicazione √® una barriera.</p>
<p>Il monitoraggio continuo fornisce visibilit√† sulle prestazioni del mondo reale dopo l‚Äôimplementazione, rivelando colli di bottiglia non evidenziati durante i test. Ad esempio, un aggiornamento del modello di una smart camera potrebbe essere inizialmente testato su 100 telecamere e poi annullato se si osserva un calo della precisione, prima di essere esteso a tutte le 5000 telecamere.</p>
</section>
<section id="distribuzione-del-modello-1" class="level4">
<h4 class="anchored" data-anchor-id="distribuzione-del-modello-1">Distribuzione del Modello</h4>
<p>Negli MLOps tradizionali, le nuove versioni del modello vengono distribuite direttamente sui server tramite endpoint API. Tuttavia, i dispositivi embedded richiedono meccanismi di distribuzione ottimizzati per ricevere modelli aggiornati. Gli aggiornamenti over-the-air (OTA) forniscono un approccio standardizzato alla distribuzione wireless di nuove versioni di software o firmware ai dispositivi embedded. Invece dell‚Äôaccesso API diretto, i pacchetti OTA consentono la distribuzione remota di modelli e dipendenze come bundle pre-costruiti. In alternativa, l‚Äô<a href="@sec-fl">apprendimento federato</a> consente aggiornamenti del modello senza accesso diretto ai dati di training grezzi. Questo approccio decentralizzato ha il potenziale per un miglioramento continuo del modello, ma necessita di piattaforme MLOps robuste.</p>
<p>La distribuzione del modello si basa su interfacce fisiche come connessioni seriali USB o UART per dispositivi profondamente embedded privi di connettivit√†. Il packaging del modello segue ancora principi simili agli aggiornamenti OTA, ma il meccanismo di distribuzione √® adattato alle capacit√† dell‚Äôhardware edge. Inoltre, spesso vengono utilizzati protocolli OTA specializzati ottimizzati per reti IoT anzich√© protocolli WiFi o Bluetooth standard. I fattori chiave includono efficienza, affidabilit√†, sicurezza e telemetria, come il monitoraggio dei progressi, soluzioni come <a href="https://mender.io/">Mender. Io</a> fornisce servizi OTA incentrati su embedded che gestiscono aggiornamenti differenziali tra flotte di dispositivi.</p>
<p><a href="#fig-model-lifecycle" class="quarto-xref">Figura&nbsp;<span>13.6</span></a> presenta una panoramica di ‚ÄúModel Lifecycle Management‚Äù in un contesto MLOps, illustrando il flusso dallo sviluppo (in alto a sinistra) alla distribuzione e al monitoraggio (in basso a destra). Il processo inizia con lo sviluppo ML, in cui il codice e le configurazioni sono ‚Äúversion-controlled‚Äù. La gestione dei dati e dei modelli √® fondamentale per il processo, coinvolgendo set di dati e repository di funzionalit√†. Training continuo, conversione del modello e registro del modello sono fasi chiave nell‚Äôoperazionalizzazione della training. La distribuzione del modello include la fornitura del modello e la gestione dei log di fornitura. Sono in atto meccanismi di allarme per segnalare i problemi, che alimentano il monitoraggio continuo per garantire le prestazioni e l‚Äôaffidabilit√† del modello nel tempo. Questo approccio integrato garantisce che i modelli siano sviluppati e mantenuti in modo efficace durante tutto il loro ciclo di vita.</p>
<div id="fig-model-lifecycle" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlops_flow.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.6: Gestione del ciclo di vita del modello. Fonte: HarvardX.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="integrazione-di-sviluppo-e-operazioni" class="level3" data-number="13.7.2">
<h3 data-number="13.7.2" class="anchored" data-anchor-id="integrazione-di-sviluppo-e-operazioni"><span class="header-section-number">13.7.2</span> Integrazione di Sviluppo e Operazioni</h3>
<section id="pipeline-cicd-1" class="level4">
<h4 class="anchored" data-anchor-id="pipeline-cicd-1">Pipeline CI/CD</h4>
<p>Nelle MLOps tradizionali, una solida infrastruttura CI/CD come Jenkins e Kubernetes consente l‚Äôautomazione della pipeline per la distribuzione di modelli su larga scala. Tuttavia, le MLOps embedded necessitano di questa infrastruttura centralizzata e di flussi di lavoro CI/CD pi√π personalizzati per i dispositivi edge.</p>
<p>La creazione di pipeline CI/CD deve tenere conto di un panorama frammentato di diverse versioni hardware, firmware e vincoli di connettivit√†. Non esiste una piattaforma standard per orchestrare le pipeline e il supporto degli strumenti √® pi√π limitato.</p>
<p>I test devono coprire in anticipo questo ampio spettro di dispositivi embedded target, il che √® difficile senza un accesso centralizzato. Le aziende devono investire molto nell‚Äôacquisizione e nella gestione dell‚Äôinfrastruttura di test nell‚Äôecosistema embedded eterogeneo.</p>
<p>Gli aggiornamenti over-the-air richiedono la configurazione di server specializzati per distribuire in modo sicuro i bundle di modelli ai dispositivi sul campo. Anche le procedure di rollout e rollback devono essere attentamente personalizzate per particolari famiglie di dispositivi.</p>
<p>Con gli strumenti CI/CD tradizionali meno applicabili, le MLOps embedded si affidano maggiormente a script personalizzati e integrazione. Le aziende adottano approcci diversi, dai framework open source alle soluzioni completamente interne. Una stretta integrazione tra sviluppatori, ingegneri edge e clienti finali stabilisce processi di rilascio affidabili.</p>
<p>Pertanto, gli MLOps embedded non possono sfruttare l‚Äôinfrastruttura cloud centralizzata per CI/CD. Le aziende combinano pipeline personalizzate, infrastruttura di test e distribuzione OTA per distribuire modelli su sistemi edge frammentati e disconnessi.</p>
</section>
<section id="gestione-dellinfrastruttura-1" class="level4">
<h4 class="anchored" data-anchor-id="gestione-dellinfrastruttura-1">Gestione dell‚ÄôInfrastruttura</h4>
<p>Nei tradizionali MLOps centralizzati, l‚Äôinfrastruttura comporta l‚Äôapprovvigionamento di server cloud, GPU e reti ad alta larghezza di banda per carichi di lavoro intensivi come l‚Äôaddestramento di modelli e la fornitura di previsioni su larga scala. Tuttavia, gli MLOps embedded richiedono un‚Äôinfrastruttura pi√π eterogenea che si estende su dispositivi edge, gateway e cloud.</p>
<p>I dispositivi edge come i sensori catturano e preelaborano i dati localmente prima della trasmissione intermittente per evitare di sovraccaricare le reti: i gateway aggregano ed elaborano i dati dei dispositivi prima di inviare sottoinsiemi selezionati al cloud per l‚Äôaddestramento e l‚Äôanalisi. Il cloud fornisce gestione centralizzata ed elaborazione supplementare.</p>
<p>Questa infrastruttura necessita di una stretta integrazione e bilanciamento dei carichi di elaborazione e comunicazione. La larghezza di banda di rete √® limitata, il che richiede un attento filtraggio e compressione dei dati. Le capacit√† di elaborazione edge sono modeste rispetto al cloud, imponendo vincoli di ottimizzazione.</p>
<p>La gestione di aggiornamenti OTA sicuri su grandi flotte di dispositivi presenta sfide all‚Äôedge. I rollout devono essere incrementali e pronti per il rollback per una rapida mitigazione. Dato l‚Äôambiente decentralizzato, l‚Äôaggiornamento dell‚Äôinfrastruttura edge richiede coordinamento.</p>
<p>Ad esempio, un impianto industriale pu√≤ eseguire l‚Äôelaborazione di base del segnale sui sensori prima di inviare i dati a un gateway on-prem. Il gateway gestisce l‚Äôaggregazione dei dati, il monitoraggio dell‚Äôinfrastruttura e gli aggiornamenti OTA. Solo i dati curati vengono trasmessi al cloud per analisi avanzate e riaddestramento del modello.</p>
<p>MLOps embedded richiede una gestione olistica dell‚Äôinfrastruttura distribuita che abbraccia edge vincolato, gateway e cloud centralizzato. I carichi di lavoro sono bilanciati tra i livelli tenendo conto delle sfide di connettivit√†, elaborazione e sicurezza.</p>
</section>
<section id="comunicazione-e-collaborazione-1" class="level4">
<h4 class="anchored" data-anchor-id="comunicazione-e-collaborazione-1">Comunicazione e Collaborazione</h4>
<p>Nelle MLOps tradizionali, la collaborazione tende a concentrarsi su data scientist, ingegneri ML e team DevOps. Tuttavia, le MLOps embedded richiedono un coordinamento interfunzionale pi√π stretto tra ruoli aggiuntivi per affrontare i vincoli di sistema.</p>
<p>Gli ingegneri edge ottimizzano le architetture dei modelli per gli ambienti hardware target. Forniscono feedback ai data scientist durante lo sviluppo in modo che i modelli si adattino anticipatamente alle capacit√† dei dispositivi. Analogamente, i team di prodotto definiscono i requisiti operativi informati dai contesti degli utenti finali.</p>
<p>Con pi√π stakeholder nell‚Äôecosistema embedded, i canali di comunicazione devono facilitare la condivisione delle informazioni tra team centralizzati e remoti. Il monitoraggio dei problemi e la gestione dei progetti garantiscono l‚Äôallineamento.</p>
<p>Gli strumenti collaborativi ottimizzano i modelli per dispositivi specifici. I data scientist possono registrare i problemi replicati dai dispositivi sul campo in modo che i modelli siano specializzati in dati di nicchia. L‚Äôaccesso remoto ai dispositivi facilita il debug e la raccolta dati.</p>
<p>Ad esempio, i data scientist possono collaborare con i team sul campo che gestiscono flotte di turbine eoliche per recuperare campioni di dati operativi. Questi dati vengono utilizzati per specializzare i modelli rilevando anomalie specifiche per quella classe di turbine. Gli aggiornamenti dei modelli vengono testati in simulazioni e rivisti dagli ingegneri prima dell‚Äôimplementazione sul campo.</p>
<p>Gli MLOps embedded impongono un coordinamento continuo tra data scientist, ingegneri, clienti finali e altre parti interessate durante l‚Äôintero ciclo di vita del ML. Grazie a una stretta collaborazione, i modelli possono essere personalizzati e ottimizzati per i dispositivi edge mirati.</p>
</section>
</section>
<section id="eccellenza-operativa" class="level3" data-number="13.7.3">
<h3 data-number="13.7.3" class="anchored" data-anchor-id="eccellenza-operativa"><span class="header-section-number">13.7.3</span> Eccellenza operativa</h3>
<section id="monitoraggio-1" class="level4">
<h4 class="anchored" data-anchor-id="monitoraggio-1">Monitoraggio</h4>
<p>Il monitoraggio MLOps tradizionale si concentra sul monitoraggio centralizzato dell‚Äôaccuratezza del modello, delle metriche delle prestazioni e della deriva dei dati. Tuttavia, MLOps embedded deve tenere conto del monitoraggio decentralizzato su diversi dispositivi e ambienti edge.</p>
<p>I dispositivi edge richiedono una raccolta dati ottimizzata per trasmettere metriche di monitoraggio chiave senza sovraccaricare le reti. Le metriche aiutano a valutare le prestazioni del modello, i modelli di dati, l‚Äôutilizzo delle risorse e altri comportamenti sui dispositivi remoti.</p>
<p>Con una connettivit√† limitata, vengono eseguite pi√π analisi all‚Äôedge prima di aggregare le informazioni centralmente. I gateway svolgono un ruolo chiave nel monitoraggio dello stato di salute della flotta e nel coordinamento degli aggiornamenti software. Gli indicatori confermati vengono infine propagati al cloud.</p>
<p>Un‚Äôampia copertura dei dispositivi √® impegnativa ma critica. Possono sorgere problemi specifici per determinati tipi di dispositivi, quindi il monitoraggio deve coprire l‚Äôintero spettro. Le distribuzioni ‚Äúcanary‚Äù aiutano a testare i processi di monitoraggio prima del ridimensionamento.</p>
<p>Il rilevamento delle anomalie identifica gli incidenti che richiedono il rollback dei modelli o la riqualificazione su nuovi dati. Tuttavia, l‚Äôinterpretazione degli allarmi richiede la comprensione dei contesti dei dispositivi univoci in base all‚Äôinput di ingegneri e clienti.</p>
<p>Ad esempio, una casa automobilistica pu√≤ monitorare i veicoli autonomi per gli indicatori di degradazione del modello utilizzando la memorizzazione nella cache, l‚Äôaggregazione e i flussi in tempo reale. Gli ingegneri valutano quando le anomalie identificate garantiscono gli aggiornamenti OTA per migliorare i modelli in base a fattori come la posizione e l‚Äôet√† del veicolo.</p>
<p>Il monitoraggio MLOps embedded fornisce osservabilit√† nelle prestazioni del modello e del sistema in ambienti edge decentralizzati. Un‚Äôattenta raccolta, analisi e collaborazione dei dati fornisce informazioni significative per mantenere l‚Äôaffidabilit√†.</p>
</section>
<section id="governance-1" class="level4">
<h4 class="anchored" data-anchor-id="governance-1">Governance</h4>
<p>Nelle MLOps tradizionali, la governance si concentra sulla spiegabilit√† del modello, la correttezza e la conformit√† per i sistemi centralizzati. Tuttavia, le MLOps embedded devono anche affrontare le sfide di governance a livello di dispositivo relative alla privacy dei dati, alla sicurezza e alla protezione.</p>
<p>Con i sensori che raccolgono dati personali e sensibili, la governance dei dati locali sui dispositivi √® fondamentale. I controlli di accesso ai dati, l‚Äôanonimizzazione e la memorizzazione nella cache crittografata aiutano ad affrontare i rischi per la privacy e la conformit√† come HIPAA e GDPR. Gli aggiornamenti devono mantenere patch e impostazioni di sicurezza.</p>
<p>La governance della sicurezza considera gli impatti fisici del comportamento difettoso del dispositivo. I guasti potrebbero causare condizioni non sicure in veicoli, fabbriche e sistemi critici. Ridondanza, sistemi di sicurezza e sistemi di allarme aiutano a mitigare i rischi.</p>
<p>La governance tradizionale, come il monitoraggio dei bias e la spiegabilit√† del modello, rimane imperativa ma √® pi√π difficile da implementare per l‚Äôintelligenza artificiale embedded. Anche dare un‚Äôocchiata ai modelli black-box su dispositivi a basso consumo pone delle sfide.</p>
<p>Ad esempio, un dispositivo medico pu√≤ cancellare i dati personali sul dispositivo prima della trasmissione. I rigidi protocolli di governance dei dati approvano gli aggiornamenti del modello. La spiegabilit√† del modello √® limitata, ma l‚Äôattenzione √® rivolta al rilevamento di comportamenti anomali. I sistemi di backup prevengono i guasti.</p>
<p>La governance MLOps embedded deve comprendere privacy, sicurezza, protezione, trasparenza ed etica. Sono necessarie tecniche specializzate e collaborazione di squadra per aiutare a stabilire fiducia e responsabilit√† all‚Äôinterno di ambienti decentralizzati.</p>
</section>
</section>
<section id="confronto" class="level3" data-number="13.7.4">
<h3 data-number="13.7.4" class="anchored" data-anchor-id="confronto"><span class="header-section-number">13.7.4</span> Confronto</h3>
<p><a href="#tbl-mlops-comparison" class="quarto-xref">Tabella&nbsp;<span>13.2</span></a> evidenzia le somiglianze e le differenze tra MLOps Tradizionali e MLOps Embedded sulla base di tutto ci√≤ che abbiamo imparato finora:</p>
<div id="tbl-mlops-comparison" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-mlops-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;13.2: Confronto tra le pratiche MLOps Tradizionali e quelle MLOps Embedded.
</figcaption>
<div aria-describedby="tbl-mlops-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 38%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Area</th>
<th style="text-align: left;">MLOps Tradizionali</th>
<th style="text-align: left;">MLOps Embedded</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Gestione dei Dati</td>
<td style="text-align: left;">Grandi set di dati, data lake, feature store</td>
<td style="text-align: left;">Acquisizione dati sul dispositivo, edge caching ed elaborazione</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sviluppo del Modello</td>
<td style="text-align: left;">Sfrutta il deep learning, reti neurali complesse, addestramento GPU</td>
<td style="text-align: left;">Vincoli sulla complessit√† del modello, necessit√† di ottimizzazione</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Distribuzione</td>
<td style="text-align: left;">Cluster di server, distribuzione cloud, bassa latenza su larga scala</td>
<td style="text-align: left;">Distribuzione OTA su dispositivi, connettivit√† intermittente</td>
</tr>
<tr class="even">
<td style="text-align: left;">Monitoraggio</td>
<td style="text-align: left;">Dashboard, log, allarmi per le prestazioni del modello cloud</td>
<td style="text-align: left;">Monitoraggio sul dispositivo di previsioni, utilizzo delle risorse</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Riqualificazione</td>
<td style="text-align: left;">Ri-addestramento dei modelli su nuovi dati</td>
<td style="text-align: left;">Apprendimento federato da dispositivi, ri-addestramento edge</td>
</tr>
<tr class="even">
<td style="text-align: left;">Infrastruttura</td>
<td style="text-align: left;">Infrastruttura cloud dinamica</td>
<td style="text-align: left;">Infrastruttura edge/cloud eterogenea</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Collaborazione</td>
<td style="text-align: left;">Monitoraggio degli esperimenti condivisi e registro dei modelli</td>
<td style="text-align: left;">Collaborazione per l‚Äôottimizzazione specifica del dispositivo</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Quindi, mentre Embedded MLOps condivide i principi fondamentali di MLOps, si trova ad affrontare vincoli unici nell‚Äôadattare flussi di lavoro e infrastrutture specificamente per dispositivi edge con risorse limitate.</p>
</section>
<section id="mlops-tradizionali" class="level3" data-number="13.7.5">
<h3 data-number="13.7.5" class="anchored" data-anchor-id="mlops-tradizionali"><span class="header-section-number">13.7.5</span> MLOps Tradizionali</h3>
<p>Google, Microsoft e Amazon offrono la loro versione di servizi ML gestiti. Questi includono servizi che gestiscono il training e la sperimentazione dei modelli, l‚Äôhosting e il ridimensionamento dei modelli e il monitoraggio. Queste offerte sono disponibili tramite un‚ÄôAPI e SDK client, nonch√© tramite interfacce utente Web. Sebbene sia possibile creare le proprie soluzioni MLOps end-to-end utilizzando parti di ciascuno, i maggiori vantaggi in termini di facilit√† d‚Äôuso derivano dal rimanere all‚Äôinterno di un singolo ecosistema di provider per sfruttare le integrazioni tra servizi.</p>
<p>Le sezioni seguenti presentano una rapida panoramica dei servizi che rientrano in ogni parte del ciclo di vita MLOps descritto sopra, fornendo esempi da diversi provider. √à importante notare che lo spazio MLOps si sta evolvendo rapidamente; nuove aziende e prodotti stanno entrando in scena a un ritmo rapido. Gli esempi menzionati non intendono fungere da approvazione delle offerte di aziende specifiche, ma piuttosto illustrare i tipi di soluzioni disponibili sul mercato.</p>
<section id="gestione-dei-dati-1" class="level4">
<h4 class="anchored" data-anchor-id="gestione-dei-dati-1">Gestione dei Dati</h4>
<p>L‚Äôarchiviazione dei dati e il versioning sono elementi essenziali per qualsiasi offerta commerciale e la maggior parte sfrutta le soluzioni di archiviazione generiche esistenti come S3. Altri utilizzano opzioni pi√π specializzate come l‚Äôarchiviazione basata su git (ad esempio: <a href="https://huggingface.co/datasets">Hugging Face‚Äôs Dataset Hub</a>). Questa √® un‚Äôarea in cui i provider semplificano il supporto delle opzioni di archiviazione dati dei concorrenti, poich√© non vogliono che ci√≤ rappresenti una barriera per l‚Äôadozione del resto dei loro servizi MLOps. Ad esempio, la pipeline di addestramento di Vertex AI supporta senza problemi i set di dati archiviati in S3, Google Cloud Buckets o Dataset Hub di Hugging Face.</p>
</section>
<section id="addestramento-del-modello-2" class="level4">
<h4 class="anchored" data-anchor-id="addestramento-del-modello-2">Addestramento del Modello</h4>
<p>I servizi di training gestiti sono il punto di forza dei provider cloud, in quanto forniscono accesso on-demand a hardware che √® fuori dalla portata della maggior parte delle aziende pi√π piccole. Fatturano solo l‚Äôhardware per il tempo del training, accelerato con GPU accessibili anche ai team di sviluppatori pi√π piccoli. Il controllo che gli sviluppatori hanno sul loro flusso di lavoro del training pu√≤ variare notevolmente a seconda delle loro esigenze. Alcuni provider hanno servizi che forniscono poco pi√π dell‚Äôaccesso alle risorse e si affidano allo sviluppatore per gestire autonomamente il ciclo di training, il logging e l‚Äôarchiviazione dei modelli. Altri servizi sono semplici come puntare a un modello di base e a un set di dati etichettato per avviare un lavoro di messa a punto completamente gestito (ad esempio: <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models">Vertex AI Fine Tuning</a>).</p>
<p>Una parola di avvertimento: A partire dal 2023, la domanda di hardware GPU supera di gran lunga l‚Äôofferta e, di conseguenza, i provider cloud stanno razionando l‚Äôaccesso alle loro GPU. In alcune regioni dei data center, le GPU potrebbero non essere disponibili o richiedere contratti a lungo termine.</p>
</section>
<section id="valutazione-del-modello-2" class="level4">
<h4 class="anchored" data-anchor-id="valutazione-del-modello-2">Valutazione del Modello</h4>
<p>Le attivit√† di valutazione del modello in genere comportano il monitoraggio dell‚Äôaccuratezza, della latenza e dell‚Äôutilizzo delle risorse dei modelli sia nelle fasi di test che di produzione. A differenza dei sistemi embedded, i modelli ML distribuiti sul cloud beneficiano di una connettivit√† Internet costante e di capacit√† di logging illimitate. Di conseguenza, √® spesso possibile acquisire e loggare ogni richiesta e risposta. Ci√≤ rende trattabile la riproduzione o la generazione di richieste sintetiche per confrontare modelli e versioni diversi.</p>
<p>Alcuni provider offrono anche servizi che automatizzano il monitoraggio degli esperimenti di modifica degli iperparametri del modello. Tracciano le esecuzioni e le prestazioni e generano artefatti da queste esecuzioni di training del modello. Esempio: <a href="https://wandb.ai/">WeightsAndBiases</a></p>
</section>
<section id="distribuzione-del-modello-2" class="level4">
<h4 class="anchored" data-anchor-id="distribuzione-del-modello-2">Distribuzione del Modello</h4>
<p>Ogni provider in genere ha un servizio denominato ‚Äúmodel registry‚Äù, in cui vengono archiviati e a cui si accede ai modelli di training. Spesso, questi registri possono anche fornire accesso a modelli di base che sono open source o forniti da grandi aziende tecnologiche (o, in alcuni casi, come <a href="https://ai.meta.com/llama/">LLAMA</a>, entrambi!). Questi registri dei modelli costituiscono un luogo comune per confrontare tutti i modelli e le loro versioni per consentire un facile processo decisionale su quale scegliere per un dato caso d‚Äôuso. Esempio: <a href="https://cloud.google.com/vertex-ai/docs/model-registry/introduction">Vertex AI‚Äôs model registry</a></p>
<p>Dal registro dei modelli, distribuire un modello a un endpoint di inferenza √® rapido e semplice, e gestisce il provisioning delle risorse, il download del peso del modello e l‚Äôhosting di un dato modello. Questi servizi in genere forniscono accesso al modello tramite un‚ÄôAPI REST con cui possono essere inviate richieste di inferenza. A seconda del tipo di modello, √® possibile configurare risorse specifiche, ad esempio quale tipo di acceleratore GPU potrebbe essere necessario per raggiungere le prestazioni desiderate. Alcuni provider possono anche offrire opzioni di inferenza ‚Äúserverless‚Äù o batch che non necessitano di un endpoint persistente per accedere al modello. Esempio: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html">AWS SageMaker Inference</a></p>
</section>
</section>
<section id="mlops-embedded" class="level3 page-columns page-full" data-number="13.7.6">
<h3 data-number="13.7.6" class="anchored" data-anchor-id="mlops-embedded"><span class="header-section-number">13.7.6</span> MLOps Embedded</h3>
<p>Nonostante la proliferazione di nuovi strumenti ML Ops in risposta all‚Äôaumento della domanda, le sfide descritte in precedenza hanno limitato la disponibilit√† di tali strumenti negli ambienti di sistemi embedded. Pi√π di recente, nuovi strumenti come Edge Impulse <span class="citation" data-cites="janapa2023edge">(<a href="../../references.html#ref-janapa2023edge" role="doc-biblioref">Janapa Reddi et al. 2023</a>)</span> hanno reso il processo di sviluppo un po‚Äô pi√π semplice, come descritto di seguito.</p>
<div class="no-row-height column-margin column-container"><div id="ref-janapa2023edge" class="csl-entry" role="listitem">
Janapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler, Daniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. <span>¬´Edge Impulse: An MLOps Platform for Tiny Machine Learning¬ª</span>. <em>Proceedings of Machine Learning and Systems</em> 5.
</div></div><section id="edge-impulse" class="level4">
<h4 class="anchored" data-anchor-id="edge-impulse">Edge Impulse</h4>
<p><a href="https://edgeimpulse.com/">Edge Impulse</a> √® una piattaforma di sviluppo end-to-end per la creazione e l‚Äôimplementazione di modelli di apprendimento automatico su dispositivi edge come microcontrollori e piccoli processori. Rende l‚Äôapprendimento automatico embedded pi√π accessibile agli sviluppatori di software attraverso la sua interfaccia web di facile utilizzo e strumenti integrati per la raccolta dati, lo sviluppo di modelli, l‚Äôottimizzazione e l‚Äôimplementazione. Le sue funzionalit√† principali includono quanto segue:</p>
<ul>
<li>Flusso di lavoro intuitivo drag-and-drop per la creazione di modelli ML senza bisogno di codifica</li>
<li>Strumenti per l‚Äôacquisizione, l‚Äôetichettatura, la visualizzazione e la preelaborazione dei dati dai sensori</li>
<li>Scelta di architetture di modelli, tra cui reti neurali e apprendimento non supervisionato</li>
<li>Tecniche di ottimizzazione dei modelli per bilanciare metriche delle prestazioni e vincoli hardware</li>
<li>Distribuzione senza soluzione di continuit√† su dispositivi edge tramite compilazione, SDK e benchmark</li>
<li>Funzionalit√† di collaborazione per team e integrazione con altre piattaforme</li>
</ul>
<p>Con Edge Impulse, gli sviluppatori con competenze limitate in data science possono sviluppare modelli ML specializzati che funzionano in modo efficiente in piccoli ambienti di elaborazione. Fornisce una soluzione completa per la creazione di intelligenza embedded e l‚Äôavanzamento del machine learning.</p>
<section id="interfaccia-utente" class="level5">
<h5 class="anchored" data-anchor-id="interfaccia-utente">Interfaccia utente</h5>
<p>Edge Impulse √® stato progettato con sette principi chiave: accessibilit√†, funzionalit√† end-to-end, un approccio incentrato sui dati, interattivit√†, estensibilit√†, orientamento al team e supporto della community. L‚Äôinterfaccia utente intuitiva, mostrata in <a href="#fig-edge-impulse-ui" class="quarto-xref">Figura&nbsp;<span>13.7</span></a>, guida gli sviluppatori di tutti i livelli di esperienza attraverso il caricamento dei dati, la selezione di un‚Äôarchitettura di modello, l‚Äôaddestramento del modello e la sua distribuzione su piattaforme hardware pertinenti. Va notato che, come qualsiasi strumento, Edge Impulse √® destinato ad assistere, non a sostituire, le considerazioni fondamentali come la determinazione se ML √® una soluzione appropriata o l‚Äôacquisizione delle competenze di dominio richieste per una determinata applicazione.</p>
<div id="fig-edge-impulse-ui" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-edge-impulse-ui-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/edge_impulse_dashboard.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-edge-impulse-ui-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.7: Schermata dell‚Äôinterfaccia utente di Edge Impulse per la creazione di flussi di lavoro dai dati di input alle funzionalit√† di output.
</figcaption>
</figure>
</div>
<p>Ci√≤ che rende Edge Impulse degno di nota √® il suo flusso di lavoro end-to-end completo ma intuitivo. Gli sviluppatori iniziano caricando i propri dati tramite l‚Äôinterfaccia utente grafica (GUI) o gli strumenti dell‚Äôinterfaccia a riga di comando (CLI), dopodich√© possono esaminare campioni grezzi e visualizzare la distribuzione dei dati nelle suddivisioni di addestramento e test. Successivamente, gli utenti possono scegliere tra vari ‚Äúblocchi‚Äù di pre-elaborazione per facilitare l‚Äôelaborazione del segnale digitale (DSP). Mentre vengono forniti valori di parametri predefiniti, gli utenti possono personalizzare i parametri in base alle proprie esigenze, osservando le considerazioni su memoria e la latenza visualizzate. Gli utenti possono scegliere facilmente la propria architettura di rete neurale, senza bisogno di alcun codice.</p>
<p>Grazie all‚Äôeditor visivo della piattaforma, gli utenti possono personalizzare i componenti dell‚Äôarchitettura e i parametri specifici, assicurandosi al contempo che il modello sia ancora addestrabile. Gli utenti possono anche sfruttare algoritmi di apprendimento non supervisionato, come il clustering K-means e i Gaussian Mixture Model (GMM).</p>
</section>
<section id="ottimizzazioni" class="level5">
<h5 class="anchored" data-anchor-id="ottimizzazioni">Ottimizzazioni</h5>
<p>Per adattarsi ai vincoli di risorse delle applicazioni TinyML, Edge Impulse fornisce una ‚Äúmatrice di confusione‚Äù che riassume le metriche chiave delle prestazioni, tra cui accuratezza per classe e punteggi F1. La piattaforma chiarisce i compromessi tra prestazioni del modello, dimensioni e latenza utilizzando simulazioni in <a href="https://renode.io/">Renode</a> e benchmarking specifici del dispositivo. Per i casi di utilizzo dei dati in streaming, uno strumento di calibrazione delle prestazioni sfrutta un algoritmo genetico per trovare configurazioni di post-elaborazione ideali che bilanciano tassi di falsa accettazione e falso rifiuto. Sono disponibili tecniche come quantizzazione, ottimizzazione del codice e ottimizzazione specifica del dispositivo per i modelli. Per la distribuzione, i modelli possono essere compilati in formati appropriati per i dispositivi edge target. Gli SDK del firmware nativi consentono anche la raccolta diretta dei dati sui dispositivi.</p>
<p>Oltre a semplificare lo sviluppo, Edge Impulse ridimensiona il processo di modellazione stesso. Una funzionalit√† chiave √® <a href="https://docs.edgeimpulse.com/docs/edge-impulse-studio/eon-tuner">EON Tuner</a>, uno strumento di apprendimento automatico automatico (AutoML) che assiste gli utenti nell‚Äôottimizzazione degli iperparametri in base ai vincoli di sistema. Esegue una ricerca casuale per generare rapidamente configurazioni per l‚Äôelaborazione del segnale digitale e le fasi di training. I modelli risultanti vengono visualizzati affinch√© l‚Äôutente possa selezionarli in base a metriche di prestazioni, memoria e latenza pertinenti. Per i dati, l‚Äôapprendimento attivo facilita il training su un piccolo sottoinsieme etichettato, seguito dall‚Äôetichettatura manuale o automatica di nuovi campioni in base alla vicinanza alle classi esistenti. Ci√≤ espande l‚Äôefficienza dei dati.</p>
</section>
<section id="casi-duso" class="level5">
<h5 class="anchored" data-anchor-id="casi-duso">Casi d‚ÄôUso</h5>
<p>Oltre all‚Äôaccessibilit√† della piattaforma stessa, il team di Edge Impulse ha ampliato la base di conoscenza dell‚Äôecosistema ML embedded. La piattaforma si presta ad ambienti accademici, essendo stata utilizzata in corsi online e workshop in loco a livello globale. Sono stati pubblicati numerosi casi di studio con casi d‚Äôuso di settore e di ricerca, in particolare <a href="https://ouraring.com/">Oura Ring</a>, che utilizza ML per identificare i pattern del sonno. Il team ha reso i repository open source su GitHub, facilitando la crescita della comunit√†. Gli utenti possono anche rendere pubblici i progetti per condividere tecniche e scaricare librerie da condividere tramite Apache. L‚Äôaccesso a livello di organizzazione consente la collaborazione sui flussi di lavoro.</p>
<p>Nel complesso, Edge Impulse √® straordinariamente completo e integrabile per i flussi di lavoro degli sviluppatori. Piattaforme pi√π grandi come Google e Microsoft si concentrano maggiormente sul cloud rispetto ai sistemi embedded. I framework TinyMLOps come Neuton AI e Latent AI offrono alcune funzionalit√† ma non hanno le capacit√† end-to-end di Edge Impulse. TensorFlow Lite Micro √® il motore di inferenza standard grazie alla flessibilit√†, allo stato open source e all‚Äôintegrazione di TensorFlow, ma utilizza pi√π memoria e storage rispetto al compilatore EON di Edge Impulse. Altre piattaforme devono essere aggiornate, focalizzate sull‚Äôaspetto accademico o pi√π versatili. In sintesi, Edge Impulse semplifica e amplia l‚Äôapprendimento automatico embedded tramite una piattaforma accessibile e automatizzata.</p>
</section>
</section>
<section id="limitazioni" class="level4">
<h4 class="anchored" data-anchor-id="limitazioni">Limitazioni</h4>
<p>Sebbene Edge Impulse fornisca una pipeline accessibile per ML embedded, permangono importanti limitazioni e rischi. Una sfida fondamentale √® la qualit√† e la disponibilit√† dei dati: i modelli sono validi solo quanto i dati utilizzati per addestrarli. Gli utenti devono disporre di campioni etichettati sufficienti che catturino l‚Äôampiezza delle condizioni operative previste e delle modalit√† di errore. Le anomalie e i valori anomali etichettati sono critici, ma richiedono molto tempo per essere raccolti e identificati. Dati insufficienti o distorti comportano scarse prestazioni del modello indipendentemente dalle capacit√† dello strumento.</p>
<p>Anche il deploying su dispositivi a bassa potenza presenta sfide intrinseche. I modelli ottimizzati potrebbero comunque dover richiedere pi√π risorse per MCU a bassissimo consumo. Trovare il giusto equilibrio tra compressione e accuratezza richiede un po‚Äô di sperimentazione. Lo strumento semplifica, ma deve comunque eliminare la necessit√† di competenze di base in ML ed elaborazione del segnale. Gli ambienti embedded limitano anche il debug e l‚Äôinterpretabilit√† rispetto al cloud.</p>
<p>Sebbene siano ottenibili risultati impressionanti, gli utenti non dovrebbero considerare Edge Impulse come una soluzione ‚ÄúPush Button ML‚Äù. Un‚Äôattenta definizione dell‚Äôambito del progetto, la raccolta dati, la valutazione del modello e il test sono comunque essenziali. Come con qualsiasi strumento di sviluppo, si consigliano aspettative ragionevoli e diligenza nell‚Äôapplicazione. Tuttavia, Edge Impulse pu√≤ accelerare la prototipazione e l‚Äôimplementazione di ML embedded per gli sviluppatori disposti a investire lo sforzo di data science e ingegneria richiesto.</p>
<div id="exr-ei" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;13.1: Edge Impulse
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Pronti a far salire di livello i vostri piccoli progetti di machine-learning? Combiniamo la potenza di Edge Impulse con le fantastiche visualizzazioni di Weights &amp; Biases (WandB). In questo Colab, si imparer√† a monitorare i progressi del training del modello come un professionista! Si immagini di vedere fantastici grafici del modello che diventa pi√π intelligente, confrontando diverse versioni e assicurandovi che la vostra IA funzioni al meglio anche su dispositivi minuscoli.</p>
<p><a href="https://colab.research.google.com/github/edgeimpulse/notebooks/blob/main/notebooks/python-sdk-with-wandb.ipynb#scrollTo=7583a486-afd6-42d8-934b-fdb33a6f3362"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="casi-di-studio" class="level2 page-columns page-full" data-number="13.8">
<h2 data-number="13.8" class="anchored" data-anchor-id="casi-di-studio"><span class="header-section-number">13.8</span> Casi di Studio</h2>
<section id="oura-ring" class="level3" data-number="13.8.1">
<h3 data-number="13.8.1" class="anchored" data-anchor-id="oura-ring"><span class="header-section-number">13.8.1</span> Oura Ring</h3>
<p><a href="https://ouraring.com/">Oura Ring</a> √® un dispositivo indossabile che pu√≤ misurare l‚Äôattivit√†, il sonno e il recupero quando viene posizionato sul dito dell‚Äôutente. Utilizzando sensori per tracciare le metriche fisiologiche, il dispositivo utilizza ML embedded per prevedere le fasi del sonno. Per stabilire una base di legittimit√† nel settore, Oura ha condotto un esperimento di correlazione per valutare il successo del dispositivo nel prevedere le fasi del sonno rispetto a uno studio di base. Ci√≤ ha portato a una solida correlazione del 62% rispetto alla base di riferimento dell‚Äô82-83%. Pertanto, il team ha deciso di determinare come migliorare ulteriormente le proprie prestazioni.</p>
<p>La prima sfida √® stata ottenere dati migliori in termini sia di quantit√† che di qualit√†. Avrebbero potuto ospitare uno studio pi√π ampio per ottenere un set di dati pi√π completo, ma i dati sarebbero stati cos√¨ rumorosi e grandi che sarebbe stato difficile aggregarli, ripulirli e analizzarli. √à qui che entra in gioco Edge Impulse.</p>
<p>Abbiamo condotto un massiccio studio sul sonno su 100 uomini e donne di et√† compresa tra 15 e 73 anni in tre continenti (Asia, Europa e Nord America). Oltre a indossare l‚ÄôOura Ring, i partecipanti erano tenuti a sottoporsi al test PSG [https://it.wikipedia.org/wiki/Polisonnografia] standard del settore, che ha fornito una ‚Äúetichetta‚Äù per questo set di dati. Con 440 notti di sonno da parte di 106 partecipanti, il set di dati ha totalizzato 3.444 ore di lunghezza tra dati Ring e PSG. Con Edge Impulse, Oura ha potuto caricare e consolidare facilmente i dati da diverse fonti in un bucket S3 privato. Sono stati anche in grado di impostare una Data Pipeline per unire campioni di dati in file individuali e preelaborare i dati senza dover eseguire lo ‚Äúscrubbing‚Äù [pulizia] manuale.</p>
<p>Col tempo risparmiato nell‚Äôelaborazione dei dati grazie a Edge Impulse, il team Oura ha potuto concentrarsi sui driver chiave della propria previsione. Hanno estratto solo tre tipi di dati dei sensori: frequenza cardiaca, movimento e temperatura corporea. Dopo aver suddiviso i dati utilizzando la validazione incrociata a cinque livelli e classificato le fasi del sonno, il team ha ottenuto una correlazione del 79%, solo pochi punti percentuali in meno rispetto allo standard. Hanno prontamente distribuito due tipi di modelli di rilevamento del sonno: uno semplificato utilizzando solo l‚Äôaccelerometro dell‚Äôanello e uno pi√π completo sfruttando i segnali periferici mediati dal Autonomic Nervous System (ANS) [sistema nervoso autonomo] e le caratteristiche circadiane [ritmo cardiaco in 24 ore]. Con Edge Impulse, hanno in programma di condurre ulteriori analisi di diversi tipi di attivit√† e sfruttare la scalabilit√† della piattaforma per continuare a sperimentare con diverse fonti di dati e sottoinsiemi di caratteristiche estratte.</p>
<p>Mentre la maggior parte della ricerca ML si concentra su fasi dominanti del modello come il training e la messa a punto, questo caso di studio sottolinea l‚Äôimportanza di un approccio olistico alle operazioni ML, in cui anche le fasi iniziali di aggregazione dei dati e pre-elaborazione hanno un impatto fondamentale sulla riuscita.</p>
</section>
<section id="clinaiops" class="level3 page-columns page-full" data-number="13.8.2">
<h3 data-number="13.8.2" class="anchored" data-anchor-id="clinaiops"><span class="header-section-number">13.8.2</span> ClinAIOps</h3>
<p>Diamo un‚Äôocchiata a MLOps nel contesto del monitoraggio medico sanitario per comprendere meglio come MLOps ‚Äúmaturi‚Äù in un‚Äôimplementazione nel mondo reale. In particolare, prendiamo in considerazione il continuous therapeutic monitoring (CTM) [monitoraggio terapeutico continuo] abilitato da dispositivi e sensori indossabili. Il CTM cattura dati fisiologici dettagliati dai pazienti, offrendo l‚Äôopportunit√† di aggiustamenti pi√π frequenti e personalizzati ai trattamenti.</p>
<p>I sensori indossabili abilitati per ML consentono un monitoraggio continuo fisiologico e dell‚Äôattivit√† al di fuori delle cliniche, aprendo possibilit√† per aggiustamenti terapeutici tempestivi e basati sui dati. Ad esempio, i biosensori indossabili per l‚Äôinsulina <span class="citation" data-cites="psoma2023wearable">(<a href="../../references.html#ref-psoma2023wearable" role="doc-biblioref">Psoma e Kanthou 2023</a>)</span> e i sensori ECG da polso per il monitoraggio del glucosio <span class="citation" data-cites="li2021noninvasive">(<a href="../../references.html#ref-li2021noninvasive" role="doc-biblioref">J. Li et al. 2021</a>)</span> possono automatizzare il dosaggio di insulina per il diabete, i sensori ECG e PPG da polso possono regolare gli anticoagulanti in base ai modelli di fibrillazione atriale <span class="citation" data-cites="attia2018noninvasive guo2019mobile">(<a href="../../references.html#ref-attia2018noninvasive" role="doc-biblioref">Attia et al. 2018</a>; <a href="../../references.html#ref-guo2019mobile" role="doc-biblioref">Guo et al. 2019</a>)</span>, e gli accelerometri che tracciano l‚Äôandatura possono innescare cure preventive per la mobilit√† in declino negli anziani <span class="citation" data-cites="liu2022monitoring">(<a href="../../references.html#ref-liu2022monitoring" role="doc-biblioref">Liu et al. 2022</a>)</span>. La variet√† di segnali che ora possono essere catturati passivamente e continuamente consente la titolazione e l‚Äôottimizzazione della terapia su misura per le mutevoli esigenze di ogni paziente. Chiudendo il cerchio tra rilevamento fisiologico e risposta terapeutica con TinyML e apprendimento sul dispositivo, i dispositivi indossabili sono pronti a trasformare molte aree della medicina personalizzata.</p>
<div class="no-row-height column-margin column-container"><div id="ref-psoma2023wearable" class="csl-entry" role="listitem">
Psoma, Sotiria D., e Chryso Kanthou. 2023. <span>¬´Wearable Insulin Biosensors for Diabetes Management: Advances and Challenges¬ª</span>. <em>Biosensors</em> 13 (7): 719. <a href="https://doi.org/10.3390/bios13070719">https://doi.org/10.3390/bios13070719</a>.
</div><div id="ref-li2021noninvasive" class="csl-entry" role="listitem">
Li, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, e Zedong Nie. 2021. <span>¬´Non-invasive Monitoring of Three Glucose Ranges Based On ECG By Using DBSCAN-CNN¬ª</span>. <em>IEEE Journal of Biomedical and Health Informatics</em> 25 (9): 3340‚Äì50. <a href="https://doi.org/10.1109/jbhi.2021.3072628">https://doi.org/10.1109/jbhi.2021.3072628</a>.
</div><div id="ref-attia2018noninvasive" class="csl-entry" role="listitem">
Attia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman, Suraj Kapa, Paul A. Friedman, e Peter A. Noseworthy. 2018. <span>¬´Noninvasive assessment of dofetilide plasma concentration using a deep learning (neural network) analysis of the surface electrocardiogram: A proof of concept study¬ª</span>. <em>PLOS ONE</em> 13 (8): e0201059. <a href="https://doi.org/10.1371/journal.pone.0201059">https://doi.org/10.1371/journal.pone.0201059</a>.
</div><div id="ref-guo2019mobile" class="csl-entry" role="listitem">
Guo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia, Li Yan, et al. 2019. <span>¬´Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation¬ª</span>. <em>Journal of the American College of Cardiology</em> 74 (19): 2365‚Äì75. <a href="https://doi.org/10.1016/j.jacc.2019.08.019">https://doi.org/10.1016/j.jacc.2019.08.019</a>.
</div><div id="ref-liu2022monitoring" class="csl-entry" role="listitem">
Liu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella Jensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022. <span>¬´Monitoring gait at home with radio waves in Parkinson‚Äôs disease: A marker of severity, progression, and medication response¬ª</span>. <em>Science Translational Medicine</em> 14 (663): eadc9669. <a href="https://doi.org/10.1126/scitranslmed.adc9669">https://doi.org/10.1126/scitranslmed.adc9669</a>.
</div></div><p>Il ML √® molto promettente nell‚Äôanalisi dei dati CTM per fornire raccomandazioni basate sui dati per gli aggiustamenti della terapia. Ma semplicemente distribuire modelli di intelligenza artificiale in ‚Äúsilos‚Äù, senza integrarli correttamente nei flussi di lavoro clinici e nel processo decisionale, pu√≤ portare a una scarsa adozione o a risultati non ottimali. In altre parole, pensare solo a MLOps non √® sufficiente per renderli utili nella pratica. Questo studio dimostra che sono necessari framework per incorporare intelligenza artificiale e CTM nella pratica clinica reale senza soluzione di continuit√†.</p>
<p>Questo caso di studio analizza ‚ÄúClinAIOps‚Äù come modello per operazioni ML embedded in ambienti clinici complessi <span class="citation" data-cites="chen2023framework">(<a href="../../references.html#ref-chen2023framework" role="doc-biblioref">Chen et al. 2023</a>)</span>. Forniamo una panoramica del framework e del motivo per cui √® necessario, esaminiamo un esempio di applicazione e discutiamo le principali sfide di implementazione relative al monitoraggio del modello, all‚Äôintegrazione del flusso di lavoro e agli incentivi per gli stakeholder. L‚Äôanalisi di esempi concreti come ClinAIOps illumina principi cruciali e best practice per operazioni AI affidabili ed efficaci in molti domini.</p>
<div class="no-row-height column-margin column-container"></div><p>I framework MLOps tradizionali non sono sufficienti per integrare il monitoraggio terapeutico continuo (CTM) e l‚ÄôIA in contesti clinici per alcuni motivi chiave:</p>
<ul>
<li><p>MLOps si concentra sul ciclo di vita del modello ML: training, distribuzione, monitoraggio. Ma l‚Äôassistenza sanitaria implica il coordinamento di pi√π stakeholder umani, pazienti e medici, non solo modelli.</p></li>
<li><p>MLOps automatizza il monitoraggio e la gestione dei sistemi IT. Tuttavia, l‚Äôottimizzazione della salute del paziente richiede cure personalizzate e supervisione umana, non solo automazione.</p></li>
<li><p>CTM e l‚Äôerogazione dell‚Äôassistenza sanitaria sono sistemi sociotecnici complessi con molte parti mobili. MLOps non fornisce un framework per coordinare il processo decisionale umano e AI.</p></li>
<li><p>Le considerazioni etiche relative all‚ÄôAI sanitaria richiedono giudizio umano, supervisione e responsabilit√†. I framework MLOps non hanno processi per la supervisione etica.</p></li>
<li><p>I dati sanitari dei pazienti sono altamente sensibili e regolamentati. MLOps da solo non garantisce la gestione delle informazioni sanitarie protette secondo gli standard normativi e di privacy.</p></li>
<li><p>La convalida clinica dei piani di trattamento guidati dall‚ÄôAI √® essenziale per l‚Äôadozione da parte del provider. MLOps non incorpora la valutazione specifica del dominio delle raccomandazioni del modello.</p></li>
<li><p>L‚Äôottimizzazione delle metriche sanitarie come i risultati dei pazienti richiede l‚Äôallineamento degli incentivi e dei flussi di lavoro delle parti interessate, che MLOps puramente incentrato sulla tecnologia trascura.</p></li>
</ul>
<p>Pertanto, integrare efficacemente AI/ML e CTM nella pratica clinica richiede pi√π di semplici modelli e pipeline di dati; richiede il coordinamento di complessi processi decisionali collaborativi tra esseri umani e AI, che ClinAIOps affronta tramite i suoi cicli di feedback multi-stakeholder.</p>
<section id="cicli-di-feedback" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="cicli-di-feedback">Cicli di Feedback</h4>
<p>Il framework ClinAIOps, mostrato in <a href="#fig-clinaiops" class="quarto-xref">Figura&nbsp;<span>13.8</span></a>, fornisce questi meccanismi attraverso tre cicli di feedback. I cicli sono utili per coordinare le informazioni dal monitoraggio fisiologico continuo, l‚Äôesperienza del medico e la guida dell‚ÄôIA tramite cicli di feedback, consentendo una medicina di precisione basata sui dati mantenendo al contempo la responsabilit√† umana. ClinAIOps fornisce un modello per un‚Äôefficace simbiosi uomo-IA nell‚Äôassistenza sanitaria: il paziente √® al centro, fornendo sfide e obiettivi sanitari che informano il regime terapeutico; il medico supervisiona questo regime, fornendo input per gli aggiustamenti basati sui dati di monitoraggio continuo e sui report sanitari del paziente; mentre gli sviluppatori di IA svolgono un ruolo cruciale creando sistemi che generano allarmi per gli aggiornamenti della terapia, che il medico quindi esamina.</p>
<p>Questi cicli di feedback, di cui parleremo di seguito, aiutano a mantenere la responsabilit√† e il controllo del medico sui piani di trattamento esaminando i suggerimenti dell‚ÄôIA prima che abbiano un impatto sui pazienti. Aiutano a personalizzare dinamicamente il comportamento e gli output del modello di IA in base allo stato di salute mutevole di ciascun paziente. Contribuiscono a migliorare l‚Äôaccuratezza del modello e l‚Äôutilit√† clinica nel tempo, imparando dalle risposte del medico e del paziente. Facilitano il processo decisionale condiviso e l‚Äôassistenza personalizzata durante le interazioni paziente-medico. Consentono una rapida ottimizzazione delle terapie in base a dati frequenti del paziente che i medici non possono analizzare manualmente.</p>
<div id="fig-clinaiops" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-clinaiops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/clinaiops.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clinaiops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.8: Ciclo ClinAIOps. Fonte: <span class="citation" data-cites="chen2023framework">Chen et al. (<a href="../../references.html#ref-chen2023framework" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<section id="ciclo-paziente-ia" class="level5">
<h5 class="anchored" data-anchor-id="ciclo-paziente-ia">Ciclo Paziente-IA</h5>
<p>Il ciclo paziente-IA consente un‚Äôottimizzazione frequente della terapia guidata dal monitoraggio fisiologico continuo. Ai pazienti vengono prescritti dispositivi indossabili come smartwatch o cerotti cutanei per raccogliere passivamente segnali sanitari rilevanti. Ad esempio, un paziente diabetico potrebbe avere un monitoraggio continuo del glucosio o un paziente con malattie cardiache potrebbe indossare un cerotto ECG. Un modello di IA analizza i flussi di dati sanitari longitudinali del paziente nel contesto delle sue cartelle cliniche elettroniche: diagnosi, esami di laboratorio, farmaci e dati demografici. Il modello di IA suggerisce modifiche al regime di trattamento su misura per quell‚Äôindividuo, come la modifica di una dose di farmaco o di un programma di somministrazione. Piccole modifiche entro un intervallo di sicurezza pre-approvato possono essere apportate dal paziente in modo indipendente, mentre le modifiche pi√π importanti vengono prima esaminate dal medico. Questo stretto feedback tra la fisiologia del paziente e la terapia guidata dall‚ÄôIA consente ottimizzazioni tempestive basate sui dati come raccomandazioni automatizzate sul dosaggio di insulina basate sui livelli di glucosio in tempo reale per i pazienti diabetici.</p>
</section>
<section id="ciclo-clinico-ia" class="level5">
<h5 class="anchored" data-anchor-id="ciclo-clinico-ia">Ciclo Clinico-IA</h5>
<p>Il ciclo clinico-IA consente la supervisione clinica sulle raccomandazioni generate dall‚ÄôIA per garantire sicurezza e responsabilit√†. Il modello di IA fornisce al medico raccomandazioni terapeutiche e riepiloghi facilmente esaminabili dei dati rilevanti del paziente su cui si basano i suggerimenti. Ad esempio, un‚ÄôIA pu√≤ suggerire di ridurre la dose di farmaci per la pressione sanguigna di un paziente iperteso in base a letture costantemente basse. Il medico pu√≤ accettare, rifiutare o modificare le modifiche alla prescrizione proposte dall‚ÄôIA. Questo feedback del medico addestra e migliora ulteriormente il modello. Inoltre, il medico stabilisce i limiti per i tipi e l‚Äôentit√† delle modifiche al trattamento che l‚ÄôIA pu√≤ raccomandare autonomamente ai pazienti. Esaminando i suggerimenti dell‚ÄôIA, il medico mantiene l‚Äôautorit√† di trattamento finale in base al proprio giudizio clinico e alla propria responsabilit√†. Questo ciclo consente loro di supervisionare i casi dei pazienti con l‚Äôassistenza dell‚ÄôIA in modo efficiente.</p>
</section>
<section id="ciclo-paziente-clinico" class="level5">
<h5 class="anchored" data-anchor-id="ciclo-paziente-clinico">Ciclo Paziente-Clinico</h5>
<p>Invece di una raccolta dati di routine, il medico pu√≤ concentrarsi sull‚Äôinterpretazione di modelli di dati di alto livello e sulla collaborazione con il paziente per stabilire obiettivi e priorit√† di salute. L‚Äôassistenza AI liberer√† anche il tempo dei medici, consentendo loro di concentrarsi maggiormente sull‚Äôascolto delle storie e delle preoccupazioni dei pazienti. Ad esempio, il medico pu√≤ discutere di cambiamenti di dieta ed esercizio fisico con un paziente diabetico per migliorare il controllo del glucosio in base ai dati di monitoraggio continuo. La frequenza degli appuntamenti pu√≤ anche essere regolata dinamicamente in base ai progressi del paziente anzich√© seguire un calendario fisso. Liberato dalla raccolta di dati di base, il medico pu√≤ fornire ‚Äúcoaching‚Äù e cure personalizzate a ciascun paziente informato dai suoi dati sanitari continui. La relazione paziente-medico diventa pi√π produttiva e personalizzata.</p>
</section>
</section>
<section id="esempio-di-ipertensione" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="esempio-di-ipertensione">Esempio di Ipertensione</h4>
<p>Consideriamo un esempio. Secondo i ‚ÄúCenters for Disease Control and Prevention‚Äù, quasi la met√† degli adulti soffre di ipertensione (48.1%, 119.9 milioni). L‚Äôipertensione pu√≤ essere gestita tramite ClinAIOps con l‚Äôaiuto di sensori indossabili utilizzando il seguente approccio:</p>
<section id="raccolta-dati" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="raccolta-dati">Raccolta Dati</h5>
<p>I dati raccolti includerebbero il monitoraggio continuo della pressione sanguigna tramite un dispositivo indossato al polso dotato di sensori per fotopletismografia (PPG) ed elettrocardiografia (ECG) per stimare la pressione sanguigna <span class="citation" data-cites="zhang2017highly">(<a href="../../references.html#ref-zhang2017highly" role="doc-biblioref">Q. Zhang, Zhou, e Zeng 2017</a>)</span>. Il dispositivo indossabile monitorerebbe anche l‚Äôattivit√† fisica del paziente tramite accelerometri embedded. Il paziente registrerebbe tutti i farmaci antipertensivi assunti, insieme all‚Äôora e alla dose. Verrebbero inoltre incorporati i dettagli demografici e la storia clinica del paziente dalla sua cartella clinica elettronica (EHR). Questi dati multimodali del mondo reale forniscono un contesto prezioso al modello di intelligenza artificiale per analizzare i modelli di pressione sanguigna del paziente, i livelli di attivit√†, l‚Äôaderenza ai farmaci e le risposte alla terapia.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2017highly" class="csl-entry" role="listitem">
Zhang, Qingxue, Dian Zhou, e Xuan Zeng. 2017. <span>¬´Highly wearable cuff-less blood pressure and heart rate monitoring with single-arm electrocardiogram and photoplethysmogram signals¬ª</span>. <em>BioMedical Engineering OnLine</em> 16 (1): 23. <a href="https://doi.org/10.1186/s12938-017-0317-z">https://doi.org/10.1186/s12938-017-0317-z</a>.
</div></div></section>
<section id="modello-di-intelligenza-artificiale" class="level5">
<h5 class="anchored" data-anchor-id="modello-di-intelligenza-artificiale">Modello di Intelligenza Artificiale</h5>
<p>Il modello di intelligenza artificiale sul dispositivo analizzerebbe le tendenze continue della pressione sanguigna del paziente, i modelli circadiani, i livelli di attivit√† fisica, i comportamenti di aderenza ai farmaci e altri contesti. Utilizzerebbe ML per prevedere dosi ottimali di farmaci antipertensivi e tempi per controllare la pressione sanguigna dell‚Äôindividuo. Il modello invierebbe raccomandazioni di modifica del dosaggio direttamente al paziente per piccoli aggiustamenti o al medico revisore per l‚Äôapprovazione per modifiche pi√π significative. Osservando il feedback clinico sulle sue raccomandazioni e valutando i risultati ottenuti sulla pressione sanguigna nei pazienti, il modello di intelligenza artificiale potrebbe essere continuamente riqualificato per migliorarne le prestazioni. L‚Äôobiettivo √® una gestione della pressione sanguigna completamente personalizzata ottimizzata per le esigenze e le risposte di ciascun paziente.</p>
</section>
<section id="ciclo-paziente-ia-1" class="level5">
<h5 class="anchored" data-anchor-id="ciclo-paziente-ia-1">Ciclo Paziente-IA</h5>
<p>Nel ciclo Paziente-IA, il paziente iperteso riceverebbe notifiche sul suo dispositivo indossabile o sull‚Äôapp per smartphone collegata che raccomandano modifiche ai suoi farmaci antipertensivi. Per piccole modifiche della dose entro un intervallo di sicurezza predefinito, il paziente potrebbe implementare in modo indipendente la modifica suggerita dal modello di IA al suo regime. Tuttavia, il paziente deve ottenere l‚Äôapprovazione del medico prima di modificare il dosaggio per modifiche pi√π significative. Fornire raccomandazioni personalizzate e tempestive sui farmaci automatizza un elemento di autogestione dell‚Äôipertensione per il paziente. Pu√≤ migliorare la sua aderenza al regime e i risultati del trattamento. Il paziente √® autorizzato a sfruttare le informazioni dell‚ÄôIA per controllare meglio la sua pressione sanguigna.</p>
</section>
<section id="ciclo-clinico-ia-1" class="level5">
<h5 class="anchored" data-anchor-id="ciclo-clinico-ia-1">Ciclo Clinico-IA</h5>
<p>Nel ciclo Clinico-IA, il fornitore riceverebbe riepiloghi delle tendenze continue della pressione sanguigna del paziente e visualizzazioni dei suoi modelli di assunzione dei farmaci e dell‚Äôaderenza. Esaminano le modifiche al dosaggio antipertensivo suggerite dal modello AI e decidono se approvare, rifiutare o modificare le raccomandazioni prima che raggiungano il paziente. Il medico specifica anche i limiti di quanto l‚ÄôAI pu√≤ raccomandare in modo indipendente di modificare i dosaggi senza la supervisione del medico. Se la pressione sanguigna del paziente tende a livelli pericolosi, il sistema avvisa il medico in modo che possa intervenire tempestivamente e modificare i farmaci o richiedere una visita al pronto soccorso. Questo ciclo mantiene responsabilit√† e sicurezza consentendo al contempo al medico di sfruttare le intuizioni dell‚ÄôAI mantenendo il medico responsabile dell‚Äôapprovazione delle principali modifiche al trattamento.</p>
</section>
<section id="ciclo-paziente-clinico-1" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="ciclo-paziente-clinico-1">Ciclo Paziente-Clinico</h5>
<p>Nel ciclo Paziente-Clinico, mostrato in <a href="#fig-interactive-loop" class="quarto-xref">Figura&nbsp;<span>13.9</span></a>, le visite di persona si concentrerebbero meno sulla raccolta di dati o sulle modifiche di base dei farmaci. Invece, il medico potrebbe interpretare tendenze e modelli di alto livello nei dati di monitoraggio continuo del paziente e avere discussioni mirate su dieta, esercizio fisico, gestione dello stress e altri cambiamenti nello stile di vita per migliorare il controllo della pressione sanguigna in modo olistico. La frequenza degli appuntamenti potrebbe essere ottimizzata dinamicamente in base alla stabilit√† del paziente anzich√© seguire un calendario fisso. Poich√© il medico non avrebbe bisogno di rivedere tutti i dati granulari, potrebbe concentrarsi sulla fornitura di cure e raccomandazioni personalizzate durante le visite. Con il monitoraggio continuo e l‚Äôottimizzazione assistita dall‚Äôintelligenza artificiale dei farmaci tra le visite, la relazione medico-paziente si concentra sugli obiettivi di benessere generale e diventa pi√π incisiva. Questo approccio proattivo e personalizzato basato sui dati pu√≤ aiutare a evitare complicazioni dell‚Äôipertensione come ictus, insufficienza cardiaca e altre minacce alla salute e al benessere del paziente.</p>
<div id="fig-interactive-loop" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-interactive-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/clinaiops_loops.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interactive-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;13.9: Ciclo interattivo ClinAIOps. Fonte: <span class="citation" data-cites="chen2023framework">Chen et al. (<a href="../../references.html#ref-chen2023framework" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-chen2023framework" class="csl-entry" role="listitem">
Chen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, e Pranav Rajpurkar. 2023. <span>¬´A framework for integrating artificial intelligence for clinical care with continuous therapeutic monitoring¬ª</span>. <em>Nature Biomedical Engineering</em>, novembre. <a href="https://doi.org/10.1038/s41551-023-01115-0">https://doi.org/10.1038/s41551-023-01115-0</a>.
</div></div></figure>
</div>
</section>
</section>
<section id="mlops-vs.-clinaiops" class="level4">
<h4 class="anchored" data-anchor-id="mlops-vs.-clinaiops">MLOps vs.&nbsp;ClinAIOps</h4>
<p>L‚Äôesempio dell‚Äôipertensione illustra bene perch√© i tradizionali MLOps sono insufficienti per molte applicazioni AI del mondo reale e perch√© sono invece necessari framework come ClinAIOps.</p>
<p>Con l‚Äôipertensione, il semplice sviluppo e distribuzione di un modello ML per la regolazione dei farmaci avrebbe successo solo se considerasse il contesto clinico pi√π ampio. Il paziente, il medico e il sistema sanitario hanno preoccupazioni sulla definizione dell‚Äôadozione. Il modello AI non pu√≤ ottimizzare da solo i risultati della pressione sanguigna: richiede l‚Äôintegrazione con flussi di lavoro, comportamenti e incentivi.</p>
<ul>
<li>Alcune lacune chiave evidenziate dall‚Äôesempio in un approccio MLOps puro:</li>
<li>Il modello stesso non avrebbe i dati dei pazienti del mondo reale su larga scala per raccomandare trattamenti in modo affidabile. ClinAIOps consente ci√≤ raccogliendo feedback da medici e pazienti tramite monitoraggio continuo.</li>
<li>I medici si fiderebbero delle raccomandazioni del modello solo con trasparenza, spiegabilit√† e responsabilit√†. ClinAIOps mantiene il medico informato per creare fiducia.</li>
<li>I pazienti hanno bisogno di coaching e motivazione personalizzati, non solo di notifiche AI. Il ciclo paziente-clinico di ClinAIOps facilita questo.</li>
<li>L‚Äôaffidabilit√† dei sensori e l‚Äôaccuratezza dei dati sarebbero sufficienti solo con la supervisione clinica. ClinAIOps convalida le raccomandazioni.</li>
<li>La responsabilit√† per i risultati del trattamento deve essere chiarita solo con un modello ML. ClinAIOps mantiene la responsabilit√† umana.</li>
<li>I sistemi sanitari dovrebbero dimostrare il valore per cambiare i flussi di lavoro. ClinAIOps allinea le parti interessate.</li>
</ul>
<p>Il caso dell‚Äôipertensione mostra chiaramente la necessit√† di guardare oltre il training e l‚Äôimplementazione di un modello ML performante per considerare l‚Äôintero sistema sociotecnico umano-IA. Questa √® la lacuna principale che ClinAIOps colma rispetto ai tradizionali MLOps. I tradizionali MLOps sono eccessivamente focalizzati sulla tecnologia per automatizzare lo sviluppo e l‚Äôimplementazione del modello ML, mentre ClinAIOps incorpora il contesto clinico e il coordinamento umano-IA attraverso cicli di feedback multi-stakeholder.</p>
<p><a href="#tbl-clinical_ops" class="quarto-xref">Tabella&nbsp;<span>13.3</span></a> li confronta. Questa tabella evidenzia come, quando si implementa MLOps, sia necessario considerare pi√π dei semplici modelli ML.</p>
<div id="tbl-clinical_ops" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-clinical_ops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;13.3: Confronto tra operazioni MLOps e AI per uso clinico.
</figcaption>
<div aria-describedby="tbl-clinical_ops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 39%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">MLOps tradizionali</th>
<th style="text-align: left;">ClinAIOps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Focus</td>
<td style="text-align: left;">Sviluppo e distribuzione di modelli ML</td>
<td style="text-align: left;">Coordinamento del processo decisionale umano e AI</td>
</tr>
<tr class="even">
<td style="text-align: left;">Parti interessate</td>
<td style="text-align: left;">Data scientist, ingegneri IT</td>
<td style="text-align: left;">Pazienti, medici, sviluppatori AI</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cicli di feedback</td>
<td style="text-align: left;">Riqualificazione del modello, monitoraggio</td>
<td style="text-align: left;">Paziente-IA, clinico-IA, paziente-clinico</td>
</tr>
<tr class="even">
<td style="text-align: left;">Obiettivo</td>
<td style="text-align: left;">Rendere operative le distribuzioni ML</td>
<td style="text-align: left;">Ottimizzare i risultati di salute del paziente</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Processi</td>
<td style="text-align: left;">Pipeline e infrastruttura automatizzate</td>
<td style="text-align: left;">Integra flussi di lavoro clinici e supervisione</td>
</tr>
<tr class="even">
<td style="text-align: left;">Considerazioni sui dati</td>
<td style="text-align: left;">Creazione di set di dati di training</td>
<td style="text-align: left;">Privacy, etica, informazioni sanitarie protette</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Validazione del modello</td>
<td style="text-align: left;">Test delle metriche delle prestazioni del modello</td>
<td style="text-align: left;">Valutazione clinica delle raccomandazioni</td>
</tr>
<tr class="even">
<td style="text-align: left;">Implementazione</td>
<td style="text-align: left;">Si concentra sull‚Äôintegrazione tecnica</td>
<td style="text-align: left;">Allinea gli incentivi degli stakeholder umani</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="riepilogo-1" class="level4">
<h4 class="anchored" data-anchor-id="riepilogo-1">Riepilogo</h4>
<p>In ambiti complessi come l‚Äôassistenza sanitaria, l‚Äôimplementazione di successo dell‚ÄôIA richiede di andare oltre un focus ristretto sul training e il deploying di modelli ML performanti. Come illustrato nell‚Äôesempio dell‚Äôipertensione, l‚Äôintegrazione dell‚ÄôIA nel mondo reale richiede il coordinamento di diverse parti interessate, l‚Äôallineamento degli incentivi, la convalida delle raccomandazioni e il mantenimento della responsabilit√†. Framework come ClinAIOps, che facilitano il processo decisionale collaborativo tra uomo e IA attraverso cicli di feedback integrati, sono necessari per affrontare queste sfide multiformi. Invece di automatizzare semplicemente le attivit√†, l‚ÄôIA deve aumentare le capacit√† umane e i flussi di lavoro clinici. Ci√≤ consente all‚ÄôIA di avere un impatto positivo sui risultati dei pazienti, sulla salute della popolazione e sull‚Äôefficienza dell‚Äôassistenza sanitaria.</p>
</section>
</section>
</section>
<section id="conclusione" class="level2" data-number="13.9">
<h2 data-number="13.9" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">13.9</span> Conclusione</h2>
<p>L‚ÄôML embedded √® pronto a trasformare molti settori abilitando le funzionalit√† AI direttamente su dispositivi edge come smartphone, sensori e hardware IoT. Tuttavia, lo sviluppo e l‚Äôimplementazione di modelli TinyML su sistemi embedded con risorse limitate pone sfide uniche rispetto ai tradizionali MLOps basati su cloud.</p>
<p>Questo capitolo ha fornito un‚Äôanalisi approfondita delle principali differenze tra MLOps tradizionali ed embedded nel ciclo di vita del modello, flussi di lavoro di sviluppo, gestione dell‚Äôinfrastruttura e pratiche operative. Abbiamo discusso di come fattori come connettivit√† intermittente, dati decentralizzati e computing limitato sul dispositivo richiedano tecniche innovative come apprendimento federato, inferenza sul dispositivo e ottimizzazione del modello. Modelli architettonici come apprendimento cross-device e infrastruttura edge-cloud gerarchica aiutano a mitigare i vincoli.</p>
<p>Attraverso esempi concreti come Oura Ring e ClinAIOps, abbiamo dimostrato i principi applicati per MLOps embedded. I casi di studio hanno evidenziato considerazioni critiche che vanno oltre l‚Äôingegneria ML di base, come l‚Äôallineamento degli incentivi delle parti interessate, il mantenimento della responsabilit√† e il coordinamento del processo decisionale tra uomo e IA. Ci√≤ sottolinea la necessit√† di un approccio olistico che abbracci sia gli elementi tecnici che quelli umani.</p>
<p>Mentre gli MLOps embedded incontrano degli ostacoli, strumenti emergenti come Edge Impulse e lezioni dai pionieri aiutano ad accelerare l‚Äôinnovazione del TinyML. Una solida comprensione dei principi fondamentali degli MLOps adattati agli ambienti embedded consentir√† a pi√π organizzazioni di superare i vincoli e fornire capacit√† di intelligenza artificiale distribuita. Man mano che i framework e le best practice maturano, l‚Äôintegrazione fluida dell‚ÄôML nei dispositivi e nei processi edge trasformer√† i settori attraverso l‚Äôintelligenza localizzata.</p>
</section>
<section id="sec-embedded-aiops-resource" class="level2" data-number="13.10">
<h2 data-number="13.10" class="anchored" data-anchor-id="sec-embedded-aiops-resource"><span class="header-section-number">13.10</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1vsC8WpmvVRgMTpzTltAhEGzcVohMkatMZBqm3-P8TUY/edit?usp=drive_link">MLOps, DevOps, and AIOps.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1GVduKipd0ughTpqsHupGqAPW70h0xNOOpaIeSqLOc1M/edit?usp=drive_link">MLOps overview.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1MNjVOcx5f5Nfe3ElDqTxutezcGXm4yI8PkjWOuQYHhk/edit?usp=drive_link">Tiny MLOps.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1449rzplaL0lOPoKh0mrpds3KPPoOHWdR5LIZdd7aXhA/edit#slide=id.g2ddfdf6e85f_0_0">MLOps: a use case.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1vGCffLgemxTwTIo7vUea5CibOV7y3vY3pkJdee-y5eA/edit#slide=id.g2de2d5f2ac0_0_0">MLOps: Key Activities and Lifecycle.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1FW8Q1Yj5g_jbArFANfncbLQj36uV2vfV8pjoqaD6gjM/edit#slide=id.g94db9f9f78_0_2">ML Lifecycle.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1VxwhVztoTk3eG04FD9fFNpj2lVrVjYYPJi3jBz0O_mo/edit?resourcekey=0-bV7CCIPr7SxZf2p61oB_CA#slide=id.g94db9f9f78_0_2">Scaling TinyML: Challenges and Opportunities.</a></p></li>
<li><p>Operazionalizzazione del Training:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1YyRY6lOzdC7NjutJSvl_VXYu29qwHKqx0y98zAUCJCU/edit?resourcekey=0-PTh1FxqkQyhOO0bKKHBldQ#slide=id.g94db9f9f78_0_2">Training Ops: CI/CD trigger.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1poGgYTH44X0dVGwG9FGIyVwot4EET_jJOt-4kgcQawo/edit?usp=drive_link">Continuous Integration.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1nxbIluROAOl5cN6Ug4Dm-mHh1Fwm5aEng_S5iLfiCqo/edit?usp=drive_link&amp;resourcekey=0-xFOl8i7ea2vNtiilXz8CaQ">Continuous Deployment.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1m8KkCZRnbJCCTWsmcwMt9EJhYLoaVG_Wm7zUE2bQkZI/edit?usp=drive_link">Production Deployment.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1elFEK61X5Kc-5UV_4AEtRvCT7l1TqTdABmJV8uAYykY/edit?usp=drive_link">Production Deployment: Online Experimentation.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1-6QL2rq0ahGVz8BL1M1BT0lR-HDxsHady9lGTN93wLc/edit?usp=drive_link&amp;resourcekey=0-sRqqoa7pX9IkDDSwe2MLyw">Training Ops Impact on MLOps.</a></p></li>
</ul></li>
<li><p>Deployment del Modello:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/12sf-PvSxDIlCQCXULWy4jLY_2fIq-jpRojRsmeMGq6k/edit?resourcekey=0-knPSQ5h4ffhgeV6CXvwlSg#slide=id.gf209f12c63_0_314">Scaling ML Into Production Deployment.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1YXE4cAWMwL79Vqr_8TJi-LsQD9GFdiyBqY--HcoBpKg/edit?usp=drive_link&amp;resourcekey=0-yajtiQTx2SdJ6BCVG0Bfng">Containers for Scaling ML Deployment.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1mw5FFERf5r-q8R7iyNf6kx2MMcwNOTBd5WwFOj8Zs20/edit?resourcekey=0-u80KeJio3iIWco00crGD9g#slide=id.gdc4defd718_0_0">Challenges for Scaling TinyML Deployment: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1NB63wTHoEPGSn--KqFu1vjHx3Ild9AOhpBbflJP-k7I/edit?usp=drive_link&amp;resourcekey=0-MsEi1Lba2dpl0G-bzakHJQ">Challenges for Scaling TinyML Deployment: Part 2.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1A0pfm55s03dFbYKKFRV-x7pRCm_2-VpoIM0O9kW0TAA/edit?usp=drive_link&amp;resourcekey=0--O2AFFmVzAmz5KO0mJeVHA">Model Deployment Impact on MLOps.</a></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><p><a href="#vid-mlops" class="quarto-xref">Video&nbsp;<span>13.1</span></a></p></li>
<li><p><a href="#vid-datapipe" class="quarto-xref">Video&nbsp;<span>13.2</span></a></p></li>
<li><p><a href="#vid-monitoring" class="quarto-xref">Video&nbsp;<span>13.3</span></a></p></li>
<li><p><a href="#vid-deploy" class="quarto-xref">Video&nbsp;<span>13.4</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><a href="#exr-ei" class="quarto-xref">Esercizio&nbsp;<span>13.1</span></a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laboratori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Oltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l‚Äôesperienza di apprendimento.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/ondevice_learning/ondevice_learning.it.html" class="pagination-link" aria-label="Apprendimento On-Device">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/privacy_security/privacy_security.it.html" class="pagination-link" aria-label="Sicurezza e Privacy">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/ops/ops.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/ops/ops.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>