<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Framework di IA – Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/training/training.it.html" rel="next">
<link href="../../contents/data_engineering/data_engineering.it.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalità oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/workflow/workflow.it.html">Workflow</a></li><li class="breadcrumb-item"><a href="../../contents/frameworks/frameworks.it.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2d6cdf6fc58f1105ce2a5c5c28a11153" class="alert alert-info hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>🌟 Aiutaci a raggiungere 1.000 stelle GitHub! 🌟 Per ogni 25 stelle, Arduino e SEEED doneranno una NiclaVision o una XIAO ESP32S3 per l’istruzione sull’intelligenza artificiale. <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per una ⭐</a></p>
</div></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">PREFAZIONE</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">PARTE PRINCIPALE</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Nozioni Fondamentali</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Workflow</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell’IA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell’IA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell’IA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Argomenti Avanzati</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Impatto Sociale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Chiusura</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tool</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Dataset</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Risorse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Le Community</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Casi di Studio</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">6.1</span> Introduzione</a></li>
  <li><a href="#evoluzione-dei-framework" id="toc-evoluzione-dei-framework" class="nav-link" data-scroll-target="#evoluzione-dei-framework"><span class="header-section-number">6.2</span> Evoluzione dei Framework</a></li>
  <li><a href="#sec-deep_dive_into_tensorflow" id="toc-sec-deep_dive_into_tensorflow" class="nav-link" data-scroll-target="#sec-deep_dive_into_tensorflow"><span class="header-section-number">6.3</span> Approfondimento su TensorFlow</a>
  <ul>
  <li><a href="#ecosistema-tf" id="toc-ecosistema-tf" class="nav-link" data-scroll-target="#ecosistema-tf"><span class="header-section-number">6.3.1</span> Ecosistema TF</a></li>
  <li><a href="#grafico-di-calcolo-statico" id="toc-grafico-di-calcolo-statico" class="nav-link" data-scroll-target="#grafico-di-calcolo-statico"><span class="header-section-number">6.3.2</span> Grafico di Calcolo Statico</a></li>
  <li><a href="#usabilità-distribuzione" id="toc-usabilità-distribuzione" class="nav-link" data-scroll-target="#usabilità-distribuzione"><span class="header-section-number">6.3.3</span> Usabilità &amp; Distribuzione</a></li>
  <li><a href="#progettazione-dellarchitettura" id="toc-progettazione-dellarchitettura" class="nav-link" data-scroll-target="#progettazione-dellarchitettura"><span class="header-section-number">6.3.4</span> Progettazione dell’Architettura</a></li>
  <li><a href="#funzionalità-native-keras" id="toc-funzionalità-native-keras" class="nav-link" data-scroll-target="#funzionalità-native-keras"><span class="header-section-number">6.3.5</span> Funzionalità Native &amp; Keras</a></li>
  <li><a href="#limitazioni-e-sfide" id="toc-limitazioni-e-sfide" class="nav-link" data-scroll-target="#limitazioni-e-sfide"><span class="header-section-number">6.3.6</span> Limitazioni e Sfide</a></li>
  <li><a href="#sec-pytorch_vs_tensorflow" id="toc-sec-pytorch_vs_tensorflow" class="nav-link" data-scroll-target="#sec-pytorch_vs_tensorflow"><span class="header-section-number">6.3.7</span> PyTorch &amp; TensorFlow</a></li>
  </ul></li>
  <li><a href="#componenti-di-base-del-framework" id="toc-componenti-di-base-del-framework" class="nav-link" data-scroll-target="#componenti-di-base-del-framework"><span class="header-section-number">6.4</span> Componenti di Base del Framework</a>
  <ul>
  <li><a href="#sec-tensor-data-structures" id="toc-sec-tensor-data-structures" class="nav-link" data-scroll-target="#sec-tensor-data-structures"><span class="header-section-number">6.4.1</span> Strutture Dati Tensoriali</a></li>
  <li><a href="#grafi-computazionali" id="toc-grafi-computazionali" class="nav-link" data-scroll-target="#grafi-computazionali"><span class="header-section-number">6.4.2</span> Grafi computazionali</a>
  <ul class="collapse">
  <li><a href="#definizione-di-grafico" id="toc-definizione-di-grafico" class="nav-link" data-scroll-target="#definizione-di-grafico">Definizione di Grafico</a></li>
  <li><a href="#grafi-statici-vs.-dinamici" id="toc-grafi-statici-vs.-dinamici" class="nav-link" data-scroll-target="#grafi-statici-vs.-dinamici">Grafi Statici vs.&nbsp;Dinamici</a></li>
  </ul></li>
  <li><a href="#tool-della-pipeline-dei-dati" id="toc-tool-della-pipeline-dei-dati" class="nav-link" data-scroll-target="#tool-della-pipeline-dei-dati"><span class="header-section-number">6.4.3</span> Tool della Pipeline dei Dati</a>
  <ul class="collapse">
  <li><a href="#loader-dei-dati" id="toc-loader-dei-dati" class="nav-link" data-scroll-target="#loader-dei-dati">Loader dei Dati</a></li>
  </ul></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation"><span class="header-section-number">6.4.4</span> Data Augmentation</a></li>
  <li><a href="#funzioni-loss-e-algoritmi-di-ottimizzazione" id="toc-funzioni-loss-e-algoritmi-di-ottimizzazione" class="nav-link" data-scroll-target="#funzioni-loss-e-algoritmi-di-ottimizzazione"><span class="header-section-number">6.4.5</span> Funzioni Loss e Algoritmi di Ottimizzazione</a></li>
  <li><a href="#supporto-al-training-del-modello" id="toc-supporto-al-training-del-modello" class="nav-link" data-scroll-target="#supporto-al-training-del-modello"><span class="header-section-number">6.4.6</span> Supporto al Training del Modello</a></li>
  <li><a href="#validazione-e-analisi" id="toc-validazione-e-analisi" class="nav-link" data-scroll-target="#validazione-e-analisi"><span class="header-section-number">6.4.7</span> Validazione e Analisi</a>
  <ul class="collapse">
  <li><a href="#metriche-di-valutazione" id="toc-metriche-di-valutazione" class="nav-link" data-scroll-target="#metriche-di-valutazione">Metriche di Valutazione</a></li>
  <li><a href="#visualizzazione" id="toc-visualizzazione" class="nav-link" data-scroll-target="#visualizzazione">Visualizzazione</a></li>
  </ul></li>
  <li><a href="#programmazione-differenziabile" id="toc-programmazione-differenziabile" class="nav-link" data-scroll-target="#programmazione-differenziabile"><span class="header-section-number">6.4.8</span> Programmazione differenziabile</a></li>
  <li><a href="#accelerazione-hardware" id="toc-accelerazione-hardware" class="nav-link" data-scroll-target="#accelerazione-hardware"><span class="header-section-number">6.4.9</span> Accelerazione Hardware</a></li>
  </ul></li>
  <li><a href="#sec-ai_frameworks-advanced" id="toc-sec-ai_frameworks-advanced" class="nav-link" data-scroll-target="#sec-ai_frameworks-advanced"><span class="header-section-number">6.5</span> Funzionalità Avanzate</a>
  <ul>
  <li><a href="#training-distribuito" id="toc-training-distribuito" class="nav-link" data-scroll-target="#training-distribuito"><span class="header-section-number">6.5.1</span> Training distribuito</a></li>
  <li><a href="#conversione-del-modello" id="toc-conversione-del-modello" class="nav-link" data-scroll-target="#conversione-del-modello"><span class="header-section-number">6.5.2</span> Conversione del Modello</a></li>
  <li><a href="#automl-no-codelow-code-ml" id="toc-automl-no-codelow-code-ml" class="nav-link" data-scroll-target="#automl-no-codelow-code-ml"><span class="header-section-number">6.5.3</span> AutoML, No-Code/Low-Code ML</a></li>
  <li><a href="#metodi-di-apprendimento-avanzati" id="toc-metodi-di-apprendimento-avanzati" class="nav-link" data-scroll-target="#metodi-di-apprendimento-avanzati"><span class="header-section-number">6.5.4</span> Metodi di Apprendimento Avanzati</a>
  <ul class="collapse">
  <li><a href="#il-transfer-learning" id="toc-il-transfer-learning" class="nav-link" data-scroll-target="#il-transfer-learning">Il Transfer Learning</a></li>
  <li><a href="#il-federated-learning" id="toc-il-federated-learning" class="nav-link" data-scroll-target="#il-federated-learning">Il Federated Learning</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#specializzazione-del-framework" id="toc-specializzazione-del-framework" class="nav-link" data-scroll-target="#specializzazione-del-framework"><span class="header-section-number">6.6</span> Specializzazione del Framework</a>
  <ul>
  <li><a href="#cloud" id="toc-cloud" class="nav-link" data-scroll-target="#cloud"><span class="header-section-number">6.6.1</span> Cloud</a></li>
  <li><a href="#edge" id="toc-edge" class="nav-link" data-scroll-target="#edge"><span class="header-section-number">6.6.2</span> Edge</a></li>
  <li><a href="#embedded" id="toc-embedded" class="nav-link" data-scroll-target="#embedded"><span class="header-section-number">6.6.3</span> Embedded</a></li>
  </ul></li>
  <li><a href="#sec-ai_frameworks_embedded" id="toc-sec-ai_frameworks_embedded" class="nav-link" data-scroll-target="#sec-ai_frameworks_embedded"><span class="header-section-number">6.7</span> Framework di IA Embedded</a>
  <ul>
  <li><a href="#vincoli-di-risorse" id="toc-vincoli-di-risorse" class="nav-link" data-scroll-target="#vincoli-di-risorse"><span class="header-section-number">6.7.1</span> Vincoli di Risorse</a></li>
  <li><a href="#framework-e-librerie" id="toc-framework-e-librerie" class="nav-link" data-scroll-target="#framework-e-librerie"><span class="header-section-number">6.7.2</span> Framework e Librerie</a></li>
  <li><a href="#sfide" id="toc-sfide" class="nav-link" data-scroll-target="#sfide"><span class="header-section-number">6.7.3</span> Sfide</a>
  <ul class="collapse">
  <li><a href="#ecosistema-frammentato" id="toc-ecosistema-frammentato" class="nav-link" data-scroll-target="#ecosistema-frammentato">Ecosistema Frammentato</a></li>
  <li><a href="#esigenze-hardware-disparate" id="toc-esigenze-hardware-disparate" class="nav-link" data-scroll-target="#esigenze-hardware-disparate">Esigenze Hardware Disparate</a></li>
  <li><a href="#mancanza-di-portabilità" id="toc-mancanza-di-portabilità" class="nav-link" data-scroll-target="#mancanza-di-portabilità">Mancanza di Portabilità</a></li>
  <li><a href="#infrastruttura-incompleta" id="toc-infrastruttura-incompleta" class="nav-link" data-scroll-target="#infrastruttura-incompleta">Infrastruttura Incompleta</a></li>
  <li><a href="#nessun-benchmark-standard" id="toc-nessun-benchmark-standard" class="nav-link" data-scroll-target="#nessun-benchmark-standard">Nessun Benchmark Standard</a></li>
  <li><a href="#test-minimi-del-mondo-reale" id="toc-test-minimi-del-mondo-reale" class="nav-link" data-scroll-target="#test-minimi-del-mondo-reale">Test Minimi del Mondo Reale</a></li>
  <li><a href="#riepilogo" id="toc-riepilogo" class="nav-link" data-scroll-target="#riepilogo">Riepilogo</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#esempi" id="toc-esempi" class="nav-link" data-scroll-target="#esempi"><span class="header-section-number">6.8</span> Esempi</a>
  <ul>
  <li><a href="#interprete" id="toc-interprete" class="nav-link" data-scroll-target="#interprete"><span class="header-section-number">6.8.1</span> Interprete</a></li>
  <li><a href="#basati-su-compilatore" id="toc-basati-su-compilatore" class="nav-link" data-scroll-target="#basati-su-compilatore"><span class="header-section-number">6.8.2</span> Basati su Compilatore</a></li>
  <li><a href="#libreria" id="toc-libreria" class="nav-link" data-scroll-target="#libreria"><span class="header-section-number">6.8.3</span> Libreria</a></li>
  </ul></li>
  <li><a href="#scelta-del-framework-giusto" id="toc-scelta-del-framework-giusto" class="nav-link" data-scroll-target="#scelta-del-framework-giusto"><span class="header-section-number">6.9</span> Scelta del Framework Giusto</a>
  <ul>
  <li><a href="#modello" id="toc-modello" class="nav-link" data-scroll-target="#modello"><span class="header-section-number">6.9.1</span> Modello</a></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software"><span class="header-section-number">6.9.2</span> Software</a></li>
  <li><a href="#hardware" id="toc-hardware" class="nav-link" data-scroll-target="#hardware"><span class="header-section-number">6.9.3</span> Hardware</a></li>
  <li><a href="#altri-fattori" id="toc-altri-fattori" class="nav-link" data-scroll-target="#altri-fattori"><span class="header-section-number">6.9.4</span> Altri Fattori</a>
  <ul class="collapse">
  <li><a href="#prestazioni" id="toc-prestazioni" class="nav-link" data-scroll-target="#prestazioni">Prestazioni</a></li>
  <li><a href="#scalabilità" id="toc-scalabilità" class="nav-link" data-scroll-target="#scalabilità">Scalabilità</a></li>
  <li><a href="#integrazione-con-strumenti-di-data-engineering" id="toc-integrazione-con-strumenti-di-data-engineering" class="nav-link" data-scroll-target="#integrazione-con-strumenti-di-data-engineering">Integrazione con Strumenti di Data Engineering</a></li>
  <li><a href="#integrazione-con-strumenti-di-ottimizzazione-del-modello" id="toc-integrazione-con-strumenti-di-ottimizzazione-del-modello" class="nav-link" data-scroll-target="#integrazione-con-strumenti-di-ottimizzazione-del-modello">Integrazione con Strumenti di Ottimizzazione del Modello</a></li>
  <li><a href="#facilità-duso" id="toc-facilità-duso" class="nav-link" data-scroll-target="#facilità-duso">Facilità d’Uso</a></li>
  <li><a href="#supporto-della-community" id="toc-supporto-della-community" class="nav-link" data-scroll-target="#supporto-della-community">Supporto della Community</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#tendenze-future-nei-framework-ml" id="toc-tendenze-future-nei-framework-ml" class="nav-link" data-scroll-target="#tendenze-future-nei-framework-ml"><span class="header-section-number">6.10</span> Tendenze Future nei Framework ML</a>
  <ul>
  <li><a href="#decomposizione" id="toc-decomposizione" class="nav-link" data-scroll-target="#decomposizione"><span class="header-section-number">6.10.1</span> Decomposizione</a></li>
  <li><a href="#compilatori-e-librerie-ad-alte-prestazioni" id="toc-compilatori-e-librerie-ad-alte-prestazioni" class="nav-link" data-scroll-target="#compilatori-e-librerie-ad-alte-prestazioni"><span class="header-section-number">6.10.2</span> Compilatori e Librerie ad Alte Prestazioni</a></li>
  <li><a href="#ml-per-framework-ml" id="toc-ml-per-framework-ml" class="nav-link" data-scroll-target="#ml-per-framework-ml"><span class="header-section-number">6.10.3</span> ML per Framework ML</a></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">6.11</span> Conclusione</a></li>
  <li><a href="#sec-ai-frameworks-resource" id="toc-sec-ai-frameworks-resource" class="nav-link" data-scroll-target="#sec-ai-frameworks-resource"><span class="header-section-number">6.12</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/frameworks/frameworks.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/frameworks/frameworks.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/workflow/workflow.it.html">Workflow</a></li><li class="breadcrumb-item"><a href="../../contents/frameworks/frameworks.it.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ai_frameworks" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-ai-frameworks-resource">Slide</a>, <a href="#sec-ai-frameworks-resource">Video</a>, <a href="#sec-ai-frameworks-resource">Esercizi</a>, <a href="#sec-ai-frameworks-resource">Laboratori</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ml_frameworks.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Illustrazione in formato rettangolare, progettata per un libro di testo professionale, in cui il contenuto si estende per l’intera larghezza. Il grafico vivace rappresenta i framework di training e inferenza per l’ML. Le icone per TensorFlow, Keras, PyTorch, ONNX e TensorRT sono distribuite, riempiono l’intero spazio orizzontale e sono allineate verticalmente. Ogni icona è accompagnata da brevi annotazioni che ne descrivono le caratteristiche. I colori vividi come blu, verde e arancione evidenziano le icone e le sezioni su uno sfondo sfumato più morbido. La distinzione tra framework di training e inferenza è accentuata tramite sezioni codificate a colori, con linee pulite e tipografia moderna che mantengono chiarezza e attenzione.</em></figcaption>
</figure>
</div>
<p>Questo capitolo esplora il panorama dei framework di intelligenza artificiale che fungono da base per lo sviluppo di sistemi di apprendimento automatico. I framework di intelligenza artificiale forniscono gli strumenti, le librerie e gli ambienti per progettare, addestrare e distribuire modelli di apprendimento automatico. Esploreremo il percorso evolutivo di questi framework, analizzeremo il funzionamento di TensorFlow e forniremo approfondimenti sui componenti principali e sulle funzionalità avanzate che li definiscono.</p>
<p>Inoltre, esaminiamo la specializzazione dei framework su misura per esigenze specifiche, l’emergere di framework progettati espressamente per l’intelligenza artificiale embedded e i criteri per selezionare il framework più adatto ai vari progetti. Questa esplorazione sarà completata da uno sguardo alle tendenze future che dovrebbero modellare il panorama dei framework di apprendimento automatico nei prossimi anni.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell’Apprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere l’evoluzione e le capacità dei principali framework di apprendimento automatico. Ciò include modelli di esecuzione di grafici, paradigmi di programmazione, supporto per l’accelerazione hardware e come si sono espansi nel tempo.</p></li>
<li><p>Scoprire i componenti principali e le funzionalità dei framework, come grafi computazionali, pipeline di dati, algoritmi di ottimizzazione, loop di training, ecc., che consentono una creazione di modelli efficiente.</p></li>
<li><p>Confrontare i framework in diversi ambienti, come cloud, edge e TinyML. Scoprire come i framework si specializzano in base a vincoli computazionali e hardware.</p></li>
<li><p>Approfondire i framework embedded e focalizzarsi su TinyML come TensorFlow Lite Micro, CMSIS-NN, TinyEngine, ecc., e come ottimizzano per i microcontrollori.</p></li>
<li><p>Quando si sceglie un framework, si esplorano le considerazioni sulla conversione e l’implementazione del modello, tra cui latenza, utilizzo della memoria e supporto hardware.</p></li>
<li><p>Valutare i fattori chiave nella selezione del framework giusto, come prestazioni, compatibilità hardware, supporto della community, facilità d’uso, ecc., in base alle esigenze e ai vincoli specifici del progetto.</p></li>
<li><p>Comprendere i limiti dei framework attuali e le potenziali tendenze future, come l’uso del ML per migliorare i framework, i sistemi ML decomposti e i compilatori ad alte prestazioni.</p></li>
</ul>
</div>
</div>
<section id="introduzione" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduzione"><span class="header-section-number">6.1</span> Introduzione</h2>
<p>I framework di machine learning [apprendimento automatico] forniscono gli strumenti e l’infrastruttura per creare, addestrare e distribuire in modo efficiente modelli di apprendimento automatico. In questo capitolo esploreremo l’evoluzione e le capacità chiave dei principali framework come <a href="https://www.tensorflow.org/">TensorFlow (TF)</a>, <a href="https://pytorch.org/">PyTorch</a> e framework specializzati per dispositivi embedded. Ci immergeremo nei componenti come grafi computazionali, algoritmi di ottimizzazione, accelerazione hardware e altro che consentono agli sviluppatori di creare rapidamente modelli performanti. Comprendere questi framework è essenziale per sfruttare la potenza del deep learning in tutto lo spettro, dal cloud ai dispositivi edge [periferici].</p>
<p>I framework di apprendimento automatico gestiscono gran parte della complessità dello sviluppo di modelli tramite API di alto livello e linguaggi specifici per dominio che consentono ai professionisti di creare rapidamente modelli combinando componenti e astrazioni predefiniti. Ad esempio, framework come TensorFlow e PyTorch forniscono API Python per definire architetture di reti neurali utilizzando livelli, ottimizzatori, set di dati e altro. Ciò consente un’iterazione rapida rispetto alla codifica di ogni dettaglio del modello partendo da zero.</p>
<p>Una capacità chiave offerta da questi framework è rappresentata dai motori di training distribuiti che possono scalare l’addestramento del modello su cluster di GPU e TPU. Ciò rende possibile il training di modelli all’avanguardia con miliardi o trilioni di parametri su vasti set di dati. I framework si integrano anche con hardware specializzato come le GPU NVIDIA per accelerare ulteriormente il training tramite ottimizzazioni come la parallelizzazione ed efficienti operazioni matriciali.</p>
<p>Inoltre, i framework semplificano il deploy [distribuzione] di modelli finiti in produzione tramite strumenti come <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> per il model serving scalabile e <a href="https://www.tensorflow.org/lite">TensorFlow Lite</a> per l’ottimizzazione su dispositivi mobili ed edge. Altre capacità preziose includono visualizzazione, tecniche di ottimizzazione del modello come quantizzazione e potatura e monitoraggio delle metriche durante il training.</p>
<p>I principali framework open source come TensorFlow, PyTorch e <a href="https://mxnet.apache.org/versions/1.9.1/">MXNet</a> alimentano gran parte della ricerca e dello sviluppo dell’IA oggi. Offerte commerciali come <a href="https://aws.amazon.com/pm/sagemaker/">Amazon SageMaker</a> e <a href="https://azure.microsoft.com/en-us/free/machine-learning/search/?ef_id=_k_CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE_k_&amp;OCID=AIDcmm5edswduu_SEM__k_CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE_k_&amp;gad=1&amp;gclid=CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE">Microsoft Azure Machine Learning</a> integrano questi framework open source con funzionalità proprietarie e strumenti aziendali.</p>
<p>Gli ingegneri e i professionisti del machine learning sfruttano questi framework robusti per concentrarsi su attività di alto valore come architettura del modello, progettazione delle feature e ottimizzazione degli iperparametri anziché sull’infrastruttura. L’obiettivo è creare e distribuire modelli performanti che risolvano in modo efficiente i problemi del mondo reale.</p>
<p>In questo capitolo, esploreremo i principali framework cloud odierni e il modo in cui hanno adattato modelli e strumenti specificamente per la distribuzione embedded ed edge. Confronteremo modelli di programmazione, hardware supportato, capacità di ottimizzazione e altro ancora per comprendere appieno in che modo i framework consentono un apprendimento automatico scalabile dal cloud all’edge.</p>
</section>
<section id="evoluzione-dei-framework" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="evoluzione-dei-framework"><span class="header-section-number">6.2</span> Evoluzione dei Framework</h2>
<p>I framework di apprendimento automatico si sono evoluti in modo significativo per soddisfare le diverse esigenze dei professionisti del machine learning e i progressi nelle tecniche di intelligenza artificiale. Qualche decennio fa, la creazione e l’addestramento di modelli di apprendimento automatico richiedevano un’ampia codifica e infrastruttura di basso livello. Oltre alla necessità di una codifica di basso livello, la prima ricerca sulle reti neurali era limitata da dati e potenza di calcolo insufficienti. Tuttavia, i framework di apprendimento automatico si sono evoluti notevolmente nell’ultimo decennio per soddisfare le crescenti esigenze dei professionisti e i rapidi progressi nelle tecniche di deep learning [apprendimento profondo]. Il rilascio di grandi set di dati come <a href="https://www.image-net.org/">ImageNet</a> <span class="citation" data-cites="deng2009imagenet">(<a href="../../references.it.html#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>)</span> e i progressi nel calcolo parallelo con GPU hanno sbloccato il potenziale per reti neurali molto più profonde.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, e Fei-Fei Li. 2009. <span>«<span>ImageNet:</span> <span>A</span> large-scale hierarchical image database»</span>. In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. IEEE. <a href="https://doi.org/10.1109/cvpr.2009.5206848">https://doi.org/10.1109/cvpr.2009.5206848</a>.
</div><div id="ref-al2016theano" class="csl-entry" role="listitem">
Team, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. <span>«Theano: <span>A</span> Python framework for fast computation of mathematical expressions»</span>. <a href="https://arxiv.org/abs/1605.02688">https://arxiv.org/abs/1605.02688</a>.
</div><div id="ref-jia2014caffe" class="csl-entry" role="listitem">
Jia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, e Trevor Darrell. 2014. <span>«Caffe: Convolutional Architecture for Fast Feature Embedding»</span>. In <em>Proceedings of the 22nd ACM international conference on Multimedia</em>, 675–78. ACM. <a href="https://doi.org/10.1145/2647868.2654889">https://doi.org/10.1145/2647868.2654889</a>.
</div><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. <span>«<span>ImageNet</span> Classification with Deep Convolutional Neural Networks»</span>. In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1106–14. <a href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>.
</div><div id="ref-chollet2018keras" class="csl-entry" role="listitem">
Chollet, François. 2018. <span>«Introduction to keras»</span>. <em>March 9th</em>.
</div><div id="ref-tokui2015chainer" class="csl-entry" role="listitem">
Tokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, e Hiroyuki Yamazaki Vincent. 2019. <span>«Chainer: A Deep Learning Framework for Accelerating the Research Cycle»</span>. In <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining</em>, 5:1–6. ACM. <a href="https://doi.org/10.1145/3292500.3330756">https://doi.org/10.1145/3292500.3330756</a>.
</div><div id="ref-seide2016cntk" class="csl-entry" role="listitem">
Seide, Frank, e Amit Agarwal. 2016. <span>«Cntk: Microsoft’s Open-Source Deep-Learning Toolkit»</span>. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 2135–35. ACM. <a href="https://doi.org/10.1145/2939672.2945397">https://doi.org/10.1145/2939672.2945397</a>.
</div><div id="ref-paszke2019pytorch" class="csl-entry" role="listitem">
Ansel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. <span>«<span>PyTorch</span> 2: <span>Faster</span> Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation»</span>. In <em>Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, e Roman Garnett, 8024–35. ACM. <a href="https://doi.org/10.1145/3620665.3640366">https://doi.org/10.1145/3620665.3640366</a>.
</div></div><p>I primi framework di apprendimento automatico, <a href="https://pypi.org/project/Theano/#:~:text=Theano">Theano</a> di <span class="citation" data-cites="al2016theano">Team et al. (<a href="../../references.it.html#ref-al2016theano" role="doc-biblioref">2016</a>)</span> e <a href="https://caffe.berkeleyvision.org/">Caffe</a> di <span class="citation" data-cites="jia2014caffe">Jia et al. (<a href="../../references.it.html#ref-jia2014caffe" role="doc-biblioref">2014</a>)</span>, sono stati sviluppati da istituzioni accademiche. Theano è stato creato dal Montreal Institute for Learning Algorithms, mentre Caffe è stato sviluppato dal Berkeley Vision and Learning Center. Nel crescente interesse per il deep learning dovuto alle prestazioni all’avanguardia di AlexNet <span class="citation" data-cites="krizhevsky2012imagenet">Krizhevsky, Sutskever, e Hinton (<a href="../../references.it.html#ref-krizhevsky2012imagenet" role="doc-biblioref">2012</a>)</span> sul dataset ImageNet, aziende private e singole persone hanno iniziato a sviluppare framework di ML, dando vita a <a href="https://keras.io/">Keras</a> di <span class="citation" data-cites="chollet2018keras">Chollet (<a href="../../references.it.html#ref-chollet2018keras" role="doc-biblioref">2018</a>)</span>, <a href="https://chainer.org/">Chainer</a> di <span class="citation" data-cites="tokui2015chainer">Tokui et al. (<a href="../../references.it.html#ref-tokui2015chainer" role="doc-biblioref">2019</a>)</span>, TensorFlow di Google <span class="citation" data-cites="abadi2016tensorflow">(<a href="../../references.it.html#ref-abadi2016tensorflow" role="doc-biblioref">Yu et al. 2018</a>)</span>, <a href="https://learn.microsoft.com/en-us/cognitive-toolkit/">CNTK</a> di Microsoft <span class="citation" data-cites="seide2016cntk">(<a href="../../references.it.html#ref-seide2016cntk" role="doc-biblioref">Seide e Agarwal 2016</a>)</span> e PyTorch di Facebook <span class="citation" data-cites="paszke2019pytorch">(<a href="../../references.it.html#ref-paszke2019pytorch" role="doc-biblioref">Ansel et al. 2024</a>)</span>.</p>
<p>Molti di questi framework ML possono essere suddivisi in framework di alto livello, di basso livello e di grafi computazionali statici e dinamici. I framework di alto livello forniscono un livello di astrazione più elevato rispetto a quelli di basso livello. I framework di alto livello hanno funzioni e moduli predefiniti per attività ML comuni, come la creazione, l’addestramento e la valutazione di modelli ML comuni, la preelaborazione dei dati, le funzionalità di progettazione e la visualizzazione dei dati, che i framework di basso livello non hanno. Pertanto, i framework di alto livello possono risultare più facili da usare ma sono meno personalizzabili rispetto a quelli di basso livello (ad esempio, gli utenti di framework di basso livello possono definire livelli personalizzati, funzioni “loss” [di perdita], algoritmi di ottimizzazione, ecc.). Esempi di framework di alto livello sono TensorFlow/Keras e PyTorch. Esempi di framework ML di basso livello includono TensorFlow con API di basso livello, Theano, Caffe, Chainer e CNTK.</p>
<p>Framework come Theano e Caffe utilizzavano grafi computazionali statici, che richiedevano la definizione anticipata dell’architettura completa del modello, limitandone così la flessibilità. Al contrario, i grafici dinamici vengono costruiti al volo per uno sviluppo più iterativo. Intorno al 2016, framework come PyTorch e TensorFlow 2.0 hanno iniziato ad adottare grafici dinamici, offrendo maggiore flessibilità per lo sviluppo del modello. Discuteremo di questi concetti e dettagli più avanti nella sezione Training dell’IA.</p>
<p>Lo sviluppo di questi framework ha suscitato un’esplosione di dimensioni e complessità del modello nel tempo, dai primi perceptron multistrato e reti convoluzionali ai moderni trasformatori con miliardi o trilioni di parametri. Nel 2016, i modelli ResNet di <span class="citation" data-cites="he2016deep">He et al. (<a href="../../references.it.html#ref-he2016deep" role="doc-biblioref">2016</a>)</span> hanno raggiunto un’accuratezza ImageNet record con oltre 150 livelli e 25 milioni di parametri. Poi, nel 2020, il modello linguistico GPT-3 di OpenAI <span class="citation" data-cites="brown2020language">(<a href="../../references.it.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> ha spinto i parametri a un sorprendente numero di 175 miliardi utilizzando il parallelismo del modello nei framework per addestrare migliaia di GPU e TPU.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. <span>«Deep Residual Learning for Image Recognition»</span>. In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>«Language Models are Few-Shot Learners»</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div></div><p>Ogni generazione di framework ha sbloccato nuove capacità che hanno alimentato il progresso:</p>
<ul>
<li><p>Theano e TensorFlow (2015) hanno introdotto grafi computazionali e differenziazione automatica per semplificare la creazione di modelli.</p></li>
<li><p>CNTK (2016) ha aperto la strada a un addestramento distribuito efficiente combinando parallelismo di modelli e dati.</p></li>
<li><p>PyTorch (2016) ha fornito programmazione imperativa e grafici dinamici per una sperimentazione flessibile.</p></li>
<li><p>TensorFlow 2.0 (2019) ha impostato di default l’esecuzione Eager per intuitività e debug.</p></li>
<li><p>TensorFlow Graphics (2020) ha aggiunto strutture dati 3D per gestire nuvole di punti e mesh.</p></li>
</ul>
<p>Negli ultimi anni, i framework sono convergenti. <a href="#fig-ml-framework" class="quarto-xref">Figura&nbsp;<span>6.2</span></a> mostra che TensorFlow e PyTorch sono diventati i framework ML più dominanti, rappresentando oltre il 95% dei framework ML utilizzati nella ricerca e nella produzione. <a href="#fig-tensorflow-pytorch" class="quarto-xref">Figura&nbsp;<span>6.1</span></a> traccia un contrasto tra gli attributi di TensorFlow e PyTorch. Keras è stato integrato in TensorFlow nel 2019; Preferred Networks ha trasferito Chainer a PyTorch nel 2019; e Microsoft ha smesso di sviluppare attivamente CNTK nel 2022 per supportare PyTorch su Windows.</p>
<div id="fig-tensorflow-pytorch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensorflow-pytorch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tensorflowpytorch.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensorflow-pytorch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.1: PyTorch e TensorFlow: Caratteristiche e Funzioni. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fkruschecompany.com%2Fpytorch-vs-tensorflow%2F&amp;psig=AOvVaw1-DSFxXYprQmYH7Z4Nk6Tk&amp;ust=1722533288351000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCPDhst7m0YcDFQAAAAAdAAAAABAg">K&amp;C</a>
</figcaption>
</figure>
</div>
<div id="fig-ml-framework" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image6.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.2: Popolarità dei framework ML negli Stati Uniti misurata dalle ricerche web di Google. Fonte: Google.
</figcaption>
</figure>
</div>
<p>Un approccio unico non funziona bene in tutto lo spettro, dal cloud ai piccoli dispositivi edge. Diversi framework rappresentano varie filosofie sull’esecuzione di grafici, API dichiarative rispetto a quelle imperative e altro ancora. Le dichiarative definiscono cosa dovrebbe fare il programma, mentre le imperative si concentrano su come dovrebbe essere fatto passo dopo passo. Ad esempio, TensorFlow utilizza l’esecuzione di grafici e la modellazione in stile dichiarativo, mentre PyTorch adotta l’esecuzione rapida e la modellazione imperativa per una maggiore flessibilità con Python. Ogni approccio comporta dei compromessi che discuteremo in <a href="#sec-pytorch_vs_tensorflow" class="quarto-xref"><span>Sezione 6.3.7</span></a>.</p>
<p>Gli attuali framework avanzati consentono ai professionisti di sviluppare e distribuire modelli sempre più complessi, un fattore chiave dell’innovazione nel campo dell’intelligenza artificiale. Questi framework continuano a evolversi ed espandere le loro capacità per la prossima generazione di machine learning. Per capire come questi sistemi continuano a evolversi, approfondiremo TensorFlow come esempio di come il framework sia cresciuto in complessità nel tempo.</p>
</section>
<section id="sec-deep_dive_into_tensorflow" class="level2 page-columns page-full" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec-deep_dive_into_tensorflow"><span class="header-section-number">6.3</span> Approfondimento su TensorFlow</h2>
<p>TensorFlow è stato sviluppato dal team di Google Brain ed è stato rilasciato come libreria software open source il 9 novembre 2015. È stato progettato per il calcolo numerico utilizzando grafici di flusso di dati e da allora è diventato popolare per un’ampia gamma di applicazioni di apprendimento automatico e deep learning.</p>
<p>TensorFlow è un framework di training e inferenza che fornisce funzionalità integrate per gestire tutto, dalla creazione e training del modello alla distribuzione, come mostrato in <a href="#fig-tensorflow-architecture" class="quarto-xref">Figura&nbsp;<span>6.3</span></a>. Sin dal suo sviluppo iniziale, l’ecosistema TensorFlow è cresciuto fino a includere molte diverse “varietà” di TensorFlow, ciascuna pensata per consentire agli utenti di supportare ML su diverse piattaforme. In questa sezione, discuteremo principalmente solo del pacchetto core.</p>
<section id="ecosistema-tf" class="level3 page-columns page-full" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="ecosistema-tf"><span class="header-section-number">6.3.1</span> Ecosistema TF</h3>
<ol type="1">
<li><p><a href="https://www.tensorflow.org/tutorials">TensorFlow Core</a>: pacchetto principale con cui interagiscono la maggior parte degli sviluppatori. Fornisce una piattaforma completa e flessibile per definire, addestrare e distribuire modelli di apprendimento automatico. Include <a href="https://www.tensorflow.org/guide/keras">tf.keras</a> come API di alto livello.</p></li>
<li><p><a href="https://www.tensorflow.org/lite">TensorFlow Lite</a>: progettato per distribuire modelli leggeri su dispositivi mobili, embedded ed edge. Offre strumenti per convertire i modelli TensorFlow in un formato più compatto adatto a dispositivi con risorse limitate e fornisce modelli pre-addestrati ottimizzati per dispositivi mobili.</p></li>
<li><p><a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite Micro</a>: progettato per eseguire modelli di apprendimento automatico su microcontrollori con risorse minime. Funziona senza la necessità di supporto del sistema operativo, librerie C o C++ standard o allocazione dinamica della memoria, utilizzando solo pochi kilobyte di memoria.</p></li>
<li><p><a href="https://www.tensorflow.org/js">TensorFlow.js</a>: libreria JavaScript che consente l’addestramento e la distribuzione di modelli di apprendimento automatico direttamente nel browser o su Node.js. Fornisce inoltre strumenti per il porting di modelli TensorFlow pre-addestrati nel formato browser-friendly.</p></li>
<li><p><a href="https://developers.googleblog.com/2019/03/introducing-coral-our-platform-for.html">TensorFlow su dispositivi edge (Coral)</a>: piattaforma di componenti hardware e strumenti software di Google che consente l’esecuzione di modelli TensorFlow su dispositivi edge, sfruttando Edge TPU per l’accelerazione.</p></li>
<li><p><a href="https://www.tensorflow.org/federated">TensorFlow Federated (TFF)</a>: framework per l’apprendimento automatico e altri calcoli su dati decentralizzati. TFF facilita l’apprendimento “federato”, consentendo l’addestramento del modello su molti dispositivi senza centralizzare i dati.</p></li>
<li><p><a href="https://www.tensorflow.org/graphics">TensorFlow Graphics</a>: libreria per l’utilizzo di TensorFlow per svolgere attività correlate alla grafica, tra cui l’elaborazione di forme 3D e nuvole di punti, utilizzando il deep learning.</p></li>
<li><p><a href="https://www.tensorflow.org/hub">TensorFlow Hub</a>: repository di componenti di modelli di apprendimento automatico riutilizzabili che consente agli sviluppatori di riutilizzare componenti di modelli pre-addestrati, facilitando l’apprendimento per trasferimento e la composizione del modello.</p></li>
<li><p><a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a>: framework progettato per servire e distribuire modelli di apprendimento automatico per l’inferenza in ambienti di produzione. Fornisce strumenti per il versioning e l’aggiornamento dinamico dei modelli distribuiti senza interruzione del servizio.</p></li>
<li><p><a href="https://www.tensorflow.org/tfx">TensorFlow Extended (TFX)</a>: piattaforma end-to-end progettata per distribuire e gestire pipeline di apprendimento automatico in ambienti di produzione. TFX comprende validazione dei dati, pre-elaborazione, addestramento del modello, convalida e componenti di servizio.</p></li>
</ol>
<div id="fig-tensorflow-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensorflow-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tensorflow.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensorflow-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.3: Panoramica dell’architettura di TensorFlow 2.0. Fonte: <a href="https://blog.tensorflow.org/2019/01/whats-coming-in-tensorflow-2-0.html">Tensorflow.</a>
</figcaption>
</figure>
</div>
<p>TensorFlow è stato sviluppato per affrontare le limitazioni di DistBelief <span class="citation" data-cites="abadi2016tensorflow">(<a href="../../references.it.html#ref-abadi2016tensorflow" role="doc-biblioref">Yu et al. 2018</a>)</span>—il framework in uso presso Google dal 2011 al 2015—offrendo flessibilità lungo tre direttrici: 1) definizione di nuovi livelli [livelli], 2) perfezionamento degli algoritmi di training e 3) definizione di nuovi algoritmi di training. Per comprendere quali limitazioni di DistBelief hanno portato allo sviluppo di TensorFlow, faremo prima una breve panoramica dell’architettura del server dei parametri utilizzata da DistBelief <span class="citation" data-cites="dean2012large">(<a href="../../references.it.html#ref-dean2012large" role="doc-biblioref">Dean et al. 2012</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-abadi2016tensorflow" class="csl-entry" role="listitem">
Yu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, et al. 2018. <span>«Dynamic control flow in large-scale machine learning»</span>. In <em>Proceedings of the Thirteenth EuroSys Conference</em>, 265–83. ACM. <a href="https://doi.org/10.1145/3190508.3190551">https://doi.org/10.1145/3190508.3190551</a>.
</div><div id="ref-dean2012large" class="csl-entry" role="listitem">
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. <span>«Large Scale Distributed Deep Networks»</span>. In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1232–40. <a href="https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html</a>.
</div></div><p>L’architettura Parameter Server (PS) è un design popolare per distribuire il training di modelli di apprendimento automatico, in particolare reti neurali profonde, su più macchine. L’idea fondamentale è di separare l’archiviazione e la gestione dei parametri del modello dal calcolo utilizzato per aggiornare tali parametri. In genere, i server dei parametri gestiscono l’archiviazione e la gestione dei parametri del modello, suddividendoli su più server. I processi worker eseguono le attività di calcolo, tra cui l’elaborazione dei dati e il calcolo dei gradienti, che vengono poi inviati ai server dei parametri per l’aggiornamento.</p>
<p><strong>Storage:</strong> I processi del server dei parametri stateful [con stato] gestivano l’archiviazione e la gestione dei parametri del modello. Data l’ampia scala dei modelli e la natura distribuita del sistema, questi parametri erano condivisi tra più server dei parametri. Ogni server manteneva una parte dei parametri del modello, rendendolo "stateful" poiché doveva mantenere e gestire questo stato durante il processo di training.</p>
<p><strong>Computation:</strong> I processi worker, che potevano essere eseguiti in parallelo, erano senza stato e puramente computazionali. Elaboravano dati e calcolavano gradienti senza mantenere alcuno stato o memoria a lungo termine <span class="citation" data-cites="li2014communication">(<a href="../../references.it.html#ref-li2014communication" role="doc-biblioref">M. Li et al. 2014</a>)</span>. I worker non conservavano informazioni tra le diverse attività. Invece, comunicavano periodicamente con i server dei parametri per recuperare i parametri più recenti e restituire i gradienti calcolati.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2014communication" class="csl-entry" role="listitem">
Li, Mu, David G. Andersen, Alexander J. Smola, e Kai Yu. 2014. <span>«Communication Efficient Distributed Machine Learning with the Parameter Server»</span>. In <em>Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada</em>, a cura di Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, e Kilian Q. Weinberger, 19–27. <a href="https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html">https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html</a>.
</div></div><div id="exr-tfc" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;6.1: TensorFlow Core
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Andiamo a comprendere in modo completo gli algoritmi di apprendimento automatico di base utilizzando TensorFlow e le loro applicazioni pratiche nell’analisi dei dati e nella modellazione predittiva. Inizieremo con la regressione lineare per prevedere i tassi di sopravvivenza dal set di dati del Titanic. Poi, utilizzando TensorFlow, costruiremo classificatori per identificare diverse specie di fiori in base ai loro attributi. Successivamente, utilizzeremo l’algoritmo K-Means e la sua applicazione nella segmentazione dei set di dati in cluster coesi. Infine, applicheremo modelli hidden [nascosti] di Markov (HMM) per prevedere i pattern meteorologici.</p>
<p><a href="https://colab.research.google.com/drive/15Cyy2H7nT40sGR7TBN5wBvgTd57mVKay#scrollTo=IEeIRxlbx0wY"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<div id="exr-tfl" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;6.2: TensorFlow Lite
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Qui vedremo come costruire un modello di apprendimento automatico in miniatura per microcontrollori. Costruiremo una mini rete neurale semplificata per apprendere dai dati anche con risorse limitate e ottimizzata per l’implementazione riducendo il nostro modello per un uso efficiente sui microcontrollori. TensorFlow Lite, una potente tecnologia derivata da TensorFlow, riduce i modelli per dispositivi minuscoli e aiuta ad abilitare funzionalità sul dispositivo come il riconoscimento delle immagini nei dispositivi smart [intelligenti]. Viene utilizzato nell’edge computing per consentire analisi e decisioni più rapide nei dispositivi che elaborano i dati localmente.</p>
<p><a href="https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/TFLite-Micro-Hello-World/train_TFL_Micro_hello_world_model.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<p>DistBelief e la sua architettura definita sopra sono stati fondamentali per abilitare il deep learning distribuito in Google, ma hanno anche introdotto delle limitazioni che hanno motivato lo sviluppo di TensorFlow:</p>
</section>
<section id="grafico-di-calcolo-statico" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="grafico-di-calcolo-statico"><span class="header-section-number">6.3.2</span> Grafico di Calcolo Statico</h3>
<p>I parametri del modello sono distribuiti su vari server di parametri nell’architettura del server di parametri. Poiché DistBelief è stato progettato principalmente per il paradigma della rete neurale, i parametri corrispondevano a una struttura di rete neurale fissa. Se il computation graph [grafico di calcolo] fosse dinamico, la distribuzione e il coordinamento dei parametri diventerebbero significativamente più complicati. Ad esempio, una modifica nel grafico potrebbe richiedere l’inizializzazione di nuovi parametri o la rimozione di quelli esistenti, complicando le attività di gestione e sincronizzazione dei server di parametri. Ciò ha reso più difficile implementare modelli al di fuori del framework neurale o modelli che richiedevano grafici di calcolo dinamici.</p>
<p>TensorFlow è stato progettato come un framework di calcolo più generale che esprime il calcolo come un grafico del flusso di dati. Ciò consente una più ampia varietà di modelli e algoritmi di apprendimento automatico al di fuori delle reti neurali e fornisce flessibilità nel perfezionamento dei modelli.</p>
</section>
<section id="usabilità-distribuzione" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="usabilità-distribuzione"><span class="header-section-number">6.3.3</span> Usabilità &amp; Distribuzione</h3>
<p>Il modello del server dei parametri delinea i ruoli (nodi worker e server dei parametri) ed è ottimizzato per i deployment [distribuzioni] dei data center, che potrebbero essere ottimali solo per alcuni casi d’uso. Ad esempio, questa divisione introduce overhead o complessità sui dispositivi edge o in altri ambienti non data center.</p>
<p>TensorFlow è stato creato per funzionare su più piattaforme, dai dispositivi mobili e edge all’infrastruttura cloud. Mirava anche a essere più leggero e intuitivo per gli sviluppatori e a fornire facilità d’uso tra il training locale e quello distribuito.</p>
</section>
<section id="progettazione-dellarchitettura" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="progettazione-dellarchitettura"><span class="header-section-number">6.3.4</span> Progettazione dell’Architettura</h3>
<p>Invece di utilizzare l’architettura del server dei parametri, TensorFlow distribuisce i task [attività] su un cluster. Queste attività sono processi denominati che possono comunicare su una rete e ciascuna può eseguire la struttura principale di TensorFlow, il grafico del flusso di dati e l’interfaccia con vari dispositivi di elaborazione (come CPU o GPU). Questo grafico [grafo] è una rappresentazione diretta in cui i nodi simboleggiano le operazioni di elaborazione e gli edge rappresentano i tensori (dati) che scorrono tra queste operazioni.</p>
<p>Nonostante l’assenza di server di parametri tradizionali, alcuni “task PS” memorizzano e gestiscono parametri che ricordano i server di parametri di altri sistemi. I task rimanenti, che di solito gestiscono calcoli, elaborazione dati e gradienti, sono denominati “task worker”. I task PS di TensorFlow possono eseguire qualsiasi calcolo rappresentabile dal grafico del flusso di dati, il che significa che non sono limitati solo all’archiviazione dei parametri e il calcolo può essere distribuito. Questa capacità li rende significativamente più versatili e offre agli utenti il potere di programmare i task PS utilizzando l’interfaccia TensorFlow standard, la stessa che userebbero per definire i loro modelli. Come accennato in precedenza, la struttura dei grafici del flusso di dati li rende anche intrinsecamente buoni per il parallelismo, consentendo l’elaborazione di grandi set di dati.</p>
</section>
<section id="funzionalità-native-keras" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="funzionalità-native-keras"><span class="header-section-number">6.3.5</span> Funzionalità Native &amp; Keras</h3>
<p>TensorFlow include librerie per aiutare gli utenti a sviluppare e distribuire più modelli specifici per i casi d’uso e, poiché questo framework è open source, questo elenco continua a crescere. Queste librerie affrontano l’intero ciclo di vita dello sviluppo ML: preparazione dei dati, creazione di modelli, distribuzione e IA responsabile.</p>
<p>Uno dei maggiori vantaggi di TensorFlow è la sua integrazione con Keras, anche se, come vedremo nella prossima sezione, Pytorch ha recentemente aggiunto un’integrazione Keras. Keras è un altro framework ML creato per essere estremamente intuitivo e, di conseguenza, ha un alto livello di astrazione. Parleremo di Keras più approfonditamente più avanti in questo capitolo. Tuttavia, quando si discute della sua integrazione con TensorFlow, è importante notare che era stato originariamente creato per essere indipendente dal backend. Ciò significa che gli utenti potrebbero astrarre queste complessità, offrendo un modo più pulito e intuitivo per definire e addestrare modelli senza preoccuparsi di problemi di compatibilità con diversi backend. Gli utenti di TensorFlow hanno evidenziato alcuni problemi sull’usabilità e la leggibilità dell’API di TensorFlow, quindi, man mano che TF acquisiva importanza, ha integrato Keras come API di alto livello. Questa integrazione ha offerto grandi vantaggi agli utenti di TensorFlow poiché ha introdotto una leggibilità e una portabilità più intuitive dei modelli, sfruttando comunque le potenti funzionalità di backend, il supporto di Google e l’infrastruttura per distribuire i modelli su varie piattaforme.</p>
<div id="exr-k" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;6.3: Esplorazione di Keras: Creazione, Addestramento e Valutazione di Reti Neurali
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Qui, impareremo come utilizzare Keras, un’API di reti neurali di alto livello, per lo sviluppo e l’addestramento (training) di modelli. Esploreremo l’API funzionale per la creazione di modelli concisi, comprenderemo le classi “loss” e metriche per la valutazione dei modelli e utilizzeremo gli ottimizzatori nativi per aggiornare i parametri del modello durante l’addestramento. Inoltre, scopriremo come definire layer e metriche personalizzati su misura per le nostre esigenze. Infine, esamineremo i cicli di addestramento di Keras per semplificare il processo di addestramento delle reti neurali su grandi set di dati. Questa conoscenza ci consentirà di costruire e ottimizzare modelli di reti neurali in varie applicazioni di machine learning e intelligenza artificiale.</p>
<p><a href="https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO#scrollTo=fxINLLGitX_n"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="limitazioni-e-sfide" class="level3" data-number="6.3.6">
<h3 data-number="6.3.6" class="anchored" data-anchor-id="limitazioni-e-sfide"><span class="header-section-number">6.3.6</span> Limitazioni e Sfide</h3>
<p>TensorFlow è uno dei framework di deep learning più popolari, ma ha dovuto affrontare critiche e debolezze, principalmente legate all’usabilità e all’utilizzo delle risorse. Sebbene vantaggioso, il ritmo rapido degli aggiornamenti tramite il supporto di Google ha talvolta portato a problemi di retrocompatibilità, funzioni deprecate e documentazione instabile. Inoltre, anche con l’implementazione di Keras, la sintassi e la curva di apprendimento di TensorFlow possono risultare difficili per i nuovi utenti. Un’altra critica importante di TensorFlow è il suo elevato overhead e consumo di memoria dovuto alla gamma di librerie integrate e al supporto. Sebbene le versioni ridotte possano risolvere alcuni di questi problemi, potrebbero comunque essere limitate in ambienti con risorse limitate.</p>
</section>
<section id="sec-pytorch_vs_tensorflow" class="level3" data-number="6.3.7">
<h3 data-number="6.3.7" class="anchored" data-anchor-id="sec-pytorch_vs_tensorflow"><span class="header-section-number">6.3.7</span> PyTorch &amp; TensorFlow</h3>
<p>PyTorch e TensorFlow si sono affermati come leader nel settore. Entrambi i framework offrono funzionalità robuste ma differiscono per filosofie di progettazione, facilità d’uso, ecosistema e capacità di distribuzione.</p>
<p><strong>Filosofia di Progettazione e Paradigma di Programmazione:</strong> PyTorch utilizza un grafo computazionale dinamico denominato eager execution [esecuzione rapida]. Ciò lo rende intuitivo e facilita il debug poiché le operazioni vengono eseguite immediatamente e possono essere ispezionate al volo. Al contrario, le versioni precedenti di TensorFlow erano incentrate su un grafo computazionale statico, che richiedeva la definizione completa del grafico prima dell’esecuzione. Tuttavia, TensorFlow 2.0 ha introdotto la “eager execution” per default, rendendolo più allineato con PyTorch. La natura dinamica di PyTorch e l’approccio basato su Python hanno consentito la sua semplicità e flessibilità, in particolare per la prototipazione rapida. L’approccio grafico statico di TensorFlow nelle sue versioni precedenti aveva una curva di apprendimento più ripida; l’introduzione di TensorFlow 2.0, con la sua integrazione Keras come API di alto livello, ha semplificato notevolmente il processo di sviluppo.</p>
<p><strong>Deployment:</strong> PyTorch è fortemente favorito negli ambienti di ricerca, ma la distribuzione dei modelli PyTorch in contesti di produzione è sempre stata un problema. Tuttavia, la distribuzione è diventata più fattibile con l’introduzione di TorchScript, lo strumento TorchServe e <a href="https://pytorch.org/mobile/home/">PyTorch Mobile</a>. TensorFlow si distingue per la sua forte scalabilità e capacità di distribuzione, in particolare su piattaforme embedded e mobili con TensorFlow Lite. TensorFlow Serving e TensorFlow.js facilitano ulteriormente la distribuzione in vari ambienti, conferendogli così una portata più ampia nell’ecosistema.</p>
<p><strong>Prestazioni:</strong> Entrambi i framework offrono un’accelerazione hardware efficiente per le loro operazioni. Tuttavia, TensorFlow ha un flusso di lavoro di ottimizzazione leggermente più robusto, come il compilatore XLA (Accelerated Linear Algebra), che può aumentare ulteriormente le prestazioni. Il suo grafo computazionale statico era anche vantaggioso per alcune ottimizzazioni nelle prime versioni.</p>
<p><strong>Ecosistema:</strong> PyTorch ha un ecosistema in crescita con strumenti come TorchServe per servire modelli e librerie come TorchVision, TorchText e TorchAudio per domini specifici. Come abbiamo detto prima, TensorFlow ha un ecosistema ampio e maturo. TensorFlow Extended (TFX) fornisce una piattaforma end-to-end per distribuire pipeline di apprendimento automatico di produzione. Altri strumenti e librerie includono TensorFlow Lite, TensorFlow Lite Micro, TensorFlow.js, TensorFlow Hub e TensorFlow Serving.</p>
<p><a href="#tbl-pytorch_vs_tf" class="quarto-xref">Tabella&nbsp;<span>6.1</span></a> fornisce un’analisi comparativa:</p>
<div id="tbl-pytorch_vs_tf" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pytorch_vs_tf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;6.1: Confronto tra PyTorch e TensorFlow.
</figcaption>
<div aria-describedby="tbl-pytorch_vs_tf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 31%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspetto</th>
<th style="text-align: left;">Pytorch</th>
<th style="text-align: left;">TensorFlow</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Filosofia di Progettazione</td>
<td style="text-align: left;">Grafo computazionale dinamico (eager execution)</td>
<td style="text-align: left;">Grafo computazionale statico (prime versioni); Esecuzione rapida in TensorFlow 2.0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Deployment</td>
<td style="text-align: left;">Tradizionalmente impegnativa; Migliorata con TorchScript e TorchServe</td>
<td style="text-align: left;">Scalabile, specialmente su piattaforme embedded con TensorFlow Lite</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Prestazioni e Ottimizzazione</td>
<td style="text-align: left;">Accelerazione GPU efficiente</td>
<td style="text-align: left;">Ottimizzazione robusta con compilatore XLA</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ecosistema</td>
<td style="text-align: left;">TorchServe, TorchVision, TorchText, TorchAudio, PyTorch Mobile</td>
<td style="text-align: left;">TensorFlow Extended (TFX), TensorFlow Lite, TensorFlow Lite Micro TensorFlow.js, TensorFlow Hub, TensorFlow Serving</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Facilità d’uso</td>
<td style="text-align: left;">Preferito per il suo approccio Pythonic e la prototipazione rapida</td>
<td style="text-align: left;">Curva di apprendimento inizialmente ripida; Semplificato con Keras in TensorFlow 2.0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="componenti-di-base-del-framework" class="level2 page-columns page-full" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="componenti-di-base-del-framework"><span class="header-section-number">6.4</span> Componenti di Base del Framework</h2>
<p>Dopo aver introdotto i popolari framework di machine learning e aver fornito un confronto di alto livello, questa sezione presenterà le funzionalità principali che formano la struttura di questi framework. Tratterà la struttura speciale chiamata tensori, che questi framework utilizzano per gestire più facilmente dati multidimensionali complessi. Si vedrà anche come questi framework rappresentano diversi tipi di architetture di reti neurali e le loro operazioni richieste tramite grafi computazionali. Inoltre, si vedrà come offrono strumenti che rendono lo sviluppo di modelli di machine learning più astratto ed efficiente, come caricatori di dati, algoritmi di ottimizzazione delle perdite implementate, tecniche di differenziazione efficienti e la capacità di accelerare il processo di training su acceleratori hardware.</p>
<section id="sec-tensor-data-structures" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="sec-tensor-data-structures"><span class="header-section-number">6.4.1</span> Strutture Dati Tensoriali</h3>
<p>Per comprendere i tensori, partiamo dai concetti familiari dell’algebra lineare. Come mostrato in <a href="#fig-tensor-data-structure" class="quarto-xref">Figura&nbsp;<span>6.5</span></a>, i vettori possono essere rappresentati come una pila di numeri in un array unidimensionale. Le matrici seguono la stessa idea e si possono pensare a loro come a molti vettori impilati l’uno sull’altro, rendendoli bidimensionali. I tensori di dimensioni superiori funzionano allo stesso modo. Un tensore tridimensionale è semplicemente un insieme di matrici impilate l’una sull’altra in una direzione aggiuntiva. Pertanto, vettori e matrici possono essere considerati casi speciali di tensori con dimensioni 1D e 2D, rispettivamente.</p>
<div id="fig-tensor-data-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.4: Visualizzazione della Struttura Dati del Tensore.
</figcaption>
</figure>
</div>
<p>I tensori offrono una struttura flessibile che può rappresentare dati in dimensioni superiori. Ad esempio, per rappresentare i dati di un’immagine, i pixel in ogni posizione di un’immagine sono strutturati come matrici. Tuttavia, le immagini non sono rappresentate da una sola matrice di valori di pixel; in genere hanno tre canali in cui ogni canale è una matrice contenente valori di pixel che rappresentano l’intensità di rosso, verde o blu. Insieme, questi canali creano un’immagine colorata. Senza i tensori, archiviare tutte queste informazioni da più matrici può risultare complesso. Con i tensori, è facile contenere i dati dell’immagine in un singolo tensore tridimensionale, con ogni numero che rappresenta un certo valore di colore in una posizione specifica nell’immagine.</p>
<div id="fig-tensor-data-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/color_channels_of_image.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.5: Visualizzazione della struttura dell’immagine colorata che può essere facilmente memorizzata come un Tensore 3D. Credito: <a href="https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff">Niklas Lang</a>
</figcaption>
</figure>
</div>
<p>Non finisce qui. Se volessimo archiviare una serie di immagini, potremmo usare un tensore quadridimensionale, in cui la nuova dimensione rappresenta immagini diverse. Ciò significa che si stanno archiviando più immagini, ciascuna con tre matrici che rappresentano i tre canali del colore. Questo dà un’idea dell’utilità dei tensori quando si gestiscono dati multidimensionali in modo efficiente.</p>
<p>I tensori hanno anche un attributo unico che consente ai framework di calcolare automaticamente i gradienti, semplificando l’implementazione di modelli complessi e algoritmi di ottimizzazione. Nel machine learning, come discusso nel <a href="../dl_primer/dl_primer.qmd#sec-backward_pass">Capitolo 3</a>, la backpropagation richiede di prendere la derivata delle equazioni. Una delle caratteristiche principali dei tensori in PyTorch e TensorFlow è la loro capacità di tracciare i calcoli e calcolare i gradienti. Ciò è fondamentale per la backpropagation nelle reti neurali. Ad esempio, in PyTorch, si può usare l’attributo <code>requires_grad</code>, che consente di calcolare e memorizzare automaticamente i gradienti durante il “backward pass”, facilitando il processo di ottimizzazione. Analogamente, in TensorFlow, <code>tf.GradientTape</code> registra le operazioni per la differenziazione automatica.</p>
<p>Si consideri questa semplice equazione matematica che si vuole differenziare. Matematicamente, il calcolo del gradiente si effettua nel modo seguente:</p>
<p>Dato: <span class="math display">\[
y = x^2
\]</span></p>
<p>La derivata di <span class="math inline">\(y\)</span> rispetto a <span class="math inline">\(x\)</span> è: <span class="math display">\[
\frac{dy}{dx} = 2x
\]</span></p>
<p>Quando <span class="math inline">\(x = 2\)</span>: <span class="math display">\[
\frac{dy}{dx} = 2*2 = 4
\]</span></p>
<p>Il gradiente di <span class="math inline">\(y\)</span> rispetto a <span class="math inline">\(x\)</span>, con <span class="math inline">\(x = 2\)</span>, è 4.</p>
<p>Una potente caratteristica dei tensori in PyTorch e TensorFlow è la loro capacità di calcolare facilmente le derivate (gradienti). Ecco gli esempi di codice corrispondenti in PyTorch e TensorFlow:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" aria-current="page">PyTorch</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">TensorFlow</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor with gradient tracking</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple function</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the gradient</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>tensor(<span class="fl">4.0</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor with gradient tracking</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.Variable(<span class="fl">2.0</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple function</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> tape.gradient(y, x)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the gradient</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(grad)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>tf.Tensor(<span class="fl">4.0</span>, shape<span class="op">=</span>(), dtype<span class="op">=</span>float32)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>Questa differenziazione automatica è una potente funzionalità dei tensori in framework come PyTorch e TensorFlow, che semplifica l’implementazione e l’ottimizzazione di modelli complessi di apprendimento automatico.</p>
</section>
<section id="grafi-computazionali" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="grafi-computazionali"><span class="header-section-number">6.4.2</span> Grafi computazionali</h3>
<section id="definizione-di-grafico" class="level4">
<h4 class="anchored" data-anchor-id="definizione-di-grafico">Definizione di Grafico</h4>
<p>I grafi computazionali sono una componente chiave di framework di deep learning come TensorFlow e PyTorch. Ci consentono di esprimere architetture di reti neurali complesse in modo efficiente e differenziato. Un grafo computazionale è costituito da un grafo aciclico diretto (directed acyclic graph, DAG) in cui ogni nodo rappresenta un’operazione o una variabile e gli spigoli rappresentano le dipendenze dei dati tra di essi.</p>
<p>È importante differenziare i grafi computazionali dai diagrammi di reti neurali, come quelli per i perceptron multistrato (multilayer perceptrons, MLP), che rappresentano nodi e layer. I diagrammi di reti neurali, come illustrato nel <a href="../dl_primer/dl_primer.qmd">Capitolo 3</a>, visualizzano l’architettura e il flusso di dati attraverso nodi e layer, fornendo una comprensione intuitiva della struttura del modello. Al contrario, i grafi computazionali forniscono una rappresentazione di basso livello delle operazioni matematiche sottostanti e delle dipendenze dei dati necessarie per implementare e addestrare queste reti.</p>
<p>Ad esempio, un nodo potrebbe rappresentare un’operazione di moltiplicazione di matrici, prendendo due matrici di input (o tensori) e producendo una matrice di output (o tensore). Per visualizzarlo, si consideri il semplice esempio in <a href="../training/training.it.html#fig-computational-graph" class="quarto-xref">Figura&nbsp;<span>7.3</span></a>. Il grafo aciclico orientato sopra calcola <span class="math inline">\(z = x \times y\)</span>, dove ogni variabile è composta solo da numeri.</p>
<div id="fig-computational-graph" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-computational-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image1.png" style="width:50.0%" data-align="center" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-computational-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.6: Esempio elementare di un grafo computazionale.
</figcaption>
</figure>
</div>
<p>Framework come TensorFlow e PyTorch creano grafi computazionali per implementare le architetture delle reti neurali che in genere rappresentiamo con diagrammi. Quando si definisce un layer di rete neurale nel codice (ad esempio, un “layer denso” in TensorFlow), il framework costruisce un grafo computazionale che include tutte le operazioni necessarie (come moltiplicazione di matrici, addizione e funzioni di attivazione) e le relative dipendenze dai dati. Questo grafo consente al framework di gestire in modo efficiente il flusso di dati, ottimizzare l’esecuzione delle operazioni e calcolare automaticamente i gradienti per l’addestramento. Internamente, i grafi computazionali rappresentano astrazioni per layer comuni come quelli convoluzionali, di pooling, ricorrenti e densi, con dati che includono attivazioni, pesi e bias rappresentati in tensori. Questa rappresentazione consente un calcolo efficiente, sfruttando la struttura del grafico per parallelizzare le operazioni e applicare ottimizzazioni.</p>
<p>Alcuni livelli comuni che i grafi computazionali potrebbero implementare includono layer convoluzionali, di attenzione, ricorrenti e densi. I layer fungono da astrazioni di livello superiore che definiscono calcoli specifici in cima alle operazioni di base rappresentate nel grafo. Ad esempio, un layer Denso esegue la moltiplicazione e l’addizione di matrici tra tensori di input, peso e bias. È importante notare che un layer opera su tensori come input e output; il layer stesso non è un tensore. Alcune differenze chiave tra layer e tensori sono:</p>
<ul>
<li><p>I layer contengono stati come pesi e bias. I tensori sono senza stato, contengono solo dati.</p></li>
<li><p>I layer possono modificare lo stato interno durante l’addestramento. I tensori sono immutabili/di sola lettura.</p></li>
<li><p>I layer sono astrazioni di livello superiore. I tensori sono a un livello inferiore e rappresentano direttamente dati e operazioni matematiche.</p></li>
<li><p>I layer definiscono pattern di calcolo fissi. I tensori scorrono tra i livelli durante l’esecuzione.</p></li>
<li><p>I layer vengono utilizzati indirettamente durante la creazione di modelli. I tensori scorrono tra i livelli durante l’esecuzione.</p></li>
</ul>
<p>Quindi, mentre i tensori sono una struttura dati fondamentale che i layer consumano e producono, i layer hanno funzionalità aggiuntive per definire operazioni parametrizzate e addestramento. Mentre un layer configura le operazioni tensoriali in background, il layer rimane distinto dagli oggetti tensoriali. L’astrazione del layer rende la creazione e l’addestramento di reti neurali molto più intuitive. Questa astrazione consente agli sviluppatori di creare modelli impilando insieme questi layer senza implementare la logica del layer. Ad esempio, la chiamata di <code>tf.keras.layers.Conv2D</code> in TensorFlow crea un layer convoluzionale. Il framework gestisce il calcolo delle convoluzioni, la gestione dei parametri, ecc. Ciò semplifica lo sviluppo del modello, consentendo agli sviluppatori di concentrarsi sull’architettura anziché sulle implementazioni di basso livello. Le astrazioni dei layer utilizzano implementazioni altamente ottimizzate per le prestazioni. Consentono inoltre la portabilità, poiché la stessa architettura può essere eseguita su backend hardware diversi come GPU e TPU.</p>
<p>Inoltre, i grafi computazionali includono funzioni di attivazione come ReLU, sigmoide e tanh che sono essenziali per le reti neurali e molti framework le forniscono come astrazioni standard. Queste funzioni introducono non linearità che consentono ai modelli di approssimare funzioni complesse. I framework le forniscono come operazioni semplici e predefinite che possono essere utilizzate durante la costruzione di modelli, ad esempio if.nn.relu in TensorFlow. Questa astrazione consente flessibilità, poiché gli sviluppatori possono facilmente scambiare le funzioni di attivazione per ottimizzare le prestazioni. Le attivazioni predefinite sono inoltre ottimizzate dal framework per un’esecuzione più rapida.</p>
<p>Negli ultimi anni, modelli come ResNets e MobileNets sono emersi come architetture popolari, con i framework attuali che li pre-confezionano come grafi computazionali. Invece di preoccuparsi dei dettagli, gli sviluppatori possono utilizzarli come punto di partenza, personalizzandoli secondo necessità sostituendo i layer. Ciò semplifica e velocizza lo sviluppo del modello, evitando di reinventare le architetture da zero. I modelli predefiniti includono implementazioni ben collaudate e ottimizzate che garantiscono buone prestazioni. Il loro design modulare consente inoltre di trasferire le funzionalità apprese a nuove attività tramite apprendimento tramite trasferimento. Queste architetture predefinite forniscono i mattoni ad alte prestazioni per creare rapidamente modelli robusti.</p>
<p>Queste astrazioni di layer, funzioni di attivazione e architetture predefinite fornite dai framework costituiscono un grafo computazionale. Quando un utente definisce un layer in un framework (ad esempio, <code>tf.keras.layers.Dense()</code>), il framework configura nodi e bordi del grafo computazionale per rappresentare tale layer. I parametri del layer come pesi e bias diventano variabili nel grafo. I calcoli del layer diventano nodi operativi (come x e y nella figura sopra). Quando si chiama una funzione di attivazione come <code>tf.nn.relu()</code>, il framework aggiunge un nodo operativo ReLU al grafo. Le architetture predefinite sono solo sottografi preconfigurati che possono essere inseriti nel grafo del modello. Quindi, la definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i livelli, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafo.</p>
<p>Costruiamo implicitamente un grafo computazionale quando definiamo un’architettura di rete neurale in un framework. Il framework utilizza questo grafo per determinare le operazioni da eseguire durante l’addestramento e l’inferenza. I grafi computazionali offrono diversi vantaggi rispetto al codice grezzo e questa è una delle funzionalità principali offerte da un buon framework di ML:</p>
<ul>
<li><p>Rappresentazione esplicita del flusso di dati e delle operazioni</p></li>
<li><p>Capacità di ottimizzare il grafo prima dell’esecuzione</p></li>
<li><p>Differenziazione automatica per il training</p></li>
<li><p>Agnosticismo linguistico: il grafo può essere tradotto per essere eseguito su GPU, TPU, ecc.</p></li>
<li><p>Portabilità: il grafo può essere serializzato, salvato e ripristinato in seguito</p></li>
</ul>
<p>I grafi computazionali sono i componenti fondamentali dei framework di ML. La definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i layer, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafico. I compilatori e gli ottimizzatori del framework operano su questo grafo per generare codice eseguibile. Le astrazioni forniscono un’API intuitiva per gli sviluppatori per la creazione di grafi computazionali. Sotto, ci sono ancora grafi! Quindi, anche se non si possono manipolare direttamente i grafi come utente del framework, consentono di eseguire ad alto livello e in modo efficiente le specifiche del modello. Le astrazioni semplificano la creazione del modello, mentre i grafi computazionali la rendono possibile.</p>
</section>
<section id="grafi-statici-vs.-dinamici" class="level4">
<h4 class="anchored" data-anchor-id="grafi-statici-vs.-dinamici">Grafi Statici vs.&nbsp;Dinamici</h4>
<p>I framework di deep learning hanno tradizionalmente seguito uno dei due approcci per esprimere grafi computazionali.</p>
<p><strong>Grafi statici (declare-then-execute):</strong> Con questo modello, l’intero grafo computazionale deve essere definito in anticipo prima di eseguirlo. Tutte le operazioni e le dipendenze dei dati devono essere specificate durante la fase di dichiarazione. TensorFlow originariamente seguiva questo approccio statico: i modelli venivano definiti in un contesto separato e poi veniva creata una sessione per eseguirli. Il vantaggio dei grafi statici è che consentono un’ottimizzazione più aggressiva poiché il framework può vedere il grafo completo. Tuttavia, tende anche a essere meno flessibile per la ricerca e l’interattività. Le modifiche al grafo richiedono la nuova dichiarazione del modello completo.</p>
<p>Per esempio:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.placeholder(tf.float32)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.matmul(x, weights) <span class="op">+</span> biases</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In questo esempio, x è un segnaposto per i dati di input e y è il risultato di un’operazione di moltiplicazione di matrici seguita da un’addizione. Il modello è definito in questa fase di dichiarazione, in cui tutte le operazioni e le variabili devono essere specificate in anticipo.</p>
<p>Una volta definito l’intero grafo, il framework lo compila e lo ottimizza. Ciò significa che i passaggi computazionali sono definitivamente “scolpiti” e il framework può applicare varie ottimizzazioni per migliorare l’efficienza e le prestazioni. Quando in seguito si esegue il grafo, si forniscono i tensori di input effettivi e le operazioni predefinite vengono eseguite nella sequenza ottimizzata.</p>
<p>Questo approccio è simile alla creazione di un progetto in cui ogni dettaglio è pianificato prima dell’inizio della costruzione. Sebbene ciò consenta potenti ottimizzazioni, significa anche che qualsiasi modifica al modello richiede la ridefinizione dell’intero grafo da zero.</p>
<p><strong>Grafi dinamici (define-by-run):</strong> A differenza della dichiarazione (di tutto) prima e dell’esecuzione poi, il grafo viene creato dinamicamente durante l’esecuzione. Non esiste una fase di dichiarazione separata: le operazioni vengono eseguite immediatamente come definite. Questo stile è imperativo e flessibile, facilitando la sperimentazione.</p>
<p>PyTorch utilizza grafi dinamici, creandoli al volo mentre avviene l’esecuzione. Ad esempio, si consideri il seguente frammento di codice, in cui il grafo viene creato durante l’esecuzione:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">4</span>,<span class="dv">784</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.matmul(x, weights) <span class="op">+</span> biases</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>L’esempio sopra non ha fasi separate di compilazione/build/esecuzione. Le operazioni definiscono ed eseguono immediatamente. Con i grafi dinamici, la definizione è intrecciata con l’esecuzione, fornendo un flusso di lavoro più intuitivo e interattivo. Tuttavia, lo svantaggio è che c’è meno potenziale di ottimizzazione poiché il framework vede solo il grafo mentre viene creato. <a href="#fig-static-vs-dynamic" class="quarto-xref">Figura&nbsp;<span>6.7</span></a> mostra le differenze tra un grafo di calcolo statico e uno dinamico.</p>
<div id="fig-static-vs-dynamic" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-static-vs-dynamic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/staticvsdynamic.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-static-vs-dynamic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.7: Confronto tra grafi statici e dinamici. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fdev-jm.tistory.com%2F4&amp;psig=AOvVaw0r1cZbZa6iImYP-fesrN7H&amp;ust=1722533107591000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBQQjhxqFwoTCLC8nYHm0YcDFQAAAAAdAAAAABAY">Dev</a>
</figcaption>
</figure>
</div>
<p>Di recente, la distinzione si è offuscata poiché i framework adottano entrambe le modalità. TensorFlow 2.0 passa automaticamente alla modalità di grafo dinamico, consentendo agli utenti di lavorare con quelli statici quando necessario. La dichiarazione dinamica offre flessibilità e facilità d’uso, rendendo i framework più intuitivi, mentre i grafi statici forniscono vantaggi di ottimizzazione a scapito dell’interattività. Il framework ideale bilancia questi approcci. <a href="#tbl-exec-graph" class="quarto-xref">Tabella&nbsp;<span>6.2</span></a> confronta i pro e i contro dei grafi di esecuzione statici e dinamici:</p>
<div id="tbl-exec-graph" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-exec-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;6.2: Confronto tra Grafi di Esecuzione Statici (Declare-then-execute) e Dinamici (Define-by-run), evidenziandone i rispettivi pro e contro.
</figcaption>
<div aria-describedby="tbl-exec-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 37%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Grafo di esecuzione</th>
<th style="text-align: left;">Pro</th>
<th style="text-align: left;">Contro</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Statico (Declare-then-execute)</td>
<td style="text-align: left;"><ul>
<li>Abilita le ottimizzazioni del grafo visualizzando il modello completo in anticipo</li>
<li>Può esportare e distribuire grafici congelati</li>
<li>Il grafo è impacchettato indipendentemente dal codice</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Meno flessibile per la ricerca e l’iterazione</li>
<li>Le modifiche richiedono la ricostruzione del grafo</li>
<li>L’esecuzione ha fasi di compilazione ed esecuzione separate</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;">Dinamico (Define-by-run)</td>
<td style="text-align: left;"><ul>
<li>Stile imperativo intuitivo come il codice Python</li>
<li>Alterna la creazione del grafo con l’esecuzione</li>
<li>Facile da modificare i grafi</li>
<li>Il debug si adatta perfettamente al flusso di lavoro</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Più difficile da ottimizzare senza un grafo completo</li>
<li>Possibili rallentamenti dalla creazione del grafo durante l’esecuzione</li>
<li>Può richiedere più memoria</li>
</ul></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="tool-della-pipeline-dei-dati" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="tool-della-pipeline-dei-dati"><span class="header-section-number">6.4.3</span> Tool della Pipeline dei Dati</h3>
<p>I grafi computazionali possono essere validi solo quanto i dati da cui apprendono e su cui lavorano. Pertanto, alimentare i dati di training in modo efficiente è fondamentale per ottimizzare le prestazioni della “deep neural network” [rete neurale profonda], sebbene spesso venga trascurata come una delle funzionalità principali. Molti framework di IA moderni forniscono pipeline specializzate per acquisire, elaborare e aumentare i set di dati per il training del modello.</p>
<section id="loader-dei-dati" class="level4">
<h4 class="anchored" data-anchor-id="loader-dei-dati">Loader dei Dati</h4>
<p>Al centro di queste pipeline ci sono i “data loader”, che gestiscono esempi di training di lettura da fonti come file, database e storage di oggetti. I data loader facilitano il caricamento e la pre-elaborazione efficienti dei dati, cruciali per i modelli di deep learning. Ad esempio, la pipeline di caricamento dati <a href="https://www.tensorflow.org/guide/data">tf.data</a> di TensorFlow è progettata per gestire questo processo. A seconda dell’applicazione, i modelli di deep learning richiedono diversi formati di dati come file CSV o cartelle di immagini. Alcuni formati popolari includono:</p>
<ul>
<li><p>CSV, un formato versatile e semplice spesso utilizzato per dati tabellari.</p></li>
<li><p>TFRecord: Formato proprietario di TensorFlow, ottimizzato per le prestazioni.</p></li>
<li><p>Parquet: Storage a colonne, che offre compressione e recupero dati efficienti.</p></li>
<li><p>JPEG/PNG: Comunemente utilizzato per dati di immagini.</p></li>
<li><p>WAV/MP3: Formati prevalenti per dati audio.</p></li>
</ul>
<p>Esempi di batch di data loader per sfruttare il supporto di vettorizzazione nell’hardware. Il “batching” si riferisce al raggruppamento di più dati per l’elaborazione simultanea, sfruttando le capacità di calcolo vettorizzate di hardware come le GPU. Sebbene le dimensioni tipiche dei batch siano comprese tra 32 e 512 esempi, la dimensione ottimale spesso dipende dall’ingombro di memoria dei dati e dai vincoli hardware specifici. I loader avanzati possono trasmettere in streaming set di dati virtualmente illimitati da dischi e archivi cloud. Trasmettono in streaming grandi dataset da dischi o reti anziché caricarli completamente in memoria, consentendo dimensioni illimitate.</p>
<p>I data loader possono anche mescolare i dati tra “epoche” per la randomizzazione e le funzionalità di preelaborazione in parallelo con l’addestramento del modello per accelerarne il processo. Mescolare casualmente l’ordine degli esempi tra epoche di training riduce il bias e migliora la generalizzazione.</p>
<p>I data loader supportano anche strategie di “caching” e “prefetching” per ottimizzare la distribuzione dei dati per un addestramento del modello rapido e fluido. Il caching [memorizzazione nella cache] dei batch preelaborati consente di riutilizzarli in modo efficiente durante più fasi di addestramento ed elimina l’elaborazione ridondante. Il prefetching, al contrario, comporta il precaricamento dei batch successivi, assicurando che il modello non resti mai inattivo in attesa di dati.</p>
</section>
</section>
<section id="data-augmentation" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="data-augmentation"><span class="header-section-number">6.4.4</span> Data Augmentation</h3>
<p>Framework di apprendimento automatico come TensorFlow e PyTorch forniscono strumenti per semplificare e snellire il processo di “data augmentation” [aumento dei dati], migliorando l’efficienza dell’espansione sintetica dei set di dati. Questi framework offrono funzionalità integrate per applicare trasformazioni casuali, come capovolgimento, ritaglio, rotazione, modifica del colore e aggiunta di rumore per le immagini. Per i dati audio, gli aumenti comuni comportano la miscelazione di clip con rumore di fondo o la modulazione di velocità, tono e volume.</p>
<p>Integrando gli strumenti di “augmentation” nella pipeline di dati, i framework consentono di applicare queste trasformazioni al volo durante ogni epoca di addestramento. Questo approccio incrementa la variazione nella distribuzione dei dati di addestramento, riducendo così l’overfitting e migliorando la generalizzazione del modello. <a href="#fig-overfitting" class="quarto-xref">Figura&nbsp;<span>6.8</span></a> mostra i casi di overfitting e underfitting. L’uso di “data loader” performanti in combinazione con ampie capacità di “augmentation” consente ai professionisti di alimentare in modo efficiente set di dati massicci e vari alle reti neurali.</p>
<div id="fig-overfitting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-overfitting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/overfittingunderfitting.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-overfitting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.8: Overfitting e underfitting. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.aquariumlearning.com%2Fblog-posts%2Fto-make-your-model-better-first-figure-out-whats-wrong&amp;psig=AOvVaw3FodMJATpeLeeSsuQZBD51&amp;ust=1722534629114000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqGAoTCNiU49br0YcDFQAAAAAdAAAAABCoAQ">Aquarium Learning</a>
</figcaption>
</figure>
</div>
<p>Queste pipeline di dati “hands-off” rappresentano un miglioramento significativo in termini di usabilità e produttività. Consentono agli sviluppatori di concentrarsi maggiormente sull’architettura del modello e meno sulla manipolazione dei dati durante l’addestramento di modelli di deep learning.</p>
</section>
<section id="funzioni-loss-e-algoritmi-di-ottimizzazione" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="funzioni-loss-e-algoritmi-di-ottimizzazione"><span class="header-section-number">6.4.5</span> Funzioni Loss e Algoritmi di Ottimizzazione</h3>
<p>L’addestramento di una rete neurale è fondamentalmente un processo iterativo che cerca di minimizzare una funzione di loss [perdita]. L’obiettivo è di mettere a punto i pesi e i parametri del modello per produrre previsioni vicine alle vere etichette target. I framework di apprendimento automatico hanno notevolmente semplificato questo processo offrendo funzioni di loss [perdita] e algoritmi di ottimizzazione.</p>
<p>I framework di apprendimento automatico forniscono funzioni di perdita implementate che sono necessarie per quantificare la differenza tra le previsioni del modello e i valori reali. Diversi set di dati richiedono una diversa funzione di perdita per funzionare correttamente, poiché tale funzione indica al computer l’“obiettivo” a cui mirare. Le funzioni di perdita comunemente utilizzate includono il “Mean Squared Error (MSE)” [errore quadratico medio] per le attività di regressione, la “Cross-Entropy Loss” per le attività di classificazione, e la Kullback-Leibler (KL) per i modelli probabilistici. Ad esempio, <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses">tf.keras.losses</a> di TensorFlow contiene una serie di queste funzioni di perdita comunemente utilizzate.</p>
<p>Gli algoritmi di ottimizzazione vengono utilizzati per trovare in modo efficiente il set di parametri del modello che minimizzano la funzione di perdita, assicurando che il modello funzioni bene sui dati di training e si generalizzi a nuovi dati. I framework moderni sono dotati di implementazioni efficienti di diversi algoritmi di ottimizzazione, molti dei quali sono varianti della “discesa del gradiente” con metodi stocastici e tassi di apprendimento adattivo. Alcuni esempi di queste varianti sono Stochastic Gradient Descent, Adagrad, Adadelta e Adam. L’implementazione di tali varianti è fornita in <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">tf.keras.optimizers</a>. Ulteriori informazioni con esempi chiari sono disponibili nella sezione Training dell’IA.</p>
</section>
<section id="supporto-al-training-del-modello" class="level3" data-number="6.4.6">
<h3 data-number="6.4.6" class="anchored" data-anchor-id="supporto-al-training-del-modello"><span class="header-section-number">6.4.6</span> Supporto al Training del Modello</h3>
<p>È richiesta una fase di compilazione prima di addestrare un modello di rete neurale definito. Durante questa fase, l’architettura di alto livello della rete neurale viene trasformata in un formato eseguibile ottimizzato. Questo processo comprende diverse fasi. La prima fase consiste nel costruire il grafo computazionale, che rappresenta tutte le operazioni matematiche e il flusso di dati all’interno del modello. Ne abbiamo discusso in precedenza.</p>
<p>Durante l’addestramento, l’attenzione è rivolta all’esecuzione del grafo computazionale. A ogni parametro all’interno del grafo, come pesi e bias, viene assegnato un valore iniziale. A seconda del metodo di inizializzazione scelto, questo valore potrebbe essere casuale o basato su una logica predefinita.</p>
<p>Il passaggio critico successivo è l’allocazione della memoria. La memoria essenziale è riservata alle operazioni del modello sia su CPU che su GPU, garantendo un’elaborazione efficiente dei dati. Le operazioni del modello vengono poi mappate sulle risorse hardware disponibili, in particolare GPU o TPU, per accelerare l’elaborazione. Una volta completata la compilazione, il modello viene preparato per l’addestramento.</p>
<p>Il processo di addestramento impiega vari strumenti per migliorare l’efficienza. L’elaborazione batch è comunemente utilizzata per massimizzare la produttività computazionale. Tecniche come la vettorizzazione consentono operazioni su interi array di dati anziché procedere elemento per elemento, il che aumenta la velocità. Ottimizzazioni come la “kernel fusion” (fare riferimento al capitolo Ottimizzazioni) amalgamano più operazioni in un’unica azione, riducendo al minimo il sovraccarico computazionale. Le operazioni possono anche essere segmentate in fasi, facilitando l’elaborazione simultanea di diversi mini-batch in varie parti.</p>
<p>I framework eseguono costantemente il checkpoint dello stato, preservando le versioni intermedie del modello durante l’addestramento. Ciò garantisce che i progressi vengano recuperati in caso di interruzione e che l’addestramento possa essere ripreso dall’ultimo checkpoint. Inoltre, il sistema monitora attentamente le prestazioni del modello rispetto a un set di dati di convalida. Se il modello inizia a sovradimensionarsi (se le sue prestazioni sul set di convalida diminuiscono), l’addestramento viene automaticamente interrotto, conservando risorse computazionali e tempo.</p>
<p>I framework di ML incorporano una combinazione di compilazione del modello, metodi di elaborazione batch avanzati e utilità come il checkpoint e l’arresto anticipato. Queste risorse gestiscono gli aspetti complessi delle prestazioni, consentendo ai professionisti di concentrarsi sullo sviluppo e l’addestramento del modello. Di conseguenza, gli sviluppatori sperimentano sia velocità che facilità quando utilizzano le capacità delle reti neurali.</p>
</section>
<section id="validazione-e-analisi" class="level3" data-number="6.4.7">
<h3 data-number="6.4.7" class="anchored" data-anchor-id="validazione-e-analisi"><span class="header-section-number">6.4.7</span> Validazione e Analisi</h3>
<p>Dopo aver addestrato i modelli di deep learning, i framework forniscono utilità per valutare le prestazioni e ottenere informazioni sul funzionamento dei modelli. Questi strumenti consentono una sperimentazione e un debug disciplinati.</p>
<section id="metriche-di-valutazione" class="level4">
<h4 class="anchored" data-anchor-id="metriche-di-valutazione">Metriche di Valutazione</h4>
<p>I framework includono implementazioni di comuni metriche di valutazione per la convalida:</p>
<ul>
<li><p>Accuratezza: Frazione di previsioni corrette complessive. Sono ampiamente utilizzate per la classificazione.</p></li>
<li><p>Precisione: Delle previsioni positive, quante erano positive. Utile per set di dati sbilanciati.</p></li>
<li><p>Richiamo: Dei positivi effettivi, quanti ne abbiamo previsti correttamente? Misura della Completezza.</p></li>
<li><p>Punteggio F1: Media armonica di precisione e richiamo. Combina entrambe le metriche.</p></li>
<li><p>AUC-ROC - Area sotto la curva ROC. Sono utilizzate per l’analisi della soglia di classificazione.</p></li>
<li><p>MAP - Mean Average Precision. Valuta le previsioni classificate nel recupero/rilevamento.</p></li>
<li><p>Matrice di Confusione: Matrice che mostra i veri positivi, i veri negativi, i falsi positivi e i falsi negativi. Fornisce una visione più dettagliata delle prestazioni di classificazione.</p></li>
</ul>
<p>Queste metriche quantificano le prestazioni del modello sui dati di convalida per il confronto.</p>
</section>
<section id="visualizzazione" class="level4">
<h4 class="anchored" data-anchor-id="visualizzazione">Visualizzazione</h4>
<p>Gli strumenti di visualizzazione forniscono informazioni sui modelli:</p>
<ul>
<li><p>Curve di perdita: Tracciano la perdita di training e validazione nel tempo per individuare l’overfitting.</p></li>
<li><p>Loss curves [Griglie di attivazione]: Illustrano le funzionalità apprese dai filtri convoluzionali.</p></li>
<li><p>Projection [Proiezione]: Riduce la dimensionalità per una visualizzazione intuitiva.</p></li>
<li><p>Precision-recall curves [Curve di richiamo della precisione]: Valutano i compromessi di classificazione. <a href="#fig-precision-recall" class="quarto-xref">Figura&nbsp;<span>6.9</span></a> mostra un esempio di una curva di precisione-richiamo.</p></li>
</ul>
<div id="fig-precision-recall" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-precision-recall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/precisionrecall.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-precision-recall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.9: Lettura di una curva “precision-recall”. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.techtarget.com%2Fsearchcio%2Fdefinition%2Ftransfer-learning&amp;psig=AOvVaw0Cbiewbu_6NsNVf314C9Q8&amp;ust=1722534991962000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCPj5jITt0YcDFQAAAAAdAAAAABAE">AIM</a>
</figcaption>
</figure>
</div>
<p>Strumenti come <a href="https://www.tensorflow.org/tensorboard/scalars_and_keras">TensorBoard</a> per TensorFlow e <a href="https://github.com/microsoft/tensorwatch">TensorWatch</a> per PyTorch consentono metriche e visualizzazioni in tempo reale durante il training.</p>
</section>
</section>
<section id="programmazione-differenziabile" class="level3" data-number="6.4.8">
<h3 data-number="6.4.8" class="anchored" data-anchor-id="programmazione-differenziabile"><span class="header-section-number">6.4.8</span> Programmazione differenziabile</h3>
<p>I metodi di addestramento per il machine learning come la backpropagation si basano sulla modifica della funzione di perdita rispetto alla modifica dei pesi (che essenzialmente è la definizione di derivata). Pertanto, la capacità di addestrare rapidamente ed efficientemente grandi modelli di machine learning si basa sulla capacità del computer di prendere derivate. Ciò rende la programmazione differenziabile uno degli elementi più importanti di un framework di apprendimento automatico.</p>
<p>Possiamo utilizzare quattro metodi principali per far sì che i computer prendano derivate. Innanzitutto, possiamo calcolare manualmente le derivate a mano e inserirle nel computer. Questo diventerebbe rapidamente un incubo con molti layer di reti neurali se dovessimo calcolare manualmente tutte le derivate nei passaggi di backpropagation. Un altro metodo è la differenziazione simbolica utilizzando sistemi di computer algebrici come Mathematica, che può introdurre un layer di inefficienza, poiché è necessario un livello di astrazione per prendere le derivate. Le derivate numeriche, la pratica di approssimare i gradienti utilizzando metodi di differenze finite, soffrono di molti problemi, tra cui elevati costi computazionali e dimensioni della griglia più grandi, che portano a molti errori. Ciò porta alla differenziazione automatica, che sfrutta le funzioni primitive che i computer utilizzano per rappresentare le operazioni per ottenere una derivata esatta. Con la differenziazione automatica, la complessità computazionale del calcolo del gradiente è proporzionale al calcolo della funzione stessa. Le complessità della differenziazione automatica non sono gestite dagli utenti finali al momento, ma le risorse per saperne di più possono essere trovate ampiamente, ad esempio <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf">qui</a>. La differenziazione automatica e la programmazione differenziabile di oggi sono onnipresenti e vengono eseguite in modo efficiente e automatico dai moderni framework di machine learning.</p>
</section>
<section id="accelerazione-hardware" class="level3 page-columns page-full" data-number="6.4.9">
<h3 data-number="6.4.9" class="anchored" data-anchor-id="accelerazione-hardware"><span class="header-section-number">6.4.9</span> Accelerazione Hardware</h3>
<p>La tendenza a formare e distribuire continuamente modelli di apprendimento automatico più grandi ha reso necessario il supporto dell’accelerazione hardware per le piattaforme di machine-learning. <a href="#fig-hardware-accelerator" class="quarto-xref">Figura&nbsp;<span>6.10</span></a> mostra il gran numero di aziende che offrono acceleratori hardware in diversi domini, come il machine learning “Very Low Power” e quello “Embedded”. I “deep layer” delle reti neurali richiedono molte moltiplicazioni di matrici, che attraggono hardware in grado di calcolare rapidamente e in parallelo tali operazioni. In questo panorama, due architetture hardware, <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">GPU e TPU</a>, sono emerse come scelte principali per l’addestramento di modelli di apprendimento automatico.</p>
<p>L’uso di acceleratori hardware è iniziato con <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a>, che ha aperto la strada a lavori futuri per utilizzare le GPU come acceleratori hardware per l’addestramento di modelli di visione artificiale. Le GPU, o “Graphics Processing Units” [unità di elaborazione grafica], eccellono nella gestione di molti calcoli contemporaneamente, il che le rende ideali per le operazioni matriciali centrali per l’addestramento delle reti neurali. La loro architettura, progettata per il rendering della grafica, è perfetta per le operazioni matematiche richieste nell’apprendimento automatico. Sebbene siano molto utili per le attività di apprendimento automatico e siano state implementate in molte piattaforme hardware, le GPU sono comunque di uso generale in quanto possono essere utilizzate per altre applicazioni.</p>
<p>D’altro canto, le <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Tensor Processing Units</a> (TPU) sono unità hardware progettate specificamente per le reti neurali. Si concentrano sull’operazione di “moltiplicazione e accumulazione” (MAC) e il loro hardware è costituito da una grande matrice hardware che contiene elementi che calcolano in modo efficiente l’operazione MAC. Questo concetto, chiamato <a href="https://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf">systolic array architecture</a>, è stato ideato da <span class="citation" data-cites="kung1979systolic">Kung e Leiserson (<a href="../../references.it.html#ref-kung1979systolic" role="doc-biblioref">1979</a>)</span>, ma ha dimostrato di essere una struttura utile per calcolare in modo efficiente i prodotti matriciali e altre operazioni all’interno delle reti neurali (come le convoluzioni).</p>
<div class="no-row-height column-margin column-container"><div id="ref-kung1979systolic" class="csl-entry" role="listitem">
Kung, Hsiang Tsung, e Charles E Leiserson. 1979. <span>«Systolic arrays (for <span>VLSI)</span>»</span>. In <em>Sparse Matrix Proceedings 1978</em>, 1:256–82. Society for industrial; applied mathematics Philadelphia, PA, USA.
</div></div><p>Sebbene le TPU possano ridurre drasticamente i tempi di addestramento, presentano anche degli svantaggi. Ad esempio, molte operazioni all’interno dei framework di apprendimento automatico (principalmente TensorFlow in questo caso, poiché la TPU si integra direttamente con esso) non sono supportate dalle TPU. Non possono inoltre supportare operazioni personalizzate dai framework di apprendimento automatico e la progettazione della rete deve essere strettamente allineata alle capacità hardware.</p>
<p>Oggi, le GPU NVIDIA dominano il training, supportate da librerie software come <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a>, <a href="https://developer.nvidia.com/cudnn">cuDNN</a> e <a href="https://developer.nvidia.com/tensorrt#:~:text=NVIDIA">TensorRT</a>. I framework includono anche ottimizzazioni per massimizzare le prestazioni su questi tipi di hardware, come l’eliminazione di connessioni non importanti e la fusione di layer. La combinazione di queste tecniche con l’accelerazione hardware fornisce una maggiore efficienza. Per l’inferenza, l’hardware si sta spostando sempre di più verso ASIC e SoC ottimizzati. Le TPU di Google accelerano i modelli nei data center, mentre Apple, Qualcomm, la famiglia NVIDIA Jetson e altri ora producono chip “mobili” incentrati sull’intelligenza artificiale.</p>
<div id="fig-hardware-accelerator" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hardware-accelerator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hardware_accelerator.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-accelerator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.10: Aziende che offrono acceleratori hardware di ML. Fonte: <a href="https://gradientflow.com/one-simple-chart-companies-that-offer-deep-neural-network-accelerators/">Gradient Flow.</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ai_frameworks-advanced" class="level2 page-columns page-full" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="sec-ai_frameworks-advanced"><span class="header-section-number">6.5</span> Funzionalità Avanzate</h2>
<p>Oltre a fornire gli strumenti essenziali per il training di modelli di apprendimento automatico, i framework offrono anche funzionalità avanzate. Queste funzionalità includono la distribuzione del training su diverse piattaforme hardware, la facile messa a punto di grandi modelli pre-addestrati e l’esemplificazione del “federated learning”. L’implementazione di queste funzionalità in modo indipendente sarebbe altamente complessa e richiederebbe molte risorse, ma i framework semplificano questi processi, rendendo le tecniche avanzate di apprendimento automatico più accessibili.</p>
<section id="training-distribuito" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="training-distribuito"><span class="header-section-number">6.5.1</span> Training distribuito</h3>
<p>Poiché i modelli di apprendimento automatico sono diventati più grandi nel corso degli anni, è diventato essenziale per i modelli di grandi dimensioni utilizzare più nodi di elaborazione nel processo di training. Questo processo, l’apprendimento distribuito, ha consentito maggiori capacità di training, ma ha anche imposto problemi nell’implementazione.</p>
<p>Possiamo considerare tre diversi modi per distribuire il lavoro di training dei modelli di apprendimento automatico su più nodi di elaborazione. Il partizionamento dei dati di input (o parallelismo dei dati) si riferisce a più processori che eseguono lo stesso modello su diverse partizioni di input. Questa è l’implementazione più semplice ed è disponibile per molti framework di machine learning. La distribuzione più impegnativa del lavoro è rappresentata dal parallelismo del modello, che si riferisce a più nodi di elaborazione che lavorano su parti diverse del modello, e dal parallelismo del modello pipelined, che si riferisce a più nodi di elaborazione che lavorano su diversi layer del modello sullo stesso input. Gli ultimi due menzionati qui sono aree di ricerca attive.</p>
<p>I framework di ML che supportano l’apprendimento distribuito includono TensorFlow (tramite il suo modulo <a href="https://www.tensorflow.org/api_docs/python/tf/distribute">tf.distribute</a>), PyTorch (tramite i suoi moduli <a href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html">torch.nn.DataParallel</a> e <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">torch.nn.DistributedDataParallel</a>) e MXNet (tramite la sua API <a href="https://mxnet.apache.org/versions/1.9.1/api/python/docs/api/gluon/index.html">gluon</a>).</p>
</section>
<section id="conversione-del-modello" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="conversione-del-modello"><span class="header-section-number">6.5.2</span> Conversione del Modello</h3>
<p>I modelli di machine learning hanno vari metodi per essere rappresentati e utilizzati in diversi framework e per diversi tipi di dispositivi. Ad esempio, un modello può essere convertito per essere compatibile con i framework di inferenza all’interno del dispositivo mobile. Il formato di default per i modelli TensorFlow sono i file di checkpoint contenenti pesi e architetture, necessari per riaddestrare i modelli. Tuttavia, i modelli vengono in genere convertiti nel formato TensorFlow Lite per la distribuzione mobile. TensorFlow Lite utilizza una rappresentazione compatta del “flat buffer” e ottimizzazioni per un’inferenza rapida su hardware mobile, eliminando tutto il bagaglio non necessario associato ai metadati di addestramento, come le strutture dei file di checkpoint.</p>
<p>Le ottimizzazioni del modello come la quantizzazione (vedere il capitolo <a href="../optimizations/optimizations.qmd">Ottimizzazioni</a>) possono ottimizzare ulteriormente i modelli per architetture target come i dispositivi mobili. Ciò riduce la precisione di pesi e attivazioni a <code>uint8</code> o a <code>int8</code> per un ingombro ridotto e un’esecuzione più rapida con acceleratori hardware supportati. Per la quantizzazione post-training, il convertitore di TensorFlow gestisce automaticamente analisi e conversione.</p>
<p>Framework come TensorFlow semplificano la distribuzione di modelli addestrati su dispositivi IoT mobili ed embedded tramite API di conversione semplici per il formato TFLite e la quantizzazione. La conversione pronta all’uso consente un’inferenza ad alte prestazioni su dispositivi mobili senza l’onere dell’ottimizzazione manuale. Oltre a TFLite, altri target comuni includono TensorFlow.js per la distribuzione Web, TensorFlow Serving per i servizi cloud e TensorFlow Hub per l’apprendimento tramite trasferimento. Le utility di conversione di TensorFlow gestiscono questi scenari per semplificare i flussi di lavoro end-to-end.</p>
<p>Ulteriori informazioni sulla conversione dei modelli in TensorFlow sono linkate <a href="https://www.tensorflow.org/lite/models/convert">qui</a>.</p>
</section>
<section id="automl-no-codelow-code-ml" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="automl-no-codelow-code-ml"><span class="header-section-number">6.5.3</span> AutoML, No-Code/Low-Code ML</h3>
<p>In molti casi, l’apprendimento automatico può avere una barriera d’ingresso relativamente alta rispetto ad altri campi. Per addestrare e distribuire con successo modelli, è necessario avere una comprensione critica di una varietà di discipline, dalla scienza dei dati (elaborazione dei dati, pulizia dei dati), strutture di modelli (ottimizzazione degli iperparametri, architettura delle reti neurali), hardware (accelerazione, elaborazione parallela) e altro a seconda del problema in questione. La complessità di questi problemi ha portato all’introduzione di framework come AutoML, che cerca di rendere “l’apprendimento automatico disponibile anche a chi non è esperto di apprendimento automatico” e di “automatizzare la ricerca nell’apprendimento automatico”. Hanno creato AutoWEKA, che aiuta nel complesso processo di selezione degli iperparametri, e Auto-sklearn e Auto-pytorch, un’estensione di AutoWEKA nelle popolari librerie sklearn e PyTorch.</p>
<p>Mentre questi sforzi per automatizzare parti delle attività di apprendimento automatico sono in corso, altri si sono concentrati sulla semplificazione dei modelli tramite l’implementazione di apprendimento automatico “no-code” [senza codice]/low-code [a basso codice], utilizzando un’interfaccia drag-and-drop con un’interfaccia utente di facile navigazione. Aziende come Apple, Google e Amazon hanno già creato queste piattaforme di facile utilizzo per consentire agli utenti di costruire modelli di apprendimento automatico che possono essere integrati nel loro ecosistema.</p>
<p>Questi passaggi per rimuovere le barriere all’ingresso continuano a democratizzare il machine learning, semplificano l’accesso per i principianti e semplificano il flusso di lavoro per gli esperti.</p>
</section>
<section id="metodi-di-apprendimento-avanzati" class="level3 page-columns page-full" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="metodi-di-apprendimento-avanzati"><span class="header-section-number">6.5.4</span> Metodi di Apprendimento Avanzati</h3>
<section id="il-transfer-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="il-transfer-learning">Il Transfer Learning</h4>
<p>Il “transfer learning” è la pratica di utilizzare le conoscenze acquisite da un modello pre-addestrato per addestrare e migliorare le prestazioni di un modello per un’attività diversa. Ad esempio, modelli come MobileNet e ResNet vengono addestrati sul set di dati ImageNet. Per fare ciò, si può congelare il modello pre-addestrato, utilizzandolo come un estrattore di feature per addestrare un modello molto più piccolo costruito sopra l’estrazione di feature. Si può anche mettere a punto l’intero modello per adattarlo al nuovo compito. I framework di apprendimento automatico semplificano il caricamento di modelli pre-addestrati, il congelamento di layer specifici e l’addestramento di layer personalizzati in cima. Semplificano questo processo fornendo API intuitive e un facile accesso a grandi repository di <a href="https://keras.io/api/applications/">modelli pre-addestrati</a>.</p>
<p>L’apprendimento tramite trasferimento presenta delle sfide, come l’incapacità del modello modificato di svolgere le sue attività originali dopo l’apprendimento tramite trasferimento. Articoli come <a href="https://browse.arxiv.org/pdf/1606.09282.pdf">“Learning without Forgetting”</a> di <span class="citation" data-cites="li2017learning">Z. Li e Hoiem (<a href="../../references.it.html#ref-li2017learning" role="doc-biblioref">2018</a>)</span> cercano di affrontare queste sfide e sono stati implementati nelle moderne piattaforme di apprendimento automatico. <a href="../robust_ai/robust_ai.it.html#fig-transfer-learning" class="quarto-xref">Figura&nbsp;<span>17.33</span></a> semplifica il concetto di apprendimento tramite un esempio.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017learning" class="csl-entry" role="listitem">
Li, Zhizhong, e Derek Hoiem. 2018. <span>«Learning without Forgetting»</span>. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em> 40 (12): 2935–47. <a href="https://doi.org/10.1109/tpami.2017.2773081">https://doi.org/10.1109/tpami.2017.2773081</a>.
</div></div><div id="fig-transfer-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/transferlearning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.11: Trasferimento dell’apprendimento. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fanalyticsindiamag.com%2Fdevelopers-corner%2Fcomplete-guide-to-understanding-precision-and-recall-curves%2F&amp;psig=AOvVaw3MosZItazJt2eermLTArjj&amp;ust=1722534897757000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCIi389bs0YcDFQAAAAAdAAAAABAw">Tech Target</a>
</figcaption>
</figure>
</div>
</section>
<section id="il-federated-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="il-federated-learning">Il Federated Learning</h4>
<p>Il “Federated learning” di <span class="citation" data-cites="mcmahan2023communicationefficient">McMahan et al. (<a href="../../references.it.html#ref-mcmahan2023communicationefficient" role="doc-biblioref">2017</a>)</span> è una forma di elaborazione distribuita che prevede l’addestramento di modelli su dispositivi personali anziché la centralizzazione dei dati su un singolo server (<a href="../ondevice_learning/ondevice_learning.it.html#fig-federated-learning" class="quarto-xref">Figura&nbsp;<span>12.7</span></a>). Inizialmente, un modello globale di base viene addestrato su un server centrale per essere distribuito a tutti i dispositivi. Utilizzando questo modello di base, i dispositivi calcolano individualmente i gradienti e li inviano all’hub centrale. Intuitivamente, questo trasferisce i parametri del modello anziché i dati stessi. L’apprendimento federato migliora la privacy mantenendo i dati sensibili sui dispositivi locali e condividendo gli aggiornamenti del modello solo con un server centrale. Questo metodo è particolarmente utile quando si gestiscono dati sensibili o quando un’infrastruttura su larga scala non è praticabile.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mcmahan2023communicationefficient" class="csl-entry" role="listitem">
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, e Blaise Agüera y Arcas. 2017. <span>«Communication-Efficient Learning of Deep Networks from Decentralized Data»</span>. In <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA</em>, a cura di Aarti Singh e Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.
</div></div><div id="fig-federated-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-federated-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/federated_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-federated-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.12: Un approccio con server centralizzato al “federated learning”. Fonte: <a href="https://blogs.nvidia.com/blog/what-is-federated-learning/">NVIDIA.</a>
</figcaption>
</figure>
</div>
<p>Tuttavia, il federated learning deve affrontare sfide come garantire l’accuratezza dei dati, gestire dati non-IID (independent and identically distributed) [indipendenti e distribuiti in modo identico], gestire la produzione di dati non bilanciata e superare il sovraccarico della comunicazione e l’eterogeneità dei dispositivi. Anche i problemi di privacy e sicurezza, come gli attacchi di inversione del gradiente, pongono sfide significative.</p>
<p>I framework di apprendimento automatico semplificano l’implementazione dell’apprendimento federato fornendo gli strumenti e le librerie necessarie. Ad esempio, <a href="https://www.tensorflow.org/federated">TensorFlow Federated (TFF)</a> offre un framework open source per supportare l’apprendimento federato. TFF consente agli sviluppatori di simulare e implementare algoritmi di apprendimento federato, offrendo un core federato per operazioni di basso livello e API di alto livello per attività federate comuni. Si integra perfettamente con TensorFlow, consentendo l’uso di modelli e ottimizzatori TensorFlow in un ambiente federato. TFF supporta tecniche di aggregazione sicure per migliorare la privacy e consente la personalizzazione degli algoritmi di apprendimento federato. Sfruttando questi strumenti, gli sviluppatori possono distribuire in modo efficiente il training, perfezionare i modelli pre-addestrati e gestire le complessità intrinseche dell’apprendimento federato.</p>
<p>Sono stati sviluppati anche altri programmi open source come <a href="https://flower.dev/">Flower</a> per semplificare l’implementazione dell’apprendimento federato con vari framework di machine learning.</p>
</section>
</section>
</section>
<section id="specializzazione-del-framework" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="specializzazione-del-framework"><span class="header-section-number">6.6</span> Specializzazione del Framework</h2>
<p>Finora abbiamo parlato in generale dei framework di ML. Tuttavia, in genere, i framework sono ottimizzati in base alle capacità computazionali e ai requisiti applicativi dell’ambiente target, che vanno dal cloud all’edge ai dispositivi minuscoli. La scelta del framework giusto è fondamentale in base all’ambiente target per la distribuzione. Questa sezione fornisce una panoramica dei principali tipi di framework di IA su misura per ambienti cloud, edge e TinyML per aiutare a comprendere le somiglianze e le differenze tra questi ecosistemi.</p>
<section id="cloud" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="cloud"><span class="header-section-number">6.6.1</span> Cloud</h3>
<p>I framework di IA basati su cloud presuppongono l’accesso a un’ampia potenza di calcolo, memoria e risorse di archiviazione nel cloud. In genere supportano sia il training che l’inferenza. I framework di IA basati su cloud sono adatti per applicazioni in cui i dati possono essere inviati al cloud per l’elaborazione, come servizi di IA basati su cloud, analisi di dati su larga scala e applicazioni Web. I framework di IA cloud più diffusi includono quelli che abbiamo menzionato in precedenza, come TensorFlow, PyTorch, MXNet, Keras, ecc. Questi framework utilizzano GPU, TPU, training distribuito e AutoML per fornire IA scalabile. Concetti come model serving, MLOps e AIOps sono correlati all’operatività dell’IA nel cloud. L’IA cloud alimenta servizi come Google Cloud AI e consente il “transfer learning” tramite modelli pre-addestrati.</p>
</section>
<section id="edge" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="edge"><span class="header-section-number">6.6.2</span> Edge</h3>
<p>I framework Edge AI sono pensati per distribuire modelli di IA su dispositivi IoT, smartphone e server edge. I framework Edge AI sono ottimizzati per dispositivi con risorse di calcolo moderate, bilanciando potenza e prestazioni. I framework Edge AI sono ideali per applicazioni che richiedono elaborazione in tempo reale o quasi reale, tra cui robotica, veicoli autonomi e dispositivi intelligenti. I principali framework Edge AI includono TensorFlow Lite, PyTorch Mobile, CoreML e altri. Impiegano ottimizzazioni come compressione del modello, quantizzazione ed architetture di reti neurali efficienti. Il supporto hardware include CPU, GPU, NPU e acceleratori come Edge TPU. Edge AI consente casi d’uso come visione mobile, riconoscimento vocale e rilevamento di anomalie in tempo reale.</p>
</section>
<section id="embedded" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="embedded"><span class="header-section-number">6.6.3</span> Embedded</h3>
<p>I framework TinyML sono specializzati per distribuire modelli AI su dispositivi con risorse estremamente limitate, in particolare microcontrollori e sensori all’interno dell’ecosistema IoT. I framework TinyML sono progettati per dispositivi con risorse limitate, enfatizzando memoria minima e consumo energetico. I framework TinyML sono specializzati per casi d’uso su dispositivi IoT con risorse limitate per applicazioni di manutenzione predittiva, riconoscimento dei gesti e monitoraggio ambientale. I principali framework TinyML includono TensorFlow Lite Micro, uTensor e ARM NN. Ottimizzano modelli complessi per adattarli a kilobyte di memoria tramite tecniche come l’addestramento consapevole della quantizzazione e la precisione ridotta. TinyML consente il rilevamento intelligente su dispositivi alimentati a batteria, consentendo l’apprendimento collaborativo tramite apprendimento federato. La scelta del framework implica il bilanciamento delle prestazioni del modello e dei vincoli computazionali della piattaforma target, che sia cloud, edge o TinyML. <a href="#tbl-ml_frameworks" class="quarto-xref">Tabella&nbsp;<span>6.3</span></a> confronta i principali framework di IA negli ambienti cloud, edge e TinyML:</p>
<div id="tbl-ml_frameworks" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ml_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;6.3: Confronto dei tipi di framework per Cloud AI, Edge AI e TinyML.
</figcaption>
<div aria-describedby="tbl-ml_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 17%">
<col style="width: 38%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Tipo di framework</th>
<th style="text-align: left;">Esempi</th>
<th style="text-align: left;">Tecnologie chiave</th>
<th style="text-align: left;">Casi d’uso</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Cloud AI</td>
<td style="text-align: left;">TensorFlow, PyTorch, MXNet, Keras</td>
<td style="text-align: left;">GPU, TPU, addestramento distribuito, AutoML, MLOps</td>
<td style="text-align: left;">Servizi cloud, app Web, analisi di big data</td>
</tr>
<tr class="even">
<td style="text-align: left;">Edge AI</td>
<td style="text-align: left;">TensorFlow Lite, PyTorch Mobile, Core ML</td>
<td style="text-align: left;">Ottimizzazione del modello, compressione, quantizzazione, architetture NN efficienti</td>
<td style="text-align: left;">App mobili, sistemi autonomi, elaborazione in tempo reale</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TinyML</td>
<td style="text-align: left;">TensorFlow Lite Micro, uTensor, ARM NN</td>
<td style="text-align: left;">Training consapevole della quantizzazione, precisione ridotta, ricerca di architettura neurale</td>
<td style="text-align: left;">Sensori IoT, dispositivi indossabili, manutenzione predittiva, riconoscimento dei gesti</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Differenze principali:</strong></p>
<ul>
<li><p>Cloud AI sfrutta un’enorme potenza di calcolo per modelli complessi utilizzando GPU/TPU e training distribuito.</p></li>
<li><p>Edge AI ottimizza i modelli per l’esecuzione locale su dispositivi edge con risorse limitate.</p></li>
<li><p>TinyML adatta i modelli a una memoria estremamente bassa e calcola ambienti come i microcontrollori.</p></li>
</ul>
</section>
</section>
<section id="sec-ai_frameworks_embedded" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sec-ai_frameworks_embedded"><span class="header-section-number">6.7</span> Framework di IA Embedded</h2>
<section id="vincoli-di-risorse" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="vincoli-di-risorse"><span class="header-section-number">6.7.1</span> Vincoli di Risorse</h3>
<p>I sistemi embedded affrontano gravi limitazioni di risorse che pongono sfide uniche quando si distribuiscono modelli di machine learning rispetto alle piattaforme di elaborazione tradizionali. Ad esempio, le unità microcontrollore (MCU) comunemente utilizzate nei dispositivi IoT hanno spesso:</p>
<ul>
<li><p><strong>RAM</strong> varia da decine di kilobyte a pochi megabyte. Il popolare <a href="https://www.espressif.com/en/products/socs/esp8266">MCU ESP8266</a> ha circa 80 KB di RAM a disposizione degli sviluppatori. Ciò contrasta con 8 GB o più su laptop e desktop tipici odierni.</p></li>
<li><p><strong>Memoria Flash</strong> varia da centinaia di kilobyte a pochi megabyte. Il microcontrollore Arduino Uno fornisce solo 32 KB di archiviazione del codice. I computer standard odierni hanno un’archiviazione su disco nell’ordine dei terabyte.</p></li>
<li><p><strong>Potenza di elaborazione</strong> da pochi MHz a circa 200 MHz. L’ESP8266 funziona a 80 MHz. Questo è di diversi ordini di grandezza più lento delle CPU multi-core multi-GHz nei server e nei laptop di fascia alta.</p></li>
</ul>
<p>Questi vincoli rigorosi spesso rendono impossibile l’addestramento di modelli di apprendimento automatico direttamente sui microcontrollori. La RAM limitata impedisce la gestione di grandi set di dati per il training. L’uso di energia per l’addestramento esaurirebbe rapidamente anche i dispositivi alimentati a batteria. Al contrario, i modelli vengono addestrati su sistemi ricchi di risorse e distribuiti su microcontrollori per un’inferenza ottimizzata. Ma anche l’inferenza pone delle sfide:</p>
<ol type="1">
<li><p><strong>Dimensioni del Modello:</strong> I modelli di intelligenza artificiale sono troppo grandi per adattarsi a dispositivi IoT ed embedded. Ciò richiede tecniche di compressione del modello, come quantizzazione, potatura e “knowledge distillation” [distillazione della conoscenza]. Inoltre, come vedremo, molti dei framework utilizzati dagli sviluppatori di intelligenza artificiale hanno grandi quantità di overhead e librerie integrate che i sistemi embedded non possono supportare.</p></li>
<li><p><strong>Complessità delle Attività:</strong> Con solo decine di KB o pochi MB di RAM, i dispositivi IoT e i sistemi embedded sono limitati nella complessità delle attività che possono gestire. Le attività che richiedono grandi set di dati o algoritmi sofisticati, ad esempio LLM, che verrebbero eseguiti senza problemi su piattaforme di elaborazione tradizionali potrebbero non essere fattibili su sistemi embedded senza compressione o altre tecniche di ottimizzazione a causa delle limitazioni di memoria.</p></li>
<li><p><strong>Archiviazione ed Elaborazione dei Dati:</strong> I sistemi embedded spesso elaborano i dati in tempo reale e potrebbero archiviarne solo piccole quantità localmente. Al contrario, i sistemi di elaborazione tradizionali possono contenere ed elaborare grandi set di dati in memoria, consentendo un’analisi più rapida delle operazioni sui dati e aggiornamenti in tempo reale.</p></li>
<li><p><strong>Sicurezza e Privacy:</strong> La poca memoria limita anche la complessità degli algoritmi e dei protocolli di sicurezza, la crittografia dei dati, le protezioni da reverse engineering e altro che può essere implementato sul dispositivo. Ciò potrebbe rendere alcuni dispositivi IoT più vulnerabili agli attacchi.</p></li>
</ol>
<p>Di conseguenza, le ottimizzazioni software specializzate e i framework ML su misura per i microcontrollori devono funzionare entro questi stretti limiti delle risorse. Tecniche di ottimizzazione intelligenti come quantizzazione, potatura e distillazione della conoscenza comprimono i modelli per adattarli alla memoria limitata (vedere la sezione Ottimizzazioni). Gli insegnamenti tratti dalla ricerca di architettura neurale aiutano a guidare la progettazione dei modelli.</p>
<p>I miglioramenti hardware come gli acceleratori ML dedicati sui microcontrollori aiutano anche ad alleviare i vincoli. Ad esempio, <a href="https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor">Hexagon DSP di Qualcomm</a> accelera i modelli TensorFlow Lite sui chip mobili Snapdragon. <a href="https://cloud.google.com/edge-tpu">Google Edge TPU</a> racchiude le prestazioni ML in un piccolo ASIC per dispositivi edge. <a href="https://www.arm.com/products/silicon-ip-cpu/ethos/ethos-u55">ARM Ethos-U55</a> offre un’inferenza efficiente sui microcontrollori di classe Cortex-M. Questi chip ML personalizzati sbloccano funzionalità avanzate per applicazioni con risorse limitate.</p>
<p>A causa della potenza di elaborazione limitata, è quasi sempre impossibile addestrare modelli di intelligenza artificiale su IoT o sistemi embedded. Invece, i modelli vengono addestrati su potenti computer tradizionali (spesso con GPU) e poi distribuiti sul dispositivo embedded per l’inferenza. TinyML si occupa specificamente di questo, assicurando che i modelli siano sufficientemente leggeri per l’inferenza in tempo reale su questi dispositivi limitati.</p>
</section>
<section id="framework-e-librerie" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="framework-e-librerie"><span class="header-section-number">6.7.2</span> Framework e Librerie</h3>
<p>I framework di intelligenza artificiale embedded sono strumenti software e librerie progettati per abilitare funzionalità di intelligenza artificiale e ML su sistemi embedded. Questi framework sono essenziali per portare l’intelligenza artificiale su dispositivi IoT, robotica e altre piattaforme di edge computing e sono progettati per funzionare dove risorse di elaborazione, memoria e consumo energetico sono limitati.</p>
</section>
<section id="sfide" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="sfide"><span class="header-section-number">6.7.3</span> Sfide</h3>
<p>Sebbene i sistemi embedded rappresentino un’enorme opportunità per l’implementazione dell’apprendimento automatico per abilitare capacità intelligenti in edge, questi ambienti con risorse limitate pongono sfide significative. A differenza dei tipici ambienti cloud o desktop ricchi di risorse computazionali, i dispositivi embedded introducono gravi limitazioni in termini di memoria, potenza di elaborazione, efficienza energetica e hardware specializzato. Di conseguenza, le tecniche e i framework di apprendimento automatico esistenti progettati per cluster di server con risorse abbondanti non si traducono direttamente nei sistemi embedded. Questa sezione svela alcune delle sfide e delle opportunità per i sistemi embedded e i framework ML.</p>
<section id="ecosistema-frammentato" class="level4">
<h4 class="anchored" data-anchor-id="ecosistema-frammentato">Ecosistema Frammentato</h4>
<p>La mancanza di un framework ML unificato ha portato a un ecosistema altamente frammentato. Gli ingegneri di aziende come <a href="https://www.st.com/">STMicroelectronics</a>, <a href="https://www.nxp.com/">NXP Semiconductors</a> e <a href="https://www.renesas.com/">Renesas</a> hanno dovuto sviluppare soluzioni personalizzate su misura per le loro specifiche architetture di microcontrollori e DSP. Questi framework ad hoc richiedevano un’ampia ottimizzazione manuale per ogni piattaforma hardware di basso livello. Ciò ha reso estremamente difficile il porting dei modelli, richiedendo la riqualificazione per nuove architetture Arm, RISC-V o proprietarie.</p>
</section>
<section id="esigenze-hardware-disparate" class="level4">
<h4 class="anchored" data-anchor-id="esigenze-hardware-disparate">Esigenze Hardware Disparate</h4>
<p>Senza un framework condiviso, non esisteva un modo standard per valutare le capacità dell’hardware. Fornitori come Intel, Qualcomm e NVIDIA crearono soluzioni integrate, combinando modelli e migliorando software e hardware. Ciò rese difficile discernere i motivi del guadagni di prestazioni, se fosse merito dei nuovi progetti di chip come i core x86 a basso consumo di Intel o le ottimizzazioni software. Era necessario un framework standard affinché i fornitori potessero valutare le capacità del loro hardware in modo equo e riproducibile.</p>
</section>
<section id="mancanza-di-portabilità" class="level4">
<h4 class="anchored" data-anchor-id="mancanza-di-portabilità">Mancanza di Portabilità</h4>
<p>Con strumenti standardizzati, adattare modelli addestrati in framework comuni come TensorFlow o PyTorch per funzionare in modo efficiente sui microcontrollori era più facile. Richiedeva una traduzione manuale dispendiosa, in termini di tempo, dei modelli per l’esecuzione su DSP specializzati di aziende come CEVA o core Arm M-series a basso consumo. Nessuno strumento immediato consentiva l’implementazione portatile su diverse architetture.</p>
</section>
<section id="infrastruttura-incompleta" class="level4">
<h4 class="anchored" data-anchor-id="infrastruttura-incompleta">Infrastruttura Incompleta</h4>
<p>L’infrastruttura per supportare i flussi di lavoro di sviluppo dei modelli chiave doveva essere migliorata. È necessario un maggiore supporto per le tecniche di compressione per adattare modelli di grandi dimensioni a budget di memoria limitati. Mancavano strumenti per la quantizzazione per ridurre la precisione per un’inferenza più rapida. Le API standardizzate per l’integrazione nelle applicazioni erano incomplete. Mancavano funzionalità essenziali come il debugging sul dispositivo, le metriche e la profilazione delle prestazioni. Queste lacune hanno aumentato i costi e la difficoltà dello sviluppo ML embedded.</p>
</section>
<section id="nessun-benchmark-standard" class="level4">
<h4 class="anchored" data-anchor-id="nessun-benchmark-standard">Nessun Benchmark Standard</h4>
<p>Senza benchmark unificati, non esisteva un modo standard per valutare e confrontare le capacità di diverse piattaforme hardware di fornitori come NVIDIA, Arm e Ambiq Micro.. Le valutazioni esistenti si basavano su benchmark proprietari pensati per mostrare i punti di forza di specifici chip. Ciò rendeva impossibile misurare i miglioramenti hardware in modo oggettivo, imparziale e imparziale. Il capitolo <a href="../benchmarking/benchmarking.qmd">Benchmarking dell’IA</a> affronta questo argomento in modo più dettagliato.</p>
</section>
<section id="test-minimi-del-mondo-reale" class="level4">
<h4 class="anchored" data-anchor-id="test-minimi-del-mondo-reale">Test Minimi del Mondo Reale</h4>
<p>Gran parte dei benchmark si basava su dati sintetici. Testare rigorosamente i modelli su applicazioni embedded nel mondo reale era difficile senza set di dati e benchmark standardizzati, sollevando dubbi su come le dichiarazioni sulle prestazioni si sarebbero tradotte in un utilizzo nel mondo reale. Erano necessari test più approfonditi per convalidare i chip in casi di utilizzo reali.</p>
<p>La mancanza di framework e infrastrutture condivisi ha rallentato l’adozione di TinyML, ostacolandone l’integrazione nei prodotti embedded. I recenti framework standard hanno iniziato ad affrontare questi problemi attraverso una migliore portabilità, profilazione delle prestazioni e supporto per il benchmarking. Tuttavia, è ancora necessaria un’innovazione continua per consentire un’implementazione fluida e conveniente dell’IA nei dispositivi edge.</p>
</section>
<section id="riepilogo" class="level4">
<h4 class="anchored" data-anchor-id="riepilogo">Riepilogo</h4>
<p>L’assenza di framework, benchmark e infrastrutture standardizzati per ML embedded ne ha tradizionalmente ostacolato l’adozione. Tuttavia, sono stati compiuti recenti progressi nello sviluppo di framework condivisi come TensorFlow Lite Micro e suite di benchmark come MLPerf Tiny che mirano ad accelerare la proliferazione di soluzioni TinyML. Tuttavia, superare la frammentazione e la difficoltà dell’implementazione embedded rimane un processo in corso.</p>
</section>
</section>
</section>
<section id="esempi" class="level2 page-columns page-full" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="esempi"><span class="header-section-number">6.8</span> Esempi</h2>
<p>Il deployment [distribuzione] di machine learning su microcontrollori e altri dispositivi embedded richiede spesso librerie software e framework appositamente ottimizzati per funzionare entro vincoli rigorosi di memoria, elaborazione e potenza. Esistono diverse opzioni per eseguire l’inferenza su hardware con risorse limitate, ciascuna con il proprio approccio all’ottimizzazione dell’esecuzione del modello. Questa sezione esplorerà le caratteristiche chiave e i principi di progettazione alla base di TFLite Micro, TinyEngine e CMSIS-NN, fornendo informazioni su come ogni framework affronta il complesso problema dell’esecuzione di reti neurali molto accurata ma efficiente sui microcontrollori. Mostrerà inoltre diversi approcci per l’implementazione di framework TinyML efficienti.</p>
<p><a href="#tbl-compare_frameworks" class="quarto-xref">Tabella&nbsp;<span>6.4</span></a> riassume le principali differenze e somiglianze tra questi tre framework di inferenza di apprendimento automatico specializzati per sistemi embedded e microcontrollori.</p>
<div id="tbl-compare_frameworks" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-compare_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;6.4: Confronto dei framework: TensorFlow Lite Micro, TinyEngine e CMSIS-NN
</figcaption>
<div aria-describedby="tbl-compare_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 24%">
<col style="width: 27%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Framework</th>
<th style="text-align: left;">TensorFlow Lite Micro</th>
<th style="text-align: left;">TinyEngine</th>
<th style="text-align: left;">CMSIS-NN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Approccio</td>
<td style="text-align: left;">Basato su interprete</td>
<td style="text-align: left;">Compilazione statica</td>
<td style="text-align: left;">Kernel di reti neurali ottimizzati</td>
</tr>
<tr class="even">
<td style="text-align: left;">Focus sull’hardware</td>
<td style="text-align: left;">Dispositivi embedded generali</td>
<td style="text-align: left;">Microcontrollori</td>
<td style="text-align: left;">Processori ARM Cortex-M</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Supporto aritmetico</td>
<td style="text-align: left;">Virgola mobile</td>
<td style="text-align: left;">Virgola mobile, virgola fissa</td>
<td style="text-align: left;">Virgola mobile, virgola fissa</td>
</tr>
<tr class="even">
<td style="text-align: left;">Supporto del modello</td>
<td style="text-align: left;">Modelli di rete neurale generale</td>
<td style="text-align: left;">Modelli co-progettati con TinyNAS</td>
<td style="text-align: left;">Tipi di livelli di rete neurale comuni</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Impronta del codice</td>
<td style="text-align: left;">Più grande grazie all’inclusione di interprete e operazioni</td>
<td style="text-align: left;">Piccola, include solo le operazioni necessarie per il modello</td>
<td style="text-align: left;">Nativamente leggera</td>
</tr>
<tr class="even">
<td style="text-align: left;">Latenza</td>
<td style="text-align: left;">Più alta grazie a overhead di interpretazione</td>
<td style="text-align: left;">Molto bassa grazie al modello compilato</td>
<td style="text-align: left;">focalizzato sulla bassa latenza</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Gestione della memoria</td>
<td style="text-align: left;">Gestita dinamicamente da interprete</td>
<td style="text-align: left;">Ottimizzazione a livello di modello</td>
<td style="text-align: left;">Strumenti per un’allocazione efficiente</td>
</tr>
<tr class="even">
<td style="text-align: left;">Approccio di ottimizzazione</td>
<td style="text-align: left;">Alcune funzionalità di e generazione del codice</td>
<td style="text-align: left;">Kernel specializzati, fusione di operatori</td>
<td style="text-align: left;">Ottimizzazioni di assemblaggio specifiche dell’architettura</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Principali vantaggi</td>
<td style="text-align: left;">Flessibilità, portabilità, facile aggiornamento dei modelli</td>
<td style="text-align: left;">Massimizza le prestazioni, ottimizza l’utilizzo della memoria</td>
<td style="text-align: left;">Accelerazione hardware, API standardizzata, portabilità</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Ne comprenderemo ciascuno in modo più dettagliato nelle sezioni seguenti.</p>
<section id="interprete" class="level3 page-columns page-full" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="interprete"><span class="header-section-number">6.8.1</span> Interprete</h3>
<p><a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite Micro (TFLM)</a> è un framework di inferenza di apprendimento automatico progettato per dispositivi embedded con risorse limitate. Utilizza un interprete per caricare ed eseguire modelli di apprendimento automatico, il che fornisce flessibilità e facilità di aggiornamento dei modelli sul campo <span class="citation" data-cites="david2021tensorflow">(<a href="../../references.it.html#ref-david2021tensorflow" role="doc-biblioref">David et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-david2021tensorflow" class="csl-entry" role="listitem">
David, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. <span>«Tensorflow lite micro: <span>Embedded</span> machine learning for tinyml systems»</span>. <em>Proceedings of Machine Learning and Systems</em> 3: 800–811.
</div></div><p>Gli interpreti tradizionali spesso hanno un overhead di branching [diramazione] significativo, che può ridurre le prestazioni. Tuttavia, l’interpretazione del modello di machine learning trae vantaggio dall’efficienza dei kernel di lunga durata, in cui ogni runtime del kernel è relativamente grande e aiuta a mitigare l’overhead dell’interprete.</p>
<p>Un’alternativa a un motore di inferenza basato su interprete è quella di generare codice nativo da un modello durante l’esportazione. Ciò può migliorare le prestazioni, ma sacrifica portabilità e flessibilità, poiché il codice generato deve essere ricompilato per ogni piattaforma target e deve essere sostituito completamente per modificare un modello.</p>
<p>TFLM bilancia la semplicità della compilazione del codice e la flessibilità di un approccio basato su interprete includendo alcune funzionalità di generazione del codice. Ad esempio, la libreria può essere costruita esclusivamente da file sorgenti, offrendo gran parte della semplicità della compilazione associata alla generazione di codice, pur mantenendo i vantaggi di un framework che esegue il modello interpretandolo.</p>
<p>Un approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice per l’inferenza di apprendimento automatico su dispositivi embedded:</p>
<ul>
<li><p><strong>Flessibilità:</strong> I modelli possono essere aggiornati sul campo senza ricompilare l’intera applicazione.</p></li>
<li><p><strong>Portabilità:</strong> L’interprete può essere utilizzato per eseguire modelli su diverse piattaforme target senza dover effettuare il porting del codice.</p></li>
<li><p><strong>Efficienza della Memoria:</strong> L’interprete può condividere il codice su più modelli, riducendo l’utilizzo della memoria.</p></li>
<li><p><strong>Facilità di sviluppo:</strong> Gli interpreti sono più facili da sviluppare e gestire rispetto ai generatori di codice.</p></li>
</ul>
<p>TensorFlow Lite Micro è un framework potente e flessibile per l’inferenza di apprendimento automatico su dispositivi embedded. Il suo approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice, tra cui flessibilità, portabilità, efficienza della memoria e facilità di sviluppo.</p>
</section>
<section id="basati-su-compilatore" class="level3 page-columns page-full" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="basati-su-compilatore"><span class="header-section-number">6.8.2</span> Basati su Compilatore</h3>
<p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine</a> è un framework di inferenza ML progettato specificamente per microcontrollori con risorse limitate. Utilizza diverse ottimizzazioni per consentire l’esecuzione di reti neurali molto accurate entro i vincoli rigorosi di memoria, elaborazione e archiviazione sui microcontrollori <span class="citation" data-cites="lin2020mcunet">(<a href="../../references.it.html#ref-lin2020mcunet" role="doc-biblioref">Lin et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. <span>«<span>MCUNet:</span> <span>Tiny</span> Deep Learning on <span>IoT</span> Devices»</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html</a>.
</div></div><p>Mentre framework di inferenza come TFLite Micro utilizzano interpreti per eseguire il grafo della rete neurale in modo dinamico in fase di esecuzione, ciò aggiunge un overhead significativo per quanto riguarda l’utilizzo della memoria per archiviare metadati, latenza di interpretazione e mancanza di ottimizzazioni. Tuttavia, TFLite sostiene che l’overhead è piccolo. TinyEngine elimina questo overhead utilizzando un approccio di generazione del codice. Analizza il grafo di rete durante la compilazione e genera codice specializzato per eseguire solo quel modello. Questo codice viene compilato in modo nativo nel binario dell’applicazione, evitando i costi di interpretazione in fase di esecuzione.</p>
<p>I framework ML convenzionali pianificano la memoria per layer, cercando di ridurre al minimo l’utilizzo per ogni layer separatamente. TinyEngine esegue la pianificazione a livello di modello anziché analizzare l’utilizzo della memoria tra i layer. Assegna una dimensione di buffer comune in base alle esigenze massime di memoria di tutti i layer. Questo buffer viene quindi condiviso in modo efficiente tra i layer per aumentare il riutilizzo dei dati.</p>
<p>TinyEngine è inoltre specializzato nei kernel per ogni layer tramite tecniche come operatori di tiling, unrolling e fusing. Ad esempio, genererà kernel di calcolo unrolled [srotolato] con il numero di loop necessari per una convoluzione 3x3 o 5x5. Questi kernel specializzati estraggono le massime prestazioni dall’hardware del microcontrollore. Utilizza convoluzioni depthwise [in profondità] ottimizzate per ridurre al minimo le allocazioni di memoria calcolando l’output di ogni canale posizionato sui dati del canale di input. Questa tecnica sfrutta la natura separabile dei canali delle convoluzioni depthwise per ridurre le dimensioni di picco della memoria.</p>
<p>Come TFLite Micro, il binario TinyEngine compilato include solo le operazioni necessarie per un modello specifico anziché tutte le operazioni possibili. Ciò si traduce in un footprint binario molto piccolo, mantenendo basse le dimensioni del codice per i dispositivi con limiti di memoria.</p>
<p>Una differenza tra TFLite Micro e TinyEngine è che quest’ultimo è co-progettato con “TinyNAS”, un metodo di ricerca di architettura per modelli di microcontrollori simile al NAS differenziale per microcontrollori. L’efficienza di TinyEngine consente di esplorare modelli più grandi e accurati tramite NAS. Fornisce inoltre feedback a TinyNAS su quali modelli possono rientrare nei vincoli hardware.</p>
<p>Attraverso varie tecniche personalizzate, come la compilazione statica, la pianificazione basata sul modello, kernel specializzati e la co-progettazione con NAS, TinyEngine consente un’inferenza di deep learning ad alta precisione entro i vincoli di risorse rigorosi dei microcontrollori.</p>
</section>
<section id="libreria" class="level3 page-columns page-full" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="libreria"><span class="header-section-number">6.8.3</span> Libreria</h3>
<p><a href="https://www.keil.com/pack/doc/CMSIS/NN/html/index.html">CMSIS-NN</a>, acronimo di Cortex Microcontroller Software Interface Standard for Neural Networks, è una libreria software ideata da ARM. Offre un’interfaccia standardizzata per distribuire l’inferenza di reti neurali su microcontrollori e sistemi embedded, concentrandosi sull’ottimizzazione per i processori ARM Cortex-M <span class="citation" data-cites="lai2018cmsis">(<a href="../../references.it.html#ref-lai2018cmsis" role="doc-biblioref">Lai, Suda, e Chandra 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lai2018cmsis" class="csl-entry" role="listitem">
Lai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. <span>«Cmsis-nn: <span>Efficient</span> neural network kernels for arm cortex-m cpus»</span>. <em>ArXiv preprint</em> abs/1801.06601. <a href="https://arxiv.org/abs/1801.06601">https://arxiv.org/abs/1801.06601</a>.
</div></div><p><strong>Kernel di Reti Neurali:</strong> CMSIS-NN ha kernel altamente efficienti che gestiscono operazioni fondamentali di reti neurali come convoluzione, pooling, layer completamente connessi e funzioni di attivazione. Si rivolge a un’ampia gamma di modelli di reti neurali supportando l’aritmetica a virgola mobile e fissa. Quest’ultima è particolarmente utile per i dispositivi con risorse limitate in quanto riduce i requisiti di memoria e di calcolo (Quantization).</p>
<p><strong>Accelerazione Hardware:</strong> CMSIS-NN sfrutta la potenza delle istruzioni SIMD (Single Instruction, Multiple Data) disponibili su molti processori Cortex-M. Ciò consente l’elaborazione parallela di più elementi di dati all’interno di una singola istruzione, aumentando così l’efficienza computazionale. Alcuni processori Cortex-M dispongono di estensioni di Digital Signal Processing (DSP) che CMSIS-NN può sfruttare per l’esecuzione accelerata della rete neurale. La libreria include anche ottimizzazioni a livello di assembly su misura per specifiche architetture di microcontrollori per migliorare ulteriormente le prestazioni.</p>
<p><strong>API standardizzata:</strong> CMSIS-NN offre un’API coerente e astratta che protegge gli sviluppatori dalle complessità dei dettagli hardware di basso livello. Ciò semplifica l’integrazione dei modelli di rete neurale nelle applicazioni. Può anche comprendere strumenti o utilità per convertire i formati di modelli di rete neurale più diffusi in un formato compatibile con CMSIS-NN.</p>
<p><strong>Gestione della Memoria:</strong> CMSIS-NN fornisce funzioni per un’allocazione e una gestione efficienti della memoria, il che è fondamentale nei sistemi embedded in cui le risorse di memoria sono scarse. Garantisce un utilizzo ottimale della memoria durante l’inferenza e, in alcuni casi, consente operazioni in loco per ridurre il sovraccarico di memoria.</p>
<p><strong>Portabilità:</strong> CMSIS-NN è progettato per la portabilità su vari processori Cortex-M. Questo consente agli sviluppatori di scrivere codice che possa funzionare su diversi microcontrollori senza modifiche significative.</p>
<p><strong>Bassa Latenza:</strong> CMSIS-NN riduce al minimo la latenza di inferenza, rendendolo una scelta ideale per applicazioni in tempo reale in cui è fondamentale prendere decisioni rapide.</p>
<p><strong>Efficienza Energetica:</strong> La libreria è progettata con un focus sull’efficienza energetica, rendendola adatta per dispositivi alimentati a batteria e con vincoli energetici.</p>
</section>
</section>
<section id="scelta-del-framework-giusto" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="scelta-del-framework-giusto"><span class="header-section-number">6.9</span> Scelta del Framework Giusto</h2>
<p>La scelta del framework di machine learning giusto per una determinata applicazione richiede un’attenta valutazione di modelli, hardware e considerazioni software. Analizzando questi tre aspetti (modelli, hardware e software), gli ingegneri di ML possono selezionare il framework ottimale e personalizzarlo in base alle esigenze per applicazioni ML su dispositivo efficienti e performanti. L’obiettivo è bilanciare complessità del modello, limitazioni hardware e integrazione software per progettare una pipeline ML su misura per dispositivi embedded e edge.</p>
<div id="fig-tf-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - General">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image4.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - General" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.13: Confronto tra Framework TensorFlow - Generale. Fonte: TensorFlow.
</figcaption>
</figure>
</div>
<section id="modello" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="modello"><span class="header-section-number">6.9.1</span> Modello</h3>
<p>TensorFlow supporta molte più operazioni (op) rispetto a TensorFlow Lite e TensorFlow Lite Micro, in quanto viene solitamente utilizzato per la ricerca o l’implementazione cloud, che richiedono un numero elevato di operatori e una maggiore flessibilità (vedere <a href="#fig-tf-comparison" class="quarto-xref">Figura&nbsp;<span>6.13</span></a>). TensorFlow Lite supporta operazioni selezionate per il training sul dispositivo, mentre TensorFlow Micro no. TensorFlow Lite supporta anche forme dinamiche e training consapevole della quantizzazione, mentre TensorFlow Micro no. Al contrario, TensorFlow Lite e TensorFlow Micro offrono strumenti e supporto di quantizzazione nativi, dove la quantizzazione si riferisce alla trasformazione di un programma ML in una rappresentazione approssimata con operazioni di precisione inferiore disponibili.</p>
</section>
<section id="software" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="software"><span class="header-section-number">6.9.2</span> Software</h3>
<div id="fig-tf-sw-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - Model">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-sw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image5.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - Model" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-sw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.14: Confronto tra Framework TensorFlow - Software. Fonte: TensorFlow.
</figcaption>
</figure>
</div>
<p>TensorFlow Lite Micro non supporta il sistema operativo, mentre TensorFlow e TensorFlow Lite sì, per ridurre il sovraccarico di memoria, velocizzare i tempi di avvio e consumare meno energia (vedere <a href="#fig-tf-sw-comparison" class="quarto-xref">Figura&nbsp;<span>6.14</span></a>). TensorFlow Lite Micro può essere utilizzato insieme a sistemi operativi in tempo reale (RTOS) come FreeRTOS, Zephyr e Mbed OS. TensorFlow Lite e TensorFlow Lite Micro supportano la mappatura della memoria del modello, consentendo l’accesso diretto ai modelli dalla memoria flash anziché caricarli nella RAM, cosa che TensorFlow non fa. TensorFlow e TensorFlow Lite supportano la “accelerator delegation” per pianificare il codice su diversi acceleratori, mentre TensorFlow Lite Micro no, poiché i sistemi embedded tendono ad avere una gamma limitata di acceleratori specializzati.</p>
</section>
<section id="hardware" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="hardware"><span class="header-section-number">6.9.3</span> Hardware</h3>
<div id="fig-tf-hw-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - Hardware">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-hw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image3.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - Hardware" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-hw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.15: Confronto tra Framework TensorFlow - Hardware. Fonte: TensorFlow.
</figcaption>
</figure>
</div>
<p>TensorFlow Lite e TensorFlow Lite Micro hanno dimensioni binarie di base e footprint di memoria significativamente più piccoli rispetto a TensorFlow (vedere <a href="#fig-tf-hw-comparison" class="quarto-xref">Figura&nbsp;<span>6.15</span></a>). Ad esempio, un tipico binario TensorFlow Lite Micro è inferiore a 200 KB, mentre TensorFlow è molto più grande. Ciò è dovuto agli ambienti con risorse limitate dei sistemi embedded. TensorFlow supporta x86, TPU e GPU come NVIDIA, AMD e Intel. TensorFlow Lite supporta i processori Arm Cortex-A e x86 comunemente utilizzati su telefoni cellulari e tablet. Quest’ultimo è privo di tutta la logica di training non necessaria per l’implementazione sul dispositivo. TensorFlow Lite Micro fornisce supporto per core Arm Cortex M focalizzati sui microcontrollori come M0, M3, M4 e M7, nonché DSP come Hexagon e SHARC e MCU come STM32, NXP Kinetis, Microchip AVR.</p>
</section>
<section id="altri-fattori" class="level3" data-number="6.9.4">
<h3 data-number="6.9.4" class="anchored" data-anchor-id="altri-fattori"><span class="header-section-number">6.9.4</span> Altri Fattori</h3>
<p>La selezione del framework di IA appropriato è essenziale per garantire che i sistemi embedded possano eseguire in modo efficiente i modelli di IA. Diversi fattori chiave oltre a modelli, hardware e software dovrebbero essere presi in considerazione quando si valutano i framework IA per i sistemi embedded. Altri fattori chiave da considerare quando si sceglie un framework di apprendimento automatico sono prestazioni, scalabilità, facilità d’uso, integrazione con strumenti di ingegneria dei dati, integrazione con strumenti di ottimizzazione dei modelli e supporto della community. Comprendendo questi fattori, si possono prendere decisioni informate e massimizzare il potenziale delle iniziative di machine-learning.</p>
<section id="prestazioni" class="level4">
<h4 class="anchored" data-anchor-id="prestazioni">Prestazioni</h4>
<p>Le prestazioni sono fondamentali nei sistemi embedded in cui le risorse di calcolo sono limitate. Valutare la capacità del framework di ottimizzare l’inferenza del modello per l’hardware embedded. La quantizzazione del modello e il supporto dell’accelerazione hardware sono cruciali per ottenere un’inferenza efficiente.</p>
</section>
<section id="scalabilità" class="level4">
<h4 class="anchored" data-anchor-id="scalabilità">Scalabilità</h4>
<p>La scalabilità è essenziale quando si considera la potenziale crescita di un progetto di IA embedded. Il framework dovrebbe supportare l’implementazione di modelli su vari dispositivi embedded, dai microcontrollori ai processori più potenti. Dovrebbe inoltre gestire senza problemi sia le distribuzioni su piccola che su larga scala.</p>
</section>
<section id="integrazione-con-strumenti-di-data-engineering" class="level4">
<h4 class="anchored" data-anchor-id="integrazione-con-strumenti-di-data-engineering">Integrazione con Strumenti di Data Engineering</h4>
<p>Gli strumenti di ingegneria dei dati sono essenziali per la pre-elaborazione dei dati e la gestione della pipeline. Un framework di intelligenza artificiale ideale per sistemi embedded dovrebbe integrarsi perfettamente con questi strumenti, consentendo un’efficiente acquisizione dei dati, trasformazione e addestramento del modello.</p>
</section>
<section id="integrazione-con-strumenti-di-ottimizzazione-del-modello" class="level4">
<h4 class="anchored" data-anchor-id="integrazione-con-strumenti-di-ottimizzazione-del-modello">Integrazione con Strumenti di Ottimizzazione del Modello</h4>
<p>L’ottimizzazione del modello garantisce che i modelli di intelligenza artificiale siano adatti per la distribuzione embedded. Valutare se il framework si integra con strumenti di ottimizzazione del modello come TensorFlow Lite Converter o ONNX Runtime per facilitare la quantizzazione del modello e la riduzione delle dimensioni.</p>
</section>
<section id="facilità-duso" class="level4">
<h4 class="anchored" data-anchor-id="facilità-duso">Facilità d’Uso</h4>
<p>La facilità d’uso di un framework di IA ha un impatto significativo sull’efficienza dello sviluppo. Un framework con un’interfaccia intuitiva e una documentazione chiara riduce la curva di apprendimento degli sviluppatori. Si dovrebbe considerare se il framework supporta API di alto livello, consentendo agli sviluppatori di concentrarsi sulla progettazione del modello piuttosto che sui dettagli di implementazione di basso livello. Questo fattore è incredibilmente importante per i sistemi embedded, che hanno meno funzionalità di quelle a cui gli sviluppatori tipici potrebbero essere abituati.</p>
</section>
<section id="supporto-della-community" class="level4">
<h4 class="anchored" data-anchor-id="supporto-della-community">Supporto della Community</h4>
<p>Il supporto della community gioca un altro fattore essenziale. I framework con community attive e coinvolte spesso hanno basi di codice ben mantenute, ricevono aggiornamenti regolari e forniscono forum preziosi per la risoluzione dei problemi. Di conseguenza, anche il supporto della community gioca un ruolo nella facilità d’uso perché garantisce che gli sviluppatori abbiano accesso a una vasta gamma di risorse, tra cui tutorial e progetti di esempio. Il supporto della community fornisce una certa garanzia che il framework continuerà a essere supportato per futuri aggiornamenti. Ci sono solo pochi framework che soddisfano le esigenze di TinyML. TensorFlow Lite Micro è il più popolare e ha il maggior supporto della comunità.</p>
</section>
</section>
</section>
<section id="tendenze-future-nei-framework-ml" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="tendenze-future-nei-framework-ml"><span class="header-section-number">6.10</span> Tendenze Future nei Framework ML</h2>
<section id="decomposizione" class="level3" data-number="6.10.1">
<h3 data-number="6.10.1" class="anchored" data-anchor-id="decomposizione"><span class="header-section-number">6.10.1</span> Decomposizione</h3>
<p>Attualmente, lo stack del sistema ML è costituito da quattro astrazioni come mostrato in <a href="#fig-mlsys-stack" class="quarto-xref">Figura&nbsp;<span>6.16</span></a>, vale a dire (1) grafi computazionali, (2) programmi tensoriali, (3) librerie e runtime e (4) primitive hardware.</p>
<div id="fig-mlsys-stack" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="Four Abstractions in Current ML System Stack">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlsys-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image8.png" class="img-fluid figure-img" data-align="center" data-caption="Four Abstractions in Current ML System Stack">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlsys-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.16: Quattro astrazioni negli attuali stack dei sistemi ML. Fonte: <a href="https://tvm.apache.org/2021/12/15/tvm-unity">TVM.</a>
</figcaption>
</figure>
</div>
<p>Ciò ha portato a confini verticali (ad esempio, tra i livelli di astrazione) e orizzontali (ad esempio, approcci basati sulla libreria rispetto a quelli basati sulla compilazione per il calcolo dei tensori), che ostacolano l’innovazione per il ML. Il lavoro futuro nei framework ML può guardare alla rottura di questi confini. A dicembre 2021 è stato proposto <a href="https://tvm.apache.org/2021/12/15/tvm-unity">Apache TVM</a> Unity, che mirava a facilitare le interazioni tra i diversi livelli di astrazione (nonché le persone dietro di essi, come scienziati ML, ingegneri ML e ingegneri hardware) e a co-ottimizzare le decisioni in tutti e quattro i livelli di astrazione.</p>
</section>
<section id="compilatori-e-librerie-ad-alte-prestazioni" class="level3" data-number="6.10.2">
<h3 data-number="6.10.2" class="anchored" data-anchor-id="compilatori-e-librerie-ad-alte-prestazioni"><span class="header-section-number">6.10.2</span> Compilatori e Librerie ad Alte Prestazioni</h3>
<p>Con l’ulteriore sviluppo dei framework ML, continueranno a emergere compilatori e librerie ad alte prestazioni. Alcuni esempi attuali includono <a href="https://www.tensorflow.org/xla/architecture">TensorFlow XLA</a> e <a href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS</a> di Nvidia, che accelerano le operazioni di algebra lineare nei grafi computazionali, e <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> di Nvidia, che accelera e ottimizza l’inferenza.</p>
</section>
<section id="ml-per-framework-ml" class="level3" data-number="6.10.3">
<h3 data-number="6.10.3" class="anchored" data-anchor-id="ml-per-framework-ml"><span class="header-section-number">6.10.3</span> ML per Framework ML</h3>
<p>Possiamo anche usare il ML per migliorare i framework di ML in futuro. Alcuni usi correnti di ML per framework ML includono:</p>
<ul>
<li><p>Ottimizzazione degli iperparametri tramite tecniche quali ottimizzazione bayesiana, ricerca casuale e ricerca a griglia</p></li>
<li><p>Neural Architecture Search (NAS) per cercare automaticamente architetture di rete ottimali</p></li>
<li><p>AutoML, che come descritto in <a href="#sec-ai_frameworks-advanced" class="quarto-xref"><span>Sezione 6.5</span></a>, automatizza la pipeline ML.</p></li>
</ul>
</section>
</section>
<section id="conclusione" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">6.11</span> Conclusione</h2>
<p>In sintesi, la selezione del framework di machine learning ottimale richiede una valutazione approfondita di varie opzioni in base a criteri quali usabilità, supporto della community, prestazioni, compatibilità hardware e capacità di conversione del modello. Non esiste una soluzione adatta a tutti, poiché il framework giusto dipende da vincoli e casi d’uso specifici.</p>
<p>Abbiamo prima introdotto la necessità di framework di apprendimento automatico come TensorFlow e PyTorch. Questi framework offrono funzionalità quali tensori per la gestione di dati multidimensionali, grafi computazionali per la definizione e l’ottimizzazione delle operazioni del modello e una suite di strumenti tra cui funzioni di perdita, ottimizzatori e caricatori di dati che semplificano lo sviluppo del modello.</p>
<p>Le funzionalità avanzate migliorano ulteriormente l’usabilità di questi framework, consentendo attività come la messa a punto di grandi modelli pre-addestrati e la facilitazione del “federated learning”. Queste funzionalità sono fondamentali per sviluppare modelli di apprendimento automatico sofisticati in modo efficiente.</p>
<p>I framework di intelligenza artificiale embedded, come TensorFlow Lite Micro, forniscono strumenti specializzati per la distribuzione di modelli su piattaforme con risorse limitate. TensorFlow Lite Micro, ad esempio, offre strumenti di ottimizzazione completi, tra cui la mappatura della quantizzazione e le ottimizzazioni del kernel, per garantire prestazioni elevate su piattaforme basate su microcontrollori come i processori Arm Cortex-M e RISC-V. I framework creati appositamente per hardware specializzato come CMSIS-NN su processori Cortex-M possono massimizzare ulteriormente le prestazioni ma sacrificare la portabilità. I framework integrati dei fornitori di processori adattano lo stack alle loro architetture, liberando il pieno potenziale dei loro chip ma ci si blocca nel loro ecosistema.</p>
<p>In definitiva, la scelta del framework giusto implica la ricerca della migliore corrispondenza tra le sue capacità e i requisiti della piattaforma target. Ciò richiede un bilanciamento tra esigenze di prestazioni, vincoli hardware, complessità del modello e altri fattori. Una valutazione approfondita dei modelli e dei casi d’uso previsti e la valutazione delle opzioni rispetto alle metriche chiave guideranno gli sviluppatori nella selezione del framework ideale per le loro applicazioni di machine learning.</p>
</section>
<section id="sec-ai-frameworks-resource" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="sec-ai-frameworks-resource"><span class="header-section-number">6.12</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1zbnsihiO68oIUE04TVJEcDQ_Kyec4mhdQkIG6xoR0DY/edit#slide=id.g1ff94734162_0_0">Frameworks overview.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1BK2M2krnI24jSWO0r8tXegl1wgflGZTJyMkjfGolURI/edit#slide=id.g202a6885eb3_0_0">Embedded systems software.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1Jr7HzdZ7YaKO6KY9HBGbOG0BrTnKhbboQtf9d6xy3Ls/edit?usp=drive_link">Inference engines: TF vs.&nbsp;TFLite.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1_DwBbas8wAVWnJ0tbOorqotf9Gns1qNc3JJ6tw8bce0/edit?usp=drive_link">TF flavors: TF vs.&nbsp;TFLite vs.&nbsp;TFLite Micro.</a></p></li>
<li><p>TFLite Micro:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1XdwcZS0pz6kyuk6Vx90kE11hwUMAtS1cMoFQHZAxS20/edit?usp=drive_link">TFLite Micro Big Picture.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/10llaugp6EroGekFzB1pAH1OJ1dpJ4d7yxKglK1BsqlI/edit?usp=drive_link&amp;resourcekey=0-C6_PHSaI6u4x0Mv2KxWKbg">TFLite Micro Interpreter.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/123kdwjRXvbukyaOBvdp0PJpIs2JSxQ7GoDjB8y0FgIE/edit?usp=drive_link">TFLite Micro Model Format.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1_sHuWa3DDTCB9mBzKA4ElPWaUFA8oOelqHCBOHmsvC4/edit?usp=drive_link">TFLite Micro Memory Allocation.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1ZwLOLvYbKodNmyuKKGb_gD83NskrvNmnFC0rvGugJlY/edit?usp=drive_link">TFLite Micro NN Operations.</a></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-tfc" class="quarto-xref">Esercizio&nbsp;<span>6.1</span></a></p></li>
<li><p><a href="#exr-tfl" class="quarto-xref">Esercizio&nbsp;<span>6.2</span></a></p></li>
<li><p><a href="#exr-k" class="quarto-xref">Esercizio&nbsp;<span>6.3</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laboratori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Oltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/data_engineering/data_engineering.it.html" class="pagination-link" aria-label="Data Engineering">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/training/training.it.html" class="pagination-link" aria-label="Addestramento dell'IA">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell’IA</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/frameworks/frameworks.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/frameworks/frameworks.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro è stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>