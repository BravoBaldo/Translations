<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Framework di IA ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/training/training.it.html" rel="next">
<link href="../../contents/data_engineering/data_engineering.it.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/workflow/workflow.it.html">Workflow</a></li><li class="breadcrumb-item"><a href="../../contents/frameworks/frameworks.it.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2d6cdf6fc58f1105ce2a5c5c28a11153" class="alert alert-info hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>üåü Aiutaci a raggiungere 1.000 stelle GitHub! üåü Per ogni 25 stelle, Arduino e SEEED doneranno una NiclaVision o una XIAO ESP32S3 per l‚Äôistruzione sull‚Äôintelligenza artificiale. <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per una ‚≠ê</a></p>
</div></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">PREFAZIONE</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">PARTE PRINCIPALE</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Nozioni Fondamentali</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Workflow</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Argomenti Avanzati</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Impatto Sociale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Chiusura</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tool</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Dataset</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Risorse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Le Community</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Casi di Studio</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">6.1</span> Introduzione</a></li>
  <li><a href="#evoluzione-dei-framework" id="toc-evoluzione-dei-framework" class="nav-link" data-scroll-target="#evoluzione-dei-framework"><span class="header-section-number">6.2</span> Evoluzione dei Framework</a></li>
  <li><a href="#sec-deep_dive_into_tensorflow" id="toc-sec-deep_dive_into_tensorflow" class="nav-link" data-scroll-target="#sec-deep_dive_into_tensorflow"><span class="header-section-number">6.3</span> Approfondimento su TensorFlow</a>
  <ul>
  <li><a href="#ecosistema-tf" id="toc-ecosistema-tf" class="nav-link" data-scroll-target="#ecosistema-tf"><span class="header-section-number">6.3.1</span> Ecosistema TF</a></li>
  <li><a href="#grafico-di-calcolo-statico" id="toc-grafico-di-calcolo-statico" class="nav-link" data-scroll-target="#grafico-di-calcolo-statico"><span class="header-section-number">6.3.2</span> Grafico di Calcolo Statico</a></li>
  <li><a href="#usabilit√†-distribuzione" id="toc-usabilit√†-distribuzione" class="nav-link" data-scroll-target="#usabilit√†-distribuzione"><span class="header-section-number">6.3.3</span> Usabilit√† &amp; Distribuzione</a></li>
  <li><a href="#progettazione-dellarchitettura" id="toc-progettazione-dellarchitettura" class="nav-link" data-scroll-target="#progettazione-dellarchitettura"><span class="header-section-number">6.3.4</span> Progettazione dell‚ÄôArchitettura</a></li>
  <li><a href="#funzionalit√†-native-keras" id="toc-funzionalit√†-native-keras" class="nav-link" data-scroll-target="#funzionalit√†-native-keras"><span class="header-section-number">6.3.5</span> Funzionalit√† Native &amp; Keras</a></li>
  <li><a href="#limitazioni-e-sfide" id="toc-limitazioni-e-sfide" class="nav-link" data-scroll-target="#limitazioni-e-sfide"><span class="header-section-number">6.3.6</span> Limitazioni e Sfide</a></li>
  <li><a href="#sec-pytorch_vs_tensorflow" id="toc-sec-pytorch_vs_tensorflow" class="nav-link" data-scroll-target="#sec-pytorch_vs_tensorflow"><span class="header-section-number">6.3.7</span> PyTorch &amp; TensorFlow</a></li>
  </ul></li>
  <li><a href="#componenti-di-base-del-framework" id="toc-componenti-di-base-del-framework" class="nav-link" data-scroll-target="#componenti-di-base-del-framework"><span class="header-section-number">6.4</span> Componenti di Base del Framework</a>
  <ul>
  <li><a href="#sec-tensor-data-structures" id="toc-sec-tensor-data-structures" class="nav-link" data-scroll-target="#sec-tensor-data-structures"><span class="header-section-number">6.4.1</span> Strutture Dati Tensoriali</a></li>
  <li><a href="#grafi-computazionali" id="toc-grafi-computazionali" class="nav-link" data-scroll-target="#grafi-computazionali"><span class="header-section-number">6.4.2</span> Grafi computazionali</a>
  <ul class="collapse">
  <li><a href="#definizione-di-grafico" id="toc-definizione-di-grafico" class="nav-link" data-scroll-target="#definizione-di-grafico">Definizione di Grafico</a></li>
  <li><a href="#grafi-statici-vs.-dinamici" id="toc-grafi-statici-vs.-dinamici" class="nav-link" data-scroll-target="#grafi-statici-vs.-dinamici">Grafi Statici vs.&nbsp;Dinamici</a></li>
  </ul></li>
  <li><a href="#tool-della-pipeline-dei-dati" id="toc-tool-della-pipeline-dei-dati" class="nav-link" data-scroll-target="#tool-della-pipeline-dei-dati"><span class="header-section-number">6.4.3</span> Tool della Pipeline dei Dati</a>
  <ul class="collapse">
  <li><a href="#loader-dei-dati" id="toc-loader-dei-dati" class="nav-link" data-scroll-target="#loader-dei-dati">Loader dei Dati</a></li>
  </ul></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation"><span class="header-section-number">6.4.4</span> Data Augmentation</a></li>
  <li><a href="#funzioni-loss-e-algoritmi-di-ottimizzazione" id="toc-funzioni-loss-e-algoritmi-di-ottimizzazione" class="nav-link" data-scroll-target="#funzioni-loss-e-algoritmi-di-ottimizzazione"><span class="header-section-number">6.4.5</span> Funzioni Loss e Algoritmi di Ottimizzazione</a></li>
  <li><a href="#supporto-al-training-del-modello" id="toc-supporto-al-training-del-modello" class="nav-link" data-scroll-target="#supporto-al-training-del-modello"><span class="header-section-number">6.4.6</span> Supporto al Training del Modello</a></li>
  <li><a href="#validazione-e-analisi" id="toc-validazione-e-analisi" class="nav-link" data-scroll-target="#validazione-e-analisi"><span class="header-section-number">6.4.7</span> Validazione e Analisi</a>
  <ul class="collapse">
  <li><a href="#metriche-di-valutazione" id="toc-metriche-di-valutazione" class="nav-link" data-scroll-target="#metriche-di-valutazione">Metriche di Valutazione</a></li>
  <li><a href="#visualizzazione" id="toc-visualizzazione" class="nav-link" data-scroll-target="#visualizzazione">Visualizzazione</a></li>
  </ul></li>
  <li><a href="#programmazione-differenziabile" id="toc-programmazione-differenziabile" class="nav-link" data-scroll-target="#programmazione-differenziabile"><span class="header-section-number">6.4.8</span> Programmazione differenziabile</a></li>
  <li><a href="#accelerazione-hardware" id="toc-accelerazione-hardware" class="nav-link" data-scroll-target="#accelerazione-hardware"><span class="header-section-number">6.4.9</span> Accelerazione Hardware</a></li>
  </ul></li>
  <li><a href="#sec-ai_frameworks-advanced" id="toc-sec-ai_frameworks-advanced" class="nav-link" data-scroll-target="#sec-ai_frameworks-advanced"><span class="header-section-number">6.5</span> Funzionalit√† Avanzate</a>
  <ul>
  <li><a href="#training-distribuito" id="toc-training-distribuito" class="nav-link" data-scroll-target="#training-distribuito"><span class="header-section-number">6.5.1</span> Training distribuito</a></li>
  <li><a href="#conversione-del-modello" id="toc-conversione-del-modello" class="nav-link" data-scroll-target="#conversione-del-modello"><span class="header-section-number">6.5.2</span> Conversione del Modello</a></li>
  <li><a href="#automl-no-codelow-code-ml" id="toc-automl-no-codelow-code-ml" class="nav-link" data-scroll-target="#automl-no-codelow-code-ml"><span class="header-section-number">6.5.3</span> AutoML, No-Code/Low-Code ML</a></li>
  <li><a href="#metodi-di-apprendimento-avanzati" id="toc-metodi-di-apprendimento-avanzati" class="nav-link" data-scroll-target="#metodi-di-apprendimento-avanzati"><span class="header-section-number">6.5.4</span> Metodi di Apprendimento Avanzati</a>
  <ul class="collapse">
  <li><a href="#il-transfer-learning" id="toc-il-transfer-learning" class="nav-link" data-scroll-target="#il-transfer-learning">Il Transfer Learning</a></li>
  <li><a href="#il-federated-learning" id="toc-il-federated-learning" class="nav-link" data-scroll-target="#il-federated-learning">Il Federated Learning</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#specializzazione-del-framework" id="toc-specializzazione-del-framework" class="nav-link" data-scroll-target="#specializzazione-del-framework"><span class="header-section-number">6.6</span> Specializzazione del Framework</a>
  <ul>
  <li><a href="#cloud" id="toc-cloud" class="nav-link" data-scroll-target="#cloud"><span class="header-section-number">6.6.1</span> Cloud</a></li>
  <li><a href="#edge" id="toc-edge" class="nav-link" data-scroll-target="#edge"><span class="header-section-number">6.6.2</span> Edge</a></li>
  <li><a href="#embedded" id="toc-embedded" class="nav-link" data-scroll-target="#embedded"><span class="header-section-number">6.6.3</span> Embedded</a></li>
  </ul></li>
  <li><a href="#sec-ai_frameworks_embedded" id="toc-sec-ai_frameworks_embedded" class="nav-link" data-scroll-target="#sec-ai_frameworks_embedded"><span class="header-section-number">6.7</span> Framework di IA Embedded</a>
  <ul>
  <li><a href="#vincoli-di-risorse" id="toc-vincoli-di-risorse" class="nav-link" data-scroll-target="#vincoli-di-risorse"><span class="header-section-number">6.7.1</span> Vincoli di Risorse</a></li>
  <li><a href="#framework-e-librerie" id="toc-framework-e-librerie" class="nav-link" data-scroll-target="#framework-e-librerie"><span class="header-section-number">6.7.2</span> Framework e Librerie</a></li>
  <li><a href="#sfide" id="toc-sfide" class="nav-link" data-scroll-target="#sfide"><span class="header-section-number">6.7.3</span> Sfide</a>
  <ul class="collapse">
  <li><a href="#ecosistema-frammentato" id="toc-ecosistema-frammentato" class="nav-link" data-scroll-target="#ecosistema-frammentato">Ecosistema Frammentato</a></li>
  <li><a href="#esigenze-hardware-disparate" id="toc-esigenze-hardware-disparate" class="nav-link" data-scroll-target="#esigenze-hardware-disparate">Esigenze Hardware Disparate</a></li>
  <li><a href="#mancanza-di-portabilit√†" id="toc-mancanza-di-portabilit√†" class="nav-link" data-scroll-target="#mancanza-di-portabilit√†">Mancanza di Portabilit√†</a></li>
  <li><a href="#infrastruttura-incompleta" id="toc-infrastruttura-incompleta" class="nav-link" data-scroll-target="#infrastruttura-incompleta">Infrastruttura Incompleta</a></li>
  <li><a href="#nessun-benchmark-standard" id="toc-nessun-benchmark-standard" class="nav-link" data-scroll-target="#nessun-benchmark-standard">Nessun Benchmark Standard</a></li>
  <li><a href="#test-minimi-del-mondo-reale" id="toc-test-minimi-del-mondo-reale" class="nav-link" data-scroll-target="#test-minimi-del-mondo-reale">Test Minimi del Mondo Reale</a></li>
  <li><a href="#riepilogo" id="toc-riepilogo" class="nav-link" data-scroll-target="#riepilogo">Riepilogo</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#esempi" id="toc-esempi" class="nav-link" data-scroll-target="#esempi"><span class="header-section-number">6.8</span> Esempi</a>
  <ul>
  <li><a href="#interprete" id="toc-interprete" class="nav-link" data-scroll-target="#interprete"><span class="header-section-number">6.8.1</span> Interprete</a></li>
  <li><a href="#basati-su-compilatore" id="toc-basati-su-compilatore" class="nav-link" data-scroll-target="#basati-su-compilatore"><span class="header-section-number">6.8.2</span> Basati su Compilatore</a></li>
  <li><a href="#libreria" id="toc-libreria" class="nav-link" data-scroll-target="#libreria"><span class="header-section-number">6.8.3</span> Libreria</a></li>
  </ul></li>
  <li><a href="#scelta-del-framework-giusto" id="toc-scelta-del-framework-giusto" class="nav-link" data-scroll-target="#scelta-del-framework-giusto"><span class="header-section-number">6.9</span> Scelta del Framework Giusto</a>
  <ul>
  <li><a href="#modello" id="toc-modello" class="nav-link" data-scroll-target="#modello"><span class="header-section-number">6.9.1</span> Modello</a></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software"><span class="header-section-number">6.9.2</span> Software</a></li>
  <li><a href="#hardware" id="toc-hardware" class="nav-link" data-scroll-target="#hardware"><span class="header-section-number">6.9.3</span> Hardware</a></li>
  <li><a href="#altri-fattori" id="toc-altri-fattori" class="nav-link" data-scroll-target="#altri-fattori"><span class="header-section-number">6.9.4</span> Altri Fattori</a>
  <ul class="collapse">
  <li><a href="#prestazioni" id="toc-prestazioni" class="nav-link" data-scroll-target="#prestazioni">Prestazioni</a></li>
  <li><a href="#scalabilit√†" id="toc-scalabilit√†" class="nav-link" data-scroll-target="#scalabilit√†">Scalabilit√†</a></li>
  <li><a href="#integrazione-con-strumenti-di-data-engineering" id="toc-integrazione-con-strumenti-di-data-engineering" class="nav-link" data-scroll-target="#integrazione-con-strumenti-di-data-engineering">Integrazione con Strumenti di Data Engineering</a></li>
  <li><a href="#integrazione-con-strumenti-di-ottimizzazione-del-modello" id="toc-integrazione-con-strumenti-di-ottimizzazione-del-modello" class="nav-link" data-scroll-target="#integrazione-con-strumenti-di-ottimizzazione-del-modello">Integrazione con Strumenti di Ottimizzazione del Modello</a></li>
  <li><a href="#facilit√†-duso" id="toc-facilit√†-duso" class="nav-link" data-scroll-target="#facilit√†-duso">Facilit√† d‚ÄôUso</a></li>
  <li><a href="#supporto-della-community" id="toc-supporto-della-community" class="nav-link" data-scroll-target="#supporto-della-community">Supporto della Community</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#tendenze-future-nei-framework-ml" id="toc-tendenze-future-nei-framework-ml" class="nav-link" data-scroll-target="#tendenze-future-nei-framework-ml"><span class="header-section-number">6.10</span> Tendenze Future nei Framework ML</a>
  <ul>
  <li><a href="#decomposizione" id="toc-decomposizione" class="nav-link" data-scroll-target="#decomposizione"><span class="header-section-number">6.10.1</span> Decomposizione</a></li>
  <li><a href="#compilatori-e-librerie-ad-alte-prestazioni" id="toc-compilatori-e-librerie-ad-alte-prestazioni" class="nav-link" data-scroll-target="#compilatori-e-librerie-ad-alte-prestazioni"><span class="header-section-number">6.10.2</span> Compilatori e Librerie ad Alte Prestazioni</a></li>
  <li><a href="#ml-per-framework-ml" id="toc-ml-per-framework-ml" class="nav-link" data-scroll-target="#ml-per-framework-ml"><span class="header-section-number">6.10.3</span> ML per Framework ML</a></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">6.11</span> Conclusione</a></li>
  <li><a href="#sec-ai-frameworks-resource" id="toc-sec-ai-frameworks-resource" class="nav-link" data-scroll-target="#sec-ai-frameworks-resource"><span class="header-section-number">6.12</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/frameworks/frameworks.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/frameworks/frameworks.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/workflow/workflow.it.html">Workflow</a></li><li class="breadcrumb-item"><a href="../../contents/frameworks/frameworks.it.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ai_frameworks" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-ai-frameworks-resource">Slide</a>, <a href="#sec-ai-frameworks-resource">Video</a>, <a href="#sec-ai-frameworks-resource">Esercizi</a>, <a href="#sec-ai-frameworks-resource">Laboratori</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ml_frameworks.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Illustrazione in formato rettangolare, progettata per un libro di testo professionale, in cui il contenuto si estende per l‚Äôintera larghezza. Il grafico vivace rappresenta i framework di training e inferenza per l‚ÄôML. Le icone per TensorFlow, Keras, PyTorch, ONNX e TensorRT sono distribuite, riempiono l‚Äôintero spazio orizzontale e sono allineate verticalmente. Ogni icona √® accompagnata da brevi annotazioni che ne descrivono le caratteristiche. I colori vividi come blu, verde e arancione evidenziano le icone e le sezioni su uno sfondo sfumato pi√π morbido. La distinzione tra framework di training e inferenza √® accentuata tramite sezioni codificate a colori, con linee pulite e tipografia moderna che mantengono chiarezza e attenzione.</em></figcaption>
</figure>
</div>
<p>Questo capitolo esplora il panorama dei framework di intelligenza artificiale che fungono da base per lo sviluppo di sistemi di apprendimento automatico. I framework di intelligenza artificiale forniscono gli strumenti, le librerie e gli ambienti per progettare, addestrare e distribuire modelli di apprendimento automatico. Esploreremo il percorso evolutivo di questi framework, analizzeremo il funzionamento di TensorFlow e forniremo approfondimenti sui componenti principali e sulle funzionalit√† avanzate che li definiscono.</p>
<p>Inoltre, esaminiamo la specializzazione dei framework su misura per esigenze specifiche, l‚Äôemergere di framework progettati espressamente per l‚Äôintelligenza artificiale embedded e i criteri per selezionare il framework pi√π adatto ai vari progetti. Questa esplorazione sar√† completata da uno sguardo alle tendenze future che dovrebbero modellare il panorama dei framework di apprendimento automatico nei prossimi anni.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere l‚Äôevoluzione e le capacit√† dei principali framework di apprendimento automatico. Ci√≤ include modelli di esecuzione di grafici, paradigmi di programmazione, supporto per l‚Äôaccelerazione hardware e come si sono espansi nel tempo.</p></li>
<li><p>Scoprire i componenti principali e le funzionalit√† dei framework, come grafi computazionali, pipeline di dati, algoritmi di ottimizzazione, loop di training, ecc., che consentono una creazione di modelli efficiente.</p></li>
<li><p>Confrontare i framework in diversi ambienti, come cloud, edge e TinyML. Scoprire come i framework si specializzano in base a vincoli computazionali e hardware.</p></li>
<li><p>Approfondire i framework embedded e focalizzarsi su TinyML come TensorFlow Lite Micro, CMSIS-NN, TinyEngine, ecc., e come ottimizzano per i microcontrollori.</p></li>
<li><p>Quando si sceglie un framework, si esplorano le considerazioni sulla conversione e l‚Äôimplementazione del modello, tra cui latenza, utilizzo della memoria e supporto hardware.</p></li>
<li><p>Valutare i fattori chiave nella selezione del framework giusto, come prestazioni, compatibilit√† hardware, supporto della community, facilit√† d‚Äôuso, ecc., in base alle esigenze e ai vincoli specifici del progetto.</p></li>
<li><p>Comprendere i limiti dei framework attuali e le potenziali tendenze future, come l‚Äôuso del ML per migliorare i framework, i sistemi ML decomposti e i compilatori ad alte prestazioni.</p></li>
</ul>
</div>
</div>
<section id="introduzione" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduzione"><span class="header-section-number">6.1</span> Introduzione</h2>
<p>I framework di machine learning [apprendimento automatico] forniscono gli strumenti e l‚Äôinfrastruttura per creare, addestrare e distribuire in modo efficiente modelli di apprendimento automatico. In questo capitolo esploreremo l‚Äôevoluzione e le capacit√† chiave dei principali framework come <a href="https://www.tensorflow.org/">TensorFlow (TF)</a>, <a href="https://pytorch.org/">PyTorch</a> e framework specializzati per dispositivi embedded. Ci immergeremo nei componenti come grafi computazionali, algoritmi di ottimizzazione, accelerazione hardware e altro che consentono agli sviluppatori di creare rapidamente modelli performanti. Comprendere questi framework √® essenziale per sfruttare la potenza del deep learning in tutto lo spettro, dal cloud ai dispositivi edge [periferici].</p>
<p>I framework di apprendimento automatico gestiscono gran parte della complessit√† dello sviluppo di modelli tramite API di alto livello e linguaggi specifici per dominio che consentono ai professionisti di creare rapidamente modelli combinando componenti e astrazioni predefiniti. Ad esempio, framework come TensorFlow e PyTorch forniscono API Python per definire architetture di reti neurali utilizzando livelli, ottimizzatori, set di dati e altro. Ci√≤ consente un‚Äôiterazione rapida rispetto alla codifica di ogni dettaglio del modello partendo da zero.</p>
<p>Una capacit√† chiave offerta da questi framework √® rappresentata dai motori di training distribuiti che possono scalare l‚Äôaddestramento del modello su cluster di GPU e TPU. Ci√≤ rende possibile il training di modelli all‚Äôavanguardia con miliardi o trilioni di parametri su vasti set di dati. I framework si integrano anche con hardware specializzato come le GPU NVIDIA per accelerare ulteriormente il training tramite ottimizzazioni come la parallelizzazione ed efficienti operazioni matriciali.</p>
<p>Inoltre, i framework semplificano il deploy [distribuzione] di modelli finiti in produzione tramite strumenti come <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> per il model serving scalabile e <a href="https://www.tensorflow.org/lite">TensorFlow Lite</a> per l‚Äôottimizzazione su dispositivi mobili ed edge. Altre capacit√† preziose includono visualizzazione, tecniche di ottimizzazione del modello come quantizzazione e potatura e monitoraggio delle metriche durante il training.</p>
<p>I principali framework open source come TensorFlow, PyTorch e <a href="https://mxnet.apache.org/versions/1.9.1/">MXNet</a> alimentano gran parte della ricerca e dello sviluppo dell‚ÄôIA oggi. Offerte commerciali come <a href="https://aws.amazon.com/pm/sagemaker/">Amazon SageMaker</a> e <a href="https://azure.microsoft.com/en-us/free/machine-learning/search/?ef_id=_k_CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE_k_&amp;OCID=AIDcmm5edswduu_SEM__k_CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE_k_&amp;gad=1&amp;gclid=CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE">Microsoft Azure Machine Learning</a> integrano questi framework open source con funzionalit√† proprietarie e strumenti aziendali.</p>
<p>Gli ingegneri e i professionisti del machine learning sfruttano questi framework robusti per concentrarsi su attivit√† di alto valore come architettura del modello, progettazione delle feature e ottimizzazione degli iperparametri anzich√© sull‚Äôinfrastruttura. L‚Äôobiettivo √® creare e distribuire modelli performanti che risolvano in modo efficiente i problemi del mondo reale.</p>
<p>In questo capitolo, esploreremo i principali framework cloud odierni e il modo in cui hanno adattato modelli e strumenti specificamente per la distribuzione embedded ed edge. Confronteremo modelli di programmazione, hardware supportato, capacit√† di ottimizzazione e altro ancora per comprendere appieno in che modo i framework consentono un apprendimento automatico scalabile dal cloud all‚Äôedge.</p>
</section>
<section id="evoluzione-dei-framework" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="evoluzione-dei-framework"><span class="header-section-number">6.2</span> Evoluzione dei Framework</h2>
<p>I framework di apprendimento automatico si sono evoluti in modo significativo per soddisfare le diverse esigenze dei professionisti del machine learning e i progressi nelle tecniche di intelligenza artificiale. Qualche decennio fa, la creazione e l‚Äôaddestramento di modelli di apprendimento automatico richiedevano un‚Äôampia codifica e infrastruttura di basso livello. Oltre alla necessit√† di una codifica di basso livello, la prima ricerca sulle reti neurali era limitata da dati e potenza di calcolo insufficienti. Tuttavia, i framework di apprendimento automatico si sono evoluti notevolmente nell‚Äôultimo decennio per soddisfare le crescenti esigenze dei professionisti e i rapidi progressi nelle tecniche di deep learning [apprendimento profondo]. Il rilascio di grandi set di dati come <a href="https://www.image-net.org/">ImageNet</a> <span class="citation" data-cites="deng2009imagenet">(<a href="../../references.it.html#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>)</span> e i progressi nel calcolo parallelo con GPU hanno sbloccato il potenziale per reti neurali molto pi√π profonde.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, e Fei-Fei Li. 2009. <span>¬´<span>ImageNet:</span> <span>A</span> large-scale hierarchical image database¬ª</span>. In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248‚Äì55. IEEE. <a href="https://doi.org/10.1109/cvpr.2009.5206848">https://doi.org/10.1109/cvpr.2009.5206848</a>.
</div><div id="ref-al2016theano" class="csl-entry" role="listitem">
Team, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. <span>¬´Theano: <span>A</span> Python framework for fast computation of mathematical expressions¬ª</span>. <a href="https://arxiv.org/abs/1605.02688">https://arxiv.org/abs/1605.02688</a>.
</div><div id="ref-jia2014caffe" class="csl-entry" role="listitem">
Jia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, e Trevor Darrell. 2014. <span>¬´Caffe: Convolutional Architecture for Fast Feature Embedding¬ª</span>. In <em>Proceedings of the 22nd ACM international conference on Multimedia</em>, 675‚Äì78. ACM. <a href="https://doi.org/10.1145/2647868.2654889">https://doi.org/10.1145/2647868.2654889</a>.
</div><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. <span>¬´<span>ImageNet</span> Classification with Deep Convolutional Neural Networks¬ª</span>. In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L√©on Bottou, e Kilian Q. Weinberger, 1106‚Äì14. <a href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>.
</div><div id="ref-chollet2018keras" class="csl-entry" role="listitem">
Chollet, Fran√ßois. 2018. <span>¬´Introduction to keras¬ª</span>. <em>March 9th</em>.
</div><div id="ref-tokui2015chainer" class="csl-entry" role="listitem">
Tokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, e Hiroyuki Yamazaki Vincent. 2019. <span>¬´Chainer: A Deep Learning Framework for Accelerating the Research Cycle¬ª</span>. In <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining</em>, 5:1‚Äì6. ACM. <a href="https://doi.org/10.1145/3292500.3330756">https://doi.org/10.1145/3292500.3330756</a>.
</div><div id="ref-seide2016cntk" class="csl-entry" role="listitem">
Seide, Frank, e Amit Agarwal. 2016. <span>¬´Cntk: Microsoft‚Äôs Open-Source Deep-Learning Toolkit¬ª</span>. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 2135‚Äì35. ACM. <a href="https://doi.org/10.1145/2939672.2945397">https://doi.org/10.1145/2939672.2945397</a>.
</div><div id="ref-paszke2019pytorch" class="csl-entry" role="listitem">
Ansel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. <span>¬´<span>PyTorch</span> 2: <span>Faster</span> Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation¬ª</span>. In <em>Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, e Roman Garnett, 8024‚Äì35. ACM. <a href="https://doi.org/10.1145/3620665.3640366">https://doi.org/10.1145/3620665.3640366</a>.
</div></div><p>I primi framework di apprendimento automatico, <a href="https://pypi.org/project/Theano/#:~:text=Theano">Theano</a> di <span class="citation" data-cites="al2016theano">Team et al. (<a href="../../references.it.html#ref-al2016theano" role="doc-biblioref">2016</a>)</span> e <a href="https://caffe.berkeleyvision.org/">Caffe</a> di <span class="citation" data-cites="jia2014caffe">Jia et al. (<a href="../../references.it.html#ref-jia2014caffe" role="doc-biblioref">2014</a>)</span>, sono stati sviluppati da istituzioni accademiche. Theano √® stato creato dal Montreal Institute for Learning Algorithms, mentre Caffe √® stato sviluppato dal Berkeley Vision and Learning Center. Nel crescente interesse per il deep learning dovuto alle prestazioni all‚Äôavanguardia di AlexNet <span class="citation" data-cites="krizhevsky2012imagenet">Krizhevsky, Sutskever, e Hinton (<a href="../../references.it.html#ref-krizhevsky2012imagenet" role="doc-biblioref">2012</a>)</span> sul dataset ImageNet, aziende private e singole persone hanno iniziato a sviluppare framework di ML, dando vita a <a href="https://keras.io/">Keras</a> di <span class="citation" data-cites="chollet2018keras">Chollet (<a href="../../references.it.html#ref-chollet2018keras" role="doc-biblioref">2018</a>)</span>, <a href="https://chainer.org/">Chainer</a> di <span class="citation" data-cites="tokui2015chainer">Tokui et al. (<a href="../../references.it.html#ref-tokui2015chainer" role="doc-biblioref">2019</a>)</span>, TensorFlow di Google <span class="citation" data-cites="abadi2016tensorflow">(<a href="../../references.it.html#ref-abadi2016tensorflow" role="doc-biblioref">Yu et al. 2018</a>)</span>, <a href="https://learn.microsoft.com/en-us/cognitive-toolkit/">CNTK</a> di Microsoft <span class="citation" data-cites="seide2016cntk">(<a href="../../references.it.html#ref-seide2016cntk" role="doc-biblioref">Seide e Agarwal 2016</a>)</span> e PyTorch di Facebook <span class="citation" data-cites="paszke2019pytorch">(<a href="../../references.it.html#ref-paszke2019pytorch" role="doc-biblioref">Ansel et al. 2024</a>)</span>.</p>
<p>Molti di questi framework ML possono essere suddivisi in framework di alto livello, di basso livello e di grafi computazionali statici e dinamici. I framework di alto livello forniscono un livello di astrazione pi√π elevato rispetto a quelli di basso livello. I framework di alto livello hanno funzioni e moduli predefiniti per attivit√† ML comuni, come la creazione, l‚Äôaddestramento e la valutazione di modelli ML comuni, la preelaborazione dei dati, le funzionalit√† di progettazione e la visualizzazione dei dati, che i framework di basso livello non hanno. Pertanto, i framework di alto livello possono risultare pi√π facili da usare ma sono meno personalizzabili rispetto a quelli di basso livello (ad esempio, gli utenti di framework di basso livello possono definire livelli personalizzati, funzioni ‚Äúloss‚Äù [di perdita], algoritmi di ottimizzazione, ecc.). Esempi di framework di alto livello sono TensorFlow/Keras e PyTorch. Esempi di framework ML di basso livello includono TensorFlow con API di basso livello, Theano, Caffe, Chainer e CNTK.</p>
<p>Framework come Theano e Caffe utilizzavano grafi computazionali statici, che richiedevano la definizione anticipata dell‚Äôarchitettura completa del modello, limitandone cos√¨ la flessibilit√†. Al contrario, i grafici dinamici vengono costruiti al volo per uno sviluppo pi√π iterativo. Intorno al 2016, framework come PyTorch e TensorFlow 2.0 hanno iniziato ad adottare grafici dinamici, offrendo maggiore flessibilit√† per lo sviluppo del modello. Discuteremo di questi concetti e dettagli pi√π avanti nella sezione Training dell‚ÄôIA.</p>
<p>Lo sviluppo di questi framework ha suscitato un‚Äôesplosione di dimensioni e complessit√† del modello nel tempo, dai primi perceptron multistrato e reti convoluzionali ai moderni trasformatori con miliardi o trilioni di parametri. Nel 2016, i modelli ResNet di <span class="citation" data-cites="he2016deep">He et al. (<a href="../../references.it.html#ref-he2016deep" role="doc-biblioref">2016</a>)</span> hanno raggiunto un‚Äôaccuratezza ImageNet record con oltre 150 livelli e 25 milioni di parametri. Poi, nel 2020, il modello linguistico GPT-3 di OpenAI <span class="citation" data-cites="brown2020language">(<a href="../../references.it.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> ha spinto i parametri a un sorprendente numero di 175 miliardi utilizzando il parallelismo del modello nei framework per addestrare migliaia di GPU e TPU.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. <span>¬´Deep Residual Learning for Image Recognition¬ª</span>. In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770‚Äì78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>¬´Language Models are Few-Shot Learners¬ª</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div></div><p>Ogni generazione di framework ha sbloccato nuove capacit√† che hanno alimentato il progresso:</p>
<ul>
<li><p>Theano e TensorFlow (2015) hanno introdotto grafi computazionali e differenziazione automatica per semplificare la creazione di modelli.</p></li>
<li><p>CNTK (2016) ha aperto la strada a un addestramento distribuito efficiente combinando parallelismo di modelli e dati.</p></li>
<li><p>PyTorch (2016) ha fornito programmazione imperativa e grafici dinamici per una sperimentazione flessibile.</p></li>
<li><p>TensorFlow 2.0 (2019) ha impostato di default l‚Äôesecuzione Eager per intuitivit√† e debug.</p></li>
<li><p>TensorFlow Graphics (2020) ha aggiunto strutture dati 3D per gestire nuvole di punti e mesh.</p></li>
</ul>
<p>Negli ultimi anni, i framework sono convergenti. <a href="#fig-ml-framework" class="quarto-xref">Figura&nbsp;<span>6.2</span></a> mostra che TensorFlow e PyTorch sono diventati i framework ML pi√π dominanti, rappresentando oltre il 95% dei framework ML utilizzati nella ricerca e nella produzione. <a href="#fig-tensorflow-pytorch" class="quarto-xref">Figura&nbsp;<span>6.1</span></a> traccia un contrasto tra gli attributi di TensorFlow e PyTorch. Keras √® stato integrato in TensorFlow nel 2019; Preferred Networks ha trasferito Chainer a PyTorch nel 2019; e Microsoft ha smesso di sviluppare attivamente CNTK nel 2022 per supportare PyTorch su Windows.</p>
<div id="fig-tensorflow-pytorch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensorflow-pytorch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tensorflowpytorch.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensorflow-pytorch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.1: PyTorch e TensorFlow: Caratteristiche e Funzioni. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fkruschecompany.com%2Fpytorch-vs-tensorflow%2F&amp;psig=AOvVaw1-DSFxXYprQmYH7Z4Nk6Tk&amp;ust=1722533288351000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCPDhst7m0YcDFQAAAAAdAAAAABAg">K&amp;C</a>
</figcaption>
</figure>
</div>
<div id="fig-ml-framework" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image6.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.2: Popolarit√† dei framework ML negli Stati Uniti misurata dalle ricerche web di Google. Fonte: Google.
</figcaption>
</figure>
</div>
<p>Un approccio unico non funziona bene in tutto lo spettro, dal cloud ai piccoli dispositivi edge. Diversi framework rappresentano varie filosofie sull‚Äôesecuzione di grafici, API dichiarative rispetto a quelle imperative e altro ancora. Le dichiarative definiscono cosa dovrebbe fare il programma, mentre le imperative si concentrano su come dovrebbe essere fatto passo dopo passo. Ad esempio, TensorFlow utilizza l‚Äôesecuzione di grafici e la modellazione in stile dichiarativo, mentre PyTorch adotta l‚Äôesecuzione rapida e la modellazione imperativa per una maggiore flessibilit√† con Python. Ogni approccio comporta dei compromessi che discuteremo in <a href="#sec-pytorch_vs_tensorflow" class="quarto-xref"><span>Sezione 6.3.7</span></a>.</p>
<p>Gli attuali framework avanzati consentono ai professionisti di sviluppare e distribuire modelli sempre pi√π complessi, un fattore chiave dell‚Äôinnovazione nel campo dell‚Äôintelligenza artificiale. Questi framework continuano a evolversi ed espandere le loro capacit√† per la prossima generazione di machine learning. Per capire come questi sistemi continuano a evolversi, approfondiremo TensorFlow come esempio di come il framework sia cresciuto in complessit√† nel tempo.</p>
</section>
<section id="sec-deep_dive_into_tensorflow" class="level2 page-columns page-full" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec-deep_dive_into_tensorflow"><span class="header-section-number">6.3</span> Approfondimento su TensorFlow</h2>
<p>TensorFlow √® stato sviluppato dal team di Google Brain ed √® stato rilasciato come libreria software open source il 9 novembre 2015. √à stato progettato per il calcolo numerico utilizzando grafici di flusso di dati e da allora √® diventato popolare per un‚Äôampia gamma di applicazioni di apprendimento automatico e deep learning.</p>
<p>TensorFlow √® un framework di training e inferenza che fornisce funzionalit√† integrate per gestire tutto, dalla creazione e training del modello alla distribuzione, come mostrato in <a href="#fig-tensorflow-architecture" class="quarto-xref">Figura&nbsp;<span>6.3</span></a>. Sin dal suo sviluppo iniziale, l‚Äôecosistema TensorFlow √® cresciuto fino a includere molte diverse ‚Äúvariet√†‚Äù di TensorFlow, ciascuna pensata per consentire agli utenti di supportare ML su diverse piattaforme. In questa sezione, discuteremo principalmente solo del pacchetto core.</p>
<section id="ecosistema-tf" class="level3 page-columns page-full" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="ecosistema-tf"><span class="header-section-number">6.3.1</span> Ecosistema TF</h3>
<ol type="1">
<li><p><a href="https://www.tensorflow.org/tutorials">TensorFlow Core</a>: pacchetto principale con cui interagiscono la maggior parte degli sviluppatori. Fornisce una piattaforma completa e flessibile per definire, addestrare e distribuire modelli di apprendimento automatico. Include <a href="https://www.tensorflow.org/guide/keras">tf.keras</a> come API di alto livello.</p></li>
<li><p><a href="https://www.tensorflow.org/lite">TensorFlow Lite</a>: progettato per distribuire modelli leggeri su dispositivi mobili, embedded ed edge. Offre strumenti per convertire i modelli TensorFlow in un formato pi√π compatto adatto a dispositivi con risorse limitate e fornisce modelli pre-addestrati ottimizzati per dispositivi mobili.</p></li>
<li><p><a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite Micro</a>: progettato per eseguire modelli di apprendimento automatico su microcontrollori con risorse minime. Funziona senza la necessit√† di supporto del sistema operativo, librerie C o C++ standard o allocazione dinamica della memoria, utilizzando solo pochi kilobyte di memoria.</p></li>
<li><p><a href="https://www.tensorflow.org/js">TensorFlow.js</a>: libreria JavaScript che consente l‚Äôaddestramento e la distribuzione di modelli di apprendimento automatico direttamente nel browser o su Node.js. Fornisce inoltre strumenti per il porting di modelli TensorFlow pre-addestrati nel formato browser-friendly.</p></li>
<li><p><a href="https://developers.googleblog.com/2019/03/introducing-coral-our-platform-for.html">TensorFlow su dispositivi edge (Coral)</a>: piattaforma di componenti hardware e strumenti software di Google che consente l‚Äôesecuzione di modelli TensorFlow su dispositivi edge, sfruttando Edge TPU per l‚Äôaccelerazione.</p></li>
<li><p><a href="https://www.tensorflow.org/federated">TensorFlow Federated (TFF)</a>: framework per l‚Äôapprendimento automatico e altri calcoli su dati decentralizzati. TFF facilita l‚Äôapprendimento ‚Äúfederato‚Äù, consentendo l‚Äôaddestramento del modello su molti dispositivi senza centralizzare i dati.</p></li>
<li><p><a href="https://www.tensorflow.org/graphics">TensorFlow Graphics</a>: libreria per l‚Äôutilizzo di TensorFlow per svolgere attivit√† correlate alla grafica, tra cui l‚Äôelaborazione di forme 3D e nuvole di punti, utilizzando il deep learning.</p></li>
<li><p><a href="https://www.tensorflow.org/hub">TensorFlow Hub</a>: repository di componenti di modelli di apprendimento automatico riutilizzabili che consente agli sviluppatori di riutilizzare componenti di modelli pre-addestrati, facilitando l‚Äôapprendimento per trasferimento e la composizione del modello.</p></li>
<li><p><a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a>: framework progettato per servire e distribuire modelli di apprendimento automatico per l‚Äôinferenza in ambienti di produzione. Fornisce strumenti per il versioning e l‚Äôaggiornamento dinamico dei modelli distribuiti senza interruzione del servizio.</p></li>
<li><p><a href="https://www.tensorflow.org/tfx">TensorFlow Extended (TFX)</a>: piattaforma end-to-end progettata per distribuire e gestire pipeline di apprendimento automatico in ambienti di produzione. TFX comprende validazione dei dati, pre-elaborazione, addestramento del modello, convalida e componenti di servizio.</p></li>
</ol>
<div id="fig-tensorflow-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensorflow-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tensorflow.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensorflow-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.3: Panoramica dell‚Äôarchitettura di TensorFlow 2.0. Fonte: <a href="https://blog.tensorflow.org/2019/01/whats-coming-in-tensorflow-2-0.html">Tensorflow.</a>
</figcaption>
</figure>
</div>
<p>TensorFlow √® stato sviluppato per affrontare le limitazioni di DistBelief <span class="citation" data-cites="abadi2016tensorflow">(<a href="../../references.it.html#ref-abadi2016tensorflow" role="doc-biblioref">Yu et al. 2018</a>)</span>‚Äîil framework in uso presso Google dal 2011 al 2015‚Äîoffrendo flessibilit√† lungo tre direttrici: 1) definizione di nuovi livelli [livelli], 2) perfezionamento degli algoritmi di training e 3) definizione di nuovi algoritmi di training. Per comprendere quali limitazioni di DistBelief hanno portato allo sviluppo di TensorFlow, faremo prima una breve panoramica dell‚Äôarchitettura del server dei parametri utilizzata da DistBelief <span class="citation" data-cites="dean2012large">(<a href="../../references.it.html#ref-dean2012large" role="doc-biblioref">Dean et al. 2012</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-abadi2016tensorflow" class="csl-entry" role="listitem">
Yu, Yuan, Martƒ±ÃÅn Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, et al. 2018. <span>¬´Dynamic control flow in large-scale machine learning¬ª</span>. In <em>Proceedings of the Thirteenth EuroSys Conference</em>, 265‚Äì83. ACM. <a href="https://doi.org/10.1145/3190508.3190551">https://doi.org/10.1145/3190508.3190551</a>.
</div><div id="ref-dean2012large" class="csl-entry" role="listitem">
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. <span>¬´Large Scale Distributed Deep Networks¬ª</span>. In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L√©on Bottou, e Kilian Q. Weinberger, 1232‚Äì40. <a href="https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html</a>.
</div></div><p>L‚Äôarchitettura Parameter Server (PS) √® un design popolare per distribuire il training di modelli di apprendimento automatico, in particolare reti neurali profonde, su pi√π macchine. L‚Äôidea fondamentale √® di separare l‚Äôarchiviazione e la gestione dei parametri del modello dal calcolo utilizzato per aggiornare tali parametri. In genere, i server dei parametri gestiscono l‚Äôarchiviazione e la gestione dei parametri del modello, suddividendoli su pi√π server. I processi worker eseguono le attivit√† di calcolo, tra cui l‚Äôelaborazione dei dati e il calcolo dei gradienti, che vengono poi inviati ai server dei parametri per l‚Äôaggiornamento.</p>
<p><strong>Storage:</strong> I processi del server dei parametri stateful [con stato] gestivano l‚Äôarchiviazione e la gestione dei parametri del modello. Data l‚Äôampia scala dei modelli e la natura distribuita del sistema, questi parametri erano condivisi tra pi√π server dei parametri. Ogni server manteneva una parte dei parametri del modello, rendendolo "stateful" poich√© doveva mantenere e gestire questo stato durante il processo di training.</p>
<p><strong>Computation:</strong> I processi worker, che potevano essere eseguiti in parallelo, erano senza stato e puramente computazionali. Elaboravano dati e calcolavano gradienti senza mantenere alcuno stato o memoria a lungo termine <span class="citation" data-cites="li2014communication">(<a href="../../references.it.html#ref-li2014communication" role="doc-biblioref">M. Li et al. 2014</a>)</span>. I worker non conservavano informazioni tra le diverse attivit√†. Invece, comunicavano periodicamente con i server dei parametri per recuperare i parametri pi√π recenti e restituire i gradienti calcolati.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2014communication" class="csl-entry" role="listitem">
Li, Mu, David G. Andersen, Alexander J. Smola, e Kai Yu. 2014. <span>¬´Communication Efficient Distributed Machine Learning with the Parameter Server¬ª</span>. In <em>Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada</em>, a cura di Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, e Kilian Q. Weinberger, 19‚Äì27. <a href="https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html">https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html</a>.
</div></div><div id="exr-tfc" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;6.1: TensorFlow Core
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Andiamo a comprendere in modo completo gli algoritmi di apprendimento automatico di base utilizzando TensorFlow e le loro applicazioni pratiche nell‚Äôanalisi dei dati e nella modellazione predittiva. Inizieremo con la regressione lineare per prevedere i tassi di sopravvivenza dal set di dati del Titanic. Poi, utilizzando TensorFlow, costruiremo classificatori per identificare diverse specie di fiori in base ai loro attributi. Successivamente, utilizzeremo l‚Äôalgoritmo K-Means e la sua applicazione nella segmentazione dei set di dati in cluster coesi. Infine, applicheremo modelli hidden [nascosti] di Markov (HMM) per prevedere i pattern meteorologici.</p>
<p><a href="https://colab.research.google.com/drive/15Cyy2H7nT40sGR7TBN5wBvgTd57mVKay#scrollTo=IEeIRxlbx0wY"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<div id="exr-tfl" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;6.2: TensorFlow Lite
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Qui vedremo come costruire un modello di apprendimento automatico in miniatura per microcontrollori. Costruiremo una mini rete neurale semplificata per apprendere dai dati anche con risorse limitate e ottimizzata per l‚Äôimplementazione riducendo il nostro modello per un uso efficiente sui microcontrollori. TensorFlow Lite, una potente tecnologia derivata da TensorFlow, riduce i modelli per dispositivi minuscoli e aiuta ad abilitare funzionalit√† sul dispositivo come il riconoscimento delle immagini nei dispositivi smart [intelligenti]. Viene utilizzato nell‚Äôedge computing per consentire analisi e decisioni pi√π rapide nei dispositivi che elaborano i dati localmente.</p>
<p><a href="https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/TFLite-Micro-Hello-World/train_TFL_Micro_hello_world_model.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<p>DistBelief e la sua architettura definita sopra sono stati fondamentali per abilitare il deep learning distribuito in Google, ma hanno anche introdotto delle limitazioni che hanno motivato lo sviluppo di TensorFlow:</p>
</section>
<section id="grafico-di-calcolo-statico" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="grafico-di-calcolo-statico"><span class="header-section-number">6.3.2</span> Grafico di Calcolo Statico</h3>
<p>I parametri del modello sono distribuiti su vari server di parametri nell‚Äôarchitettura del server di parametri. Poich√© DistBelief √® stato progettato principalmente per il paradigma della rete neurale, i parametri corrispondevano a una struttura di rete neurale fissa. Se il computation graph [grafico di calcolo] fosse dinamico, la distribuzione e il coordinamento dei parametri diventerebbero significativamente pi√π complicati. Ad esempio, una modifica nel grafico potrebbe richiedere l‚Äôinizializzazione di nuovi parametri o la rimozione di quelli esistenti, complicando le attivit√† di gestione e sincronizzazione dei server di parametri. Ci√≤ ha reso pi√π difficile implementare modelli al di fuori del framework neurale o modelli che richiedevano grafici di calcolo dinamici.</p>
<p>TensorFlow √® stato progettato come un framework di calcolo pi√π generale che esprime il calcolo come un grafico del flusso di dati. Ci√≤ consente una pi√π ampia variet√† di modelli e algoritmi di apprendimento automatico al di fuori delle reti neurali e fornisce flessibilit√† nel perfezionamento dei modelli.</p>
</section>
<section id="usabilit√†-distribuzione" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="usabilit√†-distribuzione"><span class="header-section-number">6.3.3</span> Usabilit√† &amp; Distribuzione</h3>
<p>Il modello del server dei parametri delinea i ruoli (nodi worker e server dei parametri) ed √® ottimizzato per i deployment [distribuzioni] dei data center, che potrebbero essere ottimali solo per alcuni casi d‚Äôuso. Ad esempio, questa divisione introduce overhead o complessit√† sui dispositivi edge o in altri ambienti non data center.</p>
<p>TensorFlow √® stato creato per funzionare su pi√π piattaforme, dai dispositivi mobili e edge all‚Äôinfrastruttura cloud. Mirava anche a essere pi√π leggero e intuitivo per gli sviluppatori e a fornire facilit√† d‚Äôuso tra il training locale e quello distribuito.</p>
</section>
<section id="progettazione-dellarchitettura" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="progettazione-dellarchitettura"><span class="header-section-number">6.3.4</span> Progettazione dell‚ÄôArchitettura</h3>
<p>Invece di utilizzare l‚Äôarchitettura del server dei parametri, TensorFlow distribuisce i task [attivit√†] su un cluster. Queste attivit√† sono processi denominati che possono comunicare su una rete e ciascuna pu√≤ eseguire la struttura principale di TensorFlow, il grafico del flusso di dati e l‚Äôinterfaccia con vari dispositivi di elaborazione (come CPU o GPU). Questo grafico [grafo] √® una rappresentazione diretta in cui i nodi simboleggiano le operazioni di elaborazione e gli edge rappresentano i tensori (dati) che scorrono tra queste operazioni.</p>
<p>Nonostante l‚Äôassenza di server di parametri tradizionali, alcuni ‚Äútask PS‚Äù memorizzano e gestiscono parametri che ricordano i server di parametri di altri sistemi. I task rimanenti, che di solito gestiscono calcoli, elaborazione dati e gradienti, sono denominati ‚Äútask worker‚Äù. I task PS di TensorFlow possono eseguire qualsiasi calcolo rappresentabile dal grafico del flusso di dati, il che significa che non sono limitati solo all‚Äôarchiviazione dei parametri e il calcolo pu√≤ essere distribuito. Questa capacit√† li rende significativamente pi√π versatili e offre agli utenti il potere di programmare i task PS utilizzando l‚Äôinterfaccia TensorFlow standard, la stessa che userebbero per definire i loro modelli. Come accennato in precedenza, la struttura dei grafici del flusso di dati li rende anche intrinsecamente buoni per il parallelismo, consentendo l‚Äôelaborazione di grandi set di dati.</p>
</section>
<section id="funzionalit√†-native-keras" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="funzionalit√†-native-keras"><span class="header-section-number">6.3.5</span> Funzionalit√† Native &amp; Keras</h3>
<p>TensorFlow include librerie per aiutare gli utenti a sviluppare e distribuire pi√π modelli specifici per i casi d‚Äôuso e, poich√© questo framework √® open source, questo elenco continua a crescere. Queste librerie affrontano l‚Äôintero ciclo di vita dello sviluppo ML: preparazione dei dati, creazione di modelli, distribuzione e IA responsabile.</p>
<p>Uno dei maggiori vantaggi di TensorFlow √® la sua integrazione con Keras, anche se, come vedremo nella prossima sezione, Pytorch ha recentemente aggiunto un‚Äôintegrazione Keras. Keras √® un altro framework ML creato per essere estremamente intuitivo e, di conseguenza, ha un alto livello di astrazione. Parleremo di Keras pi√π approfonditamente pi√π avanti in questo capitolo. Tuttavia, quando si discute della sua integrazione con TensorFlow, √® importante notare che era stato originariamente creato per essere indipendente dal backend. Ci√≤ significa che gli utenti potrebbero astrarre queste complessit√†, offrendo un modo pi√π pulito e intuitivo per definire e addestrare modelli senza preoccuparsi di problemi di compatibilit√† con diversi backend. Gli utenti di TensorFlow hanno evidenziato alcuni problemi sull‚Äôusabilit√† e la leggibilit√† dell‚ÄôAPI di TensorFlow, quindi, man mano che TF acquisiva importanza, ha integrato Keras come API di alto livello. Questa integrazione ha offerto grandi vantaggi agli utenti di TensorFlow poich√© ha introdotto una leggibilit√† e una portabilit√† pi√π intuitive dei modelli, sfruttando comunque le potenti funzionalit√† di backend, il supporto di Google e l‚Äôinfrastruttura per distribuire i modelli su varie piattaforme.</p>
<div id="exr-k" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;6.3: Esplorazione di Keras: Creazione, Addestramento e Valutazione di Reti Neurali
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Qui, impareremo come utilizzare Keras, un‚ÄôAPI di reti neurali di alto livello, per lo sviluppo e l‚Äôaddestramento (training) di modelli. Esploreremo l‚ÄôAPI funzionale per la creazione di modelli concisi, comprenderemo le classi ‚Äúloss‚Äù e metriche per la valutazione dei modelli e utilizzeremo gli ottimizzatori nativi per aggiornare i parametri del modello durante l‚Äôaddestramento. Inoltre, scopriremo come definire layer e metriche personalizzati su misura per le nostre esigenze. Infine, esamineremo i cicli di addestramento di Keras per semplificare il processo di addestramento delle reti neurali su grandi set di dati. Questa conoscenza ci consentir√† di costruire e ottimizzare modelli di reti neurali in varie applicazioni di machine learning e intelligenza artificiale.</p>
<p><a href="https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO#scrollTo=fxINLLGitX_n"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="limitazioni-e-sfide" class="level3" data-number="6.3.6">
<h3 data-number="6.3.6" class="anchored" data-anchor-id="limitazioni-e-sfide"><span class="header-section-number">6.3.6</span> Limitazioni e Sfide</h3>
<p>TensorFlow √® uno dei framework di deep learning pi√π popolari, ma ha dovuto affrontare critiche e debolezze, principalmente legate all‚Äôusabilit√† e all‚Äôutilizzo delle risorse. Sebbene vantaggioso, il ritmo rapido degli aggiornamenti tramite il supporto di Google ha talvolta portato a problemi di retrocompatibilit√†, funzioni deprecate e documentazione instabile. Inoltre, anche con l‚Äôimplementazione di Keras, la sintassi e la curva di apprendimento di TensorFlow possono risultare difficili per i nuovi utenti. Un‚Äôaltra critica importante di TensorFlow √® il suo elevato overhead e consumo di memoria dovuto alla gamma di librerie integrate e al supporto. Sebbene le versioni ridotte possano risolvere alcuni di questi problemi, potrebbero comunque essere limitate in ambienti con risorse limitate.</p>
</section>
<section id="sec-pytorch_vs_tensorflow" class="level3" data-number="6.3.7">
<h3 data-number="6.3.7" class="anchored" data-anchor-id="sec-pytorch_vs_tensorflow"><span class="header-section-number">6.3.7</span> PyTorch &amp; TensorFlow</h3>
<p>PyTorch e TensorFlow si sono affermati come leader nel settore. Entrambi i framework offrono funzionalit√† robuste ma differiscono per filosofie di progettazione, facilit√† d‚Äôuso, ecosistema e capacit√† di distribuzione.</p>
<p><strong>Filosofia di Progettazione e Paradigma di Programmazione:</strong> PyTorch utilizza un grafo computazionale dinamico denominato eager execution [esecuzione rapida]. Ci√≤ lo rende intuitivo e facilita il debug poich√© le operazioni vengono eseguite immediatamente e possono essere ispezionate al volo. Al contrario, le versioni precedenti di TensorFlow erano incentrate su un grafo computazionale statico, che richiedeva la definizione completa del grafico prima dell‚Äôesecuzione. Tuttavia, TensorFlow 2.0 ha introdotto la ‚Äúeager execution‚Äù per default, rendendolo pi√π allineato con PyTorch. La natura dinamica di PyTorch e l‚Äôapproccio basato su Python hanno consentito la sua semplicit√† e flessibilit√†, in particolare per la prototipazione rapida. L‚Äôapproccio grafico statico di TensorFlow nelle sue versioni precedenti aveva una curva di apprendimento pi√π ripida; l‚Äôintroduzione di TensorFlow 2.0, con la sua integrazione Keras come API di alto livello, ha semplificato notevolmente il processo di sviluppo.</p>
<p><strong>Deployment:</strong> PyTorch √® fortemente favorito negli ambienti di ricerca, ma la distribuzione dei modelli PyTorch in contesti di produzione √® sempre stata un problema. Tuttavia, la distribuzione √® diventata pi√π fattibile con l‚Äôintroduzione di TorchScript, lo strumento TorchServe e <a href="https://pytorch.org/mobile/home/">PyTorch Mobile</a>. TensorFlow si distingue per la sua forte scalabilit√† e capacit√† di distribuzione, in particolare su piattaforme embedded e mobili con TensorFlow Lite. TensorFlow Serving e TensorFlow.js facilitano ulteriormente la distribuzione in vari ambienti, conferendogli cos√¨ una portata pi√π ampia nell‚Äôecosistema.</p>
<p><strong>Prestazioni:</strong> Entrambi i framework offrono un‚Äôaccelerazione hardware efficiente per le loro operazioni. Tuttavia, TensorFlow ha un flusso di lavoro di ottimizzazione leggermente pi√π robusto, come il compilatore XLA (Accelerated Linear Algebra), che pu√≤ aumentare ulteriormente le prestazioni. Il suo grafo computazionale statico era anche vantaggioso per alcune ottimizzazioni nelle prime versioni.</p>
<p><strong>Ecosistema:</strong> PyTorch ha un ecosistema in crescita con strumenti come TorchServe per servire modelli e librerie come TorchVision, TorchText e TorchAudio per domini specifici. Come abbiamo detto prima, TensorFlow ha un ecosistema ampio e maturo. TensorFlow Extended (TFX) fornisce una piattaforma end-to-end per distribuire pipeline di apprendimento automatico di produzione. Altri strumenti e librerie includono TensorFlow Lite, TensorFlow Lite Micro, TensorFlow.js, TensorFlow Hub e TensorFlow Serving.</p>
<p><a href="#tbl-pytorch_vs_tf" class="quarto-xref">Tabella&nbsp;<span>6.1</span></a> fornisce un‚Äôanalisi comparativa:</p>
<div id="tbl-pytorch_vs_tf" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pytorch_vs_tf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;6.1: Confronto tra PyTorch e TensorFlow.
</figcaption>
<div aria-describedby="tbl-pytorch_vs_tf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 31%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspetto</th>
<th style="text-align: left;">Pytorch</th>
<th style="text-align: left;">TensorFlow</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Filosofia di Progettazione</td>
<td style="text-align: left;">Grafo computazionale dinamico (eager execution)</td>
<td style="text-align: left;">Grafo computazionale statico (prime versioni); Esecuzione rapida in TensorFlow 2.0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Deployment</td>
<td style="text-align: left;">Tradizionalmente impegnativa; Migliorata con TorchScript e TorchServe</td>
<td style="text-align: left;">Scalabile, specialmente su piattaforme embedded con TensorFlow Lite</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Prestazioni e Ottimizzazione</td>
<td style="text-align: left;">Accelerazione GPU efficiente</td>
<td style="text-align: left;">Ottimizzazione robusta con compilatore XLA</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ecosistema</td>
<td style="text-align: left;">TorchServe, TorchVision, TorchText, TorchAudio, PyTorch Mobile</td>
<td style="text-align: left;">TensorFlow Extended (TFX), TensorFlow Lite, TensorFlow Lite Micro TensorFlow.js, TensorFlow Hub, TensorFlow Serving</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Facilit√† d‚Äôuso</td>
<td style="text-align: left;">Preferito per il suo approccio Pythonic e la prototipazione rapida</td>
<td style="text-align: left;">Curva di apprendimento inizialmente ripida; Semplificato con Keras in TensorFlow 2.0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="componenti-di-base-del-framework" class="level2 page-columns page-full" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="componenti-di-base-del-framework"><span class="header-section-number">6.4</span> Componenti di Base del Framework</h2>
<p>Dopo aver introdotto i popolari framework di machine learning e aver fornito un confronto di alto livello, questa sezione presenter√† le funzionalit√† principali che formano la struttura di questi framework. Tratter√† la struttura speciale chiamata tensori, che questi framework utilizzano per gestire pi√π facilmente dati multidimensionali complessi. Si vedr√† anche come questi framework rappresentano diversi tipi di architetture di reti neurali e le loro operazioni richieste tramite grafi computazionali. Inoltre, si vedr√† come offrono strumenti che rendono lo sviluppo di modelli di machine learning pi√π astratto ed efficiente, come caricatori di dati, algoritmi di ottimizzazione delle perdite implementate, tecniche di differenziazione efficienti e la capacit√† di accelerare il processo di training su acceleratori hardware.</p>
<section id="sec-tensor-data-structures" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="sec-tensor-data-structures"><span class="header-section-number">6.4.1</span> Strutture Dati Tensoriali</h3>
<p>Per comprendere i tensori, partiamo dai concetti familiari dell‚Äôalgebra lineare. Come mostrato in <a href="#fig-tensor-data-structure" class="quarto-xref">Figura&nbsp;<span>6.5</span></a>, i vettori possono essere rappresentati come una pila di numeri in un array unidimensionale. Le matrici seguono la stessa idea e si possono pensare a loro come a molti vettori impilati l‚Äôuno sull‚Äôaltro, rendendoli bidimensionali. I tensori di dimensioni superiori funzionano allo stesso modo. Un tensore tridimensionale √® semplicemente un insieme di matrici impilate l‚Äôuna sull‚Äôaltra in una direzione aggiuntiva. Pertanto, vettori e matrici possono essere considerati casi speciali di tensori con dimensioni 1D e 2D, rispettivamente.</p>
<div id="fig-tensor-data-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.4: Visualizzazione della Struttura Dati del Tensore.
</figcaption>
</figure>
</div>
<p>I tensori offrono una struttura flessibile che pu√≤ rappresentare dati in dimensioni superiori. Ad esempio, per rappresentare i dati di un‚Äôimmagine, i pixel in ogni posizione di un‚Äôimmagine sono strutturati come matrici. Tuttavia, le immagini non sono rappresentate da una sola matrice di valori di pixel; in genere hanno tre canali in cui ogni canale √® una matrice contenente valori di pixel che rappresentano l‚Äôintensit√† di rosso, verde o blu. Insieme, questi canali creano un‚Äôimmagine colorata. Senza i tensori, archiviare tutte queste informazioni da pi√π matrici pu√≤ risultare complesso. Con i tensori, √® facile contenere i dati dell‚Äôimmagine in un singolo tensore tridimensionale, con ogni numero che rappresenta un certo valore di colore in una posizione specifica nell‚Äôimmagine.</p>
<div id="fig-tensor-data-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/color_channels_of_image.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.5: Visualizzazione della struttura dell‚Äôimmagine colorata che pu√≤ essere facilmente memorizzata come un Tensore 3D. Credito: <a href="https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff">Niklas Lang</a>
</figcaption>
</figure>
</div>
<p>Non finisce qui. Se volessimo archiviare una serie di immagini, potremmo usare un tensore quadridimensionale, in cui la nuova dimensione rappresenta immagini diverse. Ci√≤ significa che si stanno archiviando pi√π immagini, ciascuna con tre matrici che rappresentano i tre canali del colore. Questo d√† un‚Äôidea dell‚Äôutilit√† dei tensori quando si gestiscono dati multidimensionali in modo efficiente.</p>
<p>I tensori hanno anche un attributo unico che consente ai framework di calcolare automaticamente i gradienti, semplificando l‚Äôimplementazione di modelli complessi e algoritmi di ottimizzazione. Nel machine learning, come discusso nel <a href="../dl_primer/dl_primer.qmd#sec-backward_pass">Capitolo 3</a>, la backpropagation richiede di prendere la derivata delle equazioni. Una delle caratteristiche principali dei tensori in PyTorch e TensorFlow √® la loro capacit√† di tracciare i calcoli e calcolare i gradienti. Ci√≤ √® fondamentale per la backpropagation nelle reti neurali. Ad esempio, in PyTorch, si pu√≤ usare l‚Äôattributo <code>requires_grad</code>, che consente di calcolare e memorizzare automaticamente i gradienti durante il ‚Äúbackward pass‚Äù, facilitando il processo di ottimizzazione. Analogamente, in TensorFlow, <code>tf.GradientTape</code> registra le operazioni per la differenziazione automatica.</p>
<p>Si consideri questa semplice equazione matematica che si vuole differenziare. Matematicamente, il calcolo del gradiente si effettua nel modo seguente:</p>
<p>Dato: <span class="math display">\[
y = x^2
\]</span></p>
<p>La derivata di <span class="math inline">\(y\)</span> rispetto a <span class="math inline">\(x\)</span> √®: <span class="math display">\[
\frac{dy}{dx} = 2x
\]</span></p>
<p>Quando <span class="math inline">\(x = 2\)</span>: <span class="math display">\[
\frac{dy}{dx} = 2*2 = 4
\]</span></p>
<p>Il gradiente di <span class="math inline">\(y\)</span> rispetto a <span class="math inline">\(x\)</span>, con <span class="math inline">\(x = 2\)</span>, √® 4.</p>
<p>Una potente caratteristica dei tensori in PyTorch e TensorFlow √® la loro capacit√† di calcolare facilmente le derivate (gradienti). Ecco gli esempi di codice corrispondenti in PyTorch e TensorFlow:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" aria-current="page">PyTorch</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">TensorFlow</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor with gradient tracking</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple function</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the gradient</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>tensor(<span class="fl">4.0</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor with gradient tracking</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.Variable(<span class="fl">2.0</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple function</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> tape.gradient(y, x)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the gradient</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(grad)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>tf.Tensor(<span class="fl">4.0</span>, shape<span class="op">=</span>(), dtype<span class="op">=</span>float32)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>Questa differenziazione automatica √® una potente funzionalit√† dei tensori in framework come PyTorch e TensorFlow, che semplifica l‚Äôimplementazione e l‚Äôottimizzazione di modelli complessi di apprendimento automatico.</p>
</section>
<section id="grafi-computazionali" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="grafi-computazionali"><span class="header-section-number">6.4.2</span> Grafi computazionali</h3>
<section id="definizione-di-grafico" class="level4">
<h4 class="anchored" data-anchor-id="definizione-di-grafico">Definizione di Grafico</h4>
<p>I grafi computazionali sono una componente chiave di framework di deep learning come TensorFlow e PyTorch. Ci consentono di esprimere architetture di reti neurali complesse in modo efficiente e differenziato. Un grafo computazionale √® costituito da un grafo aciclico diretto (directed acyclic graph, DAG) in cui ogni nodo rappresenta un‚Äôoperazione o una variabile e gli spigoli rappresentano le dipendenze dei dati tra di essi.</p>
<p>√à importante differenziare i grafi computazionali dai diagrammi di reti neurali, come quelli per i perceptron multistrato (multilayer perceptrons, MLP), che rappresentano nodi e layer. I diagrammi di reti neurali, come illustrato nel <a href="../dl_primer/dl_primer.qmd">Capitolo 3</a>, visualizzano l‚Äôarchitettura e il flusso di dati attraverso nodi e layer, fornendo una comprensione intuitiva della struttura del modello. Al contrario, i grafi computazionali forniscono una rappresentazione di basso livello delle operazioni matematiche sottostanti e delle dipendenze dei dati necessarie per implementare e addestrare queste reti.</p>
<p>Ad esempio, un nodo potrebbe rappresentare un‚Äôoperazione di moltiplicazione di matrici, prendendo due matrici di input (o tensori) e producendo una matrice di output (o tensore). Per visualizzarlo, si consideri il semplice esempio in <a href="../training/training.it.html#fig-computational-graph" class="quarto-xref">Figura&nbsp;<span>7.3</span></a>. Il grafo aciclico orientato sopra calcola <span class="math inline">\(z = x \times y\)</span>, dove ogni variabile √® composta solo da numeri.</p>
<div id="fig-computational-graph" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-computational-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image1.png" style="width:50.0%" data-align="center" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-computational-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.6: Esempio elementare di un grafo computazionale.
</figcaption>
</figure>
</div>
<p>Framework come TensorFlow e PyTorch creano grafi computazionali per implementare le architetture delle reti neurali che in genere rappresentiamo con diagrammi. Quando si definisce un layer di rete neurale nel codice (ad esempio, un ‚Äúlayer denso‚Äù in TensorFlow), il framework costruisce un grafo computazionale che include tutte le operazioni necessarie (come moltiplicazione di matrici, addizione e funzioni di attivazione) e le relative dipendenze dai dati. Questo grafo consente al framework di gestire in modo efficiente il flusso di dati, ottimizzare l‚Äôesecuzione delle operazioni e calcolare automaticamente i gradienti per l‚Äôaddestramento. Internamente, i grafi computazionali rappresentano astrazioni per layer comuni come quelli convoluzionali, di pooling, ricorrenti e densi, con dati che includono attivazioni, pesi e bias rappresentati in tensori. Questa rappresentazione consente un calcolo efficiente, sfruttando la struttura del grafico per parallelizzare le operazioni e applicare ottimizzazioni.</p>
<p>Alcuni livelli comuni che i grafi computazionali potrebbero implementare includono layer convoluzionali, di attenzione, ricorrenti e densi. I layer fungono da astrazioni di livello superiore che definiscono calcoli specifici in cima alle operazioni di base rappresentate nel grafo. Ad esempio, un layer Denso esegue la moltiplicazione e l‚Äôaddizione di matrici tra tensori di input, peso e bias. √à importante notare che un layer opera su tensori come input e output; il layer stesso non √® un tensore. Alcune differenze chiave tra layer e tensori sono:</p>
<ul>
<li><p>I layer contengono stati come pesi e bias. I tensori sono senza stato, contengono solo dati.</p></li>
<li><p>I layer possono modificare lo stato interno durante l‚Äôaddestramento. I tensori sono immutabili/di sola lettura.</p></li>
<li><p>I layer sono astrazioni di livello superiore. I tensori sono a un livello inferiore e rappresentano direttamente dati e operazioni matematiche.</p></li>
<li><p>I layer definiscono pattern di calcolo fissi. I tensori scorrono tra i livelli durante l‚Äôesecuzione.</p></li>
<li><p>I layer vengono utilizzati indirettamente durante la creazione di modelli. I tensori scorrono tra i livelli durante l‚Äôesecuzione.</p></li>
</ul>
<p>Quindi, mentre i tensori sono una struttura dati fondamentale che i layer consumano e producono, i layer hanno funzionalit√† aggiuntive per definire operazioni parametrizzate e addestramento. Mentre un layer configura le operazioni tensoriali in background, il layer rimane distinto dagli oggetti tensoriali. L‚Äôastrazione del layer rende la creazione e l‚Äôaddestramento di reti neurali molto pi√π intuitive. Questa astrazione consente agli sviluppatori di creare modelli impilando insieme questi layer senza implementare la logica del layer. Ad esempio, la chiamata di <code>tf.keras.layers.Conv2D</code> in TensorFlow crea un layer convoluzionale. Il framework gestisce il calcolo delle convoluzioni, la gestione dei parametri, ecc. Ci√≤ semplifica lo sviluppo del modello, consentendo agli sviluppatori di concentrarsi sull‚Äôarchitettura anzich√© sulle implementazioni di basso livello. Le astrazioni dei layer utilizzano implementazioni altamente ottimizzate per le prestazioni. Consentono inoltre la portabilit√†, poich√© la stessa architettura pu√≤ essere eseguita su backend hardware diversi come GPU e TPU.</p>
<p>Inoltre, i grafi computazionali includono funzioni di attivazione come ReLU, sigmoide e tanh che sono essenziali per le reti neurali e molti framework le forniscono come astrazioni standard. Queste funzioni introducono non linearit√† che consentono ai modelli di approssimare funzioni complesse. I framework le forniscono come operazioni semplici e predefinite che possono essere utilizzate durante la costruzione di modelli, ad esempio if.nn.relu in TensorFlow. Questa astrazione consente flessibilit√†, poich√© gli sviluppatori possono facilmente scambiare le funzioni di attivazione per ottimizzare le prestazioni. Le attivazioni predefinite sono inoltre ottimizzate dal framework per un‚Äôesecuzione pi√π rapida.</p>
<p>Negli ultimi anni, modelli come ResNets e MobileNets sono emersi come architetture popolari, con i framework attuali che li pre-confezionano come grafi computazionali. Invece di preoccuparsi dei dettagli, gli sviluppatori possono utilizzarli come punto di partenza, personalizzandoli secondo necessit√† sostituendo i layer. Ci√≤ semplifica e velocizza lo sviluppo del modello, evitando di reinventare le architetture da zero. I modelli predefiniti includono implementazioni ben collaudate e ottimizzate che garantiscono buone prestazioni. Il loro design modulare consente inoltre di trasferire le funzionalit√† apprese a nuove attivit√† tramite apprendimento tramite trasferimento. Queste architetture predefinite forniscono i mattoni ad alte prestazioni per creare rapidamente modelli robusti.</p>
<p>Queste astrazioni di layer, funzioni di attivazione e architetture predefinite fornite dai framework costituiscono un grafo computazionale. Quando un utente definisce un layer in un framework (ad esempio, <code>tf.keras.layers.Dense()</code>), il framework configura nodi e bordi del grafo computazionale per rappresentare tale layer. I parametri del layer come pesi e bias diventano variabili nel grafo. I calcoli del layer diventano nodi operativi (come x e y nella figura sopra). Quando si chiama una funzione di attivazione come <code>tf.nn.relu()</code>, il framework aggiunge un nodo operativo ReLU al grafo. Le architetture predefinite sono solo sottografi preconfigurati che possono essere inseriti nel grafo del modello. Quindi, la definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i livelli, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafo.</p>
<p>Costruiamo implicitamente un grafo computazionale quando definiamo un‚Äôarchitettura di rete neurale in un framework. Il framework utilizza questo grafo per determinare le operazioni da eseguire durante l‚Äôaddestramento e l‚Äôinferenza. I grafi computazionali offrono diversi vantaggi rispetto al codice grezzo e questa √® una delle funzionalit√† principali offerte da un buon framework di ML:</p>
<ul>
<li><p>Rappresentazione esplicita del flusso di dati e delle operazioni</p></li>
<li><p>Capacit√† di ottimizzare il grafo prima dell‚Äôesecuzione</p></li>
<li><p>Differenziazione automatica per il training</p></li>
<li><p>Agnosticismo linguistico: il grafo pu√≤ essere tradotto per essere eseguito su GPU, TPU, ecc.</p></li>
<li><p>Portabilit√†: il grafo pu√≤ essere serializzato, salvato e ripristinato in seguito</p></li>
</ul>
<p>I grafi computazionali sono i componenti fondamentali dei framework di ML. La definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i layer, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafico. I compilatori e gli ottimizzatori del framework operano su questo grafo per generare codice eseguibile. Le astrazioni forniscono un‚ÄôAPI intuitiva per gli sviluppatori per la creazione di grafi computazionali. Sotto, ci sono ancora grafi! Quindi, anche se non si possono manipolare direttamente i grafi come utente del framework, consentono di eseguire ad alto livello e in modo efficiente le specifiche del modello. Le astrazioni semplificano la creazione del modello, mentre i grafi computazionali la rendono possibile.</p>
</section>
<section id="grafi-statici-vs.-dinamici" class="level4">
<h4 class="anchored" data-anchor-id="grafi-statici-vs.-dinamici">Grafi Statici vs.&nbsp;Dinamici</h4>
<p>I framework di deep learning hanno tradizionalmente seguito uno dei due approcci per esprimere grafi computazionali.</p>
<p><strong>Grafi statici (declare-then-execute):</strong> Con questo modello, l‚Äôintero grafo computazionale deve essere definito in anticipo prima di eseguirlo. Tutte le operazioni e le dipendenze dei dati devono essere specificate durante la fase di dichiarazione. TensorFlow originariamente seguiva questo approccio statico: i modelli venivano definiti in un contesto separato e poi veniva creata una sessione per eseguirli. Il vantaggio dei grafi statici √® che consentono un‚Äôottimizzazione pi√π aggressiva poich√© il framework pu√≤ vedere il grafo completo. Tuttavia, tende anche a essere meno flessibile per la ricerca e l‚Äôinterattivit√†. Le modifiche al grafo richiedono la nuova dichiarazione del modello completo.</p>
<p>Per esempio:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.placeholder(tf.float32)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.matmul(x, weights) <span class="op">+</span> biases</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In questo esempio, x √® un segnaposto per i dati di input e y √® il risultato di un‚Äôoperazione di moltiplicazione di matrici seguita da un‚Äôaddizione. Il modello √® definito in questa fase di dichiarazione, in cui tutte le operazioni e le variabili devono essere specificate in anticipo.</p>
<p>Una volta definito l‚Äôintero grafo, il framework lo compila e lo ottimizza. Ci√≤ significa che i passaggi computazionali sono definitivamente ‚Äúscolpiti‚Äù e il framework pu√≤ applicare varie ottimizzazioni per migliorare l‚Äôefficienza e le prestazioni. Quando in seguito si esegue il grafo, si forniscono i tensori di input effettivi e le operazioni predefinite vengono eseguite nella sequenza ottimizzata.</p>
<p>Questo approccio √® simile alla creazione di un progetto in cui ogni dettaglio √® pianificato prima dell‚Äôinizio della costruzione. Sebbene ci√≤ consenta potenti ottimizzazioni, significa anche che qualsiasi modifica al modello richiede la ridefinizione dell‚Äôintero grafo da zero.</p>
<p><strong>Grafi dinamici (define-by-run):</strong> A differenza della dichiarazione (di tutto) prima e dell‚Äôesecuzione poi, il grafo viene creato dinamicamente durante l‚Äôesecuzione. Non esiste una fase di dichiarazione separata: le operazioni vengono eseguite immediatamente come definite. Questo stile √® imperativo e flessibile, facilitando la sperimentazione.</p>
<p>PyTorch utilizza grafi dinamici, creandoli al volo mentre avviene l‚Äôesecuzione. Ad esempio, si consideri il seguente frammento di codice, in cui il grafo viene creato durante l‚Äôesecuzione:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">4</span>,<span class="dv">784</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.matmul(x, weights) <span class="op">+</span> biases</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>L‚Äôesempio sopra non ha fasi separate di compilazione/build/esecuzione. Le operazioni definiscono ed eseguono immediatamente. Con i grafi dinamici, la definizione √® intrecciata con l‚Äôesecuzione, fornendo un flusso di lavoro pi√π intuitivo e interattivo. Tuttavia, lo svantaggio √® che c‚Äô√® meno potenziale di ottimizzazione poich√© il framework vede solo il grafo mentre viene creato. <a href="#fig-static-vs-dynamic" class="quarto-xref">Figura&nbsp;<span>6.7</span></a> mostra le differenze tra un grafo di calcolo statico e uno dinamico.</p>
<div id="fig-static-vs-dynamic" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-static-vs-dynamic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/staticvsdynamic.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-static-vs-dynamic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.7: Confronto tra grafi statici e dinamici. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fdev-jm.tistory.com%2F4&amp;psig=AOvVaw0r1cZbZa6iImYP-fesrN7H&amp;ust=1722533107591000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBQQjhxqFwoTCLC8nYHm0YcDFQAAAAAdAAAAABAY">Dev</a>
</figcaption>
</figure>
</div>
<p>Di recente, la distinzione si √® offuscata poich√© i framework adottano entrambe le modalit√†. TensorFlow 2.0 passa automaticamente alla modalit√† di grafo dinamico, consentendo agli utenti di lavorare con quelli statici quando necessario. La dichiarazione dinamica offre flessibilit√† e facilit√† d‚Äôuso, rendendo i framework pi√π intuitivi, mentre i grafi statici forniscono vantaggi di ottimizzazione a scapito dell‚Äôinterattivit√†. Il framework ideale bilancia questi approcci. <a href="#tbl-exec-graph" class="quarto-xref">Tabella&nbsp;<span>6.2</span></a> confronta i pro e i contro dei grafi di esecuzione statici e dinamici:</p>
<div id="tbl-exec-graph" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-exec-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;6.2: Confronto tra Grafi di Esecuzione Statici (Declare-then-execute) e Dinamici (Define-by-run), evidenziandone i rispettivi pro e contro.
</figcaption>
<div aria-describedby="tbl-exec-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 37%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Grafo di esecuzione</th>
<th style="text-align: left;">Pro</th>
<th style="text-align: left;">Contro</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Statico (Declare-then-execute)</td>
<td style="text-align: left;"><ul>
<li>Abilita le ottimizzazioni del grafo visualizzando il modello completo in anticipo</li>
<li>Pu√≤ esportare e distribuire grafici congelati</li>
<li>Il grafo √® impacchettato indipendentemente dal codice</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Meno flessibile per la ricerca e l‚Äôiterazione</li>
<li>Le modifiche richiedono la ricostruzione del grafo</li>
<li>L‚Äôesecuzione ha fasi di compilazione ed esecuzione separate</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;">Dinamico (Define-by-run)</td>
<td style="text-align: left;"><ul>
<li>Stile imperativo intuitivo come il codice Python</li>
<li>Alterna la creazione del grafo con l‚Äôesecuzione</li>
<li>Facile da modificare i grafi</li>
<li>Il debug si adatta perfettamente al flusso di lavoro</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Pi√π difficile da ottimizzare senza un grafo completo</li>
<li>Possibili rallentamenti dalla creazione del grafo durante l‚Äôesecuzione</li>
<li>Pu√≤ richiedere pi√π memoria</li>
</ul></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="tool-della-pipeline-dei-dati" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="tool-della-pipeline-dei-dati"><span class="header-section-number">6.4.3</span> Tool della Pipeline dei Dati</h3>
<p>I grafi computazionali possono essere validi solo quanto i dati da cui apprendono e su cui lavorano. Pertanto, alimentare i dati di training in modo efficiente √® fondamentale per ottimizzare le prestazioni della ‚Äúdeep neural network‚Äù [rete neurale profonda], sebbene spesso venga trascurata come una delle funzionalit√† principali. Molti framework di IA moderni forniscono pipeline specializzate per acquisire, elaborare e aumentare i set di dati per il training del modello.</p>
<section id="loader-dei-dati" class="level4">
<h4 class="anchored" data-anchor-id="loader-dei-dati">Loader dei Dati</h4>
<p>Al centro di queste pipeline ci sono i ‚Äúdata loader‚Äù, che gestiscono esempi di training di lettura da fonti come file, database e storage di oggetti. I data loader facilitano il caricamento e la pre-elaborazione efficienti dei dati, cruciali per i modelli di deep learning. Ad esempio, la pipeline di caricamento dati <a href="https://www.tensorflow.org/guide/data">tf.data</a> di TensorFlow √® progettata per gestire questo processo. A seconda dell‚Äôapplicazione, i modelli di deep learning richiedono diversi formati di dati come file CSV o cartelle di immagini. Alcuni formati popolari includono:</p>
<ul>
<li><p>CSV, un formato versatile e semplice spesso utilizzato per dati tabellari.</p></li>
<li><p>TFRecord: Formato proprietario di TensorFlow, ottimizzato per le prestazioni.</p></li>
<li><p>Parquet: Storage a colonne, che offre compressione e recupero dati efficienti.</p></li>
<li><p>JPEG/PNG: Comunemente utilizzato per dati di immagini.</p></li>
<li><p>WAV/MP3: Formati prevalenti per dati audio.</p></li>
</ul>
<p>Esempi di batch di data loader per sfruttare il supporto di vettorizzazione nell‚Äôhardware. Il ‚Äúbatching‚Äù si riferisce al raggruppamento di pi√π dati per l‚Äôelaborazione simultanea, sfruttando le capacit√† di calcolo vettorizzate di hardware come le GPU. Sebbene le dimensioni tipiche dei batch siano comprese tra 32 e 512 esempi, la dimensione ottimale spesso dipende dall‚Äôingombro di memoria dei dati e dai vincoli hardware specifici. I loader avanzati possono trasmettere in streaming set di dati virtualmente illimitati da dischi e archivi cloud. Trasmettono in streaming grandi dataset da dischi o reti anzich√© caricarli completamente in memoria, consentendo dimensioni illimitate.</p>
<p>I data loader possono anche mescolare i dati tra ‚Äúepoche‚Äù per la randomizzazione e le funzionalit√† di preelaborazione in parallelo con l‚Äôaddestramento del modello per accelerarne il processo. Mescolare casualmente l‚Äôordine degli esempi tra epoche di training riduce il bias e migliora la generalizzazione.</p>
<p>I data loader supportano anche strategie di ‚Äúcaching‚Äù e ‚Äúprefetching‚Äù per ottimizzare la distribuzione dei dati per un addestramento del modello rapido e fluido. Il caching [memorizzazione nella cache] dei batch preelaborati consente di riutilizzarli in modo efficiente durante pi√π fasi di addestramento ed elimina l‚Äôelaborazione ridondante. Il prefetching, al contrario, comporta il precaricamento dei batch successivi, assicurando che il modello non resti mai inattivo in attesa di dati.</p>
</section>
</section>
<section id="data-augmentation" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="data-augmentation"><span class="header-section-number">6.4.4</span> Data Augmentation</h3>
<p>Framework di apprendimento automatico come TensorFlow e PyTorch forniscono strumenti per semplificare e snellire il processo di ‚Äúdata augmentation‚Äù [aumento dei dati], migliorando l‚Äôefficienza dell‚Äôespansione sintetica dei set di dati. Questi framework offrono funzionalit√† integrate per applicare trasformazioni casuali, come capovolgimento, ritaglio, rotazione, modifica del colore e aggiunta di rumore per le immagini. Per i dati audio, gli aumenti comuni comportano la miscelazione di clip con rumore di fondo o la modulazione di velocit√†, tono e volume.</p>
<p>Integrando gli strumenti di ‚Äúaugmentation‚Äù nella pipeline di dati, i framework consentono di applicare queste trasformazioni al volo durante ogni epoca di addestramento. Questo approccio incrementa la variazione nella distribuzione dei dati di addestramento, riducendo cos√¨ l‚Äôoverfitting e migliorando la generalizzazione del modello. <a href="#fig-overfitting" class="quarto-xref">Figura&nbsp;<span>6.8</span></a> mostra i casi di overfitting e underfitting. L‚Äôuso di ‚Äúdata loader‚Äù performanti in combinazione con ampie capacit√† di ‚Äúaugmentation‚Äù consente ai professionisti di alimentare in modo efficiente set di dati massicci e vari alle reti neurali.</p>
<div id="fig-overfitting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-overfitting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/overfittingunderfitting.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-overfitting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.8: Overfitting e underfitting. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.aquariumlearning.com%2Fblog-posts%2Fto-make-your-model-better-first-figure-out-whats-wrong&amp;psig=AOvVaw3FodMJATpeLeeSsuQZBD51&amp;ust=1722534629114000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqGAoTCNiU49br0YcDFQAAAAAdAAAAABCoAQ">Aquarium Learning</a>
</figcaption>
</figure>
</div>
<p>Queste pipeline di dati ‚Äúhands-off‚Äù rappresentano un miglioramento significativo in termini di usabilit√† e produttivit√†. Consentono agli sviluppatori di concentrarsi maggiormente sull‚Äôarchitettura del modello e meno sulla manipolazione dei dati durante l‚Äôaddestramento di modelli di deep learning.</p>
</section>
<section id="funzioni-loss-e-algoritmi-di-ottimizzazione" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="funzioni-loss-e-algoritmi-di-ottimizzazione"><span class="header-section-number">6.4.5</span> Funzioni Loss e Algoritmi di Ottimizzazione</h3>
<p>L‚Äôaddestramento di una rete neurale √® fondamentalmente un processo iterativo che cerca di minimizzare una funzione di loss [perdita]. L‚Äôobiettivo √® di mettere a punto i pesi e i parametri del modello per produrre previsioni vicine alle vere etichette target. I framework di apprendimento automatico hanno notevolmente semplificato questo processo offrendo funzioni di loss [perdita] e algoritmi di ottimizzazione.</p>
<p>I framework di apprendimento automatico forniscono funzioni di perdita implementate che sono necessarie per quantificare la differenza tra le previsioni del modello e i valori reali. Diversi set di dati richiedono una diversa funzione di perdita per funzionare correttamente, poich√© tale funzione indica al computer l‚Äô‚Äúobiettivo‚Äù a cui mirare. Le funzioni di perdita comunemente utilizzate includono il ‚ÄúMean Squared Error (MSE)‚Äù [errore quadratico medio] per le attivit√† di regressione, la ‚ÄúCross-Entropy Loss‚Äù per le attivit√† di classificazione, e la Kullback-Leibler (KL) per i modelli probabilistici. Ad esempio, <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses">tf.keras.losses</a> di TensorFlow contiene una serie di queste funzioni di perdita comunemente utilizzate.</p>
<p>Gli algoritmi di ottimizzazione vengono utilizzati per trovare in modo efficiente il set di parametri del modello che minimizzano la funzione di perdita, assicurando che il modello funzioni bene sui dati di training e si generalizzi a nuovi dati. I framework moderni sono dotati di implementazioni efficienti di diversi algoritmi di ottimizzazione, molti dei quali sono varianti della ‚Äúdiscesa del gradiente‚Äù con metodi stocastici e tassi di apprendimento adattivo. Alcuni esempi di queste varianti sono Stochastic Gradient Descent, Adagrad, Adadelta e Adam. L‚Äôimplementazione di tali varianti √® fornita in <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">tf.keras.optimizers</a>. Ulteriori informazioni con esempi chiari sono disponibili nella sezione Training dell‚ÄôIA.</p>
</section>
<section id="supporto-al-training-del-modello" class="level3" data-number="6.4.6">
<h3 data-number="6.4.6" class="anchored" data-anchor-id="supporto-al-training-del-modello"><span class="header-section-number">6.4.6</span> Supporto al Training del Modello</h3>
<p>√à richiesta una fase di compilazione prima di addestrare un modello di rete neurale definito. Durante questa fase, l‚Äôarchitettura di alto livello della rete neurale viene trasformata in un formato eseguibile ottimizzato. Questo processo comprende diverse fasi. La prima fase consiste nel costruire il grafo computazionale, che rappresenta tutte le operazioni matematiche e il flusso di dati all‚Äôinterno del modello. Ne abbiamo discusso in precedenza.</p>
<p>Durante l‚Äôaddestramento, l‚Äôattenzione √® rivolta all‚Äôesecuzione del grafo computazionale. A ogni parametro all‚Äôinterno del grafo, come pesi e bias, viene assegnato un valore iniziale. A seconda del metodo di inizializzazione scelto, questo valore potrebbe essere casuale o basato su una logica predefinita.</p>
<p>Il passaggio critico successivo √® l‚Äôallocazione della memoria. La memoria essenziale √® riservata alle operazioni del modello sia su CPU che su GPU, garantendo un‚Äôelaborazione efficiente dei dati. Le operazioni del modello vengono poi mappate sulle risorse hardware disponibili, in particolare GPU o TPU, per accelerare l‚Äôelaborazione. Una volta completata la compilazione, il modello viene preparato per l‚Äôaddestramento.</p>
<p>Il processo di addestramento impiega vari strumenti per migliorare l‚Äôefficienza. L‚Äôelaborazione batch √® comunemente utilizzata per massimizzare la produttivit√† computazionale. Tecniche come la vettorizzazione consentono operazioni su interi array di dati anzich√© procedere elemento per elemento, il che aumenta la velocit√†. Ottimizzazioni come la ‚Äúkernel fusion‚Äù (fare riferimento al capitolo Ottimizzazioni) amalgamano pi√π operazioni in un‚Äôunica azione, riducendo al minimo il sovraccarico computazionale. Le operazioni possono anche essere segmentate in fasi, facilitando l‚Äôelaborazione simultanea di diversi mini-batch in varie parti.</p>
<p>I framework eseguono costantemente il checkpoint dello stato, preservando le versioni intermedie del modello durante l‚Äôaddestramento. Ci√≤ garantisce che i progressi vengano recuperati in caso di interruzione e che l‚Äôaddestramento possa essere ripreso dall‚Äôultimo checkpoint. Inoltre, il sistema monitora attentamente le prestazioni del modello rispetto a un set di dati di convalida. Se il modello inizia a sovradimensionarsi (se le sue prestazioni sul set di convalida diminuiscono), l‚Äôaddestramento viene automaticamente interrotto, conservando risorse computazionali e tempo.</p>
<p>I framework di ML incorporano una combinazione di compilazione del modello, metodi di elaborazione batch avanzati e utilit√† come il checkpoint e l‚Äôarresto anticipato. Queste risorse gestiscono gli aspetti complessi delle prestazioni, consentendo ai professionisti di concentrarsi sullo sviluppo e l‚Äôaddestramento del modello. Di conseguenza, gli sviluppatori sperimentano sia velocit√† che facilit√† quando utilizzano le capacit√† delle reti neurali.</p>
</section>
<section id="validazione-e-analisi" class="level3" data-number="6.4.7">
<h3 data-number="6.4.7" class="anchored" data-anchor-id="validazione-e-analisi"><span class="header-section-number">6.4.7</span> Validazione e Analisi</h3>
<p>Dopo aver addestrato i modelli di deep learning, i framework forniscono utilit√† per valutare le prestazioni e ottenere informazioni sul funzionamento dei modelli. Questi strumenti consentono una sperimentazione e un debug disciplinati.</p>
<section id="metriche-di-valutazione" class="level4">
<h4 class="anchored" data-anchor-id="metriche-di-valutazione">Metriche di Valutazione</h4>
<p>I framework includono implementazioni di comuni metriche di valutazione per la convalida:</p>
<ul>
<li><p>Accuratezza: Frazione di previsioni corrette complessive. Sono ampiamente utilizzate per la classificazione.</p></li>
<li><p>Precisione: Delle previsioni positive, quante erano positive. Utile per set di dati sbilanciati.</p></li>
<li><p>Richiamo: Dei positivi effettivi, quanti ne abbiamo previsti correttamente? Misura della Completezza.</p></li>
<li><p>Punteggio F1: Media armonica di precisione e richiamo. Combina entrambe le metriche.</p></li>
<li><p>AUC-ROC - Area sotto la curva ROC. Sono utilizzate per l‚Äôanalisi della soglia di classificazione.</p></li>
<li><p>MAP - Mean Average Precision. Valuta le previsioni classificate nel recupero/rilevamento.</p></li>
<li><p>Matrice di Confusione: Matrice che mostra i veri positivi, i veri negativi, i falsi positivi e i falsi negativi. Fornisce una visione pi√π dettagliata delle prestazioni di classificazione.</p></li>
</ul>
<p>Queste metriche quantificano le prestazioni del modello sui dati di convalida per il confronto.</p>
</section>
<section id="visualizzazione" class="level4">
<h4 class="anchored" data-anchor-id="visualizzazione">Visualizzazione</h4>
<p>Gli strumenti di visualizzazione forniscono informazioni sui modelli:</p>
<ul>
<li><p>Curve di perdita: Tracciano la perdita di training e validazione nel tempo per individuare l‚Äôoverfitting.</p></li>
<li><p>Loss curves [Griglie di attivazione]: Illustrano le funzionalit√† apprese dai filtri convoluzionali.</p></li>
<li><p>Projection [Proiezione]: Riduce la dimensionalit√† per una visualizzazione intuitiva.</p></li>
<li><p>Precision-recall curves [Curve di richiamo della precisione]: Valutano i compromessi di classificazione. <a href="#fig-precision-recall" class="quarto-xref">Figura&nbsp;<span>6.9</span></a> mostra un esempio di una curva di precisione-richiamo.</p></li>
</ul>
<div id="fig-precision-recall" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-precision-recall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/precisionrecall.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-precision-recall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.9: Lettura di una curva ‚Äúprecision-recall‚Äù. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.techtarget.com%2Fsearchcio%2Fdefinition%2Ftransfer-learning&amp;psig=AOvVaw0Cbiewbu_6NsNVf314C9Q8&amp;ust=1722534991962000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCPj5jITt0YcDFQAAAAAdAAAAABAE">AIM</a>
</figcaption>
</figure>
</div>
<p>Strumenti come <a href="https://www.tensorflow.org/tensorboard/scalars_and_keras">TensorBoard</a> per TensorFlow e <a href="https://github.com/microsoft/tensorwatch">TensorWatch</a> per PyTorch consentono metriche e visualizzazioni in tempo reale durante il training.</p>
</section>
</section>
<section id="programmazione-differenziabile" class="level3" data-number="6.4.8">
<h3 data-number="6.4.8" class="anchored" data-anchor-id="programmazione-differenziabile"><span class="header-section-number">6.4.8</span> Programmazione differenziabile</h3>
<p>I metodi di addestramento per il machine learning come la backpropagation si basano sulla modifica della funzione di perdita rispetto alla modifica dei pesi (che essenzialmente √® la definizione di derivata). Pertanto, la capacit√† di addestrare rapidamente ed efficientemente grandi modelli di machine learning si basa sulla capacit√† del computer di prendere derivate. Ci√≤ rende la programmazione differenziabile uno degli elementi pi√π importanti di un framework di apprendimento automatico.</p>
<p>Possiamo utilizzare quattro metodi principali per far s√¨ che i computer prendano derivate. Innanzitutto, possiamo calcolare manualmente le derivate a mano e inserirle nel computer. Questo diventerebbe rapidamente un incubo con molti layer di reti neurali se dovessimo calcolare manualmente tutte le derivate nei passaggi di backpropagation. Un altro metodo √® la differenziazione simbolica utilizzando sistemi di computer algebrici come Mathematica, che pu√≤ introdurre un layer di inefficienza, poich√© √® necessario un livello di astrazione per prendere le derivate. Le derivate numeriche, la pratica di approssimare i gradienti utilizzando metodi di differenze finite, soffrono di molti problemi, tra cui elevati costi computazionali e dimensioni della griglia pi√π grandi, che portano a molti errori. Ci√≤ porta alla differenziazione automatica, che sfrutta le funzioni primitive che i computer utilizzano per rappresentare le operazioni per ottenere una derivata esatta. Con la differenziazione automatica, la complessit√† computazionale del calcolo del gradiente √® proporzionale al calcolo della funzione stessa. Le complessit√† della differenziazione automatica non sono gestite dagli utenti finali al momento, ma le risorse per saperne di pi√π possono essere trovate ampiamente, ad esempio <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf">qui</a>. La differenziazione automatica e la programmazione differenziabile di oggi sono onnipresenti e vengono eseguite in modo efficiente e automatico dai moderni framework di machine learning.</p>
</section>
<section id="accelerazione-hardware" class="level3 page-columns page-full" data-number="6.4.9">
<h3 data-number="6.4.9" class="anchored" data-anchor-id="accelerazione-hardware"><span class="header-section-number">6.4.9</span> Accelerazione Hardware</h3>
<p>La tendenza a formare e distribuire continuamente modelli di apprendimento automatico pi√π grandi ha reso necessario il supporto dell‚Äôaccelerazione hardware per le piattaforme di machine-learning. <a href="#fig-hardware-accelerator" class="quarto-xref">Figura&nbsp;<span>6.10</span></a> mostra il gran numero di aziende che offrono acceleratori hardware in diversi domini, come il machine learning ‚ÄúVery Low Power‚Äù e quello ‚ÄúEmbedded‚Äù. I ‚Äúdeep layer‚Äù delle reti neurali richiedono molte moltiplicazioni di matrici, che attraggono hardware in grado di calcolare rapidamente e in parallelo tali operazioni. In questo panorama, due architetture hardware, <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">GPU e TPU</a>, sono emerse come scelte principali per l‚Äôaddestramento di modelli di apprendimento automatico.</p>
<p>L‚Äôuso di acceleratori hardware √® iniziato con <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a>, che ha aperto la strada a lavori futuri per utilizzare le GPU come acceleratori hardware per l‚Äôaddestramento di modelli di visione artificiale. Le GPU, o ‚ÄúGraphics Processing Units‚Äù [unit√† di elaborazione grafica], eccellono nella gestione di molti calcoli contemporaneamente, il che le rende ideali per le operazioni matriciali centrali per l‚Äôaddestramento delle reti neurali. La loro architettura, progettata per il rendering della grafica, √® perfetta per le operazioni matematiche richieste nell‚Äôapprendimento automatico. Sebbene siano molto utili per le attivit√† di apprendimento automatico e siano state implementate in molte piattaforme hardware, le GPU sono comunque di uso generale in quanto possono essere utilizzate per altre applicazioni.</p>
<p>D‚Äôaltro canto, le <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Tensor Processing Units</a> (TPU) sono unit√† hardware progettate specificamente per le reti neurali. Si concentrano sull‚Äôoperazione di ‚Äúmoltiplicazione e accumulazione‚Äù (MAC) e il loro hardware √® costituito da una grande matrice hardware che contiene elementi che calcolano in modo efficiente l‚Äôoperazione MAC. Questo concetto, chiamato <a href="https://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf">systolic array architecture</a>, √® stato ideato da <span class="citation" data-cites="kung1979systolic">Kung e Leiserson (<a href="../../references.it.html#ref-kung1979systolic" role="doc-biblioref">1979</a>)</span>, ma ha dimostrato di essere una struttura utile per calcolare in modo efficiente i prodotti matriciali e altre operazioni all‚Äôinterno delle reti neurali (come le convoluzioni).</p>
<div class="no-row-height column-margin column-container"><div id="ref-kung1979systolic" class="csl-entry" role="listitem">
Kung, Hsiang Tsung, e Charles E Leiserson. 1979. <span>¬´Systolic arrays (for <span>VLSI)</span>¬ª</span>. In <em>Sparse Matrix Proceedings 1978</em>, 1:256‚Äì82. Society for industrial; applied mathematics Philadelphia, PA, USA.
</div></div><p>Sebbene le TPU possano ridurre drasticamente i tempi di addestramento, presentano anche degli svantaggi. Ad esempio, molte operazioni all‚Äôinterno dei framework di apprendimento automatico (principalmente TensorFlow in questo caso, poich√© la TPU si integra direttamente con esso) non sono supportate dalle TPU. Non possono inoltre supportare operazioni personalizzate dai framework di apprendimento automatico e la progettazione della rete deve essere strettamente allineata alle capacit√† hardware.</p>
<p>Oggi, le GPU NVIDIA dominano il training, supportate da librerie software come <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a>, <a href="https://developer.nvidia.com/cudnn">cuDNN</a> e <a href="https://developer.nvidia.com/tensorrt#:~:text=NVIDIA">TensorRT</a>. I framework includono anche ottimizzazioni per massimizzare le prestazioni su questi tipi di hardware, come l‚Äôeliminazione di connessioni non importanti e la fusione di layer. La combinazione di queste tecniche con l‚Äôaccelerazione hardware fornisce una maggiore efficienza. Per l‚Äôinferenza, l‚Äôhardware si sta spostando sempre di pi√π verso ASIC e SoC ottimizzati. Le TPU di Google accelerano i modelli nei data center, mentre Apple, Qualcomm, la famiglia NVIDIA Jetson e altri ora producono chip ‚Äúmobili‚Äù incentrati sull‚Äôintelligenza artificiale.</p>
<div id="fig-hardware-accelerator" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hardware-accelerator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hardware_accelerator.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-accelerator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.10: Aziende che offrono acceleratori hardware di ML. Fonte: <a href="https://gradientflow.com/one-simple-chart-companies-that-offer-deep-neural-network-accelerators/">Gradient Flow.</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ai_frameworks-advanced" class="level2 page-columns page-full" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="sec-ai_frameworks-advanced"><span class="header-section-number">6.5</span> Funzionalit√† Avanzate</h2>
<p>Oltre a fornire gli strumenti essenziali per il training di modelli di apprendimento automatico, i framework offrono anche funzionalit√† avanzate. Queste funzionalit√† includono la distribuzione del training su diverse piattaforme hardware, la facile messa a punto di grandi modelli pre-addestrati e l‚Äôesemplificazione del ‚Äúfederated learning‚Äù. L‚Äôimplementazione di queste funzionalit√† in modo indipendente sarebbe altamente complessa e richiederebbe molte risorse, ma i framework semplificano questi processi, rendendo le tecniche avanzate di apprendimento automatico pi√π accessibili.</p>
<section id="training-distribuito" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="training-distribuito"><span class="header-section-number">6.5.1</span> Training distribuito</h3>
<p>Poich√© i modelli di apprendimento automatico sono diventati pi√π grandi nel corso degli anni, √® diventato essenziale per i modelli di grandi dimensioni utilizzare pi√π nodi di elaborazione nel processo di training. Questo processo, l‚Äôapprendimento distribuito, ha consentito maggiori capacit√† di training, ma ha anche imposto problemi nell‚Äôimplementazione.</p>
<p>Possiamo considerare tre diversi modi per distribuire il lavoro di training dei modelli di apprendimento automatico su pi√π nodi di elaborazione. Il partizionamento dei dati di input (o parallelismo dei dati) si riferisce a pi√π processori che eseguono lo stesso modello su diverse partizioni di input. Questa √® l‚Äôimplementazione pi√π semplice ed √® disponibile per molti framework di machine learning. La distribuzione pi√π impegnativa del lavoro √® rappresentata dal parallelismo del modello, che si riferisce a pi√π nodi di elaborazione che lavorano su parti diverse del modello, e dal parallelismo del modello pipelined, che si riferisce a pi√π nodi di elaborazione che lavorano su diversi layer del modello sullo stesso input. Gli ultimi due menzionati qui sono aree di ricerca attive.</p>
<p>I framework di ML che supportano l‚Äôapprendimento distribuito includono TensorFlow (tramite il suo modulo <a href="https://www.tensorflow.org/api_docs/python/tf/distribute">tf.distribute</a>), PyTorch (tramite i suoi moduli <a href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html">torch.nn.DataParallel</a> e <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">torch.nn.DistributedDataParallel</a>) e MXNet (tramite la sua API <a href="https://mxnet.apache.org/versions/1.9.1/api/python/docs/api/gluon/index.html">gluon</a>).</p>
</section>
<section id="conversione-del-modello" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="conversione-del-modello"><span class="header-section-number">6.5.2</span> Conversione del Modello</h3>
<p>I modelli di machine learning hanno vari metodi per essere rappresentati e utilizzati in diversi framework e per diversi tipi di dispositivi. Ad esempio, un modello pu√≤ essere convertito per essere compatibile con i framework di inferenza all‚Äôinterno del dispositivo mobile. Il formato di default per i modelli TensorFlow sono i file di checkpoint contenenti pesi e architetture, necessari per riaddestrare i modelli. Tuttavia, i modelli vengono in genere convertiti nel formato TensorFlow Lite per la distribuzione mobile. TensorFlow Lite utilizza una rappresentazione compatta del ‚Äúflat buffer‚Äù e ottimizzazioni per un‚Äôinferenza rapida su hardware mobile, eliminando tutto il bagaglio non necessario associato ai metadati di addestramento, come le strutture dei file di checkpoint.</p>
<p>Le ottimizzazioni del modello come la quantizzazione (vedere il capitolo <a href="../optimizations/optimizations.qmd">Ottimizzazioni</a>) possono ottimizzare ulteriormente i modelli per architetture target come i dispositivi mobili. Ci√≤ riduce la precisione di pesi e attivazioni a <code>uint8</code> o a <code>int8</code> per un ingombro ridotto e un‚Äôesecuzione pi√π rapida con acceleratori hardware supportati. Per la quantizzazione post-training, il convertitore di TensorFlow gestisce automaticamente analisi e conversione.</p>
<p>Framework come TensorFlow semplificano la distribuzione di modelli addestrati su dispositivi IoT mobili ed embedded tramite API di conversione semplici per il formato TFLite e la quantizzazione. La conversione pronta all‚Äôuso consente un‚Äôinferenza ad alte prestazioni su dispositivi mobili senza l‚Äôonere dell‚Äôottimizzazione manuale. Oltre a TFLite, altri target comuni includono TensorFlow.js per la distribuzione Web, TensorFlow Serving per i servizi cloud e TensorFlow Hub per l‚Äôapprendimento tramite trasferimento. Le utility di conversione di TensorFlow gestiscono questi scenari per semplificare i flussi di lavoro end-to-end.</p>
<p>Ulteriori informazioni sulla conversione dei modelli in TensorFlow sono linkate <a href="https://www.tensorflow.org/lite/models/convert">qui</a>.</p>
</section>
<section id="automl-no-codelow-code-ml" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="automl-no-codelow-code-ml"><span class="header-section-number">6.5.3</span> AutoML, No-Code/Low-Code ML</h3>
<p>In molti casi, l‚Äôapprendimento automatico pu√≤ avere una barriera d‚Äôingresso relativamente alta rispetto ad altri campi. Per addestrare e distribuire con successo modelli, √® necessario avere una comprensione critica di una variet√† di discipline, dalla scienza dei dati (elaborazione dei dati, pulizia dei dati), strutture di modelli (ottimizzazione degli iperparametri, architettura delle reti neurali), hardware (accelerazione, elaborazione parallela) e altro a seconda del problema in questione. La complessit√† di questi problemi ha portato all‚Äôintroduzione di framework come AutoML, che cerca di rendere ‚Äúl‚Äôapprendimento automatico disponibile anche a chi non √® esperto di apprendimento automatico‚Äù e di ‚Äúautomatizzare la ricerca nell‚Äôapprendimento automatico‚Äù. Hanno creato AutoWEKA, che aiuta nel complesso processo di selezione degli iperparametri, e Auto-sklearn e Auto-pytorch, un‚Äôestensione di AutoWEKA nelle popolari librerie sklearn e PyTorch.</p>
<p>Mentre questi sforzi per automatizzare parti delle attivit√† di apprendimento automatico sono in corso, altri si sono concentrati sulla semplificazione dei modelli tramite l‚Äôimplementazione di apprendimento automatico ‚Äúno-code‚Äù [senza codice]/low-code [a basso codice], utilizzando un‚Äôinterfaccia drag-and-drop con un‚Äôinterfaccia utente di facile navigazione. Aziende come Apple, Google e Amazon hanno gi√† creato queste piattaforme di facile utilizzo per consentire agli utenti di costruire modelli di apprendimento automatico che possono essere integrati nel loro ecosistema.</p>
<p>Questi passaggi per rimuovere le barriere all‚Äôingresso continuano a democratizzare il machine learning, semplificano l‚Äôaccesso per i principianti e semplificano il flusso di lavoro per gli esperti.</p>
</section>
<section id="metodi-di-apprendimento-avanzati" class="level3 page-columns page-full" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="metodi-di-apprendimento-avanzati"><span class="header-section-number">6.5.4</span> Metodi di Apprendimento Avanzati</h3>
<section id="il-transfer-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="il-transfer-learning">Il Transfer Learning</h4>
<p>Il ‚Äútransfer learning‚Äù √® la pratica di utilizzare le conoscenze acquisite da un modello pre-addestrato per addestrare e migliorare le prestazioni di un modello per un‚Äôattivit√† diversa. Ad esempio, modelli come MobileNet e ResNet vengono addestrati sul set di dati ImageNet. Per fare ci√≤, si pu√≤ congelare il modello pre-addestrato, utilizzandolo come un estrattore di feature per addestrare un modello molto pi√π piccolo costruito sopra l‚Äôestrazione di feature. Si pu√≤ anche mettere a punto l‚Äôintero modello per adattarlo al nuovo compito. I framework di apprendimento automatico semplificano il caricamento di modelli pre-addestrati, il congelamento di layer specifici e l‚Äôaddestramento di layer personalizzati in cima. Semplificano questo processo fornendo API intuitive e un facile accesso a grandi repository di <a href="https://keras.io/api/applications/">modelli pre-addestrati</a>.</p>
<p>L‚Äôapprendimento tramite trasferimento presenta delle sfide, come l‚Äôincapacit√† del modello modificato di svolgere le sue attivit√† originali dopo l‚Äôapprendimento tramite trasferimento. Articoli come <a href="https://browse.arxiv.org/pdf/1606.09282.pdf">‚ÄúLearning without Forgetting‚Äù</a> di <span class="citation" data-cites="li2017learning">Z. Li e Hoiem (<a href="../../references.it.html#ref-li2017learning" role="doc-biblioref">2018</a>)</span> cercano di affrontare queste sfide e sono stati implementati nelle moderne piattaforme di apprendimento automatico. <a href="../robust_ai/robust_ai.it.html#fig-transfer-learning" class="quarto-xref">Figura&nbsp;<span>17.33</span></a> semplifica il concetto di apprendimento tramite un esempio.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017learning" class="csl-entry" role="listitem">
Li, Zhizhong, e Derek Hoiem. 2018. <span>¬´Learning without Forgetting¬ª</span>. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em> 40 (12): 2935‚Äì47. <a href="https://doi.org/10.1109/tpami.2017.2773081">https://doi.org/10.1109/tpami.2017.2773081</a>.
</div></div><div id="fig-transfer-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/transferlearning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.11: Trasferimento dell‚Äôapprendimento. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fanalyticsindiamag.com%2Fdevelopers-corner%2Fcomplete-guide-to-understanding-precision-and-recall-curves%2F&amp;psig=AOvVaw3MosZItazJt2eermLTArjj&amp;ust=1722534897757000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCIi389bs0YcDFQAAAAAdAAAAABAw">Tech Target</a>
</figcaption>
</figure>
</div>
</section>
<section id="il-federated-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="il-federated-learning">Il Federated Learning</h4>
<p>Il ‚ÄúFederated learning‚Äù di <span class="citation" data-cites="mcmahan2023communicationefficient">McMahan et al. (<a href="../../references.it.html#ref-mcmahan2023communicationefficient" role="doc-biblioref">2017</a>)</span> √® una forma di elaborazione distribuita che prevede l‚Äôaddestramento di modelli su dispositivi personali anzich√© la centralizzazione dei dati su un singolo server (<a href="../ondevice_learning/ondevice_learning.it.html#fig-federated-learning" class="quarto-xref">Figura&nbsp;<span>12.7</span></a>). Inizialmente, un modello globale di base viene addestrato su un server centrale per essere distribuito a tutti i dispositivi. Utilizzando questo modello di base, i dispositivi calcolano individualmente i gradienti e li inviano all‚Äôhub centrale. Intuitivamente, questo trasferisce i parametri del modello anzich√© i dati stessi. L‚Äôapprendimento federato migliora la privacy mantenendo i dati sensibili sui dispositivi locali e condividendo gli aggiornamenti del modello solo con un server centrale. Questo metodo √® particolarmente utile quando si gestiscono dati sensibili o quando un‚Äôinfrastruttura su larga scala non √® praticabile.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mcmahan2023communicationefficient" class="csl-entry" role="listitem">
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, e Blaise Ag√ºera y Arcas. 2017. <span>¬´Communication-Efficient Learning of Deep Networks from Decentralized Data¬ª</span>. In <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA</em>, a cura di Aarti Singh e Xiaojin (Jerry) Zhu, 54:1273‚Äì82. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.
</div></div><div id="fig-federated-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-federated-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/federated_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-federated-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.12: Un approccio con server centralizzato al ‚Äúfederated learning‚Äù. Fonte: <a href="https://blogs.nvidia.com/blog/what-is-federated-learning/">NVIDIA.</a>
</figcaption>
</figure>
</div>
<p>Tuttavia, il federated learning deve affrontare sfide come garantire l‚Äôaccuratezza dei dati, gestire dati non-IID (independent and identically distributed) [indipendenti e distribuiti in modo identico], gestire la produzione di dati non bilanciata e superare il sovraccarico della comunicazione e l‚Äôeterogeneit√† dei dispositivi. Anche i problemi di privacy e sicurezza, come gli attacchi di inversione del gradiente, pongono sfide significative.</p>
<p>I framework di apprendimento automatico semplificano l‚Äôimplementazione dell‚Äôapprendimento federato fornendo gli strumenti e le librerie necessarie. Ad esempio, <a href="https://www.tensorflow.org/federated">TensorFlow Federated (TFF)</a> offre un framework open source per supportare l‚Äôapprendimento federato. TFF consente agli sviluppatori di simulare e implementare algoritmi di apprendimento federato, offrendo un core federato per operazioni di basso livello e API di alto livello per attivit√† federate comuni. Si integra perfettamente con TensorFlow, consentendo l‚Äôuso di modelli e ottimizzatori TensorFlow in un ambiente federato. TFF supporta tecniche di aggregazione sicure per migliorare la privacy e consente la personalizzazione degli algoritmi di apprendimento federato. Sfruttando questi strumenti, gli sviluppatori possono distribuire in modo efficiente il training, perfezionare i modelli pre-addestrati e gestire le complessit√† intrinseche dell‚Äôapprendimento federato.</p>
<p>Sono stati sviluppati anche altri programmi open source come <a href="https://flower.dev/">Flower</a> per semplificare l‚Äôimplementazione dell‚Äôapprendimento federato con vari framework di machine learning.</p>
</section>
</section>
</section>
<section id="specializzazione-del-framework" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="specializzazione-del-framework"><span class="header-section-number">6.6</span> Specializzazione del Framework</h2>
<p>Finora abbiamo parlato in generale dei framework di ML. Tuttavia, in genere, i framework sono ottimizzati in base alle capacit√† computazionali e ai requisiti applicativi dell‚Äôambiente target, che vanno dal cloud all‚Äôedge ai dispositivi minuscoli. La scelta del framework giusto √® fondamentale in base all‚Äôambiente target per la distribuzione. Questa sezione fornisce una panoramica dei principali tipi di framework di IA su misura per ambienti cloud, edge e TinyML per aiutare a comprendere le somiglianze e le differenze tra questi ecosistemi.</p>
<section id="cloud" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="cloud"><span class="header-section-number">6.6.1</span> Cloud</h3>
<p>I framework di IA basati su cloud presuppongono l‚Äôaccesso a un‚Äôampia potenza di calcolo, memoria e risorse di archiviazione nel cloud. In genere supportano sia il training che l‚Äôinferenza. I framework di IA basati su cloud sono adatti per applicazioni in cui i dati possono essere inviati al cloud per l‚Äôelaborazione, come servizi di IA basati su cloud, analisi di dati su larga scala e applicazioni Web. I framework di IA cloud pi√π diffusi includono quelli che abbiamo menzionato in precedenza, come TensorFlow, PyTorch, MXNet, Keras, ecc. Questi framework utilizzano GPU, TPU, training distribuito e AutoML per fornire IA scalabile. Concetti come model serving, MLOps e AIOps sono correlati all‚Äôoperativit√† dell‚ÄôIA nel cloud. L‚ÄôIA cloud alimenta servizi come Google Cloud AI e consente il ‚Äútransfer learning‚Äù tramite modelli pre-addestrati.</p>
</section>
<section id="edge" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="edge"><span class="header-section-number">6.6.2</span> Edge</h3>
<p>I framework Edge AI sono pensati per distribuire modelli di IA su dispositivi IoT, smartphone e server edge. I framework Edge AI sono ottimizzati per dispositivi con risorse di calcolo moderate, bilanciando potenza e prestazioni. I framework Edge AI sono ideali per applicazioni che richiedono elaborazione in tempo reale o quasi reale, tra cui robotica, veicoli autonomi e dispositivi intelligenti. I principali framework Edge AI includono TensorFlow Lite, PyTorch Mobile, CoreML e altri. Impiegano ottimizzazioni come compressione del modello, quantizzazione ed architetture di reti neurali efficienti. Il supporto hardware include CPU, GPU, NPU e acceleratori come Edge TPU. Edge AI consente casi d‚Äôuso come visione mobile, riconoscimento vocale e rilevamento di anomalie in tempo reale.</p>
</section>
<section id="embedded" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="embedded"><span class="header-section-number">6.6.3</span> Embedded</h3>
<p>I framework TinyML sono specializzati per distribuire modelli AI su dispositivi con risorse estremamente limitate, in particolare microcontrollori e sensori all‚Äôinterno dell‚Äôecosistema IoT. I framework TinyML sono progettati per dispositivi con risorse limitate, enfatizzando memoria minima e consumo energetico. I framework TinyML sono specializzati per casi d‚Äôuso su dispositivi IoT con risorse limitate per applicazioni di manutenzione predittiva, riconoscimento dei gesti e monitoraggio ambientale. I principali framework TinyML includono TensorFlow Lite Micro, uTensor e ARM NN. Ottimizzano modelli complessi per adattarli a kilobyte di memoria tramite tecniche come l‚Äôaddestramento consapevole della quantizzazione e la precisione ridotta. TinyML consente il rilevamento intelligente su dispositivi alimentati a batteria, consentendo l‚Äôapprendimento collaborativo tramite apprendimento federato. La scelta del framework implica il bilanciamento delle prestazioni del modello e dei vincoli computazionali della piattaforma target, che sia cloud, edge o TinyML. <a href="#tbl-ml_frameworks" class="quarto-xref">Tabella&nbsp;<span>6.3</span></a> confronta i principali framework di IA negli ambienti cloud, edge e TinyML:</p>
<div id="tbl-ml_frameworks" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ml_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;6.3: Confronto dei tipi di framework per Cloud AI, Edge AI e TinyML.
</figcaption>
<div aria-describedby="tbl-ml_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 17%">
<col style="width: 38%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Tipo di framework</th>
<th style="text-align: left;">Esempi</th>
<th style="text-align: left;">Tecnologie chiave</th>
<th style="text-align: left;">Casi d‚Äôuso</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Cloud AI</td>
<td style="text-align: left;">TensorFlow, PyTorch, MXNet, Keras</td>
<td style="text-align: left;">GPU, TPU, addestramento distribuito, AutoML, MLOps</td>
<td style="text-align: left;">Servizi cloud, app Web, analisi di big data</td>
</tr>
<tr class="even">
<td style="text-align: left;">Edge AI</td>
<td style="text-align: left;">TensorFlow Lite, PyTorch Mobile, Core ML</td>
<td style="text-align: left;">Ottimizzazione del modello, compressione, quantizzazione, architetture NN efficienti</td>
<td style="text-align: left;">App mobili, sistemi autonomi, elaborazione in tempo reale</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TinyML</td>
<td style="text-align: left;">TensorFlow Lite Micro, uTensor, ARM NN</td>
<td style="text-align: left;">Training consapevole della quantizzazione, precisione ridotta, ricerca di architettura neurale</td>
<td style="text-align: left;">Sensori IoT, dispositivi indossabili, manutenzione predittiva, riconoscimento dei gesti</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Differenze principali:</strong></p>
<ul>
<li><p>Cloud AI sfrutta un‚Äôenorme potenza di calcolo per modelli complessi utilizzando GPU/TPU e training distribuito.</p></li>
<li><p>Edge AI ottimizza i modelli per l‚Äôesecuzione locale su dispositivi edge con risorse limitate.</p></li>
<li><p>TinyML adatta i modelli a una memoria estremamente bassa e calcola ambienti come i microcontrollori.</p></li>
</ul>
</section>
</section>
<section id="sec-ai_frameworks_embedded" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sec-ai_frameworks_embedded"><span class="header-section-number">6.7</span> Framework di IA Embedded</h2>
<section id="vincoli-di-risorse" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="vincoli-di-risorse"><span class="header-section-number">6.7.1</span> Vincoli di Risorse</h3>
<p>I sistemi embedded affrontano gravi limitazioni di risorse che pongono sfide uniche quando si distribuiscono modelli di machine learning rispetto alle piattaforme di elaborazione tradizionali. Ad esempio, le unit√† microcontrollore (MCU) comunemente utilizzate nei dispositivi IoT hanno spesso:</p>
<ul>
<li><p><strong>RAM</strong> varia da decine di kilobyte a pochi megabyte. Il popolare <a href="https://www.espressif.com/en/products/socs/esp8266">MCU ESP8266</a> ha circa 80 KB di RAM a disposizione degli sviluppatori. Ci√≤ contrasta con 8 GB o pi√π su laptop e desktop tipici odierni.</p></li>
<li><p><strong>Memoria Flash</strong> varia da centinaia di kilobyte a pochi megabyte. Il microcontrollore Arduino Uno fornisce solo 32 KB di archiviazione del codice. I computer standard odierni hanno un‚Äôarchiviazione su disco nell‚Äôordine dei terabyte.</p></li>
<li><p><strong>Potenza di elaborazione</strong> da pochi MHz a circa 200 MHz. L‚ÄôESP8266 funziona a 80 MHz. Questo √® di diversi ordini di grandezza pi√π lento delle CPU multi-core multi-GHz nei server e nei laptop di fascia alta.</p></li>
</ul>
<p>Questi vincoli rigorosi spesso rendono impossibile l‚Äôaddestramento di modelli di apprendimento automatico direttamente sui microcontrollori. La RAM limitata impedisce la gestione di grandi set di dati per il training. L‚Äôuso di energia per l‚Äôaddestramento esaurirebbe rapidamente anche i dispositivi alimentati a batteria. Al contrario, i modelli vengono addestrati su sistemi ricchi di risorse e distribuiti su microcontrollori per un‚Äôinferenza ottimizzata. Ma anche l‚Äôinferenza pone delle sfide:</p>
<ol type="1">
<li><p><strong>Dimensioni del Modello:</strong> I modelli di intelligenza artificiale sono troppo grandi per adattarsi a dispositivi IoT ed embedded. Ci√≤ richiede tecniche di compressione del modello, come quantizzazione, potatura e ‚Äúknowledge distillation‚Äù [distillazione della conoscenza]. Inoltre, come vedremo, molti dei framework utilizzati dagli sviluppatori di intelligenza artificiale hanno grandi quantit√† di overhead e librerie integrate che i sistemi embedded non possono supportare.</p></li>
<li><p><strong>Complessit√† delle Attivit√†:</strong> Con solo decine di KB o pochi MB di RAM, i dispositivi IoT e i sistemi embedded sono limitati nella complessit√† delle attivit√† che possono gestire. Le attivit√† che richiedono grandi set di dati o algoritmi sofisticati, ad esempio LLM, che verrebbero eseguiti senza problemi su piattaforme di elaborazione tradizionali potrebbero non essere fattibili su sistemi embedded senza compressione o altre tecniche di ottimizzazione a causa delle limitazioni di memoria.</p></li>
<li><p><strong>Archiviazione ed Elaborazione dei Dati:</strong> I sistemi embedded spesso elaborano i dati in tempo reale e potrebbero archiviarne solo piccole quantit√† localmente. Al contrario, i sistemi di elaborazione tradizionali possono contenere ed elaborare grandi set di dati in memoria, consentendo un‚Äôanalisi pi√π rapida delle operazioni sui dati e aggiornamenti in tempo reale.</p></li>
<li><p><strong>Sicurezza e Privacy:</strong> La poca memoria limita anche la complessit√† degli algoritmi e dei protocolli di sicurezza, la crittografia dei dati, le protezioni da reverse engineering e altro che pu√≤ essere implementato sul dispositivo. Ci√≤ potrebbe rendere alcuni dispositivi IoT pi√π vulnerabili agli attacchi.</p></li>
</ol>
<p>Di conseguenza, le ottimizzazioni software specializzate e i framework ML su misura per i microcontrollori devono funzionare entro questi stretti limiti delle risorse. Tecniche di ottimizzazione intelligenti come quantizzazione, potatura e distillazione della conoscenza comprimono i modelli per adattarli alla memoria limitata (vedere la sezione Ottimizzazioni). Gli insegnamenti tratti dalla ricerca di architettura neurale aiutano a guidare la progettazione dei modelli.</p>
<p>I miglioramenti hardware come gli acceleratori ML dedicati sui microcontrollori aiutano anche ad alleviare i vincoli. Ad esempio, <a href="https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor">Hexagon DSP di Qualcomm</a> accelera i modelli TensorFlow Lite sui chip mobili Snapdragon. <a href="https://cloud.google.com/edge-tpu">Google Edge TPU</a> racchiude le prestazioni ML in un piccolo ASIC per dispositivi edge. <a href="https://www.arm.com/products/silicon-ip-cpu/ethos/ethos-u55">ARM Ethos-U55</a> offre un‚Äôinferenza efficiente sui microcontrollori di classe Cortex-M. Questi chip ML personalizzati sbloccano funzionalit√† avanzate per applicazioni con risorse limitate.</p>
<p>A causa della potenza di elaborazione limitata, √® quasi sempre impossibile addestrare modelli di intelligenza artificiale su IoT o sistemi embedded. Invece, i modelli vengono addestrati su potenti computer tradizionali (spesso con GPU) e poi distribuiti sul dispositivo embedded per l‚Äôinferenza. TinyML si occupa specificamente di questo, assicurando che i modelli siano sufficientemente leggeri per l‚Äôinferenza in tempo reale su questi dispositivi limitati.</p>
</section>
<section id="framework-e-librerie" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="framework-e-librerie"><span class="header-section-number">6.7.2</span> Framework e Librerie</h3>
<p>I framework di intelligenza artificiale embedded sono strumenti software e librerie progettati per abilitare funzionalit√† di intelligenza artificiale e ML su sistemi embedded. Questi framework sono essenziali per portare l‚Äôintelligenza artificiale su dispositivi IoT, robotica e altre piattaforme di edge computing e sono progettati per funzionare dove risorse di elaborazione, memoria e consumo energetico sono limitati.</p>
</section>
<section id="sfide" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="sfide"><span class="header-section-number">6.7.3</span> Sfide</h3>
<p>Sebbene i sistemi embedded rappresentino un‚Äôenorme opportunit√† per l‚Äôimplementazione dell‚Äôapprendimento automatico per abilitare capacit√† intelligenti in edge, questi ambienti con risorse limitate pongono sfide significative. A differenza dei tipici ambienti cloud o desktop ricchi di risorse computazionali, i dispositivi embedded introducono gravi limitazioni in termini di memoria, potenza di elaborazione, efficienza energetica e hardware specializzato. Di conseguenza, le tecniche e i framework di apprendimento automatico esistenti progettati per cluster di server con risorse abbondanti non si traducono direttamente nei sistemi embedded. Questa sezione svela alcune delle sfide e delle opportunit√† per i sistemi embedded e i framework ML.</p>
<section id="ecosistema-frammentato" class="level4">
<h4 class="anchored" data-anchor-id="ecosistema-frammentato">Ecosistema Frammentato</h4>
<p>La mancanza di un framework ML unificato ha portato a un ecosistema altamente frammentato. Gli ingegneri di aziende come <a href="https://www.st.com/">STMicroelectronics</a>, <a href="https://www.nxp.com/">NXP Semiconductors</a> e <a href="https://www.renesas.com/">Renesas</a> hanno dovuto sviluppare soluzioni personalizzate su misura per le loro specifiche architetture di microcontrollori e DSP. Questi framework ad hoc richiedevano un‚Äôampia ottimizzazione manuale per ogni piattaforma hardware di basso livello. Ci√≤ ha reso estremamente difficile il porting dei modelli, richiedendo la riqualificazione per nuove architetture Arm, RISC-V o proprietarie.</p>
</section>
<section id="esigenze-hardware-disparate" class="level4">
<h4 class="anchored" data-anchor-id="esigenze-hardware-disparate">Esigenze Hardware Disparate</h4>
<p>Senza un framework condiviso, non esisteva un modo standard per valutare le capacit√† dell‚Äôhardware. Fornitori come Intel, Qualcomm e NVIDIA crearono soluzioni integrate, combinando modelli e migliorando software e hardware. Ci√≤ rese difficile discernere i motivi del guadagni di prestazioni, se fosse merito dei nuovi progetti di chip come i core x86 a basso consumo di Intel o le ottimizzazioni software. Era necessario un framework standard affinch√© i fornitori potessero valutare le capacit√† del loro hardware in modo equo e riproducibile.</p>
</section>
<section id="mancanza-di-portabilit√†" class="level4">
<h4 class="anchored" data-anchor-id="mancanza-di-portabilit√†">Mancanza di Portabilit√†</h4>
<p>Con strumenti standardizzati, adattare modelli addestrati in framework comuni come TensorFlow o PyTorch per funzionare in modo efficiente sui microcontrollori era pi√π facile. Richiedeva una traduzione manuale dispendiosa, in termini di tempo, dei modelli per l‚Äôesecuzione su DSP specializzati di aziende come CEVA o core Arm M-series a basso consumo. Nessuno strumento immediato consentiva l‚Äôimplementazione portatile su diverse architetture.</p>
</section>
<section id="infrastruttura-incompleta" class="level4">
<h4 class="anchored" data-anchor-id="infrastruttura-incompleta">Infrastruttura Incompleta</h4>
<p>L‚Äôinfrastruttura per supportare i flussi di lavoro di sviluppo dei modelli chiave doveva essere migliorata. √à necessario un maggiore supporto per le tecniche di compressione per adattare modelli di grandi dimensioni a budget di memoria limitati. Mancavano strumenti per la quantizzazione per ridurre la precisione per un‚Äôinferenza pi√π rapida. Le API standardizzate per l‚Äôintegrazione nelle applicazioni erano incomplete. Mancavano funzionalit√† essenziali come il debugging sul dispositivo, le metriche e la profilazione delle prestazioni. Queste lacune hanno aumentato i costi e la difficolt√† dello sviluppo ML embedded.</p>
</section>
<section id="nessun-benchmark-standard" class="level4">
<h4 class="anchored" data-anchor-id="nessun-benchmark-standard">Nessun Benchmark Standard</h4>
<p>Senza benchmark unificati, non esisteva un modo standard per valutare e confrontare le capacit√† di diverse piattaforme hardware di fornitori come NVIDIA, Arm e Ambiq Micro.. Le valutazioni esistenti si basavano su benchmark proprietari pensati per mostrare i punti di forza di specifici chip. Ci√≤ rendeva impossibile misurare i miglioramenti hardware in modo oggettivo, imparziale e imparziale. Il capitolo <a href="../benchmarking/benchmarking.qmd">Benchmarking dell‚ÄôIA</a> affronta questo argomento in modo pi√π dettagliato.</p>
</section>
<section id="test-minimi-del-mondo-reale" class="level4">
<h4 class="anchored" data-anchor-id="test-minimi-del-mondo-reale">Test Minimi del Mondo Reale</h4>
<p>Gran parte dei benchmark si basava su dati sintetici. Testare rigorosamente i modelli su applicazioni embedded nel mondo reale era difficile senza set di dati e benchmark standardizzati, sollevando dubbi su come le dichiarazioni sulle prestazioni si sarebbero tradotte in un utilizzo nel mondo reale. Erano necessari test pi√π approfonditi per convalidare i chip in casi di utilizzo reali.</p>
<p>La mancanza di framework e infrastrutture condivisi ha rallentato l‚Äôadozione di TinyML, ostacolandone l‚Äôintegrazione nei prodotti embedded. I recenti framework standard hanno iniziato ad affrontare questi problemi attraverso una migliore portabilit√†, profilazione delle prestazioni e supporto per il benchmarking. Tuttavia, √® ancora necessaria un‚Äôinnovazione continua per consentire un‚Äôimplementazione fluida e conveniente dell‚ÄôIA nei dispositivi edge.</p>
</section>
<section id="riepilogo" class="level4">
<h4 class="anchored" data-anchor-id="riepilogo">Riepilogo</h4>
<p>L‚Äôassenza di framework, benchmark e infrastrutture standardizzati per ML embedded ne ha tradizionalmente ostacolato l‚Äôadozione. Tuttavia, sono stati compiuti recenti progressi nello sviluppo di framework condivisi come TensorFlow Lite Micro e suite di benchmark come MLPerf Tiny che mirano ad accelerare la proliferazione di soluzioni TinyML. Tuttavia, superare la frammentazione e la difficolt√† dell‚Äôimplementazione embedded rimane un processo in corso.</p>
</section>
</section>
</section>
<section id="esempi" class="level2 page-columns page-full" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="esempi"><span class="header-section-number">6.8</span> Esempi</h2>
<p>Il deployment [distribuzione] di machine learning su microcontrollori e altri dispositivi embedded richiede spesso librerie software e framework appositamente ottimizzati per funzionare entro vincoli rigorosi di memoria, elaborazione e potenza. Esistono diverse opzioni per eseguire l‚Äôinferenza su hardware con risorse limitate, ciascuna con il proprio approccio all‚Äôottimizzazione dell‚Äôesecuzione del modello. Questa sezione esplorer√† le caratteristiche chiave e i principi di progettazione alla base di TFLite Micro, TinyEngine e CMSIS-NN, fornendo informazioni su come ogni framework affronta il complesso problema dell‚Äôesecuzione di reti neurali molto accurata ma efficiente sui microcontrollori. Mostrer√† inoltre diversi approcci per l‚Äôimplementazione di framework TinyML efficienti.</p>
<p><a href="#tbl-compare_frameworks" class="quarto-xref">Tabella&nbsp;<span>6.4</span></a> riassume le principali differenze e somiglianze tra questi tre framework di inferenza di apprendimento automatico specializzati per sistemi embedded e microcontrollori.</p>
<div id="tbl-compare_frameworks" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-compare_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;6.4: Confronto dei framework: TensorFlow Lite Micro, TinyEngine e CMSIS-NN
</figcaption>
<div aria-describedby="tbl-compare_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 24%">
<col style="width: 27%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Framework</th>
<th style="text-align: left;">TensorFlow Lite Micro</th>
<th style="text-align: left;">TinyEngine</th>
<th style="text-align: left;">CMSIS-NN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Approccio</td>
<td style="text-align: left;">Basato su interprete</td>
<td style="text-align: left;">Compilazione statica</td>
<td style="text-align: left;">Kernel di reti neurali ottimizzati</td>
</tr>
<tr class="even">
<td style="text-align: left;">Focus sull‚Äôhardware</td>
<td style="text-align: left;">Dispositivi embedded generali</td>
<td style="text-align: left;">Microcontrollori</td>
<td style="text-align: left;">Processori ARM Cortex-M</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Supporto aritmetico</td>
<td style="text-align: left;">Virgola mobile</td>
<td style="text-align: left;">Virgola mobile, virgola fissa</td>
<td style="text-align: left;">Virgola mobile, virgola fissa</td>
</tr>
<tr class="even">
<td style="text-align: left;">Supporto del modello</td>
<td style="text-align: left;">Modelli di rete neurale generale</td>
<td style="text-align: left;">Modelli co-progettati con TinyNAS</td>
<td style="text-align: left;">Tipi di livelli di rete neurale comuni</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Impronta del codice</td>
<td style="text-align: left;">Pi√π grande grazie all‚Äôinclusione di interprete e operazioni</td>
<td style="text-align: left;">Piccola, include solo le operazioni necessarie per il modello</td>
<td style="text-align: left;">Nativamente leggera</td>
</tr>
<tr class="even">
<td style="text-align: left;">Latenza</td>
<td style="text-align: left;">Pi√π alta grazie a overhead di interpretazione</td>
<td style="text-align: left;">Molto bassa grazie al modello compilato</td>
<td style="text-align: left;">focalizzato sulla bassa latenza</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Gestione della memoria</td>
<td style="text-align: left;">Gestita dinamicamente da interprete</td>
<td style="text-align: left;">Ottimizzazione a livello di modello</td>
<td style="text-align: left;">Strumenti per un‚Äôallocazione efficiente</td>
</tr>
<tr class="even">
<td style="text-align: left;">Approccio di ottimizzazione</td>
<td style="text-align: left;">Alcune funzionalit√† di e generazione del codice</td>
<td style="text-align: left;">Kernel specializzati, fusione di operatori</td>
<td style="text-align: left;">Ottimizzazioni di assemblaggio specifiche dell‚Äôarchitettura</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Principali vantaggi</td>
<td style="text-align: left;">Flessibilit√†, portabilit√†, facile aggiornamento dei modelli</td>
<td style="text-align: left;">Massimizza le prestazioni, ottimizza l‚Äôutilizzo della memoria</td>
<td style="text-align: left;">Accelerazione hardware, API standardizzata, portabilit√†</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Ne comprenderemo ciascuno in modo pi√π dettagliato nelle sezioni seguenti.</p>
<section id="interprete" class="level3 page-columns page-full" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="interprete"><span class="header-section-number">6.8.1</span> Interprete</h3>
<p><a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite Micro (TFLM)</a> √® un framework di inferenza di apprendimento automatico progettato per dispositivi embedded con risorse limitate. Utilizza un interprete per caricare ed eseguire modelli di apprendimento automatico, il che fornisce flessibilit√† e facilit√† di aggiornamento dei modelli sul campo <span class="citation" data-cites="david2021tensorflow">(<a href="../../references.it.html#ref-david2021tensorflow" role="doc-biblioref">David et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-david2021tensorflow" class="csl-entry" role="listitem">
David, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. <span>¬´Tensorflow lite micro: <span>Embedded</span> machine learning for tinyml systems¬ª</span>. <em>Proceedings of Machine Learning and Systems</em> 3: 800‚Äì811.
</div></div><p>Gli interpreti tradizionali spesso hanno un overhead di branching [diramazione] significativo, che pu√≤ ridurre le prestazioni. Tuttavia, l‚Äôinterpretazione del modello di machine learning trae vantaggio dall‚Äôefficienza dei kernel di lunga durata, in cui ogni runtime del kernel √® relativamente grande e aiuta a mitigare l‚Äôoverhead dell‚Äôinterprete.</p>
<p>Un‚Äôalternativa a un motore di inferenza basato su interprete √® quella di generare codice nativo da un modello durante l‚Äôesportazione. Ci√≤ pu√≤ migliorare le prestazioni, ma sacrifica portabilit√† e flessibilit√†, poich√© il codice generato deve essere ricompilato per ogni piattaforma target e deve essere sostituito completamente per modificare un modello.</p>
<p>TFLM bilancia la semplicit√† della compilazione del codice e la flessibilit√† di un approccio basato su interprete includendo alcune funzionalit√† di generazione del codice. Ad esempio, la libreria pu√≤ essere costruita esclusivamente da file sorgenti, offrendo gran parte della semplicit√† della compilazione associata alla generazione di codice, pur mantenendo i vantaggi di un framework che esegue il modello interpretandolo.</p>
<p>Un approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice per l‚Äôinferenza di apprendimento automatico su dispositivi embedded:</p>
<ul>
<li><p><strong>Flessibilit√†:</strong> I modelli possono essere aggiornati sul campo senza ricompilare l‚Äôintera applicazione.</p></li>
<li><p><strong>Portabilit√†:</strong> L‚Äôinterprete pu√≤ essere utilizzato per eseguire modelli su diverse piattaforme target senza dover effettuare il porting del codice.</p></li>
<li><p><strong>Efficienza della Memoria:</strong> L‚Äôinterprete pu√≤ condividere il codice su pi√π modelli, riducendo l‚Äôutilizzo della memoria.</p></li>
<li><p><strong>Facilit√† di sviluppo:</strong> Gli interpreti sono pi√π facili da sviluppare e gestire rispetto ai generatori di codice.</p></li>
</ul>
<p>TensorFlow Lite Micro √® un framework potente e flessibile per l‚Äôinferenza di apprendimento automatico su dispositivi embedded. Il suo approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice, tra cui flessibilit√†, portabilit√†, efficienza della memoria e facilit√† di sviluppo.</p>
</section>
<section id="basati-su-compilatore" class="level3 page-columns page-full" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="basati-su-compilatore"><span class="header-section-number">6.8.2</span> Basati su Compilatore</h3>
<p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine</a> √® un framework di inferenza ML progettato specificamente per microcontrollori con risorse limitate. Utilizza diverse ottimizzazioni per consentire l‚Äôesecuzione di reti neurali molto accurate entro i vincoli rigorosi di memoria, elaborazione e archiviazione sui microcontrollori <span class="citation" data-cites="lin2020mcunet">(<a href="../../references.it.html#ref-lin2020mcunet" role="doc-biblioref">Lin et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. <span>¬´<span>MCUNet:</span> <span>Tiny</span> Deep Learning on <span>IoT</span> Devices¬ª</span>. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, a cura di Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html</a>.
</div></div><p>Mentre framework di inferenza come TFLite Micro utilizzano interpreti per eseguire il grafo della rete neurale in modo dinamico in fase di esecuzione, ci√≤ aggiunge un overhead significativo per quanto riguarda l‚Äôutilizzo della memoria per archiviare metadati, latenza di interpretazione e mancanza di ottimizzazioni. Tuttavia, TFLite sostiene che l‚Äôoverhead √® piccolo. TinyEngine elimina questo overhead utilizzando un approccio di generazione del codice. Analizza il grafo di rete durante la compilazione e genera codice specializzato per eseguire solo quel modello. Questo codice viene compilato in modo nativo nel binario dell‚Äôapplicazione, evitando i costi di interpretazione in fase di esecuzione.</p>
<p>I framework ML convenzionali pianificano la memoria per layer, cercando di ridurre al minimo l‚Äôutilizzo per ogni layer separatamente. TinyEngine esegue la pianificazione a livello di modello anzich√© analizzare l‚Äôutilizzo della memoria tra i layer. Assegna una dimensione di buffer comune in base alle esigenze massime di memoria di tutti i layer. Questo buffer viene quindi condiviso in modo efficiente tra i layer per aumentare il riutilizzo dei dati.</p>
<p>TinyEngine √® inoltre specializzato nei kernel per ogni layer tramite tecniche come operatori di tiling, unrolling e fusing. Ad esempio, generer√† kernel di calcolo unrolled [srotolato] con il numero di loop necessari per una convoluzione 3x3 o 5x5. Questi kernel specializzati estraggono le massime prestazioni dall‚Äôhardware del microcontrollore. Utilizza convoluzioni depthwise [in profondit√†] ottimizzate per ridurre al minimo le allocazioni di memoria calcolando l‚Äôoutput di ogni canale posizionato sui dati del canale di input. Questa tecnica sfrutta la natura separabile dei canali delle convoluzioni depthwise per ridurre le dimensioni di picco della memoria.</p>
<p>Come TFLite Micro, il binario TinyEngine compilato include solo le operazioni necessarie per un modello specifico anzich√© tutte le operazioni possibili. Ci√≤ si traduce in un footprint binario molto piccolo, mantenendo basse le dimensioni del codice per i dispositivi con limiti di memoria.</p>
<p>Una differenza tra TFLite Micro e TinyEngine √® che quest‚Äôultimo √® co-progettato con ‚ÄúTinyNAS‚Äù, un metodo di ricerca di architettura per modelli di microcontrollori simile al NAS differenziale per microcontrollori. L‚Äôefficienza di TinyEngine consente di esplorare modelli pi√π grandi e accurati tramite NAS. Fornisce inoltre feedback a TinyNAS su quali modelli possono rientrare nei vincoli hardware.</p>
<p>Attraverso varie tecniche personalizzate, come la compilazione statica, la pianificazione basata sul modello, kernel specializzati e la co-progettazione con NAS, TinyEngine consente un‚Äôinferenza di deep learning ad alta precisione entro i vincoli di risorse rigorosi dei microcontrollori.</p>
</section>
<section id="libreria" class="level3 page-columns page-full" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="libreria"><span class="header-section-number">6.8.3</span> Libreria</h3>
<p><a href="https://www.keil.com/pack/doc/CMSIS/NN/html/index.html">CMSIS-NN</a>, acronimo di Cortex Microcontroller Software Interface Standard for Neural Networks, √® una libreria software ideata da ARM. Offre un‚Äôinterfaccia standardizzata per distribuire l‚Äôinferenza di reti neurali su microcontrollori e sistemi embedded, concentrandosi sull‚Äôottimizzazione per i processori ARM Cortex-M <span class="citation" data-cites="lai2018cmsis">(<a href="../../references.it.html#ref-lai2018cmsis" role="doc-biblioref">Lai, Suda, e Chandra 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lai2018cmsis" class="csl-entry" role="listitem">
Lai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. <span>¬´Cmsis-nn: <span>Efficient</span> neural network kernels for arm cortex-m cpus¬ª</span>. <em>ArXiv preprint</em> abs/1801.06601. <a href="https://arxiv.org/abs/1801.06601">https://arxiv.org/abs/1801.06601</a>.
</div></div><p><strong>Kernel di Reti Neurali:</strong> CMSIS-NN ha kernel altamente efficienti che gestiscono operazioni fondamentali di reti neurali come convoluzione, pooling, layer completamente connessi e funzioni di attivazione. Si rivolge a un‚Äôampia gamma di modelli di reti neurali supportando l‚Äôaritmetica a virgola mobile e fissa. Quest‚Äôultima √® particolarmente utile per i dispositivi con risorse limitate in quanto riduce i requisiti di memoria e di calcolo (Quantization).</p>
<p><strong>Accelerazione Hardware:</strong> CMSIS-NN sfrutta la potenza delle istruzioni SIMD (Single Instruction, Multiple Data) disponibili su molti processori Cortex-M. Ci√≤ consente l‚Äôelaborazione parallela di pi√π elementi di dati all‚Äôinterno di una singola istruzione, aumentando cos√¨ l‚Äôefficienza computazionale. Alcuni processori Cortex-M dispongono di estensioni di Digital Signal Processing (DSP) che CMSIS-NN pu√≤ sfruttare per l‚Äôesecuzione accelerata della rete neurale. La libreria include anche ottimizzazioni a livello di assembly su misura per specifiche architetture di microcontrollori per migliorare ulteriormente le prestazioni.</p>
<p><strong>API standardizzata:</strong> CMSIS-NN offre un‚ÄôAPI coerente e astratta che protegge gli sviluppatori dalle complessit√† dei dettagli hardware di basso livello. Ci√≤ semplifica l‚Äôintegrazione dei modelli di rete neurale nelle applicazioni. Pu√≤ anche comprendere strumenti o utilit√† per convertire i formati di modelli di rete neurale pi√π diffusi in un formato compatibile con CMSIS-NN.</p>
<p><strong>Gestione della Memoria:</strong> CMSIS-NN fornisce funzioni per un‚Äôallocazione e una gestione efficienti della memoria, il che √® fondamentale nei sistemi embedded in cui le risorse di memoria sono scarse. Garantisce un utilizzo ottimale della memoria durante l‚Äôinferenza e, in alcuni casi, consente operazioni in loco per ridurre il sovraccarico di memoria.</p>
<p><strong>Portabilit√†:</strong> CMSIS-NN √® progettato per la portabilit√† su vari processori Cortex-M. Questo consente agli sviluppatori di scrivere codice che possa funzionare su diversi microcontrollori senza modifiche significative.</p>
<p><strong>Bassa Latenza:</strong> CMSIS-NN riduce al minimo la latenza di inferenza, rendendolo una scelta ideale per applicazioni in tempo reale in cui √® fondamentale prendere decisioni rapide.</p>
<p><strong>Efficienza Energetica:</strong> La libreria √® progettata con un focus sull‚Äôefficienza energetica, rendendola adatta per dispositivi alimentati a batteria e con vincoli energetici.</p>
</section>
</section>
<section id="scelta-del-framework-giusto" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="scelta-del-framework-giusto"><span class="header-section-number">6.9</span> Scelta del Framework Giusto</h2>
<p>La scelta del framework di machine learning giusto per una determinata applicazione richiede un‚Äôattenta valutazione di modelli, hardware e considerazioni software. Analizzando questi tre aspetti (modelli, hardware e software), gli ingegneri di ML possono selezionare il framework ottimale e personalizzarlo in base alle esigenze per applicazioni ML su dispositivo efficienti e performanti. L‚Äôobiettivo √® bilanciare complessit√† del modello, limitazioni hardware e integrazione software per progettare una pipeline ML su misura per dispositivi embedded e edge.</p>
<div id="fig-tf-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - General">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image4.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - General" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.13: Confronto tra Framework TensorFlow - Generale. Fonte: TensorFlow.
</figcaption>
</figure>
</div>
<section id="modello" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="modello"><span class="header-section-number">6.9.1</span> Modello</h3>
<p>TensorFlow supporta molte pi√π operazioni (op) rispetto a TensorFlow Lite e TensorFlow Lite Micro, in quanto viene solitamente utilizzato per la ricerca o l‚Äôimplementazione cloud, che richiedono un numero elevato di operatori e una maggiore flessibilit√† (vedere <a href="#fig-tf-comparison" class="quarto-xref">Figura&nbsp;<span>6.13</span></a>). TensorFlow Lite supporta operazioni selezionate per il training sul dispositivo, mentre TensorFlow Micro no. TensorFlow Lite supporta anche forme dinamiche e training consapevole della quantizzazione, mentre TensorFlow Micro no. Al contrario, TensorFlow Lite e TensorFlow Micro offrono strumenti e supporto di quantizzazione nativi, dove la quantizzazione si riferisce alla trasformazione di un programma ML in una rappresentazione approssimata con operazioni di precisione inferiore disponibili.</p>
</section>
<section id="software" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="software"><span class="header-section-number">6.9.2</span> Software</h3>
<div id="fig-tf-sw-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - Model">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-sw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image5.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - Model" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-sw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.14: Confronto tra Framework TensorFlow - Software. Fonte: TensorFlow.
</figcaption>
</figure>
</div>
<p>TensorFlow Lite Micro non supporta il sistema operativo, mentre TensorFlow e TensorFlow Lite s√¨, per ridurre il sovraccarico di memoria, velocizzare i tempi di avvio e consumare meno energia (vedere <a href="#fig-tf-sw-comparison" class="quarto-xref">Figura&nbsp;<span>6.14</span></a>). TensorFlow Lite Micro pu√≤ essere utilizzato insieme a sistemi operativi in tempo reale (RTOS) come FreeRTOS, Zephyr e Mbed OS. TensorFlow Lite e TensorFlow Lite Micro supportano la mappatura della memoria del modello, consentendo l‚Äôaccesso diretto ai modelli dalla memoria flash anzich√© caricarli nella RAM, cosa che TensorFlow non fa. TensorFlow e TensorFlow Lite supportano la ‚Äúaccelerator delegation‚Äù per pianificare il codice su diversi acceleratori, mentre TensorFlow Lite Micro no, poich√© i sistemi embedded tendono ad avere una gamma limitata di acceleratori specializzati.</p>
</section>
<section id="hardware" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="hardware"><span class="header-section-number">6.9.3</span> Hardware</h3>
<div id="fig-tf-hw-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - Hardware">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-hw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image3.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - Hardware" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-hw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.15: Confronto tra Framework TensorFlow - Hardware. Fonte: TensorFlow.
</figcaption>
</figure>
</div>
<p>TensorFlow Lite e TensorFlow Lite Micro hanno dimensioni binarie di base e footprint di memoria significativamente pi√π piccoli rispetto a TensorFlow (vedere <a href="#fig-tf-hw-comparison" class="quarto-xref">Figura&nbsp;<span>6.15</span></a>). Ad esempio, un tipico binario TensorFlow Lite Micro √® inferiore a 200 KB, mentre TensorFlow √® molto pi√π grande. Ci√≤ √® dovuto agli ambienti con risorse limitate dei sistemi embedded. TensorFlow supporta x86, TPU e GPU come NVIDIA, AMD e Intel. TensorFlow Lite supporta i processori Arm Cortex-A e x86 comunemente utilizzati su telefoni cellulari e tablet. Quest‚Äôultimo √® privo di tutta la logica di training non necessaria per l‚Äôimplementazione sul dispositivo. TensorFlow Lite Micro fornisce supporto per core Arm Cortex M focalizzati sui microcontrollori come M0, M3, M4 e M7, nonch√© DSP come Hexagon e SHARC e MCU come STM32, NXP Kinetis, Microchip AVR.</p>
</section>
<section id="altri-fattori" class="level3" data-number="6.9.4">
<h3 data-number="6.9.4" class="anchored" data-anchor-id="altri-fattori"><span class="header-section-number">6.9.4</span> Altri Fattori</h3>
<p>La selezione del framework di IA appropriato √® essenziale per garantire che i sistemi embedded possano eseguire in modo efficiente i modelli di IA. Diversi fattori chiave oltre a modelli, hardware e software dovrebbero essere presi in considerazione quando si valutano i framework IA per i sistemi embedded. Altri fattori chiave da considerare quando si sceglie un framework di apprendimento automatico sono prestazioni, scalabilit√†, facilit√† d‚Äôuso, integrazione con strumenti di ingegneria dei dati, integrazione con strumenti di ottimizzazione dei modelli e supporto della community. Comprendendo questi fattori, si possono prendere decisioni informate e massimizzare il potenziale delle iniziative di machine-learning.</p>
<section id="prestazioni" class="level4">
<h4 class="anchored" data-anchor-id="prestazioni">Prestazioni</h4>
<p>Le prestazioni sono fondamentali nei sistemi embedded in cui le risorse di calcolo sono limitate. Valutare la capacit√† del framework di ottimizzare l‚Äôinferenza del modello per l‚Äôhardware embedded. La quantizzazione del modello e il supporto dell‚Äôaccelerazione hardware sono cruciali per ottenere un‚Äôinferenza efficiente.</p>
</section>
<section id="scalabilit√†" class="level4">
<h4 class="anchored" data-anchor-id="scalabilit√†">Scalabilit√†</h4>
<p>La scalabilit√† √® essenziale quando si considera la potenziale crescita di un progetto di IA embedded. Il framework dovrebbe supportare l‚Äôimplementazione di modelli su vari dispositivi embedded, dai microcontrollori ai processori pi√π potenti. Dovrebbe inoltre gestire senza problemi sia le distribuzioni su piccola che su larga scala.</p>
</section>
<section id="integrazione-con-strumenti-di-data-engineering" class="level4">
<h4 class="anchored" data-anchor-id="integrazione-con-strumenti-di-data-engineering">Integrazione con Strumenti di Data Engineering</h4>
<p>Gli strumenti di ingegneria dei dati sono essenziali per la pre-elaborazione dei dati e la gestione della pipeline. Un framework di intelligenza artificiale ideale per sistemi embedded dovrebbe integrarsi perfettamente con questi strumenti, consentendo un‚Äôefficiente acquisizione dei dati, trasformazione e addestramento del modello.</p>
</section>
<section id="integrazione-con-strumenti-di-ottimizzazione-del-modello" class="level4">
<h4 class="anchored" data-anchor-id="integrazione-con-strumenti-di-ottimizzazione-del-modello">Integrazione con Strumenti di Ottimizzazione del Modello</h4>
<p>L‚Äôottimizzazione del modello garantisce che i modelli di intelligenza artificiale siano adatti per la distribuzione embedded. Valutare se il framework si integra con strumenti di ottimizzazione del modello come TensorFlow Lite Converter o ONNX Runtime per facilitare la quantizzazione del modello e la riduzione delle dimensioni.</p>
</section>
<section id="facilit√†-duso" class="level4">
<h4 class="anchored" data-anchor-id="facilit√†-duso">Facilit√† d‚ÄôUso</h4>
<p>La facilit√† d‚Äôuso di un framework di IA ha un impatto significativo sull‚Äôefficienza dello sviluppo. Un framework con un‚Äôinterfaccia intuitiva e una documentazione chiara riduce la curva di apprendimento degli sviluppatori. Si dovrebbe considerare se il framework supporta API di alto livello, consentendo agli sviluppatori di concentrarsi sulla progettazione del modello piuttosto che sui dettagli di implementazione di basso livello. Questo fattore √® incredibilmente importante per i sistemi embedded, che hanno meno funzionalit√† di quelle a cui gli sviluppatori tipici potrebbero essere abituati.</p>
</section>
<section id="supporto-della-community" class="level4">
<h4 class="anchored" data-anchor-id="supporto-della-community">Supporto della Community</h4>
<p>Il supporto della community gioca un altro fattore essenziale. I framework con community attive e coinvolte spesso hanno basi di codice ben mantenute, ricevono aggiornamenti regolari e forniscono forum preziosi per la risoluzione dei problemi. Di conseguenza, anche il supporto della community gioca un ruolo nella facilit√† d‚Äôuso perch√© garantisce che gli sviluppatori abbiano accesso a una vasta gamma di risorse, tra cui tutorial e progetti di esempio. Il supporto della community fornisce una certa garanzia che il framework continuer√† a essere supportato per futuri aggiornamenti. Ci sono solo pochi framework che soddisfano le esigenze di TinyML. TensorFlow Lite Micro √® il pi√π popolare e ha il maggior supporto della comunit√†.</p>
</section>
</section>
</section>
<section id="tendenze-future-nei-framework-ml" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="tendenze-future-nei-framework-ml"><span class="header-section-number">6.10</span> Tendenze Future nei Framework ML</h2>
<section id="decomposizione" class="level3" data-number="6.10.1">
<h3 data-number="6.10.1" class="anchored" data-anchor-id="decomposizione"><span class="header-section-number">6.10.1</span> Decomposizione</h3>
<p>Attualmente, lo stack del sistema ML √® costituito da quattro astrazioni come mostrato in <a href="#fig-mlsys-stack" class="quarto-xref">Figura&nbsp;<span>6.16</span></a>, vale a dire (1) grafi computazionali, (2) programmi tensoriali, (3) librerie e runtime e (4) primitive hardware.</p>
<div id="fig-mlsys-stack" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="Four Abstractions in Current ML System Stack">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlsys-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image8.png" class="img-fluid figure-img" data-align="center" data-caption="Four Abstractions in Current ML System Stack">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlsys-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;6.16: Quattro astrazioni negli attuali stack dei sistemi ML. Fonte: <a href="https://tvm.apache.org/2021/12/15/tvm-unity">TVM.</a>
</figcaption>
</figure>
</div>
<p>Ci√≤ ha portato a confini verticali (ad esempio, tra i livelli di astrazione) e orizzontali (ad esempio, approcci basati sulla libreria rispetto a quelli basati sulla compilazione per il calcolo dei tensori), che ostacolano l‚Äôinnovazione per il ML. Il lavoro futuro nei framework ML pu√≤ guardare alla rottura di questi confini. A dicembre 2021 √® stato proposto <a href="https://tvm.apache.org/2021/12/15/tvm-unity">Apache TVM</a> Unity, che mirava a facilitare le interazioni tra i diversi livelli di astrazione (nonch√© le persone dietro di essi, come scienziati ML, ingegneri ML e ingegneri hardware) e a co-ottimizzare le decisioni in tutti e quattro i livelli di astrazione.</p>
</section>
<section id="compilatori-e-librerie-ad-alte-prestazioni" class="level3" data-number="6.10.2">
<h3 data-number="6.10.2" class="anchored" data-anchor-id="compilatori-e-librerie-ad-alte-prestazioni"><span class="header-section-number">6.10.2</span> Compilatori e Librerie ad Alte Prestazioni</h3>
<p>Con l‚Äôulteriore sviluppo dei framework ML, continueranno a emergere compilatori e librerie ad alte prestazioni. Alcuni esempi attuali includono <a href="https://www.tensorflow.org/xla/architecture">TensorFlow XLA</a> e <a href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS</a> di Nvidia, che accelerano le operazioni di algebra lineare nei grafi computazionali, e <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> di Nvidia, che accelera e ottimizza l‚Äôinferenza.</p>
</section>
<section id="ml-per-framework-ml" class="level3" data-number="6.10.3">
<h3 data-number="6.10.3" class="anchored" data-anchor-id="ml-per-framework-ml"><span class="header-section-number">6.10.3</span> ML per Framework ML</h3>
<p>Possiamo anche usare il ML per migliorare i framework di ML in futuro. Alcuni usi correnti di ML per framework ML includono:</p>
<ul>
<li><p>Ottimizzazione degli iperparametri tramite tecniche quali ottimizzazione bayesiana, ricerca casuale e ricerca a griglia</p></li>
<li><p>Neural Architecture Search (NAS) per cercare automaticamente architetture di rete ottimali</p></li>
<li><p>AutoML, che come descritto in <a href="#sec-ai_frameworks-advanced" class="quarto-xref"><span>Sezione 6.5</span></a>, automatizza la pipeline ML.</p></li>
</ul>
</section>
</section>
<section id="conclusione" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">6.11</span> Conclusione</h2>
<p>In sintesi, la selezione del framework di machine learning ottimale richiede una valutazione approfondita di varie opzioni in base a criteri quali usabilit√†, supporto della community, prestazioni, compatibilit√† hardware e capacit√† di conversione del modello. Non esiste una soluzione adatta a tutti, poich√© il framework giusto dipende da vincoli e casi d‚Äôuso specifici.</p>
<p>Abbiamo prima introdotto la necessit√† di framework di apprendimento automatico come TensorFlow e PyTorch. Questi framework offrono funzionalit√† quali tensori per la gestione di dati multidimensionali, grafi computazionali per la definizione e l‚Äôottimizzazione delle operazioni del modello e una suite di strumenti tra cui funzioni di perdita, ottimizzatori e caricatori di dati che semplificano lo sviluppo del modello.</p>
<p>Le funzionalit√† avanzate migliorano ulteriormente l‚Äôusabilit√† di questi framework, consentendo attivit√† come la messa a punto di grandi modelli pre-addestrati e la facilitazione del ‚Äúfederated learning‚Äù. Queste funzionalit√† sono fondamentali per sviluppare modelli di apprendimento automatico sofisticati in modo efficiente.</p>
<p>I framework di intelligenza artificiale embedded, come TensorFlow Lite Micro, forniscono strumenti specializzati per la distribuzione di modelli su piattaforme con risorse limitate. TensorFlow Lite Micro, ad esempio, offre strumenti di ottimizzazione completi, tra cui la mappatura della quantizzazione e le ottimizzazioni del kernel, per garantire prestazioni elevate su piattaforme basate su microcontrollori come i processori Arm Cortex-M e RISC-V. I framework creati appositamente per hardware specializzato come CMSIS-NN su processori Cortex-M possono massimizzare ulteriormente le prestazioni ma sacrificare la portabilit√†. I framework integrati dei fornitori di processori adattano lo stack alle loro architetture, liberando il pieno potenziale dei loro chip ma ci si blocca nel loro ecosistema.</p>
<p>In definitiva, la scelta del framework giusto implica la ricerca della migliore corrispondenza tra le sue capacit√† e i requisiti della piattaforma target. Ci√≤ richiede un bilanciamento tra esigenze di prestazioni, vincoli hardware, complessit√† del modello e altri fattori. Una valutazione approfondita dei modelli e dei casi d‚Äôuso previsti e la valutazione delle opzioni rispetto alle metriche chiave guideranno gli sviluppatori nella selezione del framework ideale per le loro applicazioni di machine learning.</p>
</section>
<section id="sec-ai-frameworks-resource" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="sec-ai-frameworks-resource"><span class="header-section-number">6.12</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1zbnsihiO68oIUE04TVJEcDQ_Kyec4mhdQkIG6xoR0DY/edit#slide=id.g1ff94734162_0_0">Frameworks overview.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1BK2M2krnI24jSWO0r8tXegl1wgflGZTJyMkjfGolURI/edit#slide=id.g202a6885eb3_0_0">Embedded systems software.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1Jr7HzdZ7YaKO6KY9HBGbOG0BrTnKhbboQtf9d6xy3Ls/edit?usp=drive_link">Inference engines: TF vs.&nbsp;TFLite.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1_DwBbas8wAVWnJ0tbOorqotf9Gns1qNc3JJ6tw8bce0/edit?usp=drive_link">TF flavors: TF vs.&nbsp;TFLite vs.&nbsp;TFLite Micro.</a></p></li>
<li><p>TFLite Micro:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1XdwcZS0pz6kyuk6Vx90kE11hwUMAtS1cMoFQHZAxS20/edit?usp=drive_link">TFLite Micro Big Picture.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/10llaugp6EroGekFzB1pAH1OJ1dpJ4d7yxKglK1BsqlI/edit?usp=drive_link&amp;resourcekey=0-C6_PHSaI6u4x0Mv2KxWKbg">TFLite Micro Interpreter.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/123kdwjRXvbukyaOBvdp0PJpIs2JSxQ7GoDjB8y0FgIE/edit?usp=drive_link">TFLite Micro Model Format.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1_sHuWa3DDTCB9mBzKA4ElPWaUFA8oOelqHCBOHmsvC4/edit?usp=drive_link">TFLite Micro Memory Allocation.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1ZwLOLvYbKodNmyuKKGb_gD83NskrvNmnFC0rvGugJlY/edit?usp=drive_link">TFLite Micro NN Operations.</a></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-tfc" class="quarto-xref">Esercizio&nbsp;<span>6.1</span></a></p></li>
<li><p><a href="#exr-tfl" class="quarto-xref">Esercizio&nbsp;<span>6.2</span></a></p></li>
<li><p><a href="#exr-k" class="quarto-xref">Esercizio&nbsp;<span>6.3</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laboratori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Oltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l‚Äôesperienza di apprendimento.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/data_engineering/data_engineering.it.html" class="pagination-link" aria-label="Data Engineering">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/training/training.it.html" class="pagination-link" aria-label="Addestramento dell'IA">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/frameworks/frameworks.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/frameworks/frameworks.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>