<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>17&nbsp; IA Robusta ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/generative_ai/generative_ai.it.html" rel="next">
<link href="../../contents/sustainable_ai/sustainable_ai.it.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>


</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalit√† oscura"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalit√† lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/privacy_security/privacy_security.it.html">Argomenti Avanzati</a></li><li class="breadcrumb-item"><a href="../../contents/robust_ai/robust_ai.it.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="2d6cdf6fc58f1105ce2a5c5c28a11153" class="alert alert-info hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>üåü Aiutaci a raggiungere 1.000 stelle GitHub! üåü Per ogni 25 stelle, Arduino e SEEED doneranno una NiclaVision o una XIAO ESP32S3 per l‚Äôistruzione sull‚Äôintelligenza artificiale. <a href="https://github.com/harvard-edge/cs249r_book">Cliccare qui per una ‚≠ê</a></p>
</div></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">PREFAZIONE</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Collaboratori e Ringraziamenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni sul Libro</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">PARTE PRINCIPALE</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Nozioni Fondamentali</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sistemi di ML</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Avvio al Deep Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Workflow</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Workflow dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Framework di IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Addestramento dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">IA Efficiente</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Ottimizzazioni dei Modelli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Accelerazione IA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking dell‚ÄôIA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Apprendimento On-Device</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Operazioni di ML</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Argomenti Avanzati</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sicurezza e Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">IA Responsabile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.it.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Impatto Sociale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Chiusura</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusione</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/labs.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Panoramica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/getting_started.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guida Introduttiva</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/arduino/nicla_vision/nicla_vision.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/kws/kws.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione del Movimento e Rilevamento delle Anomalie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/raspi/raspi.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/setup/setup.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/image_classification/image_classification.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classificazione delle Immagini</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/object_detection/object_detection.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rilevamento degli Oggetti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/raspi/llm/llm.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs/shared/shared.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab Condivisi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/kws_feature_eng/kws_feature_eng.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blocco delle Feature Spettrali DSP</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">RIFERIMENTI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riferimenti</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tool</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Dataset</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Risorse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Le Community</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.it.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Casi di Studio</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">17.1</span> Introduzione</a></li>
  <li><a href="#esempi-del-mondo-reale" id="toc-esempi-del-mondo-reale" class="nav-link" data-scroll-target="#esempi-del-mondo-reale"><span class="header-section-number">17.2</span> Esempi del mondo reale</a>
  <ul>
  <li><a href="#cloud" id="toc-cloud" class="nav-link" data-scroll-target="#cloud"><span class="header-section-number">17.2.1</span> Cloud</a></li>
  <li><a href="#edge" id="toc-edge" class="nav-link" data-scroll-target="#edge"><span class="header-section-number">17.2.2</span> Edge</a></li>
  <li><a href="#embedded" id="toc-embedded" class="nav-link" data-scroll-target="#embedded"><span class="header-section-number">17.2.3</span> Embedded</a></li>
  </ul></li>
  <li><a href="#guasti-hardware" id="toc-guasti-hardware" class="nav-link" data-scroll-target="#guasti-hardware"><span class="header-section-number">17.3</span> Guasti Hardware</a>
  <ul>
  <li><a href="#guasti-transitori" id="toc-guasti-transitori" class="nav-link" data-scroll-target="#guasti-transitori"><span class="header-section-number">17.3.1</span> Guasti Transitori</a>
  <ul class="collapse">
  <li><a href="#definizione-e-caratteristiche" id="toc-definizione-e-caratteristiche" class="nav-link" data-scroll-target="#definizione-e-caratteristiche">Definizione e Caratteristiche</a></li>
  <li><a href="#cause-di-guasti-transitori" id="toc-cause-di-guasti-transitori" class="nav-link" data-scroll-target="#cause-di-guasti-transitori">Cause di Guasti Transitori</a></li>
  <li><a href="#meccanismi-di-guasti-transitori" id="toc-meccanismi-di-guasti-transitori" class="nav-link" data-scroll-target="#meccanismi-di-guasti-transitori">Meccanismi di Guasti Transitori</a></li>
  <li><a href="#impatto-sui-sistemi-ml" id="toc-impatto-sui-sistemi-ml" class="nav-link" data-scroll-target="#impatto-sui-sistemi-ml">Impatto sui Sistemi ML</a></li>
  </ul></li>
  <li><a href="#guasti-permanenti" id="toc-guasti-permanenti" class="nav-link" data-scroll-target="#guasti-permanenti"><span class="header-section-number">17.3.2</span> Guasti Permanenti</a>
  <ul class="collapse">
  <li><a href="#definizione-e-caratteristiche-1" id="toc-definizione-e-caratteristiche-1" class="nav-link" data-scroll-target="#definizione-e-caratteristiche-1">Definizione e Caratteristiche</a></li>
  <li><a href="#cause-dei-guasti-permanenti" id="toc-cause-dei-guasti-permanenti" class="nav-link" data-scroll-target="#cause-dei-guasti-permanenti">Cause dei Guasti Permanenti</a></li>
  <li><a href="#meccanismi-dei-guasti-permanenti" id="toc-meccanismi-dei-guasti-permanenti" class="nav-link" data-scroll-target="#meccanismi-dei-guasti-permanenti">Meccanismi dei Guasti Permanenti</a></li>
  <li><a href="#impatto-sui-sistemi-ml-1" id="toc-impatto-sui-sistemi-ml-1" class="nav-link" data-scroll-target="#impatto-sui-sistemi-ml-1">Impatto sui Sistemi ML</a></li>
  </ul></li>
  <li><a href="#guasti-intermittenti" id="toc-guasti-intermittenti" class="nav-link" data-scroll-target="#guasti-intermittenti"><span class="header-section-number">17.3.3</span> Guasti Intermittenti</a>
  <ul class="collapse">
  <li><a href="#definizione-e-caratteristiche-2" id="toc-definizione-e-caratteristiche-2" class="nav-link" data-scroll-target="#definizione-e-caratteristiche-2">Definizione e Caratteristiche</a></li>
  <li><a href="#cause-degli-errori-intermittenti" id="toc-cause-degli-errori-intermittenti" class="nav-link" data-scroll-target="#cause-degli-errori-intermittenti">Cause degli Errori Intermittenti</a></li>
  <li><a href="#meccanismi-dei-guasti-intermittenti" id="toc-meccanismi-dei-guasti-intermittenti" class="nav-link" data-scroll-target="#meccanismi-dei-guasti-intermittenti">Meccanismi dei Guasti Intermittenti</a></li>
  <li><a href="#impatto-sui-sistemi-ml-2" id="toc-impatto-sui-sistemi-ml-2" class="nav-link" data-scroll-target="#impatto-sui-sistemi-ml-2">Impatto sui Sistemi ML</a></li>
  </ul></li>
  <li><a href="#sec-hw-intermittent-detect-mitigate" id="toc-sec-hw-intermittent-detect-mitigate" class="nav-link" data-scroll-target="#sec-hw-intermittent-detect-mitigate"><span class="header-section-number">17.3.4</span> Rilevamento e Mitigazione</a>
  <ul class="collapse">
  <li><a href="#tecniche-di-rilevamento-degli-errori" id="toc-tecniche-di-rilevamento-degli-errori" class="nav-link" data-scroll-target="#tecniche-di-rilevamento-degli-errori">Tecniche di Rilevamento degli Errori</a></li>
  </ul></li>
  <li><a href="#riepilogo" id="toc-riepilogo" class="nav-link" data-scroll-target="#riepilogo"><span class="header-section-number">17.3.5</span> Riepilogo</a></li>
  </ul></li>
  <li><a href="#robustezza-del-modello-ml" id="toc-robustezza-del-modello-ml" class="nav-link" data-scroll-target="#robustezza-del-modello-ml"><span class="header-section-number">17.4</span> Robustezza del Modello ML</a>
  <ul>
  <li><a href="#attacchi-avversari" id="toc-attacchi-avversari" class="nav-link" data-scroll-target="#attacchi-avversari"><span class="header-section-number">17.4.1</span> Attacchi Avversari</a>
  <ul class="collapse">
  <li><a href="#definizione-e-caratteristiche-3" id="toc-definizione-e-caratteristiche-3" class="nav-link" data-scroll-target="#definizione-e-caratteristiche-3">Definizione e Caratteristiche</a></li>
  <li><a href="#meccanismi-degli-attacchi-avversari" id="toc-meccanismi-degli-attacchi-avversari" class="nav-link" data-scroll-target="#meccanismi-degli-attacchi-avversari">Meccanismi degli Attacchi Avversari</a></li>
  <li><a href="#impatto-sui-sistemi-ml-3" id="toc-impatto-sui-sistemi-ml-3" class="nav-link" data-scroll-target="#impatto-sui-sistemi-ml-3">Impatto sui Sistemi ML</a></li>
  </ul></li>
  <li><a href="#avvelenamento-dei-dati" id="toc-avvelenamento-dei-dati" class="nav-link" data-scroll-target="#avvelenamento-dei-dati"><span class="header-section-number">17.4.2</span> Avvelenamento dei Dati</a>
  <ul class="collapse">
  <li><a href="#definizione-e-caratteristiche-4" id="toc-definizione-e-caratteristiche-4" class="nav-link" data-scroll-target="#definizione-e-caratteristiche-4">Definizione e Caratteristiche</a></li>
  <li><a href="#meccanismi-di-avvelenamento-dei-dati" id="toc-meccanismi-di-avvelenamento-dei-dati" class="nav-link" data-scroll-target="#meccanismi-di-avvelenamento-dei-dati">Meccanismi di Avvelenamento dei Dati</a></li>
  <li><a href="#impatto-sui-sistemi-ml-4" id="toc-impatto-sui-sistemi-ml-4" class="nav-link" data-scroll-target="#impatto-sui-sistemi-ml-4">Impatto sui Sistemi ML</a></li>
  </ul></li>
  <li><a href="#distribution-shift" id="toc-distribution-shift" class="nav-link" data-scroll-target="#distribution-shift"><span class="header-section-number">17.4.3</span> Distribution Shift</a>
  <ul class="collapse">
  <li><a href="#definizione-e-caratteristiche-5" id="toc-definizione-e-caratteristiche-5" class="nav-link" data-scroll-target="#definizione-e-caratteristiche-5">Definizione e Caratteristiche</a></li>
  <li><a href="#meccanismi-delle-distribution-shift" id="toc-meccanismi-delle-distribution-shift" class="nav-link" data-scroll-target="#meccanismi-delle-distribution-shift">Meccanismi delle Distribution Shift</a></li>
  <li><a href="#impatto-sui-sistemi-ml-5" id="toc-impatto-sui-sistemi-ml-5" class="nav-link" data-scroll-target="#impatto-sui-sistemi-ml-5">Impatto sui Sistemi ML</a></li>
  </ul></li>
  <li><a href="#rilevamento-e-mitigazione" id="toc-rilevamento-e-mitigazione" class="nav-link" data-scroll-target="#rilevamento-e-mitigazione"><span class="header-section-number">17.4.4</span> Rilevamento e Mitigazione</a>
  <ul class="collapse">
  <li><a href="#attacchi-avversari-2" id="toc-attacchi-avversari-2" class="nav-link" data-scroll-target="#attacchi-avversari-2">Attacchi Avversari</a></li>
  <li><a href="#avvelenamento-dei-dati-1" id="toc-avvelenamento-dei-dati-1" class="nav-link" data-scroll-target="#avvelenamento-dei-dati-1">Avvelenamento dei Dati</a></li>
  <li><a href="#distribution-shift-1" id="toc-distribution-shift-1" class="nav-link" data-scroll-target="#distribution-shift-1">Distribution Shift</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#errori-software" id="toc-errori-software" class="nav-link" data-scroll-target="#errori-software"><span class="header-section-number">17.5</span> Errori Software</a>
  <ul>
  <li><a href="#definizione-e-caratteristiche-6" id="toc-definizione-e-caratteristiche-6" class="nav-link" data-scroll-target="#definizione-e-caratteristiche-6"><span class="header-section-number">17.5.1</span> Definizione e Caratteristiche</a></li>
  <li><a href="#meccanismi-degli-errori-software-nei-framework-ml" id="toc-meccanismi-degli-errori-software-nei-framework-ml" class="nav-link" data-scroll-target="#meccanismi-degli-errori-software-nei-framework-ml"><span class="header-section-number">17.5.2</span> Meccanismi degli Errori Software nei Framework ML</a></li>
  <li><a href="#impatto-sui-sistemi-ml-6" id="toc-impatto-sui-sistemi-ml-6" class="nav-link" data-scroll-target="#impatto-sui-sistemi-ml-6"><span class="header-section-number">17.5.3</span> Impatto sui Sistemi ML</a></li>
  <li><a href="#rilevamento-e-mitigazione-1" id="toc-rilevamento-e-mitigazione-1" class="nav-link" data-scroll-target="#rilevamento-e-mitigazione-1"><span class="header-section-number">17.5.4</span> Rilevamento e Mitigazione</a></li>
  </ul></li>
  <li><a href="#strumenti-e-framework" id="toc-strumenti-e-framework" class="nav-link" data-scroll-target="#strumenti-e-framework"><span class="header-section-number">17.6</span> Strumenti e Framework</a>
  <ul>
  <li><a href="#modelli-di-guasto-e-modelli-di-errore" id="toc-modelli-di-guasto-e-modelli-di-errore" class="nav-link" data-scroll-target="#modelli-di-guasto-e-modelli-di-errore"><span class="header-section-number">17.6.1</span> Modelli di Guasto e Modelli di Errore</a></li>
  <li><a href="#injection-hardware-based-di-guasti" id="toc-injection-hardware-based-di-guasti" class="nav-link" data-scroll-target="#injection-hardware-based-di-guasti"><span class="header-section-number">17.6.2</span> Injection Hardware-based di Guasti</a>
  <ul class="collapse">
  <li><a href="#metodi" id="toc-metodi" class="nav-link" data-scroll-target="#metodi">Metodi</a></li>
  <li><a href="#limitazioni" id="toc-limitazioni" class="nav-link" data-scroll-target="#limitazioni">Limitazioni</a></li>
  </ul></li>
  <li><a href="#strumenti-di-injection-di-guasti-software-based" id="toc-strumenti-di-injection-di-guasti-software-based" class="nav-link" data-scroll-target="#strumenti-di-injection-di-guasti-software-based"><span class="header-section-number">17.6.3</span> Strumenti di Injection di Guasti Software-based</a></li>
  <li><a href="#colmare-il-divario-tra-modelli-di-errore-hardware-e-software" id="toc-colmare-il-divario-tra-modelli-di-errore-hardware-e-software" class="nav-link" data-scroll-target="#colmare-il-divario-tra-modelli-di-errore-hardware-e-software"><span class="header-section-number">17.6.4</span> Colmare il Divario tra Modelli di Errore Hardware e Software</a>
  <ul class="collapse">
  <li><a href="#fidelity-colmare-il-gap" id="toc-fidelity-colmare-il-gap" class="nav-link" data-scroll-target="#fidelity-colmare-il-gap">Fidelity: Colmare il Gap</a></li>
  <li><a href="#limportanza-di-catturare-il-vero-comportamento-hardware" id="toc-limportanza-di-catturare-il-vero-comportamento-hardware" class="nav-link" data-scroll-target="#limportanza-di-catturare-il-vero-comportamento-hardware">L‚ÄôImportanza di Catturare il Vero Comportamento Hardware</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">17.7</span> Conclusione</a></li>
  <li><a href="#sec-robust-ai-resource" id="toc-sec-robust-ai-resource" class="nav-link" data-scroll-target="#sec-robust-ai-resource"><span class="header-section-number">17.8</span> Risorse</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/robust_ai/robust_ai.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/robust_ai/robust_ai.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/privacy_security/privacy_security.it.html">Argomenti Avanzati</a></li><li class="breadcrumb-item"><a href="../../contents/robust_ai/robust_ai.it.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-robust_ai" class="quarto-section-identifier"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">IA Robusta</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Risorse: <a href="#sec-robust-ai-resource">Slide</a>, <a href="#sec-robust-ai-resource">Video</a>, <a href="#sec-robust-ai-resource">Esercizi</a>, <a href="#sec-robust-ai-resource">Laboratori</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/cover_robust_ai.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL¬∑E 3 Prompt: Creare un‚Äôimmagine che rappresenti un sistema di intelligenza artificiale avanzato simboleggiato da una rete neurale intricata e luminosa, profondamente annidata all‚Äôinterno di una serie di scudi progressivamente pi√π grandi e fortificati. Ogni strato di scudo rappresenta uno strato di difesa, che mostra la robustezza del sistema contro minacce esterne ed errori interni. La rete neurale, al centro di questa fortezza di scudi, irradia connessioni che simboleggiano la capacit√† di apprendimento e adattamento dell‚Äôintelligenza artificiale. Questa metafora visiva enfatizza non solo la sofisticatezza tecnologica dell‚Äôintelligenza artificiale, ma anche la sua resilienza e sicurezza, sullo sfondo di una sala server sicura e all‚Äôavanguardia, piena delle ultime innovazioni tecnologiche. L‚Äôimmagine mira a trasmettere il concetto di massima protezione e resilienza nel campo dell‚Äôintelligenza artificiale.</em></figcaption>
</figure>
</div>
<p>Lo sviluppo di sistemi di apprendimento automatico robusti √® diventato sempre pi√π cruciale. Poich√© questi sistemi vengono implementati in varie applicazioni critiche, dai veicoli autonomi alla diagnostica sanitaria, garantire la loro resilienza a guasti ed errori √® fondamentale.</p>
<p>L‚ÄôIA robusta, nel contesto di guasti hardware, guasti software ed errori, svolge un ruolo importante nel mantenimento dell‚Äôaffidabilit√†, della sicurezza e delle prestazioni dei sistemi di apprendimento automatico. Affrontando le sfide poste da guasti hardware transitori, permanenti e intermittenti <span class="citation" data-cites="ahmadilivani2024systematic">(<a href="../../references.it.html#ref-ahmadilivani2024systematic" role="doc-biblioref">Ahmadilivani et al. 2024</a>)</span>, nonch√© bug, difetti di progettazione ed errori di implementazione nel software <span class="citation" data-cites="zhang2008distribution">(<a href="../../references.it.html#ref-zhang2008distribution" role="doc-biblioref">H. Zhang 2008</a>)</span>, le tecniche di intelligenza artificiale robuste consentono ai sistemi di apprendimento automatico di funzionare efficacemente anche in condizioni avverse.</p>
<div class="no-row-height column-margin column-container"></div><p>Questo capitolo esplora i concetti fondamentali, le tecniche e gli strumenti per la creazione di sistemi di apprendimento automatico tolleranti ai guasti e resilienti agli errori. Consente a ricercatori e professionisti di sviluppare soluzioni di IA in grado di resistere alle complessit√† e alle incertezze degli ambienti del mondo reale.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi dell‚ÄôApprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Comprendere l‚Äôimportanza di sistemi di IA robusti e resilienti nelle applicazioni del mondo reale.</p></li>
<li><p>Identificare e caratterizzare guasti hardware, guasti software e il loro impatto sui sistemi ML.</p></li>
<li><p>Riconoscere e sviluppare strategie difensive contro le minacce poste da attacchi avversari, avvelenamento dei dati e cambiamenti nella distribuzione.</p></li>
<li><p>Imparare tecniche per rilevare, mitigare e progettare sistemi ML tolleranti ai guasti.</p></li>
<li><p>Acquisire familiarit√† con strumenti e framework per studiare e migliorare la resilienza del sistema ML durante l‚Äôintero ciclo di vita dello sviluppo dell‚ÄôIA.</p></li>
</ul>
</div>
</div>
<section id="introduzione" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="introduzione"><span class="header-section-number">17.1</span> Introduzione</h2>
<p>Per IA robusta si intende la capacit√† di un sistema di mantenere le proprie prestazioni e affidabilit√† anche in presenza di errori. Un sistema di apprendimento automatico robusto √® progettato per essere tollerante ai guasti e resiliente agli errori, in grado di funzionare efficacemente anche in condizioni avverse.</p>
<p>Man mano che i sistemi ML diventano sempre pi√π integrati in vari aspetti della nostra vita, dai servizi basati su cloud ai dispositivi edge e ai sistemi embedded, l‚Äôimpatto dei guasti hardware e software sulle loro prestazioni e affidabilit√† diventa pi√π significativo. In futuro, man mano che i sistemi ML diventano pi√π complessi e vengono implementati in applicazioni ancora pi√π critiche, la necessit√† di progetti robusti e tolleranti ai guasti sar√† fondamentale.</p>
<p>Si prevede che i sistemi ML svolgeranno ruoli cruciali nei veicoli autonomi, nelle citt√† intelligenti, nell‚Äôassistenza sanitaria e nei domini dell‚Äôautomazione industriale. In questi domini, le conseguenze dei guasti hardware o software possono essere gravi, potenzialmente causa di perdita di vite umane, danni economici o danni ambientali.</p>
<p>I ricercatori e gli ingegneri devono concentrarsi sullo sviluppo di tecniche avanzate per il rilevamento, l‚Äôisolamento e il ripristino dei guasti per mitigare questi rischi e garantire il funzionamento affidabile dei futuri sistemi ML.</p>
<p>Questo capitolo si concentrer√† in modo specifico su tre categorie principali di guasti ed errori che possono influire sulla robustezza dei sistemi ML: guasti hardware, guasti software ed errori umani.</p>
<ul>
<li><p><strong>Guasti Hardware:</strong> Guasti transitori, permanenti e intermittenti possono influire sui componenti hardware di un sistema ML, corrompendo i calcoli e degradando le prestazioni.</p></li>
<li><p><strong>Robustezza del Modello:</strong> I modelli ML possono essere vulnerabili ad attacchi avversari, avvelenamento dei dati e cambiamenti di distribuzione, che possono indurre classificazioni errate mirate, alterare il comportamento appreso del modello o compromettere l‚Äôintegrit√† e l‚Äôaffidabilit√† del sistema.</p></li>
<li><p><strong>Guasti software:</strong> Bug, difetti di progettazione ed errori di implementazione nei componenti software, come algoritmi, librerie e framework, possono propagare errori e introdurre vulnerabilit√†.</p></li>
</ul>
<p>Le sfide e gli approcci specifici per ottenere la robustezza possono variare a seconda della scala e dei vincoli del sistema ML. I sistemi di cloud computing o data center su larga scala possono concentrarsi sulla tolleranza ai guasti e sulla resilienza tramite ridondanza, elaborazione distribuita e tecniche avanzate di rilevamento e correzione degli errori. Al contrario, i dispositivi edge con risorse limitate o i sistemi embedded affrontano sfide uniche a causa della potenza di calcolo, della memoria e delle risorse energetiche limitate.</p>
<p>Indipendentemente dalla scala e dai vincoli, le caratteristiche chiave di un sistema ML robusto includono tolleranza ai guasti, resilienza agli errori e mantenimento delle prestazioni. Comprendendo e affrontando le sfide multiformi alla robustezza, possiamo sviluppare sistemi ML affidabili e sicuri in grado di navigare nelle complessit√† degli ambienti del mondo reale.</p>
<p>Questo capitolo non riguarda solo l‚Äôesplorazione di strumenti, framework e tecniche dei sistemi ML per rilevare e mitigare guasti, attacchi e cambiamenti durante la distribuzione. Si tratta di sottolineare il ruolo cruciale di ognuno di nel dare priorit√† alla resilienza durante tutto il ciclo di vita dello sviluppo dell‚ÄôIA, dalla raccolta dati e dall‚Äôaddestramento del modello all‚Äôimplementazione e al monitoraggio. Affrontando in modo proattivo le sfide alla robustezza, possiamo sbloccare il pieno potenziale delle tecnologie ML garantendone al contempo un‚Äôimplementazione sicura, affidabile e responsabile nelle applicazioni del mondo reale.</p>
<p>Mentre l‚ÄôIA continua a plasmare il nostro futuro, il potenziale delle tecnologie ML √® immenso. Ma √® solo quando creiamo sistemi resilienti in grado di resistere alle sfide del mondo reale che possiamo davvero sfruttare questo potenziale. Questo √® un fattore determinante per il successo e l‚Äôimpatto sociale di questa tecnologia trasformativa ed √® alla nostra portata.</p>
</section>
<section id="esempi-del-mondo-reale" class="level2 page-columns page-full" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="esempi-del-mondo-reale"><span class="header-section-number">17.2</span> Esempi del mondo reale</h2>
<p>Ecco alcuni esempi reali di casi in cui guasti nell‚Äôhardware o nel software hanno causato problemi importanti nei sistemi ML in ambienti cloud, edge ed embedded:</p>
<section id="cloud" class="level3 page-columns page-full" data-number="17.2.1">
<h3 data-number="17.2.1" class="anchored" data-anchor-id="cloud"><span class="header-section-number">17.2.1</span> Cloud</h3>
<p>Nel febbraio 2017, Amazon Web Services (AWS) ha subito <a href="https://aws.amazon.com/message/41926/">un‚Äôinterruzione significativa</a> a causa di un errore umano durante la manutenzione. Un tecnico ha inserito inavvertitamente un comando errato, causando la disconnessione di molti server. Questa interruzione ha interrotto molti servizi AWS, tra cui l‚Äôassistente basato sull‚Äôintelligenza artificiale di Amazon, Alexa. Di conseguenza, i dispositivi basati su Alexa, come Amazon Echo e prodotti di terze parti che utilizzano Alexa Voice Service, non hanno potuto rispondere alle richieste degli utenti per diverse ore. Questo incidente evidenzia il potenziale impatto degli errori umani sui sistemi ML basati su cloud e la necessit√† di procedure di manutenzione robuste e meccanismi di sicurezza.</p>
<p>In un altro esempio <span class="citation" data-cites="dixit2021silent">(<a href="../../references.it.html#ref-dixit2021silent" role="doc-biblioref">Vangal et al. 2021</a>)</span>, Facebook ha riscontrato un problema di ‚Äúsilent data corruption (SDC)‚Äù [corruzione silenziosa dei dati] all‚Äôinterno della sua infrastruttura di query distribuita, come mostrato in <a href="#8owvod923jax"><span class="quarto-xref">Figura&nbsp;<span>17.1</span></span></a>. L‚Äôinfrastruttura di Facebook include un sistema di query che preleva ed esegue query SQL e simili a SQL su pi√π set di dati utilizzando framework come Presto, Hive e Spark. Una delle applicazioni che ha utilizzato questa infrastruttura di query √® stata un‚Äôapplicazione di compressione per ridurre l‚Äôingombro degli archivi dati. In questa applicazione di compressione, i file venivano compressi quando non venivano letti e decompressi quando veniva effettuata una richiesta di lettura. Prima della decompressione, la dimensione del file veniva controllata per assicurarsi che fosse maggiore di zero, indicando un file compresso valido con contenuti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dixit2021silent" class="csl-entry" role="listitem">
Vangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar, Ram Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, e Chris H. Kim. 2021. <span>¬´Wide-Range Many-Core <span>SoC</span> Design in Scaled <span>CMOS:</span> <span>Challenges</span> and Opportunities¬ª</span>. <em>IEEE Trans. Very Large Scale Integr. VLSI Syst.</em> 29 (5): 843‚Äì56. <a href="https://doi.org/10.1109/tvlsi.2021.3061649">https://doi.org/10.1109/tvlsi.2021.3061649</a>.
</div></div><div id="fig-sdc-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/sdc_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.1: Corruzione silenzioso dei dati nelle applicazioni di database. Fonte: <a href="https://arxiv.org/pdf/2102.11245">Facebook</a>
</figcaption>
</figure>
</div>
<p>Tuttavia, in un caso, quando la dimensione del file veniva calcolata per un file valido di dimensioni diverse da zero, l‚Äôalgoritmo di decompressione ha richiamato una funzione di potenza dalla libreria Scala. Inaspettatamente, la funzione Scala ha restituito un valore di dimensione zero per il file nonostante avesse una dimensione decompressa nota diversa da zero. Di conseguenza, la decompressione non √® stata eseguita e il file non √® stato scritto nel database di output. Questo problema si √® manifestato sporadicamente, con alcune occorrenze dello stesso calcolo della dimensione del file che restituivano il valore corretto diverso da zero.</p>
<p>L‚Äôimpatto di questa corruzione silenziosa dei dati √® stato significativo, portando a file mancanti e dati errati nel database di output. L‚Äôapplicazione che si basava sui file decompressi ha fallito a causa delle incongruenze dei dati. Nel caso di studio presentato nel documento, l‚Äôinfrastruttura di Facebook, che consiste in centinaia di migliaia di server che gestiscono miliardi di richieste al giorno dalla loro enorme base di utenti, ha riscontrato un problema di corruzione silenziosa dei dati. Il sistema interessato elaborava query utente, caricamenti di immagini e contenuti multimediali, che richiedevano un‚Äôesecuzione rapida, affidabile e sicura.</p>
<p>Questo caso di studio illustra come la corruzione silenziosa dei dati pu√≤ propagarsi attraverso pi√π strati di uno stack applicativo, causando perdita di dati e guasti delle applicazioni in un sistema distribuito su larga scala. La natura intermittente del problema e la mancanza di messaggi di errore espliciti lo hanno reso particolarmente difficile da diagnosticare e risolvere. Ma questo non √® limitato solo a Meta, anche altre aziende come Google che gestiscono ipercomputer IA affrontano questi problemi. <a href="#fig-sdc-jeffdean" class="quarto-xref">Figura&nbsp;<span>17.2</span></a> <a href="https://en.wikipedia.org/wiki/Jeff_Dean">Jeff Dean</a>, Chief Scientist presso Google DeepMind e Google Research, parla degli SDC e del loro impatto sui sistemi di apprendimento automatico.</p>
<div id="fig-sdc-jeffdean" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-jeffdean-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/sdc-google-jeff-dean.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-jeffdean-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.2: Gli errori ‚ÄúSilent data corruption (SDC)‚Äù sono un problema importante per gli ipercomputer di IA. Fonte: <a href="https://en.wikipedia.org/wiki/Jeff_Dean">Jeff Dean</a> at <a href="https://mlsys.org/">MLSys 2024</a>, Keynote (Google)
</figcaption>
</figure>
</div>
</section>
<section id="edge" class="level3" data-number="17.2.2">
<h3 data-number="17.2.2" class="anchored" data-anchor-id="edge"><span class="header-section-number">17.2.2</span> Edge</h3>
<p>Per quanto riguarda esempi di guasti ed errori nei sistemi edge ML, un‚Äôarea che ha ricevuto notevole attenzione √® il dominio delle auto a guida autonoma. I veicoli a guida autonoma si basano in larga misura su algoritmi di apprendimento automatico per la percezione, il processo decisionale e il controllo, rendendoli particolarmente sensibili all‚Äôimpatto di guasti hardware e software. Negli ultimi anni, diversi incidenti di alto profilo che hanno coinvolto veicoli autonomi hanno evidenziato le sfide e i rischi associati all‚Äôimplementazione di questi sistemi in ambienti reali.</p>
<p>A maggio 2016, si √® verificato un incidente mortale quando una Tesla Model S con pilota automatico si √® schiantata contro un autoarticolato bianco che attraversava l‚Äôautostrada. Il sistema Autopilot, che si basava su algoritmi di visione artificiale e apprendimento automatico, non √® riuscito a riconoscere il rimorchio bianco sullo sfondo di un cielo luminoso. Il conducente, che secondo quanto riferito stava guardando un film al momento dell‚Äôincidente, non √® intervenuto in tempo e il veicolo √® entrato in collisione con il rimorchio a tutta velocit√†. Questo incidente ha sollevato preoccupazioni sui limiti dei sistemi di percezione basati sull‚Äôintelligenza artificiale e sulla necessit√† di solidi meccanismi di sicurezza nei veicoli autonomi. Ha inoltre evidenziato l‚Äôimportanza della consapevolezza del conducente e la necessit√† di linee guida chiare sull‚Äôuso delle funzionalit√† di guida semi-autonoma, come mostrato in <a href="#tckwqf2ctxw"><span class="quarto-xref">Figura&nbsp;<span>17.3</span></span></a>.</p>
<div id="fig-tesla-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tesla-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/tesla_example.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tesla-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.3: Tesla nell‚Äôincidente mortale in California era in modalit√† Autopilot. Fonte: <a href="https://www.bbc.com/news/world-us-canada-43604440">BBC News</a>
</figcaption>
</figure>
</div>
<p>A marzo 2018, un veicolo di prova a guida autonoma di Uber ha investito e ucciso un pedone che attraversava la strada a Tempe, in Arizona. L‚Äôincidente √® stato causato da un difetto software nel sistema di riconoscimento degli oggetti del veicolo, che non √® riuscito a identificare i pedoni in modo appropriato per evitarli come ostacoli. L‚Äôautista di sicurezza, che avrebbe dovuto monitorare il funzionamento del veicolo e intervenire se necessario, √® stato trovato distratto durante l‚Äôincidente. <a href="https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html?iid=EL">Questo incidente</a> ha portato ad un‚Äôampia revisione del programma di guida autonoma di Uber e ha sollevato dubbi sulla prontezza della tecnologia dei veicoli autonomi per le strade pubbliche. Ha inoltre sottolineato la necessit√† di rigorosi test, convalide e misure di sicurezza nello sviluppo e nell‚Äôimplementazione di sistemi di guida autonoma basati sull‚Äôintelligenza artificiale.</p>
<p>Nel 2021, Tesla ha dovuto affrontare un controllo pi√π rigoroso a seguito di diversi incidenti che hanno coinvolto veicoli in modalit√† Autopilot. Alcuni di questi incidenti sono stati attribuiti a problemi con la capacit√† del sistema Autopilot di rilevare e rispondere a determinate situazioni stradali, come veicoli di emergenza fermi o ostacoli sulla strada. Ad esempio, nell‚Äôaprile 2021, una Tesla Model S si √® schiantata contro un albero in Texas, uccidendo due passeggeri. <a href="https://www.cnbc.com/2021/04/18/no-one-was-driving-in-tesla-crash-that-killed-two-men-in-spring-texas-report.html">I primi rapporti</a> suggerivano che nessuno si trovasse al posto di guida al momento dell‚Äôincidente, sollevando interrogativi sull‚Äôuso e il potenziale uso improprio delle funzionalit√† Autopilot. Questi incidenti evidenziano le sfide in corso nello sviluppo di sistemi di guida autonoma affidabili e robusti e la necessit√† di normative chiare e di istruzione dei consumatori in merito alle capacit√† e ai limiti di queste tecnologie.</p>
</section>
<section id="embedded" class="level3" data-number="17.2.3">
<h3 data-number="17.2.3" class="anchored" data-anchor-id="embedded"><span class="header-section-number">17.2.3</span> Embedded</h3>
<p>I sistemi embedded, che spesso operano in ambienti con risorse limitate e applicazioni critiche per la sicurezza, hanno da tempo dovuto affrontare sfide legate a guasti hardware e software. Poich√© le tecnologie di IA e apprendimento automatico sono sempre pi√π integrate in questi sistemi, il potenziale di guasti ed errori assume nuove dimensioni, con l‚Äôaggiunta di complessit√† degli algoritmi di IA e la natura critica delle applicazioni in cui vengono distribuiti.</p>
<p>Consideriamo alcuni esempi, a partire dall‚Äôesplorazione dello spazio. La missione Mars Polar Lander della NASA nel 1999 ha subito <a href="https://spaceref.com/uncategorized/nasa-reveals-probable-cause-of-mars-polar-lander-and-deep-space-2-mission-failures/">un guasto catastrofico</a> a causa di un errore software nel sistema di rilevamento dell‚Äôatterraggio (<a href="#e3z8hq3qpwn4"><span class="quarto-xref">Figura&nbsp;<span>17.4</span></span></a>). Il software di bordo della navicella spaziale ha interpretato erroneamente il rumore proveniente dall‚Äôapertura delle sue gambe di atterraggio come un segnale di atterraggio sulla superficie marziana. Di conseguenza, la navicella ha spento prematuramente i suoi motori, causando lo schianto sulla superficie. Questo incidente evidenzia l‚Äôimportanza critica di una progettazione software solida e di test approfonditi nei sistemi embedded, in particolare quelli che operano in ambienti remoti e ostili. Poich√© le capacit√† di IA sono integrate nelle future missioni spaziali, garantire l‚Äôaffidabilit√† e la tolleranza ai guasti di questi sistemi sar√† fondamentale per il successo della missione.</p>
<div id="fig-nasa-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nasa-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/nasa_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nasa-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.4: La missione fallita della NASA Mars Polar Lander nel 1999 √® costata oltre $200M. Fonte: <a href="https://www.slashgear.com/1094840/nasas-failed-mars-missions-that-cost-over-200-million/">SlashGear</a>
</figcaption>
</figure>
</div>
<p>Tornando sulla Terra, nel 2015, un Boeing 787 Dreamliner ha subito un arresto elettrico completo durante un volo a causa di un bug del software nelle sue unit√† di controllo del generatore. Questo incidente sottolinea come i guasti software possano avere gravi conseguenze nei sistemi integrati complessi come quelli degli aeromobili. Poich√© le tecnologie di IA sono sempre pi√π applicate all‚Äôaviazione, come nei sistemi di volo autonomi e nella manutenzione predittiva, garantire la robustezza e l‚Äôaffidabilit√† di questi sistemi sar√† fondamentale per la sicurezza dei passeggeri.</p>
<blockquote class="blockquote">
<p><em>‚ÄúSe le quattro unit√† di controllo del generatore principale (associate ai generatori montati sul motore) fossero accese contemporaneamente, dopo 248 giorni di alimentazione continua, tutte e quattro le GCU entrerebbero in modalit√† fail-safe contemporaneamente, con conseguente perdita di tutta l‚Äôalimentazione elettrica CA indipendentemente dalla fase di volo.‚Äù ‚Äì <a href="https://s3.amazonaws.com/public-inspection.federalregister.gov/2015-10066.pdf">Direttiva della Federal Aviation Administration</a> (2015)</em></p>
</blockquote>
<p>Poich√© le capacit√† di IA si integrano sempre di pi√π nei sistemi embedded, il potenziale di guasti ed errori diventa pi√π complesso e grave. Si immagini un <a href="https://www.bbc.com/future/article/20221011-how-space-weather-causes-computer-errors">pacemaker</a> intelligente che ha un improvviso problema tecnico. Un paziente potrebbe morire a causa di tale effetto. Pertanto, gli algoritmi AI, come quelli utilizzati per la percezione, il processo decisionale e il controllo, introducono nuove fonti di potenziali guasti, come problemi relativi ai dati, incertezze del modello e comportamenti inaspettati nei casi limite. Inoltre, la natura opaca di alcuni modelli di IA pu√≤ rendere difficile identificare e diagnosticare i guasti quando si verificano.</p>
</section>
</section>
<section id="guasti-hardware" class="level2 page-columns page-full" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="guasti-hardware"><span class="header-section-number">17.3</span> Guasti Hardware</h2>
<p>I guasti hardware rappresentano una sfida significativa nei sistemi informatici, inclusi i sistemi tradizionali e ML. Questi guasti si verificano quando componenti fisici, come processori, moduli di memoria, dispositivi di archiviazione o interconnessioni, funzionano male o si comportano in modo anomalo. I guasti hardware possono causare calcoli errati, danneggiamento dei dati, crash del sistema o guasti completi del sistema, compromettendo l‚Äôintegrit√† e l‚Äôaffidabilit√† dei calcoli eseguiti <span class="citation" data-cites="jha2019ml">(<a href="../../references.it.html#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span>. Un guasto completo del sistema si riferisce a una situazione in cui l‚Äôintero sistema informatico diventa non reattivo o inutilizzabile a causa di un malfunzionamento hardware critico. Questo tipo di guasto √® il pi√π grave, poich√© rende il sistema inutilizzabile e pu√≤ portare alla perdita o al danneggiamento dei dati, richiedendo un intervento manuale per riparare o sostituire i componenti difettosi.</p>
<div class="no-row-height column-margin column-container"></div><p>Comprendere la tassonomia dei guasti hardware √® essenziale per chiunque lavori con sistemi informatici, in particolare nel contesto dei sistemi ML. I sistemi ML si basano su architetture hardware complesse e calcoli su larga scala per addestrare e distribuire modelli che apprendono dai dati e fanno previsioni o decisioni intelligenti. Tuttavia, i guasti hardware possono introdurre errori e incongruenze nella <a href="../ops/ops.qmd">pipeline MLOps</a>, influenzando l‚Äôaccuratezza, la robustezza e l‚Äôaffidabilit√† dei modelli addestrati <span class="citation" data-cites="li2017understanding">(<a href="../../references.it.html#ref-li2017understanding" role="doc-biblioref">G. Li et al. 2017</a>)</span>.</p>
<p>Conoscere i diversi tipi di guasti hardware, i loro meccanismi e il loro potenziale impatto sul comportamento del sistema √® fondamentale per sviluppare strategie efficaci per rilevarli, mitigarli e ripristinarli. Questa conoscenza √® necessaria per progettare sistemi di elaborazione tolleranti ai guasti, implementare algoritmi ML robusti e garantire l‚Äôaffidabilit√† complessiva delle applicazioni basate su ML.</p>
<p>Le sezioni seguenti esploreranno le tre categorie principali di guasti hardware: transitori, permanenti e intermittenti. Discuteremo le loro definizioni, caratteristiche, cause, meccanismi ed esempi di come si manifestano nei sistemi di elaborazione. Tratteremo anche tecniche di rilevamento e mitigazione specifiche per ogni tipo di guasto.</p>
<ul>
<li><p><strong>Guasti Transitori:</strong> I guasti transitori sono temporanei e non ricorrenti. Sono spesso causati da fattori esterni come raggi cosmici, interferenze elettromagnetiche o fluttuazioni di potenza. Un esempio comune di guasto transitorio √® un bit flip, in cui un singolo bit in una posizione di memoria o registro cambia il suo valore in modo imprevisto. I guasti transitori possono causare calcoli errati o corruzione dei dati, ma non causano danni permanenti all‚Äôhardware.</p></li>
<li><p><strong>Guasti permanenti:</strong> I guasti permanenti, chiamati anche errori hard, sono irreversibili e persistono nel tempo. Sono in genere causati da difetti fisici o usura dei componenti hardware. Esempi di guasti permanenti includono guasti bloccati, in cui un bit o un segnale √® impostato in modo permanente su un valore specifico (ad esempio, sempre 0 o sempre 1) e guasti del dispositivo, come un processore malfunzionante o un modulo di memoria danneggiato. I guasti permanenti possono causare un guasto completo del sistema o un significativo degrado delle prestazioni.</p></li>
<li><p><strong>Guasti Intermittenti:</strong> I guasti intermittenti sono guasti ricorrenti che compaiono e scompaiono in modo intermittente. Condizioni hardware instabili, come connessioni allentate, componenti obsoleti o difetti di fabbricazione, spesso ne sono la causa. I guasti intermittenti possono essere difficili da diagnosticare e riprodurre perch√© possono verificarsi sporadicamente e in condizioni specifiche. Esempi includono cortocircuiti intermittenti o problemi di resistenza dei contatti. I guasti intermittenti possono portare a un comportamento imprevedibile del sistema e a errori intermittenti.</p></li>
</ul>
<p>Alla fine di questa discussione, i lettori avranno una solida comprensione della tassonomia dei guasti e della sua rilevanza per i sistemi di elaborazione e ML tradizionali. Questa base li aiuter√† a prendere decisioni informate durante la progettazione, l‚Äôimplementazione e la distribuzione di soluzioni tolleranti ai guasti, migliorando l‚Äôaffidabilit√† e la credibilit√† dei loro sistemi di elaborazione e delle applicazioni ML.</p>
<section id="guasti-transitori" class="level3 page-columns page-full" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="guasti-transitori"><span class="header-section-number">17.3.1</span> Guasti Transitori</h3>
<p>I guasti transitori nell‚Äôhardware possono manifestarsi in varie forme, ciascuna con le sue caratteristiche e cause uniche. Questi guasti sono di natura temporanea e non causano danni permanenti ai componenti hardware.</p>
<section id="definizione-e-caratteristiche" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definizione-e-caratteristiche">Definizione e Caratteristiche</h4>
<p>Alcuni dei tipi comuni di guasti transitori includono Single Event Upset (SEU) causati da radiazioni ionizzanti, fluttuazioni di tensione <span class="citation" data-cites="reddi2013resilient">(<a href="../../references.it.html#ref-reddi2013resilient" role="doc-biblioref">Reddi e Gupta 2013</a>)</span> dovute a rumore dell‚Äôalimentatore o interferenze elettromagnetiche, ‚ÄúElectromagnetic Interference (EMI)‚Äù indotte da campi elettromagnetici esterni, ‚ÄúElectrostatic Discharge (ESD)‚Äù risultanti da un improvviso flusso di elettricit√† statica, diafonia causata da accoppiamento di segnali involontari, rimbalzo di massa innescato dalla commutazione simultanea di pi√π uscite, violazioni di temporizzazione dovute a violazioni dei vincoli di temporizzazione del segnale ed errori soft nella logica combinatoria che influenzano l‚Äôuscita dei circuiti logici <span class="citation" data-cites="mukherjee2005soft">(<a href="../../references.it.html#ref-mukherjee2005soft" role="doc-biblioref">Mukherjee, Emer, e Reinhardt 2005</a>)</span>. Comprendere questi diversi tipi di guasti transitori √® fondamentale per progettare sistemi hardware robusti e resilienti che possano mitigarne l‚Äôimpatto e garantire un funzionamento affidabile.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reddi2013resilient" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, e Meeta Sharma Gupta. 2013. <em>Resilient Architecture Design for Voltage Variation</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01739-1">https://doi.org/10.1007/978-3-031-01739-1</a>.
</div><div id="ref-mukherjee2005soft" class="csl-entry" role="listitem">
Mukherjee, S. S., J. Emer, e S. K. Reinhardt. 2005. <span>¬´The Soft Error Problem: <span>An</span> Architectural Perspective¬ª</span>. In <em>11th International Symposium on High-Performance Computer Architecture</em>, 243‚Äì47. IEEE; IEEE. <a href="https://doi.org/10.1109/hpca.2005.37">https://doi.org/10.1109/hpca.2005.37</a>.
</div></div><p>Tutti questi guasti transitori sono caratterizzati dalla loro breve durata e dalla loro natura non permanente. Non persistono n√© lasciano alcun impatto duraturo sull‚Äôhardware. Tuttavia, possono comunque portare a calcoli errati, corruzione dei dati o comportamento scorretto del sistema se non gestiti correttamente.</p>
<p><img src="./images/png/image22.png" class="img-fluid"></p>
</section>
<section id="cause-di-guasti-transitori" class="level4">
<h4 class="anchored" data-anchor-id="cause-di-guasti-transitori">Cause di Guasti Transitori</h4>
<p>I guasti transitori possono essere attribuiti a vari fattori esterni. Una causa comune sono i raggi cosmici, particelle ad alta energia provenienti dallo spazio. Quando queste particelle colpiscono aree sensibili dell‚Äôhardware, come celle di memoria o transistor, possono indurre disturbi di carica che alterano i dati memorizzati o trasmessi. Ci√≤ √® illustrato in <a href="#9jd0z5evi3fa"><span class="quarto-xref">Figura&nbsp;<span>17.5</span></span></a>. Un‚Äôaltra causa di guasti transitori √® l‚Äô<a href="https://www.trentonsystems.com/en-us/resource-hub/blog/what-is-electromagnetic-interference">electromagnetic interference (EMI)</a> [interferenza elettromagnetica] da dispositivi vicini o fluttuazioni di potenza. L‚ÄôEMI pu√≤ accoppiarsi con i circuiti e causare picchi di tensione o glitch che interrompono temporaneamente il normale funzionamento dell‚Äôhardware.</p>
<div id="fig-transient-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transient-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/transient_fault.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transient-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.5: Meccanismo di Occorrenza di Guasti Transitori Hardware. Fonte: <a href="https://group.ntt/en/newsrelease/2018/11/22/181122a.html">NTT</a>
</figcaption>
</figure>
</div>
</section>
<section id="meccanismi-di-guasti-transitori" class="level4">
<h4 class="anchored" data-anchor-id="meccanismi-di-guasti-transitori">Meccanismi di Guasti Transitori</h4>
<p>I guasti transitori possono manifestarsi attraverso meccanismi diversi a seconda del componente hardware interessato. Nei dispositivi di memoria come DRAM o SRAM, i guasti transitori spesso portano a inversioni di bit, in cui un singolo bit cambia il suo valore da 0 a 1 o viceversa. Ci√≤ pu√≤ corrompere i dati o le istruzioni archiviati. Nei circuiti logici, i guasti transitori possono causare glitch o picchi di tensione che si propagano attraverso la logica combinatoria, con conseguenti output o segnali di controllo errati. I guasti transitori possono anche influenzare i canali di comunicazione, causando errori di bit o perdite di pacchetti durante la trasmissione dei dati.</p>
</section>
<section id="impatto-sui-sistemi-ml" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impatto-sui-sistemi-ml">Impatto sui Sistemi ML</h4>
<p>Un esempio comune di guasto transitorio √® un‚Äôinversione di bit nella memoria principale. Se una struttura dati importante o un‚Äôistruzione critica viene archiviata nella posizione di memoria interessata, pu√≤ portare a calcoli errati o a un comportamento errato del programma. Se si verifica un guasto transitorio nella memoria che archivia i pesi o i gradienti del modello. Ad esempio, un bit flip nella memoria che memorizza un contatore di loop pu√≤ causare l‚Äôesecuzione indefinita del loop o la sua terminazione prematura. Errori transitori nei registri di controllo o nei bit di flag possono alterare il flusso di esecuzione del programma, causando salti imprevisti o decisioni di diramazione errate. Nei sistemi di comunicazione, gli errori transitori possono danneggiare i pacchetti di dati trasmessi, causando ritrasmissioni o perdita di dati.</p>
<p>Nei sistemi ML, gli errori transitori possono avere implicazioni significative durante la fase di training <span class="citation" data-cites="he2023understanding">(<a href="../../references.it.html#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>. Il training ML comporta calcoli iterativi e aggiornamenti dei parametri del modello basati su grandi set di dati. Se si verifica un errore transitorio nella memoria dei pesi o dei gradienti del modello, pu√≤ causare aggiornamenti errati e compromettere la convergenza e l‚Äôaccuratezza del processo di training. <a href="#fig-sdc-training-fault" class="quarto-xref">Figura&nbsp;<span>17.6</span></a> mostra un esempio concreto tratto dalla flotta di produzione di Google, in cui un‚Äôanomalia SDC ha causato una differenza significativa nella norma del gradiente.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-sdc-training-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-training-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/google_sdc_jeff_dean_anomaly.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-training-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.6: SDC nella fase di training ML determina anomalie nella norma del gradiente. Fonte: Jeff Dean, MLSys 2024 Keynote (Google)
</figcaption>
</figure>
</div>
<p>Ad esempio, un‚Äôinversione di bit nella matrice dei pesi di una rete neurale pu√≤ far s√¨ che il modello apprenda pattern o associazioni errati, con conseguente peggioramento delle prestazioni <span class="citation" data-cites="wan2021analyzing">(<a href="../../references.it.html#ref-wan2021analyzing" role="doc-biblioref">Wan et al. 2021</a>)</span>. Errori transitori nella pipeline dei dati, come la corruzione dei campioni di training o delle etichette, possono anche introdurre rumore e influire sulla qualit√† del modello appreso.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wan2021analyzing" class="csl-entry" role="listitem">
Wan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi, e Arijit Raychowdhury. 2021. <span>¬´Analyzing and Improving Fault Tolerance of Learning-Based Navigation Systems¬ª</span>. In <em>2021 58th ACM/IEEE Design Automation Conference (DAC)</em>, 841‚Äì46. IEEE; IEEE. <a href="https://doi.org/10.1109/dac18074.2021.9586116">https://doi.org/10.1109/dac18074.2021.9586116</a>.
</div></div><p>Durante la fase di inferenza, gli errori transitori possono influire sull‚Äôaffidabilit√† e l‚Äôattendibilit√† delle previsioni ML. Se si verifica un errore transitorio nella memoria dei parametri del modello addestrato o nel calcolo dei risultati dell‚Äôinferenza, pu√≤ portare a previsioni errate o incoerenti. Ad esempio, un‚Äôinversione di bit nei valori di attivazione di una rete neurale pu√≤ alterare l‚Äôoutput finale di classificazione o regressione <span class="citation" data-cites="mahmoud2020pytorchfi">(<a href="../../references.it.html#ref-mahmoud2020pytorchfi" role="doc-biblioref">Mahmoud et al. 2020</a>)</span>.</p>
<p>Nelle applicazioni ‚Äúsafety-critical‚Äù, come i veicoli autonomi o la diagnosi medica, i guasti transitori durante l‚Äôinferenza possono avere gravi conseguenze, portando a decisioni o azioni errate <span class="citation" data-cites="li2017understanding jha2019ml">(<a href="../../references.it.html#ref-li2017understanding" role="doc-biblioref">G. Li et al. 2017</a>; <a href="../../references.it.html#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span>. Garantire la resilienza dei sistemi ML contro i guasti transitori √® fondamentale per mantenere l‚Äôintegrit√† e l‚Äôaffidabilit√† delle previsioni.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017understanding" class="csl-entry" role="listitem">
Li, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, e Stephen W. Keckler. 2017. <span>¬´Understanding error propagation in deep learning neural network <span>(DNN)</span> accelerators and applications¬ª</span>. In <em>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>, 1‚Äì12. ACM. <a href="https://doi.org/10.1145/3126908.3126964">https://doi.org/10.1145/3126908.3126964</a>.
</div><div id="ref-courbariaux2016binarized" class="csl-entry" role="listitem">
Courbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, e Yoshua Bengio. 2016. <span>¬´Binarized neural networks: <span>Training</span> deep neural networks with weights and activations constrained to+ 1 or-1¬ª</span>. <em>arXiv preprint arXiv:1602.02830</em>.
</div><div id="ref-Aygun2021BSBNN" class="csl-entry" role="listitem">
Aygun, Sercan, Ece Olcay Gunes, e Christophe De Vleeschouwer. 2021. <span>¬´Efficient and robust bitstream processing in binarised neural networks¬ª</span>. <em>Electron. Lett.</em> 57 (5): 219‚Äì22. <a href="https://doi.org/10.1049/ell2.12045">https://doi.org/10.1049/ell2.12045</a>.
</div></div><p>All‚Äôaltro estremo, in ambienti con risorse limitate come TinyML, le ‚ÄúBinarized Neural Networks [BNNs]‚Äù [reti neurali binarizzate] <span class="citation" data-cites="courbariaux2016binarized">(<a href="../../references.it.html#ref-courbariaux2016binarized" role="doc-biblioref">Courbariaux et al. 2016</a>)</span> sono emerse come una soluzione promettente. Le BNN rappresentano pesi di rete in precisione a bit singolo, offrendo efficienza computazionale e tempi di inferenza pi√π rapidi. Tuttavia, questa rappresentazione binaria rende le BNN fragili agli errori di inversione di bit sui pesi della rete. Ad esempio, lavori precedenti <span class="citation" data-cites="Aygun2021BSBNN">(<a href="../../references.it.html#ref-Aygun2021BSBNN" role="doc-biblioref">Aygun, Gunes, e De Vleeschouwer 2021</a>)</span> hanno dimostrato che un‚Äôarchitettura BNN a due strati nascosti per un‚Äôattivit√† semplice come la classificazione MNIST subisce un degrado delle prestazioni dal 98% di accuratezza del test al 70% quando vengono inseriti errori soft di inversione di bit casuali tramite pesi del modello con una probabilit√† del 10%.</p>
<p>Per affrontare tali problemi √® necessario considerare tecniche di training ‚Äúflip-aware‚Äù o sfruttare paradigmi di elaborazione emergenti (ad esempio, <a href="https://en.wikipedia.org/wiki/Stochastic_computing">elaborazione stocastica</a>) per migliorare la tolleranza ai guasti e la robustezza, di cui parleremo in <a href="#sec-hw-intermittent-detect-mitigate" class="quarto-xref"><span>Sezione 17.3.4</span></a>. Le direzioni di ricerca future mirano a sviluppare architetture ibride, nuove funzioni di attivazione e funzioni di perdita su misura per colmare il divario di accuratezza rispetto ai modelli a precisione completa mantenendo al contempo la loro efficienza computazionale.</p>
</section>
</section>
<section id="guasti-permanenti" class="level3 page-columns page-full" data-number="17.3.2">
<h3 data-number="17.3.2" class="anchored" data-anchor-id="guasti-permanenti"><span class="header-section-number">17.3.2</span> Guasti Permanenti</h3>
<p>I guasti permanenti sono difetti hardware che persistono e causano danni irreversibili ai componenti interessati. Questi guasti sono caratterizzati dalla loro natura persistente e richiedono la riparazione o la sostituzione dell‚Äôhardware difettoso per ripristinare la normale funzionalit√† del sistema.</p>
<section id="definizione-e-caratteristiche-1" class="level4">
<h4 class="anchored" data-anchor-id="definizione-e-caratteristiche-1">Definizione e Caratteristiche</h4>
<p>I guasti permanenti sono difetti hardware che causano malfunzionamenti persistenti e irreversibili nei componenti interessati. Il componente difettoso rimane non operativo finch√© un guasto permanente non viene riparato o sostituito. Questi guasti sono caratterizzati dalla loro natura coerente e riproducibile, il che significa che il comportamento difettoso viene osservato ogni volta che il componente interessato viene utilizzato. I guasti permanenti possono avere un impatto su vari componenti hardware, come processori, moduli di memoria, dispositivi di archiviazione o interconnessioni, causando crash del sistema, danneggiamento dei dati o guasto completo del sistema.</p>
<p>Un esempio notevole di guasto permanente √® il <a href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug">bug Intel FDIV</a>, scoperto nel 1994. Il bug FDIV era un difetto in alcune unit√† di divisione a virgola mobile (FDIV) dei processori Intel Pentium. Il bug causava risultati errati per specifiche operazioni di divisione, portando a calcoli imprecisi.</p>
<p>Il bug FDIV si √® verificato a causa di un errore nella tabella di ricerca utilizzata dall‚Äôunit√† di divisione. In rari casi, il processore recuperava un valore errato dalla tabella di ricerca, con un risultato leggermente meno preciso del previsto. Ad esempio, <a href="#djy9mhqllriw"><span class="quarto-xref">Figura&nbsp;<span>17.7</span></span></a> mostra una frazione 4195835/3145727 tracciata su un processore Pentium con l‚Äôerrore permanente FDIV. Le regioni triangolari sono quelle in cui si sono verificati calcoli errati. Idealmente, tutti i valori corretti verrebbero arrotondati a 1,3338, ma i risultati errati mostrano 1,3337, indicando un errore nella quinta cifra.</p>
<p>Sebbene l‚Äôerrore fosse piccolo, poteva accumularsi su molte operazioni di divisione, portando a significative imprecisioni nei calcoli matematici. L‚Äôimpatto del bug FDIV era significativo, soprattutto per le applicazioni che si basavano in modo massiccio sulla divisione precisa in virgola mobile, come simulazioni scientifiche, calcoli finanziari e progettazione assistita da computer. Il bug ha portato a risultati errati, che potrebbero avere gravi conseguenze in settori come la finanza o l‚Äôingegneria.</p>
<div id="fig-permanent-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-permanent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/permanent_fault.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-permanent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.7: Processore Intel Pentium con errore permanente FDIV. Le regioni triangolari sono quelle in cui si sono verificati calcoli errati. Fonte: <a href="https://www.halfhill.com/byte/1995-3_truth.html">Byte Magazine</a>
</figcaption>
</figure>
</div>
<p>Il bug Intel FDIV √® un monito per il potenziale impatto di guasti permanenti sui sistemi ML. Nel contesto del ML, guasti permanenti nei componenti hardware possono portare a calcoli errati, influenzando l‚Äôaccuratezza e l‚Äôaffidabilit√† dei modelli. Ad esempio, se un sistema ML si basa su un processore con un‚Äôunit√† a virgola mobile difettosa, simile al bug Intel FDIV, potrebbe introdurre errori nei calcoli eseguiti durante l‚Äôaddestramento o l‚Äôinferenza.</p>
<p>Questi errori possono propagarsi attraverso il modello, portando a previsioni imprecise o apprendimento distorto. Nelle applicazioni in cui il ML viene utilizzato per attivit√† critiche, come la guida autonoma, la diagnosi medica o le previsioni finanziarie, le conseguenze di calcoli errati dovuti a guasti permanenti possono essere gravi.</p>
<p>√à fondamentale che i professionisti del ML siano consapevoli del potenziale impatto dei guasti permanenti e incorporino tecniche di tolleranza ai guasti, come ridondanza hardware, meccanismi di rilevamento e correzione degli errori e progettazione di algoritmi robusti, per mitigare i rischi associati a questi guasti. Inoltre, test approfonditi e convalida dei componenti hardware ML possono aiutare a identificare e risolvere i guasti permanenti prima che influiscano sulle prestazioni e l‚Äôaffidabilit√† del sistema.</p>
</section>
<section id="cause-dei-guasti-permanenti" class="level4">
<h4 class="anchored" data-anchor-id="cause-dei-guasti-permanenti">Cause dei Guasti Permanenti</h4>
<p>I guasti permanenti possono derivare da diverse cause, tra cui difetti di fabbricazione e meccanismi di usura. I <a href="https://www.sciencedirect.com/science/article/pii/B9780128181058000206">difetti di fabbricazione</a> sono difetti intrinseci introdotti durante il processo di fabbricazione dei componenti hardware. Questi difetti includono incisione impropria, doping non corretto o contaminazione, che portano a componenti non funzionali o parzialmente funzionali.</p>
<p>D‚Äôaltro canto, i <a href="https://semiengineering.com/what-causes-semiconductor-aging/">meccanismi di usura</a> si verificano nel tempo man mano che i componenti hardware sono sottoposti a un uso prolungato e a stress. Fattori come elettromigrazione, rottura dell‚Äôossido o stress termico possono causare una graduale degradazione dei componenti, portando infine a guasti permanenti.</p>
</section>
<section id="meccanismi-dei-guasti-permanenti" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="meccanismi-dei-guasti-permanenti">Meccanismi dei Guasti Permanenti</h4>
<p>I guasti permanenti possono manifestarsi attraverso vari meccanismi, a seconda della natura e della posizione del guasto. Gli ‚ÄúStuck-at fault‚Äù [guasti bloccati] <span class="citation" data-cites="seong2010safer">(<a href="../../references.it.html#ref-seong2010safer" role="doc-biblioref">Seong et al. 2010</a>)</span> sono guasti permanenti comuni in cui un segnale o una cella di memoria rimane fissata a un valore particolare (0 o 1) indipendentemente dagli input, come illustrato in <a href="#ahtmh1s1mxgf"><span class="quarto-xref">Figura&nbsp;<span>17.8</span></span></a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-seong2010safer" class="csl-entry" role="listitem">
Seong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, e Hsien-Hsin S. Lee. 2010. <span>¬´<span>SAFER:</span> <span>Stuck-at-fault</span> Error Recovery for Memories¬ª</span>. In <em>2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture</em>, 115‚Äì24. IEEE; IEEE. <a href="https://doi.org/10.1109/micro.2010.46">https://doi.org/10.1109/micro.2010.46</a>.
</div></div><div id="fig-stuck-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stuck-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/stuck_fault.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stuck-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.8: Modello di Guasto Bloccato nei Circuiti Digitali. Fonte: <a href="https://accendoreliability.com/digital-circuits-stuck-fault-model/">Accendo Reliability</a>
</figcaption>
</figure>
</div>
<p>I guasti bloccati possono verificarsi in porte logiche, celle di memoria o interconnessioni, causando calcoli errati o corruzione dei dati. Un altro meccanismo sono i guasti del dispositivo, in cui un componente, come un transistor o una cella di memoria, cessa completamente di funzionare. Ci√≤ pu√≤ essere dovuto a difetti di fabbricazione o grave usura. I guasti di ‚Äúbridging‚Äù si verificano quando due o pi√π linee di segnale sono collegate involontariamente, causando cortocircuiti o un comportamento logico errato.</p>
<p>Oltre ai guasti stuck-at, ci sono diversi altri tipi di guasti permanenti che possono influenzare i circuiti digitali e che possono avere un impatto su un sistema ML. I guasti di ritardo possono causare il superamento del limite specificato del ritardo di propagazione di un segnale, portando a violazioni di temporizzazione. I guasti di interconnessione, come guasti aperti (fili rotti), guasti resistivi (resistenza aumentata) o guasti capacitivi (capacit√† aumentata), possono causare problemi di integrit√† del segnale o violazioni di temporizzazione. Le celle di memoria possono anche subire vari guasti, tra cui guasti di transizione (impossibilit√† di cambiare stato), guasti di accoppiamento (interferenza tra celle adiacenti) e guasti sensibili al pattern di vicinato (guasti che dipendono dai valori delle celle vicine). Altri guasti permanenti possono verificarsi nella rete di alimentazione o nella rete di distribuzione del clock, influenzando la funzionalit√† e la temporizzazione del circuito.</p>
</section>
<section id="impatto-sui-sistemi-ml-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impatto-sui-sistemi-ml-1">Impatto sui Sistemi ML</h4>
<p>I guasti permanenti possono influire gravemente sul comportamento e l‚Äôaffidabilit√† dei sistemi di elaborazione. Ad esempio, un guasto nell‚Äôunit√† logica aritmetica (ALU) di un processore pu√≤ causare calcoli errati, portando a risultati errati o crash del sistema. Un guasto permanente in un modulo di memoria, in una specifica cella di memoria, pu√≤ danneggiare i dati archiviati, causando la perdita di dati o un comportamento errato del programma. Nei dispositivi di archiviazione, guasti permanenti come settori danneggiati o guasti del dispositivo possono causare l‚Äôinaccessibilit√† dei dati o la perdita completa delle informazioni archiviate. I guasti permanenti di interconnessione possono interrompere i canali di comunicazione, causando il danneggiamento dei dati o il blocco del sistema.</p>
<p>I guasti permanenti possono influire significativamente sui sistemi ML durante le fasi di addestramento e inferenza. Durante l‚Äôaddestramento, guasti permanenti nelle unit√† di elaborazione o nella memoria possono causare calcoli errati, con conseguenti modelli danneggiati o non ottimali <span class="citation" data-cites="he2023understanding">(<a href="../../references.it.html#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>. Inoltre, i guasti nei dispositivi di archiviazione possono corrompere i dati di training o i parametri del modello archiviati, causando la perdita di dati o incongruenze del modello <span class="citation" data-cites="he2023understanding">(<a href="../../references.it.html#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2018analyzing" class="csl-entry" role="listitem">
Zhang, Jeff Jun, Tianyu Gu, Kanad Basu, e Siddharth Garg. 2018. <span>¬´Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator¬ª</span>. In <em>2018 IEEE 36th VLSI Test Symposium (VTS)</em>, 1‚Äì6. IEEE; IEEE. <a href="https://doi.org/10.1109/vts.2018.8368656">https://doi.org/10.1109/vts.2018.8368656</a>.
</div></div><p>Durante l‚Äôinferenza, i guasti permanenti possono influire sull‚Äôaffidabilit√† e la correttezza delle previsioni ML. I guasti nelle unit√† di elaborazione possono produrre risultati errati o causare guasti del sistema, mentre i guasti nella memoria che archivia i parametri del modello possono portare all‚Äôutilizzo di modelli corrotti o obsoleti per l‚Äôinferenza <span class="citation" data-cites="zhang2018analyzing">(<a href="../../references.it.html#ref-zhang2018analyzing" role="doc-biblioref">J. J. Zhang et al. 2018</a>)</span>.</p>
<p>Per mitigare l‚Äôimpatto dei guasti permanenti nei sistemi ML, devono essere impiegate tecniche di tolleranza ai guasti sia a livello hardware che software. La ridondanza hardware, come la duplicazione di componenti critici o l‚Äôutilizzo di codici di correzione degli errori <span class="citation" data-cites="kim2015bamboo">(<a href="../../references.it.html#ref-kim2015bamboo" role="doc-biblioref">Kim, Sullivan, e Erez 2015</a>)</span>, pu√≤ aiutare a rilevare e ripristinare i guasti permanenti. Le tecniche software, come i meccanismi di checkpoint e riavvio <span class="citation" data-cites="egwutuoha2013survey">(<a href="../../references.it.html#ref-egwutuoha2013survey" role="doc-biblioref">Egwutuoha et al. 2013</a>)</span>, possono consentire al sistema di recuperare da guasti permanenti tornando a uno stato salvato in precedenza. Il monitoraggio, il test e la manutenzione regolari dei sistemi ML possono aiutare a identificare e sostituire i componenti difettosi prima che causino interruzioni significative.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kim2015bamboo" class="csl-entry" role="listitem">
Kim, Jungrae, Michael Sullivan, e Mattan Erez. 2015. <span>¬´Bamboo <span>ECC:</span> <span>Strong,</span> safe, and flexible codes for reliable computer memory¬ª</span>. In <em>2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</em>, 101‚Äì12. IEEE; IEEE. <a href="https://doi.org/10.1109/hpca.2015.7056025">https://doi.org/10.1109/hpca.2015.7056025</a>.
</div><div id="ref-egwutuoha2013survey" class="csl-entry" role="listitem">
Egwutuoha, Ifeanyi P., David Levy, Bran Selic, e Shiping Chen. 2013. <span>¬´A survey of fault tolerance mechanisms and checkpoint/restart implementations for high performance computing systems¬ª</span>. <em>The Journal of Supercomputing</em> 65 (3): 1302‚Äì26. <a href="https://doi.org/10.1007/s11227-013-0884-0">https://doi.org/10.1007/s11227-013-0884-0</a>.
</div></div><p>Progettare sistemi ML tenendo a mente la tolleranza ai guasti √® fondamentale per garantirne l‚Äôaffidabilit√† e la robustezza in presenza di guasti permanenti. Ci√≤ pu√≤ comportare l‚Äôincorporazione di ridondanza, meccanismi di rilevamento e correzione degli errori e strategie di sicurezza nell‚Äôarchitettura del sistema. Affrontando in modo proattivo le sfide poste dai guasti permanenti, i sistemi ML possono mantenere la loro integrit√†, accuratezza e affidabilit√†, anche di fronte a guasti hardware.</p>
</section>
</section>
<section id="guasti-intermittenti" class="level3 page-columns page-full" data-number="17.3.3">
<h3 data-number="17.3.3" class="anchored" data-anchor-id="guasti-intermittenti"><span class="header-section-number">17.3.3</span> Guasti Intermittenti</h3>
<p>I guasti intermittenti sono guasti hardware che si verificano sporadicamente e in modo imprevedibile in un sistema. Un esempio √® illustrato in <a href="#kix.1c0l0udn3cp7"><span class="quarto-xref">Figura&nbsp;<span>17.9</span></span></a>, dove le crepe nel materiale possono introdurre una maggiore resistenza [elettrica] nei circuiti. Questi guasti sono particolarmente difficili da rilevare e diagnosticare perch√© compaiono e scompaiono in modo intermittente, rendendo difficile riprodurre e isolare la causa principale. I guasti intermittenti possono causare instabilit√† del sistema, corruzione dei dati e degrado delle prestazioni.</p>
<div id="fig-intermittent-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intermittent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/intermittent_fault.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intermittent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.9: Maggiore resistenza dovuta a un guasto intermittente, ovvero una crepa tra la protuberanza di rame e la saldatura del package. Fonte: <a href="https://ieeexplore.ieee.org/document/4925824">Constantinescu</a>
</figcaption>
</figure>
</div>
<section id="definizione-e-caratteristiche-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definizione-e-caratteristiche-2">Definizione e Caratteristiche</h4>
<p>I guasti intermittenti sono caratterizzati dalla loro natura sporadica e non deterministica. Si verificano in modo irregolare e possono apparire e scomparire spontaneamente, con durate e frequenze variabili. Questi guasti non si manifestano in modo coerente ogni volta che viene utilizzato il componente interessato, il che li rende pi√π difficili da rilevare rispetto ai guasti permanenti. I guasti intermittenti possono interessare vari componenti hardware, tra cui processori, moduli di memoria, dispositivi di archiviazione o interconnessioni. Possono causare errori transitori, danneggiamento dei dati o comportamento imprevisto del sistema.</p>
<p>I guasti intermittenti possono avere un impatto significativo sul comportamento e l‚Äôaffidabilit√† dei sistemi di elaborazione <span class="citation" data-cites="rashid2014characterizing">(<a href="../../references.it.html#ref-rashid2014characterizing" role="doc-biblioref">Rashid, Pattabiraman, e Gopalakrishnan 2015</a>)</span>. Ad esempio, un guasto intermittente nella logica di controllo di un processore pu√≤ causare un flusso di programma irregolare, portando a calcoli errati o blocchi del sistema. I guasti intermittenti nei moduli di memoria possono danneggiare i valori dei dati, con conseguente esecuzione errata del programma o incoerenze nei dati. Nei dispositivi di archiviazione, i guasti intermittenti possono causare errori di lettura/scrittura o perdita di dati. Errori intermittenti nei canali di comunicazione possono causare corruzione dei dati, perdita di pacchetti o problemi di connettivit√† intermittenti. Questi errori possono causare crash del sistema, problemi di integrit√† dei dati o degrado delle prestazioni, a seconda della gravit√† e della frequenza degli errori intermittenti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rashid2014characterizing" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî. 2015. <span>¬´Characterizing the Impact of Intermittent Hardware Faults on Programs¬ª</span>. <em>IEEE Trans. Reliab.</em> 64 (1): 297‚Äì310. <a href="https://doi.org/10.1109/tr.2014.2363152">https://doi.org/10.1109/tr.2014.2363152</a>.
</div></div></section>
<section id="cause-degli-errori-intermittenti" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="cause-degli-errori-intermittenti">Cause degli Errori Intermittenti</h4>
<p>I guasti intermittenti possono derivare da diverse cause, sia interne che esterne, ai componenti hardware <span class="citation" data-cites="constantinescu2008intermittent">(<a href="../../references.it.html#ref-constantinescu2008intermittent" role="doc-biblioref">Constantinescu 2008</a>)</span>. Una causa comune √® l‚Äôinvecchiamento e l‚Äôusura dei componenti. Man mano che i dispositivi elettronici invecchiano, diventano pi√π suscettibili a guasti intermittenti dovuti a meccanismi di degradazione come elettromigrazione, rottura dell‚Äôossido o affaticamento dei giunti di saldatura.</p>
<div class="no-row-height column-margin column-container"><div id="ref-constantinescu2008intermittent" class="csl-entry" role="listitem">
Constantinescu, Cristian. 2008. <span>¬´Intermittent faults and effects on reliability of integrated circuits¬ª</span>. In <em>2008 Annual Reliability and Maintainability Symposium</em>, 370‚Äì74. IEEE; IEEE. <a href="https://doi.org/10.1109/rams.2008.4925824">https://doi.org/10.1109/rams.2008.4925824</a>.
</div></div><p>Anche difetti di fabbricazione o variazioni di processo possono causare guasti intermittenti, in cui componenti marginali o borderline possono presentare guasti sporadici in condizioni specifiche, come mostrato in <a href="#kix.7lswkjecl7ra"><span class="quarto-xref">Figura&nbsp;<span>17.10</span></span></a>.</p>
<p>Fattori ambientali, come fluttuazioni di temperatura, umidit√† o vibrazioni, possono innescare guasti intermittenti alterando le caratteristiche elettriche dei componenti. Collegamenti allentati o degradati, come quelli nei connettori o nei circuiti stampati, possono causare guasti intermittenti.</p>
<div id="fig-intermittent-fault-dram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intermittent-fault-dram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/intermittent_fault_dram.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intermittent-fault-dram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.10: Guasto intermittente indotto da residui in un chip DRAM. Fonte: <a href="https://ieeexplore.ieee.org/document/4925824">Hynix Semiconductor</a>
</figcaption>
</figure>
</div>
</section>
<section id="meccanismi-dei-guasti-intermittenti" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="meccanismi-dei-guasti-intermittenti">Meccanismi dei Guasti Intermittenti</h4>
<p>I guasti intermittenti possono manifestarsi attraverso vari meccanismi, a seconda della causa sottostante e del componente interessato. Un meccanismo √® il circuito aperto o cortocircuito intermittente, in cui un percorso o una connessione del segnale viene temporaneamente interrotto o cortocircuitato, causando un comportamento irregolare. Un altro meccanismo √® il guasto di ritardo intermittente <span class="citation" data-cites="zhang2018thundervolt">(<a href="../../references.it.html#ref-zhang2018thundervolt" role="doc-biblioref">J. Zhang et al. 2018</a>)</span>, in cui la temporizzazione dei segnali o i ritardi di propagazione diventano incoerenti, causando problemi di sincronizzazione o calcoli errati. I guasti intermittenti possono manifestarsi come bit flip [inversioni] transitori o errori soft nelle celle di memoria o nei registri, causando corruzione dei dati o esecuzione errata del programma.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2018thundervolt" class="csl-entry" role="listitem">
Zhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, e Siddharth Garg. 2018. <span>¬´<span>ThUnderVolt:</span> <span>Enabling</span> Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Learning Accelerators¬ª</span>. In <em>2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)</em>, 1‚Äì6. IEEE. <a href="https://doi.org/10.1109/dac.2018.8465918">https://doi.org/10.1109/dac.2018.8465918</a>.
</div></div></section>
<section id="impatto-sui-sistemi-ml-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impatto-sui-sistemi-ml-2">Impatto sui Sistemi ML</h4>
<p>Nel contesto dei sistemi ML, i guasti intermittenti possono introdurre sfide significative e avere un impatto sull‚Äôaffidabilit√† e le prestazioni del sistema. Durante la fase di addestramento, i guasti intermittenti nelle unit√† di elaborazione o nella memoria possono portare a incongruenze nei calcoli, con conseguenti gradienti e aggiornamenti del peso errati o rumorosi. Ci√≤ pu√≤ influire sulla convergenza e l‚Äôaccuratezza del processo di addestramento, portando a modelli sub-ottimali o instabili. Errori intermittenti di archiviazione o recupero dei dati possono corrompere i dati di training, introducendo rumore o errori che degradano la qualit√† dei modelli addestrati <span class="citation" data-cites="he2023understanding">(<a href="../../references.it.html#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2023understanding" class="csl-entry" role="listitem">
He, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, e Yanjing Li. 2023. <span>¬´Understanding and Mitigating Hardware Failures in Deep Learning Training Systems¬ª</span>. In <em>Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 1‚Äì16. IEEE; ACM. <a href="https://doi.org/10.1145/3579371.3589105">https://doi.org/10.1145/3579371.3589105</a>.
</div></div><p>Durante la fase di inferenza, gli errori intermittenti possono influire sull‚Äôaffidabilit√† e la coerenza delle previsioni ML. Gli errori nelle unit√† di elaborazione o nella memoria possono causare calcoli errati o corruzione dei dati, portando a previsioni errate o incoerenti. Gli errori intermittenti nella pipeline dei dati possono introdurre rumore o errori nei dati di input, influenzando l‚Äôaccuratezza e la robustezza delle previsioni. Nelle applicazioni safety-critical, come veicoli autonomi o sistemi di diagnosi medica, gli errori intermittenti possono avere gravi conseguenze, portando a decisioni o azioni errate che compromettono la sicurezza e l‚Äôaffidabilit√†.</p>
<p>Per mitigare l‚Äôimpatto degli errori intermittenti nei sistemi ML √® necessario un approccio poliedrico <span class="citation" data-cites="rashid2012intermittent">(<a href="../../references.it.html#ref-rashid2012intermittent" role="doc-biblioref">Rashid, Pattabiraman, e Gopalakrishnan 2012</a>)</span>. A livello hardware, tecniche come pratiche di progettazione robuste, selezione dei componenti e controllo ambientale possono aiutare a ridurre il verificarsi di guasti intermittenti. Meccanismi di ridondanza e correzione degli errori possono essere impiegati per rilevare e ripristinare guasti intermittenti. A livello software, monitoraggio del runtime, rilevamento delle anomalie e tecniche di tolleranza ai guasti possono essere incorporate nella pipeline ML. Ci√≤ pu√≤ includere tecniche come convalida dei dati, rilevamento di valori anomali, assemblaggio di modelli o adattamento del modello di runtime per gestire con eleganza i guasti intermittenti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rashid2012intermittent" class="csl-entry" role="listitem">
Rashid, Layali, Karthik Pattabiraman, e Sathish Gopalakrishnan. 2012. <span>¬´Intermittent Hardware Errors Recovery: <span>Modeling</span> and Evaluation¬ª</span>. In <em>2012 Ninth International Conference on Quantitative Evaluation of Systems</em>, 220‚Äì29. IEEE; IEEE. <a href="https://doi.org/10.1109/qest.2012.37">https://doi.org/10.1109/qest.2012.37</a>.
</div></div><p>Progettare sistemi ML resilienti ai guasti intermittenti √® fondamentale per garantirne affidabilit√† e robustezza. Ci√≤ comporta l‚Äôincorporazione di tecniche di tolleranza ai guasti, monitoraggio del runtime e meccanismi adattivi nell‚Äôarchitettura del sistema. Affrontando in modo proattivo le sfide dei guasti intermittenti, i sistemi ML possono mantenere la loro accuratezza, coerenza e affidabilit√†, anche in caso di guasti hardware sporadici. Test, monitoraggio e manutenzione regolari dei sistemi ML possono aiutare a identificare e mitigare i guasti intermittenti prima che causino interruzioni significative o un degrado delle prestazioni.</p>
</section>
</section>
<section id="sec-hw-intermittent-detect-mitigate" class="level3 page-columns page-full" data-number="17.3.4">
<h3 data-number="17.3.4" class="anchored" data-anchor-id="sec-hw-intermittent-detect-mitigate"><span class="header-section-number">17.3.4</span> Rilevamento e Mitigazione</h3>
<p>Questa sezione esplora varie tecniche di rilevamento degli errori, inclusi approcci a livello hardware e software, e discute strategie di mitigazione efficaci per migliorare la resilienza dei sistemi ML. Inoltre, esamineremo le considerazioni sulla progettazione di sistemi ML resilienti, presenteremo casi di studio ed esempi e metteremo in evidenza le future direzioni di ricerca nei sistemi ML tolleranti agli errori.</p>
<section id="tecniche-di-rilevamento-degli-errori" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="tecniche-di-rilevamento-degli-errori">Tecniche di Rilevamento degli Errori</h4>
<p>Le tecniche di rilevamento degli errori sono importanti per identificare e localizzare gli errori hardware nei sistemi ML. Queste tecniche possono essere ampiamente categorizzate in approcci a livello hardware e software, ognuno dei quali offre capacit√† e vantaggi unici.</p>
<section id="rilevamento-degli-errori-a-livello-hardware" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="rilevamento-degli-errori-a-livello-hardware">Rilevamento degli errori a livello hardware</h5>
<p>Le tecniche di rilevamento degli errori a livello hardware sono implementate a livello fisico del sistema e mirano a identificare gli errori nei componenti hardware sottostanti. Esistono diverse tecniche hardware, ma in generale, possiamo raggruppare questi diversi meccanismi nelle seguenti categorie.</p>
<p><strong>Built-in self-test (BIST) mechanisms:</strong> BIST √® una tecnica potente per rilevare guasti nei componenti hardware <span class="citation" data-cites="bushnell2002built">(<a href="../../references.it.html#ref-bushnell2002built" role="doc-biblioref">Bushnell e Agrawal 2002</a>)</span>.. Comporta l‚Äôincorporazione di circuiti hardware aggiuntivi nel sistema per l‚Äôautotest e il rilevamento dei guasti. BIST pu√≤ essere applicato a vari componenti, come processori, moduli di memoria o circuiti integrati specifici per applicazione (ASIC). Ad esempio, BIST pu√≤ essere implementato in un processore utilizzando catene di scansione, che sono percorsi dedicati che consentono l‚Äôaccesso ai registri interni e alla logica per scopi di test.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bushnell2002built" class="csl-entry" role="listitem">
Bushnell, Michael L, e Vishwani D Agrawal. 2002. <span>¬´Built-in self-test¬ª</span>. <em>Essentials of electronic testing for digital, memory and mixed-signal VLSI circuits</em>, 489‚Äì548.
</div></div><p>Durante il processo BIST, vengono applicati pattern di test predefiniti ai circuiti interni del processore e le risposte vengono confrontate con i valori previsti. Eventuali discrepanze indicano la presenza di guasti. I processori Xeon di Intel, ad esempio, includono meccanismi BIST per testare i core della CPU, la memoria cache e altri componenti critici durante l‚Äôavvio del sistema.</p>
<p><strong>Codici di rilevamento degli errori:</strong> I codici di rilevamento degli errori sono ampiamente utilizzati per rilevare errori di archiviazione e trasmissione dei dati <span class="citation" data-cites="hamming1950error">(<a href="../../references.it.html#ref-hamming1950error" role="doc-biblioref">Hamming 1950</a>)</span>. Questi codici aggiungono bit ridondanti ai dati originali, consentendo il rilevamento di errori di bit. Esempio: I controlli di parit√† sono una forma semplice di codice di rilevamento degli errori mostrato in <a href="#kix.2vxlbeehnemj"><span class="quarto-xref">Figura&nbsp;<span>17.11</span></span></a>. In uno schema di parit√† a bit singolo, un bit extra viene aggiunto a ogni parola di dati, rendendo il numero di 1 nella parola pari (parit√† pari) o dispari (parit√† dispari).</p>
<div class="no-row-height column-margin column-container"><div id="ref-hamming1950error" class="csl-entry" role="listitem">
Hamming, R. W. 1950. <span>¬´Error Detecting and Error Correcting Codes¬ª</span>. <em>Bell Syst. Tech. J.</em> 29 (2): 147‚Äì60. <a href="https://doi.org/10.1002/j.1538-7305.1950.tb00463.x">https://doi.org/10.1002/j.1538-7305.1950.tb00463.x</a>.
</div></div><div id="fig-parity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/parity.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.11: Esempio di bit di parit√†. Fonte: <a href="https://www.computerhope.com/jargon/p/paritybi.htm">Computer Hope</a>
</figcaption>
</figure>
</div>
<p>Quando si leggono i dati, la parit√† viene controllata e, se non corrisponde al valore previsto, viene rilevato un errore. Codici di rilevamento degli errori pi√π avanzati, come i ‚Äúcyclic redundancy checks (CRC)‚Äù [controlli di ridondanza ciclica], calcolano un checksum in base ai dati e lo aggiungono al messaggio. Il checksum viene ricalcolato all‚Äôestremit√† ricevente e confrontato con il checksum trasmesso per rilevare gli errori. I moduli di memoria con ‚ÄúError-correcting code (ECC)‚Äù [codice di correzione degli errori], comunemente utilizzati nei server e nei sistemi critici, impiegano codici avanzati di rilevamento e correzione degli errori per rilevare e correggere errori a bit singolo o multi-bit nella memoria.</p>
<p><strong>Ridondanza hardware e meccanismi di voto:</strong> La ridondanza hardware implica la duplicazione dei componenti critici e il confronto dei loro output per rilevare e mascherare i guasti <span class="citation" data-cites="sheaffer2007hardware">(<a href="../../references.it.html#ref-sheaffer2007hardware" role="doc-biblioref">Sheaffer, Luebke, e Skadron 2007</a>)</span>. I meccanismi di voto, come la ‚Äútriple modular redundancy (TMR)‚Äù [ridondanza modulare tripla], impiegano pi√π istanze di un componente e confrontano i loro output per identificare e mascherare comportamenti difettosi <span class="citation" data-cites="arifeen2020approximate">(<a href="../../references.it.html#ref-arifeen2020approximate" role="doc-biblioref">Arifeen, Hassan, e Lee 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sheaffer2007hardware" class="csl-entry" role="listitem">
Sheaffer, Jeremy W, David P Luebke, e Kevin Skadron. 2007. <span>¬´A hardware redundancy and recovery mechanism for reliable scientific computation on graphics processors¬ª</span>. In <em>Graphics Hardware</em>, 2007:55‚Äì64. Citeseer.
</div><div id="ref-arifeen2020approximate" class="csl-entry" role="listitem">
Arifeen, Tooba, Abdus Sami Hassan, e Jeong-A Lee. 2020. <span>¬´Approximate Triple Modular Redundancy: <span>A</span> Survey¬ª</span>. <em>#IEEE_O_ACC#</em> 8: 139851‚Äì67. <a href="https://doi.org/10.1109/access.2020.3012673">https://doi.org/10.1109/access.2020.3012673</a>.
</div><div id="ref-yeh1996triple" class="csl-entry" role="listitem">
Yeh, Y. C. 1996. <span>¬´Triple-triple redundant 777 primary flight computer¬ª</span>. In <em>1996 IEEE Aerospace Applications Conference. Proceedings</em>, 1:293‚Äì307. IEEE; IEEE. <a href="https://doi.org/10.1109/aero.1996.495891">https://doi.org/10.1109/aero.1996.495891</a>.
</div></div><p>In un sistema TMR, tre istanze identiche di un componente hardware, come un processore o un sensore, eseguono lo stesso calcolo in parallelo. Gli output di queste istanze vengono immessi in un circuito di voto, che confronta i risultati e seleziona il valore di maggioranza come output finale. Se una delle istanze produce un risultato non corretto a causa di un guasto, il meccanismo di voto maschera l‚Äôerrore e mantiene l‚Äôoutput corretto. Il TMR √® comunemente utilizzato nei sistemi aerospaziali e aeronautici, dove l‚Äôelevata affidabilit√† √® fondamentale. Ad esempio, l‚Äôaereo Boeing 777 impiega il TMR nel suo sistema di computer di volo primario per garantire la disponibilit√† e la correttezza delle funzioni di controllo del volo <span class="citation" data-cites="yeh1996triple">(<a href="../../references.it.html#ref-yeh1996triple" role="doc-biblioref">Yeh 1996</a>)</span>.</p>
<p>I computer a guida autonoma di Tesla impiegano un‚Äôarchitettura hardware ridondante per garantire la sicurezza e l‚Äôaffidabilit√† delle funzioni critiche, come percezione, processo decisionale e controllo del veicolo, come mostrato in <a href="#kix.nsc1yczcug9r"><span class="quarto-xref">Figura&nbsp;<span>17.12</span></span></a>. Un componente chiave di questa architettura √® l‚Äôutilizzo della ‚Äúdual modular redundancy (DMR)‚Äù [ridondanza modulare duale] nei sistemi di computer di bordo dell‚Äôauto.</p>
<div id="fig-tesla-dmr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tesla-dmr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/tesla_dmr.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tesla-dmr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.12: Computer Tesla a guida autonoma completa con SoC duali ridondanti. Fonte: <a href="https://old.hotchips.org/hc31/HC31_2.3_Tesla_Hotchips_ppt_Final_0817.pdf">Tesla</a>
</figcaption>
</figure>
</div>
<p>Nell‚Äôimplementazione DMR di Tesla, due unit√† hardware identiche, spesso chiamate ‚Äúcomputer ridondanti‚Äù o ‚Äúunit√† di controllo ridondanti‚Äù, eseguono gli stessi calcoli in parallelo <span class="citation" data-cites="bannon2019computer">(<a href="../../references.it.html#ref-bannon2019computer" role="doc-biblioref">Bannon et al. 2019</a>)</span>. Ogni unit√† elabora in modo indipendente i dati dei sensori, esegue algoritmi di percezione e decisionali e genera comandi di controllo per gli attuatori del veicolo (ad esempio, sterzo, accelerazione e frenata).</p>
<div class="no-row-height column-margin column-container"><div id="ref-bannon2019computer" class="csl-entry" role="listitem">
Bannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, e Emil Talpes. 2019. <span>¬´Computer and Redundancy Solution for the Full Self-Driving Computer¬ª</span>. In <em>2019 IEEE Hot Chips 31 Symposium (HCS)</em>, 1‚Äì22. IEEE Computer Society; IEEE. <a href="https://doi.org/10.1109/hotchips.2019.8875645">https://doi.org/10.1109/hotchips.2019.8875645</a>.
</div></div><p>Gli output di queste due unit√† ridondanti vengono costantemente confrontati per rilevare eventuali discrepanze o guasti. Se gli output corrispondono, il sistema presuppone che entrambe le unit√† funzionino correttamente e i comandi di controllo vengono inviati agli attuatori del veicolo. Tuttavia, se c‚Äô√® una mancata corrispondenza tra gli output, il sistema identifica un potenziale guasto in una delle unit√† e adotta le misure appropriate per garantire un funzionamento sicuro.</p>
<p>Il sistema pu√≤ impiegare meccanismi aggiuntivi per determinare quale unit√† √® difettosa in una mancata corrispondenza. Ci√≤ pu√≤ comportare l‚Äôutilizzo di algoritmi diagnostici, il confronto degli output con i dati di altri sensori o sottosistemi o l‚Äôanalisi della coerenza degli output nel tempo. Una volta identificata l‚Äôunit√† difettosa, il sistema pu√≤ isolarla e continuare a funzionare utilizzando l‚Äôoutput dell‚Äôunit√† non difettosa.</p>
<p>Il DMR nel computer di guida autonoma di Tesla fornisce un ulteriore livello di sicurezza e tolleranza ai guasti. Avendo due unit√† indipendenti che eseguono gli stessi calcoli, il sistema pu√≤ rilevare e mitigare i guasti che possono verificarsi in una delle unit√†. Questa ridondanza aiuta a prevenire singoli punti di guasto e garantisce che le funzioni critiche rimangano operative nonostante i guasti hardware.</p>
<p>Inoltre, Tesla incorpora anche meccanismi di ridondanza aggiuntivi oltre al DMR. Ad esempio, utilizzano alimentatori ridondanti, sistemi di sterzo e frenata e diverse suite di sensori (ad esempio, telecamere, radar e sensori a ultrasuoni) per fornire pi√π livelli di tolleranza ai guasti. Queste ridondanze contribuiscono collettivamente alla sicurezza e all‚Äôaffidabilit√† complessive del sistema di guida autonoma.</p>
<p>√à importante notare che mentre DMR fornisce rilevamento guasti e un certo livello di tolleranza ai guasti, TMR pu√≤ fornire un diverso livello di mascheramento dei guasti. In DMR, se entrambe le unit√† subiscono guasti simultanei o il guasto influisce sul meccanismo di confronto, il sistema potrebbe non essere in grado di identificare il guasto. Pertanto, gli SDC di Tesla si basano su una combinazione di DMR e altri meccanismi di ridondanza per raggiungere un elevato livello di tolleranza ai guasti.</p>
<p>L‚Äôuso di DMR nel computer a guida autonoma di Tesla evidenzia l‚Äôimportanza della ridondanza hardware nelle applicazioni critiche per la sicurezza. Utilizzando unit√† di elaborazione ridondanti e confrontando i loro output, il sistema pu√≤ rilevare e mitigare i guasti, migliorando la sicurezza e l‚Äôaffidabilit√† complessive della funzionalit√† di guida autonoma.</p>
<p>Google utilizza ‚Äúhot spare‚Äù ridondanti per gestire i problemi SDC nei suoi data center, migliorando cos√¨ l‚Äôaffidabilit√† delle funzioni critiche. Come illustrato in <a href="#fig-sdc-controller" class="quarto-xref">Figura&nbsp;<span>17.13</span></a>, durante la normale fase di addestramento, pi√π ‚Äúworker‚Äù di training sincroni funzionano in modo impeccabile. Tuttavia, se un worker diventa difettoso e causa SDC, un verificatore SDC identifica automaticamente i problemi. Dopo aver rilevato l‚ÄôSDC, il verificatore SDC sposta il training su un hot spare e invia la macchina difettosa per la riparazione. Questa ridondanza salvaguarda la continuit√† e l‚Äôaffidabilit√† del training ML, riducendo al minimo i tempi di inattivit√† e preservando l‚Äôintegrit√† dei dati.</p>
<div id="fig-sdc-controller" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-controller-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/sdc_controller_google.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-controller-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.13: Google impiega ‚Äúcore hot spare‚Äù per gestire in modo trasparente gli SDC nel data center. Fonte: Jeff Dean, MLSys 2024 Keynote (Google)
</figcaption>
</figure>
</div>
<p><strong>Watchdog timer:</strong> I watchdog timer sono componenti hardware che monitorano l‚Äôesecuzione di attivit√† o processi critici <span class="citation" data-cites="pont2002using">(<a href="../../references.it.html#ref-pont2002using" role="doc-biblioref">Pont e Ong 2002</a>)</span>. Sono comunemente utilizzati per rilevare e ripristinare guasti software o hardware che causano la mancata risposta di un sistema o il suo blocco in un ciclo infinito. In un sistema embedded, un watchdog timer pu√≤ essere configurato per monitorare l‚Äôesecuzione del loop principale, come illustrato in <a href="#3l259jcz0lli"><span class="quarto-xref">Figura&nbsp;<span>17.14</span></span></a>. Il software reimposta periodicamente il watchdog timer per indicare che funziona correttamente. Supponiamo che il software non riesca a reimpostare il timer entro un limite di tempo specificato (periodo di timeout). In tal caso, il watchdog timer presuppone che il sistema abbia riscontrato un guasto e attiva un‚Äôazione di ripristino predefinita, come il reset del sistema o il passaggio a un componente di backup. I watchdog timer sono ampiamente utilizzati nell‚Äôelettronica automobilistica, nei sistemi di controllo industriale e in altre applicazioni critiche per la sicurezza per garantire il rilevamento e il ripristino tempestivi dai guasti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-pont2002using" class="csl-entry" role="listitem">
Pont, Michael J, e Royan HL Ong. 2002. <span>¬´Using watchdog timers to improve the reliability of single-processor embedded systems: <span>Seven</span> new patterns and a case study¬ª</span>. In <em>Proceedings of the First Nordic Conference on Pattern Languages of Programs</em>, 159‚Äì200. Citeseer.
</div></div><div id="fig-watchdog" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watchdog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/watchdog.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watchdog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.14: Esempio di watchdog timer nel rilevamento di guasti MCU. Fonte: <a href="https://www.ablic.com/en/semicon/products/automotive/automotive-watchdog-timer/intro/">Ablic</a>
</figcaption>
</figure>
</div>
</section>
<section id="rilevamento-guasti-a-livello-software" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="rilevamento-guasti-a-livello-software">Rilevamento guasti a livello software</h5>
<p>Le tecniche di rilevamento degli errori a livello software si basano su algoritmi software e meccanismi di monitoraggio per identificare gli errori di sistema. Queste tecniche possono essere implementate a vari livelli dello stack software, tra cui il sistema operativo, il middleware o il livello dell‚Äôapplicazione.</p>
<p><strong>Monitoraggio del runtime e rilevamento delle anomalie:</strong> Il monitoraggio del runtime comporta l‚Äôosservazione continua del comportamento del sistema e dei suoi componenti durante l‚Äôesecuzione <span class="citation" data-cites="francalanza2017foundation">(<a href="../../references.it.html#ref-francalanza2017foundation" role="doc-biblioref">Francalanza et al. 2017</a>)</span>. Aiuta a rilevare anomalie, errori o comportamenti imprevisti che potrebbero indicare la presenza di errori. Ad esempio, si consideri un sistema di classificazione delle immagini basato su ML distribuito in un‚Äôauto a guida autonoma. Il monitoraggio del runtime pu√≤ essere implementato per tracciare le prestazioni e il comportamento del modello di classificazione <span class="citation" data-cites="mahmoud2021issre">(<a href="../../references.it.html#ref-mahmoud2021issre" role="doc-biblioref">Mahmoud et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-francalanza2017foundation" class="csl-entry" role="listitem">
Francalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard, Ian Cassar, Dario Della Monica, e Anna Ing√≥lfsd√≥ttir. 2017. <span>¬´A foundation for runtime monitoring¬ª</span>. In <em>International Conference on Runtime Verification</em>, 8‚Äì29. Springer.
</div><div id="ref-mahmoud2021issre" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, e Stephen W. Keckler. 2021. <span>¬´Optimizing Selective Protection for <span>CNN</span> Resilience¬ª</span>. In <em>2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE)</em>, 127‚Äì38. IEEE. <a href="https://doi.org/10.1109/issre52982.2021.00025">https://doi.org/10.1109/issre52982.2021.00025</a>.
</div><div id="ref-chandola2009anomaly" class="csl-entry" role="listitem">
Chandola, Varun, Arindam Banerjee, e Vipin Kumar. 2009. <span>¬´Anomaly detection: A survey¬ª</span>. <em>ACM Comput. Surv.</em> 41 (3): 1‚Äì58. <a href="https://doi.org/10.1145/1541880.1541882">https://doi.org/10.1145/1541880.1541882</a>.
</div></div><p>Gli algoritmi di rilevamento delle anomalie possono essere applicati alle previsioni del modello o alle attivazioni di livelli intermedi, come il rilevamento statistico di valori anomali o approcci basati sull‚Äôapprendimento automatico (ad esempio, One-Class SVM o Autoencoders) <span class="citation" data-cites="chandola2009anomaly">(<a href="../../references.it.html#ref-chandola2009anomaly" role="doc-biblioref">Chandola, Banerjee, e Kumar 2009</a>)</span>. <a href="#a0u8fu59ui0r"><span class="quarto-xref">Figura&nbsp;<span>17.15</span></span></a> mostra un esempio di rilevamento delle anomalie. Supponiamo che il sistema di monitoraggio rilevi una deviazione significativa dai pattern previsti, come un calo improvviso dell‚Äôaccuratezza della classificazione o campioni fuori distribuzione. In tal caso, pu√≤ generare un ‚Äúalert‚Äù che indica un potenziale errore nel modello o nella pipeline dei dati di input. Questo rilevamento precoce consente di applicare strategie di intervento tempestivo e di mitigazione degli errori.</p>
<div id="fig-ad" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/ad.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.15: Esempi di rilevamento delle anomalie. (a) Rilevamento delle anomalie completamente supervisionato, (b) rilevamento delle anomalie solo normali, (c, d, e) rilevamento delle anomalie semi-supervisionato, (f) rilevamento delle anomalie non supervisionato. Fonte: <a href="https://www.google.com/url?sa=i&amp;url=http%3A%2F%2Fresearch.google%2Fblog%2Funsupervised-and-semi-supervised-anomaly-detection-with-data-centric-ml%2F&amp;psig=AOvVaw1p9owe13lxfZogUHTZnxrj&amp;ust=1714877457779000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBIQjRxqFwoTCIjMmMP-8oUDFQAAAAAdAAAAABAE">Google</a>
</figcaption>
</figure>
</div>
<p><strong>Controlli di coerenza e convalida dei dati:</strong> I controlli di coerenza e le tecniche di convalida dei dati garantiscono l‚Äôintegrit√† e la correttezza dei dati in diverse fasi di elaborazione in un sistema ML <span class="citation" data-cites="lindholm2019data">(<a href="../../references.it.html#ref-lindholm2019data" role="doc-biblioref">Lindholm et al. 2019</a>)</span>. Questi controlli aiutano a rilevare danneggiamenti dei dati, incongruenze o errori che potrebbero propagarsi e influenzare il comportamento del sistema. Esempio: In un sistema ML distribuito in cui pi√π nodi collaborano per addestrare un modello, √® possibile implementare controlli di coerenza per convalidare l‚Äôintegrit√† dei parametri condivisi del modello. Ogni nodo pu√≤ calcolare un checksum o un hash dei parametri del modello prima e dopo l‚Äôiterazione di addestramento, come mostrato in <a href="#fig-ad" class="quarto-xref">Figura&nbsp;<span>17.15</span></a>. Eventuali incongruenze o danneggiamenti dei dati possono essere rilevati confrontando i checksum tra i nodi. Inoltre, √® possibile applicare controlli di intervallo ai dati di input e agli output del modello per garantire che rientrino nei limiti previsti. Ad esempio, se il sistema di percezione di un veicolo autonomo rileva un oggetto con dimensioni o velocit√† non realistiche, pu√≤ indicare un errore nei dati del sensore o negli algoritmi di percezione <span class="citation" data-cites="wan2023vpp">(<a href="../../references.it.html#ref-wan2023vpp" role="doc-biblioref">Wan et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lindholm2019data" class="csl-entry" role="listitem">
Lindholm, Andreas, Dave Zachariah, Petre Stoica, e Thomas B. Schon. 2019. <span>¬´Data Consistency Approach to Model Validation¬ª</span>. <em>#IEEE_O_ACC#</em> 7: 59788‚Äì96. <a href="https://doi.org/10.1109/access.2019.2915109">https://doi.org/10.1109/access.2019.2915109</a>.
</div><div id="ref-wan2023vpp" class="csl-entry" role="listitem">
Wan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, e Y Zhu. 2023. <span>¬´Vpp: <span>The</span> vulnerability-proportional protection paradigm towards reliable autonomous machines¬ª</span>. In <em>Proceedings of the 5th International Workshop on Domain Specific System Architecture (DOSSA)</em>, 1‚Äì6.
</div><div id="ref-kawazoe1997heartbeat" class="csl-entry" role="listitem">
Kawazoe Aguilera, Marcos, Wei Chen, e Sam Toueg. 1997. <span>¬´Heartbeat: <span>A</span> timeout-free failure detector for quiescent reliable communication¬ª</span>. In <em>Distributed Algorithms: 11th International Workshop, WDAG‚Äô97 Saarbr√ºcken, Germany, September 24<span></span>26, 1997 Proceedings 11</em>, 126‚Äì40. Springer.
</div></div><p><strong>Meccanismi di heartbeat e timeout:</strong> I meccanismi di heartbeat e timeout sono comunemente utilizzati per rilevare errori nei sistemi distribuiti e garantire la vitalit√† e la reattivit√† dei componenti <span class="citation" data-cites="kawazoe1997heartbeat">(<a href="../../references.it.html#ref-kawazoe1997heartbeat" role="doc-biblioref">Kawazoe Aguilera, Chen, e Toueg 1997</a>)</span>. Sono molto simili ai timer watchdog presenti nell‚Äôhardware. Ad esempio, in un sistema ML distribuito, in cui pi√π nodi collaborano per eseguire attivit√† quali pre-elaborazione dei dati, training del modello o inferenza, √® possibile implementare meccanismi heartbeat per monitorare lo stato e la disponibilit√† di ciascun nodo. Ogni nodo invia periodicamente un messaggio heartbeat a un coordinatore centrale o ai suoi nodi peer, indicando il suo stato e la sua disponibilit√†. Supponiamo che un nodo non riesca a inviare un heartbeat entro un periodo di timeout specificato, come mostrato in <a href="#ojufkz2g56e"><span class="quarto-xref">Figura&nbsp;<span>17.16</span></span></a>. In tal caso, viene considerato difettoso e possono essere intraprese azioni appropriate, come la ridistribuzione del carico di lavoro o l‚Äôavvio di un meccanismo di ‚Äúfailover‚Äù. I timeout possono anche essere utilizzati per rilevare e gestire componenti bloccati o non reattivi. Ad esempio, se un processo di caricamento dati supera una soglia di timeout predefinita, potrebbe indicare un errore nella pipeline dati e il sistema pu√≤ adottare misure correttive.</p>
<div id="fig-heartbeat" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-heartbeat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/heartbeat.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-heartbeat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.16: Messaggi heartbeat nei sistemi distribuiti. Fonte: <a href="https://www.geeksforgeeks.org/what-are-heartbeat-messages/">GeeksforGeeks</a>
</figcaption>
</figure>
</div>
<!-- @fig-Reed-Solomon Heartbeat messages in distributed systems. Source: [GeeksforGeeks](https://www.geeksforgeeks.org/what-are-heartbeat-messages/) -->
<p><strong>Tecniche di ‚ÄúSoftware-implemented fault tolerance (SIFT)‚Äù:</strong> Le tecniche SIFT introducono meccanismi di ridondanza e rilevamento degli errori a livello software per migliorare l‚Äôaffidabilit√† e la tolleranza agli errori del sistema <span class="citation" data-cites="reis2005swift">(<a href="../../references.it.html#ref-reis2005swift" role="doc-biblioref">Reis et al. 2005</a>)</span>. Esempio: La programmazione N-version √® una tecnica SIFT in cui pi√π versioni di componenti software funzionalmente equivalenti vengono sviluppate in modo indipendente da team diversi. Questo pu√≤ essere applicato a componenti critici come il motore di inferenza del modello in un sistema ML. Pi√π versioni del motore di inferenza possono essere eseguite in parallelo e i loro output possono essere confrontati per coerenza. √à considerato il risultato corretto se la maggior parte delle versioni produce lo stesso output. Se c‚Äô√® una discrepanza, indica un potenziale errore in una o pi√π versioni e possono essere attivati meccanismi di gestione degli errori appropriati. Un altro esempio √® l‚Äôutilizzo di codici di correzione degli errori basati su software, come i codici Reed-Solomon <span class="citation" data-cites="plank1997tutorial">(<a href="../../references.it.html#ref-plank1997tutorial" role="doc-biblioref">Plank 1997</a>)</span>, per rilevare e correggere errori nell‚Äôarchiviazione o nella trasmissione dei dati, come mostrato in <a href="#kjmtegsny44z"><span class="quarto-xref">Figura&nbsp;<span>17.17</span></span></a>. Questi codici aggiungono ridondanza ai dati, consentendo di rilevare e correggere determinati errori e migliorare la tolleranza agli errori del sistema.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reis2005swift" class="csl-entry" role="listitem">
Reis, G. A., J. Chang, N. Vachharajani, R. Rangan, e D. I. August. 2005. <span>¬´<span>SWIFT:</span> <span>Software</span> Implemented Fault Tolerance¬ª</span>. In <em>International Symposium on Code Generation and Optimization</em>, 243‚Äì54. IEEE; IEEE. <a href="https://doi.org/10.1109/cgo.2005.34">https://doi.org/10.1109/cgo.2005.34</a>.
</div><div id="ref-plank1997tutorial" class="csl-entry" role="listitem">
Plank, James S. 1997. <span>¬´A tutorial on <span>Reed<span></span>Solomon</span> coding for fault-tolerance in <span>RAID</span>-like systems¬ª</span>. <em>Software: Practice and Experience</em> 27 (9): 995‚Äì1012.
</div></div><div id="fig-Reed-Solomon" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Reed-Solomon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/Reed-Solomon.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Reed-Solomon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.17: Rappresentazione a n bit dei codici Reed-Solomon. Fonte: <a href="https://www.geeksforgeeks.org/what-is-reed-solomon-code/">GeeksforGeeks</a>
</figcaption>
</figure>
</div>
<div id="exr-ad" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;17.1: Rilevamento delle Anomalie
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In questo Colab, si svolge il ruolo di un detective di guasti IA! Si costruir√† un rilevatore di anomalie basato su autoencoder per individuare gli errori nei dati sulla salute cardiaca. Si scopre come identificare i malfunzionamenti nei sistemi ML, un‚Äôabilit√† fondamentale per creare un‚ÄôIA affidabile. Utilizzeremo Keras Tuner per mettere a punto l‚Äôautoencoder per un rilevamento di guasti di prim‚Äôordine. Questa esperienza si collega direttamente al capitolo Robust AI, dimostrando l‚Äôimportanza del rilevamento di guasti in applicazioni reali come l‚Äôassistenza sanitaria e i sistemi autonomi. Preparatevi a rafforzare l‚Äôaffidabilit√† delle creazioni IA!</p>
<p><a href="https://colab.research.google.com/drive/1TXaQzsSj2q0E3Ni1uxFDXGpY1SCnu46v?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="riepilogo" class="level3" data-number="17.3.5">
<h3 data-number="17.3.5" class="anchored" data-anchor-id="riepilogo"><span class="header-section-number">17.3.5</span> Riepilogo</h3>
<p><a href="#tbl-fault_types" class="quarto-xref">Tabella&nbsp;<span>17.1</span></a> fornisce un‚Äôanalisi comparativa estesa di guasti transitori, permanenti e intermittenti. Descrive le caratteristiche o dimensioni primarie che distinguono questi tipi di guasti. Qui, riassumiamo le dimensioni rilevanti che abbiamo esaminato ed esploriamo le sfumature che differenziano i guasti transitori, permanenti e intermittenti in modo pi√π dettagliato.</p>
<div id="tbl-fault_types" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-fault_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;17.1: Confronto tra guasti transitori, permanenti e intermittenti.
</figcaption>
<div aria-describedby="tbl-fault_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 25%">
<col style="width: 29%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Dimensione</th>
<th style="text-align: left;">Guasti Transitori</th>
<th style="text-align: left;">Guasti Permanenti</th>
<th style="text-align: left;">Guasti intermittenti</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Durata</td>
<td style="text-align: left;">Di breve durata, temporaneo</td>
<td style="text-align: left;">Persistente, rimane fino alla riparazione o alla sostituzione</td>
<td style="text-align: left;">Sporadica, appare e scompare in modo intermittente</td>
</tr>
<tr class="even">
<td style="text-align: left;">Persistenza</td>
<td style="text-align: left;">Scompare dopo che la condizione di errore √® passata</td>
<td style="text-align: left;">√à costantemente presente finch√© non viene affrontato</td>
<td style="text-align: left;">Si ripete in modo irregolare, non sempre presente</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cause</td>
<td style="text-align: left;">Fattori esterni (ad esempio, interferenza elettromagnetica raggi cosmici)</td>
<td style="text-align: left;">Difetti hardware, danni fisici, usura</td>
<td style="text-align: left;">Condizioni hardware instabili, connessioni allentate, componenti obsoleti</td>
</tr>
<tr class="even">
<td style="text-align: left;">Manifestazione</td>
<td style="text-align: left;">Bit flip, glitch, danneggiamento temporaneo dei dati</td>
<td style="text-align: left;">Errori bloccati, componenti rotti, guasti completi del dispositivo</td>
<td style="text-align: left;">Bit flip occasionali, problemi di segnale intermittenti, malfunzionamenti sporadici</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Impatto sui Sistemi ML</td>
<td style="text-align: left;">Introduce errori temporanei o rumore nei calcoli</td>
<td style="text-align: left;">Causa errori o guasti costanti, che influiscono sull‚Äôaffidabilit√†</td>
<td style="text-align: left;">Porta a errori sporadici e imprevedibili, difficili da diagnosticare e mitigare</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rilevamento</td>
<td style="text-align: left;">Codici di rilevamento degli errori, confronto con i valori previsti</td>
<td style="text-align: left;">Autotest integrati, codici di rilevamento degli errori, controlli di coerenza</td>
<td style="text-align: left;">Monitoraggio delle anomalie, analisi di pattern di errore e correlazioni</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mitigazione</td>
<td style="text-align: left;">Codici di correzione degli errori, ridondanza, checkpoint e riavvio</td>
<td style="text-align: left;">Riparazione o sostituzione hardware, ridondanza dei componenti, meccanismi di failover</td>
<td style="text-align: left;">Progettazione robusta, controllo ambientale, monitoraggio del runtime, tecniche di tolleranza agli errori</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="robustezza-del-modello-ml" class="level2 page-columns page-full" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="robustezza-del-modello-ml"><span class="header-section-number">17.4</span> Robustezza del Modello ML</h2>
<section id="attacchi-avversari" class="level3 page-columns page-full" data-number="17.4.1">
<h3 data-number="17.4.1" class="anchored" data-anchor-id="attacchi-avversari"><span class="header-section-number">17.4.1</span> Attacchi Avversari</h3>
<section id="definizione-e-caratteristiche-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definizione-e-caratteristiche-3">Definizione e Caratteristiche</h4>
<p>Gli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) <span class="citation" data-cites="parrish2023adversarial">(<a href="../../references.it.html#ref-parrish2023adversarial" role="doc-biblioref">Parrish et al. 2023</a>)</span>. Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono ‚Äúhackerare‚Äù il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui piccole, spesso impercettibili modifiche ai dati di input possono indurre un modello ML a fare una previsione errata, come mostrato in <a href="#fig-adversarial-attack-noise-example" class="quarto-xref">Figura&nbsp;<span>17.18</span></a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-parrish2023adversarial" class="csl-entry" role="listitem">
Parrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. <span>¬´Adversarial Nibbler: <span>A</span> Data-Centric Challenge for Improving the Safety of Text-to-Image Models¬ª</span>. <em>ArXiv preprint</em> abs/2305.14384. <a href="https://arxiv.org/abs/2305.14384">https://arxiv.org/abs/2305.14384</a>.
</div></div><div id="fig-adversarial-attack-noise-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-attack-noise-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/adversarial_attack_detection.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-attack-noise-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.18: Un piccolo rumore avversario aggiunto all‚Äôimmagine originale pu√≤ far s√¨ che la rete neurale classifichi l‚Äôimmagine come un Guacamole anzich√© come un gatto egiziano. Fonte: <a href="https://www.mdpi.com/2079-9292/10/1/52">Sutanto</a>
</figcaption>
</figure>
</div>
<p>√à possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE <span class="citation" data-cites="ramesh2021zero">(<a href="../../references.it.html#ref-ramesh2021zero" role="doc-biblioref">Ramesh et al. 2021</a>)</span> o Stable Diffusion <span class="citation" data-cites="rombach2022highresolution">(<a href="../../references.it.html#ref-rombach2022highresolution" role="doc-biblioref">Rombach et al. 2022</a>)</span>. Ad esempio, alterando i valori dei pixel di un‚Äôimmagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ramesh2021zero" class="csl-entry" role="listitem">
Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. <span>¬´Zero-Shot Text-to-Image Generation¬ª</span>. In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, a cura di Marina Meila e Tong Zhang, 139:8821‚Äì31. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/ramesh21a.html">http://proceedings.mlr.press/v139/ramesh21a.html</a>.
</div><div id="ref-rombach2022highresolution" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. <span>¬´High-Resolution Image Synthesis with Latent Diffusion Models¬ª</span>. In <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE. <a href="https://doi.org/10.1109/cvpr52688.2022.01042">https://doi.org/10.1109/cvpr52688.2022.01042</a>.
</div></div><p>Gli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l‚Äôinferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input speciali con perturbazioni per confondere il riconoscimento degli pattern del modello, in pratica ‚Äúhackerando‚Äù le percezioni del modello.</p>
<p>Gli attacchi avversari rientrano in diversi scenari:</p>
<ul>
<li><p><strong>Attacchi Whitebox:</strong> L‚Äôattaccante conosce perfettamente il funzionamento interno del modello target, inclusi i dati di training, i parametri e l‚Äôarchitettura <span class="citation" data-cites="ye2021thundernna">(<a href="../../references.it.html#ref-ye2021thundernna" role="doc-biblioref">Ye e Hamidi 2021</a>)</span>. Questo accesso completo crea condizioni favorevoli per gli aggressori per sfruttare le vulnerabilit√† del modello. L‚Äôattaccante pu√≤ usare debolezze specifiche e sottili per creare esempi avversari efficaci.</p></li>
<li><p><strong>Attacchi Blackbox:</strong> A differenza degli attacchi White-box, i Black-box implicano che l‚Äôattaccante abbia poca o nessuna conoscenza del modello target <span class="citation" data-cites="guo2019simple">(<a href="../../references.it.html#ref-guo2019simple" role="doc-biblioref">Guo et al. 2019</a>)</span>. Per eseguire l‚Äôattacco, l‚Äôattore avversario deve osservare attentamente il comportamento dell‚Äôoutput del modello.</p></li>
<li><p><strong>Attacchi Greybox:</strong> Si collocano tra gli attacchi Blackbox e Whitebox. L‚Äôattaccante ha solo una conoscenza parziale della progettazione interna del modello target <span class="citation" data-cites="xu2021grey">(<a href="../../references.it.html#ref-xu2021grey" role="doc-biblioref">Xu et al. 2021</a>)</span>. Ad esempio, l‚Äôattaccante potrebbe avere conoscenza dei dati di training ma non dell‚Äôarchitettura o dei parametri. Nel mondo reale, gli attacchi pratici rientrano solitamente nelle categorie black-box o grey-box.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-ye2021thundernna" class="csl-entry" role="listitem">
Ye, Linfeng, e Shayan Mohajer Hamidi. 2021. <span>¬´Thundernna: <span>A</span> white box adversarial attack¬ª</span>. <em>arXiv preprint arXiv:2111.12305</em>.
</div><div id="ref-guo2019simple" class="csl-entry" role="listitem">
Guo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, e Kilian Weinberger. 2019. <span>¬´Simple black-box adversarial attacks¬ª</span>. In <em>International conference on machine learning</em>, 2484‚Äì93. PMLR.
</div><div id="ref-xu2021grey" class="csl-entry" role="listitem">
Xu, Ying, Xu Zhong, Antonio Jimeno Yepes, e Jey Han Lau. 2021. <span>¬´<span>Grey</span>-box adversarial attack and defence for sentiment classification¬ª</span>. <em>arXiv preprint arXiv:2103.11576</em>.
</div></div><p>Il panorama dei modelli di apprendimento automatico √® complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilit√† all‚Äôinterno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:</p>
<ul>
<li><p>Le <strong>Generative Adversarial Network (GAN)</strong> sono modelli di deep learning costituiti da due reti in competizione tra loro: un generatore e un discriminatore <span class="citation" data-cites="goodfellow2020generative">(<a href="../../references.it.html#ref-goodfellow2020generative" role="doc-biblioref">Goodfellow et al. 2020</a>)</span>. Il generatore cerca di sintetizzare dati realistici mentre il discriminatore valuta se sono reali o falsi. Le GAN possono essere utilizzate per creare esempi avversari. La rete del generatore √® addestrata per produrre input che il modello target classifica erroneamente. Queste immagini generate da GAN possono quindi attaccare un classificatore target o un modello di rilevamento. Il generatore e il modello target sono impegnati in un processo competitivo, con il generatore che migliora continuamente la sua capacit√† di creare esempi ingannevoli e il modello target che aumenta la sua resistenza a tali esempi. Le reti GAN forniscono un potente framework per la creazione di input avversari complessi e diversificati, dimostrando l‚Äôadattabilit√† dei modelli generativi nel panorama avversario.</p></li>
<li><p>I <strong>Transfer Learning Adversarial Attacks</strong> [attacchi avversari di apprendimento di trasferimento] sfruttano la conoscenza trasferita da un modello pre-addestrato a un modello target, creando esempi avversari che possono ingannare entrambi i modelli. Questi attacchi rappresentano una preoccupazione crescente, in particolare quando gli avversari hanno conoscenza dell‚Äôestrattore di feature ma non hanno accesso alla testa di classificazione (la parte o il layer responsabile della creazione delle classificazioni finali). Denominate ‚Äúattacchi headless‚Äù, queste strategie avversarie trasferibili sfruttano le capacit√† espressive degli estrattori di feature per creare perturbazioni, senza tenere conto dello spazio delle etichette o dei dati di addestramento. L‚Äôesistenza di tali attacchi sottolinea l‚Äôimportanza di sviluppare difese robuste per le applicazioni di apprendimento tramite trasferimento, soprattutto perch√© i modelli pre-addestrati sono comunemente utilizzati <span class="citation" data-cites="ahmed2020headless">(<a href="../../references.it.html#ref-ahmed2020headless" role="doc-biblioref">Abdelkader et al. 2020</a>)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-goodfellow2020generative" class="csl-entry" role="listitem">
Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. <span>¬´Generative adversarial networks¬ª</span>. <em>Commun. ACM</em> 63 (11): 139‚Äì44. <a href="https://doi.org/10.1145/3422622">https://doi.org/10.1145/3422622</a>.
</div><div id="ref-ahmed2020headless" class="csl-entry" role="listitem">
Abdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. <span>¬´Headless Horseman: <span>Adversarial</span> Attacks on Transfer Learning Models¬ª</span>. In <em>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 3087‚Äì91. IEEE. <a href="https://doi.org/10.1109/icassp40776.2020.9053181">https://doi.org/10.1109/icassp40776.2020.9053181</a>.
</div></div></section>
<section id="meccanismi-degli-attacchi-avversari" class="level4">
<h4 class="anchored" data-anchor-id="meccanismi-degli-attacchi-avversari">Meccanismi degli Attacchi Avversari</h4>
<div id="fig-gradient-attack" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-attack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/gradient_attack.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-attack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.19: Attacchi Basati sul Gradiente. Fonte: <a href="https://defence.ai/ai-security/gradient-based-attacks/">Ivezic</a>
</figcaption>
</figure>
</div>
<p><strong>Attacchi Basati sul Gradiente</strong></p>
<p>Una categoria importante di attacchi avversari √® quella degli attacchi basati sul gradiente. Questi attacchi sfruttano i gradienti della funzione di perdita del modello ML per creare esempi avversari. Il <a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">Fast Gradient Sign Method</a> (FGSM) √® una tecnica ben nota in questa categoria. FGSM perturba i dati di input aggiungendo un piccolo rumore nella direzione del gradiente, con l‚Äôobiettivo di massimizzare l‚Äôerrore di previsione del modello. FGSM pu√≤ generare rapidamente esempi avversari, come mostrato in <a href="#fig-gradient-attack" class="quarto-xref">Figura&nbsp;<span>17.19</span></a>, eseguendo un singolo passaggio nella direzione del gradiente.</p>
<p>Un‚Äôaltra variante, l‚Äôattacco ‚ÄúProjected Gradient Descent (PGD)‚Äù, estende FGSM applicando iterativamente la fase di aggiornamento del gradiente, consentendo esempi avversari pi√π raffinati e potenti. L‚Äôattacco ‚ÄúJacobian-based Saliency Map (JSMA)‚Äù √® un altro approccio basato sul gradiente che identifica le caratteristiche di input pi√π influenti e le perturba per creare esempi avversari.</p>
<p><strong>Attacchi Basati sull‚ÄôOttimizzazione</strong></p>
<p>Questi attacchi formulano la generazione di esempi avversari come un problema di ottimizzazione. L‚Äôattacco Carlini e Wagner (C&amp;W) √® un esempio importante in questa categoria. Trova la perturbazione pi√π piccola che pu√≤ causare una classificazione errata mantenendo la somiglianza percettiva con l‚Äôinput originale. L‚Äôattacco C&amp;W impiega un processo di ottimizzazione iterativo per ridurre al minimo la perturbazione massimizzando al contempo l‚Äôerrore di previsione del modello.</p>
<p>Un altro approccio basato sull‚Äôottimizzazione √® l‚ÄôElastic Net Attack to DNNs (EAD), che incorpora la regolarizzazione elastica della rete per generare esempi avversari con perturbazioni sparse.</p>
<p><strong>Attacchi Basati sul Trasferimento</strong></p>
<p>Gli attacchi basati sul trasferimento sfruttano la propriet√† di trasferibilit√† degli esempi avversari. La trasferibilit√† si riferisce al fenomeno per cui gli esempi avversari creati per un modello ML possono spesso ingannare altri modelli, anche se hanno architetture diverse o sono stati addestrati su set di dati diversi. Ci√≤ consente agli aggressori di generare esempi avversari utilizzando un modello surrogato e quindi trasferirli al modello target senza richiedere l‚Äôaccesso diretto ai suoi parametri o gradienti. Gli attacchi basati sul trasferimento evidenziano la generalizzazione delle vulnerabilit√† avversarie su diversi modelli e il potenziale per attacchi black-box.</p>
<p><strong>Attacchi nel Mondo Fisico</strong></p>
<p>Gli attacchi nel mondo fisico portano gli esempi avversari nel regno degli scenari del mondo reale. Questi attacchi comportano la creazione di oggetti fisici o manipolazioni che possono ingannare i modelli ML quando vengono catturati da sensori o telecamere. Le patch avversarie, ad esempio, sono piccole patch progettate con cura che possono essere posizionate sugli oggetti per ingannare i modelli di rilevamento o classificazione degli oggetti. Quando vengono applicate a oggetti del mondo reale, queste patch possono causare una classificazione errata dei modelli o il mancato rilevamento accurato degli oggetti. Gli oggetti avversari, come sculture stampate in 3D o segnali stradali modificati, possono anche essere creati per ingannare i sistemi ML in ambienti fisici.</p>
<p><strong>Riepilogo</strong></p>
<p><a href="#tbl-attack_types" class="quarto-xref">Tabella&nbsp;<span>17.2</span></a> una panoramica concisa delle diverse categorie di attacchi avversari, tra cui attacchi basati su gradiente (FGSM, PGD, JSMA), attacchi basati sull‚Äôottimizzazione (C&amp;W, EAD), attacchi basati sul trasferimento e attacchi nel mondo fisico (patch e oggetti avversari). Ogni attacco viene brevemente descritto, evidenziandone le caratteristiche e i meccanismi principali.</p>
<div id="tbl-attack_types" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-attack_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Tabella&nbsp;17.2: Diversi tipi di attacco sui modelli ML.
</figcaption>
<div aria-describedby="tbl-attack_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 20%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Categoria di attacco</th>
<th style="text-align: left;">Nome attacco</th>
<th style="text-align: left;">Descrizione</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Basato sul gradiente</td>
<td style="text-align: left;">Fast Gradient Sign Method (FGSM) Projected Gradient Descent (PGD) Jacobian-based Saliency Map Attack (JSMA)</td>
<td style="text-align: left;">Perturba i dati di input aggiungendo un piccolo rumore nella direzione del gradiente per massimizzare l‚Äôerrore di previsione. Estende FGSM applicando iterativamente il passaggio di aggiornamento del gradiente per esempi avversari pi√π raffinati. Identifica le caratteristiche di input influenti e le perturba per creare esempi avversari.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Basato sull‚Äôottimizzazione</td>
<td style="text-align: left;">Carlini and Wagner (C&amp;W) Attack Elastic Net Attack to DNNs (EAD)</td>
<td style="text-align: left;">Trova la perturbazione pi√π piccola che causa una classificazione errata mantenendo la somiglianza percettiva. Incorpora la regolarizzazione elastica della rete per generare esempi avversari con perturbazioni sparse.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Basato sul trasferimento</td>
<td style="text-align: left;">Transferability-based Attacks</td>
<td style="text-align: left;">Sfrutta la trasferibilit√† di esempi avversari su modelli diversi, consentendo attacchi black-box.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mondo fisico</td>
<td style="text-align: left;">Adversarial Patches Adversarial Objects</td>
<td style="text-align: left;">Piccole patch attentamente progettate, posizionate sugli oggetti per ingannare i modelli di rilevamento o classificazione degli oggetti. Oggetti fisici (ad esempio, sculture stampate in 3D, segnali stradali modificati) creati per ingannare i sistemi ML in scenari del mondo reale.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>I meccanismi degli attacchi avversari rivelano l‚Äôintricata interazione tra i limiti decisionali del modello ML, i dati di input e gli obiettivi dell‚Äôattaccante. Manipolando attentamente i dati di input, gli aggressori possono sfruttare le sensibilit√† e i punti ciechi del modello, portando a previsioni errate. Il successo degli attacchi avversari evidenzia la necessit√† di una comprensione pi√π approfondita delle propriet√† di robustezza e generalizzazione dei modelli ML.</p>
<p>La difesa dagli attacchi avversari richiede un approccio multiforme. L‚Äôaddestramento avversario √® una strategia di difesa comune in cui i modelli vengono addestrati su esempi avversari per migliorare la robustezza. Esporre il modello a esempi avversari durante l‚Äôaddestramento gli insegna a classificarli correttamente e a diventare pi√π resiliente agli attacchi. La distillazione difensiva, la preelaborazione degli input e i metodi di ensemble sono altre tecniche che possono aiutare a mitigare l‚Äôimpatto degli attacchi avversari.</p>
<p>Man mano che l‚Äôapprendimento automatico avversario si evolve, i ricercatori esplorano nuovi meccanismi di attacco e sviluppano difese pi√π sofisticate. La corsa agli armamenti tra aggressori e difensori spinge la necessit√† di innovazione e vigilanza costanti nel proteggere i sistemi ML dalle minacce avversarie. Comprendere i meccanismi degli attacchi avversari √® fondamentale per sviluppare modelli ML robusti e affidabili in grado di resistere al panorama in continua evoluzione degli esempi avversari.</p>
</section>
<section id="impatto-sui-sistemi-ml-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impatto-sui-sistemi-ml-3">Impatto sui Sistemi ML</h4>
<p>Gli attacchi avversari sui sistemi di apprendimento automatico sono emersi come una preoccupazione significativa negli ultimi anni, evidenziando le potenziali vulnerabilit√† e i rischi associati all‚Äôadozione diffusa delle tecnologie ML. Questi attacchi comportano perturbazioni attentamente studiate per immettere dati che possono ingannare o fuorviare i modelli ML, portando a previsioni errate o classificazioni errate, come mostrato in <a href="#fig-adversarial-googlenet" class="quarto-xref">Figura&nbsp;<span>17.20</span></a>. L‚Äôimpatto degli attacchi avversari sui sistemi ML √® di vasta portata e pu√≤ avere gravi conseguenze in vari domini.</p>
<p>Un esempio lampante dell‚Äôimpatto degli attacchi avversari √® stato dimostrato dai ricercatori nel 2017. Hanno sperimentato piccoli adesivi in bianco e nero sui segnali di stop <span class="citation" data-cites="eykholt2018robust">(<a href="../../references.it.html#ref-eykholt2018robust" role="doc-biblioref">Eykholt et al. 2017</a>)</span>. All‚Äôocchio umano, questi adesivi non oscuravano il segnale n√© ne impedivano l‚Äôinterpretazione. Tuttavia, quando le immagini dei segnali di stop modificati dagli adesivi sono state inserite nei modelli ML standard di classificazione dei segnali stradali, √® emerso un risultato scioccante. I modelli hanno classificato erroneamente i segnali di stop come segnali di limite di velocit√† nell‚Äô85% dei casi.</p>
<div class="no-row-height column-margin column-container"><div id="ref-eykholt2018robust" class="csl-entry" role="listitem">
Eykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. <span>¬´Robust Physical-World Attacks on Deep Learning Models¬ª</span>. <em>ArXiv preprint</em> abs/1707.08945. <a href="https://arxiv.org/abs/1707.08945">https://arxiv.org/abs/1707.08945</a>.
</div></div><p>Questa dimostrazione ha fatto luce sul potenziale allarmante di semplici adesivi avversari per ingannare i sistemi ML e fargli interpretare male i segnali stradali critici. Le implicazioni di tali attacchi nel mondo reale sono significative, in particolare nel contesto dei veicoli autonomi. Se utilizzati su strade reali, questi adesivi avversari potrebbero far s√¨ che le auto a guida autonoma interpretino erroneamente i segnali di stop come limiti di velocit√†, portando a situazioni pericolose, come mostrato in <a href="#fig-graffiti" class="quarto-xref">Figura&nbsp;<span>17.21</span></a>. I ricercatori hanno avvertito che ci√≤ potrebbe causare arresti a rotazione o accelerazioni involontarie negli incroci, mettendo a repentaglio la sicurezza pubblica.</p>
<div id="fig-adversarial-googlenet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-googlenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/adversarial_googlenet.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-googlenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.20: Generazione di esempi avversari applicata a GoogLeNet (Szegedy et al., 2014a) su ImageNet. Fonte: <a href="https://arxiv.org/abs/1412.6572">Goodfellow</a>
</figcaption>
</figure>
</div>
<div id="fig-graffiti" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graffiti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/graffiti.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graffiti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.21: I graffiti su un segnale di stop hanno ingannato un‚Äôauto a guida autonoma facendole credere che si trattasse di un segnale di limite di velocit√† di 45 mph. Fonte: <a href="https://arxiv.org/abs/1707.08945">Eykholt</a>
</figcaption>
</figure>
</div>
<p>Il caso di studio degli adesivi avversari sui segnali di stop fornisce un‚Äôillustrazione concreta di come gli esempi avversari sfruttino il modo in cui i modelli ML riconoscono i pattern. Manipolando in modo sottile i dati di input in modi invisibili agli esseri umani, gli aggressori possono indurre previsioni errate e creare gravi rischi, specialmente in applicazioni critiche per la sicurezza come i veicoli autonomi. La semplicit√† dell‚Äôattacco evidenzia la vulnerabilit√† dei modelli ML anche a piccole modifiche nell‚Äôinput, sottolineando la necessit√† di difese robuste contro tali minacce.</p>
<p>L‚Äôimpatto degli attacchi avversari si estende oltre il degrado delle prestazioni del modello. Questi attacchi sollevano notevoli preoccupazioni in termini di sicurezza e protezione, in particolare nei domini in cui i modelli ML sono utilizzati per prendere decisioni critiche. Nelle applicazioni sanitarie, gli attacchi avversari sui modelli di imaging medico potrebbero portare a diagnosi errate o raccomandazioni di trattamento errate, mettendo a repentaglio il benessere del paziente <span class="citation" data-cites="tsai2023adversarial">(<a href="../../references.it.html#ref-tsai2023adversarial" role="doc-biblioref">M.-J. Tsai, Lin, e Lee 2023</a>)</span>. Nei sistemi finanziari, gli attacchi avversari potrebbero consentire frodi o manipolazioni di algoritmi di trading, con conseguenti perdite economiche sostanziali.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tsai2023adversarial" class="csl-entry" role="listitem">
Tsai, Min-Jen, Ping-Yi Lin, e Ming-En Lee. 2023. <span>¬´Adversarial Attacks on Medical Image Classification¬ª</span>. <em>Cancers</em> 15 (17): 4228. <a href="https://doi.org/10.3390/cancers15174228">https://doi.org/10.3390/cancers15174228</a>.
</div><div id="ref-fursov2021adversarial" class="csl-entry" role="listitem">
Fursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, e Evgeny Burnaev. 2021. <span>¬´Adversarial Attacks on Deep Models for Financial Transaction Records¬ª</span>. In <em>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp;amp; Data Mining</em>, 2868‚Äì78. ACM. <a href="https://doi.org/10.1145/3447548.3467145">https://doi.org/10.1145/3447548.3467145</a>.
</div></div><p>Inoltre, le vulnerabilit√† avversarie compromettono l‚Äôaffidabilit√† e l‚Äôinterpretabilit√† dei modelli ML. Se perturbazioni attentamente realizzate possono facilmente ingannare i modelli, la fiducia nelle loro previsioni e decisioni si erode. Gli esempi avversari espongono la dipendenza dei modelli da pattern superficiali e l‚Äôincapacit√† di catturare i veri concetti sottostanti, mettendo in discussione l‚Äôaffidabilit√† dei sistemi ML <span class="citation" data-cites="fursov2021adversarial">(<a href="../../references.it.html#ref-fursov2021adversarial" role="doc-biblioref">Fursov et al. 2021</a>)</span>.</p>
<p>La difesa dagli attacchi avversari richiede spesso risorse computazionali aggiuntive e pu√≤ influire sulle prestazioni complessive del sistema. Tecniche come l‚Äôaddestramento avversariale, in cui i modelli vengono addestrati su esempi avversari per migliorare la robustezza, possono aumentare significativamente i tempi di addestramento e i requisiti computazionali <span class="citation" data-cites="bai2021recent">(<a href="../../references.it.html#ref-bai2021recent" role="doc-biblioref">Bai et al. 2021</a>)</span>. I meccanismi di rilevamento e mitigazione del runtime, come la preelaborazione dell‚Äôinput <span class="citation" data-cites="addepalli2020towards">(<a href="../../references.it.html#ref-addepalli2020towards" role="doc-biblioref">Addepalli et al. 2020</a>)</span> o i controlli di coerenza delle previsioni, introducono latenza e influenzano le prestazioni in tempo reale dei sistemi ML.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bai2021recent" class="csl-entry" role="listitem">
Bai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, e Qian Wang. 2021. <span>¬´Recent advances in adversarial training for adversarial robustness¬ª</span>. <em>arXiv preprint arXiv:2102.01356</em>.
</div><div id="ref-addepalli2020towards" class="csl-entry" role="listitem">
Addepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, e R. Venkatesh Babu. 2020. <span>¬´Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes¬ª</span>. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 1020‚Äì29. IEEE. <a href="https://doi.org/10.1109/cvpr42600.2020.00110">https://doi.org/10.1109/cvpr42600.2020.00110</a>.
</div></div><p>La presenza di vulnerabilit√† avversarie complica anche l‚Äôimplementazione e la manutenzione dei sistemi ML. I progettisti e gli operatori di sistema devono considerare il potenziale di attacchi avversari e incorporare difese e meccanismi di monitoraggio appropriati. Aggiornamenti regolari e riqualificazione dei modelli diventano necessari per adattarsi alle nuove tecniche avversarie e mantenere la sicurezza e le prestazioni del sistema nel tempo.</p>
<p>L‚Äôimpatto degli attacchi avversari sui sistemi ML √® significativo e multiforme. Questi attacchi espongono le vulnerabilit√† dei modelli ML, dal degrado delle prestazioni del modello e dall‚Äôaumento di preoccupazioni sulla sicurezza e la protezione alla sfida dell‚Äôaffidabilit√† e dell‚Äôinterpretabilit√† del modello. Sviluppatori e ricercatori devono dare priorit√† allo sviluppo di difese e contromisure robuste per mitigare i rischi posti dagli attacchi avversari. Affrontando queste sfide, possiamo creare sistemi ML pi√π sicuri, affidabili e degni di fiducia in grado di resistere al panorama in continua evoluzione delle minacce avversarie.</p>
<div id="exr-aa" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;17.2: Attacchi Avversari
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Preparatevi a diventare un avversario dell‚ÄôIA! In questo Colab, si diventer√† un hacker white-box, imparando a creare attacchi che ingannano i modelli di classificazione delle immagini. Ci concentreremo sul Fast Gradient Sign Method (FGSM), sfruttando i gradienti di un modello contro di esso! Si distorceranno deliberatamente le immagini con piccole perturbazioni, osservando come inganneranno sempre pi√π intensamente l‚ÄôIA. Questo esercizio pratico evidenzia l‚Äôimportanza di creare un‚ÄôIA sicura, un‚Äôabilit√† critica man mano che l‚ÄôIA si integra nelle auto e nell‚Äôassistenza sanitaria. Il Colab si collega direttamente al capitolo Robust AI del libro, spostando gli attacchi avversari dalla teoria alla esperienza pratica.</p>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb#scrollTo=W1L3zJP6pPGD"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
<p>Pensate di poter superare in astuzia un‚ÄôIA? In questo Colab, scopriremo come ingannare i modelli di classificazione delle immagini con attacchi avversari. Utilizzeremo metodi come FGSM per modificare le immagini e ingannare sottilmente l‚ÄôIA. Scopriremo come progettare patch di immagini ingannevoli e osserveremo la sorprendente vulnerabilit√† di questi potenti modelli. Questa √® una conoscenza fondamentale per costruire sistemi di IA veramente robusti!</p>
<p><a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb#scrollTo=C5HNmh1-Ka9J"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="avvelenamento-dei-dati" class="level3 page-columns page-full" data-number="17.4.2">
<h3 data-number="17.4.2" class="anchored" data-anchor-id="avvelenamento-dei-dati"><span class="header-section-number">17.4.2</span> Avvelenamento dei Dati</h3>
<section id="definizione-e-caratteristiche-4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definizione-e-caratteristiche-4">Definizione e Caratteristiche</h4>
<p>L‚Äôavvelenamento dei dati √® un attacco in cui i dati di addestramento vengono manomessi, portando alla compromissione del modello <span class="citation" data-cites="biggio2012poisoning">(<a href="../../references.it.html#ref-biggio2012poisoning" role="doc-biblioref">Biggio, Nelson, e Laskov 2012</a>)</span>, come mostrato in <a href="#fig-poisoning-example" class="quarto-xref">Figura&nbsp;<span>17.22</span></a>. Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ci√≤ pu√≤ essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.</p>
<div class="no-row-height column-margin column-container"><div id="ref-biggio2012poisoning" class="csl-entry" role="listitem">
Biggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. <span>¬´Poisoning Attacks against Support Vector Machines¬ª</span>. In <em>Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012</em>. icml.cc / Omnipress. <a href="http://icml.cc/2012/papers/880.pdf">http://icml.cc/2012/papers/880.pdf</a>.
</div></div><div id="fig-poisoning-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poisoning-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/poisoning_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.22: Effetti dell‚ÄôAvvelenamento di NightShade sulla Diffusione Stabile. Fonte: <a href="https://telefonicatech.com/en/blog/attacks-on-artificial-intelligence-iii-data-poisoning">TOM√â</a>
</figcaption>
</figure>
</div>
<p>Il processo di solito prevede i seguenti passaggi:</p>
<ul>
<li><p><strong>Injection:</strong> L‚Äôaggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un‚Äôispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.</p></li>
<li><p><strong>Training:</strong> Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei pattern di dati.</p></li>
<li><p><strong>Deployment:</strong> Una volta distribuito il modello, l‚Äôaddestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilit√† prevedibili che l‚Äôaggressore pu√≤ sfruttare.</p></li>
</ul>
<p>L‚Äôimpatto dell‚Äôavvelenamento dei dati si estende oltre gli errori di classificazione o i cali di accuratezza. In applicazioni critiche come l‚Äôassistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza <span class="citation" data-cites="marulli2022sensitivity">(<a href="../../references.it.html#ref-marulli2022sensitivity" role="doc-biblioref">Marulli, Marrone, e Verde 2022</a>)</span>. Pi√π avanti, discuteremo alcuni casi di studio di questi problemi.</p>
<div class="no-row-height column-margin column-container"><div id="ref-marulli2022sensitivity" class="csl-entry" role="listitem">
Marulli, Fiammetta, Stefano Marrone, e Laura Verde. 2022. <span>¬´Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain¬ª</span>. <em>Journal of Sensor and Actuator Networks</em> 11 (2): 21. <a href="https://doi.org/10.3390/jsan11020021">https://doi.org/10.3390/jsan11020021</a>.
</div><div id="ref-oprea2022poisoning" class="csl-entry" role="listitem">
Oprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. <span>¬´Poisoning Attacks Against Machine Learning: <span>Can</span> Machine Learning Be Trustworthy?¬ª</span> <em>Computer</em> 55 (11): 94‚Äì99. <a href="https://doi.org/10.1109/mc.2022.3190787">https://doi.org/10.1109/mc.2022.3190787</a>.
</div></div><p>Esistono sei categorie principali di avvelenamento dei dati <span class="citation" data-cites="oprea2022poisoning">(<a href="../../references.it.html#ref-oprea2022poisoning" role="doc-biblioref">Oprea, Singhal, e Vassilev 2022</a>)</span>:</p>
<ul>
<li><p><strong>Attacchi alla Disponibilit√†:</strong> Questi attacchi mirano a compromettere la funzionalit√† complessiva di un modello. Fanno s√¨ che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio √® il ‚Äúlabel flipping‚Äù, in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.</p></li>
<li><p><strong>Attacchi Mirati:</strong> A differenza degli attacchi alla disponibilit√†, gli attacchi mirati mirano a compromettere un piccolo numero di campioni di test. Quindi, l‚Äôeffetto √® localizzato a un numero limitato di classi, mentre il modello mantiene lo stesso livello originale di accuratezza per la maggior parte delle classi. La natura mirata dell‚Äôattacco richiede che l‚Äôaggressore conosca le classi del modello, rendendo pi√π difficile il rilevamento di questi attacchi.</p></li>
<li><p><strong>Attacchi Backdoor:</strong> In questi attacchi, un avversario prende di mira pattern specifici nei dati. L‚Äôaggressore introduce una backdoor (un trigger o un pattern nascosto e dannoso) nei dati di training, ad esempio manipolando determinate feature nei dati strutturati o manipolando un pattern di pixel in una posizione fissa. Ci√≤ fa s√¨ che il modello associ il pattern dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di prova che contengono un pattern dannoso, effettua previsioni false.</p></li>
<li><p><strong>Attacchi di Sotto-popolazione:</strong> Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l‚Äôaccuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilit√† e mirati: eseguire attacchi di disponibilit√† (degrado delle prestazioni) nell‚Äôambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:</p></li>
<li><p><strong>Scope:</strong> Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un aggressore inserisce immagini manipolate di un cartello di avvertimento di ‚Äúdosso‚Äù (con perturbazioni o pattern accuratamente studiati), che fanno s√¨ che un‚Äôauto autonoma non riesca a riconoscere tale cartello e rallenti. D‚Äôaltro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica √® un esempio di attacco di sotto-popolazione.</p></li>
<li><p><strong>Conoscenza:</strong> Mentre gli attacchi mirati richiedono un alto grado di familiarit√† con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.</p></li>
</ul>
<p>Le caratteristiche del data poisoning includono:</p>
<p><strong>Manipolazioni sottili e difficili da rilevare dei dati di training:</strong> Il data poisoning spesso comporta manipolazioni sottili dei dati di training che sono attentamente studiate per essere difficili da rilevare tramite un‚Äôispezione casuale. Gli aggressori impiegano tecniche sofisticate per garantire che i campioni avvelenati si fondano perfettamente con i dati legittimi, rendendoli pi√π facili da identificare con un‚Äôanalisi approfondita. Queste manipolazioni possono mirare a caratteristiche o attributi specifici dei dati, come l‚Äôalterazione di valori numerici, la modifica di etichette categoriali o l‚Äôintroduzione di pattern attentamente progettati. L‚Äôobiettivo √® influenzare il processo di apprendimento del modello eludendo il rilevamento, consentendo ai dati avvelenati di corrompere sottilmente il comportamento del modello.</p>
<p><strong>Pu√≤ essere eseguito da insider o aggressori esterni:</strong> Gli attacchi di data poisoning possono essere eseguiti da vari attori, tra cui insider malintenzionati con accesso ai dati di training e aggressori esterni che trovano modi per influenzare la raccolta dati o la pipeline di pre-elaborazione. Gli insider rappresentano una minaccia significativa perch√© spesso hanno accesso privilegiato e conoscenza del sistema, il che consente loro di introdurre dati avvelenati senza destare sospetti. D‚Äôaltro canto, gli aggressori esterni possono sfruttare le vulnerabilit√† nell‚Äôapprovvigionamento dei dati, nelle piattaforme di crowdsourcing o nei processi di aggregazione dei dati per iniettare campioni avvelenati nel set di dati di addestramento. Ci√≤ evidenzia l‚Äôimportanza di implementare controlli di accesso rigorosi, policy di governance dei dati e meccanismi di monitoraggio per mitigare il rischio di minacce interne e attacchi esterni.</p>
<p><strong>Sfrutta le vulnerabilit√† nella raccolta e pre-elaborazione dei dati:</strong> Gli attacchi di avvelenamento dei dati spesso sfruttano le vulnerabilit√† nelle fasi di raccolta e pre-elaborazione dei dati della pipeline di apprendimento automatico. Gli aggressori progettano attentamente campioni avvelenati per eludere le comuni tecniche di convalida dei dati, assicurandosi che i dati manipolati rientrino comunque in intervalli accettabili, seguano le distribuzioni previste o mantengano la coerenza con altre funzionalit√†. Ci√≤ consente ai dati avvelenati di passare attraverso le fasi di pre-elaborazione dei dati senza essere rilevati. Inoltre, gli attacchi di avvelenamento possono sfruttare le debolezze nella preelaborazione dei dati, come una pulizia dei dati inadeguata, un rilevamento insufficiente di valori anomali o la mancanza di controlli di integrit√†. Gli aggressori possono anche sfruttare la mancanza di solidi meccanismi di tracciamento della provenienza e della discendenza dei dati per introdurre dati avvelenati senza lasciare una traccia. Per affrontare queste vulnerabilit√† sono necessarie rigorose tecniche di convalida dei dati, rilevamento delle anomalie e tracciamento della provenienza dei dati per garantire l‚Äôintegrit√† e l‚Äôaffidabilit√† dei dati di training.</p>
<p><strong>Interrompe il processo di apprendimento e distorce il comportamento del modello:</strong> Gli attacchi di avvelenamento dei dati sono progettati per interrompere il processo di apprendimento dei modelli di apprendimento automatico e distorcere il loro comportamento verso gli obiettivi dell‚Äôaggressore. I dati avvelenati vengono in genere manipolati con obiettivi specifici, come distorcere il comportamento del modello verso determinate classi, introdurre backdoor o degradare le prestazioni complessive. Queste manipolazioni non sono casuali, ma mirate a ottenere i risultati desiderati dall‚Äôaggressore. Introducendo incongruenze nelle etichette, in cui i campioni manipolati hanno etichette che non si allineano con la loro vera natura, gli attacchi di avvelenamento possono confondere il modello durante l‚Äôaddestramento e portare a previsioni distorte o errate. L‚Äôinterruzione causata dai dati avvelenati pu√≤ avere conseguenze di vasta portata, poich√© il modello compromesso pu√≤ prendere decisioni imperfette o mostrare un comportamento indesiderato quando viene distribuito in applicazioni del mondo reale.</p>
<p><strong>Influisce sulle prestazioni, l‚Äôequit√† e l‚Äôaffidabilit√† del modello:</strong> I dati avvelenati nel dataset di addestramento possono avere gravi implicazioni sulle prestazioni, l‚Äôequit√† e l‚Äôaffidabilit√† dei modelli di apprendimento automatico. I dati avvelenati possono degradare l‚Äôaccuratezza e le prestazioni del modello addestrato, portando a un aumento delle classificazioni errate o degli errori nelle previsioni. Ci√≤ pu√≤ avere conseguenze significative, soprattutto nelle applicazioni critiche in cui gli output del modello influenzano decisioni importanti. Inoltre, gli attacchi di avvelenamento possono introdurre distorsioni e problemi di equit√†, facendo s√¨ che il modello prenda decisioni discriminatorie o ingiuste per determinati sottogruppi o classi. Ci√≤ mina le responsabilit√† etiche e sociali dei sistemi di apprendimento automatico e pu√≤ perpetuare o amplificare i pregiudizi esistenti. Inoltre, i dati avvelenati erodono l‚Äôaffidabilit√† e la credibilit√† dell‚Äôintero sistema di apprendimento automatico. Gli output del modello diventano discutibili e potenzialmente dannosi, portando a una perdita di fiducia nell‚Äôintegrit√† del sistema. L‚Äôimpatto dei dati avvelenati pu√≤ propagarsi nell‚Äôintera pipeline ML, influenzando i componenti downstream e le decisioni che si basano sul modello compromesso. Per affrontare queste preoccupazioni √® necessaria una solida governance dei dati, un auditing regolare del modello e un monitoraggio continuo per rilevare e mitigare gli effetti degli attacchi di avvelenamento dei dati.</p>
</section>
<section id="meccanismi-di-avvelenamento-dei-dati" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="meccanismi-di-avvelenamento-dei-dati">Meccanismi di Avvelenamento dei Dati</h4>
<p>Gli attacchi di avvelenamento dei dati possono essere eseguiti tramite vari meccanismi, sfruttando diverse vulnerabilit√† della pipeline ML. Questi meccanismi consentono agli aggressori di manipolare i dati di training e introdurre campioni dannosi che possono compromettere le prestazioni, l‚Äôequit√† o l‚Äôintegrit√† del modello. Comprendere questi meccanismi √® fondamentale per sviluppare difese efficaci contro l‚Äôavvelenamento dei dati e garantire la robustezza dei sistemi ML. I meccanismi di avvelenamento dei dati possono essere ampiamente categorizzati in base all‚Äôapproccio dell‚Äôaggressore e alla fase della pipeline ML a cui mirano. Alcuni meccanismi comuni includono la modifica delle etichette dei dati di training, l‚Äôalterazione dei valori delle feature, l‚Äôiniezione di campioni dannosi accuratamente realizzati, lo sfruttamento delle vulnerabilit√† di raccolta e pre-elaborazione dei dati, la manipolazione dei dati alla fonte, l‚Äôavvelenamento dei dati in scenari di apprendimento online e la collaborazione con addetti ai lavori per manipolare i dati.</p>
<p>Ognuno di questi meccanismi presenta sfide uniche e richiede diverse strategie di mitigazione. Ad esempio, rilevare la manipolazione delle etichette pu√≤ comportare l‚Äôanalisi della distribuzione delle etichette e l‚Äôidentificazione delle anomalie <span class="citation" data-cites="zhou2018learning">(<a href="../../references.it.html#ref-zhou2018learning" role="doc-biblioref">Zhou et al. 2018</a>)</span>, mentre prevenire la manipolazione delle feature pu√≤ richiedere tecniche di pre-elaborazione dei dati e rilevamento delle anomalie sicure <span class="citation" data-cites="carta2020local">(<a href="../../references.it.html#ref-carta2020local" role="doc-biblioref">Carta et al. 2020</a>)</span>. La difesa dalle minacce interne pu√≤ comportare rigide policy di controllo degli accessi e il monitoraggio dei pattern di accesso ai dati. Inoltre, l‚Äôefficacia degli attacchi di avvelenamento dei dati spesso dipende dalla conoscenza del sistema ML da parte dell‚Äôattaccante, tra cui l‚Äôarchitettura del modello, gli algoritmi di training e la distribuzione dei dati. Gli aggressori possono utilizzare tecniche di apprendimento automatico avversario o di sintesi dei dati per creare campioni che hanno maggiori probabilit√† di aggirare il rilevamento e raggiungere i loro obiettivi malevoli.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhou2018learning" class="csl-entry" role="listitem">
Zhou, Peng, Xintong Han, Vlad I. Morariu, e Larry S. Davis. 2018. <span>¬´Learning Rich Features for Image Manipulation Detection¬ª</span>. In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 1053‚Äì61. IEEE. <a href="https://doi.org/10.1109/cvpr.2018.00116">https://doi.org/10.1109/cvpr.2018.00116</a>.
</div><div id="ref-carta2020local" class="csl-entry" role="listitem">
Carta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero, e Roberto Saia. 2020. <span>¬´A Local Feature Engineering Strategy to Improve Network Anomaly Detection¬ª</span>. <em>Future Internet</em> 12 (10): 177. <a href="https://doi.org/10.3390/fi12100177">https://doi.org/10.3390/fi12100177</a>.
</div></div><div id="fig-distribution-shift-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-distribution-shift-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/distribution_shift_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distribution-shift-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.23: Garbage In ‚Äì Garbage Out. Fonte: <a href="https://informationmatters.net/data-poisoning-ai/">Information Matters</a>
</figcaption>
</figure>
</div>
<p><strong>Modifica delle etichette dei dati di training:</strong> Uno dei meccanismi pi√π semplici di avvelenamento dei dati √® la modifica delle etichette dei dati di training. In questo approccio, l‚Äôaggressore modifica selettivamente le etichette di un sottoinsieme dei campioni di training per fuorviare il processo di apprendimento del modello, come mostrato in <a href="#fig-distribution-shift-example" class="quarto-xref">Figura&nbsp;<span>17.23</span></a>. Ad esempio, in un‚Äôattivit√† di classificazione binaria, l‚Äôaggressore potrebbe capovolgere le etichette di alcuni campioni positivi in negativi o viceversa. Introducendo tale rumore di etichetta, l‚Äôaggressore degrada le prestazioni del modello o fa s√¨ che faccia previsioni errate per istanze target specifiche.</p>
<p><strong>Alterazione dei valori delle feature nei dati di training:</strong> Un altro meccanismo di avvelenamento dei dati consiste nell‚Äôalterare i valori delle caratteristiche dei campioni di training senza modificare le etichette. L‚Äôaggressore elabora attentamente i valori delle feature per introdurre specifici pregiudizi o vulnerabilit√† nel modello. Ad esempio, in un‚Äôattivit√† di classificazione delle immagini, l‚Äôaggressore potrebbe aggiungere perturbazioni impercettibili a un sottoinsieme di immagini, facendo s√¨ che il modello apprenda un particolare pattern o associazione. Questo tipo di avvelenamento pu√≤ creare backdoor o trojan nel modello addestrato, che possono essere attivati da specifici pattern di input.</p>
<p><strong>Iniezione di campioni dannosi accuratamente realizzati:</strong> In questo meccanismo, l‚Äôaggressore crea campioni dannosi progettati per avvelenare il modello. Questi campioni sono realizzati per avere un impatto specifico sul comportamento del modello, mentre si fondono con i dati di addestramento legittimi. L‚Äôaggressore potrebbe utilizzare tecniche come perturbazioni avversarie o sintesi dei dati per generare campioni avvelenati difficili da rilevare. L‚Äôaggressore manipola i limiti decisionali del modello iniettando questi campioni dannosi nei dati di addestramento o introducendo classificazioni errate mirate.</p>
<p><strong>Sfruttamento delle vulnerabilit√† di raccolta e preelaborazione dei dati:</strong> Gli attacchi di avvelenamento dei dati possono anche sfruttare le vulnerabilit√† della pipeline di raccolta e preelaborazione dei dati. Se il processo di raccolta dati non √® sicuro o ci sono debolezze nelle fasi di pre-elaborazione dei dati, un aggressore pu√≤ manipolare i dati prima che raggiungano la fase di addestramento. Ad esempio, se i dati vengono raccolti da fonti non attendibili o ci sono problemi nella pulizia o nell‚Äôaggregazione dei dati, un aggressore pu√≤ introdurre campioni avvelenati o manipolare i dati a proprio vantaggio.</p>
<p><strong>Manipolazione dei dati alla fonte (ad esempio, dati dei sensori):</strong> In alcuni casi, gli aggressori possono manipolare i dati alla fonte, come dati dei sensori o dispositivi di input. Manomettendo i sensori o manipolando l‚Äôambiente in cui vengono raccolti i dati, gli aggressori possono introdurre campioni avvelenati o alterare la distribuzione dei dati. Ad esempio, in uno scenario di auto a guida autonoma, un aggressore potrebbe manipolare i sensori o l‚Äôambiente per immettere informazioni fuorvianti nei dati di addestramento, compromettendo la capacit√† del modello di prendere decisioni sicure e affidabili.</p>
<div id="fig-poisoning-attack-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poisoning-attack-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/poisoning_attack_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-attack-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.24: Attacco di Avvelenamento dei Dati. Fonte: <a href="https://www.researchgate.net/publication/366883200_A_Detailed_Survey_on_Federated_Learning_Attacks_and_Defenses">Sikandar</a>
</figcaption>
</figure>
</div>
<p><strong>Avvelenamento dei dati in scenari di apprendimento online:</strong> Gli attacchi di avvelenamento dei dati possono anche colpire sistemi ML che impiegano l‚Äôapprendimento online, in cui il modello viene costantemente aggiornato con nuovi dati in tempo reale. In tali scenari, un aggressore pu√≤ gradualmente iniettare campioni avvelenati nel tempo, manipolando lentamente il comportamento del modello. I sistemi di apprendimento online sono particolarmente vulnerabili all‚Äôavvelenamento dei dati perch√© si adattano ai nuovi dati senza una convalida estesa, rendendo pi√π facile per gli aggressori introdurre campioni dannosi, come mostrato in <a href="#fig-poisoning-attack-example" class="quarto-xref">Figura&nbsp;<span>17.24</span></a>.</p>
<p><strong>Collaborazione con addetti ai lavori per manipolare i dati:</strong> A volte, gli attacchi di avvelenamento dei dati possono comportare la collaborazione con addetti ai lavori con accesso ai dati di training. Gli addetti ai lavori malintenzionati, come dipendenti o provider di dati, possono manipolare i dati prima che vengano utilizzati per addestrare il modello. Le minacce interne sono particolarmente difficili da rilevare e prevenire, poich√© gli aggressori hanno un accesso legittimo ai dati e possono elaborare attentamente la strategia di avvelenamento per eludere il rilevamento.</p>
<p>Questi sono i meccanismi chiave dell‚Äôavvelenamento dei dati nei sistemi ML. Gli aggressori spesso impiegano questi meccanismi per rendere i loro attacchi pi√π efficaci e difficili da rilevare. Il rischio di attacchi di avvelenamento dei dati aumenta man mano che i sistemi ML diventano sempre pi√π complessi e si basano su set di dati pi√π grandi provenienti da fonti diverse. La difesa dall‚Äôavvelenamento dei dati richiede un approccio poliedrico. I professionisti ML e i progettisti di sistemi devono essere consapevoli dei vari meccanismi di avvelenamento dei dati e adottare un approccio completo alla sicurezza dei dati e alla resilienza del modello. Ci√≤ include la raccolta dati sicura, la convalida dati robusta e il monitoraggio continuo delle prestazioni del modello. L‚Äôimplementazione di pratiche di raccolta dati e pre-elaborazione sicure √® fondamentale per prevenire l‚Äôavvelenamento dei dati alla fonte. Le tecniche di convalida dati e rilevamento anomalie possono anche aiutare a identificare e mitigare potenziali tentativi di avvelenamento. Il monitoraggio delle prestazioni del modello per segnali di avvelenamento dei dati √® inoltre essenziale per rilevare e rispondere prontamente agli attacchi.</p>
</section>
<section id="impatto-sui-sistemi-ml-4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impatto-sui-sistemi-ml-4">Impatto sui Sistemi ML</h4>
<p>Gli attacchi di avvelenamento dei dati possono avere gravi ripercussioni sui sistemi ML, compromettendone le prestazioni, l‚Äôaffidabilit√† e la credibilit√†. L‚Äôimpatto dell‚Äôavvelenamento dei dati pu√≤ manifestarsi in vari modi, a seconda degli obiettivi dell‚Äôaggressore e del meccanismo specifico utilizzato. Analizziamo in dettaglio ciascuno dei potenziali impatti.</p>
<p><strong>Degrado delle prestazioni del modello:</strong> Uno degli impatti principali dell‚Äôavvelenamento dei dati √® il degrado delle prestazioni complessive del modello. Manipolando i dati di training, gli aggressori possono introdurre rumore, distorsioni o incongruenze che ostacolano la capacit√† del modello di apprendere pattern accurati e fare previsioni affidabili. Ci√≤ pu√≤ ridurre accuratezza, precisione, richiamo o altre metriche delle prestazioni. Il degrado delle prestazioni del modello pu√≤ avere conseguenze significative, soprattutto in applicazioni critiche come sanit√†, finanza o sicurezza, dove l‚Äôaffidabilit√† delle previsioni √® fondamentale.</p>
<p><strong>Errore di classificazione di target specifici:</strong> Gli attacchi di avvelenamento dei dati possono anche essere progettati per far s√¨ che il modello classifichi in modo errato istanze target specifiche. Gli aggressori possono introdurre campioni avvelenati realizzati con cura simili alle istanze target, portando il modello ad apprendere associazioni errate. Ci√≤ pu√≤ comportare che il modello classifichi in modo errato le istanze target in modo coerente, anche se funziona bene su altri input. Tale errata classificazione mirata pu√≤ avere gravi conseguenze, come far s√¨ che un sistema di rilevamento malware trascuri file dannosi specifici o portare a una diagnosi errata in un‚Äôapplicazione di imaging medico.</p>
<p><strong>Backdoor e trojan nei modelli addestrati:</strong> L‚Äôavvelenamento dei dati pu√≤ introdurre backdoor o trojan nel modello addestrato. Le backdoor sono funzionalit√† nascoste che consentono agli aggressori di innescare comportamenti specifici o bypassare i normali meccanismi di autenticazione. D‚Äôaltro canto, i trojan sono componenti dannosi insinuati nel modello che possono attivare specifici pattern di input. Avvelenando i dati di training, gli aggressori possono creare modelli che sembrano funzionare normalmente ma contengono vulnerabilit√† nascoste che possono essere sfruttate in seguito. Backdoor e trojan possono compromettere l‚Äôintegrit√† e la sicurezza del sistema ML, consentendo agli aggressori di ottenere accesso non autorizzato, manipolare previsioni o esfiltrare informazioni sensibili.</p>
<p><strong>Risultati del modello distorti o ingiusti:</strong> Gli attacchi di avvelenamento dei dati possono introdurre distorsioni o ingiustizie nelle previsioni del modello. Manipolando la distribuzione dei dati di training o iniettando campioni con distorsioni specifiche, gli aggressori possono far s√¨ che il modello apprenda e perpetui pattern discriminatori. Ci√≤ pu√≤ portare a un trattamento ingiusto di determinati gruppi o individui in base ad attributi sensibili come razza, genere o et√†. I modelli distorti possono avere gravi implicazioni sociali, rafforzando le disuguaglianze e le pratiche discriminatorie esistenti. Garantire l‚Äôequit√† e mitigare i pregiudizi √® fondamentale per creare sistemi ML affidabili ed etici..</p>
<p><strong>Aumento di falsi positivi o falsi negativi:</strong> L‚Äôavvelenamento dei dati pu√≤ anche influire sulla capacit√† del modello di identificare correttamente istanze positive o negative, portando a un aumento di falsi positivi o falsi negativi. I falsi positivi si verificano quando il modello identifica erroneamente un‚Äôistanza negativa come positiva, mentre i falsi negativi si verificano quando un‚Äôistanza positiva viene classificata erroneamente come negativa. Le conseguenze dell‚Äôaumento di falsi positivi o falsi negativi possono essere significative a seconda dell‚Äôapplicazione. Ad esempio, in un sistema di rilevamento delle frodi, un elevato numero di falsi positivi pu√≤ portare a indagini non necessarie e frustrazione dei clienti, mentre un elevato numero di falsi negativi pu√≤ consentire che le attivit√† fraudolente passino inosservate.</p>
<p><strong>Affidabilit√† e fiducia del sistema compromesse:</strong> Gli attacchi di avvelenamento dei dati possono minare l‚Äôaffidabilit√† e la fiducia complessiva dei sistemi ML. Quando i modelli vengono addestrati su dati contaminati, le loro previsioni diventano inaffidabili e inaffidabili. Ci√≤ pu√≤ erodere la fiducia dell‚Äôutente nel sistema e portare a una perdita di fiducia nelle decisioni prese dal modello. Nelle applicazioni critiche in cui si fa affidamento sui sistemi ML per il processo decisionale, come veicoli autonomi o diagnosi mediche, l‚Äôaffidabilit√† compromessa pu√≤ avere gravi conseguenze, mettendo a rischio vite e propriet√†.</p>
<p>Per affrontare l‚Äôimpatto dell‚Äôavvelenamento dei dati √® necessario un approccio proattivo alla sicurezza dei dati, ai test dei modelli e al monitoraggio. Le organizzazioni devono implementare misure robuste per garantire l‚Äôintegrit√† e la qualit√† dei dati di training, impiegare tecniche per rilevare e mitigare i tentativi di avvelenamento e monitorare costantemente le prestazioni e il comportamento dei modelli distribuiti. La collaborazione tra professionisti ML, esperti di sicurezza e specialisti di dominio √® essenziale per sviluppare strategie complete per prevenire e rispondere agli attacchi di avvelenamento dei dati.</p>
<section id="caso-di-studio" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="caso-di-studio">Caso di Studio</h5>
<div id="fig-dirty-label-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dirty-label-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/dirty_label_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dirty-label-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.25: Campioni di dati avvelenati con etichette sbagliateriguardanti coppie testo/immagine non corrispondenti. Fonte: <a href="https://arxiv.org/pdf/2310.13828">Shan</a>
</figcaption>
</figure>
</div>
<p>√à interessante notare che gli attacchi di ‚Äúdata poisoning‚Äù non sono sempre dannosi <span class="citation" data-cites="shan2023prompt">(<a href="../../references.it.html#ref-shan2023prompt" role="doc-biblioref">Shan et al. 2023</a>)</span>. Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l‚ÄôUniversit√† di Chicago, utilizza l‚Äôavvelenamento dei dati per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di IA generativa. Gli artisti possono utilizzare lo strumento per apportare modifiche impercettibili alle proprie immagini prima di caricarle online, come mostrato in <a href="#fig-dirty-label-example" class="quarto-xref">Figura&nbsp;<span>17.25</span></a>.</p>
<div class="no-row-height column-margin column-container"></div><p>Sebbene queste modifiche siano impercettibili all‚Äôocchio umano, possono compromettere significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando vengono incorporate nei dati di addestramento. I modelli generativi possono essere manipolati per generare allucinazioni e immagini strane. Ad esempio, con solo 300 immagini avvelenate, i ricercatori dell‚ÄôUniversit√† di Chicago potrebbero ingannare l‚Äôultimo modello Stable Diffusion per generare immagini di cani che sembrano gatti o immagini di mucche quando vengono richieste le auto.</p>
<p>Man mano che aumenta il numero di immagini avvelenate su Internet, le prestazioni dei modelli che utilizzano dati acquisiti peggioreranno in modo esponenziale. In primo luogo, i dati avvelenati sono difficili da rilevare e richiedono l‚Äôeliminazione manuale. In secondo luogo, il ‚Äúveleno‚Äù si diffonde rapidamente ad altre etichette perch√© i modelli generativi si basano su connessioni tra parole e concetti mentre generano immagini. Quindi un‚Äôimmagine avvelenata di una ‚Äúmacchina‚Äù potrebbe diffondersi in immagini generate associate a parole come ‚Äúcamion‚Äù, ‚Äútreno‚Äù, ‚Äúautobus‚Äù, ecc.</p>
<p>D‚Äôaltra parte, questo strumento pu√≤ essere utilizzato in modo dannoso e pu√≤ influenzare le applicazioni legittime dei modelli generativi. Ci√≤ dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.</p>
<p><a href="#fig-poisoning" class="quarto-xref">Figura&nbsp;<span>17.26</span></a> mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in diverse categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un‚Äôauto genera una mucca.</p>
<div id="fig-poisoning" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-poisoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image14.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.26: Avvelenamento dei Dati. Fonte: <span class="citation" data-cites="shan2023prompt">Shan et al. (<a href="../../references.it.html#ref-shan2023prompt" role="doc-biblioref">2023</a>)</span>)
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-shan2023prompt" class="csl-entry" role="listitem">
Shan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, e Ben Y Zhao. 2023. <span>¬´Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models¬ª</span>. <em>ArXiv preprint</em> abs/2310.13828. <a href="https://arxiv.org/abs/2310.13828">https://arxiv.org/abs/2310.13828</a>.
</div></div></figure>
</div>
<div id="exr-pa" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;17.3: Attacchi Avvelenati
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Preparatevi a esplorare il lato oscuro della sicurezza dell‚ÄôIA! In questo Colab, impareremo cos‚Äô√® l‚Äôavvelenamento dei dati, ovvero come dati errati possono ingannare i modelli di IA e fargli prendere decisioni sbagliate. Ci concentreremo su un attacco reale contro una Support Vector Machine (SVM), osservando come cambia il comportamento dell‚ÄôIA sotto attacco. Questo esercizio pratico metter√† in evidenza perch√© proteggere i sistemi di IA √® fondamentale, soprattutto man mano che diventano pi√π integrati nelle nostre vite. Pensare come un hacker, comprendere la vulnerabilit√† e fare brainstorming su come difendere i sistemi di IA!</p>
<p><a href="https://colab.research.google.com/github/pralab/secml/blob/HEAD/tutorials/05-Poisoning.ipynb#scrollTo=-8onNPNTOLk2"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="distribution-shift" class="level3" data-number="17.4.3">
<h3 data-number="17.4.3" class="anchored" data-anchor-id="distribution-shift"><span class="header-section-number">17.4.3</span> Distribution Shift</h3>
<section id="definizione-e-caratteristiche-5" class="level4">
<h4 class="anchored" data-anchor-id="definizione-e-caratteristiche-5">Definizione e Caratteristiche</h4>
<p>La ‚Äúdistribution shift‚Äù [slittamento della distribuzione] si riferisce al fenomeno in cui la distribuzione dei dati incontrata da un modello ML durante la distribuzione (inferenza) differisce dalla distribuzione su cui √® stato addestrato, come mostrato in <a href="#fig-distribution-shift" class="quarto-xref">Figura&nbsp;<span>17.27</span></a>. Questo non √® tanto un attacco quanto il fatto che la robustezza del modello varier√† nel tempo. In altre parole, le propriet√† statistiche, i pattern o le ipotesi sottostanti dei dati possono cambiare tra le fasi di addestramento e di test.</p>
<div id="fig-distribution-shift" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-distribution-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/distribution_shift.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distribution-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.27: Le parentesi graffe racchiudono la ‚Äúdistribution shift‚Äù tra gli ambienti. Qui, z sta per la caratteristica spuria e y sta per la classe dell‚Äôetichetta. Fonte: <a href="https://www.researchgate.net/publication/366423741_On_the_Connection_between_Invariant_Learning_and_Adversarial_Training_for_Out-of-Distribution_Generalization">Xin</a>
</figcaption>
</figure>
</div>
<p>Le caratteristiche principali della ‚Äúdistribution shift‚Äù includono:</p>
<p><strong>Discordanza di dominio:</strong> I dati di input durante l‚Äôinferenza provengono da un dominio o una distribuzione diversi rispetto ai dati di addestramento. Quando i dati di input durante l‚Äôinferenza provengono da un dominio o una distribuzione diversi dai dati di training, possono influenzare significativamente le prestazioni del modello. Questo perch√© il modello ha imparato pattern e relazioni specifici del dominio di training e, se applicati a un dominio diverso, tali pattern appresi potrebbero non essere validi. Ad esempio, si consideri un modello di analisi del sentiment addestrato sulle recensioni di film. Supponiamo che questo modello venga applicato per analizzare il sentiment nei tweet. In tal caso, potrebbe aver bisogno di aiuto per classificare accuratamente il sentiment perch√© la lingua, la grammatica e il contesto dei tweet possono differire dalle recensioni dei film. Questa discrepanza di dominio pu√≤ causare scarse prestazioni e previsioni inaffidabili, limitando l‚Äôutilit√† pratica del modello.</p>
<p><strong>Deriva temporale:</strong> La distribuzione dei dati si evolve, portando a uno spostamento graduale o improvviso nelle caratteristiche di input. La deriva temporale √® importante perch√© i modelli ML vengono spesso distribuiti in ambienti dinamici in cui la distribuzione dei dati pu√≤ cambiare nel tempo. Se il modello non viene aggiornato o adattato a questi cambiamenti, le sue prestazioni possono gradualmente peggiorare. Ad esempio, i pattern e i comportamenti associati alle attivit√† fraudolente possono evolversi in un sistema di rilevamento delle frodi man mano che i truffatori adattano le loro tecniche. Se il modello non viene riqualificato o aggiornato per catturare questi nuovi pattern, potrebbe non riuscire a rilevare efficacemente nuovi tipi di frode. La deriva temporale pu√≤ portare a un calo dell‚Äôaccuratezza e dell‚Äôaffidabilit√† del modello nel tempo, rendendo cruciale il monitoraggio e l‚Äôaffronto di questo tipo di spostamento della distribuzione.</p>
<p><strong>Cambiamenti contestuali:</strong> Il contesto del modello ML pu√≤ variare, determinando diverse distribuzioni di dati in base a fattori quali posizione, comportamento dell‚Äôutente o condizioni ambientali. I cambiamenti contestuali sono importanti perch√© i modelli ML vengono spesso distribuiti in vari contesti o ambienti che possono avere diverse distribuzioni di dati. Se il modello non riesce a generalizzarsi bene a questi diversi contesti, le sue prestazioni potrebbero deteriorarsi. Ad esempio, si consideri un modello di visione artificiale addestrato per riconoscere oggetti in un ambiente di laboratorio controllato. Quando distribuito in un contesto reale, fattori quali condizioni di illuminazione, angoli della telecamera o confusione sullo sfondo possono variare in modo significativo, determinando una ‚Äúdistribution shift‚Äù. Se il modello √® robusto a questi cambiamenti contestuali, potrebbe essere in grado di riconoscere accuratamente gli oggetti nel nuovo ambiente, limitandone l‚Äôutilit√† pratica.</p>
<p><strong>Dati di addestramento non rappresentativi:</strong> I dati di addestramento potrebbero catturare solo parzialmente la variabilit√† e la diversit√† dei dati del mondo reale riscontrati durante la distribuzione. I dati di training non rappresentativi possono portare a modelli parziali o distorti che funzionano male sui dati del mondo reale. Supponiamo che i dati di training debbano catturare adeguatamente la variabilit√† e la diversit√† dei dati del mondo reale. In tal caso, il modello potrebbe apprendere pattern specifici del set di training, ma deve essere meglio generalizzato a dati nuovi e mai visti. Ci√≤ pu√≤ comportare scarse prestazioni, previsioni parziali e limitata applicabilit√† del modello. Ad esempio, se un modello di riconoscimento facciale viene addestrato principalmente su immagini di individui di uno specifico gruppo demografico, potrebbe avere difficolt√† a riconoscere accuratamente i volti di altri gruppi demografici quando viene distribuito in un contesto reale. Garantire che i dati di training siano rappresentativi e diversificati √® fondamentale per creare modelli che possano essere generalizzati bene a scenari del mondo reale.</p>
<div id="fig-drift-over-time" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-drift-over-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/drift_over_time.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-drift-over-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.28: La deriva concettuale si riferisce a un cambiamento nei pattern e nelle relazioni dei dati nel tempo. Fonte: <a href="https://www.evidentlyai.com/ml-in-production/concept-drift">Evidently AI</a>
</figcaption>
</figure>
</div>
<p>La ‚Äúdistribution shift‚Äù pu√≤ manifestarsi in varie forme, come:</p>
<p><strong>Covariate shift:</strong> La distribuzione delle feature di input (covariate) cambia mentre la distribuzione condizionale della variabile target dato l‚Äôinput rimane la stessa. La ‚Äúcovariate shift‚Äù √® importante perch√© pu√≤ influire sulla capacit√† del modello di fare previsioni accurate quando le feature di input (covariate) differiscono tra i dati di training e quelli di test. Anche se la relazione tra le feature di input e la variabile target rimane la stessa, un cambiamento nella distribuzione delle feature di input pu√≤ influire sulle prestazioni del modello. Ad esempio, si consideri un modello addestrato per prevedere i prezzi delle case in base a caratteristiche come la metratura, il numero di camere da letto e la posizione. Supponiamo che la distribuzione di queste caratteristiche nei dati di test differisca significativamente dai dati di training (ad esempio, i dati di test contengono case con una metratura molto pi√π ampia). In tal caso, le previsioni del modello potrebbero diventare meno accurate. √à importante tenere conto dei ‚Äúcovariate shift‚Äù per garantire la robustezza e l‚Äôaffidabilit√† del modello quando viene applicato a nuovi dati.</p>
<p><strong>Concept drift:</strong> La relazione tra le feature di input e la variabile target cambia nel tempo, alterando il concetto sottostante che il modello sta cercando di apprendere, come mostrato in <a href="#fig-drift-over-time" class="quarto-xref">Figura&nbsp;<span>17.28</span></a>. Il ‚Äúconcept drift‚Äù √® importante perch√© indica cambiamenti nella relazione fondamentale tra le feature di input e la variabile target nel tempo. Quando il concetto sottostante che il modello sta cercando di apprendere cambia, le sue prestazioni possono deteriorarsi se non vengono adattate al nuovo concetto. Ad esempio, in un modello di previsione dell‚Äôabbandono dei clienti, i fattori che influenzano l‚Äôabbandono dei clienti possono evolversi a causa delle condizioni di mercato, delle offerte della concorrenza o delle preferenze dei clienti. Se il modello non viene aggiornato per catturare questi cambiamenti, le sue previsioni potrebbero diventare meno accurate e irrilevanti. Rilevare e adattarsi al ‚Äúconcept drift‚Äù √® fondamentale per mantenere l‚Äôefficacia del modello e l‚Äôallineamento con i concetti del mondo reale in evoluzione.</p>
<p><strong>Generalizzazione del dominio:</strong> Il modello deve generalizzare a domini o distribuzioni invisibili non presenti durante l‚Äôaddestramento. La generalizzazione di dominio √® importante perch√© consente di applicare i modelli ML a nuovi domini mai visti senza richiedere un‚Äôampia riqualificazione o adattamento. Negli scenari del mondo reale, i dati di addestramento che coprono tutti i possibili domini o distribuzioni che il modello pu√≤ incontrare sono spesso irrealizzabili. Le tecniche di generalizzazione di dominio mirano ad apprendere caratteristiche o modelli invarianti al dominio che possono essere generalizzati bene a nuovi domini. Ad esempio, si consideri un modello addestrato per classificare immagini di animali. Se il modello pu√≤ apprendere caratteristiche invarianti a diversi sfondi, condizioni di illuminazione o pose, pu√≤ essere generalizzato bene per classificare animali in nuovi ambienti mai visti. La generalizzazione del dominio √® fondamentale per creare modelli che possono essere distribuiti in contesti reali diversi e in continua evoluzione.</p>
<p>La presenza di un ‚Äúdistribution shift‚Äù pu√≤ avere un impatto significativo sulle prestazioni e l‚Äôaffidabilit√† dei modelli ML, poich√© i modelli potrebbero aver bisogno di aiuto per generalizzare bene alla nuova distribuzione dei dati. Rilevare e adattarsi ai ‚Äúdistribution shift‚Äù √® fondamentale per garantire la robustezza e l‚Äôutilit√† pratica dei sistemi ML negli scenari del mondo reale.</p>
</section>
<section id="meccanismi-delle-distribution-shift" class="level4">
<h4 class="anchored" data-anchor-id="meccanismi-delle-distribution-shift">Meccanismi delle Distribution Shift</h4>
<p>I meccanismi della ‚Äúdistribution shift‚Äù, come cambiamenti nelle fonti dei dati, evoluzione temporale, variazioni specifiche del dominio, bias di selezione, cicli di feedback e manipolazioni avversarie, sono importanti da comprendere perch√© aiutano a identificarne le cause. Comprendendo questi meccanismi, i professionisti possono sviluppare strategie mirate per mitigarne l‚Äôimpatto e migliorare la robustezza del modello. Ecco alcuni meccanismi comuni:</p>
<div id="fig-temporal-evoltion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-temporal-evoltion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/temporal_evoltion.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-temporal-evoltion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.29: Evoluzione temporale. Fonte: <a href="https://www.nannyml.com/blog/types-of-data-shift">Bia≈Çek</a>
</figcaption>
</figure>
</div>
<p><strong>Cambiamenti nelle fonti di dati:</strong> Possono verificarsi cambiamenti di distribuzione quando le fonti di dati utilizzate per l‚Äôaddestramento e l‚Äôinferenza sono diverse. Ad esempio, se un modello viene addestrato sui dati di un sensore ma distribuito sui dati di un altro sensore con caratteristiche diverse, pu√≤ portare a un ‚Äúdistribution shift‚Äù.</p>
<p><strong>Evoluzione temporale:</strong> Nel tempo, la distribuzione dei dati sottostante pu√≤ evolversi a causa di cambiamenti nel comportamento dell‚Äôutente, dinamiche di mercato o altri fattori temporali. Ad esempio, in un sistema di raccomandazione, le preferenze dell‚Äôutente possono cambiare nel tempo, portando a un ‚Äúdistribution shift‚Äù nei dati di input, come mostrato in <a href="#fig-temporal-evoltion" class="quarto-xref">Figura&nbsp;<span>17.29</span></a>.</p>
<p><strong>Variazioni specifiche del dominio:</strong> Domini o contesti diversi possono avere distribuzioni di dati distinte. Un modello addestrato sui dati di un dominio pu√≤ generalizzare bene con un altro dominio solo con tecniche di adattamento appropriate. Ad esempio, un modello di classificazione delle immagini addestrato su scene di interni potrebbe avere difficolt√† se applicato a scene in esterno.</p>
<p><strong>Bias di selezione:</strong> Un ‚ÄúDistribution shift‚Äù pu√≤ derivare da un bias di selezione durante la raccolta o il campionamento dei dati. Se i dati di training non rappresentano la popolazione reale o determinati sottogruppi sono sovrarappresentati o sottorappresentati, si pu√≤ arrivare a una mancata corrispondenza tra le distribuzioni di training e di test.</p>
<p><strong>Cicli di feedback:</strong> In alcuni casi, le previsioni o le azioni intraprese da un modello ML possono influenzare la futura distribuzione dei dati. Ad esempio, in un sistema di prezzi dinamici, i prezzi stabiliti dal modello possono influire sul comportamento dei clienti, determinando uno spostamento nella distribuzione dei dati nel tempo.</p>
<p><strong>Manipolazioni avversarie:</strong> Gli avversari possono manipolare intenzionalmente i dati di input per creare uno spostamento della distribuzione e ingannare il modello ML. Introducendo perturbazioni attentamente studiate o generando campioni fuori distribuzione, gli aggressori possono sfruttare le vulnerabilit√† del modello e fargli fare previsioni errate.</p>
<p>Comprendere i meccanismi del ‚Äúdistribution shift‚Äù √® importante per sviluppare strategie efficaci per rilevare e mitigare il suo impatto sui sistemi ML. Identificando le fonti e le caratteristiche dello spostamento, i professionisti possono progettare tecniche appropriate, come l‚Äôadattamento del dominio, l‚Äôapprendimento tramite trasferimento o l‚Äôapprendimento continuo, per migliorare la robustezza e le prestazioni del modello in caso di cambiamenti distributivi.</p>
</section>
<section id="impatto-sui-sistemi-ml-5" class="level4">
<h4 class="anchored" data-anchor-id="impatto-sui-sistemi-ml-5">Impatto sui Sistemi ML</h4>
<p>I ‚Äúdistribution shift‚Äù possono avere un impatto negativo significativo sulle prestazioni e l‚Äôaffidabilit√† dei sistemi ML. Ecco alcuni modi chiave in cui il ‚Äúdistribution shift‚Äù pu√≤ influenzare i modelli ML:</p>
<p><strong>Prestazioni predittive degradate:</strong> Quando la distribuzione dei dati riscontrata durante l‚Äôinferenza differisce dalla distribuzione di training, l‚Äôaccuratezza predittiva del modello pu√≤ deteriorarsi. Il modello potrebbe aver bisogno di aiuto per generalizzare bene i nuovi dati, il che porta a un aumento degli errori e a prestazioni non ottimali.</p>
<p><strong>Affidabilit√† e attendibilit√† ridotte:</strong> Il ‚Äúdistribution shift‚Äù pu√≤ compromettere l‚Äôaffidabilit√† e l‚Äôattendibilit√† dei modelli ML. Se le previsioni del modello diventano inaffidabili o incoerenti a causa dello spostamento, gli utenti potrebbero perdere fiducia negli output del sistema, il che porta a un potenziale uso improprio o non uso del modello.</p>
<p><strong>Predizioni distorte:</strong> Lo spostamento di distribuzione pu√≤ introdurre ‚Äúbias‚Äù [distorsioni] nelle previsioni del modello. Se i dati di training non rappresentano la distribuzione nel mondo reale o alcuni sottogruppi sono sottorappresentati, il modello potrebbe fare previsioni distorte che discriminano determinati gruppi o perpetuano pregiudizi sociali.</p>
<p><strong>Maggiore incertezza e rischio:</strong> Lo spostamento della distribuzione introduce ulteriore incertezza e rischio nel sistema ML. Il comportamento e le prestazioni del modello potrebbero diventare meno prevedibili, rendendo difficile valutarne l‚Äôaffidabilit√† e l‚Äôidoneit√† per applicazioni critiche. Questa incertezza pu√≤ portare a maggiori rischi operativi e potenziali guasti.</p>
<p><strong>Sfide di adattabilit√†:</strong> I modelli ML addestrati su una distribuzione dati specifica potrebbero aver bisogno di aiuto per adattarsi ad ambienti mutevoli o nuovi domini. La mancanza di adattabilit√† pu√≤ limitare l‚Äôutilit√† e l‚Äôapplicabilit√† del modello in scenari reali dinamici in cui la distribuzione dei dati si evolve.</p>
<p><strong>Difficolt√† di manutenzione e aggiornamento:</strong> Il ‚Äúdistribution shift‚Äù pu√≤ complicare la manutenzione e l‚Äôaggiornamento dei modelli ML. Man mano che la distribuzione dei dati cambia, il modello potrebbe richiedere frequenti riqualificazioni o ottimizzazioni per mantenere le sue prestazioni. Ci√≤ pu√≤ richiedere molto tempo e risorse, soprattutto se il cambiamento avviene rapidamente o continuamente.</p>
<p><strong>Vulnerabilit√† agli attacchi avversari:</strong> Il ‚Äúdistribution shift‚Äù pu√≤ rendere i modelli ML pi√π vulnerabili agli attacchi avversari. Gli avversari possono sfruttare la sensibilit√† del modello ai cambiamenti distributivi creando esempi avversari al di fuori della distribuzione di addestramento, facendo s√¨ che il modello faccia previsioni errate o si comporti in modo inaspettato.</p>
<p>Per mitigare l‚Äôimpatto dei ‚Äúdistribution shift‚Äù, √® fondamentale sviluppare sistemi ML robusti che rilevino e si adattino ai cambiamenti delle distribuzioni. Tecniche come l‚Äôadattamento del dominio, l‚Äôapprendimento tramite trasferimento e l‚Äôapprendimento continuo possono aiutare a migliorare la capacit√† di generalizzazione del modello su diverse distribuzioni. Il monitoraggio, il test e l‚Äôaggiornamento del modello ML sono inoltre necessari per garantirne le prestazioni e l‚Äôaffidabilit√† durante i ‚Äúdistribution shift‚Äù.</p>
</section>
</section>
<section id="rilevamento-e-mitigazione" class="level3 page-columns page-full" data-number="17.4.4">
<h3 data-number="17.4.4" class="anchored" data-anchor-id="rilevamento-e-mitigazione"><span class="header-section-number">17.4.4</span> Rilevamento e Mitigazione</h3>
<section id="attacchi-avversari-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="attacchi-avversari-2">Attacchi Avversari</h4>
<p>Come si ricorder√† da quanto sopra, gli attacchi avversari rappresentano una minaccia significativa per la robustezza e l‚Äôaffidabilit√† dei sistemi ML. Questi attacchi comportano la creazione di input attentamente progettati, noti come ‚Äúesempi avversari‚Äù, per ingannare i modelli ML e fargli fare previsioni errate. Per proteggere i sistemi ML dagli attacchi avversari, √® fondamentale sviluppare tecniche efficaci per rilevare e mitigare queste minacce.</p>
<section id="tecniche-di-rilevamento-degli-esempi-avversari" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="tecniche-di-rilevamento-degli-esempi-avversari">Tecniche di Rilevamento degli Esempi Avversari</h5>
<p>Il rilevamento degli esempi avversari √® la prima linea di difesa contro gli attacchi avversari. Sono state proposte diverse tecniche per identificare e segnalare input sospetti che potrebbero essere avversari.</p>
<p>I metodi statistici mirano a rilevare gli esempi avversari analizzando le propriet√† statistiche dei dati di input. Questi metodi spesso confrontano la distribuzione dei dati di input con una di riferimento, come quella dei dati di training o una nota distribuzione benigna. Tecniche come il test <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm">Kolmogorov-Smirnov</a> <span class="citation" data-cites="berger2014kolmogorov">(<a href="../../references.it.html#ref-berger2014kolmogorov" role="doc-biblioref">Berger e Zhou 2014</a>)</span> o il test <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35e.htm">Anderson-Darling</a> possono essere utilizzate per misurare la discrepanza tra le distribuzioni e segnalare gli input che si discostano in modo significativo dalla distribuzione prevista.</p>
<div class="no-row-height column-margin column-container"><div id="ref-berger2014kolmogorov" class="csl-entry" role="listitem">
Berger, Vance W, e YanYan Zhou. 2014. <span>¬´Kolmogorov<span></span>smirnov test: <span>Overview</span>¬ª</span>. <em>Wiley statsref: Statistics reference online</em>.
</div></div><p><a href="https://mathisonian.github.io/kde/">Kernel density estimation (KDE)</a> √® una tecnica non parametrica utilizzata per stimare la funzione di densit√† di probabilit√† di un set di dati. Nel contesto del rilevamento di esempi avversari, KDE pu√≤ essere utilizzato per stimare la densit√† di esempi benigni nello spazio di input. Gli esempi avversari spesso si trovano in regioni a bassa densit√† e possono essere rilevati confrontando la loro densit√† stimata con una soglia. Gli input con una densit√† stimata al di sotto della soglia vengono segnalati come potenziali esempi avversari.</p>
<p>Un‚Äôaltra tecnica √® la compressione delle feature <span class="citation" data-cites="panda2019discretization">(<a href="../../references.it.html#ref-panda2019discretization" role="doc-biblioref">Panda, Chakraborty, e Roy 2019</a>)</span>, che riduce la complessit√† dello spazio di input applicando la riduzione della dimensionalit√† o la discretizzazione. L‚Äôidea alla base della compressione delle feature √® che gli esempi avversari spesso si basano su piccole perturbazioni impercettibili che possono essere eliminate o ridotte tramite queste trasformazioni. Le incongruenze possono essere rilevate confrontando le previsioni del modello sull‚Äôinput originale e sull‚Äôinput compresso, indicando la presenza di esempi avversari.</p>
<div class="no-row-height column-margin column-container"><div id="ref-panda2019discretization" class="csl-entry" role="listitem">
Panda, Priyadarshini, Indranil Chakraborty, e Kaushik Roy. 2019. <span>¬´Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks¬ª</span>. <em>#IEEE_O_ACC#</em> 7: 70157‚Äì68. <a href="https://doi.org/10.1109/access.2019.2919463">https://doi.org/10.1109/access.2019.2919463</a>.
</div></div><p>Le tecniche di stima dell‚Äôincertezza del modello mirano a quantificare la fiducia o l‚Äôincertezza associata alle previsioni di un modello. Gli esempi avversari spesso sfruttano regioni di elevata incertezza nel confine di decisione del modello. Stimando l‚Äôincertezza utilizzando tecniche come reti neurali bayesiane, stima dell‚Äôincertezza basata su dropout o metodi di ensemble, gli input con elevata incertezza possono essere contrassegnati come potenziali esempi avversari.</p>
</section>
<section id="strategie-di-difesa-avversarie" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="strategie-di-difesa-avversarie">Strategie di Difesa Avversarie</h5>
<p>Una volta rilevati gli esempi avversari, possono essere impiegate varie strategie di difesa per mitigarne l‚Äôimpatto e migliorare la robustezza dei modelli ML.</p>
<p>L‚Äôaddestramento avversario √® una tecnica che prevede l‚Äôaumento dei dati di addestramento con esempi avversari e il riaddestramento del modello su questo set di dati aumentato. Esporre il modello a esempi avversari durante l‚Äôaddestramento gli insegna a classificarli correttamente e diventa pi√π robusto agli attacchi avversari. L‚Äôaddestramento avversario pu√≤ essere eseguito utilizzando vari metodi di attacco, come il <a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">Fast Gradient Sign Method (FGSM)</a> o il Projected Gradient Descent (PGD) <span class="citation" data-cites="madry2017towards">(<a href="../../references.it.html#ref-madry2017towards" role="doc-biblioref">Madry et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-madry2017towards" class="csl-entry" role="listitem">
Madry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, e Adrian Vladu. 2017. <span>¬´Towards deep learning models resistant to adversarial attacks¬ª</span>. <em>arXiv preprint arXiv:1706.06083</em>.
</div><div id="ref-papernot2016distillation" class="csl-entry" role="listitem">
Papernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, e Ananthram Swami. 2016. <span>¬´Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks¬ª</span>. In <em>2016 IEEE Symposium on Security and Privacy (SP)</em>, 582‚Äì97. IEEE; IEEE. <a href="https://doi.org/10.1109/sp.2016.41">https://doi.org/10.1109/sp.2016.41</a>.
</div></div><p>La distillazione difensiva <span class="citation" data-cites="papernot2016distillation">(<a href="../../references.it.html#ref-papernot2016distillation" role="doc-biblioref">Papernot et al. 2016</a>)</span> √® una tecnica che addestra un secondo modello (il modello studente) per imitare il comportamento di quello originale (il modello insegnante). Il modello studente viene addestrato sulle etichette soft prodotte dal modello insegnante, che sono meno sensibili alle piccole perturbazioni. L‚Äôutilizzo del modello studente per l‚Äôinferenza pu√≤ ridurre l‚Äôimpatto delle perturbazioni avversarie, poich√© il modello studente impara a generalizzare meglio ed √® meno sensibile al rumore avversario.</p>
<p>Le tecniche di pre-elaborazione e trasformazione dell‚Äôinput mirano a rimuovere o mitigare l‚Äôeffetto delle perturbazioni avversarie prima di alimentare l‚Äôinput nel modello ML. Queste tecniche includono la rimozione del rumore dalle immagini, la compressione JPEG, il ridimensionamento casuale, il padding o l‚Äôapplicazione di trasformazioni casuali ai dati di input. Riducendo l‚Äôimpatto delle perturbazioni avversarie, questi passaggi di pre-elaborazione possono aiutare a migliorare la robustezza del modello agli attacchi avversari.</p>
<p>I metodi ensemble combinano pi√π modelli per fare previsioni pi√π robuste. L‚Äôensemble pu√≤ ridurre l‚Äôimpatto degli attacchi avversari utilizzando un set diversificato di modelli con diverse architetture, dati di training o iperparametri. Esempi avversari che ingannano un modello potrebbero non ingannare gli altri nell‚Äôinsieme, portando a previsioni pi√π affidabili e robuste. Le tecniche di diversificazione del modello, come l‚Äôutilizzo di diverse tecniche di pre-elaborazione o rappresentazioni delle caratteristiche per ogni modello nell‚Äôinsieme, possono migliorare ulteriormente la robustezza.</p>
</section>
<section id="valutazione-e-test-della-robustezza" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="valutazione-e-test-della-robustezza">Valutazione e Test della Robustezza</h5>
<p>Condurre valutazioni e test approfonditi per valutare l‚Äôefficacia delle tecniche di difesa avversarie e misurare la robustezza dei modelli ML.</p>
<p>Le metriche di robustezza avversaria quantificano la resilienza del modello agli attacchi avversari. Queste metriche possono includere l‚Äôaccuratezza del modello sugli esempi avversari, la distorsione media richiesta per ingannare il modello o le prestazioni del modello in base a diversi livelli di attacco. Confrontando queste metriche tra diversi modelli o tecniche di difesa, i professionisti possono valutare e confrontare i loro livelli di robustezza.</p>
<p>I benchmark e i set di dati standardizzati per gli attacchi avversari forniscono una base comune per valutare e confrontare la robustezza dei modelli ML. Questi benchmark includono set di dati con esempi avversari pre-generati e strumenti e framework per generare attacchi avversari. Esempi di benchmark di attacchi avversari popolari includono i set di dati <a href="https://github.com/google-research/mnist-c">MNIST-C</a>, <a href="https://paperswithcode.com/dataset/cifar-10c">CIFAR-10-C</a> e ImageNet-C <span class="citation" data-cites="hendrycks2019benchmarking">(<a href="../../references.it.html#ref-hendrycks2019benchmarking" role="doc-biblioref">Hendrycks e Dietterich 2019</a>)</span>, che contengono versioni corrotte o perturbate dei set di dati originali.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hendrycks2019benchmarking" class="csl-entry" role="listitem">
Hendrycks, Dan, e Thomas Dietterich. 2019. <span>¬´Benchmarking neural network robustness to common corruptions and perturbations¬ª</span>. <em>arXiv preprint arXiv:1903.12261</em>.
</div></div><p>I professionisti possono sviluppare sistemi ML pi√π robusti e resilienti sfruttando queste tecniche di rilevamento di esempi avversari, strategie di difesa e metodi di valutazione della robustezza. Tuttavia, √® importante notare che la robustezza avversaria √® un‚Äôarea di ricerca in corso e nessuna tecnica singola fornisce una protezione completa contro tutti i tipi di attacchi avversari. Un approccio completo che combina pi√π meccanismi di difesa e test regolari √® essenziale per mantenere la sicurezza e l‚Äôaffidabilit√† dei sistemi ML di fronte alle minacce avversarie in evoluzione.</p>
</section>
</section>
<section id="avvelenamento-dei-dati-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="avvelenamento-dei-dati-1">Avvelenamento dei Dati</h4>
<p>Si ricorda che il data poisoning √® un attacco che prende di mira l‚Äôintegrit√† dei dati di training utilizzati per creare modelli ML. Manipolando o corrompendo i dati di training, gli aggressori possono influenzare il comportamento del modello e fargli fare previsioni errate o eseguire azioni indesiderate. Rilevare e mitigare gli attacchi di data poisoning √® fondamentale per garantire l‚Äôaffidabilit√† e la sicurezza dei sistemi ML, come mostrato in <a href="#fig-adversarial-attack-injection" class="quarto-xref">Figura&nbsp;<span>17.30</span></a>.</p>
<section id="tecniche-di-rilevamento-delle-anomalie-per-identificare-i-dati-avvelenati" class="level5">
<h5 class="anchored" data-anchor-id="tecniche-di-rilevamento-delle-anomalie-per-identificare-i-dati-avvelenati">Tecniche di rilevamento delle anomalie per identificare i Dati Avvelenati</h5>
<div id="fig-adversarial-attack-injection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-attack-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/adversarial_attack_injection.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-attack-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.30: Iniezione di dati dannosi. Fonte: <a href="https://www.mdpi.com/2227-7390/12/2/247">Li</a>
</figcaption>
</figure>
</div>
<p>I metodi di rilevamento statistico degli outlier identificano i dati che si discostano in modo significativo dalla maggior parte. Questi metodi presuppongono che le istanze di dati avvelenati siano probabilmente ‚Äúoutlier‚Äù statistici‚Äù. Tecniche come il <a href="https://ubalt.pressbooks.pub/mathstatsguides/chapter/z-score-basics/">Metodo Z-score</a>, il <a href="https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm">Metodo di Tukey</a> o la <a href="https://www.statisticshowto.com/mahalanobis-distance/">Distanza di Mahalanobis</a> possono essere utilizzate per misurare la deviazione di ciascun punto dati dalla tendenza centrale del set di dati. I dati che superano una soglia predefinita vengono contrassegnati come potenziali valori anomali e considerati sospetti di avvelenamento dei dati.</p>
<p>I metodi basati sul clustering raggruppano dati simili in base alle loro caratteristiche o attributi. Il presupposto √® che le istanze di dati avvelenate possano formare cluster distinti o trovarsi lontano dai normali cluster di dati. Applicando algoritmi di clustering come <a href="https://www.oreilly.com/library/view/data-algorithms/9781491906170/ch12.html">K-means</a>, <a href="https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/50efb27d-abbe-4855-ad81-a5357050161f.xhtml">DBSCAN</a> o <a href="https://www.oreilly.com/library/view/cluster-analysis-5th/9780470978443/chapter04.html">clustering gerarchico</a>, √® possibile identificare cluster anomali o dati che non appartengono a nessun cluster. Queste istanze anomale vengono poi trattate come dati potenzialmente avvelenati.</p>
<div id="fig-autoencoder" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-autoencoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/autoencoder.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-autoencoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.31: Autoencoder. Fonte: <a href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798">Dertat</a>
</figcaption>
</figure>
</div>
<p>Gli autoencoder sono reti neurali addestrate per ricostruire i dati di input da una rappresentazione compressa, come mostrato in <a href="#fig-autoencoder" class="quarto-xref">Figura&nbsp;<span>17.31</span></a>. Possono essere utilizzati per il rilevamento di anomalie apprendendo i pattern normali nei dati e identificando le istanze che si discostano da essi. Durante l‚Äôaddestramento, l‚Äôautoencoder viene addestrato su dati puliti e non avvelenati. Al momento dell‚Äôinferenza, viene calcolato l‚Äôerrore di ricostruzione per ogni dato. I dati con errori di ricostruzione elevati sono considerati anomali e potenzialmente avvelenati, poich√© non sono conformi ai pattern normali appresi.</p>
</section>
<section id="tecniche-di-sanificazione-e-preelaborazione-dei-dati" class="level5">
<h5 class="anchored" data-anchor-id="tecniche-di-sanificazione-e-preelaborazione-dei-dati">Tecniche di Sanificazione e Preelaborazione dei Dati</h5>
<p>L‚Äôavvelenamento dei dati pu√≤ essere evitato pulendo i dati, il che implica l‚Äôidentificazione e la rimozione o la correzione di dati rumorosi, incompleti o incoerenti. Tecniche come la deduplicazione dei dati, l‚Äôimputazione dei valori mancanti e la rimozione dei valori anomali possono essere applicate per migliorare la qualit√† dei dati di addestramento. Eliminando o filtrando i dati sospetti o anomali, √® possibile ridurre l‚Äôimpatto delle istanze avvelenate.</p>
<p>La validazione dei dati implica la verifica dell‚Äôintegrit√† e della coerenza dei dati di training. Ci√≤ pu√≤ includere il controllo della coerenza del tipo di dati, la convalida dell‚Äôintervallo e le dipendenze tra campi. Definendo e applicando le regole di validazione dei dati, i dati anomali o incoerenti indicativi di avvelenamento possono essere identificati e segnalati per ulteriori indagini.</p>
<p>La provenienza dei dati e il tracciamento della discendenza implicano il mantenimento di un registro dell‚Äôorigine, delle trasformazioni e dei movimenti dei dati in tutta la pipeline ML. Documentando le fonti dei dati, i passaggi di pre-elaborazione e qualsiasi modifica apportata, i professionisti possono risalire alle anomalie o ai pattern sospetti fino alla loro origine. Ci√≤ aiuta a identificare potenziali punti di avvelenamento dei dati e facilita il processo di indagine e mitigazione.</p>
</section>
<section id="tecniche-di-training-robusti" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="tecniche-di-training-robusti">Tecniche di Training Robusti</h5>
<p>√à possibile utilizzare tecniche di ottimizzazione robuste per modificare l‚Äôobiettivo del training per ridurre al minimo l‚Äôimpatto di valori anomali o istanze avvelenate. Ci√≤ pu√≤ essere ottenuto utilizzando funzioni di perdita robuste meno sensibili ai valori estremi, come la ‚ÄúHuber loss‚Äù o la ‚Äúmodified Huber loss‚Äù. Le tecniche di regolarizzazione, come la <a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">regolarizzazione L1 o L2</a>, possono anche aiutare a ridurre la sensibilit√† del modello ai dati avvelenati, limitando la complessit√† del modello e prevenendo l‚Äôoverfitting.</p>
<p>Le funzioni di ‚Äúloss‚Äù [perdita] robuste sono progettate per essere meno sensibili ai valori anomali o ai dati rumorosi. Esempi includono la <a href="https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html">Huber loss</a> modificata, la perdita di Tukey <span class="citation" data-cites="beaton1974fitting">(<a href="../../references.it.html#ref-beaton1974fitting" role="doc-biblioref">Beaton e Tukey 1974</a>)</span> e la ‚Äútrimmed mean loss‚Äù. Queste funzioni di perdita riducono o ignorano il contributo delle istanze anomale durante il training, riducendo il loro impatto sul processo di apprendimento del modello. Le funzioni ‚Äúobiettivo‚Äù robuste, come l‚Äôobiettivo minimax o la ‚Äúdistributivamente robusto‚Äù, mirano a ottimizzare le prestazioni del modello negli scenari peggiori o in presenza di perturbazioni avversarie.</p>
<div class="no-row-height column-margin column-container"><div id="ref-beaton1974fitting" class="csl-entry" role="listitem">
Beaton, Albert E., e John W. Tukey. 1974. <span>¬´The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data¬ª</span>. <em>Technometrics</em> 16 (2): 147. <a href="https://doi.org/10.2307/1267936">https://doi.org/10.2307/1267936</a>.
</div></div><p>Le tecniche di ‚Äúdata augmentation‚Äù comportano la generazione di esempi di addestramento aggiuntivi applicando trasformazioni o perturbazioni casuali ai dati esistenti <a href="#fig-data-augmentation" class="quarto-xref">Figura&nbsp;<span>17.32</span></a>. Ci√≤ aiuta ad aumentare la diversit√† e la robustezza del set di dati di addestramento. Introducendo variazioni controllate nei dati, il modello diventa meno sensibile a pattern o artefatti specifici che possono essere presenti in istanze avvelenate. Le tecniche di randomizzazione, come il sottocampionamento casuale o l‚Äôaggregazione bootstrap, possono anche aiutare a ridurre l‚Äôimpatto dei dati avvelenati addestrando pi√π modelli su diversi sottoinsiemi di dati e combinando le loro previsioni.</p>
<div id="fig-data-augmentation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-augmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/data_augmentation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-augmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.32: Un‚Äôimmagine del numero ‚Äú3‚Äù nella forma originale e con applicati degli aumenti di base.
</figcaption>
</figure>
</div>
</section>
<section id="approvvigionamento-di-dati-sicuro-e-affidabile" class="level5">
<h5 class="anchored" data-anchor-id="approvvigionamento-di-dati-sicuro-e-affidabile">Approvvigionamento di Dati Sicuro e Affidabile</h5>
<p>L‚Äôimplementazione delle migliori pratiche di raccolta e cura dei dati pu√≤ aiutare a mitigare il rischio di avvelenamento dei dati. Ci√≤ include l‚Äôistituzione di protocolli di raccolta dati chiari, la verifica dell‚Äôautenticit√† e dell‚Äôaffidabilit√† delle fonti dati e la conduzione di valutazioni regolari della qualit√† dei dati. L‚Äôapprovvigionamento di dati da provider affidabili e rispettabili e il rispetto di pratiche di gestione dei dati sicure possono ridurre la probabilit√† di introdurre dati avvelenati nella pipeline di training.</p>
<p>Solidi meccanismi di governance dei dati e controllo degli accessi sono essenziali per prevenire modifiche non autorizzate o manomissioni dei dati di training. Ci√≤ implica la definizione di ruoli e responsabilit√† chiari per l‚Äôaccesso ai dati, l‚Äôimplementazione di policy di controllo degli accessi basate sul principio del privilegio minimo e il monitoraggio e il logging delle attivit√† di accesso ai dati. Limitando l‚Äôaccesso ai dati di training e mantenendo un audit trail, √® possibile rilevare e investigare potenziali tentativi di avvelenamento dei dati.</p>
<p>Rilevare e mitigare gli attacchi di avvelenamento dei dati richiede un approccio poliedrico che combini rilevamento delle anomalie, sanificazione dei dati, tecniche di training affidabili e pratiche di approvvigionamento dei dati sicure. Implementando queste misure, i professionisti del ML possono migliorare la resilienza dei loro modelli contro l‚Äôavvelenamento dei dati e garantire l‚Äôintegrit√† e l‚Äôaffidabilit√† dei dati di training. Tuttavia, √® importante notare che l‚Äôavvelenamento dei dati √® un‚Äôarea di ricerca attiva e continuano a emergere nuovi vettori di attacco e meccanismi di difesa. Rimanere informati sugli ultimi sviluppi e adottare un approccio proattivo e adattivo alla sicurezza dei dati √® fondamentale per mantenere la robustezza dei sistemi ML.</p>
</section>
</section>
<section id="distribution-shift-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="distribution-shift-1">Distribution Shift</h4>
<section id="rilevamento-e-mitigazione-dei-distribution-shift" class="level5">
<h5 class="anchored" data-anchor-id="rilevamento-e-mitigazione-dei-distribution-shift">Rilevamento e Mitigazione dei ‚ÄúDistribution Shift‚Äù</h5>
<p>Ricordiamo che i ‚Äúdistribution shift‚Äù [spostamenti di distribuzione] si verificano quando la distribuzione dei dati incontrata da un modello di machine learning (ML) durante l‚Äôimplementazione differisce dalla distribuzione su cui √® stato addestrato. Questi spostamenti possono avere un impatto significativo sulle prestazioni e sulla capacit√† di generalizzazione del modello, portando a previsioni non ottimali o errate. Rilevare e mitigare i ‚Äúdistribution shift‚Äù √® fondamentale per garantire la robustezza e l‚Äôaffidabilit√† dei sistemi ML in scenari reali.</p>
</section>
<section id="tecniche-di-rilevamento-per-i-distribution-shift" class="level5">
<h5 class="anchored" data-anchor-id="tecniche-di-rilevamento-per-i-distribution-shift">Tecniche di Rilevamento per i ‚ÄúDistribution Shift‚Äù</h5>
<p>I test statistici possono essere utilizzati per confrontare le distribuzioni dei dati di training e di test per identificare differenze significative. Tecniche come il test di Kolmogorov-Smirnov o il test di Anderson-Darling misurano la discrepanza tra due distribuzioni e forniscono una valutazione quantitativa della presenza di un ‚Äúdistribution shift‚Äù. Applicando questi test alle funzionalit√† di input o alle previsioni del modello, i professionisti possono rilevare se esiste una differenza statisticamente significativa tra le distribuzioni di training e di test.</p>
<p>Le metriche di divergenza quantificano la dissimilarit√† tra due distribuzioni di probabilit√†. Le metriche di divergenza comunemente utilizzate includono la <a href="https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254">Divergenza Kullback-Leibler (KL)</a> e la <a href="https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6">Divergenza Jensen-Shannon (JS)</a>. Calcolando la divergenza tra le distribuzioni dei dati di training e di test, i professionisti possono valutare l‚Äôentit√† dello ‚Äúspostamento della distribuzione‚Äù. Valori di divergenza elevati indicano una differenza significativa tra le distribuzioni, suggerendo la presenza di uno spostamento della distribuzione.</p>
<p>Le tecniche di quantificazione dell‚Äôincertezza, come le reti neurali bayesiane o i metodi di ensemble, possono stimare l‚Äôincertezza associata alle previsioni del modello. Quando un modello viene applicato a dati da una distribuzione diversa, le sue previsioni potrebbero avere un‚Äôincertezza maggiore. Monitorando i livelli di incertezza, i professionisti possono rilevare gli spostamenti della distribuzione. Se l‚Äôincertezza supera costantemente una soglia predeterminata per i campioni di test, ci√≤ suggerisce che il modello sta operando al di fuori della sua distribuzione addestrata.</p>
<p>Inoltre, i classificatori di dominio sono addestrati a distinguere tra diversi domini o distribuzioni. I professionisti possono rilevare gli spostamenti di distribuzione addestrando un classificatore a distinguere tra i domini di addestramento e di test. Se il classificatore di dominio raggiunge un‚Äôelevata accuratezza nel distinguere tra i due domini, indica una differenza significativa nelle distribuzioni sottostanti. Le prestazioni del classificatore di dominio servono come misura dello spostamento di distribuzione.</p>
</section>
<section id="tecniche-di-mitigazione-per-i-distribution-shift" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="tecniche-di-mitigazione-per-i-distribution-shift">Tecniche di Mitigazione per i ‚ÄúDistribution Shift‚Äù</h5>
<div id="fig-transfer-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/transfer_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.33: Trasferimento dell‚Äôapprendimento. Fonte: <a href="https://medium.com/modern-nlp/transfer-learning-in-nlp-f5035cc3f62f">Bhavsar</a>
</figcaption>
</figure>
</div>
<p>Il ‚Äútransfer learning‚Äù [trasferimento dell‚Äôapprendimento.] sfrutta le conoscenze acquisite da un dominio per migliorare le prestazioni in un altro, come mostrato in <a href="#fig-transfer-learning" class="quarto-xref">Figura&nbsp;<span>17.33</span></a>. Utilizzando modelli pre-addestrati o trasferendo le feature apprese da un dominio di origine a un dominio di destinazione, il transfer learning pu√≤ aiutare a mitigare l‚Äôimpatto dei ‚Äúdistribution shift‚Äù. Il modello pre-addestrato pu√≤ essere messo a punto su una piccola quantit√† di dati etichettati dal dominio target, consentendogli di adattarsi alla nuova distribuzione. Il transfer learning √® particolarmente efficace quando i domini di origine e di destinazione condividono caratteristiche simili o quando i dati etichettati nel dominio di destinazione sono scarsi.</p>
<p>L‚Äôapprendimento continuo, noto anche come apprendimento permanente, consente ai modelli ML di apprendere continuamente da nuove distribuzioni di dati, mantenendo al contempo le conoscenze delle distribuzioni precedenti. Tecniche come la ‚Äúelastic weight consolidation (EWC)‚Äù <span class="citation" data-cites="kirkpatrick2017overcoming">(<a href="../../references.it.html#ref-kirkpatrick2017overcoming" role="doc-biblioref">Kirkpatrick et al. 2017</a>)</span> o la ‚Äúgradient episodic memory (GEM)‚Äù <span class="citation" data-cites="lopez2017gradient">(<a href="../../references.it.html#ref-lopez2017gradient" role="doc-biblioref">Lopez-Paz e Ranzato 2017</a>)</span> consentono ai modelli di adattarsi alle distribuzioni di dati in evoluzione nel tempo. Queste tecniche mirano a bilanciare la plasticit√† del modello (capacit√† di apprendere da nuovi dati) con la stabilit√† del modello (mantenendo le conoscenze apprese in precedenza). Aggiornando gradualmente il modello con nuovi dati e mitigando l‚Äôoblio catastrofico, l‚Äôapprendimento continuo aiuta i modelli a rimanere robusti ai ‚Äúdistribution shift‚Äù.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kirkpatrick2017overcoming" class="csl-entry" role="listitem">
Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017. <span>¬´Overcoming catastrophic forgetting in neural networks¬ª</span>. <em>Proc. Natl. Acad. Sci.</em> 114 (13): 3521‚Äì26. <a href="https://doi.org/10.1073/pnas.1611835114">https://doi.org/10.1073/pnas.1611835114</a>.
</div><div id="ref-lopez2017gradient" class="csl-entry" role="listitem">
Lopez-Paz, David, e Marc‚ÄôAurelio Ranzato. 2017. <span>¬´Gradient episodic memory for continual learning¬ª</span>. <em>Adv Neural Inf Process Syst</em> 30.
</div></div><p>Le tecniche di aumento dei dati, come quelle viste in precedenza, comportano l‚Äôapplicazione di trasformazioni o perturbazioni ai dati di training esistenti per aumentarne la diversit√† e migliorare la robustezza del modello ai ‚Äúdistribution shift‚Äù. Introducendo variazioni nei dati, come rotazioni, traslazioni, ridimensionamenti o aggiunta di rumore, l‚Äôaumento dei dati aiuta il modello ad apprendere caratteristiche invarianti e a generalizzare meglio a distribuzioni mai viste. Il ‚Äúdata augmentation‚Äù pu√≤ essere eseguito durante l‚Äôaddestramento e l‚Äôinferenza per migliorare la capacit√† del modello di gestire i ‚Äúdistribution shift‚Äù.</p>
<p>I metodi ensemble combinano pi√π modelli per rendere le previsioni pi√π robuste ai ‚Äúdistribution shift‚Äù. Addestrando i modelli su diversi sottoinsiemi di dati, utilizzando algoritmi diversi o con diversi iperparametri, i metodi ensemble possono catturare diversi aspetti della distribuzione dei dati. Quando viene presentata una distribuzione ‚Äúspostata‚Äù, l‚Äôensemble pu√≤ sfruttare i punti di forza dei singoli modelli per fare previsioni pi√π accurate e stabili. Tecniche come il bagging, il boosting o lo stacking possono creare ensemble efficaci.</p>
<p>Aggiornare regolarmente i modelli con nuovi dati dalla distribuzione target √® fondamentale per mitigare l‚Äôimpatto dei ‚Äúdistribution shift‚Äù. Man mano che la distribuzione dei dati si evolve, i modelli dovrebbero essere riaddestrati o perfezionati sui dati disponibili pi√π recenti per adattarsi ai pattern mutevoli. Il monitoraggio delle prestazioni del modello e delle caratteristiche dei dati pu√≤ aiutare a rilevare quando √® necessario un aggiornamento. Mantenendo aggiornati i modelli, i professionisti possono garantire che rimangano pertinenti e accurati di fronte ai distribution shift‚Äù.</p>
<p>La valutazione dei modelli utilizzando metriche robuste meno sensibili ai ‚Äúdistribution shift‚Äù pu√≤ fornire una valutazione pi√π affidabile delle prestazioni del modello. Metriche come l‚Äô‚Äúarea under the precision-recall curve (AUPRC)‚Äù [area sotto la curva di precisione-richiamo] o il punteggio F1 sono pi√π robuste allo squilibrio di classe e possono catturare meglio le prestazioni del modello su diverse distribuzioni. Inoltre, l‚Äôutilizzo di metriche di valutazione specifiche del dominio che si allineano con i risultati desiderati nel dominio target pu√≤ fornire una misura pi√π significativa dell‚Äôefficacia del modello.</p>
<p>Rilevare e mitigare i ‚Äúdistribution shift‚Äù √® un processo continuo che richiede monitoraggio, adattamento e miglioramento continui. Utilizzando una combinazione di tecniche di rilevamento e strategie di mitigazione, i professionisti del ML possono identificare e affrontare in modo proattivo i ‚Äúdistribution shift‚Äù, garantendo la robustezza e l‚Äôaffidabilit√† dei loro modelli nelle distribuzioni del mondo reale. √à importante notare che i ‚Äúdistribution shift‚Äù possono assumere varie forme e potrebbero richiedere approcci specifici del dominio a seconda della natura dei dati e dell‚Äôapplicazione. Rimanere informati sulle ultime ricerche e sulle best practice nella gestione dei ‚Äúdistribution shift‚Äù √® essenziale per creare sistemi ML resilienti.</p>
</section>
</section>
</section>
</section>
<section id="errori-software" class="level2 page-columns page-full" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="errori-software"><span class="header-section-number">17.5</span> Errori Software</h2>
<section id="definizione-e-caratteristiche-6" class="level3 page-columns page-full" data-number="17.5.1">
<h3 data-number="17.5.1" class="anchored" data-anchor-id="definizione-e-caratteristiche-6"><span class="header-section-number">17.5.1</span> Definizione e Caratteristiche</h3>
<p>Gli errori software si riferiscono a difetti, errori o bug nei framework software runtime e nei componenti che supportano l‚Äôesecuzione e la distribuzione di modelli ML <span class="citation" data-cites="myllyaho2022misbehaviour">(<a href="../../references.it.html#ref-myllyaho2022misbehaviour" role="doc-biblioref">Myllyaho et al. 2022</a>)</span>. Questi guasti possono derivare da varie fonti, come errori di programmazione, difetti di progettazione o problemi di compatibilit√† <span class="citation" data-cites="zhang2008distribution">(<a href="../../references.it.html#ref-zhang2008distribution" role="doc-biblioref">H. Zhang 2008</a>)</span>, e possono avere implicazioni significative per le prestazioni, l‚Äôaffidabilit√† e la sicurezza dei sistemi ML. Gli errori software nei framework ML presentano diverse caratteristiche chiave:</p>
<div class="no-row-height column-margin column-container"><div id="ref-myllyaho2022misbehaviour" class="csl-entry" role="listitem">
Myllyaho, Lalli, Mikko Raatikainen, Tomi M√§nnist√∂, Jukka K. Nurminen, e Tommi Mikkonen. 2022. <span>¬´On misbehaviour and fault tolerance in machine learning systems¬ª</span>. <em>J. Syst. Software</em> 183 (gennaio): 111096. <a href="https://doi.org/10.1016/j.jss.2021.111096">https://doi.org/10.1016/j.jss.2021.111096</a>.
</div><div id="ref-zhang2008distribution" class="csl-entry" role="listitem">
Zhang, Hongyu. 2008. <span>¬´On the Distribution of Software Faults¬ª</span>. <em>IEEE Trans. Software Eng.</em> 34 (2): 301‚Äì2. <a href="https://doi.org/10.1109/tse.2007.70771">https://doi.org/10.1109/tse.2007.70771</a>.
</div></div><ul>
<li><p><strong>Diversit√†:</strong> Gli errori software possono manifestarsi in forme diverse, che vanno da semplici errori di logica e sintassi a problemi pi√π complessi come perdite di memoria, condizioni di ‚Äúrace‚Äù e problemi di integrazione. La variet√† di tipi di errori aumenta la sfida di rilevarli e mitigarli in modo efficace.</p></li>
<li><p><strong>Propagazione:</strong> Nei sistemi ML, gli errori software possono propagarsi attraverso i vari layer e componenti del framework. Un errore in un modulo pu√≤ innescare una cascata di errori o comportamenti imprevisti in altre parti del sistema, rendendo difficile individuare la causa principale e valutare l‚Äôimpatto completo dell‚Äôerrore.</p></li>
<li><p><strong>Intermittenza:</strong> Alcuni errori software possono presentare un comportamento intermittente, che si verifica sporadicamente o in condizioni specifiche. Questi errori possono essere particolarmente difficili da riprodurre e correggere, poich√© possono manifestarsi in modo incoerente durante i test o il normale funzionamento.</p></li>
<li><p><strong>Interazione con i modelli ML:</strong> Gli errori software nei framework ML possono interagire con i modelli addestrati in modi sottili. Ad esempio, un errore nella pipeline di preelaborazione dei dati pu√≤ introdurre rumore o distorsione negli input del modello, causando prestazioni degradate o previsioni errate. Analogamente, gli errori nel componente di servizio del modello possono causare incongruenze tra gli ambienti di training e inferenza.</p></li>
<li><p><strong>Impatto sulle propriet√† del sistema:</strong> Gli errori software possono compromettere varie propriet√† desiderabili dei sistemi ML, come prestazioni, scalabilit√†, affidabilit√† e sicurezza. Gli errori possono causare rallentamenti, crash, output errati o vulnerabilit√† che gli aggressori possono sfruttare.</p></li>
<li><p><strong>Dipendenza da fattori esterni:</strong> Il verificarsi e l‚Äôimpatto degli errori software nei framework ML dipendono spesso da fattori esterni, come la scelta di hardware, sistema operativo, librerie e configurazioni. Problemi di compatibilit√† e mancate corrispondenze di versione possono introdurre errori difficili da anticipare e mitigare.</p></li>
</ul>
<p>Comprendere le caratteristiche degli errori software nei framework ML √® fondamentale per sviluppare strategie efficaci di prevenzione, rilevamento e mitigazione degli errori. Riconoscendo la diversit√†, la propagazione, l‚Äôintermittenza e l‚Äôimpatto dei guasti software, i professionisti del ML possono progettare sistemi pi√π robusti e affidabili, resilienti a questi problemi.</p>
</section>
<section id="meccanismi-degli-errori-software-nei-framework-ml" class="level3" data-number="17.5.2">
<h3 data-number="17.5.2" class="anchored" data-anchor-id="meccanismi-degli-errori-software-nei-framework-ml"><span class="header-section-number">17.5.2</span> Meccanismi degli Errori Software nei Framework ML</h3>
<p>I framework di apprendimento automatico, come TensorFlow, PyTorch e sci-kit-learn, forniscono potenti strumenti e astrazioni per la creazione e l‚Äôimplementazione di modelli ML. Tuttavia, questi framework non sono immuni da errori software che possono influire sulle prestazioni, l‚Äôaffidabilit√† e la correttezza dei sistemi ML. Esploriamo alcuni degli errori software comuni che possono verificarsi nei framework ML:</p>
<p><strong>Memory Leak e Problemi di Gestione delle Risorse:</strong> Una gestione della memoria non corretta, come il mancato rilascio di memoria o la chiusura di handle di file, pu√≤ portare a perdite di memoria e all‚Äôesaurimento delle risorse nel tempo. Questo problema √® aggravato dall‚Äôutilizzo inefficiente della memoria, in cui la creazione di copie non necessarie di grandi tensori o il mancato sfruttamento di strutture dati efficienti in termini di memoria pu√≤ causare un consumo eccessivo di memoria e degradare le prestazioni del sistema. Inoltre, la mancata gestione corretta della memoria GPU pu√≤ causare errori di ‚Äúout-of-memory‚Äù o un utilizzo non ottimale delle risorse GPU, aggravando ulteriormente il problema come mostrato in <a href="#nt13lz9kgr7t"><span class="quarto-xref">Figura&nbsp;<span>17.34</span></span></a>.</p>
<div id="fig-gpu-out-of-memory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpu-out-of-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/gpu_out_of_memory.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpu-out-of-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.34: Esempio di problemi di memoria e utilizzo non ottimale della GPU
</figcaption>
</figure>
</div>
<p><strong>Problemi di Sincronizzazione e Concorrenza:</strong> Una sincronizzazione non corretta tra thread o processi pu√≤ causare condizioni di ‚Äúrace‚Äù, deadlock o comportamento incoerente nei sistemi ML multi-thread o distribuiti. Questo problema √® spesso legato alla gestione impropria delle <a href="https://odsc.medium.com/optimizing-ml-serving-with-asynchronous-architectures-1071fc1be8e2">operazioni asincrone</a>, come I/O non bloccante o caricamento dati parallelo, che pu√≤ causare problemi di sincronizzazione e influire sulla correttezza della pipeline ML. Inoltre, un coordinamento e una comunicazione adeguati tra nodi distribuiti in un cluster possono causare coerenza o dati obsoleti durante l‚Äôaddestramento o l‚Äôinferenza, compromettendo l‚Äôaffidabilit√† del sistema ML.</p>
<p><strong>Problemi di Compatibilit√†:</strong> Le discrepanze tra le versioni di framework, librerie o dipendenze ML possono introdurre problemi di compatibilit√† ed errori di runtime. L‚Äôaggiornamento o la modifica delle versioni delle librerie sottostanti senza testare a fondo l‚Äôimpatto sul sistema ML pu√≤ portare a comportamenti imprevisti o malfunzionamenti. Inoltre, le incongruenze tra gli ambienti di training e distribuzione, come differenze nell‚Äôhardware, nei sistemi operativi o nelle versioni dei pacchetti, possono causare problemi di compatibilit√† e influire sulla riproducibilit√† dei modelli ML, rendendo difficile garantire prestazioni coerenti sulle diverse piattaforme.</p>
<p><strong>Instabilit√† Numerica ed Errori di Precisione:</strong> Una gestione inadeguata delle <a href="https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter22.04-Numerical-Error-and-Instability.html">instabilit√† numeriche</a>, come la divisione per zero, l‚Äôunderflow o l‚Äôoverflow, pu√≤ portare a calcoli errati o problemi di convergenza durante l‚Äôaddestramento. Questo problema √® aggravato da errori di precisione o arrotondamento insufficienti, che possono accumularsi nel tempo e influire sull‚Äôaccuratezza dei modelli ML, specialmente nelle architetture di deep learning con molti livelli. Inoltre, un ridimensionamento o una normalizzazione impropri dei dati di input possono causare instabilit√† numeriche e influire sulla convergenza e sulle prestazioni degli algoritmi di ottimizzazione, con conseguenti prestazioni del modello non ottimali o inaffidabili.</p>
<p><strong>Gestione degli Errori e delle Eccezioni Inadeguata:</strong> Una corretta gestione degli errori e delle eccezioni pu√≤ impedire ai sistemi ML di bloccarsi o comportarsi in modo imprevisto quando si verificano condizioni eccezionali o input non validi. Non riuscire a catturare e gestire eccezioni specifiche o affidarsi alla gestione generica delle eccezioni pu√≤ rendere difficile diagnosticare e recuperare gli errori in modo corretto, portando a instabilit√† del sistema e affidabilit√† ridotta. Inoltre, messaggi di errore incompleti o fuorvianti possono ostacolare la capacit√† di eseguire il debug e risolvere efficacemente gli errori software nei framework ML, prolungando il tempo necessario per identificare e risolvere i problemi.</p>
</section>
<section id="impatto-sui-sistemi-ml-6" class="level3 page-columns page-full" data-number="17.5.3">
<h3 data-number="17.5.3" class="anchored" data-anchor-id="impatto-sui-sistemi-ml-6"><span class="header-section-number">17.5.3</span> Impatto sui Sistemi ML</h3>
<p>Gli errori software nei framework di apprendimento automatico possono avere impatti significativi e di vasta portata sulle prestazioni, l‚Äôaffidabilit√† e la sicurezza dei sistemi ML. Esploriamo i vari modi in cui gli errori software possono influenzare i sistemi ML:</p>
<p><strong>Degrado delle Prestazioni e Rallentamenti del Sistema:</strong> Memory leak e gestione inefficiente delle risorse possono portare a un graduale degrado delle prestazioni nel tempo, poich√© il sistema diventa sempre pi√π vincolato dalla memoria e impiega pi√π tempo nella garbage collection o nello swapping della memoria <span class="citation" data-cites="maas2008combining">(<a href="../../references.it.html#ref-maas2008combining" role="doc-biblioref">Maas et al. 2024</a>)</span>. Questo problema √® aggravato da problemi di sincronizzazione e bug di concorrenza, che possono causare ritardi, riduzione della produttivit√† e utilizzo non ottimale delle risorse di elaborazione, in particolare nei sistemi ML multi-thread o distribuiti. Inoltre, problemi di compatibilit√† o percorsi di codice inefficienti possono introdurre ulteriori overhead e rallentamenti, influenzando le prestazioni complessive del sistema ML.</p>
<div class="no-row-height column-margin column-container"><div id="ref-maas2008combining" class="csl-entry" role="listitem">
Maas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi Javanmard, Kathryn S. McKinley, e Colin Raffel. 2024. <span>¬´Combining Machine Learning and Lifetime-Based Resource Management for Memory Allocation and Beyond¬ª</span>. <em>Commun. ACM</em> 67 (4): 87‚Äì96. <a href="https://doi.org/10.1145/3611018">https://doi.org/10.1145/3611018</a>.
</div></div><p><strong>Previsioni o Output Errati:</strong> Gli errori software nella pre-elaborazione dei dati, nell‚Äôingegneria delle feature o nella valutazione del modello possono introdurre distorsioni, rumore o errori che si propagano attraverso la pipeline ML e che determinano previsioni o output errati. Nel tempo, instabilit√† numeriche, errori di precisione o <a href="https://www.cs.drexel.edu/~popyack/Courses/CSP/Fa17/extras/Rounding/index.html">problemi di arrotondamento</a> possono accumularsi e portare a problemi di accuratezza o convergenza degradati nei modelli addestrati. Inoltre, gli errori nei componenti di servizio o inferenza del modello possono causare incongruenze tra gli output previsti e quelli effettivi, portando a previsioni errate o inaffidabili in produzione.</p>
<p><strong>Problemi di Affidabilit√† e Stabilit√†:</strong> Gli errori software possono causare eccezioni senza precedenti, crash o terminazioni improvvise che possono compromettere l‚Äôaffidabilit√† e la stabilit√† dei sistemi ML, specialmente negli ambienti di produzione. Gli errori intermittenti o sporadici possono essere difficili da riprodurre e diagnosticare, portando a un comportamento imprevedibile e a una ridotta fiducia negli output del sistema ML. Inoltre, errori nel checkpointing, nella serializzazione del modello o nella gestione dello stato possono causare perdite di dati o incongruenze, influenzando l‚Äôaffidabilit√† e la recuperabilit√† del sistema ML.</p>
<p><strong>Vulnerabilit√† di Sicurezza:</strong> Errori software, come buffer overflow, vulnerabilit√† di ‚Äúinjection‚Äù o controllo di accesso improprio, possono introdurre rischi per la sicurezza ed esporre il sistema ML a potenziali attacchi o accessi non autorizzati. Gli avversari possono sfruttare errori nelle fasi di pre-elaborazione o estrazione delle funzionalit√† per manipolare i dati di input e ingannare i modelli ML, portando a comportamenti errati o dannosi. Inoltre, una protezione inadeguata dei dati sensibili, come le informazioni utente o i parametri riservati del modello, pu√≤ portare a violazioni dei dati o violazioni della privacy <span class="citation" data-cites="li2021survey">(<a href="../../references.it.html#ref-li2021survey" role="doc-biblioref">Q. Li et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2021survey" class="csl-entry" role="listitem">
Li, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, e Bingsheng He. 2023. <span>¬´A Survey on Federated Learning Systems: <span>Vision,</span> Hype and Reality for Data Privacy and Protection¬ª</span>. <em>IEEE Trans. Knowl. Data Eng.</em> 35 (4): 3347‚Äì66. <a href="https://doi.org/10.1109/tkde.2021.3124599">https://doi.org/10.1109/tkde.2021.3124599</a>.
</div></div><p><strong>Difficolt√† nella Riproduzione e nel Debug:</strong> Gli errori software possono rendere difficile la riproduzione e il debug dei problemi nei sistemi ML, soprattutto quando gli errori sono intermittenti o dipendono da condizioni di runtime specifiche. Messaggi di errore incompleti o ambigui, uniti alla complessit√† dei framework e dei modelli ML, possono prolungare il processo di debug e ostacolare la capacit√† di identificare e correggere i guasti sottostanti. Inoltre, le incongruenze tra gli ambienti di sviluppo, test e produzione possono rendere difficile la riproduzione e la diagnosi dei guasti in contesti specifici.</p>
<p><strong>Maggiori Costi di Sviluppo e Manutenzione</strong> I guasti software possono comportare maggiori costi di sviluppo e manutenzione, poich√© i team dedicano pi√π tempo e risorse al debug, alla correzione e alla validazione del sistema ML. La necessit√† di test estesi, monitoraggio e meccanismi di tolleranza agli errori per mitigare l‚Äôimpatto degli errori software pu√≤ aggiungere complessit√† e sovraccarico al processo di sviluppo ML. Patch, aggiornamenti e correzioni di bug frequenti per risolvere gli errori software possono interrompere il flusso di lavoro di sviluppo e richiedere sforzi aggiuntivi per garantire la stabilit√† e la compatibilit√† del sistema ML.</p>
<p>Comprendere il potenziale impatto degli errori software sui sistemi ML √® fondamentale per dare priorit√† agli sforzi di test, implementare progetti di tolleranza agli errori e stabilire pratiche di monitoraggio e debug efficaci. Affrontando in modo proattivo gli errori software e le loro conseguenze, i professionisti ML possono creare sistemi ML pi√π solidi, affidabili e sicuri che forniscono risultati accurati e affidabili.</p>
</section>
<section id="rilevamento-e-mitigazione-1" class="level3 page-columns page-full" data-number="17.5.4">
<h3 data-number="17.5.4" class="anchored" data-anchor-id="rilevamento-e-mitigazione-1"><span class="header-section-number">17.5.4</span> Rilevamento e Mitigazione</h3>
<p>Rilevare e mitigare i guasti software nei framework di apprendimento automatico √® essenziale per garantire l‚Äôaffidabilit√†, le prestazioni e la sicurezza dei sistemi ML. Esploriamo varie tecniche e approcci che possono essere impiegati per identificare e risolvere efficacemente i guasti software:</p>
<p><strong>Test e Validazione Approfonditi:</strong> ‚ÄúUnit test‚Äù completi di singoli componenti e moduli possono verificarne la correttezza e identificare potenziali guasti nelle prime fasi dello sviluppo. I test di integrazione convalidano l‚Äôinterazione e la compatibilit√† tra diversi componenti del framework ML, garantendo un‚Äôintegrazione senza soluzione di continuit√†. I test sistematici di casi limite, condizioni al contorno e scenari eccezionali aiutano a scoprire guasti e vulnerabilit√† nascosti. Il <a href="https://u-tor.com/topic/regression-vs-integration">‚Äúcontinuous testing‚Äù e i test di regressione</a> come mostrato in <a href="#gaprh7zcofc9"><span class="quarto-xref">Figura&nbsp;<span>17.35</span></span></a> rilevano i guasti introdotti da modifiche al codice o aggiornamenti al framework ML.</p>
<div id="fig-regression-testing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-regression-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/regression_testing.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-regression-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.35: Test di regressione automatizzati. Fonte: <a href="https://u-tor.com/topic/regression-vs-integration">UTOR</a>
</figcaption>
</figure>
</div>
<p><strong>Analisi Statica del Codice e Linting:</strong> L‚Äôutilizzo di strumenti di analisi statica del codice identifica automaticamente potenziali problemi di codifica, come errori di sintassi, variabili non definite o vulnerabilit√† di sicurezza. L‚Äôapplicazione di standard di codifica e best practice tramite strumenti di ‚Äúlinting‚Äù mantiene la qualit√† del codice e riduce la probabilit√† di comuni errori di programmazione. L‚Äôesecuzione di revisioni regolari del codice consente l‚Äôispezione manuale della base di codice, l‚Äôidentificazione di potenziali errori e garantisce l‚Äôaderenza alle linee guida di codifica e ai principi di progettazione.</p>
<p><strong>Monitoraggio e Logging in Fase di Esecuzione:</strong> L‚Äôimplementazione di meccanismi di logging completi cattura informazioni rilevanti durante l‚Äôesecuzione, come dati di input, parametri del modello ed eventi di sistema. Il monitoraggio delle metriche delle prestazioni chiave, dell‚Äôutilizzo delle risorse e dei tassi di errore aiuta a rilevare anomalie, colli di bottiglia delle prestazioni o comportamenti imprevisti. L‚Äôimpiego di controlli di asserzione in fase di esecuzione e invarianti, convalida le ipotesi e rileva violazioni delle condizioni previste durante l‚Äôesecuzione del programma. L‚Äôutilizzo di <a href="https://microsoft.github.io/code-with-engineering-playbook/machine-learning/ml-profiling/">strumenti di profilazione</a> consente di identificare colli di bottiglia nelle prestazioni, memory leak o percorsi di codice inefficienti che potrebbero indicare la presenza di errori software.</p>
<p><strong>Design Pattern a Tolleranza di Errore:</strong> L‚Äôimplementazione di meccanismi di gestione degli errori e delle eccezioni consente una gestione e un ripristino controllato da condizioni eccezionali o errori di runtime. L‚Äôimpiego di meccanismi di ridondanza e failover, come sistemi di backup o calcoli ridondanti, garantisce la disponibilit√† e l‚Äôaffidabilit√† del sistema ML in presenza di errori. La progettazione di architetture modulari e debolmente accoppiate riduce al minimo la propagazione e l‚Äôimpatto dei guasti su diversi componenti del sistema ML. L‚Äôutilizzo di meccanismi di checkpointing e ripristino <span class="citation" data-cites="eisenman2022check">(<a href="../../references.it.html#ref-eisenman2022check" role="doc-biblioref">Eisenman et al. 2022</a>)</span> consente al sistema di riprendere da uno stato stabile noto in caso di guasti o interruzioni.</p>
<div class="no-row-height column-margin column-container"><div id="ref-eisenman2022check" class="csl-entry" role="listitem">
Eisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, e Murali Annavaram. 2022. <span>¬´Check-N-Run: <span>A</span> checkpointing system for training deep learning recommendation models¬ª</span>. In <em>19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)</em>, 929‚Äì43.
</div></div><p><strong>Aggiornamenti e Patch Regolari:</strong> Rimanere aggiornati con le ultime versioni e patch dei framework, delle librerie e delle dipendenze ML offre vantaggi in termini di correzioni di bug, aggiornamenti di sicurezza e miglioramenti delle prestazioni. Il monitoraggio delle note di rilascio, degli avvisi di sicurezza e dei forum della community informa i professionisti su problemi noti, vulnerabilit√† o problemi di compatibilit√† nel framework ML. L‚Äôistituzione di un processo sistematico per testare e convalidare aggiornamenti e patch prima di applicarli ai sistemi di produzione garantisce stabilit√† e compatibilit√†.</p>
<p><strong>Containerizzazione e Isolamento:</strong> Sfruttando le tecnologie di containerizzazione, come <a href="https://www.docker.com">Docker</a> o <a href="https://kubernetes.io">Kubernetes</a>, si incapsulano i componenti ML e le relative dipendenze in ambienti isolati. L‚Äôutilizzo della containerizzazione garantisce ambienti di runtime coerenti e riproducibili nelle fasi di sviluppo, test e produzione, riducendo la probabilit√† di problemi di compatibilit√† o errori specifici dell‚Äôambiente. L‚Äôimpiego di tecniche di isolamento, come ambienti virtuali o sandbox, impedisce che errori o vulnerabilit√† in un componente influiscano su altre parti del sistema ML.</p>
<p><strong>Test Automatizzati e Continuous Integration/Continuous Deployment (CI/CD):</strong> Implementare framework e script di test automatizzati, eseguire suite di test complete e individuare gli errori nelle prime fasi dello sviluppo. L‚Äôintegrazione di test automatizzati nella pipeline CI/CD, come mostrato in <a href="#f14k3aj3u8av"><span class="quarto-xref">Figura&nbsp;<span>17.36</span></span></a>, garantisce che le modifiche al codice siano testate a fondo prima di essere unite o distribuite in produzione. L‚Äôutilizzo di sistemi di monitoraggio continuo e di allerta automatizzati rilevano e notificano a sviluppatori e operatori potenziali guasti o anomalie in tempo reale.</p>
<div id="fig-CI-CD-procedure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-CI-CD-procedure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/CI_CD_procedure.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-CI-CD-procedure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.36: Procedura di Continuous Integration/Continuous Deployment (CI/CD). Fonte: <a href="https://www.geeksforgeeks.org/ci-cd-continuous-integration-and-continuous-delivery/">geeksforgeeks</a>
</figcaption>
</figure>
</div>
<p>L‚Äôadozione di un approccio proattivo e sistematico al rilevamento e alla mitigazione degli errori pu√≤ migliorare significativamente la robustezza, l‚Äôaffidabilit√† e la manutenibilit√† dei sistemi ML. Investendo in pratiche complete di test, monitoraggio e progettazione tollerante agli errori, le organizzazioni possono ridurre al minimo l‚Äôimpatto degli errori software e garantire il regolare funzionamento dei loro sistemi ML negli ambienti di produzione.</p>
<div id="exr-ft" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio&nbsp;17.4: Tolleranza agli Errori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Preparatevi a diventare supereroi che combattono gli errori dell‚ÄôIA! I problemi software possono far deragliare i sistemi di apprendimento automatico, ma in questo Colab impareremo come renderli resilienti. Simuleremo errori software per vedere come l‚ÄôIA pu√≤ ‚Äúrompersi‚Äù, poi esploreremo tecniche per salvare i progressi del modello ML, come i checkpoint in un gioco. Vedremo come addestrare l‚ÄôIA a riprendersi dopo un crash, assicurando che rimanga sulla buona strada. Questo √® fondamentale per creare un‚ÄôIA affidabile e degna di fiducia, soprattutto nelle applicazioni critiche. Quindi preparatevi perch√© questo Colab si collega direttamente al capitolo IA Robusta: passeremo dalla teoria alla risoluzione pratica dei problemi e creeremo sistemi di intelligenza artificiale in grado di gestire l‚Äôimprevisto!</p>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/migrate/fault_tolerance.ipynb#scrollTo=77z2OchJTk0l"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="strumenti-e-framework" class="level2 page-columns page-full" data-number="17.6">
<h2 data-number="17.6" class="anchored" data-anchor-id="strumenti-e-framework"><span class="header-section-number">17.6</span> Strumenti e Framework</h2>
<p>Data l‚Äôimportanza di sviluppare sistemi di IA robusti, negli ultimi anni ricercatori e professionisti hanno sviluppato un‚Äôampia gamma di strumenti e framework per comprendere come i guasti hardware si manifestano e si propagano per avere un impatto sui sistemi ML. Questi strumenti e framework svolgono un ruolo cruciale nella valutazione della resilienza dei sistemi ML ai guasti hardware simulando vari scenari di guasto e analizzandone l‚Äôimpatto sulle prestazioni del sistema. Ci√≤ consente ai progettisti di identificare potenziali vulnerabilit√† e sviluppare strategie di mitigazione efficaci, creando in definitiva sistemi ML pi√π robusti e affidabili in grado di funzionare in sicurezza nonostante i guasti hardware. Questa sezione fornisce una panoramica dei modelli di guasto ampiamente utilizzati nella letteratura e degli strumenti e framework sviluppati per valutare l‚Äôimpatto di tali guasti sui sistemi ML.</p>
<section id="modelli-di-guasto-e-modelli-di-errore" class="level3 page-columns page-full" data-number="17.6.1">
<h3 data-number="17.6.1" class="anchored" data-anchor-id="modelli-di-guasto-e-modelli-di-errore"><span class="header-section-number">17.6.1</span> Modelli di Guasto e Modelli di Errore</h3>
<p>Come discusso in precedenza, i guasti hardware possono manifestarsi in vari modi, tra cui guasti transitori, permanenti e intermittenti. Oltre al tipo di guasto in esame, √® importante anche <em>come</em> si manifesta il guasto. Ad esempio, l‚Äôerrore si verifica in una cella di memoria o durante il calcolo di un‚Äôunit√† funzionale? L‚Äôimpatto √® su un singolo bit o su pi√π bit? L‚Äôerrore si propaga per tutto il percorso e ha un impatto sull‚Äôapplicazione (causando un errore) o viene mascherato rapidamente ed √® considerato benigno? Tutti questi dettagli hanno un impatto su ci√≤ che √® noto come <em>fault model</em> [modello di errore], che svolge un ruolo importante nella simulazione e nella misurazione di ci√≤ che accade a un sistema quando si verifica un errore.</p>
<p>Per studiare e comprendere efficacemente l‚Äôimpatto degli errori hardware sui sistemi ML, √® essenziale comprendere i concetti di ‚Äúfault model‚Äù e ‚Äúerror model‚Äù. Un ‚Äúfault model‚Äù [guasto] descrive come si manifesta un errore hardware nel sistema, mentre un ‚Äúerror model‚Äù [modello di errore] rappresenta come l‚Äôerrore si propaga e influisce sul comportamento del sistema.</p>
<p>I ‚Äúfault model‚Äù possono essere categorizzati in base a varie caratteristiche:</p>
<ul>
<li><p><strong>Durata:</strong> I guasti transitori si verificano brevemente e poi scompaiono, mentre quelli permanenti persistono indefinitamente. I guasti intermittenti si verificano sporadicamente e possono essere difficili da diagnosticare.</p></li>
<li><p><strong>Posizione:</strong> I guasti possono verificarsi in componenti hardware, come celle di memoria, unit√† funzionali o interconnessioni.</p></li>
<li><p><strong>Granularit√†:</strong> I guastipossono interessare un singolo bit (ad esempio, bitflip) o pi√π bit (ad esempio, errori burst) all‚Äôinterno di un componente hardware.</p></li>
</ul>
<p>D‚Äôaltro canto, gli ‚Äúerror model‚Äù descrivono come un guasto si propaga nel sistema e si manifesta come un errore. Un errore pu√≤ causare la deviazione del sistema dal comportamento previsto, portando a risultati errati o persino a guasti del sistema. I modelli di errore possono essere definiti a diversi livelli di astrazione, da quello hardware (ad esempio, bitflip a livello di registro) al livello software (ad esempio, pesi o attivazioni corrotti in un modello ML).</p>
<p>Il ‚Äúfault model‚Äù (o il modello di errore, in genere la terminologia pi√π applicabile per comprendere la robustezza di un sistema ML) svolge un ruolo importante nella simulazione e nella misura di ci√≤ che accade a un sistema quando si verifica un guasto. Il modello scelto informa le ipotesi fatte sul sistema in fase di studio. Ad esempio, un sistema incentrato su errori transitori a bit singolo <span class="citation" data-cites="sangchoolie2017one">(<a href="../../references.it.html#ref-sangchoolie2017one" role="doc-biblioref">Sangchoolie, Pattabiraman, e Karlsson 2017</a>)</span> non sarebbe adatto a comprendere l‚Äôimpatto di errori permanenti di flip multi-bit <span class="citation" data-cites="wilkening2014calculating">(<a href="../../references.it.html#ref-wilkening2014calculating" role="doc-biblioref">Wilkening et al. 2014</a>)</span>, poich√© √® progettato presupponendo un modello completamente diverso.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wilkening2014calculating" class="csl-entry" role="listitem">
Wilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva Gurumurthi, e David R. Kaeli. 2014. <span>¬´Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults¬ª</span>. In <em>2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</em>, 293‚Äì305. IEEE; IEEE. <a href="https://doi.org/10.1109/micro.2014.15">https://doi.org/10.1109/micro.2014.15</a>.
</div></div><p>Inoltre, anche l‚Äôimplementazione di un modello di errore √® una considerazione importante, in particolare per quanto riguarda il punto in cui si dice che si verifichi un errore nello stack di elaborazione. Ad esempio, un modello di flip a bit singolo a livello di registro architetturale differisce da un modello di flip a bit singolo nel peso di un modello a livello di PyTorch. Sebbene entrambi mirino a un modello di errore simile, il primo verrebbe solitamente modellato in un simulatore architetturalmente accurato (come gem5 [binkert2011gem5]), che cattura la propagazione dell‚Äôerrore rispetto al secondo, concentrandosi sulla propagazione del valore attraverso un modello.</p>
<p>Ricerche recenti hanno dimostrato che alcune caratteristiche dei modelli di errore possono mostrare comportamenti simili a diversi livelli di astrazione <span class="citation" data-cites="sangchoolie2017one">(<a href="../../references.it.html#ref-sangchoolie2017one" role="doc-biblioref">Sangchoolie, Pattabiraman, e Karlsson 2017</a>)</span> <span class="citation" data-cites="papadimitriou2021demystifying">(<a href="../../references.it.html#ref-papadimitriou2021demystifying" role="doc-biblioref">Papadimitriou e Gizopoulos 2021</a>)</span>. Ad esempio, gli errori a bit singolo sono generalmente pi√π problematici degli errori a bit multiplo, indipendentemente dal fatto che siano modellati a livello hardware o software. Tuttavia, altre caratteristiche, come il mascheramento degli errori <span class="citation" data-cites="mohanram2003partial">(<a href="../../references.it.html#ref-mohanram2003partial" role="doc-biblioref">Mohanram e Touba 2003</a>)</span> come mostrato in <a href="#kncu0umx706t"><span class="quarto-xref">Figura&nbsp;<span>17.37</span></span></a>, potrebbero non essere sempre catturate accuratamente dai modelli a livello software, poich√© possono nascondere gli effetti di sistema sottostanti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sangchoolie2017one" class="csl-entry" role="listitem">
Sangchoolie, Behrooz, Karthik Pattabiraman, e Johan Karlsson. 2017. <span>¬´One Bit is <span>(Not)</span> Enough: <span>An</span> Empirical Study of the Impact of Single and Multiple Bit-Flip Errors¬ª</span>. In <em>2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 97‚Äì108. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn.2017.30">https://doi.org/10.1109/dsn.2017.30</a>.
</div><div id="ref-papadimitriou2021demystifying" class="csl-entry" role="listitem">
Papadimitriou, George, e Dimitris Gizopoulos. 2021. <span>¬´Demystifying the System Vulnerability Stack: <span>Transient</span> Fault Effects Across the Layers¬ª</span>. In <em>2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</em>, 902‚Äì15. IEEE; IEEE. <a href="https://doi.org/10.1109/isca52012.2021.00075">https://doi.org/10.1109/isca52012.2021.00075</a>.
</div><div id="ref-mohanram2003partial" class="csl-entry" role="listitem">
Mohanram, K., e N. A. Touba. 2003. <span>¬´Partial error masking to reduce soft error failure rate in logic circuits¬ª</span>. In <em>Proceedings. 16th IEEE Symposium on Computer Arithmetic</em>, 433‚Äì40. IEEE; IEEE Comput. Soc. <a href="https://doi.org/10.1109/dftvs.2003.1250141">https://doi.org/10.1109/dftvs.2003.1250141</a>.
</div></div><div id="fig-error-masking" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-error-masking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/error_masking.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-error-masking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.37: Esempio di mascheramento degli errori nei componenti microarchitettonici <span class="citation" data-cites="ko2021characterizing">(<a href="../../references.it.html#ref-ko2021characterizing" role="doc-biblioref">Ko 2021</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-ko2021characterizing" class="csl-entry" role="listitem">
Ko, Yohan. 2021. <span>¬´Characterizing System-Level Masking Effects against Soft Errors¬ª</span>. <em>Electronics</em> 10 (18): 2286. <a href="https://doi.org/10.3390/electronics10182286">https://doi.org/10.3390/electronics10182286</a>.
</div></div></figure>
</div>
<p>Alcuni strumenti, come Fidelity <span class="citation" data-cites="he2020fidelity">(<a href="../../references.it.html#ref-he2020fidelity" role="doc-biblioref">He, Balaprakash, e Li 2020</a>)</span>, mirano a colmare il divario tra modelli di errore a livello hardware e software mappando i pattern tra i due livelli di astrazione <span class="citation" data-cites="cheng2016clear">(<a href="../../references.it.html#ref-cheng2016clear" role="doc-biblioref">Cheng et al. 2016</a>)</span>. Ci√≤ consente una modellazione pi√π accurata dei guasti hardware negli strumenti basati su software, essenziale per lo sviluppo di sistemi ML robusti e affidabili. Gli strumenti a pi√π basso livello in genere rappresentano caratteristiche di propagazione degli errori pi√π accurate, ma devono essere pi√π rapidi nella simulazione di molti errori a causa della natura complessa delle progettazioni dei sistemi hardware. D‚Äôaltro canto, gli strumenti a pi√π alto livello, come quelli implementati in framework ML come PyTorch o TensorFlow, di cui parleremo presto nelle sezioni successive, sono spesso pi√π rapidi ed efficienti per valutare la robustezza dei sistemi ML.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cheng2016clear" class="csl-entry" role="listitem">
Cheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher, Hyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. <span>¬´Clear: uC/u ross u-L/u ayer uE/u xploration for uA/u rchitecting uR/u esilience - Combining hardware and software techniques to tolerate soft errors in processor cores¬ª</span>. In <em>Proceedings of the 53rd Annual Design Automation Conference</em>, 1‚Äì6. ACM. <a href="https://doi.org/10.1145/2897937.2897996">https://doi.org/10.1145/2897937.2897996</a>.
</div></div><p>Nelle sottosezioni seguenti, discuteremo vari metodi e strumenti di iniezione di guasti basati su hardware e software, evidenziandone le capacit√†, le limitazioni e i modelli di guasti ed errori che supportano.</p>
</section>
<section id="injection-hardware-based-di-guasti" class="level3 page-columns page-full" data-number="17.6.2">
<h3 data-number="17.6.2" class="anchored" data-anchor-id="injection-hardware-based-di-guasti"><span class="header-section-number">17.6.2</span> Injection Hardware-based di Guasti</h3>
<p>Uno strumento di ‚Äúiniezione di errori‚Äù √® uno strumento che consente all‚Äôutente di implementare un particolare modello di errore, come un singolo bit flip transitorio durante l‚Äôinferenza <a href="#fig-hardware-errors" class="quarto-xref">Figura&nbsp;<span>17.38</span></a>. La maggior parte degli strumenti di iniezione di errori sono basati su software, poich√© sono pi√π rapidi per gli studi di robustezza ML. Tuttavia, i metodi di iniezione di guasti basati su hardware sono ancora importanti per radicare i modelli di errore ad alto livello, poich√© sono considerati il modo pi√π accurato per studiare l‚Äôimpatto dei guasti sui sistemi ML manipolando direttamente l‚Äôhardware per introdurli. Questi metodi consentono ai ricercatori di osservare il comportamento del sistema in condizioni di guasti reali. In questa sezione vengono descritti in modo pi√π dettagliato sia gli strumenti di iniezione di errori basati su software che quelli basati su hardware.</p>
<div id="fig-hardware-errors" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hardware-errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/hardware_errors.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.38: Gli errori hardware possono verificarsi per una serie di motivi e in momenti e/o posizioni diverse in un sistema, il che pu√≤ essere esplorato quando si studia l‚Äôimpatto degli errori basati sull‚Äôhardware sui sistemi <span class="citation" data-cites="ahmadilivani2024systematic">(<a href="../../references.it.html#ref-ahmadilivani2024systematic" role="doc-biblioref">Ahmadilivani et al. 2024</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-ahmadilivani2024systematic" class="csl-entry" role="listitem">
Ahmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, e Maksim Jenihhin. 2024. <span>¬´A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks¬ª</span>. <em>ACM Comput. Surv.</em> 56 (6): 1‚Äì39. <a href="https://doi.org/10.1145/3638242">https://doi.org/10.1145/3638242</a>.
</div></div></figure>
</div>
<section id="metodi" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="metodi">Metodi</h4>
<p>Due dei metodi di iniezione di guasti basati su hardware pi√π comuni sono quelli basati su FPGA e il test di radiazione o di fascio.</p>
<p><strong>Iniezione di Guasti FPGA-based:</strong> I ‚ÄúField-Programmable Gate Array (FPGA)‚Äù sono circuiti integrati riconfigurabili che possono essere programmati per implementare vari progetti hardware. Nel contesto dell‚Äôiniezione di guasti, gli FPGA offrono elevata precisione e accuratezza, poich√© i ricercatori possono mirare a bit specifici o set di bit all‚Äôinterno dell‚Äôhardware. Modificando la configurazione dell‚ÄôFPGA, i guasti possono essere introdotti in posizioni e tempi specifici durante l‚Äôesecuzione di un modello ML. L‚Äôiniezione di guasti basata su FPGA consente un controllo dettagliato sul ‚Äúfault model‚Äù, consentendo ai ricercatori di studiare l‚Äôimpatto di diversi tipi di guasti, come i flip di bit singoli o gli errori multi-bit. Questo livello di controllo rende l‚Äôiniezione di guasti basata su FPGA uno strumento prezioso per comprendere la resilienza dei sistemi ML ai guasti hardware.</p>
<p><strong>Test di Radiazioni o Fasci:</strong> Il test di radiazioni o fasci <span class="citation" data-cites="velazco2010combining">(<a href="../../references.it.html#ref-velazco2010combining" role="doc-biblioref">Velazco, Foucard, e Peronnard 2010</a>)</span> comporta l‚Äôesposizione dell‚Äôhardware che esegue un modello ML a particelle ad alta energia, come protoni o neutroni, come illustrato in <a href="#5a77jp776dxi"><span class="quarto-xref">Figura&nbsp;<span>17.39</span></span></a>. Queste particelle possono causare bitflip o altri tipi di guasti nell‚Äôhardware, imitando gli effetti di quelli indotti dalle radiazioni nel mondo reale. Il test di fasci √® ampiamente considerato un metodo altamente accurato per misurare il tasso di errore indotto da impatti di particelle su un‚Äôapplicazione in esecuzione. Fornisce una rappresentazione realistica dei guasti in ambienti reali, in particolare in applicazioni esposte ad alti livelli di radiazioni, come sistemi spaziali o esperimenti di fisica delle particelle. Tuttavia, a differenza dell‚Äôiniezione di guasti basata su FPGA, il test di fasci potrebbe essere pi√π preciso nel puntare a bit o componenti specifici all‚Äôinterno dell‚Äôhardware, poich√© potrebbe essere difficile puntare il fascio di particelle a un bit particolare nell‚Äôhardware. Nonostante sia piuttosto costoso dal punto di vista della ricerca, il test del fascio √® una pratica industriale molto apprezzata per l‚Äôaffidabilit√†.</p>
<div class="no-row-height column-margin column-container"><div id="ref-velazco2010combining" class="csl-entry" role="listitem">
Velazco, Raoul, Gilles Foucard, e Paul Peronnard. 2010. <span>¬´Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in <span>SRAM</span>-Based <span>FPGAs</span>¬ª</span>. <em>IEEE Trans. Nucl. Sci.</em> 57 (6): 3500‚Äì3505. <a href="https://doi.org/10.1109/tns.2010.2087355">https://doi.org/10.1109/tns.2010.2087355</a>.
</div></div><p><img src="./images/png/image15.png" class="img-fluid"></p>
<div id="fig-beam-testing" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-beam-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/image14.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-beam-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.39: Configurazione del test di radiazione per componenti semiconduttori <span class="citation" data-cites="lee2022design">(<a href="../../references.it.html#ref-lee2022design" role="doc-biblioref">Lee et al. 2022</a>)</span> Fonte: <a href="https://jdinstruments.net/tester-capabilities-radiation-test/">JD Instrument</a>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-lee2022design" class="csl-entry" role="listitem">
Lee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang, e Seongik Cho. 2022. <span>¬´Design of Radiation-Tolerant High-Speed Signal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear Explosion¬ª</span>. <em>Electronics</em> 11 (18): 2970. <a href="https://doi.org/10.3390/electronics11182970">https://doi.org/10.3390/electronics11182970</a>.
</div></div></figure>
</div>
</section>
<section id="limitazioni" class="level4">
<h4 class="anchored" data-anchor-id="limitazioni">Limitazioni</h4>
<p>Nonostante la loro elevata accuratezza, i metodi di iniezione di guasti basati su hardware presentano diverse limitazioni che possono ostacolarne l‚Äôadozione diffusa:</p>
<p><strong>Costo:</strong> L‚Äôiniezione di guasti e il test del fascio basati su FPGA richiedono hardware e strutture specializzate, la cui configurazione e manutenzione possono essere costose. Il costo di questi metodi pu√≤ rappresentare un ostacolo significativo per ricercatori e organizzazioni con risorse limitate.</p>
<p><strong>Scalabilit√†:</strong> I metodi basati su hardware sono generalmente pi√π lenti e meno scalabili rispetto ai metodi basati su software. L‚Äôiniezione di guasti e la raccolta di dati sull‚Äôhardware possono richiedere tempo, limitando il numero di esperimenti eseguiti in un determinato lasso di tempo. Ci√≤ pu√≤ essere particolarmente impegnativo quando si studia la resilienza di sistemi ML su larga scala o si conducono analisi statistiche che richiedono molti esperimenti di iniezione di guasti.</p>
<p><strong>Flessibilit√†:</strong> I metodi basati su hardware potrebbero non essere flessibili quanto quelli basati su software in termini di gamma di modelli di guasto e modelli di errore che possono supportare. Modificare la configurazione hardware o l‚Äôimpostazione sperimentale per adattarsi a diversi modelli di errore pu√≤ essere pi√π impegnativo e richiedere pi√π tempo rispetto ai metodi basati su software.</p>
<p>Nonostante queste limitazioni, i metodi di iniezione di errori basati su hardware rimangono strumenti essenziali per convalidare l‚Äôaccuratezza dei metodi basati su software e per studiare l‚Äôimpatto degli errori sui sistemi ML in contesti realistici. Combinando metodi basati su hardware e basati su software, i ricercatori possono acquisire una comprensione pi√π completa della resilienza dei sistemi ML ai guasti hardware e sviluppare strategie di mitigazione efficaci.</p>
</section>
</section>
<section id="strumenti-di-injection-di-guasti-software-based" class="level3 page-columns page-full" data-number="17.6.3">
<h3 data-number="17.6.3" class="anchored" data-anchor-id="strumenti-di-injection-di-guasti-software-based"><span class="header-section-number">17.6.3</span> Strumenti di Injection di Guasti Software-based</h3>
<p>Con il rapido sviluppo di framework ML negli ultimi anni, gli strumenti di iniezione di guasti basati su software hanno guadagnato popolarit√† nello studio della resilienza dei sistemi ML ai guasti hardware. Questi strumenti simulano gli effetti dei guasti hardware modificando la rappresentazione software del modello ML o il grafo computazionale sottostante. L‚Äôascesa di framework ML come TensorFlow, PyTorch e Keras ha facilitato lo sviluppo di strumenti di iniezione di guasti che sono strettamente integrati con questi framework, rendendo pi√π facile per i ricercatori condurre esperimenti di iniezione di guasti e analizzare i risultati.</p>
<section id="vantaggi-e-compromessi" class="level5">
<h5 class="anchored" data-anchor-id="vantaggi-e-compromessi">Vantaggi e Compromessi</h5>
<p>Gli strumenti di iniezione di guasti basati su software offrono diversi vantaggi rispetto a quelli basati su hardware:</p>
<p><strong>Velocit√†:</strong> Gli strumenti basati su software sono generalmente pi√π rapidi dei metodi basati su hardware, poich√© non richiedono la modifica dell‚Äôhardware fisico o la configurazione di apparecchiature specializzate. Ci√≤ consente ai ricercatori di condurre pi√π esperimenti di iniezione di guasti in tempi pi√π brevi, consentendo analisi pi√π complete della resilienza dei sistemi ML.</p>
<p><strong>Flessibilit√†:</strong> Gli strumenti basati su software sono pi√π flessibili di quelli basati su hardware in termini di gamma di modelli di guasti ed errori che possono supportare. I ricercatori possono facilmente modificare l‚Äôimplementazione software dello strumento di iniezione di guasti per adattarsi a diversi modelli di guasti o per indirizzare componenti specifici del sistema ML.</p>
<p><strong>Accessibilit√†:</strong> Gli strumenti basati su software sono pi√π accessibili dei metodi basati su hardware, poich√© non richiedono hardware o strutture specializzate. Ci√≤ semplifica per ricercatori e professionisti condurre esperimenti di iniezione di guasti e studiare la resilienza dei sistemi ML, anche con risorse limitate.</p>
</section>
<section id="limitazioni-1" class="level5">
<h5 class="anchored" data-anchor-id="limitazioni-1">Limitazioni</h5>
<p>Gli strumenti di iniezione di guasti basati su software presentano anche alcune limitazioni rispetto ai metodi basati su hardware:</p>
<p><strong>Precisione:</strong> Gli strumenti basati su software potrebbero non sempre catturano l‚Äôintera gamma di effetti che i guasti hardware possono avere sul sistema. Poich√© questi strumenti operano a un livello di astrazione pi√π elevato, potrebbero dover recuperare alcune delle interazioni hardware di basso livello e dei meccanismi di propagazione degli errori che possono influire sul comportamento del sistema ML.</p>
<p><strong>Fedelt√†:</strong> Gli strumenti basati su software potrebbero fornire un livello di fedelt√† diverso rispetto ai metodi basati su hardware in termini di rappresentazione delle condizioni di guasto del mondo reale. L‚Äôaccuratezza dei risultati ottenuti dagli esperimenti di iniezione di guasti basati su software potrebbe dipendere da quanto il modello software si avvicini al comportamento hardware effettivo.</p>
<div id="fig-mavfi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mavfi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/mavfi.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mavfi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.40: Confronto delle tecniche a livelli di astrazione. Fonte: <a href="https://ieeexplore.ieee.org/abstract/document/10315202">MAVFI</a>
</figcaption>
</figure>
</div>
</section>
<section id="tipi-di-strumenti-di-iniezione-di-guasti" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="tipi-di-strumenti-di-iniezione-di-guasti">Tipi di Strumenti di Iniezione di Guasti</h5>
<p>Gli strumenti di iniezione di guasti basati su software possono essere categorizzati in base ai loro framework di destinazione o casi d‚Äôuso. Qui, discuteremo alcuni degli strumenti pi√π popolari in ciascuna categoria:</p>
<p>Ares <span class="citation" data-cites="reagen2018ares">(<a href="../../references.it.html#ref-reagen2018ares" role="doc-biblioref">Reagen et al. 2018</a>)</span>, uno strumento di iniezione di guasti inizialmente sviluppato per il framework Keras nel 2018, √® emerso come uno dei primi strumenti per studiare l‚Äôimpatto dei guasti hardware sulle reti deep neural network (DNN) nel contesto della crescente popolarit√† dei framework ML a met√†-fine anni 2010. Lo strumento √® stato convalidato rispetto a un acceleratore DNN implementato in silicio, dimostrando la sua efficacia nella modellazione dei guasti hardware. Ares fornisce uno studio completo sull‚Äôimpatto dei guasti hardware sia nei pesi che nei valori di attivazione, caratterizzando gli effetti dei flip di bit singoli e dei bit-error rate (BER) sulle strutture hardware. Successivamente, il framework Ares √® stato esteso per supportare l‚Äôecosistema PyTorch, consentendo ai ricercatori di investigare i guasti hardware in un contesto pi√π moderno e ampliando ulteriormente la sua utilit√† sul campo.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reagen2018ares" class="csl-entry" role="listitem">
Reagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, e Gu-Yeon Wei. 2018. <span>¬´Ares: <span>A</span> framework for quantifying the resilience of deep neural networks¬ª</span>. In <em>2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)</em>, 1‚Äì6. IEEE. <a href="https://doi.org/10.1109/dac.2018.8465834">https://doi.org/10.1109/dac.2018.8465834</a>.
</div></div><div id="fig-phantom-objects" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-phantom-objects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/phantom_objects.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-phantom-objects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.41: I bitflip hardware nei carichi di lavoro ML possono causare oggetti fantasma e classificazioni errate, che possono essere erroneamente utilizzati a valle da sistemi pi√π grandi, come nella guida autonoma. Quella mostrata sopra √® una versione corretta e difettosa della stessa immagine che utilizza il framework di iniezione PyTorchFI.
</figcaption>
</figure>
</div>
<p>PyTorchFI <span class="citation" data-cites="mahmoud2020pytorchfi">(<a href="../../references.it.html#ref-mahmoud2020pytorchfi" role="doc-biblioref">Mahmoud et al. 2020</a>)</span>, uno strumento di iniezione di guasti progettato specificamente per il framework PyTorch, √® stato sviluppato nel 2020 in collaborazione con Nvidia Research. Consente l‚Äôiniezione di guasti nei pesi, nelle attivazioni e nei gradienti dei modelli PyTorch, supportando un‚Äôampia gamma di modelli di guasti. Sfruttando le capacit√† di accelerazione GPU di PyTorch, PyTorchFI fornisce un‚Äôimplementazione rapida ed efficiente per condurre esperimenti di iniezione di guasti su sistemi ML su larga scala, come mostrato in <a href="#txkz61sj1mj4"><span class="quarto-xref">Figura&nbsp;<span>17.41</span></span></a>. La velocit√† e la facilit√† d‚Äôuso dello strumento hanno portato a un‚Äôadozione diffusa nella comunit√†, con conseguenti molteplici progetti guidati dagli sviluppatori, come PyTorchALFI di Intel xColabs, che si concentra sulla sicurezza negli ambienti automobilistici. Gli strumenti successivi incentrati su PyTorch per l‚Äôiniezione di guasti includono Dr.&nbsp;DNA di Meta <span class="citation" data-cites="ma2024dr">(<a href="../../references.it.html#ref-ma2024dr" role="doc-biblioref">Ma et al. 2024</a>)</span> (che facilita ulteriormente il modello di programmazione Pythonic per una maggiore facilit√† d‚Äôuso) e il framework GoldenEye <span class="citation" data-cites="mahmoud2022dsn">(<a href="../../references.it.html#ref-mahmoud2022dsn" role="doc-biblioref">Mahmoud et al. 2022</a>)</span>, che incorpora nuovi tipi di dati numerici (come AdaptivFloat <span class="citation" data-cites="tambe2020algorithm">(<a href="../../references.it.html#ref-tambe2020algorithm" role="doc-biblioref">Tambe et al. 2020</a>)</span> e <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">BlockFloat</a> nel contesto di bit flip hardware.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mahmoud2020pytorchfi" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez Vicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, e Siva Kumar Sastry Hari. 2020. <span>¬´<span>PyTorchFI:</span> <span>A</span> Runtime Perturbation Tool for <span>DNNs</span>¬ª</span>. In <em>2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)</em>, 25‚Äì31. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn-w50199.2020.00014">https://doi.org/10.1109/dsn-w50199.2020.00014</a>.
</div><div id="ref-ma2024dr" class="csl-entry" role="listitem">
Ma, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore, Sriram Sankar, e Xun Jiao. 2024. <span>¬´<span>Dr.</span> <span>DNA:</span> <span>Combating</span> Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations¬ª</span>. In <em>Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</em>, 239‚Äì52. ACM. <a href="https://doi.org/10.1145/3620666.3651349">https://doi.org/10.1145/3620666.3651349</a>.
</div><div id="ref-mahmoud2022dsn" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, e Gu-Yeon Wei. 2022. <span>¬´<span>GoldenEye:</span> <span>A</span> Platform for Evaluating Emerging Numerical Data Formats in <span>DNN</span> Accelerators¬ª</span>. In <em>2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 206‚Äì14. IEEE. <a href="https://doi.org/10.1109/dsn53405.2022.00031">https://doi.org/10.1109/dsn53405.2022.00031</a>.
</div><div id="ref-tambe2020algorithm" class="csl-entry" role="listitem">
Tambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, e Gu-Yeon Wei. 2020. <span>¬´Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference¬ª</span>. In <em>2020 57th ACM/IEEE Design Automation Conference (DAC)</em>, 1‚Äì6. IEEE; IEEE. <a href="https://doi.org/10.1109/dac18072.2020.9218516">https://doi.org/10.1109/dac18072.2020.9218516</a>.
</div><div id="ref-chen2020tensorfi" class="csl-entry" role="listitem">
Chen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2020. <span>¬´<span>TensorFI:</span> <span>A</span> Flexible Fault Injection Framework for <span>TensorFlow</span> Applications¬ª</span>. In <em>2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)</em>, 426‚Äì35. IEEE; IEEE. <a href="https://doi.org/10.1109/issre5003.2020.00047">https://doi.org/10.1109/issre5003.2020.00047</a>.
</div><div id="ref-chen2019sc" class="csl-entry" role="listitem">
Chen, Zitao, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2019. <span>¬´<span>iBinFI/i</span>: an efficient fault injector for safety-critical machine learning systems¬ª</span>. In <em>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>. SC ‚Äô19. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3295500.3356177">https://doi.org/10.1145/3295500.3356177</a>.
</div></div><p>TensorFI <span class="citation" data-cites="chen2020tensorfi">(<a href="../../references.it.html#ref-chen2020tensorfi" role="doc-biblioref">Chen et al. 2020</a>)</span>, o TensorFlow Fault Injector, √® uno strumento di iniezione di guasti sviluppato specificamente per il framework TensorFlow. Analogo ad Ares e PyTorchFI, TensorFI √® considerato lo strumento all‚Äôavanguardia per gli studi di robustezza ML nell‚Äôecosistema TensorFlow. Consente ai ricercatori di iniettare guasti nel grafo computazionale di Modelli TensorFlow e studia il loro impatto sulle prestazioni del modello, supportando un‚Äôampia gamma di modelli di errore. Uno dei principali vantaggi di TensorFI √® la sua capacit√† di valutare la resilienza di vari modelli ML, non solo DNN. Ulteriori progressi, come BinFi <span class="citation" data-cites="chen2019sc">(<a href="../../references.it.html#ref-chen2019sc" role="doc-biblioref">Chen et al. 2019</a>)</span>, forniscono un meccanismo per accelerare gli esperimenti di iniezione di errori concentrandosi sui bit ‚Äúimportanti‚Äù nel sistema, accelerando il processo di analisi della robustezza ML e dando priorit√† ai componenti critici di un modello.</p>
<p>NVBitFI <span class="citation" data-cites="tsai2021nvbitfi">(<a href="../../references.it.html#ref-tsai2021nvbitfi" role="doc-biblioref">T. Tsai et al. 2021</a>)</span>, uno strumento di iniezione di errori generico sviluppato da Nvidia per le sue piattaforme GPU, opera a un pi√π basso livello rispetto a strumenti specifici del framework come Ares, PyTorchFI e TensorFlow. Mentre questi strumenti si concentrano su varie piattaforme di deep learning per implementare ed eseguire analisi di robustezza, NVBitFI mira al codice di assemblaggio hardware sottostante per l‚Äôiniezione di guasti. Ci√≤ consente ai ricercatori di iniettare guasti in qualsiasi applicazione in esecuzione su GPU Nvidia, rendendolo uno strumento versatile per studiare la resilienza dei sistemi ML e di altre applicazioni accelerate da GPU. Consentendo agli utenti di iniettare errori a livello di architettura, NVBitFI fornisce un modello di guasto pi√π generico che non √® limitato ai soli modelli ML. Poich√© i sistemi GPU di Nvidia sono comunemente utilizzati in molti sistemi basati su ML, NVBitFI √® uno strumento prezioso per un‚Äôanalisi completa dell‚Äôiniezione di guasti in varie applicazioni.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tsai2021nvbitfi" class="csl-entry" role="listitem">
Tsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa, e Stephen W. Keckler. 2021. <span>¬´<span>NVBitFI:</span> <span>Dynamic</span> Fault Injection for <span>GPUs</span>¬ª</span>. In <em>2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 284‚Äì91. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn48987.2021.00041">https://doi.org/10.1109/dsn48987.2021.00041</a>.
</div></div><section id="esempi-specifici-di-dominio" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="esempi-specifici-di-dominio">Esempi specifici di dominio</h6>
<p>Sono stati sviluppati strumenti di iniezione di guasti specifici per dominio per affrontare le sfide e i requisiti unici di vari domini applicativi ML, come veicoli autonomi e robotica. Questa sezione evidenzia tre strumenti di iniezione di guasti specifici per dominio: DriveFI e PyTorchALFI per veicoli autonomi e MAVFI per ‚Äúuncrewed aerial vehicles (UAV)‚Äù [veicoli aerei senza equipaggio]. Questi strumenti consentono ai ricercatori di iniettare guasti hardware nei sottosistemi di percezione, controllo e altri sistemi complessi, consentendo loro di studiare l‚Äôimpatto dei guasti sulle prestazioni e sulla sicurezza del sistema. Lo sviluppo di questi strumenti di iniezione di guasti basati su software ha notevolmente ampliato le capacit√† della comunit√† ML di sviluppare sistemi pi√π robusti e affidabili in grado di funzionare in modo sicuro ed efficace in presenza di guasti hardware.</p>
<p>DriveFI <span class="citation" data-cites="jha2019ml">(<a href="../../references.it.html#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span> √® uno strumento di iniezione di guasti progettato per veicoli autonomi. Consente l‚Äôiniezione di guasti hardware nelle pipeline di percezione e controllo dei sistemi di veicoli autonomi, consentendo ai ricercatori di studiare l‚Äôimpatto di questi guasti sulle prestazioni e sulla sicurezza del sistema. DriveFI √® stato integrato con piattaforme di guida autonoma standard del settore, come Nvidia DriveAV e Baidu Apollo, rendendolo uno strumento prezioso per valutare la resilienza dei sistemi di veicoli autonomi.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jha2019ml" class="csl-entry" role="listitem">
Jha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, e Ravishankar K. Iyer. 2019. <span>¬´<span>ML</span>-Based Fault Injection for Autonomous Vehicles: <span>A</span> Case for <span>Bayesian</span> Fault Injection¬ª</span>. In <em>2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 112‚Äì24. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn.2019.00025">https://doi.org/10.1109/dsn.2019.00025</a>.
</div><div id="ref-grafe2023large" class="csl-entry" role="listitem">
Gr√§fe, Ralf, Qutub Syed Sha, Florian Geissler, e Michael Paulitsch. 2023. <span>¬´Large-Scale Application of Fault Injection into <span>PyTorch</span> Models -an Extension to <span>PyTorchFI</span> for Validation Efficiency¬ª</span>. In <em>2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S)</em>, 56‚Äì62. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn-s58398.2023.00025">https://doi.org/10.1109/dsn-s58398.2023.00025</a>.
</div></div><p>PyTorchALFI <span class="citation" data-cites="grafe2023large">(<a href="../../references.it.html#ref-grafe2023large" role="doc-biblioref">Gr√§fe et al. 2023</a>)</span> √® un‚Äôestensione di PyTorchFI sviluppata da Intel xColabs per il dominio dei veicoli autonomi. Si basa sulle capacit√† di inserimento di guasti di PyTorchFI. Aggiunge funzionalit√† specificamente studiate per valutare la resilienza dei sistemi di veicoli autonomi, come la capacit√† di inserire guasti nei dati della telecamera e del sensore LiDAR.</p>
<p>MAVFI <span class="citation" data-cites="hsiao2023mavfi">(<a href="../../references.it.html#ref-hsiao2023mavfi" role="doc-biblioref">Hsiao et al. 2023</a>)</span> √® uno strumento di inserimento di guasti progettato per il dominio della robotica, in particolare per i veicoli aerei senza equipaggio (UAV). MAVFI √® basato sul framework Robot Operating System (ROS) e consente ai ricercatori di inserire guasti nei vari componenti di un sistema UAV, come sensori, attuatori e algoritmi di controllo. Valutando l‚Äôimpatto di questi guasti sulle prestazioni e sulla stabilit√† del UAV, i ricercatori possono sviluppare sistemi UAV pi√π resilienti e tolleranti ai guasti.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hsiao2023mavfi" class="csl-entry" role="listitem">
Hsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. <span>¬´<span>MAVFI:</span> <span>An</span> End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles¬ª</span>. In <em>2023 Design, Automation &amp;amp; Test in Europe Conference &amp;amp; Exhibition (DATE)</em>, 1‚Äì6. IEEE; IEEE. <a href="https://doi.org/10.23919/date56975.2023.10137246">https://doi.org/10.23919/date56975.2023.10137246</a>.
</div></div><p>Lo sviluppo di strumenti di iniezione di guasti basati su software ha notevolmente ampliato le capacit√† di ricercatori e professionisti di studiare la resilienza dei sistemi ML ai guasti hardware. Sfruttando la velocit√†, la flessibilit√† e l‚Äôaccessibilit√† di questi strumenti, la comunit√† ML pu√≤ sviluppare sistemi pi√π robusti e affidabili in grado di funzionare in modo sicuro ed efficace in presenza di guasti hardware.</p>
</section>
</section>
</section>
<section id="colmare-il-divario-tra-modelli-di-errore-hardware-e-software" class="level3 page-columns page-full" data-number="17.6.4">
<h3 data-number="17.6.4" class="anchored" data-anchor-id="colmare-il-divario-tra-modelli-di-errore-hardware-e-software"><span class="header-section-number">17.6.4</span> Colmare il Divario tra Modelli di Errore Hardware e Software</h3>
<p>Sebbene gli strumenti di iniezione di guasti basati su software offrano molti vantaggi in termini di velocit√†, flessibilit√† e accessibilit√†, potrebbero non sempre catturare accuratamente l‚Äôintera gamma di effetti che i guasti hardware possono avere sul sistema. Questo perch√© gli strumenti basati su software operano a un livello di astrazione pi√π alto rispetto ai metodi basati su hardware e potrebbero non rilevare alcune delle interazioni hardware di basso livello e dei meccanismi di propagazione degli errori che possono influire sul comportamento del sistema ML.</p>
<p>Come illustra <span class="citation" data-cites="bolchini2022fast">Bolchini et al. (<a href="../../references.it.html#ref-bolchini2022fast" role="doc-biblioref">2023</a>)</span> nel suo lavoro, gli errori hardware possono manifestarsi in complessi pattern di distribuzione spaziale che sono difficili da replicare completamente con la sola iniezione di guasti basata su software. Identificano quattro pattern distinti: (a) singolo punto, in cui il guasto corrompe un singolo valore in una feature map; (b) stessa riga, in cui il guasto corrompe una riga parziale o intera in una singola feature map; (c) bullet wake, in cui il guasto corrompe la stessa posizione su pi√π feature map; e (d) shatter glass, che combina gli effetti dei pattern della stessa riga e bullet wake, come mostrato in <a href="#fig-hardware-errors-bolchini" class="quarto-xref">Figura&nbsp;<span>17.42</span></a>. Questi intricati meccanismi di propagazione degli errori evidenziano la necessit√† di tecniche di iniezione di guasti consapevoli dell‚Äôhardware per valutare accuratamente la resilienza dei sistemi ML.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-hardware-errors-bolchini" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hardware-errors-bolchini-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/hardware_errors_Bolchini.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-errors-bolchini-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;17.42: Gli errori hardware possono manifestarsi in modi diversi a livello software, come classificato da Bolchini et al. <span class="citation" data-cites="bolchini2022fast">(<a href="../../references.it.html#ref-bolchini2022fast" role="doc-biblioref">Bolchini et al. 2023</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-bolchini2022fast" class="csl-entry" role="listitem">
Bolchini, Cristiana, Luca Cassano, Antonio Miele, e Alessandro Toschi. 2023. <span>¬´Fast and Accurate Error Simulation for <span>CNNs</span> Against Soft Errors¬ª</span>. <em>IEEE Trans. Comput.</em> 72 (4): 984‚Äì97. <a href="https://doi.org/10.1109/tc.2022.3184274">https://doi.org/10.1109/tc.2022.3184274</a>.
</div></div></figure>
</div>
<p>I ricercatori hanno sviluppato strumenti per affrontare questo problema colmando il divario tra modelli di errore hardware di basso livello e modelli di errore software di livello superiore. Uno di questi strumenti √® Fidelity, progettato per mappare i pattern tra guasti a livello hardware e le loro manifestazioni a livello software.</p>
<section id="fidelity-colmare-il-gap" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="fidelity-colmare-il-gap">Fidelity: Colmare il Gap</h4>
<p>Fidelity <span class="citation" data-cites="he2020fidelity">(<a href="../../references.it.html#ref-he2020fidelity" role="doc-biblioref">He, Balaprakash, e Li 2020</a>)</span> √® uno strumento per modellare accuratamente i guasti hardware negli esperimenti di iniezione di guasti basati su software. Ci√≤ avviene studiando attentamente la relazione tra i guasti a livello hardware e il loro impatto sulla rappresentazione software del sistema ML.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2020fidelity" class="csl-entry" role="listitem">
He, Yi, Prasanna Balaprakash, e Yanjing Li. 2020. <span>¬´<span>FIdelity:</span> <span>Efficient</span> Resilience Analysis Framework for Deep Learning Accelerators¬ª</span>. In <em>2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</em>, 270‚Äì81. IEEE; IEEE. <a href="https://doi.org/10.1109/micro50266.2020.00033">https://doi.org/10.1109/micro50266.2020.00033</a>.
</div></div><p>Le intuizioni chiave alla base di Fidelity sono:</p>
<ul>
<li><p><strong>Propagazione dei Guasti:</strong> Fidelity modella il modo in cui gli errori si propagano attraverso l‚Äôhardware e si manifestano come errori nello stato del sistema visibili al software. Comprendendo questi pattern di propagazione, Fidelity pu√≤ simulare con maggiore accuratezza gli effetti dei guasti hardware negli esperimenti basati sul software.</p></li>
<li><p><strong>Equivalenza dei Guasti:</strong> Fidelity identifica classi equivalenti di guasti hardware che producono errori simili a livello software. Ci√≤ consente ai ricercatori di progettare modelli di guasti basati sul software che siano rappresentativi dei guasti hardware sottostanti senza la necessit√† di modellare ogni possibile guasto hardware singolarmente.</p></li>
<li><p><strong>Approccio a Strati:</strong> Fidelity impiega un approccio a strati alla modellazione dei guasti, in cui gli effetti dei guasti hardware vengono propagati attraverso pi√π livelli di astrazione, dall‚Äôhardware al livello software. Questo approccio garantisce che i modelli di guasti basati sul software siano basati sul comportamento effettivo dell‚Äôhardware.</p></li>
</ul>
<p>Incorporando queste informazioni, Fidelity consente agli strumenti di iniezione di guasti basati su software di catturare con precisione gli effetti dei guasti hardware sui sistemi ML. Ci√≤ √® particolarmente importante per le applicazioni critiche per la sicurezza, in cui la resilienza del sistema ai guasti hardware √® fondamentale.</p>
</section>
<section id="limportanza-di-catturare-il-vero-comportamento-hardware" class="level4">
<h4 class="anchored" data-anchor-id="limportanza-di-catturare-il-vero-comportamento-hardware">L‚ÄôImportanza di Catturare il Vero Comportamento Hardware</h4>
<p>Catturare il vero comportamento hardware negli strumenti di iniezione di guasti basati su software √® fondamentale per diversi motivi:</p>
<ul>
<li><p><strong>Precisione:</strong> Modellando con precisione gli effetti dei guasti hardware, gli strumenti basati su software possono fornire informazioni pi√π affidabili sulla resilienza dei sistemi ML. Ci√≤ √® essenziale per progettare e convalidare sistemi tolleranti ai guasti che possono funzionare in modo sicuro ed efficace in presenza di guasti hardware.</p></li>
<li><p><strong>Riproducibilit√†:</strong> Quando gli strumenti basati su software catturano con precisione il comportamento hardware, gli esperimenti di iniezione di guasti diventano pi√π riproducibili su diverse piattaforme e ambienti. Ci√≤ √® importante per lo studio scientifico della resilienza del sistema ML, poich√© consente ai ricercatori di confrontare e convalidare i risultati su diversi studi e implementazioni.</p></li>
<li><p><strong>Efficienza:</strong> Gli strumenti basati su software che catturano il vero comportamento dell‚Äôhardware possono essere pi√π efficienti nei loro esperimenti di iniezione di guasti concentrandosi sui modelli di guasti pi√π rappresentativi e impattanti. Ci√≤ consente ai ricercatori di coprire una gamma pi√π ampia di scenari di guasti e configurazioni di sistema con risorse computazionali limitate.</p></li>
<li><p><strong>Strategie di Mitigazione:</strong> Comprendere come i guasti hardware si manifestano a livello software √® fondamentale per sviluppare strategie di mitigazione efficaci. Catturando con precisione il comportamento dell‚Äôhardware, gli strumenti di iniezione di guasti basati su software possono aiutare i ricercatori a identificare i componenti pi√π vulnerabili del sistema ML e progettare tecniche di rafforzamento mirate per migliorare la resilienza.</p></li>
</ul>
<p>Strumenti come Fidelity sono essenziali per far progredire lo stato dell‚Äôarte nella ricerca sulla resilienza del sistema ML. Questi strumenti consentono ai ricercatori di condurre esperimenti di iniezione di guasti pi√π accurati, riproducibili ed efficienti colmando il divario tra modelli di errore hardware e software. Man mano che la complessit√† e la criticit√† dei sistemi ML continuano a crescere, l‚Äôimportanza di catturare il vero comportamento hardware negli strumenti di iniezione di guasti basati su software diventer√† sempre pi√π evidente.</p>
<p>La ricerca in corso in quest‚Äôarea cerca di perfezionare la mappatura tra modelli di errore hardware e software e di sviluppare nuove tecniche per simulare in modo efficiente i guasti hardware negli esperimenti basati su software. Man mano che questi strumenti maturano, forniranno alla comunit√† ML mezzi sempre pi√π potenti e accessibili per studiare e migliorare la resilienza dei sistemi ML ai guasti hardware.</p>
</section>
</section>
</section>
<section id="conclusione" class="level2" data-number="17.7">
<h2 data-number="17.7" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">17.7</span> Conclusione</h2>
<p>Sviluppare un‚ÄôIA solida e resiliente √® fondamentale man mano che i sistemi di apprendimento automatico diventano sempre pi√π integrati in applicazioni critiche per la sicurezza e in ambienti reali. Questo capitolo ha esplorato le principali sfide alla robustezza dell‚ÄôIA derivanti da guasti hardware, attacchi dannosi, cambiamenti di distribuzione e bug software.</p>
<p>Alcune delle conclusioni principali includono quanto segue:</p>
<ul>
<li><p><strong>Guasti Hardware:</strong> Guasti transitori, permanenti e intermittenti nei componenti hardware possono corrompere i calcoli e degradare le prestazioni dei modelli di apprendimento automatico se non vengono rilevati e mitigati correttamente. Tecniche come ridondanza, correzione degli errori e progetti fault-tolerant svolgono un ruolo cruciale nella creazione di sistemi ML resilienti in grado di resistere ai guasti hardware.</p></li>
<li><p><strong>Robustezza del Modello:</strong> Gli attori malintenzionati possono sfruttare le vulnerabilit√† nei modelli ML tramite attacchi avversari e avvelenamento dei dati, mirando a indurre classificazioni errate mirate, distorcere il comportamento appreso del modello o compromettere l‚Äôintegrit√† e l‚Äôaffidabilit√† del sistema. Inoltre, possono verificarsi ‚Äúdistribution shift‚Äù quando la distribuzione dei dati riscontrata durante l‚Äôimplementazione differisce da quella osservata durante il training, con conseguente degrado delle prestazioni. L‚Äôimplementazione di misure difensive, tra cui training avversario, rilevamento delle anomalie, architetture di modelli robuste e tecniche come adattamento del dominio, apprendimento per trasferimento e apprendimento continuo, √® essenziale per proteggersi da queste sfide e garantire l‚Äôaffidabilit√† e la generalizzazione del modello in ambienti dinamici.</p></li>
<li><p><strong>Errori Software:</strong> Gli errori nei framework ML, nelle librerie e negli stack software possono propagarsi, degradare le prestazioni e introdurre vulnerabilit√† di sicurezza. Test rigorosi, monitoraggio del runtime e adozione di ‚Äúdesign pattern‚Äù tolleranti agli errori sono essenziali per la creazione di un‚Äôinfrastruttura software robusta che supporti sistemi ML affidabili.</p></li>
</ul>
<p>Poich√© i sistemi ML affrontano attivit√† sempre pi√π complesse con conseguenze nel mondo reale, dare priorit√† alla resilienza diventa fondamentale. Gli strumenti e i framework discussi in questo capitolo, tra cui tecniche di ‚Äúfault injection‚Äù [iniezione di guasti], metodi di analisi degli errori e framework di valutazione della robustezza, forniscono ai professionisti i mezzi per testare a fondo e rafforzare i propri sistemi ML contro varie modalit√† di errore e condizioni avverse.</p>
<p>Andando avanti, la resilienza deve essere un obiettivo centrale durante l‚Äôintero ciclo di vita dello sviluppo dell‚ÄôIA, dalla raccolta dei dati e dall‚Äôaddestramento del modello all‚Äôimplementazione e al monitoraggio. Affrontando in modo proattivo le molteplici sfide alla robustezza, possiamo sviluppare sistemi di apprendimento automatico affidabili e sicuri, in grado di affrontare le complessit√† e le incertezze degli ambienti del mondo reale.</p>
<p>La ricerca futura sul ML robusto dovrebbe continuare a far progredire le tecniche per rilevare e mitigare guasti, attacchi e ‚Äúshift‚Äù delle distribuzioni. Inoltre, esplorare nuovi paradigmi per lo sviluppo di architetture IA intrinsecamente resilienti, come sistemi auto-riparanti o meccanismi a prova di errore, sar√† fondamentale per spingere i confini della robustezza dell‚ÄôIA. Dando priorit√† alla resilienza e investendo nello sviluppo di sistemi di IA robusti, possiamo liberare il pieno potenziale delle tecnologie di apprendimento automatico, garantendone al contempo un‚Äôimplementazione sicura, affidabile e responsabile in applicazioni del mondo reale. Mentre l‚ÄôIA continua a plasmare il nostro futuro, la creazione di sistemi resilienti in grado di resistere alle sfide del mondo reale sar√† un fattore determinante per il successo e l‚Äôimpatto sociale di questa tecnologia trasformativa.</p>
</section>
<section id="sec-robust-ai-resource" class="level2" data-number="17.8">
<h2 data-number="17.8" class="anchored" data-anchor-id="sec-robust-ai-resource"><span class="header-section-number">17.8</span> Risorse</h2>
<p>Ecco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slide
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Queste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Per rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.</p>
<ul>
<li><p><a href="#exr-ad" class="quarto-xref">Esercizio&nbsp;<span>17.1</span></a></p></li>
<li><p><a href="#exr-aa" class="quarto-xref">Esercizio&nbsp;<span>17.2</span></a></p></li>
<li><p><a href="#exr-pa" class="quarto-xref">Esercizio&nbsp;<span>17.3</span></a></p></li>
<li><p><a href="#exr-ft" class="quarto-xref">Esercizio&nbsp;<span>17.4</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laboratori
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Oltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l‚Äôesperienza di apprendimento.</p>
<ul>
<li><em>Prossimamente.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/sustainable_ai/sustainable_ai.it.html" class="pagination-link" aria-label="IA Sostenibile">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">IA Sostenibile</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/generative_ai/generative_ai.it.html" class="pagination-link" aria-label="IA Generativa">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">IA Generativa</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Scritto, modificato e curato dal Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/robust_ai/robust_ai.it.qmd" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/robust_ai/robust_ai.it.qmd" class="toc-action"><i class="bi empty"></i>Mostra il codice</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro √® stato creato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>