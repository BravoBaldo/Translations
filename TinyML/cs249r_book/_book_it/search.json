[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Systems",
    "section": "",
    "text": "Prefazione\nBenvenuti in Machine Learning Systems. Questo libro è la porta d’accesso al mondo frenetico dei sistemi di intelligenza artificiale. È un’estensione del corso CS249r alla Harvard University.\n\nAbbiamo creato questo libro open source come sforzo collaborativo per riunire spunti di studenti, professionisti e la più ampia comunità di professionisti dell’IA. Il nostro obiettivo è sviluppare una guida completa che esplori le complessità dei sistemi di intelligenza artificiale e le loro numerose applicazioni.\n\n“Se vuoi andare veloce, vai da solo. Se vuoi andare lontano, vai insieme ad altre persone”. – Proverbio africano\n\nQuesto non è un libro statico; è un documento vivo e pulsante. Lo stiamo rendendo open source e lo aggiorniamo costantemente per soddisfare le esigenze in continua evoluzione di questo campo dinamico. Contiene un ricco mix di conoscenze specialistiche che guideranno attraverso la complessa interazione tra algoritmi all’avanguardia e i principi fondamentali che li fanno funzionare. Stiamo preparando il terreno per il prossimo grande balzo nell’innovazione dell’IA.\n\n\nPerché Abbiamo Scritto Questo Libro\nViviamo in un’epoca in cui la tecnologia è in continua evoluzione. La collaborazione aperta e la condivisione delle conoscenze sono gli elementi costitutivi della vera innovazione. Questo è lo spirito alla base di questo lavoro. Andiamo oltre il tradizionale modello di libro di testo per creare un hub di conoscenza vivo, in modo che possiamo tutti condividere e imparare gli uni dagli altri.\nIl libro si concentra sui principi e sui casi di studio dei sistemi di IA, con l’obiettivo di fornire una comprensione approfondita che aiuterà a navigare nel panorama in continua evoluzione dei sistemi di IA. Mantenendolo “open source”, non stiamo solo rendendo accessibile l’apprendimento, ma stiamo anche invitando nuove idee e miglioramenti continui. In breve, stiamo costruendo una comunità in cui la conoscenza è libera di crescere e illuminare la strada verso la tecnologia AI globale.\n\n\nCosa c’è da Sapere\nPer immergersi in questo libro, non si dev’essere un esperto di AI. Tutto ciò di cui c’è bisogno è una conoscenza di base dei concetti di informatica e la curiosità di esplorare su come funzionano i sistemi AI. È qui che avviene l’innovazione e una conoscenza di base della programmazione e delle strutture dati sarà la bussola.\n\n\nConvenzioni del Libro\nPer i dettagli sulle convenzioni utilizzate in questo libro, consultare la sezione Convenzioni.\n\n\nDichiarazione di Trasparenza dei Contenuti\nQuesto libro è un progetto guidato dalla comunità, con contenuti generati da numerosi collaboratori nel tempo. Il processo di creazione dei contenuti potrebbe aver coinvolto vari strumenti di editing, tra cui la tecnologia AI generativa. In qualità di autore principale, editore e curatore, il Prof. Vijay Janapa Reddi mantiene la supervisione umana e la supervisione editoriale per garantire che il contenuto sia accurato e pertinente. Tuttavia, nessuno è perfetto, quindi potrebbero comunque esserci delle inesattezze. Apprezziamo molto i feedback e invitiamo a fornire correzioni e suggerimenti. Questo approccio collaborativo è fondamentale per migliorare e mantenere la qualità del contenuto e rendere le informazioni accessibili a livello globale.\n\n\nPer dare una mano\nSe si è interessati a contribuire, le linee guida si trovano qui.\n\n\nContatti\nCi sono domande o feedback? Si è liberi di inviare una e-mail al Prof. Vijay Janapa Reddi direttamente, oppure si può avviare un thread di discussione su GitHub.\n\n\nCollaboratori\nUn grande ringraziamento a tutti coloro che hanno contribuito a rendere questo libro quello che è! L’elenco completo dei singoli collaboratori è qui e ulteriori dettagli sullo stile GitHub qui. Benvenuti come collaboratori!",
    "crumbs": [
      "PREFAZIONE",
      "Prefazione"
    ]
  },
  {
    "objectID": "contents/dedication.it.html",
    "href": "contents/dedication.it.html",
    "title": "Dedica",
    "section": "",
    "text": "Questo libro è una testimonianza dell’idea che, nell’immensità della tecnologia e dell’innovazione, non sono sempre i sistemi più grandi, ma quelli più piccoli, a poter cambiare il mondo.",
    "crumbs": [
      "PREFAZIONE",
      "Dedica"
    ]
  },
  {
    "objectID": "contents/acknowledgements/acknowledgements.it.html",
    "href": "contents/acknowledgements/acknowledgements.it.html",
    "title": "Ringraziamenti",
    "section": "",
    "text": "Singoli Collaboratori\nEstendiamo la nostra sincera gratitudine alla comunità open source di studenti, insegnanti e contributori. Che abbiate contribuito con un’intera sezione, una singola frase o semplicemente corretto un errore di battitura, i vostri sforzi hanno migliorato questo libro. Apprezziamo profondamente il tempo, la competenza e l’impegno di tutti. Questo libro è tanto vostro quanto nostro.\nUn ringraziamento speciale va al professor Vijay Janapa Reddi, la cui convinzione nel potere trasformativo delle comunità open source e la cui guida inestimabile sono state la nostra luce guida fin dall’inizio.\nDobbiamo molto anche al team di GitHub e di Quarto. Avete rivoluzionato il modo in cui le persone collaborano e questo libro è una testimonianza di ciò che si può ottenere quando vengono rimosse le barriere alla cooperazione globale.",
    "crumbs": [
      "PREFAZIONE",
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/acknowledgements/acknowledgements.it.html#agenzie-e-aziende-finanziatrici",
    "href": "contents/acknowledgements/acknowledgements.it.html#agenzie-e-aziende-finanziatrici",
    "title": "Ringraziamenti",
    "section": "Agenzie e Aziende Finanziatrici",
    "text": "Agenzie e Aziende Finanziatrici\nSiamo immensamente grati per il generoso supporto delle varie agenzie e aziende finanziatrici che hanno supportato gli assistenti didattici coinvolti in questo lavoro. Le organizzazioni elencate di seguito hanno svolto un ruolo cruciale nel dare vita a questo progetto con i loro contributi.",
    "crumbs": [
      "PREFAZIONE",
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/acknowledgements/acknowledgements.it.html#ai-nostri-lettori",
    "href": "contents/acknowledgements/acknowledgements.it.html#ai-nostri-lettori",
    "title": "Ringraziamenti",
    "section": "Ai Nostri Lettori",
    "text": "Ai Nostri Lettori\nA tutti coloro che acquisteranno questo libro, vogliamo ringraziarvi! L’abbiamo scritto pensando a voi, sperando di provocare riflessioni, ispirare domande e forse anche accendere una scintilla di ispirazione. Dopo tutto, che senso ha scrivere se nessuno legge?",
    "crumbs": [
      "PREFAZIONE",
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/contributors.it.html",
    "href": "contents/contributors.it.html",
    "title": "Collaboratori e Ringraziamenti",
    "section": "",
    "text": "Estendiamo i nostri più sinceri ringraziamenti al gruppo eterogeneo di persone che hanno generosamente contribuito con la loro competenza, intuizioni, tempo e supporto per migliorare sia il contenuto che la base di codice di questo progetto. Ciò include non solo coloro che hanno contribuito direttamente tramite codice e scrittura, ma anche coloro che hanno aiutato identificando problemi, fornendo feedback e offrendo suggerimenti. Di seguito c’è l’elenco di tutti i collaboratori. Per contribuire a questo progetto, visitare la nostra pagina GitHub per maggiori informazioni.\n\n\n\n\n\n\n\n\nVijay Janapa Reddi\n\n\nIkechukwu Uchendu\n\n\nNaeem Khoshnevis\n\n\njasonjabbour\n\n\nDouwe den Blanken\n\n\n\n\nshanzehbatool\n\n\nMarcelo Rovai\n\n\nElias Nuwara\n\n\nkai4avaya\n\n\nJared Ping\n\n\n\n\nMatthew Stewart\n\n\nItai Shapira\n\n\nMaximilian Lam\n\n\nJayson Lin\n\n\nSophia Cho\n\n\n\n\nJeffrey Ma\n\n\nAndrea\n\n\nAlex Rodriguez\n\n\nKorneel Van den Berghe\n\n\nZishen Wan\n\n\n\n\nColby Banbury\n\n\nSara Khosravi\n\n\nDivya Amirtharaj\n\n\nAbdulrahman Mahmoud\n\n\nSrivatsan Krishnan\n\n\n\n\nmarin-llobet\n\n\nAghyad Deeb\n\n\noishib\n\n\nAditi Raju\n\n\nELSuitorHarvard\n\n\n\n\nJared Ni\n\n\nEmil Njor\n\n\nHaoran Qiu\n\n\nMichael Schnebly\n\n\nHenry Bae\n\n\n\n\nJae-Won Chung\n\n\nMark Mazumder\n\n\nYu-Shun Hsiao\n\n\nEmeka Ezike\n\n\nEura Nofshin\n\n\n\n\nJennifer Zhou\n\n\nMarco Zennaro\n\n\nShvetank Prakash\n\n\nAndrew Bass\n\n\nPong Trairatvorakul\n\n\n\n\nAllen-Kuang\n\n\nBruno Scaglione\n\n\nSercan Aygün\n\n\nGauri Jain\n\n\nFin Amin\n\n\n\n\ngnodipac886\n\n\nAlex Oesterling\n\n\nabigailswallow\n\n\nYang Zhou\n\n\nEmmanuel Rassou\n\n\n\n\nhappyappledog\n\n\nJessica Quaye\n\n\nJason Yik\n\n\nSonia Murthy\n\n\nShreya Johri\n\n\n\n\nThe Random DIY\n\n\nCostin-Andrei Oncescu\n\n\nBaldassarre Cesarano\n\n\nAnnie Laurie Cook\n\n\nVijay Edupuganti\n\n\n\n\nJothi Ramaswamy\n\n\nBatur Arslan\n\n\nCurren Iyer\n\n\nyanjingl\n\n\na-saraf\n\n\n\n\nsonghan\n\n\nZishen",
    "crumbs": [
      "PREFAZIONE",
      "Collaboratori e Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/copyright.it.html",
    "href": "contents/copyright.it.html",
    "title": "Copyright",
    "section": "",
    "text": "Questo libro è open source e sviluppato in modo collaborativo tramite GitHub. Salvo diversa indicazione, questo lavoro è concesso in licenza con Creative Commons Attribuzione-Non commerciale-Condividi allo stesso modo 4.0 Internazionale (CC BY-NC-SA 4.0 CC BY-SA 4.0). Il testo completo della licenza si trova qui.\nI collaboratori di questo progetto hanno dedicato i loro contributi al pubblico dominio o con la stessa licenza aperta del progetto originale. Sebbene i contributi siano collaborativi, ogni collaboratore mantiene il copyright sui rispettivi contributi.\nPer i dettagli sulla paternità, i contributi e come contribuire, consultare il repository del progetto su GitHub.\nTutti i marchi e i marchi registrati menzionati in questo libro sono di proprietà dei rispettivi proprietari.\nLe informazioni fornite in questo libro sono ritenute accurate e affidabili. Tuttavia, gli autori, i curatori e gli editori non possono essere ritenuti responsabili per eventuali danni causati o presumibilmente causati, direttamente o indirettamente, dalle informazioni contenute nel presente libro.",
    "crumbs": [
      "PREFAZIONE",
      "Copyright"
    ]
  },
  {
    "objectID": "contents/about.it.html",
    "href": "contents/about.it.html",
    "title": "Informazioni sul Libro",
    "section": "",
    "text": "Panoramica\nBenvenuti a questo progetto collaborativo avviato dalla classe CS249r Machine Learning Systems presso l’Università di Harvard. Il nostro obiettivo è rendere questo libro una risorsa della comunità che aiuti educatori e studenti a comprendere i sistemi di ML. Il libro verrà aggiornato regolarmente per riflettere nuove intuizioni sui sistemi ML e metodi di insegnamento efficaci.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/about.it.html#argomenti-esplorati",
    "href": "contents/about.it.html#argomenti-esplorati",
    "title": "Informazioni sul Libro",
    "section": "Argomenti Esplorati",
    "text": "Argomenti Esplorati\nQuesto libro offre una panoramica completa di vari aspetti dei sistemi di apprendimento automatico. Copriamo l’intero flusso di lavoro dei sistemi ML end-to-end, iniziando con concetti fondamentali e procedendo attraverso l’ingegneria dei dati, i framework AI e il training dei modelli.\nSi imparerà ad ottimizzare i modelli per l’efficienza, a distribuire l’AI su varie piattaforme hardware e a confrontare le prestazioni. Il libro esplora anche argomenti più avanzati come la sicurezza, la privacy, l’intelligenza artificiale responsabile e sostenibile, l’IA solida e generativa e l’impatto sociale dell’IA. Alla fine, si avranno solide basi e approfondimenti pratici sia sulle dimensioni tecniche che etiche dell’apprendimento automatico.\nCi auguriamo che una volta terminato questo libro si abbia una conoscenza di base dell’apprendimento automatico e delle sue applicazioni. Si impareranno anche le implementazioni reali dei sistemi di apprendimento automatico e si acquisirà esperienza pratica tramite laboratori e compiti basati su progetti.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/about.it.html#chi-dovrebbe-leggerlo",
    "href": "contents/about.it.html#chi-dovrebbe-leggerlo",
    "title": "Informazioni sul Libro",
    "section": "Chi Dovrebbe Leggerlo",
    "text": "Chi Dovrebbe Leggerlo\nQuesto libro è pensato per chi è alle prime armi con l’entusiasmante campo dei sistemi di apprendimento automatico. Inizia con le basi dell’apprendimento automatico e passa ad argomenti più avanzati rilevanti per la comunità ML e aree di ricerca più ampie. Il libro è particolarmente utile per:\n\nStudenti di Informatica e Ingegneria Elettrica: Questo libro è una risorsa utile per gli studenti di informatica e di ingegneria elettrica. Li introduce alle tecniche utilizzate nei sistemi ML, preparandoli alle sfide del mondo reale nell’apprendimento automatico.\nIngegneri di Sistema: Per gli ingegneri di vari settori, questo libro funge da guida ai sistemi ML, aiutandoli a creare applicazioni intelligenti, in particolare su piattaforme con risorse limitate.\nRicercatori e Accademici: Coloro che sono coinvolti nella ricerca su apprendimento automatico, visione artificiale ed elaborazione del segnale potrebbero trovare questo libro interessante. Fa luce sulle sfide uniche dell’esecuzione di algoritmi di apprendimento automatico su diverse piattaforme.\nProfessionisti del Settore: Se si lavora in settori come IoT, robotica, tecnologia indossabile o dispositivi intelligenti, questo libro fornirà le conoscenze necessarie per aggiungere funzionalità di apprendimento automatico ai prodotti.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/about.it.html#principali-risultati-dellapprendimento",
    "href": "contents/about.it.html#principali-risultati-dellapprendimento",
    "title": "Informazioni sul Libro",
    "section": "Principali Risultati dell’Apprendimento",
    "text": "Principali Risultati dell’Apprendimento\nI lettori acquisiranno competenze nel training e nell’implementazione di modelli di reti neurali profonde su diverse piattaforme, oltre a comprendere le sfide più ampie coinvolte nella loro progettazione, sviluppo e implementazione. Nello specifico, si imparerà:\n\nConcetti Fondamentali nel Machine Learning [apprendimento automatico]\nFondamenti dei sistemi di Intelligenza Artificiale\nPiattaforme Hardware Adatte all’Implementazione dell’Intelligenza Artificiale\nTecniche per Modelli di Addestramento per Diversi Sistemi di Intelligenza Artificiale\nStrategie per l’Ottimizzazione dei Modelli di Intelligenza Artificiale\nApplicazioni Reali dei Sistemi di Intelligenza Artificiale\nSfide Attuali e Tendenze Future nei Sistemi di Intelligenza Artificiale\n\nIl nostro obiettivo è rendere questo libro una risorsa per chiunque sia interessato a sviluppare applicazioni intelligenti su vari sistemi. Dopo aver completato il libro, si sarà ben equipaggiati per progettare e implementare progetti abilitati all’apprendimento automatico.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/about.it.html#prerequisiti-per-i-lettori",
    "href": "contents/about.it.html#prerequisiti-per-i-lettori",
    "title": "Informazioni sul Libro",
    "section": "Prerequisiti per i Lettori",
    "text": "Prerequisiti per i Lettori\n\nCompetenze di programmazione di base: Consigliamo di avere una certa esperienza di programmazione, idealmente in Python. Una conoscenza delle variabili, tipi di dati e strutture di controllo faciliterà l’interazione col libro.\nAlcune Conoscenze di Machine Learning: Sebbene non sia obbligatorio, una conoscenza di base dei concetti di apprendimento automatico aiuterà ad assorbire il materiale più facilmente. Se si è nuovi nel campo, il libro fornisce sufficienti informazioni di base per mettersi al passo.\nConoscenza di Base dei Sistemi: Si consiglia un livello di conoscenza di base dei sistemi a livello universitario junior o senior. Sarà utile comprendere l’architettura di sistema, i sistemi operativi e le reti di base.\nProgrammazione Python (Facoltativo): Se si ha familiarità con Python, si troverà più facile interagire con le sezioni di codifica del libro. Conoscere librerie come NumPy, scikit-learn e TensorFlow sarà particolarmente utile.\nVoglia di Imparare: Il libro è progettato per essere accessibile a un vasto pubblico, con diversi livelli di competenza tecnica. La volontà di sfidare se stessi e di impegnarsi in esercizi pratici aiuterà a trarne il massimo vantaggio.\nDisponibilità delle Risorse: Per gli aspetti pratici, ci sarà bisogno di un computer con Python e le librerie pertinenti installate. L’accesso facoltativo a schede di sviluppo o hardware specifico sarà utile anche per sperimentare la distribuzione del modello di apprendimento automatico.\n\nSoddisfacendo questi prerequisiti, si sarà ben posizionati per approfondire la comprensione dei sistemi di apprendimento automatico, impegnarsi in esercizi di codifica e persino implementare applicazioni pratiche su vari dispositivi.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html",
    "href": "contents/introduction/introduction.it.html",
    "title": "1  Introduzione",
    "section": "",
    "text": "1.1 Panoramica\nAll’inizio degli anni ’90, Mark Weiser, un pioniere dell’informatica, ha introdotto il mondo a un concetto rivoluzionario che avrebbe cambiato per sempre il modo in cui interagiamo con la tecnologia. Tutto ciò è stato sintetizzato nell’articolo da lui scritto su “Il computer per il 21° secolo” (Figura 1.1). Immaginava un futuro in cui l’informatica sarebbe stata perfettamente integrata nei nostri ambienti, diventando una parte invisibile e integrante della vita quotidiana. Questa visione, che lui chiamava “ubiquitous computing” [informatica ovunque], prometteva un mondo in cui la tecnologia ci avrebbe servito senza richiedere la nostra costante attenzione o interazione. Facciamo un salto in avanti fino a oggi, e ci troviamo sul punto di realizzare la visione di Weiser, grazie all’avvento e alla proliferazione dei sistemi di machine learning [apprendimento automatico].\nNella visione dell’ubiquitous computing (Weiser 1991), l’integrazione dei processori negli oggetti di uso quotidiano è solo un aspetto di un più ampio cambiamento di paradigma. La vera essenza di questa visione risiede nella creazione di un ambiente intelligente in grado di anticipare le nostre esigenze e agire per nostro conto, migliorando le nostre esperienze senza richiedere comandi espliciti. Per raggiungere questo livello di intelligenza pervasiva, è fondamentale sviluppare e distribuire sistemi di machine learning che coprano l’intero ecosistema, dal cloud all’edge e persino ai più piccoli dispositivi IoT.\nDistribuendo le capacità di apprendimento automatico nel continuum di elaborazione, possiamo sfruttare i punti di forza di ogni livello mitigandone al contempo i limiti. Il cloud, con le sue vaste risorse di elaborazione e capacità di archiviazione, è ideale per addestrare modelli complessi su grandi set di dati ed eseguire attività che richiedono molte risorse. I dispositivi edge, come gateway e smartphone, possono elaborare i dati localmente, consentendo tempi di risposta più rapidi, una migliore privacy e requisiti di larghezza di banda ridotti. Infine, i dispositivi IoT più piccoli, dotati di capacità di apprendimento automatico, possono prendere decisioni rapide in base ai dati dei sensori, consentendo sistemi altamente reattivi ed efficienti.\nQuesta intelligenza distribuita è particolarmente cruciale per le applicazioni che richiedono elaborazione in tempo reale, come veicoli autonomi, automazione industriale e assistenza sanitaria smart [intelligente]. Elaborando i dati al livello più appropriato del continuum informatico, possiamo garantire che le decisioni vengano prese in modo rapido e accurato, senza fare affidamento su una comunicazione costante con un server centrale.\nLa migrazione dell’intelligenza di apprendimento automatico nell’ecosistema consente inoltre esperienze più personalizzate e consapevoli del contesto. Imparando dal comportamento e dalle preferenze degli utenti all’edge, i dispositivi possono adattarsi alle esigenze individuali senza compromettere la privacy. Questa intelligenza localizzata può quindi essere aggregata e perfezionata nel cloud, creando un ciclo di feedback che migliora costantemente il sistema complessivo.\nTuttavia, l’implementazione di sistemi di apprendimento automatico nel continuum informatico presenta diverse sfide. Garantire l’interoperabilità e l’integrazione senza soluzione di continuità di questi sistemi richiede protocolli e interfacce standardizzati. È inoltre necessario affrontare i problemi di sicurezza e privacy, poiché la distribuzione dell’intelligenza su più livelli aumenta la superficie di attacco e il potenziale di violazioni dei dati.\nInoltre, le diverse capacità computazionali e i vincoli energetici dei dispositivi a diversi livelli del continuum informatico richiedono lo sviluppo di modelli di apprendimento automatico efficienti e adattabili. Tecniche come la compressione del modello, il “federated learning” [apprendimento federato] e il transfer learning [apprendimento tramite trasferimento] possono aiutare ad affrontare queste sfide, consentendo l’implementazione dell’intelligenza su un’ampia gamma di dispositivi.\nMentre ci avviciniamo alla realizzazione della visione di Weiser dell’ubiquitous computing, lo sviluppo e l’implementazione di sistemi di apprendimento automatico nell’intero ecosistema saranno fondamentali. Sfruttando i punti di forza di ogni layer [livello] del continuum informatico, possiamo creare un ambiente intelligente che si integra perfettamente con la nostra vita quotidiana, anticipando le nostre esigenze e migliorando le nostre esperienze in modi che un tempo erano inimmaginabili. Mentre continuiamo a spingere i confini di ciò che è possibile con l’apprendimento automatico distribuito, ci avviciniamo sempre di più a un futuro in cui la tecnologia diventa una parte invisibile ma integrante del nostro mondo.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#panoramica",
    "href": "contents/introduction/introduction.it.html#panoramica",
    "title": "1  Introduzione",
    "section": "",
    "text": "Figura 1.1: Ubiqutous computing.\n\n\n\n\n\nWeiser, Mark. 1991. «The Computer for the 21st Century». Sci. Am. 265 (3): 94–104. https://doi.org/10.1038/scientificamerican0991-94.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#cosa-cè-nel-libro",
    "href": "contents/introduction/introduction.it.html#cosa-cè-nel-libro",
    "title": "1  Introduzione",
    "section": "1.2 Cosa c’è nel Libro",
    "text": "1.2 Cosa c’è nel Libro\nIn questo libro, esploreremo le basi tecniche dei sistemi di apprendimento automatico onnipresenti, le sfide della creazione e distribuzione di questi sistemi nel continuum informatico e la vasta gamma di applicazioni che consentono. Un aspetto unico di questo libro è la sua funzione di canale verso opere accademiche fondamentali e documenti di ricerca accademica, mirati ad arricchire la comprensione del lettore e incoraggiare un’esplorazione più approfondita dell’argomento. Questo approccio cerca di colmare il divario tra materiali pedagogici e tendenze di ricerca all’avanguardia, offrendo una guida completa che è al passo con l’evoluzione del campo dell’apprendimento automatico applicato.\nPer migliorare l’esperienza di apprendimento, abbiamo incluso una varietà di materiali supplementari. In tutto il libro, si troveranno slide che riassumono i concetti chiave, video che forniscono spiegazioni e dimostrazioni approfondite, esercizi che rafforzano la comprensione ed esercizi pratici che offrono esperienza pratica con gli strumenti e le tecniche discussi. Queste risorse aggiuntive sono progettate per soddisfare diversi stili di apprendimento e contribuire ad acquisire una comprensione più profonda e pratica dell’argomento.\nIniziamo con i fondamenti, introducendo concetti chiave nei sistemi e nell’apprendimento automatico e fornendo un avvio al deep learning. Poi guidiamo attraverso il flusso di lavoro dell’IA, dall’ingegneria dei dati alla selezione dei framework di IA giusti. La sezione sul training copre le diverse tecniche di training dell’IA efficienti, ottimizzazioni dei modelli e accelerazione dell’IA tramite hardware specializzato. Successivamente si affronta il deployment [distribuzione], con capitoli sul benchmarking dell’IA, l’apprendimento distribuito e le operazioni di ML. Argomenti avanzati come sicurezza, privacy, IA responsabile, IA sostenibile, IA robusta e IA generativa vengono quindi esplorati in profondità. Il libro si conclude evidenziando l’impatto positivo dell’IA e il suo potenziale per il bene.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#come-orientarsi-in-questo-libro",
    "href": "contents/introduction/introduction.it.html#come-orientarsi-in-questo-libro",
    "title": "1  Introduzione",
    "section": "1.3 Come Orientarsi in Questo Libro",
    "text": "1.3 Come Orientarsi in Questo Libro\nPer ottenere il massimo da questo libro, consigliamo un approccio di apprendimento strutturato che sfrutti le varie risorse fornite. Ogni capitolo include slide, video, esercizi e laboratori per soddisfare diversi stili di apprendimento e rafforzare la comprensione. Inoltre, un bot tutor AI (SocratiQ AI) è prontamente disponibile per guidare attraverso i contenuti e fornire assistenza personalizzata.\n\nI Fondamenti (Capitoli 1-3): Si inizia costruendo una solida base con i primi capitoli, che forniscono un’introduzione all’intelligenza artificiale embedded e trattano argomenti fondamentali come sistemi embedded e deep learning.\nFlusso di Lavoro (Capitoli 4-6): Con questa base, si passa ai capitoli incentrati sugli aspetti pratici del processo di creazione del modello AI come flussi di lavoro, ingegneria dei dati e framework.\nTraining (Capitoli 7-10): Questi capitoli offrono approfondimenti su come addestrare efficacemente i modelli AI, comprese tecniche per efficienza, ottimizzazioni e accelerazione.\nDeployment (Capitoli 11-13): Si esamina come distribuire l’IA sui dispositivi e monitorarne l’operatività tramite metodi come benchmarking, on-device learning e MLOps.\nArgomenti Avanzati (Capitoli 14-18): Si esaminano criticamente argomenti come sicurezza, privacy, etica, sostenibilità, robustezza e IA generativa.\nImpatto Sociale (Capitolo 19): Esplora le applicazioni positive e il potenziale dell’IA per il bene della società.\nConclusione (Capitolo 20): Riflessioni sui principali risultati e sulle direzioni future nell’IA embedded.\n\nSebbene il libro sia progettato per un apprendimento progressivo, incoraggiamo un approccio di apprendimento interconnesso che consente di navigare tra i capitoli in base ai propri interessi e alle proprie esigenze. In tutto il libro si trovano casi di studio ed esercizi pratici che aiuteranno a mettere in relazione la teoria con le applicazioni del mondo reale. Consigliamo inoltre di partecipare a forum e gruppi per partecipare a discussioni, discutere concetti e condividere approfondimenti con altri studenti. Rivedere regolarmente i capitoli può aiutare a rafforzare l’apprendimento e offrire nuove prospettive sui concetti trattati. Adottando questo approccio strutturato ma flessibile e interagendo attivamente con i contenuti e la community, si farà un’esperienza di apprendimento appagante e arricchente che massimizza la comprensione.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#suddivisione-dei-capitoli",
    "href": "contents/introduction/introduction.it.html#suddivisione-dei-capitoli",
    "title": "1  Introduzione",
    "section": "1.4 Suddivisione dei Capitoli",
    "text": "1.4 Suddivisione dei Capitoli\nEcco uno sguardo più da vicino a cosa tratta ogni capitolo. Abbiamo strutturato il libro in sei sezioni principali: Nozioni Fondamentali, Flusso di lavoro, Training, Deployment, Argomenti avanzati e Impatto. Queste sezioni riflettono da vicino i componenti principali di una tipica pipeline di machine learning, dalla comprensione dei concetti di base al la deploy e alla manutenzione dei sistemi di intelligenza artificiale in applicazioni del mondo reale. Organizzando il contenuto in questo modo, miriamo a fornire una progressione logica che rispecchi il processo effettivo di sviluppo e implementazione di soluzioni di intelligenza artificiale embedded.\n\n1.4.1 Nozioni Fondamentali\nNella sezione Nozioni Fondamentali, poniamo le basi per comprendere l’intelligenza artificiale embedded. Introduciamo concetti chiave, forniamo una panoramica dei sistemi di apprendimento automatico e approfondiamo i principi e gli algoritmi di deep learning che alimentano le applicazioni di intelligenza artificiale nei sistemi embedded. Questa sezione fornisce le conoscenze essenziali necessarie per comprendere i capitoli successivi.\n\nIntroduzione: Questo capitolo prepara il terreno, fornendo una panoramica dell’intelligenza artificiale embedded e gettando le basi per i capitoli successivi.\nSistemi di ML: Introduciamo le basi dei sistemi di machine learning [apprendimento automatico], le piattaforme in cui gli algoritmi di intelligenza artificiale sono ampiamente applicati.\nAvvio al Deep Learning: Questo capitolo offre un’introduzione completa agli algoritmi e ai principi alla base delle applicazioni AI nei sistemi embedded.\n\n\n\n1.4.2 Workflow\nLa sezione Workflow [Flusso di lavoro] guida attraverso gli aspetti pratici della creazione di modelli AI. Analizziamo il flusso di lavoro AI, discutiamo le “best practice” di data engineering e passiamo in rassegna i framework AI più diffusi. Alla fine di questa sezione, si avrà una chiara comprensione dei passaggi coinvolti nello sviluppo di applicazioni AI competenti e degli strumenti disponibili per semplificare il processo.\n\nWorkflow IA: Questo capitolo analizza il flusso di lavoro di apprendimento automatico, offrendo approfondimenti sui passaggi che portano ad applicazioni AI competenti.\nIngegneria dei Dati: Ci concentriamo sull’importanza dei dati nei sistemi di IA, discutendo su come gestire e organizzare efficacemente i dati.\nFramework di IA: Questo capitolo esamina diversi framework per lo sviluppo di modelli di apprendimento automatico, guidando nella scelta di quello più adatto ai propri progetti.\n\n\n\n1.4.3 Training\nNella sezione Training, esploriamo tecniche per il training efficiente e affidabile di modelli di IA. Trattiamo strategie per raggiungere efficienza, ottimizzazioni dei modelli e il ruolo dell’hardware specializzato nell’accelerazione IA. Questa sezione fornisce le conoscenze per sviluppare modelli ad alte prestazioni che integrabili senza problemi nei sistemi embedded.\n\nTraining IA: Questo capitolo approfondisce il training [addestramento] dei modelli, esplorando tecniche per sviluppare modelli efficienti e affidabili.\nIA Efficiente: Qui, discutiamo strategie per raggiungere l’efficienza nelle applicazioni di IA, dall’ottimizzazione delle risorse computazionali al miglioramento delle prestazioni.\nOttimizzazioni dei Modelli: Esploriamo vari modi per ottimizzare i modelli di IA per un’integrazione senza soluzione di continuità nei sistemi embedded.\nAccelerazione IA: Discutiamo il ruolo dell’hardware specializzato nel migliorare le prestazioni dei sistemi IA embedded.\n\n\n\n1.4.4 Deployment\nLa sezione Deployment [distribuzione] si concentra sulle sfide e sulle soluzioni per l’implementazione di modelli AI su dispositivi embedded. Discutiamo metodi di benchmarking per valutare le prestazioni del sistema AI, tecniche per l’apprendimento “on-device” per migliorare l’efficienza e la privacy e i processi coinvolti nelle operazioni di ML. Questa sezione fornisce le competenze per implementare e mantenere in modo efficace le funzionalità di IA nei sistemi embedded.\n\nBenchmark dell’IA: Questo capitolo si concentra su come valutare i sistemi di IA tramite metodi di benchmarking sistematici.\nApprendimento On-Device: Esploriamo tecniche per l’apprendimento localizzato, che migliora sia l’efficienza che la privacy.\nOperazioni di ML: Questo capitolo esamina i processi coinvolti nell’integrazione, nel monitoraggio e nella manutenzione senza soluzione di continuità delle funzionalità di IA nei sistemi embedded.\n\n\n\n1.4.5 Argomenti Avanzati\nNella sezione Argomenti avanzati, studieremo le problematiche critiche che circondano l’intelligenza artificiale embedded. Affrontiamo le preoccupazioni relative a privacy e sicurezza, esploriamo i principi etici dell’intelligenza artificiale responsabile, discutiamo strategie per uno sviluppo sostenibile dell’intelligenza artificiale, esaminiamo tecniche per la creazione di modelli di intelligenza artificiale solidi e introduciamo l’entusiasmante campo dell’intelligenza artificiale generativa. Questa sezione amplia la comprensione del complesso panorama dell’intelligenza artificiale embedded e prepara ad affrontarne le sfide.\n\nSicurezza e Privacy: Man mano che l’intelligenza artificiale diventa sempre più onnipresente, questo capitolo affronta gli aspetti cruciali della privacy e della sicurezza nei sistemi di intelligenza artificiale embedded.\nIA Responsabile: Discutiamo i principi etici che guidano l’uso responsabile dell’intelligenza artificiale, concentrandoci sulla correttezza, responsabilità e trasparenza.\nIA Sostenibile: Questo capitolo esplora pratiche e strategie per un’intelligenza artificiale sostenibile, garantendo fattibilità a lungo termine e un impatto ambientale ridotto.\nIA Robusta: Parliamo di tecniche per sviluppare modelli di IA affidabili e robusti che possano funzionare in modo coerente in varie condizioni.\nIA Generativa: Questo capitolo esplora gli algoritmi e le tecniche alla base dell’IA generativa, aprendo strade all’innovazione e alla creatività.\n\n\n\n1.4.6 Impatto Sociale\nLa sezione Impatto Sociale evidenzia il potenziale trasformativo dell’IA embedded in vari domini. Presentiamo applicazioni reali di TinyML in sanità, agricoltura, conservazione e altre aree in cui l’IA sta facendo una positiva differenza. Questa sezione invoglia a sfruttare la potenza dell’AI embedded per il bene della società e a contribuire allo sviluppo di soluzioni di impatto.\n\nAI per il Bene: Evidenziamo applicazioni positive di TinyML in aree come sanità, agricoltura e la conservazione.\n\n\n\n1.4.7 Chiusura\nNella sezione Chiusura, riflettiamo sugli insegnamenti chiave del libro e guardiamo al futuro dell’IA embedded. Sintetizziamo i concetti trattati, discutiamo le tendenze emergenti e forniamo indicazioni su come proseguire nel percorso di apprendimento in questo campo in rapida evoluzione. Questa sezione lascia con una comprensione completa dell’intelligenza artificiale embedded e l’entusiasmo di applicare le conoscenze in modi innovativi.\n\nConclusione: Il libro si conclude con una riflessione sugli apprendimenti chiave e sulle direzioni future nel campo dell’intelligenza artificiale embedded.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#contributi-dei-lettori",
    "href": "contents/introduction/introduction.it.html#contributi-dei-lettori",
    "title": "1  Introduzione",
    "section": "1.5 Contributi dei Lettori",
    "text": "1.5 Contributi dei Lettori\nL’apprendimento nel mondo frenetico dell’intelligenza artificiale è un viaggio collaborativo. Ci siamo prefissati di coltivare una vivace comunità di studenti, innovatori e collaboratori. Esplorando i concetti e impegnandosi con gli esercizi, incoraggiamo a condividere le intuizioni ed esperienze personali. Che si tratti di un approccio innovativo, di un’applicazione interessante o di una domanda stimolante, i contributi dei singoli possono arricchire l’ecosistema di apprendimento. Partecipare alle discussioni, offrire e cercare indicazioni e collaborare a progetti per promuovere una cultura di crescita e apprendimento reciproci. Condividendo la conoscenza, si svolge un ruolo importante nel promuovere una comunità connessa, informata e potenziata a livello globale.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html",
    "href": "contents/ml_systems/ml_systems.it.html",
    "title": "2  Sistemi di ML",
    "section": "",
    "text": "2.1 Introduzione\nIl ML si sta evolvendo rapidamente, con nuovi paradigmi che plasmano il modo in cui i modelli vengono sviluppati, addestrati e implementati. Uno di questi paradigmi è l’apprendimento automatico embedded, che sta vivendo un’innovazione significativa guidata dalla proliferazione di sensori intelligenti, dispositivi edge e microcontrollori. Il machine learning embedded si riferisce all’integrazione di algoritmi di apprendimento automatico nell’hardware di un dispositivo, consentendo l’elaborazione e l’analisi dei dati in tempo reale senza fare affidamento sulla connettività cloud. Questo capitolo esplora il panorama dell’apprendimento automatico embedded, coprendo gli approcci chiave di Cloud ML, Edge ML e TinyML (Figura 2.1).\nL’apprendimento automatico è iniziato con Cloud ML, dove potenti server nel cloud venivano utilizzati per addestrare ed eseguire grandi modelli di machine learning. Tuttavia, con l’aumento della necessità di elaborazione in tempo reale e a bassa latenza, è emerso l’Edge ML, avvicinando le capacità di inferenza alla fonte dei dati su dispositivi edge come gli smartphone. L’ultimo sviluppo in questa progressione è TinyML, che consente ai modelli ML di funzionare su microcontrollori con risorse estremamente limitate e piccoli sistemi embedded. TinyML consente l’inferenza sul dispositivo senza fare affidamento sulla connettività al cloud o all’edge, aprendo nuove possibilità per dispositivi intelligenti alimentati a batteria.\nFigura 2.2 mostra le principali differenze tra Cloud ML, Edge ML e TinyML in termini di hardware, latenza, connettività, requisiti di alimentazione e complessità del modello. Questa significativa disparità nelle risorse disponibili pone delle sfide quando si tenta di distribuire modelli di deep learning su microcontrollori, poiché questi modelli spesso richiedono memoria e storage sostanziali. Ad esempio, modelli di deep learning ampiamente utilizzati come ResNet-50 superano i limiti di risorse dei microcontrollori di un fattore di circa 100, mentre modelli più efficienti come MobileNet-V2 superano comunque questi vincoli di un fattore di circa 20. Anche se quantizzato per utilizzare interi a 8 bit (int8) per un utilizzo di memoria ridotto, MobileNetV2 richiede più di 5 volte la memoria solitamente disponibile su un microcontrollore, rendendo difficile adattare il modello a questi dispositivi minuscoli.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#introduzione",
    "href": "contents/ml_systems/ml_systems.it.html#introduzione",
    "title": "2  Sistemi di ML",
    "section": "",
    "text": "Figura 2.1: Cloud vs. Edge vs. TinyML: Lo Spettro dell’Intelligenza Distribuita. Fonte: ABI Research – TinyML.\n\n\n\n\n\n\n\n\n\n\n\nFigura 2.2: Dalle GPU cloud ai microcontrollori: Navigazione nel panorama della memoria e dell’archiviazione tra dispositivi di elaborazione. Fonte: (Lin et al. 2023)\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, e Song Han. 2023. «Tiny Machine Learning: Progress and Futures Feature». IEEE Circuits Syst. Mag. 23 (3): 8–34. https://doi.org/10.1109/mcas.2023.3302182.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#cloud-ml",
    "href": "contents/ml_systems/ml_systems.it.html#cloud-ml",
    "title": "2  Sistemi di ML",
    "section": "2.2 Cloud ML",
    "text": "2.2 Cloud ML\nCloud ML sfrutta potenti server nel cloud per il training e l’esecuzione di modelli ML complessi e di grandi dimensioni e si basa sulla connettività Internet.\n\n2.2.1 Caratteristiche\nDefinizione di Cloud ML\nIl Cloud Machine Learning (Cloud ML) è un sottocampo del machine learning che sfrutta la potenza e la scalabilità dell’infrastruttura di cloud computing per sviluppare, addestrare e distribuire modelli di machine learning. Utilizzando le vaste risorse computazionali disponibili nel cloud, Cloud ML consente la gestione efficiente di set di dati su larga scala e algoritmi di machine learning complessi.\nInfrastruttura Centralizzata\nUna delle caratteristiche principali di Cloud ML è la sua infrastruttura centralizzata. I provider di servizi cloud offrono una piattaforma virtuale composta da server ad alta capacità, soluzioni di storage espansive e architetture di rete robuste, tutte ospitate in data center distribuiti in tutto il mondo (Figura 2.3). Questa configurazione centralizzata consente la messa in comune e la gestione efficiente delle risorse computazionali, semplificando la scalabilità dei progetti di machine learning in base alle esigenze.\nElaborazione Dati Scalabile e Addestramento dei Modelli\nIl Cloud ML eccelle nella sua capacità di elaborare e analizzare enormi volumi di dati. L’infrastruttura centralizzata è progettata per gestire calcoli complessi e attività di model training che richiedono una notevole potenza di calcolo. Sfruttando la scalabilità del cloud, i modelli di apprendimento automatico possono essere addestrati su grandi quantità di dati, con conseguente miglioramento delle capacità di apprendimento e delle prestazioni predittive.\nDeployment Flessibile e Accessibilità\nUn altro vantaggio di Cloud ML è la flessibilità che offre in termini di deployment [distribuzione] e accessibilità. Una volta che un modello di machine learning è stato addestrato e convalidato, può essere facilmente distribuito e reso accessibile agli utenti tramite servizi basati su cloud. Ciò consente un’integrazione perfetta delle funzionalità di apprendimento automatico in varie applicazioni e servizi, indipendentemente dalla posizione o dal dispositivo dell’utente.\nCollaborazione e Condivisione delle Risorse\nIl Cloud ML promuove la collaborazione e la condivisione delle risorse tra team e organizzazioni. La natura centralizzata dell’infrastruttura cloud consente a più utenti di accedere e lavorare contemporaneamente sugli stessi progetti di apprendimento automatico. Questo approccio collaborativo facilita la condivisione delle conoscenze, accelera il processo di sviluppo e ottimizza l’utilizzo delle risorse.\nEfficacia dei Costi e Scalabilità\nSfruttando il modello di prezzo “pay-as-you-go” offerto dai provider di servizi cloud, Cloud ML consente alle organizzazioni di evitare i costi iniziali associati alla creazione e alla manutenzione della propria infrastruttura di machine learning. La capacità di aumentare o diminuire le risorse in base alla domanda garantisce economicità e flessibilità nella gestione dei progetti di apprendimento automatico.\nIl Cloud ML ha rivoluzionato il modo in cui ci si approccia all’apprendimento automatico, rendendolo più accessibile, scalabile ed efficiente. Ha aperto nuove possibilità per le organizzazioni di sfruttare la potenza dell’apprendimento automatico senza la necessità di investimenti significativi in hardware e infrastruttura.\n\n\n\n\n\n\nFigura 2.3: Data center Cloud TPU presso Google. Fonte: Google.\n\n\n\n\n\n2.2.2 Vantaggi\nIl Cloud ML offre diversi vantaggi significativi che lo rendono una scelta potente per i progetti di apprendimento automatico:\nImmensa Potenza di Calcolo\nUno dei principali vantaggi del Cloud ML è la sua capacità di fornire vaste risorse di calcolo. L’infrastruttura cloud è progettata per gestire algoritmi complessi ed elaborare grandi set di dati in modo efficiente. Ciò è particolarmente vantaggioso per i modelli di apprendimento automatico che richiedono una notevole potenza di calcolo, come reti di deep learning o modelli addestrati su enormi set di dati. Sfruttando le capacità di calcolo del cloud, le organizzazioni possono superare i limiti delle configurazioni hardware locali e ridimensionare i loro progetti di apprendimento automatico per soddisfare requisiti esigenti.\nScalabilità Dinamica\nIl Cloud ML offre scalabilità dinamica, consentendo alle organizzazioni di adattarsi facilmente alle mutevoli esigenze di calcolo. Man mano che il volume dei dati aumenta o la complessità dei modelli di apprendimento automatico aumenta, l’infrastruttura cloud può essere ridimensionata senza problemi verso l’alto o verso il basso per adattarsi a questi cambiamenti. Questa flessibilità garantisce prestazioni costanti e consente alle organizzazioni di gestire carichi di lavoro variabili senza la necessità di ingenti investimenti hardware. Col Cloud ML, le risorse possono essere allocate su richiesta, fornendo una soluzione conveniente ed efficiente per la gestione di progetti di machine learning.\nAccesso a Strumenti e Algoritmi Avanzati\nLe piattaforme Cloud ML forniscono accesso a un’ampia gamma di strumenti e algoritmi avanzati specificamente progettati per l’apprendimento automatico. Questi strumenti spesso includono librerie, framework e API predefiniti che semplificano lo sviluppo e l’implementazione di modelli di apprendimento automatico. Gli sviluppatori possono sfruttare queste risorse per accelerare la creazione, il training e l’ottimizzazione di modelli sofisticati. Utilizzando gli ultimi progressi negli algoritmi e nelle tecniche di apprendimento automatico, le organizzazioni possono rimanere all’avanguardia dell’innovazione e ottenere risultati migliori nei loro progetti di apprendimento automatico.\nAmbiente Collaborativo\nIl Cloud ML promuove un ambiente collaborativo che consente ai team di lavorare insieme senza problemi. La natura centralizzata dell’infrastruttura cloud consente a più utenti di accedere e contribuire agli stessi progetti di apprendimento automatico contemporaneamente. Questo approccio collaborativo facilita la condivisione delle conoscenze, promuove la collaborazione interfunzionale e accelera lo sviluppo e l’iterazione dei modelli di apprendimento automatico. I team possono condividere facilmente codice, set di dati e risultati, consentendo una collaborazione efficiente e guidando l’innovazione in tutta l’organizzazione.\nEfficacia in Termini di Costi\nL’adozione del Cloud ML può essere una soluzione conveniente per le organizzazioni, soprattutto rispetto alla creazione e alla manutenzione di un’infrastruttura di apprendimento automatico in sede. I provider di servizi cloud offrono modelli di prezzo flessibili, come piani pay-as-you-go o basati su abbonamento, consentendo alle organizzazioni di pagare solo per le risorse che consumano. Ciò elimina la necessità di investimenti di capitale iniziali in hardware e infrastruttura, riducendo il costo complessivo dell’implementazione di progetti di apprendimento automatico. Inoltre, la scalabilità di Cloud ML garantisce che le organizzazioni possano ottimizzare l’utilizzo delle risorse ed evitare l’eccesso di provisioning [fornitura], migliorando ulteriormente l’efficienza in termini di costi.\nI vantaggi di Cloud ML, tra cui l’immensa potenza di calcolo, la scalabilità dinamica, l’accesso a strumenti e algoritmi avanzati, l’ambiente collaborativo e la convenienza, lo rendono una scelta interessante per le organizzazioni che desiderano sfruttare il potenziale del machine learning. Sfruttando le capacità del cloud, le organizzazioni possono accelerare le proprie iniziative di machine learning, guidare l’innovazione e ottenere un vantaggio competitivo nell’attuale panorama basato sui dati.\n\n\n2.2.3 Sfide\nSebbene il Cloud ML offra numerosi vantaggi, presenta anche alcune sfide che le organizzazioni devono considerare:\nProblemi di Latenza\nUna delle principali sfide del Cloud ML è il potenziale di problemi della latenza, in particolare nelle applicazioni che richiedono risposte in tempo reale. Poiché i dati devono essere inviati dall’origine dei dati ai server cloud centralizzati per l’elaborazione e quindi di nuovo all’applicazione, potrebbero verificarsi ritardi dovuti alla trasmissione in rete. Questa latenza può rappresentare un notevole svantaggio in scenari sensibili al fattore tempo, come veicoli autonomi, rilevamento delle frodi in tempo reale o sistemi di controllo industriale, in cui è fondamentale prendere decisioni immediate. Gli sviluppatori devono progettare attentamente i propri sistemi per ridurre al minimo la latenza e garantire tempi di risposta accettabili.\nProblemi di Sicurezza e Privacy dei Dati\nLa centralizzazione dell’elaborazione e dell’archiviazione dei dati nel cloud può sollevare preoccupazioni sulla privacy e sulla sicurezza dei dati. Quando i dati sensibili vengono trasmessi e archiviati in data center remoti, diventano vulnerabili a potenziali attacchi informatici e accessi non autorizzati. I data center cloud possono diventare obiettivi interessanti per gli hacker che cercano di sfruttare le vulnerabilità e ottenere l’accesso a informazioni preziose. Le organizzazioni devono investire in misure di sicurezza robuste, come crittografia, controlli di accesso e monitoraggio continuo, per proteggere i propri dati nel cloud. Anche la conformità alle normative sulla privacy dei dati, come GDPR o HIPAA, diventa una considerazione critica quando si gestiscono dati sensibili nel cloud.\nConsiderazioni sui Costi\nCon l’aumento delle esigenze di elaborazione dei dati, i costi associati all’utilizzo dei servizi cloud possono aumentare. Mentre il Cloud ML offre scalabilità e flessibilità, le organizzazioni che gestiscono grandi volumi di dati potrebbero dover affrontare costi crescenti man mano che consumano più risorse cloud. Il modello di prezzo pay-as-you-go dei servizi cloud implica che i costi possono aumentare rapidamente, soprattutto per attività ad alta intensità di elaborazione come l’addestramento e l’inferenza dei modelli. Le organizzazioni devono monitorare e ottimizzare attentamente l’utilizzo del cloud per garantirne la convenienza. Potrebbero dover prendere in considerazione strategie come la compressione dei dati, la progettazione efficiente degli algoritmi e l’ottimizzazione dell’allocazione delle risorse per ridurre al minimo i costi pur ottenendo le prestazioni desiderate.\nDipendenza dalla Connettività Internet\nIl Cloud ML si basa su una connettività Internet stabile e affidabile per funzionare in modo efficace. Poiché i dati devono essere trasmessi da e verso il cloud, eventuali interruzioni o limitazioni nella connettività di rete possono influire sulle prestazioni e sulla disponibilità del sistema di apprendimento automatico. Questa dipendenza dalla connettività Internet può rappresentare un problema in scenari in cui l’accesso alla rete è limitato, inaffidabile o costoso. Le organizzazioni devono garantire un’infrastruttura di rete solida e considerare meccanismi di “failover” o capacità offline per mitigare l’impatto dei problemi di connettività.\nVendor Lock-In\nQuando si adotta il Cloud ML, le organizzazioni spesso diventano dipendenti dagli strumenti, dalle API e dai servizi specifici forniti dal fornitore cloud prescelto. Questo vendor lock-in [blocco da fornitore] può rendere difficile cambiare fornitore o migrare verso piattaforme diverse in futuro. Le organizzazioni possono affrontare sfide in termini di portabilità, interoperabilità e costi quando prendono in considerazione un cambiamento nel loro fornitore di Cloud ML. È importante valutare attentamente le offerte del fornitore, considerare obiettivi strategici a lungo termine e pianificare potenziali scenari di migrazione per ridurre al minimo i rischi associati al vendor lock-in.\nAffrontare queste sfide richiede un’attenta pianificazione, progettazione architettonica e strategie di mitigazione del rischio. Le organizzazioni devono soppesare i vantaggi del Cloud ML rispetto ai potenziali problemi e prendere decisioni informate in base ai loro requisiti specifici, alla sensibilità dei dati e agli obiettivi aziendali. Affrontando proattivamente queste sfide, le organizzazioni possono sfruttare efficacemente la potenza del Cloud ML garantendo al contempo la privacy dei dati, la sicurezza, l’economicità e l’affidabilità complessiva del sistema.\n\n\n2.2.4 Casi d’Uso di Esempio\nIl Cloud ML ha trovato ampia adozione in vari domini, rivoluzionando il modo in cui le aziende operano e gli utenti interagiscono con la tecnologia. Esploriamo alcuni esempi notevoli del Cloud ML in azione:\nAssistenti Virtuali\nIl Cloud ML svolge un ruolo cruciale nel potenziamento di assistenti virtuali come Siri e Alexa. Questi sistemi sfruttano le immense capacità computazionali del cloud per elaborare e analizzare gli input vocali in tempo reale. Sfruttando la potenza dell’elaborazione del linguaggio naturale e degli algoritmi di apprendimento automatico, gli assistenti virtuali possono comprendere le domande degli utenti, estrarre informazioni rilevanti e generare risposte intelligenti e personalizzate. La scalabilità e la potenza di elaborazione del cloud consentono a questi assistenti di gestire un vasto numero di interazioni utente contemporaneamente, offrendo un’esperienza utente fluida e reattiva.\nSistemi di Raccomandazione Commerciali\nIl Cloud ML costituisce la spina dorsale dei sistemi di raccomandazione avanzati utilizzati da piattaforme come Netflix e Amazon. Questi sistemi sfruttano la capacità del cloud di elaborare e analizzare enormi set di dati per scoprire modelli, preferenze e comportamenti degli utenti. Sfruttando il filtraggio collaborativo e altre tecniche di apprendimento automatico, i sistemi di raccomandazione possono offrire contenuti personalizzati o suggerimenti di prodotti su misura per gli interessi di ciascun utente. La scalabilità del cloud consente a questi sistemi di aggiornare e perfezionare continuamente le proprie raccomandazioni in base alla quantità sempre crescente di dati utente, migliorandone il coinvolgimento e la soddisfazione.\nRilevamento delle Frodi\nNel settore finanziario, il Cloud ML ha rivoluzionato i sistemi di rilevamento delle frodi. Sfruttando la potenza di calcolo del cloud, questi sistemi possono analizzare grandi quantità di dati transazionali in tempo reale per identificare potenziali attività fraudolente. Gli algoritmi di apprendimento automatico addestrati su modelli di frode storici possono rilevare anomalie e comportamenti sospetti, consentendo agli istituti finanziari di adottare misure proattive per prevenire le frodi e ridurre al minimo le perdite finanziarie. La capacità del cloud di elaborare e archiviare grandi volumi di dati lo rende una piattaforma ideale per implementare sistemi di rilevamento delle frodi robusti e scalabili.\nEsperienze Utente Personalizzate\nIl Cloud ML è profondamente integrato nelle nostre esperienze online, plasmando il modo in cui interagiamo con le piattaforme digitali. Dagli annunci personalizzati sui feed dei social media alle funzionalità di testo predittivo nei servizi di posta elettronica, il Cloud ML alimenta algoritmi intelligenti che migliorano il coinvolgimento e la praticità dell’utente. Consente ai siti di e-commerce di consigliare prodotti in base alla cronologia di navigazione e acquisto di un utente, ottimizza i motori di ricerca per fornire risultati accurati e pertinenti e automatizza il tagging e la categorizzazione delle foto su piattaforme come Facebook. Sfruttando le risorse di calcolo del cloud, questi sistemi possono apprendere e adattarsi continuamente alle preferenze dell’utente, offrendo un’esperienza utente più intuitiva e personalizzata.\nSicurezza e Rilevamento delle Anomalie\nIl Cloud ML svolge un ruolo nel rafforzare la sicurezza dell’utente alimentando i sistemi di rilevamento delle anomalie. Questi sistemi monitorano costantemente le attività dell’utente e i log di sistema per identificare pattern insoliti o comportamenti sospetti. Analizzando grandi quantità di dati in tempo reale, gli algoritmi Cloud ML possono rilevare potenziali minacce informatiche, come tentativi di accesso non autorizzati, infezioni da malware o violazioni dei dati. La scalabilità e la potenza di elaborazione del cloud consentono a questi sistemi di gestire la crescente complessità e il volume dei dati di sicurezza, fornendo un approccio proattivo per proteggere utenti e sistemi da potenziali minacce.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#edge-ml",
    "href": "contents/ml_systems/ml_systems.it.html#edge-ml",
    "title": "2  Sistemi di ML",
    "section": "2.3 Edge ML",
    "text": "2.3 Edge ML\n\n2.3.1 Caratteristiche\nDefinizione di Edge ML\nL’Edge Machine Learning (Edge ML) esegue algoritmi di apprendimento automatico direttamente sui dispositivi endpoint o più vicini al luogo in cui vengono generati i dati anziché affidarsi a server cloud centralizzati. Questo approccio avvicina l’elaborazione alla fonte dei dati, riducendo la necessità di inviarne grandi volumi sulle reti, con conseguente riduzione della latenza e miglioramento della privacy dei dati.\nElaborazione Dati Decentralizzata\nIn Edge ML, l’elaborazione dei dati avviene in modo decentralizzato. Invece di inviare dati a server remoti, i dati vengono elaborati localmente su dispositivi come smartphone, tablet o dispositivi Internet of Things (IoT) (Figura 2.4). Questa elaborazione locale consente ai dispositivi di prendere decisioni rapide in base ai dati che raccolgono senza affidarsi pesantemente alle risorse di un server centrale. Questa decentralizzazione è particolarmente importante nelle applicazioni in tempo reale in cui anche un leggero ritardo può avere conseguenze significative.\nArchiviazione e Calcolo dei Dati Locali\nL’archiviazione e il calcolo dei dati locali sono caratteristiche chiave di Edge ML. Questa configurazione garantisce che i dati possano essere archiviati e analizzati direttamente sui dispositivi, mantenendo così la privacy dei dati e riducendo la necessità di una connettività Internet costante. Inoltre, questo spesso porta a un calcolo più efficiente, poiché i dati non devono percorrere lunghe distanze e i calcoli vengono eseguiti con una comprensione più consapevole del contesto locale, che a volte può portare ad analisi più approfondite.\n\n\n\n\n\n\nFigura 2.4: Esempi di Edge ML. Fonte: Edge Impulse.\n\n\n\n\n\n2.3.2 Vantaggi\nLatenza Ridotta\nUno dei principali vantaggi di Edge ML è la significativa riduzione della latenza rispetto al Cloud ML. Questa ridotta latenza può essere un vantaggio fondamentale in situazioni in cui i millisecondi contano, come nei veicoli autonomi, dove un rapido processo decisionale può fare la differenza tra sicurezza e incidente.\nPrivacy dei Dati Migliorata\nEdge ML offre anche una migliore privacy dei dati, poiché i dati vengono principalmente archiviati ed elaborati localmente. Ciò riduce al minimo il rischio di violazioni dei dati, più comuni nelle soluzioni di archiviazione dati centralizzate. Le informazioni sensibili possono essere mantenute più sicure, poiché non vengono inviate su reti che potrebbero essere intercettate.\nMinore Utilizzo della Larghezza di Banda\nOperare più vicino alla fonte dei dati significa che meno dati devono essere inviati sulle reti, riducendo l’utilizzo della larghezza di banda. Ciò può comportare risparmi sui costi e guadagni di efficienza, soprattutto in ambienti in cui la larghezza di banda è limitata o costosa.\n\n\n2.3.3 Sfide\nRisorse di Calcolo Limitate Rispetto al Cloud ML\nTuttavia, Edge ML presenta le sue sfide. Una delle principali preoccupazioni sono le risorse di calcolo limitate rispetto alle soluzioni basate su cloud. I dispositivi endpoint possono avere una potenza di elaborazione o una capacità di archiviazione diverse rispetto ai server cloud, limitando la complessità dei modelli di apprendimento automatico che possono essere distribuiti.\nComplessità nella Gestione dei Nodi Edge\nLa gestione di una rete di nodi Edge può introdurre complessità, soprattutto per quanto riguarda coordinamento, aggiornamenti e manutenzione. Garantire che tutti i nodi funzionino senza problemi e siano aggiornati con gli algoritmi e i protocolli di sicurezza più recenti può essere una sfida logistica.\nProblemi di Sicurezza nei Nodi Edge\nSebbene Edge ML offra una maggiore privacy dei dati, i nodi Edge possono talvolta essere più vulnerabili ad attacchi fisici e informatici. Sviluppare protocolli di sicurezza affidabili che proteggano i dati su ogni nodo senza compromettere l’efficienza del sistema, resta una sfida significativa nell’implementazione di soluzioni Edge ML.\n\n\n2.3.4 Casi d’Uso di Esempio\nEdge ML ha molte applicazioni, dai veicoli autonomi e dalle case intelligenti all’IoT industriale. Questi esempi sono stati scelti per evidenziare scenari in cui l’elaborazione dei dati in tempo reale, la latenza ridotta e la privacy migliorata non sono solo vantaggiose, ma spesso fondamentali per il funzionamento e il successo di queste tecnologie. Dimostrano il ruolo che Edge ML può svolgere nel guidare i progressi in vari settori, promuovendo l’innovazione e aprendo la strada a sistemi più intelligenti, reattivi e adattabili.\nVeicoli Autonomi\nI veicoli autonomi sono un esempio lampante del potenziale di Edge ML. Questi veicoli si affidano in larga misura all’elaborazione dei dati in tempo reale per navigare e prendere decisioni. I modelli di apprendimento automatico localizzati aiutano ad analizzare rapidamente i dati da vari sensori per prendere decisioni di guida immediate, garantendo sicurezza e funzionamento regolare.\nCase ed Edifici Intelligenti\nEdge ML svolge un ruolo cruciale nella gestione efficiente di vari sistemi in case ed edifici intelligenti, dall’illuminazione e dal riscaldamento alla sicurezza. Elaborando i dati localmente, questi sistemi possono funzionare in modo più reattivo e armonioso con le abitudini e le preferenze degli occupanti, creando un ambiente di vita più confortevole.\nIoT industriale\nL’IoT industriale sfrutta Edge ML per monitorare e controllare processi industriali complessi. Qui, i modelli di apprendimento automatico possono analizzare i dati da numerosi sensori in tempo reale, consentendo la manutenzione predittiva, ottimizzando le operazioni e migliorando le misure di sicurezza. Questa rivoluzione nell’automazione e nell’efficienza industriale sta trasformando la manifattura e la produzione in vari settori.\nL’applicabilità di Edge ML è vasta e non si limita a questi esempi. Vari altri settori, tra cui sanità, agricoltura e pianificazione urbana, stanno esplorando e integrando l’Edge ML per sviluppare soluzioni innovative che rispondono alle esigenze e alle sfide del mondo reale, annunciando una nuova era di sistemi intelligenti e interconnessi.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#tiny-ml",
    "href": "contents/ml_systems/ml_systems.it.html#tiny-ml",
    "title": "2  Sistemi di ML",
    "section": "2.4 Tiny ML",
    "text": "2.4 Tiny ML\n\n2.4.1 Caratteristiche\nDefinizione di TinyML\nTinyML si colloca all’incrocio tra sistemi embedded e apprendimento automatico, rappresentando un campo in rapida crescita che porta algoritmi intelligenti direttamente a microcontrollori e sensori minuscoli. Questi microcontrollori operano con gravi limitazioni di risorse, in particolare per quanto riguarda memoria, archiviazione e potenza di calcolo (vedere un esempio di kit TinyML in Figura 2.5).\nMachine Learning On-Device\nIn TinyML, l’attenzione è rivolta all’apprendimento automatico sul dispositivo. Ciò significa che i modelli di apprendimento automatico vengono distribuiti e addestrati sul dispositivo, eliminando la necessità di server esterni o infrastrutture cloud. Ciò consente a TinyML di abilitare un processo decisionale intelligente proprio dove vengono generati i dati, rendendo possibili approfondimenti e azioni in tempo reale, anche in contesti in cui la connettività è limitata o non disponibile.\nAmbienti a Basso Consumo Energetico e con Risorse Limitate\nTinyML eccelle in contesti a basso consumo energetico e con risorse limitate. Questi ambienti richiedono soluzioni altamente ottimizzate che funzionino entro le risorse disponibili. TinyML soddisfa questa esigenza tramite algoritmi e modelli specializzati progettati per offrire prestazioni decenti consumando energia minima, garantendo così periodi operativi prolungati, anche nei dispositivi alimentati a batteria.\n\n\n\n\n\n\nFigura 2.5: Esempi di kit di dispositivi TinyML. Fonte: Widening Access to Applied Machine Learning with TinyML.\n\n\n\n\n\n\n\n\n\nEsercizio 2.1: TinyML con Arduino\n\n\n\n\n\nPrepararsi a portare l’apprendimento automatico sui dispositivi più piccoli! Nel mondo dell’apprendimento automatico embedded, TinyML è il luogo in cui i vincoli di risorse incontrano l’ingegnosità. Questo notebook Colab guiderà nella creazione di un modello di riconoscimento dei gesti progettato su una scheda Arduino. Si imparerà come addestrare una piccola ma efficace rete neurale, ottimizzarla per un utilizzo minimo di memoria e distribuirla al proprio microcontrollore. Se si è entusiasti di rendere più intelligenti gli oggetti di uso quotidiano, è qui che si inizia!\n\n\n\n\n\n\n2.4.2 Vantaggi\nLatenza Estremamente Bassa\nUno dei vantaggi più importanti di TinyML è la sua capacità di offrire una latenza estremamente bassa. Poiché il calcolo avviene direttamente sul dispositivo, il tempo necessario per inviare dati a server esterni e ricevere una risposta viene eliminato. Ciò è fondamentale nelle applicazioni che richiedono un processo decisionale immediato, consentendo risposte rapide a condizioni mutevoli.\nElevata Sicurezza dei Dati\nTinyML migliora intrinsecamente la sicurezza dei dati. Poiché l’elaborazione e l’analisi dei dati avvengono sul dispositivo, il rischio di intercettazione dei dati durante la trasmissione viene praticamente eliminato. Questo approccio localizzato alla gestione dei dati garantisce che le informazioni sensibili rimangano sul dispositivo, rafforzando la sicurezza dei dati dell’utente.\nEfficienza Energetica\nTinyML opera all’interno di un framework efficiente dal punto di vista energetico, una necessità dati i suoi ambienti con risorse limitate. Utilizzando algoritmi snelli e metodi di calcolo ottimizzati, TinyML garantisce che i dispositivi possano eseguire attività complesse senza esaurire rapidamente la durata della batteria, il che lo rende un’opzione sostenibile per le distribuzioni a lungo termine.\n\n\n2.4.3 Sfide\nCapacità di Calcolo Limitate\nTuttavia, il passaggio a TinyML comporta una serie di ostacoli. La limitazione principale sono le capacità di calcolo limitate dei dispositivi. La necessità di operare entro tali limiti implica che i modelli distribuiti debbano essere semplificati, il che potrebbe influire sull’accuratezza e la complessità delle soluzioni.\nCiclo di Sviluppo Complesso\nTinyML introduce anche un ciclo di sviluppo complicato. La creazione di modelli leggeri ed efficaci richiede una profonda comprensione dei principi di apprendimento automatico e competenza nei sistemi embedded. Questa complessità richiede un approccio di sviluppo collaborativo, in cui la competenza multi-dominio è essenziale per il successo.\nOttimizzazione e Compressione del Modello\nUna sfida centrale in TinyML è l’ottimizzazione e la compressione del modello. La creazione di modelli di machine learning in grado di operare efficacemente all’interno della memoria limitata e della potenza di calcolo dei microcontrollori richiede approcci innovativi alla progettazione del modello. Gli sviluppatori si trovano spesso ad affrontare la sfida di trovare un delicato equilibrio e ottimizzare i modelli per mantenere l’efficacia, pur rispettando rigidi vincoli di risorse.\n\n\n2.4.4 Casi d’Uso di Esempio\nDispositivi Indossabili\nNei dispositivi indossabili, TinyML apre le porte a gadget più intelligenti e reattivi. Dai fitness tracker che offrono feedback in tempo reale sugli allenamenti agli occhiali intelligenti che elaborano dati visivi al volo, TinyML trasforma il modo in cui interagiamo con la tecnologia indossabile, offrendo esperienze personalizzate direttamente dal dispositivo.\nManutenzione Predittiva\nNegli ambienti industriali, TinyML svolge un ruolo significativo nella manutenzione predittiva. Implementando algoritmi TinyML su sensori che monitorano lo stato di salute delle apparecchiature, le aziende possono identificare preventivamente potenziali problemi, riducendo i tempi di inattività e prevenendo costosi guasti. L’analisi dei dati in loco garantisce risposte rapide, impedendo potenzialmente a piccoli problemi di diventare problemi gravi.\nRilevamento delle Anomalie\nTinyML può essere impiegato per creare modelli di rilevamento delle anomalie che identificano pattern di dati insoliti. Ad esempio, una fabbrica intelligente potrebbe usare TinyML per monitorare i processi industriali e individuare anomalie, aiutando a prevenire incidenti e migliorare la qualità del prodotto. Allo stesso modo, un’azienda di sicurezza potrebbe usare TinyML per monitorare il traffico di rete per modelli insoliti, aiutando a rilevare e prevenire attacchi informatici. TinyML potrebbe monitorare i dati dei pazienti per anomalie nell’assistenza sanitaria, aiutando a rilevare precocemente le malattie e a migliorare il trattamento dei pazienti.\nMonitoraggio Ambientale\nNel monitoraggio ambientale, TinyML consente l’analisi dei dati in tempo reale da vari sensori distribuiti sul campo. Questi potrebbero spaziare dal monitoraggio della qualità dell’aria in città al tracciamento della fauna selvatica nelle aree protette. Tramite TinyML, i dati possono essere elaborati localmente, consentendo risposte rapide alle mutevoli condizioni e fornendo una comprensione adeguata dei modelli ambientali, cruciale per un processo decisionale informato.\nIn sintesi, TinyML funge da pioniere nell’evoluzione dell’apprendimento automatico, promuovendo l’innovazione in vari campi portando l’intelligenza direttamente nell’Edge. Il suo potenziale di trasformare la nostra interazione con la tecnologia e il mondo è immenso, promettendo un futuro in cui i dispositivi sono connessi, intelligenti e capaci di prendere decisioni e rispondere in tempo reale.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#confronto",
    "href": "contents/ml_systems/ml_systems.it.html#confronto",
    "title": "2  Sistemi di ML",
    "section": "2.5 Confronto",
    "text": "2.5 Confronto\nFino a questo punto, abbiamo esplorato singolarmente ciascuna delle diverse varianti di ML. Ora, mettiamole insieme per una visione completa. Tabella 2.1 offre un’analisi comparativa di Cloud ML, Edge ML e TinyML basata su varie caratteristiche e aspetti. Questo confronto fornisce una chiara prospettiva sui vantaggi esclusivi e sui fattori distintivi, aiutando a prendere decisioni informate in base alle esigenze e ai vincoli specifici di una determinata applicazione o progetto.\n\n\n\nTabella 2.1: Confronto degli aspetti delle funzionalità tra Cloud ML, Edge ML e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nUbicazione della elaborazione\nServer centralizzati (Data Center)\nDispositivi locali (più vicini alle fonti di dati)\nSul dispositivo (microcontrollori, sistemi embedded)\n\n\nLatenza\nAlta (dipende dalla connettività Internet)\nModerata (latenza ridotta rispetto a Cloud ML)\nBassa (elaborazione immediata senza ritardo di rete)\n\n\nPrivacy dei dati\nModerata (dati trasmessi tramite reti)\nAlta (i dati rimangono sulle reti locali)\nMolto alta (dati elaborati sul dispositivo, non trasmessi)\n\n\nPotenza di calcolo\nAlta (usa una potente infrastruttura del data center)\nModerata (utilizza le capacità del dispositivo locale)\nBassa (limitata alla potenza del sistema embedded )\n\n\nConsumo energetico\nAlto (i data center consumano molta energia)\nModerato (meno dei data center, più di TinyML)\nBasso (alta efficienza energetica, progettato per bassi consumi)\n\n\nScalabilità\nAlto (facile da scalare con risorse server aggiuntive)\nModerato (dipende dalle capacità del dispositivo locale)\nBasso (limitato dalle risorse hardware del dispositivo)\n\n\nCosto\nAlto (costi ricorrenti per l’uso del server, manutenzione)\nVariabile (dipende dalla complessità della configurazione locale)\nBasso (principalmente costi iniziali per i componenti hardware)\n\n\nConnettività\nAlto (richiede una connettività Internet stabile)\nBasso (può funzionare con connettività intermittente)\nMolto basso (può funzionare senza alcuna connettività di rete)\n\n\nElaborazione in tempo reale\nModerata (può essere influenzata dalla latenza di rete)\nAlta (capace di elaborazione in tempo reale localmente)\nMolto alta (elaborazione immediata con latenza minima)\n\n\nEsempi di applicazione\nAnalisi di Big Data, Assistenti virtuali\nVeicoli autonomi, Case intelligenti\nDispositivi indossabili, Reti di sensori\n\n\nComplessità\nDa moderata ad alta (richiede conoscenza del cloud computing)\nModerata (richiede conoscenza della configurazione della rete locale)\nDa moderata ad alta (richiede competenza nei sistemi embedded)",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#conclusione",
    "href": "contents/ml_systems/ml_systems.it.html#conclusione",
    "title": "2  Sistemi di ML",
    "section": "2.6 Conclusione",
    "text": "2.6 Conclusione\nIn questo capitolo, abbiamo offerto una panoramica in evoluzione dell’apprendimento automatico, che copre i paradigmi cloud, edge e tiny ML. L’apprendimento automatico basato su cloud sfrutta le immense risorse computazionali delle piattaforme cloud per abilitare modelli potenti e accurati, ma presenta delle limitazioni, tra cui problemi di latenza e privacy. Edge ML mitiga queste limitazioni portando l’inferenza direttamente sui dispositivi edge, offrendo una latenza inferiore e ridotte esigenze di connettività. TinyML va oltre, miniaturizzando i modelli ML per eseguirli direttamente su dispositivi con risorse altamente limitate, aprendo una nuova categoria di applicazioni intelligenti.\nOgni approccio ha i suoi compromessi, tra cui complessità del modello, latenza, privacy e costi dell’hardware. Nel tempo, prevediamo la convergenza di questi approcci ML embedded, col pre-training cloud che facilita implementazioni edge e tiny ML più sofisticate. Progressi come l’apprendimento federato e l’apprendimento “on-device” consentiranno ai dispositivi embedded di perfezionare i propri modelli imparando dai dati del mondo reale.\nIl panorama ML embedded si sta evolvendo rapidamente ed è pronto a consentire applicazioni intelligenti su un ampio spettro di dispositivi e casi d’uso. Questo capitolo funge da “istantanea” dello stato attuale del ML embedded. Man mano che algoritmi, hardware e connettività continuano a migliorare, possiamo aspettarci che i dispositivi embedded di tutte le dimensioni diventino sempre più capaci, sbloccando nuove applicazioni trasformative per l’intelligenza artificiale.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#sec-ml-systems-resource",
    "href": "contents/ml_systems/ml_systems.it.html#sec-ml-systems-resource",
    "title": "2  Sistemi di ML",
    "section": "2.7 Risorse",
    "text": "2.7 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nEmbedded Systems Overview.\nEmbedded Computer Hardware.\nEmbedded I/O.\nEmbedded systems software.\nEmbedded ML software.\nEmbedded Inference.\nTinyML on Microcontrollers.\nTinyML as a Service (TinyMLaaS):\n\nTinyMLaaS: Introduction.\nTinyMLaaS: Design Overview.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html",
    "href": "contents/dl_primer/dl_primer.it.html",
    "title": "3  Avvio al Deep Learning",
    "section": "",
    "text": "3.1 Introduzione",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html#introduzione",
    "href": "contents/dl_primer/dl_primer.it.html#introduzione",
    "title": "3  Avvio al Deep Learning",
    "section": "",
    "text": "3.1.1 Definizione e Importanza\nIl deep learning, un’area specializzata nell’apprendimento automatico e nell’intelligenza artificiale (IA), utilizza algoritmi modellati sulla struttura e la funzione del cervello umano, noti come reti neurali artificiali. Questo campo è un elemento fondamentale nell’IA, che guida il progresso in diversi settori come la visione artificiale, l’elaborazione del linguaggio naturale e i veicoli a guida autonoma. La sua importanza nei sistemi di IA embedded è evidenziata dalla sua capacità di gestire calcoli e previsioni intricati, ottimizzando le risorse limitate nelle impostazioni embedded. La Figura 3.1 illustra lo sviluppo cronologico e la segmentazione relativa dei tre campi.\n\n\n\n\n\n\nFigura 3.1: Il diagramma illustra l’intelligenza artificiale come campo onnicomprensivo che comprende tutti i metodi computazionali che imitano le funzioni cognitive umane. Il Machine learning [apprendimento automatico] è un sottoinsieme dell’IA che include algoritmi in grado di apprendere dai dati. Il deep learning, un ulteriore sottoinsieme del ML, coinvolge specificamente reti neurali in grado di apprendere pattern [schemi] più complessi in grandi volumi di dati. Fonte: NVIDIA.\n\n\n\n\n\n3.1.2 Breve Storia del Deep Learning\nL’idea del deep learning ha origine nelle prime reti neurali artificiali. Ha vissuto diversi cicli di interesse, a partire dall’introduzione del Perceptron negli anni ’50 (Rosenblatt 1957), seguita dall’invenzione degli algoritmi di backpropagation negli anni ’80 (Rumelhart, Hinton, e Williams 1986).\n\nRosenblatt, Frank. 1957. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory.\n\nRumelhart, David E., Geoffrey E. Hinton, e Ronald J. Williams. 1986. «Learning representations by back-propagating errors». Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. «ImageNet Classification with Deep Convolutional Neural Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\nIl termine “deep learning” è diventato importante negli anni 2000, caratterizzato da progressi nella potenza di calcolo e nell’accessibilità dei dati. Traguardi importanti includono l’addestramento di successo di reti profonde come AlexNet (Krizhevsky, Sutskever, e Hinton 2012) da parte di Geoffrey Hinton, una figura di spicco nell’intelligenza artificiale, e il rinnovato focus sulle reti neurali come strumenti efficaci per l’analisi e la modellazione dei dati.\nIl deep learning ha recentemente registrato una crescita esponenziale, trasformando vari settori. La crescita computazionale ha seguito un modello di raddoppio di 18 mesi dal 1952 al 2010, che poi ha accelerato fino a un ciclo di 6 mesi dal 2010 al 2022, come mostrato in Figura 3.2. Contemporaneamente, abbiamo assistito all’emergere di modelli su larga scala tra il 2015 e il 2022, che sono apparsi da 2 a 3 ordini di grandezza più veloci e hanno seguito un ciclo di raddoppio di 10 mesi.\n\n\n\n\n\n\nFigura 3.2: Crescita dei modelli di deep learning.\n\n\n\nMolteplici fattori hanno contribuito a questa impennata, tra cui i progressi nella potenza computazionale, l’abbondanza di big data e i miglioramenti nei progetti algoritmici. In primo luogo, la crescita delle capacità computazionali, in particolare l’arrivo delle Graphics Processing Units (GPU) [unità di elaborazione grafica] e delle Tensor Processing Units (TPU) [unità di elaborazione tensoriale] (Jouppi et al. 2017), ha accelerato notevolmente i tempi di training e inferenza dei modelli di apprendimento profondo. Questi miglioramenti hardware hanno consentito la costruzione e il training di reti più complesse e profonde di quanto fosse possibile negli anni precedenti.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\nIn secondo luogo, la rivoluzione digitale ha prodotto una grande quantità di big data, offrendo materiale ricco da cui i modelli di deep learning possono imparare e distinguersi in attività quali il riconoscimento di immagini e parlato, la traduzione linguistica e il gioco. I grandi set di dati etichettati sono stati fondamentali per perfezionare e distribuire con successo applicazioni di deep learning in contesti reali.\nInoltre, le collaborazioni e gli sforzi open source hanno alimentato una comunità dinamica di ricercatori e professionisti, accelerando i progressi nelle tecniche di deep learning. Innovazioni come il “deep reinforcement learning”, il “transfer learning” e l’intelligenza artificiale generativa hanno ampliato la portata di ciò che è realizzabile col deep learning, aprendo nuove possibilità in vari settori, tra cui sanità, finanza, trasporti e intrattenimento.\nLe organizzazioni di tutto il mondo riconoscono il potenziale trasformativo del deep learning e investono molto in ricerca e sviluppo per sfruttare le sue capacità nel fornire soluzioni innovative, ottimizzare le operazioni e creare nuove opportunità di business. Mentre il deep learning continua la sua traiettoria ascendente, è destinato a ridefinire il modo in cui interagiamo con la tecnologia, migliorando la praticità, la sicurezza e la connettività nelle nostre vite.\n\n\n3.1.3 Applicazioni del Deep Learning\nIl deep learning è oggi ampiamente utilizzato in numerosi settori e il suo impatto trasformativo sulla società è evidente. Nella finanza, alimenta le previsioni del mercato azionario, la valutazione del rischio e il rilevamento delle frodi. Ad esempio, gli algoritmi di deep learning possono prevedere le tendenze del mercato azionario, guidare le strategie di investimento e migliorare le decisioni finanziarie. Nel marketing, guida la segmentazione dei clienti, la personalizzazione e l’ottimizzazione dei contenuti. Il deep learning analizza il comportamento e le preferenze dei consumatori per abilitare pubblicità altamente mirate e la distribuzione di contenuti personalizzati. Nella produzione, il deep learning semplifica i processi di produzione e migliora il controllo di qualità analizzando continuamente grandi volumi di dati. Ciò consente alle aziende di aumentare la produttività e ridurre al minimo gli sprechi, portando alla produzione di beni di qualità superiore a costi inferiori. Nell’assistenza sanitaria, il machine learning aiuta nella diagnosi, nella pianificazione del trattamento e nel monitoraggio dei pazienti. Allo stesso modo, il deep learning può fare previsioni mediche che migliorano la diagnosi dei pazienti e salvano vite. I vantaggi sono chiari: il machine learning prevede con maggiore accuratezza degli esseri umani e lo fa molto più rapidamente.\nIl deep learning migliora i prodotti di uso quotidiano, come il rafforzamento dei sistemi di raccomandazione di Netflix per fornire agli utenti più consigli personalizzati. In Google, i modelli di deep learning hanno portato a notevoli miglioramenti in Google Translate, consentendogli di gestire oltre 100 lingue. I veicoli autonomi di aziende come Waymo, Cruise e Motional sono diventati realtà grazie all’uso del deep learning nel loro sistema di percezione. Inoltre, Amazon impiega l’edge deep learning nei suoi dispositivi Alexa per eseguire l’individuazione delle parole chiave.\n\n\n3.1.4 Rilevanza per l’IA Embedded\nL’IA embedded, l’integrazione di algoritmi di intelligenza artificiale direttamente nei dispositivi hardware, trae naturalmente vantaggio dalle capacità del deep learning. La combinazione di algoritmi di deep learning e sistemi embedded ha gettato le basi per dispositivi intelligenti e autonomi in grado di analisi avanzate on-device [sul dispositivo]. Il deep learning aiuta a estrarre pattern e informazioni complesse dai dati di input, il che è essenziale nello sviluppo di sistemi embedded intelligenti, dagli elettrodomestici ai macchinari industriali. Questa collaborazione inaugura una nuova era di dispositivi intelligenti e interconnessi, in grado di apprendere e adattarsi al comportamento dell’utente e alle condizioni ambientali, ottimizzando le prestazioni e offrendo praticità ed efficienza senza precedenti.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html#reti-neurali",
    "href": "contents/dl_primer/dl_primer.it.html#reti-neurali",
    "title": "3  Avvio al Deep Learning",
    "section": "3.2 Reti Neurali",
    "text": "3.2 Reti Neurali\nIl deep learning trae ispirazione dalle reti neurali del cervello umano per creare modelli decisionali. Questa sezione approfondisce i concetti fondamentali del deep learning, offrendo approfondimenti sugli argomenti più complessi trattati più avanti in questa introduzione.\nLe reti neurali fungono da fondamento del deep learning, ispirate alle reti neurali biologiche nel cervello umano per elaborare e analizzare i dati in modo gerarchico. Le reti neurali sono composte da unità di base chiamate perceptron, che sono solitamente organizzate in layer [strati]. Ogni layer è costituito da diversi perceptron e più layer sono impilati per formare l’intera rete. Le connessioni tra questi layer sono definite da insiemi di pesi o parametri che determinano come i dati vengono elaborati mentre fluiscono dall’input all’output della rete.\nDi seguito, esaminiamo i componenti e le strutture primarie nelle reti neurali.\n\n3.2.1 Perceptron\nIl Perceptron è l’unità di base o il nodo che costituisce la base per strutture più complesse. Funziona prendendo più input, ognuno dei quali rappresenta una caratteristica dell’oggetto in analisi, come le caratteristiche di una casa per prevederne il prezzo o gli attributi di una canzone per prevederne la popolarità nei servizi di streaming musicale. Questi input sono indicati come \\(x_1, x_2, ..., x_n\\).\nCiascun input \\(x_i\\) ha un peso corrispondente \\(w_{ij}\\) e il perceptron moltiplica semplicemente ogni input per il suo peso corrispondente. Questa operazione è simile alla regressione lineare, dove l’output intermedio, \\(z\\), è calcolato come la somma dei prodotti degli input e dei loro pesi:\n\\[\nz = \\sum (x_i \\cdot w_{ij})\n\\]\nA questo calcolo intermedio, viene aggiunto un termine di bias \\(b\\), che consente al modello di adattarsi meglio ai dati spostando la funzione di output lineare verso l’alto o verso il basso. Pertanto, la combinazione lineare intermedia calcolata dal perceptron, incluso il bias, diventa:\n\\[\nz = \\sum (x_i \\cdot w_{ij}) + b\n\\]\nQuesta forma base di un perceptron può modellare solo relazioni lineari tra input e output. I pattern trovati in natura sono spesso complessi e si estendono oltre le relazioni lineari. Per consentire al perceptron di gestire relazioni non lineari, una funzione di attivazione viene applicata all’output lineare \\(z\\).\n\\[\n\\hat{y} = \\sigma(z)\n\\]\nFigura 3.3 illustra un esempio in cui i dati presentano un andamento non lineare che non potrebbe essere modellato adeguatamente con un approccio lineare. La funzione di attivazione, come la sigmoide, la tanh o la ReLU, trasforma la somma di input lineare in un output non lineare. L’obiettivo principale di questa funzione è introdurre la non linearità nel modello, consentendogli di apprendere ed eseguire attività più sofisticate. Pertanto, l’output finale del perceptron, inclusa la funzione di attivazione, può essere espresso come:\n\n\n\n\n\n\nFigura 3.3: Le funzioni di attivazione consentono la modellazione di relazioni non lineari complesse. Fonte: Medium - Sachin Kaushik.\n\n\n\nUn perceptron può essere configurato per eseguire attività di regressione o classificazione. Per la regressione, viene utilizzato l’output numerico effettivo \\(\\hat{y}\\). Per la classificazione, l’output dipende dal fatto che \\(\\hat{y}\\) superi una determinata soglia. Se \\(\\hat{y}\\) supera questa soglia, il perceptron potrebbe restituire una classe (ad esempio, ‘yes’) e, in caso contrario, un’altra classe (ad esempio, ‘no’).\n\n\n\n\n\n\nFigura 3.4: Perceptron. Concepiti negli anni ’50, i perceptron hanno aperto la strada allo sviluppo di reti neurali più complesse e sono stati un elemento fondamentale nel deep learning. Fonte: Wikimedia - Chrislb.\n\n\n\nFigura 3.4 illustra gli elementi fondamentali di un perceptron, che funge da fondamento per reti neurali più complesse. Un perceptron può essere pensato come un decisore in miniatura, che utilizza i suoi pesi, il sui bias [polarizzazione] e la sua funzione di attivazione per elaborare input e generare output in base ai parametri appresi. Questo concetto costituisce la base per comprendere architetture di reti neurali più complesse, come i perceptron multilayer [multistrato]. In queste strutture avanzate, i layer di perceptron lavorano di concerto, con l’output di ogni layer che funge da input per il layer successivo. Questa disposizione gerarchica crea un modello di deep learning in grado di comprendere e modellare pattern complessi e astratti all’interno dei dati. Impilando queste semplici unità, le reti neurali acquisiscono la capacità di affrontare attività sempre più sofisticate, dal riconoscimento delle immagini all’elaborazione del linguaggio naturale.\n\n\n3.2.2 Perceptron Multilayer\nI “Multilayer perceptron” (MLP) sono un’evoluzione del modello del perceptron a singolo layer, caratterizzato da più layer di nodi collegati in modo “feedforward”. In una rete feedforward, le informazioni si muovono in una sola direzione: dal layer di input, attraverso i layer nascosti, al layer di output, senza cicli o loop. Questa struttura è illustrata in Figura 3.5. I layer di rete includono un layer di input per la ricezione dei dati, diversi layer nascosti per l’elaborazione dei dati e un layer di output per la generazione del risultato finale.\nMentre un singolo perceptron è limitato nella sua capacità di modellare pattern complessi, la vera forza delle reti neurali emerge dall’assemblaggio di più layer. Ciascun layer è costituito da numerosi perceptron che lavorano insieme, consentendo alla rete di catturare relazioni intricate e non lineari all’interno dei dati. Con sufficiente profondità e ampiezza, queste reti possono approssimare praticamente qualsiasi funzione, indipendentemente da quanto sia complessa.\n\n\n\n\n\n\nFigura 3.5: Perceptron Multilayer. Fonte: Wikimedia - Charlie.\n\n\n\n\n\n3.2.3 Processo di Training\nUna rete neurale riceve un input, esegue un calcolo e produce una previsione. La previsione è determinata dai calcoli eseguiti all’interno dei set di perceptron trovati tra i layer di input e output. Questi calcoli dipendono principalmente dall’input e dai pesi. Poiché non si ha il controllo sull’input, l’obiettivo durante il training [addestramento] è quello di regolare i pesi in modo tale che l’output della rete fornisca la previsione più accurata.\nIl processo di addestramento prevede diversi passaggi chiave, a partire dal passaggio in avanti (forward), in cui i pesi esistenti della rete vengono utilizzati per calcolare l’output per un dato input. Questo output viene poi confrontato con i veri valori target per calcolare un errore, che misura quanto bene la previsione della rete corrisponde al risultato previsto. In seguito, viene eseguito un passaggio all’indietro (backward). Ciò comporta l’utilizzo dell’errore per apportare modifiche ai pesi della rete tramite un processo chiamato “backpropagation”. Questa regolazione riduce l’errore nelle previsioni successive. Il ciclo di passaggio forward [in avanti], calcolo dell’errore e passaggio backward [all’indietro] viene ripetuto iterativamente. Questo processo continua finché le previsioni della rete non sono sufficientemente accurate o non viene raggiunto un numero predefinito di iterazioni, riducendo al minimo la “funzione di perdita” utilizzata per misurare l’errore.\n\nForward Pass\nIl “forward pass” [passo in avanti] è la fase iniziale in cui i dati si spostano attraverso la rete dal layer di input a quello di output. All’inizio dell’addestramento, i pesi della rete vengono inizializzati in modo casuale, impostando le condizioni iniziali. Durante il “forward pass”, ogni layer esegue calcoli specifici sui dati di input utilizzando questi pesi e il bias, e i risultati vengono poi passati al layer successivo. L’output finale di questa fase è la “prediction” [previsione] della rete. Questa “prediction” viene confrontata con i valori target effettivi presenti nel set di dati per calcolare la “loss” [perdita], che può essere considerata come la differenza tra gli output previsti e i valori target. La perdita quantifica le prestazioni della rete in questa fase, fornendo una metrica cruciale per la successiva regolazione dei pesi durante il backward pass.\nVideo 3.1 di seguito spiega come funzionano le reti neurali utilizzando il riconoscimento delle cifre scritte a mano come applicazione di esempio. Affronta anche la matematica alla base delle reti neurali.\n\n\n\n\n\n\nVideo 3.1: Reti Neurali\n\n\n\n\n\n\n\n\nBackward Pass (Backpropagation)\nDopo aver completato il forward pass e calcolato la perdita, che misura quanto le previsioni del modello si discostano dai valori target effettivi, il passo successivo è migliorare le prestazioni del modello regolando i pesi della rete. Poiché non possiamo controllare gli input del modello, la regolazione dei pesi diventa il nostro metodo principale per perfezionare il modello.\nDeterminiamo come regolare i pesi del nostro modello tramite un algoritmo chiave chiamato “backpropagation”. La backpropagation utilizza la perdita calcolata per determinare il gradiente di ciascun peso. Questi gradienti descrivono la direzione e l’entità in cui i pesi devono essere regolati. Regolando i pesi in base a questi gradienti, il modello è meglio posizionato per fare previsioni più vicine ai valori target effettivi nel successivo “forward pass”.\nComprendere questi concetti fondamentali apre la strada alla comprensione di architetture e tecniche di deep learning più complesse, favorendo lo sviluppo di applicazioni più sofisticate e produttive, in particolare all’interno di sistemi di intelligenza artificiale embedded.\nVideo 3.2 and Video 3.3 build upon Video 3.1. Riguardano la “gradient descent” [discesa del gradiente] e la backpropagation nelle reti neurali.\n\n\n\n\n\n\nVideo 3.2: Gradient descent\n\n\n\n\n\n\n\n\n\n\n\n\nVideo 3.3: Backpropagation\n\n\n\n\n\n\n\n\n\n3.2.4 Architetture dei Modelli\nLe architetture di deep learning si riferiscono ai vari approcci strutturati che stabiliscono come i neuroni e i layer sono organizzati e interagiscono nelle reti neurali. Queste architetture si sono evolute per affrontare efficacemente diversi problemi e diversi tipi di dati. Questa sezione fornisce una panoramica di alcune note architetture di deep learning e delle loro caratteristiche.\n\nMultilayer Perceptron (MLP)\nGli MLP sono architetture di deep learning di base che comprendono tre layer: uno di input, uno o più layer nascosti e un layer di output. Questi layer sono completamente connessi, il che significa che ogni neurone in uno layer è collegato a ogni neurone nei layer precedenti e successivi. Gli MLP possono modellare funzioni complesse e sono utilizzati in varie attività, come regressione, classificazione e riconoscimento di pattern. La loro capacità di apprendere relazioni non lineari tramite backpropagation li rende uno strumento versatile nel toolkit di deep learning.\nNei sistemi di intelligenza artificiale embedded, gli MLP possono funzionare come modelli compatti per attività più semplici come l’analisi dei dati dei sensori o il riconoscimento di pattern di base, in cui le risorse computazionali sono limitate. La loro capacità di apprendere relazioni non lineari con una complessità relativamente minore li rende una scelta adatta per i sistemi embedded.\n\n\n\n\n\n\nEsercizio 3.1: Multilayer Perceptron (MLP)\n\n\n\n\n\nAbbiamo appena scalfito la superficie delle reti neurali. Ora, proveremo ad applicare questi concetti in esempi pratici. Nei notebook Colab forniti, si esploreranno:\nPrevisione dei prezzi delle case: Scoprire come le reti neurali possono analizzare i dati sugli alloggi per stimare i valori delle proprietà. \nClassificazione delle immagini: Scoprire come creare una rete per comprendere il famoso set di dati di cifre scritte a mano MNIST. \nDiagnosi medica nel mondo reale: Usare il deep learning per affrontare l’importante compito della classificazione del cancro al seno. \n\n\n\n\n\nConvolutional Neural Networks (CNNs)\nLe CNN [reti neurali convoluzionali] sono utilizzate principalmente in attività di riconoscimento di immagini e video. Questa architettura è composta da due parti principali: la base convoluzionale e i layer completamente connessi. Nella base convoluzionale, i layer convoluzionali filtrano i dati di input per identificare caratteristiche come bordi, angoli e texture [trame]. Dopo ogni layer convoluzionale, è possibile applicare un layer di pooling [raggruppamento] per ridurre le dimensioni spaziali dei dati, diminuendo così il carico computazionale e concentrando le feature estratte. A differenza degli MLP, che trattano le feature di input come entità piatte e indipendenti, le CNN mantengono le relazioni spaziali tra i pixel, rendendole particolarmente efficaci per i dati di immagini e video. Le feature estratte dalla base convoluzionale vengono poi passate ai layer completamente connessi, simili a quelli utilizzati negli MLP, che eseguono la classificazione in base alle feature estratte dai layer di convoluzione. Le CNN si sono dimostrate altamente efficaci nel riconoscimento delle immagini, nel rilevamento di oggetti e in altre applicazioni di visione artificiale.\nNell’intelligenza artificiale embedded, le CNN sono fondamentali per le attività di riconoscimento di immagini e video, in cui è spesso necessaria l’elaborazione in tempo reale. Possono essere ottimizzate per i sistemi embedded utilizzando tecniche come la quantizzazione e il “pruning” [potatura] per ridurre al minimo l’utilizzo della memoria e le richieste computazionali, consentendo funzionalità efficienti di rilevamento di oggetti e riconoscimento facciale in dispositivi con risorse computazionali limitate.\n\n\n\n\n\n\nEsercizio 3.2: Convolutional Neural Networks (CNNs)\n\n\n\n\n\nAbbiamo discusso del fatto che le CNN [Reti neurali convoluzionali] sono eccellenti nell’identificare le caratteristiche delle immagini, il che le rende ideali per attività come la classificazione degli oggetti. Ora, si potrà mettere in pratica questa conoscenza! Questo notebook Colab si concentra sulla creazione di una CNN per classificare le immagini dal set di dati CIFAR-10, che include oggetti come aeroplani, automobili e animali. Si impareranno le principali differenze tra CIFAR-10 e il set di dati MNIST che abbiamo esplorato in precedenza e come queste differenze influenzano la scelta del modello. Alla fine di questo notebook, avremo compreso le CNN per il riconoscimento delle immagini e saremo sulla buona strada per diventare esperti di TinyML!\n\n\n\n\n\n\nRecurrent Neural Networks (RNN)\nLe RNN [Reti Neurali Ricorrenti] sono adatte per l’analisi di dati sequenziali, come la previsione di serie temporali e l’elaborazione del linguaggio naturale. In questa architettura, le connessioni tra i nodi formano un grafo diretto lungo una sequenza temporale, consentendo il trasporto delle informazioni attraverso le sequenze tramite vettori di stato nascosti. Le varianti delle RNN includono le Long Short-Term Memory (LSTM) e le Gated Recurrent Units (GRU), progettate per catturare dipendenze più lunghe nei dati sequenziali.\nQueste reti possono essere utilizzate nei sistemi di riconoscimento vocale, nella manutenzione predittiva o nei dispositivi IoT in cui sono comuni i pattern di dati sequenziali. Le ottimizzazioni specifiche per le piattaforme embedded possono aiutare a gestirne i requisiti di elaborazione e memoria tipicamente elevati.\n\n\nGenerative Adversarial Network (GAN)\nLe GAN [Reti Generative Avversarie] sono costituite da due reti, un generatore e un discriminatore, addestrate simultaneamente tramite l’addestramento adversarial [avversario] (Goodfellow et al. 2020). Il generatore produce dati che tentano di imitare la distribuzione di quelli reali, mentre il discriminatore distingue tra dati reali e dati generati. Le GAN sono ampiamente utilizzate nella generazione di immagini, nel trasferimento di stile e nell’aumento dei dati.\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Commun. ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\nIn contesti embedded, le reti GAN potrebbero essere utilizzate per l’aumento dei dati sul dispositivo per migliorare il training dei modelli direttamente sul dispositivo embedded, consentendo un apprendimento continuo e un adattamento ai nuovi dati senza la necessità di risorse di cloud computing.\n\n\nAutoencoder\nGli autoencoder sono reti neurali per la compressione dei dati e la riduzione del rumore (Bank, Koenigstein, e Giryes 2023). Sono strutturati per codificare i dati di input in una rappresentazione a dimensione inferiore e quindi decodificarli nella loro forma originale. Varianti come gli Variational Autoencoders (VAE) [Autoencoder Variazionali] introducono livelli probabilistici che consentono proprietà generative, trovando applicazioni nella generazione di immagini e nel rilevamento di anomalie.\n\nBank, Dor, Noam Koenigstein, e Raja Giryes. 2023. «Autoencoders». Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, 353–74.\nL’uso degli autoencoder può aiutare nella trasmissione e nell’archiviazione efficiente dei dati, migliorando le prestazioni complessive dei sistemi embedded con risorse di calcolo e di memoria limitate.\n\n\nTransformer Network\nLe “Transformer network” [reti di trasformatori] sono emerse come un’architettura potente, specialmente nell’elaborazione del linguaggio naturale (Vaswani et al. 2017). Queste reti utilizzano meccanismi di auto-attenzione per soppesare l’influenza di diverse parole di input su ogni parola di output, consentendo il calcolo parallelo e catturando pattern intricati nei dati. Le reti di trasformatori hanno portato a risultati all’avanguardia in attività come la traduzione linguistica, la sintesi e la generazione di testo.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, e Illia Polosukhin. 2017. «Attention is all you need». Adv Neural Inf Process Syst 30.\nQueste reti possono essere ottimizzate per eseguire attività correlate alla lingua direttamente sul dispositivo. Ad esempio, i trasformatori possono essere utilizzati nei sistemi embedded per servizi di traduzione in tempo reale o interfacce assistite dalla voce, dove latenza ed efficienza computazionale sono cruciali. Tecniche come la distillazione del modello possono essere impiegate per distribuire queste reti su dispositivi embedded con risorse limitate.\nQueste architetture servono a scopi specifici ed eccellono in diversi domini, offrendo un ricco toolkit per affrontare diversi problemi nei sistemi di intelligenza artificiale embedded. Comprendere le sfumature di queste architetture è fondamentale nella progettazione di modelli di deep learning efficaci ed efficienti per varie applicazioni.\n\n\n\n3.2.5 ML Tradizionale vs Deep Learning\nIl deep learning estende il machine learning tradizionale utilizzando reti neurali per discernere i pattern nei dati. Al contrario, il machine learning tradizionale si basa su un set di algoritmi consolidati come alberi decisionali, k-nearest neighbor e macchine a vettori di supporto, ma non coinvolge le reti neurali. Per evidenziare brevemente le differenze, Tabella 3.1 illustra le caratteristiche contrastanti tra il ML tradizionale e il deep learning:\n\n\n\nTabella 3.1: Confronto tra machine learning tradizionale e deep learning.\n\n\n\n\n\n\n\n\n\n\nAspetto :================================ Requisiti dei dati\nML tradizionale :========================================================================= Da basso a moderato (efficiente con set di dati più piccoli)\nDeep Learning :================================================================================== Alto (richiede set di dati di grandi dimensioni per un apprendimento adeguato)\n\n\n\n\nComplessità del modello\nModerata (adatta a problemi ben definiti)\nAlta (rileva modelli intricati, adatta a compiti complessi)\n\n\nRisorse di calcolo\nDa basse a moderate (economiche, meno dispendiose in termini di risorse)\nAlta (richiede una potenza di calcolo e risorse sostanziali)\n\n\nVelocità di distribuzione\nVeloce (cicli di training e distribuzione più rapidi)\nLento (tempi di training prolungati, in particolare con set di dati più grandi)\n\n\nInterpretabilità\nAlta (chiare intuizioni sui percorsi decisionali)\nBassa (strutture complesse a layer, natura “scatola nera”)\n\n\nManutenzione\nPiù facile (semplice da aggiornare e mantenere)\nComplesso (richiede più sforzi nella manutenzione e negli aggiornamenti)\n\n\n\n\n\n\n\n\n3.2.6 Scelta tra ML tradizionale e DL\n\nDisponibilità e Volume dei Dati\nQuantità di Dati: Gli algoritmi di machine learning tradizionali, come gli alberi decisionali o Naive Bayes, sono spesso più adatti quando la disponibilità dei dati è limitata. Offrono previsioni affidabili anche con set di dati più piccoli. Ciò è particolarmente vero nella diagnostica medica per la previsione delle malattie e nella segmentazione dei clienti nel marketing.\nDiversità e Qualità dei Dati: Gli algoritmi di machine learning tradizionali spesso funzionano bene con dati strutturati (l’input del modello è un set di funzionalità, idealmente indipendenti l’una dall’altra) ma possono richiedere un notevole sforzo di pre-elaborazione (ad esempio, la “feature engineering” [progettazione delle funzionalità]). D’altro canto, il deep learning adotta l’approccio di eseguire automaticamente la progettazione delle funzionalità come parte dell’architettura del modello. Questo approccio consente la costruzione di modelli end-to-end in grado di mappare direttamente da dati di input non strutturati (come testo, audio e immagini) all’output desiderato senza fare affidamento su euristiche semplicistiche con efficacia limitata. Tuttavia, ciò si traduce in modelli più grandi che richiedono più dati e risorse computazionali. Nei dati rumorosi, la necessità di set di dati più grandi è ulteriormente enfatizzata quando si utilizza il Deep Learning.\n\n\nComplessità del Problema\nGranularità del Problema: I problemi che sono semplici o moderatamente complessi, che possono coinvolgere relazioni lineari o polinomiali tra variabili, spesso trovano una migliore aderenza ai metodi tradizionali di apprendimento automatico.\nRappresentazione Gerarchica delle Feature: I modelli di deep learning sono eccellenti in attività che richiedono una rappresentazione gerarchica delle feature [caratteristiche], come il riconoscimento di immagini e voce. Tuttavia, non tutti i problemi richiedono questa complessità e gli algoritmi tradizionali di apprendimento automatico possono talvolta offrire soluzioni più semplici e ugualmente efficaci.\n\n\nRisorse Hardware e Computazionali\nVincoli di Risorse: La disponibilità di risorse computazionali spesso influenza la scelta tra ML tradizionale e deep learning. Il primo è generalmente meno dispendioso in termini di risorse e quindi preferibile in ambienti con limitazioni hardware o vincoli di budget.\nScalabilità e Velocità: Gli algoritmi tradizionali di apprendimento automatico, come le Support Vector Machines (SVM) [macchine a vettori di supporto ], spesso consentono tempi di training più rapidi e una scalabilità più semplice, il che è particolarmente vantaggioso nei progetti con tempistiche ristrette e volumi di dati in crescita.\n\n\nNormativa di Conformità\nLa conformità normativa è fondamentale in vari settori, e richiede l’aderenza a linee guida e “best practice” come il General Data Protection Regulation (GDPR) [Regolamento generale sulla protezione dei dati] nell’UE. I modelli ML tradizionali, grazie alla loro intrinseca interpretabilità, spesso si allineano meglio a queste normative, soprattutto in settori come la finanza e l’assistenza sanitaria.\n\n\nInterpretabilità\nComprendere il processo decisionale è più facile con le tecniche tradizionali di apprendimento automatico rispetto ai modelli di deep learning, che funzionano come “scatole nere”, rendendo difficile tracciare i percorsi decisionali.\n\n\n\n3.2.7 Fare una Scelta Informata\nConsiderati i vincoli dei sistemi di intelligenza artificiale embedded, comprendere le differenze tra le tecniche di ML tradizionali e il deep learning diventa essenziale. Entrambe le strade offrono vantaggi unici e le loro caratteristiche distintive spesso determinano la scelta dell’una rispetto all’altra in diversi scenari.\nNonostante ciò, il deep learning ha costantemente superato i metodi tradizionali di apprendimento automatico in diverse aree chiave grazie all’abbondanza di dati, ai progressi computazionali e alla comprovata efficacia in attività complesse. Ecco alcuni motivi specifici per cui ci concentriamo sul deep learning:\n\nPrestazioni Superiori in Attività Complesse: I modelli di deep learning, in particolare le reti neurali profonde, eccellono in attività in cui le relazioni tra i punti dati sono incredibilmente intricate. Attività come il riconoscimento di immagini e parlato, la traduzione linguistica e la riproduzione di giochi complessi come Go e Scacchi hanno visto progressi significativi principalmente attraverso algoritmi di deep learning.\nGestione Efficiente dei Dati non Strutturati: A differenza dei metodi tradizionali di apprendimento automatico, il deep learning può elaborare in modo più efficace i dati non strutturati. Ciò è fondamentale nel panorama dei dati odierno, in cui la stragrande maggioranza dei dati, come testo, immagini e video, non è strutturata.\nSfruttamento dei Big Data: Con la disponibilità dei Big Data, i modelli di deep learning possono apprendere e migliorare continuamente. Questi modelli eccellono nell’utilizzare grandi set di dati per migliorare la loro accuratezza predittiva, un limite degli approcci tradizionali di machine-learning.\nProgressi Hardware e Calcolo Parallelo: L’avvento di potenti GPU e la disponibilità di piattaforme di cloud computing hanno consentito il rapido training di modelli di deep learning. Questi progressi hanno affrontato una delle sfide significative del deep learning: la necessità di risorse computazionali sostanziali.\nAdattabilità Dinamica e Apprendimento Continuo: I modelli di deep learning possono adattarsi dinamicamente a nuove informazioni o dati. Possono essere addestrati per generalizzare il loro apprendimento a nuovi dati mai visti, cruciali in campi in rapida evoluzione come la guida autonoma o la traduzione linguistica in tempo reale.\n\nSebbene il deep learning abbia guadagnato una notevole popolarità, è essenziale comprendere che il machine learning tradizionale è ancora rilevante. Man mano che ci addentriamo nei meandri del deep learning, evidenzieremo anche le situazioni in cui i metodi tradizionali di machine learning potrebbero essere più appropriati, grazie alla loro semplicità, efficienza e interpretabilità. Concentrandoci in questo testo sul deep learning, intendiamo fornire ai lettori le conoscenze e gli strumenti per affrontare problemi moderni e complessi in vari ambiti, fornendo al contempo approfondimenti sui vantaggi comparativi e sugli scenari applicativi appropriati per il deep learning e le tecniche tradizionali di machine learning.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html#conclusione",
    "href": "contents/dl_primer/dl_primer.it.html#conclusione",
    "title": "3  Avvio al Deep Learning",
    "section": "3.3 Conclusione",
    "text": "3.3 Conclusione\nIl deep learning è diventato un potente set di tecniche per affrontare le complesse sfide del riconoscimento di pattern e della previsione. Iniziando con una panoramica, abbiamo delineato i concetti e i principi fondamentali che governano il deep learning, gettando le basi per studi più avanzati.\nAl centro del deep learning, abbiamo esplorato le idee di base delle reti neurali, potenti modelli computazionali ispirati alla struttura neuronale interconnessa del cervello umano. Questa esplorazione ci ha permesso di apprezzare le capacità e il potenziale delle reti neurali nella creazione di algoritmi sofisticati in grado di apprendere e adattarsi dai dati.\nComprendere il ruolo delle librerie e dei framework è stata una parte fondamentale della nostra discussione. Abbiamo offerto approfondimenti sugli strumenti che possono facilitare lo sviluppo e l’implementazione di modelli di deep learning. Queste risorse semplificano l’implementazione delle reti neurali e aprono strade all’innovazione e all’ottimizzazione.\nSuccessivamente, abbiamo affrontato le sfide che si potrebbero incontrare quando si racchiudono algoritmi di deep learning nei sistemi embedded, fornendo una prospettiva critica sulle complessità e sulle considerazioni relative all’introduzione dell’intelligenza artificiale nei dispositivi edge.\nInoltre, abbiamo esaminato i limiti del deep learning. Attraverso le discussioni, abbiamo svelato le sfide affrontate nelle applicazioni del deep learning e delineato scenari in cui l’apprendimento automatico tradizionale potrebbe superare il deep learning. Queste sezioni sono fondamentali per promuovere una visione equilibrata delle capacità e dei limiti del deep learning.\nIn questo “Avviamento”, abbiamo fornito le conoscenze per fare scelte informate tra l’implementazione dell’apprendimento automatico tradizionale o delle tecniche di deep learning, a seconda delle esigenze e dei vincoli unici di un problema specifico.\nConcludendo questo capitolo, ci auguriamo che sia stato acquisito il “linguaggio” di base del deep learning e si sia pronti ad approfondire i capitoli successivi con una solida comprensione e una prospettiva critica. Il viaggio che è pieno di entusiasmanti opportunità e sfide nel racchiudere l’intelligenza artificiale nei sistemi.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html#sec-deep-learning-primer-resource",
    "href": "contents/dl_primer/dl_primer.it.html#sec-deep-learning-primer-resource",
    "title": "3  Avvio al Deep Learning",
    "section": "3.4 Risorse",
    "text": "3.4 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nPast, Present, and Future of ML.\nThinking About Loss.\nMinimizing Loss.\nFirst Neural Network.\nUnderstanding Neurons.\nIntro to CLassification.\nTraining, Validation, and Test Data.\nIntro to Convolutions.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 3.1\nVideo 3.2\nVideo 3.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 3.1\nEsercizio 3.2\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\n\nProssimamente.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html",
    "href": "contents/workflow/workflow.it.html",
    "title": "4  Workflow dell’IA",
    "section": "",
    "text": "4.1 Panoramica\nLo sviluppo di un modello di apprendimento automatico di successo richiede un flusso di lavoro sistematico. Questo processo end-to-end consente di creare, distribuire e gestire modelli in modo efficace. Come mostrato in Figura 4.1, in genere prevede i seguenti passaggi chiave:\nSeguire questo flusso di lavoro ML strutturato ci guida attraverso le fasi chiave dello sviluppo. Garantisce di creare modelli efficaci e robusti pronti per la distribuzione nel mondo reale, con conseguenti modelli di qualità superiore che risolvono le varie esigenze.\nIl flusso di lavoro ML è iterativo, richiede un monitoraggio continuo e potenziali aggiustamenti. Ulteriori considerazioni includono:",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#panoramica",
    "href": "contents/workflow/workflow.it.html#panoramica",
    "title": "4  Workflow dell’IA",
    "section": "",
    "text": "Figura 4.1: Metodologia di progettazione multi-step per lo sviluppo di un modello di machine learning. Comunemente denominato ciclo di vita del machine learning\n\n\n\n\n\nDefinizione del Problema - Si inizia articolando chiaramente il problema specifico da risolvere. Questo si concentra sui problemi durante la raccolta dati e la creazione del modello.\nRaccolta e Preparazione dei Dati: Raccogliere dati di training pertinenti e di alta qualità che catturino tutti gli aspetti del problema. Pulire e pre-elaborare i dati per prepararli alla modellazione.\nSelezione e Training del Modello: Scegliere un algoritmo di apprendimento automatico adatto al tipo di problema e ai dati. Considerare i pro e i contro dei diversi approcci. Inserire i dati preparati nel modello per addestrarlo. Il tempo di addestramento varia in base alle dimensioni dei dati e alla complessità del modello.\nValutazione del Modello: Testare il modello addestrato su nuovi dati non ancora esaminati per misurarne l’accuratezza predittiva. Identificare eventuali limitazioni.\nDistribuzione del Modello: Integrare il modello convalidato in applicazioni o sistemi per avviarne l’operatività.\nMonitoraggio e Manutenzione: Tenere traccia delle prestazioni del modello in produzione. Ri-addestrare periodicamente su nuovi dati per mantenerli aggiornati.\n\n\n\n\nControllo della Versione: Tenere traccia delle modifiche al codice e ai dati per riprodurre i risultati e ripristinare le versioni precedenti se necessario.\nDocumentazione: Mantenere una documentazione dettagliata per la comprensione e la riproduzione del flusso di lavoro.\nTest: Testare rigorosamente il flusso di lavoro per garantirne la funzionalità.\nSicurezza: Proteggere il flusso di lavoro e i dati quando si distribuiscono modelli in contesti di produzione.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#ia-tradizionale-o-embedded",
    "href": "contents/workflow/workflow.it.html#ia-tradizionale-o-embedded",
    "title": "4  Workflow dell’IA",
    "section": "4.2 IA Tradizionale o Embedded",
    "text": "4.2 IA Tradizionale o Embedded\nIl flusso di lavoro ML è una guida universale applicabile su diverse piattaforme, tra cui soluzioni basate su cloud, edge computing e TinyML. Tuttavia, il flusso di lavoro per l’IA Embedded introduce complessità e sfide uniche, rendendolo un dominio accattivante e aprendo la strada a innovazioni straordinarie.\n\n4.2.1 Ottimizzazione delle Risorse\n\nFlusso di Lavoro ML Tradizionale: Questo workflow dà priorità all’accuratezza e alle prestazioni del modello, spesso sfruttando abbondanti risorse di calcolo in ambienti cloud o data center.\nFlusso di Lavoro IA Embedded: Dati i vincoli di risorse dei sistemi embedded, questo flusso di lavoro richiede un’attenta pianificazione per ottimizzare le dimensioni del modello e le richieste di calcolo. Tecniche come la quantizzazione e il pruning [potatura] del modello sono fondamentali.\n\n\n\n4.2.2 Elaborazione in Real-time\n\nFlusso di Lavoro ML Tradizionale: Meno enfasi sull’elaborazione in tempo reale, spesso basata sull’elaborazione di dati in batch.\nFlusso di Lavoro IA Embedded: Dà priorità all’elaborazione dei dati in tempo reale, rendendo essenziali bassa latenza ed esecuzione rapida, soprattutto in applicazioni come veicoli autonomi e automazione industriale.\n\n\n\n4.2.3 Gestione dei Dati e Privacy\n\nFlusso di Lavoro ML Tradizionale: Elabora i dati in posizioni centralizzate, spesso richiedendo un ampio trasferimento di dati e concentrandosi sulla sicurezza dei dati durante il transito e l’archiviazione.\nFlusso di Lavoro IA Embedded: Questo workflow sfrutta l’edge computing per elaborare i dati più vicino alla fonte, riducendo la trasmissione dei dati e migliorando la privacy tramite la localizzazione dei dati.\n\n\n\n4.2.4 Integrazione Hardware-Software\n\nFlusso di Lavoro ML Tradizionale: In genere funziona su hardware generico, con sviluppo di software indipendente.\nFlusso di Lavoro IA Embedded: Questo flusso di lavoro prevede un approccio più integrato allo sviluppo hardware e software, spesso incorporando chip personalizzati o acceleratori hardware per ottenere prestazioni ottimali.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#ruoli-e-responsabilità",
    "href": "contents/workflow/workflow.it.html#ruoli-e-responsabilità",
    "title": "4  Workflow dell’IA",
    "section": "4.3 Ruoli e responsabilità",
    "text": "4.3 Ruoli e responsabilità\nLa creazione di una soluzione ML, in particolare per l’intelligenza artificiale embedded, è uno sforzo multidisciplinare che coinvolge vari specialisti. A differenza dello sviluppo software tradizionale, la creazione di una soluzione ML richiede un approccio multidisciplinare a causa della natura sperimentale dello sviluppo del modello e dei requisiti ad alta intensità di risorse per il training e l’implementazione di questi modelli.\nC’è una forte necessità di ruoli incentrati sui dati per il successo delle pipeline di apprendimento automatico. Gli scienziati dei dati e gli ingegneri dei dati gestiscono la raccolta dei dati, creano pipeline di dati e ne garantiscono la qualità. Poiché la natura dei modelli di apprendimento automatico dipende dai dati che consumano, i modelli sono unici e variano a seconda delle diverse applicazioni, il che richiede un’ampia sperimentazione. I ricercatori e gli ingegneri di apprendimento automatico guidano questa fase sperimentale attraverso test continui, convalida e iterazione per ottenere prestazioni ottimali.\nLa fase di implementazione richiede spesso hardware e infrastrutture specializzati, poiché i modelli di machine learning possono essere ad alta intensità di risorse, richiedendo un’elevata potenza di calcolo e una gestione efficiente delle risorse. Ciò richiede la collaborazione con gli ingegneri hardware per garantire che l’infrastruttura possa supportare le esigenze computazionali di training e inferenza del modello.\nPoiché i modelli prendono decisioni che possono avere un impatto sugli individui e sulla società, gli aspetti etici e legali dell’apprendimento automatico stanno diventando sempre più importanti. Sono necessari esperti di etica e consulenti legali per garantire la conformità agli standard etici e alle normative legali.\nTabella 4.1 mostra una panoramica dei ruoli tipici coinvolti. Sebbene i confini tra questi ruoli possano a volte confondersi, la tabella seguente fornisce una panoramica generale.\n\n\n\nTabella 4.1: Ruoli e responsabilità delle persone coinvolte in Operazioni di ML.\n\n\n\n\n\n\n\n\n\nRuolo\nResponsabilità\n\n\n\n\nProject Manager\nSupervisiona il progetto, assicurando che le tempistiche e le milestone siano rispettate.\n\n\nEsperti di Dominio\nOffrono approfondimenti specifici del dominio per definire i requisiti del progetto.\n\n\nData Scientist\nSpecializzati nell’analisi dei dati e nello sviluppo di modelli.\n\n\nIngegneri di Apprendimento Automatico\nConcentrati sullo sviluppo e l’implementazione del modello.\n\n\nData Scientist\nSpecializzati nell’analisi dei dati e nello sviluppo di modelli.\n\n\nEmbedded Systems Engineer\nIntegra modelli ML in sistemi embedded.\n\n\nSoftware Developer\nSviluppa componenti software per l’integrazione del sistema IA.\n\n\nHardware Engineer\nProgetta e ottimizza l’hardware per il sistema AI embedded.\n\n\nUI/UX Designer\nConcentrato sulla progettazione incentrata sull’utente.\n\n\nQA Engineer\nAssicura che il sistema soddisfi gli standard di qualità.\n\n\nEticisti e Consulenti Legali\nConsulenti sulla conformità etica e legale.\n\n\nPersonale Operativo e di Manutenzione\nMonitora e mantiene il sistema distribuito.\n\n\nSpecialisti della sicurezza\nGarantiscono la sicurezza del sistema.\n\n\n\n\n\n\nComprendere questi ruoli è fondamentale per completare un progetto ML. Nei prossimi capitoli esploreremo l’essenza e le competenze di ciascun ruolo, favorendo una comprensione completa delle complessità implicite nei progetti di intelligenza artificiale embedded. Questa visione olistica facilita una collaborazione senza soluzione di continuità e alimenta un ambiente maturo per innovazione e scoperte.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#conclusione",
    "href": "contents/workflow/workflow.it.html#conclusione",
    "title": "4  Workflow dell’IA",
    "section": "4.4 Conclusione",
    "text": "4.4 Conclusione\nQuesto capitolo ha gettato le basi per comprendere il flusso di lavoro dell’apprendimento automatico, un approccio strutturato fondamentale per lo sviluppo, l’implementazione e la manutenzione dei modelli ML. Esplorando le diverse fasi del ciclo di vita ML, abbiamo acquisito informazioni sulle sfide uniche affrontate dai flussi di lavoro ML tradizionali e IA embedded, in particolare in termini di ottimizzazione delle risorse, elaborazione in tempo reale, gestione dei dati e integrazione hardware-software. Queste distinzioni sottolineano l’importanza di adattare i flussi di lavoro per soddisfare le esigenze specifiche dell’ambiente applicativo.\nIl capitolo ha sottolineato l’importanza della collaborazione multidisciplinare nei progetti ML. La comprensione dei diversi ruoli fornisce una visione completa del lavoro di squadra necessario per navigare nella natura sperimentale e ad alta intensità di risorse dello sviluppo ML. Mentre andiamo avanti verso discussioni più dettagliate nei capitoli successivi, questa panoramica di alto livello ci fornisce una prospettiva globale sul flusso di lavoro ML e sui vari ruoli coinvolti.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#sec-ai-workflow-resource",
    "href": "contents/workflow/workflow.it.html#sec-ai-workflow-resource",
    "title": "4  Workflow dell’IA",
    "section": "4.5 Risorse",
    "text": "4.5 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nML Workflow.\nML Lifecycle.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html",
    "href": "contents/data_engineering/data_engineering.it.html",
    "title": "5  Data Engineering",
    "section": "",
    "text": "5.1 Introduzione\nSi immagini un mondo in cui l’intelligenza artificiale può diagnosticare malattie con una precisione senza precedenti, ma solo se i dati utilizzati per addestrarla sono imparziali e affidabili. È qui che entra in gioco il “data engineering” [ingegneria dei dati]. Sebbene oltre il 90% dei dati mondiali sia stato creato negli ultimi due decenni, questa enorme quantità di informazioni è utile solo per creare modelli di intelligenza artificiale efficaci con un’elaborazione e una preparazione adeguate. L’ingegneria dei dati colma questa lacuna trasformando i dati grezzi in un formato di alta qualità che alimenta l’innovazione dell’intelligenza artificiale. Nel mondo odierno basato sui dati, proteggere la privacy degli utenti è fondamentale. Che siano obbligatorie per legge o guidate dalle preoccupazioni degli utenti, le tecniche di anonimizzazione come la privacy differenziale e l’aggregazione sono fondamentali per mitigare i rischi per la privacy. Tuttavia, un’implementazione attenta è fondamentale per garantire che questi metodi non compromettano l’utilità dei dati. I creatori di set di dati affrontano complesse sfide di privacy e rappresentazione quando creano dati di addestramento di alta qualità, in particolare per domini sensibili come l’assistenza sanitaria. Dal punto di vista legale, i creatori potrebbero dover rimuovere identificatori diretti come nomi ed età. Anche senza obblighi legali, la rimozione di tali informazioni può aiutare a creare fiducia negli utenti. Tuttavia, un’eccessiva anonimizzazione può compromettere l’utilità del set di dati. Tecniche come la privacy differenziale\\(^{1}\\), l’aggregazione e la riduzione dei dettagli forniscono alternative per bilanciare privacy e utilità, ma hanno degli svantaggi. I creatori devono trovare un equilibrio ponderato in base al caso d’uso.\nSebbene la privacy sia fondamentale, garantire modelli di intelligenza artificiale equi e solidi richiede di affrontare le lacune (gap) della rappresentazione nei dati. È fondamentale ma non sufficiente garantire la diversità tra variabili individuali come genere, razza e accento. Queste combinazioni, a volte chiamate lacune (gap) di ordine superiore, possono influire in modo significativo sulle prestazioni del modello. Ad esempio, un set di dati medico potrebbe avere dati bilanciati su genere, età e diagnosi individualmente, ma non ha abbastanza casi per catturare donne anziane con una condizione specifica. Tali higher-order gaps [lacune di ordine superiore] non sono immediatamente evidenti, ma possono influire in modo critico sulle prestazioni del modello.\nLa creazione di dati di training utili ed etici richiede una considerazione globale dei rischi per la privacy e delle lacune di rappresentazione. Le soluzioni perfette elusive necessitano di pratiche di ingegneria dei dati coscienziose come l’anonimizzazione, l’aggregazione, il sotto-campionamento di gruppi sovrarappresentati e la generazione di dati sintetizzati per bilanciare esigenze contrastanti. Ciò facilita modelli che sono sia accurati che socialmente responsabili. La collaborazione interfunzionale e i controlli esterni possono anche rafforzare i dati di training. Le sfide sono molteplici ma superabili con uno sforzo ponderato.\nIniziamo discutendo della raccolta dati: Dove reperiamo i dati e come li raccogliamo? Le opzioni spaziano dall’estrazione di dati dal web, all’accesso alle API e all’utilizzo di sensori e dispositivi IoT, fino alla conduzione di sondaggi e alla raccolta di input dagli utenti. Questi metodi riflettono pratiche del mondo reale. Successivamente, approfondiremo l’etichettatura dei dati, tenendo conto anche del coinvolgimento umano. Discuteremo i compromessi e le limitazioni dell’etichettatura umana ed esploreremo i metodi emergenti per l’etichettatura automatizzata. Successivamente, affronteremo la pulizia e la preelaborazione dei dati, un passaggio cruciale ma spesso sottovalutato nella preparazione dei dati grezzi per l’addestramento del modello di intelligenza artificiale. Segue l’aumento dei dati, una strategia per migliorare set di dati limitati generando campioni sintetici. Ciò è particolarmente pertinente per i sistemi embedded, poiché molti casi d’uso necessitano di ampi repository di dati prontamente disponibili per la cura [https://it.wikipedia.org/wiki/Data_curation]. La generazione di dati sintetici emerge come un’alternativa praticabile con vantaggi e svantaggi. Parleremo anche del versioning del dataset, sottolineando l’importanza di tracciare le modifiche dei dati nel tempo. I dati sono in continua evoluzione; quindi, è fondamentale ideare strategie per gestire e archiviare dataset espansivi. Alla fine di questa sezione, si avrà una comprensione completa dell’intera pipeline di dati, dalla raccolta all’archiviazione, essenziale per rendere operativi i sistemi di intelligenza artificiale. Intraprendiamo questo viaggio!",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#definizione-del-problema",
    "href": "contents/data_engineering/data_engineering.it.html#definizione-del-problema",
    "title": "5  Data Engineering",
    "section": "5.2 Definizione del Problema",
    "text": "5.2 Definizione del Problema\nIn molti domini di machine learning, algoritmi sofisticati sono al centro dell’attenzione, mentre l’importanza fondamentale della qualità dei dati viene spesso trascurata. Questa negligenza dà origine alle “Data Cascades” di Sambasivan et al. (2021) (cfr. Figura 5.1)—eventi in cui le carenze nella qualità dei dati si sommano, portando a conseguenze negative a valle come previsioni errate, cessazioni di progetti e persino potenziali danni alle comunità. In Figura 5.1, abbiamo un’illustrazione delle potenziali insidie dei dati in ogni fase e di come influenzano l’intero processo lungo la linea. L’influenza degli errori nella raccolta dei dati è particolarmente pronunciata. Eventuali lacune in questa fase diventeranno evidenti in fasi successive (nella valutazione e nell’implementazione del modello) e potrebbero comportare conseguenze costose, come l’abbandono dell’intero modello e il riavvio da zero. Pertanto, investire in tecniche di data engineering sin dall’inizio ci aiuterà a rilevare gli errori in anticipo.\n\n\n\n\n\n\nFigura 5.1: Data cascades: costi composti. Fonte: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. «“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI». In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–15.\n\n\nNonostante molti professionisti del ML riconoscano l’importanza dei dati, altri segnalano di dover affrontare queste “cascate”. Ciò evidenzia un problema sistemico: mentre il fascino dello sviluppo di modelli avanzati rimane, i dati spesso devono essere maggiormente apprezzati.\nPrendiamo, ad esempio, le “Keyword Spotting” (KWS) (cfr. Figura 5.2). KWS è un ottimo esempio di TinyML in azione ed è una tecnologia fondamentale alla base delle interfacce di abilitazione alla voce su dispositivi endpoint come gli smartphone. Questi sistemi, che in genere funzionano come motori leggeri di “wake-word” [parole di attivazione], sono costantemente attivi, in ascolto di una frase specifica per attivare ulteriori azioni. Quando diciamo “OK, Google” o “Alexa”, questo avvia un processo su un microcontrollore embedded nel dispositivo. Nonostante le loro risorse limitate, questi microcontrollori svolgono un ruolo importante nel consentire interazioni vocali senza interruzioni con i dispositivi, spesso operando in ambienti con elevato rumore ambientale. L’unicità della wake-word aiuta a ridurre al minimo i falsi positivi, assicurando che il sistema non venga attivato inavvertitamente.\nÈ importante comprendere che queste tecnologie di individuazione delle “parole chiave” non sono isolate; si integrano perfettamente in sistemi più grandi, elaborando segnali in modo continuo e gestendo al contempo un basso consumo energetico. Questi sistemi vanno oltre il semplice riconoscimento delle parole chiave, evolvendosi per facilitare diversi rilevamenti di suoni, come la rottura di un vetro. Questa evoluzione è orientata alla creazione di dispositivi intelligenti in grado di comprendere e rispondere ai comandi vocali, annunciando un futuro in cui anche gli elettrodomestici possono essere controllati tramite interazioni vocali.\n\n\n\n\n\n\nFigura 5.2: Esempio di individuazione delle “Keyword Spotting”: interazione con Alexa. Fonte: Amazon.\n\n\n\nCreare un modello KWS affidabile è un compito complesso. Richiede una profonda comprensione dello scenario di distribuzione, che comprenda dove e come funzioneranno questi dispositivi. Ad esempio, l’efficacia di un modello KWS non riguarda solo il riconoscimento di una parola; riguarda la sua distinzione tra vari accenti e rumori di sottofondo, che si tratti di un bar affollato o del suono stridulo di una televisione in un soggiorno o in una cucina dove questi dispositivi sono comunemente presenti. Riguarda la garanzia che un sussurrato “Alexa” nel cuore della notte o un urlato “OK Google” in un mercato rumoroso vengano riconosciuti con la stessa precisione.\nInoltre, molti degli attuali assistenti vocali KWS supportano un numero limitato di lingue, lasciando una parte sostanziale della diversità linguistica mondiale non rappresentata. Questa limitazione è in parte dovuta alla difficoltà di raccogliere e monetizzare i dati per le lingue parlate da popolazioni più piccole. La distribuzione “long-tail” [https://it.wikipedia.org/wiki/Coda_lunga] delle lingue implica che molte lingue hanno dati limitati, rendendo difficile lo sviluppo di tecnologie di supporto.\nQuesto livello di accuratezza e robustezza dipende dalla disponibilità e dalla qualità dei dati, dalla capacità di etichettare correttamente i dati e dalla trasparenza dei dati per l’utente finale prima che vengano utilizzati per addestrare il modello. Tuttavia, tutto inizia con una chiara comprensione della dichiarazione o definizione del problema.\nIn genere, in ML, la definizione del problema ha alcuni passaggi chiave:\n\nIdentificare chiaramente la definizione del problema\nDefinire obiettivi chiari\nStabilire un benchmark [riferimento] di successo\nComprendere l’impegno/l’uso dell’utente finale\nComprendere i vincoli e le limitazioni dell’implementazione\nSeguito infine dalla raccolta dati.\n\nUna solida base di progetto è essenziale per la sua traiettoria e il suo successo finale. Al centro di questa base c’è innanzitutto l’identificazione di un problema chiaro, come garantire che i comandi vocali nei sistemi di assistenza vocale siano riconosciuti in modo coerente in diversi ambienti. Obiettivi chiari, come la creazione di set di dati rappresentativi per scenari diversi, forniscono una direzione unificata. I benchmark, come l’accuratezza del sistema nel rilevamento delle parole chiave, offrono risultati misurabili per valutare i progressi. Il coinvolgimento delle parti interessate, dagli utenti finali agli investitori, fornisce informazioni preziose e garantisce l’allineamento con le esigenze del mercato. Inoltre, quando si esplorano ambiti come l’assistenza vocale, è importante comprendere i limiti della piattaforma. I sistemi embedded, come i microcontrollori, sono dotati di limitazioni intrinseche di potenza di elaborazione, memoria ed efficienza energetica. Riconoscere queste limitazioni garantisce che le funzionalità, come il rilevamento delle parole chiave, siano personalizzate per funzionare in modo ottimale, bilanciando le prestazioni col risparmio delle risorse.\nIn questo contesto, usando KWS come esempio, possiamo suddividere ciascuno dei passaggi come segue:\n\nIdentificazione del Problema: In sostanza, KWS rileva parole chiave specifiche tra suoni ambientali e altre parole pronunciate. Il problema principale è progettare un sistema in grado di riconoscere queste parole chiave con elevata accuratezza, bassa latenza e minimi falsi positivi o negativi, soprattutto se distribuito su dispositivi con risorse di elaborazione limitate.\nImpostazione di Obiettivi Chiari: Gli obiettivi per un sistema KWS potrebbero includere:\n\nRaggiungimento di un tasso di accuratezza specifico (ad esempio, accuratezza del 98% nel rilevamento delle parole chiave).\nGaranzia di bassa latenza (ad esempio, rilevamento delle parole chiave e risposta entro 200 millisecondi).\nRiduzione al minimo del consumo di energia per estendere la durata della batteria sui dispositivi embedded.\nGaranzia che le dimensioni del modello siano ottimizzate per la memoria disponibile sul dispositivo.\n\nBenchmark per il successo: Stabilire metriche chiare per misurare il successo del sistema KWS. Questo potrebbe includere:\n\nTasso di Veri Positivi: La percentuale di parole chiave identificate correttamente.\nTasso di Falsi Positivi: La percentuale di parole chiave non identificate erroneamente come parole chiave.\nTempo di Risposta: Il tempo impiegato dall’enunciazione della parola chiave alla risposta del sistema.\nConsumo Energetico: Potenza media utilizzata durante il rilevamento della parola chiave.\n\nCoinvolgimento e Comprensione delle Parti Interessate:: Coinvolgere le parti interessate, tra cui produttori di dispositivi, sviluppatori di hardware e software e utenti finali. Comprendere le loro esigenze, capacità e vincoli. Ad esempio:\n\nI produttori di dispositivi potrebbero dare priorità al basso consumo energetico.\nGli sviluppatori di software potrebbero enfatizzare la facilità di integrazione.\nGli utenti finali darebbero priorità all’accuratezza e alla reattività.\n\nComprensione dei Vincoli e delle Limitazioni dei Sistemi Embedded: I dispositivi embedded presentano una serie di problematiche:\n\nLimiti della Memoria: I modelli KWS devono essere leggeri per adattarsi ai vincoli di memoria dei dispositivi embedded. In genere, i modelli KWS devono essere piccoli quanto 16 KB per adattarsi alla “isola always-on” [porzione sempre attiva] del SoC. Inoltre, questa è solo la dimensione del modello. Anche il codice applicativo aggiuntivo per la pre-elaborazione potrebbe dover rientrare nei vincoli di memoria.\nPotenza di Elaborazione: Le capacità di calcolo dei dispositivi embedded sono limitate (alcune centinaia di MHz di velocità di clock), quindi il modello KWS deve essere ottimizzato per l’efficienza.\nConsumo Energetico: Poiché molti dispositivi embedded sono alimentati a batteria, il sistema KWS deve essere efficiente dal punto di vista energetico.\nVincoli Ambientali: I dispositivi potrebbero essere distribuiti in vari ambienti, dalle silenziose camere da letto agli ambienti industriali rumorosi. Il sistema KWS deve essere sufficientemente robusto per funzionare efficacemente in questi scenari.\n\nRaccolta e Analisi dei Dati: Per un sistema KWS, la qualità e la diversità dei dati sono fondamentali. Le considerazioni potrebbero includere:\n\nVarietà di Accenti: Raccogliere dati da parlanti con accenti diversi per garantire un riconoscimento ad ampio raggio.\nRumori di Sottofondo: Includere campioni di dati con diversi rumori ambientali per addestrare il modello per scenari del mondo reale.\nVariazioni delle Parole Chiave: Le persone potrebbero pronunciare le parole chiave in modo diverso o avere leggere variazioni nella parola di attivazione stessa. Assicurarsi che il set di dati catturi queste sfumature.\n\nFeedback e Perfezionamento Iterativo: Una volta sviluppato un prototipo di sistema KWS, è fondamentale testarlo in scenari del mondo reale, raccogliere feedback e perfezionare iterativamente il modello. Ciò garantisce che il sistema rimanga allineato con il problema e gli obiettivi definiti. Ciò è importante perché gli scenari di distribuzione cambiano nel tempo man mano che le cose si evolvono.\n\n\n\n\n\n\n\nEsercizio 5.1: Keyword Spotting con TensorFlow Lite Micro\n\n\n\n\n\nEsplorare una guida pratica per la creazione e l’implementazione di sistemi Keyword Spotting (KWS) utilizzando TensorFlow Lite Micro. Seguire i passaggi dalla raccolta dati all’addestramento del modello e all’implementazione nei microcontrollori. Imparare a creare modelli KWS efficienti che riconoscono parole chiave specifiche in mezzo al rumore di fondo. Perfetto per chi è interessato all’apprendimento automatico sui sistemi embedded. Sbloccare il potenziale dei dispositivi “voice-enabled” con TensorFlow Lite Micro!\n\n\n\n\nIl capitolo corrente sottolinea il ruolo essenziale della qualità dei dati nell’apprendimento automatico, utilizzando come esempio i sistemi Keyword Spotting (KWS). Descrive i passaggi chiave, dalla definizione del problema al coinvolgimento delle parti interessate, sottolineando il feedback iterativo. Il prossimo capitolo approfondirà la gestione della qualità dei dati, discutendone le conseguenze e le tendenze future, concentrandosi sull’importanza di dati diversificati e di alta qualità nello sviluppo di sistemi di intelligenza artificiale, affrontando considerazioni etiche e metodi di reperimento dei dati.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#ricerca-dei-dati.",
    "href": "contents/data_engineering/data_engineering.it.html#ricerca-dei-dati.",
    "title": "5  Data Engineering",
    "section": "5.3 Ricerca dei Dati.",
    "text": "5.3 Ricerca dei Dati.\nLa qualità e la diversità dei dati raccolti sono importanti per sviluppare sistemi di intelligenza artificiale accurati e robusti. Il reperimento di dati di training di alta qualità richiede un’attenta considerazione degli obiettivi, delle risorse e delle implicazioni etiche. I dati possono essere ottenuti da varie fonti a seconda delle esigenze del progetto:\n\n5.3.1 Dataset preesistenti\nPiattaforme come Kaggle e UCI Machine Learning Repository forniscono un comodo punto di partenza. I dataset preesistenti sono preziosi per ricercatori, sviluppatori e aziende. Uno dei loro principali vantaggi è l’efficienza dei costi. Creare un set di dati da zero può richiedere molto tempo ed essere costoso, quindi accedere a dati già pronti può far risparmiare risorse significative. Inoltre, molti set di dati, come ImageNet, sono diventati parametri di riferimento standard nella comunità di apprendimento automatico, consentendo confronti di prestazioni coerenti tra diversi modelli e algoritmi. Questa disponibilità di dati significa che gli esperimenti possono essere avviati immediatamente senza ritardi nella raccolta e nella preelaborazione dei dati. In un campo in rapida evoluzione come il ML, questa praticità è importante.\nLa garanzia di qualità che deriva dai dataset preesistenti più diffusi è importante da considerare perché diversi set di dati contengono errori. Ad esempio, nel set di dati ImageNet è stato riscontrato oltre il 6,4% di errori. Dato il loro uso diffuso, la comunità spesso identifica e corregge eventuali errori o distorsioni in questi set di dati. Questa garanzia è particolarmente utile per studenti e nuovi arrivati nel campo, in quanto possono concentrarsi sull’apprendimento e sulla sperimentazione senza preoccuparsi dell’integrità dei dati. La documentazione di supporto che spesso accompagna i set di dati esistenti è inestimabile, sebbene ciò si applichi generalmente solo a quelli ampiamente utilizzati. Una buona documentazione fornisce approfondimenti sul processo di raccolta dati e sulle definizioni delle variabili e talvolta offre persino prestazioni del modello di base. Queste informazioni non solo aiutano la comprensione, ma promuovono anche la riproducibilità nella ricerca, un pilastro dell’integrità scientifica; attualmente, c’è una crisi attorno al miglioramento della riproducibilità nei sistemi di apprendimento automatico. Quando altri ricercatori hanno accesso agli stessi dati, possono convalidare i risultati, testare nuove ipotesi o applicare metodologie diverse, consentendoci così di basarci più rapidamente sul lavoro reciproco.\nSebbene piattaforme come Kaggle e UCI Machine Learning Repository siano risorse inestimabili, è essenziale comprendere il contesto in cui sono stati raccolti i dati. I ricercatori dovrebbero fare attenzione al potenziale “overfitting” quando utilizzano set di dati popolari, poiché potrebbero essere stati addestrati più modelli su di essi, portando a metriche di prestazioni gonfiate. A volte, questi set di dati non riflettono i dati del mondo reale.\nInoltre, in questi set di dati possono esserci problemi di distorsione, validità e riproducibilità e negli ultimi anni si è sviluppata una crescente consapevolezza di questi problemi. Inoltre, l’utilizzo dello stesso set di dati per addestrare modelli diversi, come mostrato in Figura 5.3, può talvolta creare un disallineamento: addestrare più modelli utilizzando lo stesso set di dati comporta un “disallineamento” tra i modelli e il mondo, in cui un intero ecosistema di modelli riflette solo un sottoinsieme ristretto di dati del mondo reale.\n\n\n\n\n\n\nFigura 5.3: Addestrare modelli diversi con lo stesso set di dati. Fonte: (icons from left to right: Becris; Freepik; Freepik; Paul J; SBTS2018).\n\n\n\n\n\n5.3.2 Web Scraping\nIl “web scraping” si riferisce a tecniche automatizzate per l’estrazione di dati dai siti Web. In genere comporta l’invio di richieste HTTP ai server Web, il recupero di contenuti HTML e l’analisi di tali contenuti per estrarre informazioni rilevanti. Gli strumenti e i framework più diffusi per il web scraping includono Beautiful Soup, Scrapy e Selenium. Questi strumenti offrono diverse funzionalità, dall’analisi dei contenuti HTML all’automazione delle interazioni con i browser Web, in particolare per i siti Web che caricano i contenuti in modo dinamico tramite JavaScript.\nIl web scraping può raccogliere efficacemente grandi set di dati per l’addestramento di modelli di apprendimento automatico, in particolare quando i dati etichettati da esseri umani sono scarsi. Per la ricerca sulla visione artificiale, il web scraping consente la raccolta di enormi volumi di immagini e video. I ricercatori hanno utilizzato questa tecnica per creare set di dati influenti come ImageNet e OpenImages. Ad esempio, si potrebbero effettuare scraping di siti di e-commerce per accumulare foto di prodotti per il riconoscimento di oggetti o piattaforme di social media per raccogliere caricamenti di utenti per l’analisi facciale. Anche prima di ImageNet, il progetto LabelMe di Stanford ha raschiato (scraped) Flickr per oltre 63.000 immagini annotate che coprono centinaia di categorie di oggetti.\nOltre alla visione artificiale, lo scraping web supporta la raccolta di dati testuali per il linguaggio naturale. I ricercatori possono “raschiare” siti di notizie per dati di analisi del “sentiment”, forum e siti di recensioni per la ricerca sui sistemi di dialogo o social media per la modellazione di argomenti. Ad esempio, i dati di training per il chatbot ChatGPT sono stati ottenuti tramite scraping di gran parte dell’Internet pubblico. I repository GitHub sono stati sottoposti a scraping per addestrare l’assistente di codifica Copilot AI di GitHub.\nIl web scraping può anche raccogliere dati strutturati, come prezzi delle azioni, dati meteorologici o informazioni sui prodotti, per applicazioni analitiche. Una volta che i dati sono stati “raschiati”, è essenziale archiviarli in modo strutturato, spesso utilizzando database o data warehouse. Una corretta gestione dei dati garantisce l’usabilità dei dati raccolti per analisi e applicazioni future.\nTuttavia, mentre il web scraping offre numerosi vantaggi, ci sono limitazioni significative e considerazioni etiche da sostenere. Non tutti i siti Web consentono lo scraping e la violazione di queste restrizioni può portare a ripercussioni legali. Anche lo scraping di materiale protetto da copyright o comunicazioni private è immorale e potenzialmente illegale. Il web scraping etico impone l’aderenza al file ‘robots.txt’ di un sito web, che delinea le sezioni del sito a cui è possibile accedere e che possono essere scansionate dai bot automatizzati.\nPer scoraggiare lo scraping automatizzato, molti siti web implementano limiti di velocità. Se un bot invia troppe richieste in un breve periodo, potrebbe essere temporaneamente bloccato, limitando la velocità di accesso ai dati. Inoltre, la natura dinamica dei contenuti web implica che i dati estratti a intervalli diversi potrebbero richiedere maggiore coerenza, ponendo sfide per gli studi a lungo termine. Tuttavia, ci sono tendenze emergenti come la Web Navigation in cui gli algoritmi di apprendimento automatico possono navigare automaticamente nel sito web per accedere ai contenuti dinamici.\nIl volume di dati pertinenti disponibili per lo scraping potrebbe essere limitato per argomenti di nicchia. Ad esempio, mentre lo scraping per argomenti comuni come immagini di gatti e cani potrebbe produrre dati abbondanti, la ricerca di condizioni mediche rare potrebbe essere meno fruttuosa. Inoltre, i dati ottenuti tramite scraping sono spesso non strutturati e rumorosi, il che richiede un’accurata pre-elaborazione e pulizia. È fondamentale comprendere che non tutti i dati raccolti saranno di alta qualità o accuratezza. L’impiego di metodi di verifica, come il riferimento incrociato con fonti alternative di dati, può migliorare l’affidabilità dei dati.\nQuando si esegue lo scraping di dati personali, sorgono problemi di privacy, sottolineando la necessità di anonimizzazione. Pertanto, è fondamentale aderire ai “Termini del Servizio” di un sito Web, limitare la raccolta di dati a quelli di dominio pubblico e garantire l’anonimato di tutti i dati personali acquisiti.\nMentre il web scraping può essere un metodo scalabile per accumulare grandi set di dati di training per sistemi di intelligenza artificiale, la sua applicabilità è limitata a tipi di dati specifici. Ad esempio, il web scraping rende più complessa la ricerca di dati per unità di misura inerziali (IMU) per il riconoscimento dei gesti. Al massimo, si può effettuare lo scraping di un set di dati esistente.\nLa raccolta dal Web può produrre dati incoerenti o imprecisi. Ad esempio, la foto in Figura 5.4 viene visualizzata quando si cerca “semaforo” su Google Images. È un’immagine del 1914 che mostra semafori obsoleti, che sono anche appena distinguibili a causa della scarsa qualità dell’immagine. Questo può essere problematico per i set di dati estratti dal Web, poiché lo inquina con campioni di dati non applicabili (vecchi).\n\n\n\n\n\n\nFigura 5.4: Un’immagine di vecchi semafori (1914). Fonte: Vox.\n\n\n\n\n\n\n\n\n\nEsercizio 5.2: Web Scraping\n\n\n\n\n\nScoprire la potenza del web scraping con Python usando librerie come Beautiful Soup e Pandas. Questo esercizio estrarrà la documentazione Python per i nomi e le descrizioni delle funzioni ed esplorerà le statistiche dei giocatori NBA. Alla fine, si avranno le competenze per estrarre e analizzare dati da siti Web reali. Pronti all’immersione? Accedere al notebook Google Colab qui sotto e iniziare a fare pratica!\n\n\n\n\n\n\n5.3.3 Crowdsourcing\nIl crowdsourcing per i dataset è la pratica di ottenere dati utilizzando i servizi di molte persone, sia da una comunità specifica che dal pubblico in generale, in genere tramite Internet. Invece di affidarsi a un piccolo team o a un’organizzazione specifica per raccogliere o etichettare i dati, il crowdsourcing sfrutta lo sforzo collettivo di un vasto gruppo distribuito di partecipanti. Servizi come Amazon Mechanical Turk consentono la distribuzione di attività di annotazione a una forza lavoro ampia e diversificata. Questo facilita la raccolta di etichette per attività complesse come l’analisi del “sentiment” o il riconoscimento delle immagini che richiedono il giudizio umano.\nIl crowdsourcing è emerso come un approccio efficace per la raccolta di dati e la risoluzione dei problemi. Uno dei principali vantaggi del crowdsourcing è la scalabilità: distribuendo le attività a un ampio pool globale di collaboratori su piattaforme digitali, i progetti possono elaborare rapidamente enormi volumi di dati. Ciò rende il crowdsourcing ideale per l’etichettatura, la raccolta e l’analisi di dati su larga scala.\nInoltre, il crowdsourcing attinge a un gruppo eterogeneo di partecipanti, apportando un’ampia gamma di prospettive, intuizioni culturali e capacità linguistiche che possono arricchire i dati e migliorare la risoluzione creativa dei problemi in modi che un gruppo più omogeneo potrebbe non fare. Poiché il crowdsourcing attinge da un vasto pubblico oltre i canali tradizionali, è più conveniente rispetto ai metodi convenzionali, soprattutto per microattività più semplici.\nLe piattaforme di crowdsourcing consentono anche una grande flessibilità, poiché i parametri delle attività possono essere modificati in tempo reale in base ai risultati iniziali. Ciò crea un ciclo di feedback per miglioramenti iterativi al processo di raccolta dati. I lavori complessi possono essere suddivisi in microattività e distribuiti a più persone, con risultati convalidati in modo incrociato assegnando versioni ridondanti della stessa attività. Se gestito in modo ponderato, il crowdsourcing consente il coinvolgimento della comunità attorno a un progetto collaborativo, in cui i partecipanti trovano una ricompensa nel contribuire.\nTuttavia, mentre il crowdsourcing offre numerosi vantaggi, è essenziale affrontarlo con una strategia chiara. Mentre fornisce l’accesso a un set diversificato di annotatori, introduce anche variabilità nella qualità delle annotazioni. Inoltre, piattaforme come Mechanical Turk potrebbero non sempre catturano uno spettro demografico completo; spesso, gli individui esperti di tecnologia sono sovra-rappresentati, mentre i bambini e gli anziani potrebbero essere sotto-rappresentati. Fornire istruzioni chiare e formazione per gli annotatori è fondamentale. Controlli periodici e convalide dei dati etichettati aiutano a mantenere la qualità. Ciò si ricollega all’argomento della chiara definizione del problema di cui abbiamo discusso in precedenza. Il crowdsourcing per i set di dati richiede anche una particolare attenzione alle considerazioni etiche. È fondamentale assicurarsi che i partecipanti siano informati su come verranno utilizzati i loro dati e che la loro privacy sia protetta. Il controllo di qualità tramite protocolli dettagliati, trasparenza nell’approvvigionamento e verifica è essenziale per garantire risultati affidabili.\nPer TinyML, il crowdsourcing può presentare alcune sfide uniche. I dispositivi TinyML sono altamente specializzati per attività particolari entro vincoli rigorosi. Di conseguenza, i dati di cui hanno bisogno tendono a essere molto specifici. Ottenere tali dati specializzati da un pubblico generico può essere difficile tramite crowdsourcing. Ad esempio, le applicazioni TinyML spesso si basano su dati raccolti da determinati sensori o hardware. Il crowdsourcing richiederebbe ai partecipanti di avere accesso a dispositivi molto specifici e coerenti, come i microfoni, con le stesse frequenze di campionamento. Queste sfumature hardware presentano ostacoli anche per semplici attività audio come l’individuazione di parole chiave.\nOltre all’hardware, i dati stessi necessitano di elevata granularità e qualità, dati i limiti di TinyML. Può essere difficile garantire ciò quando si fa crowdsourcing da chi non ha familiarità con il contesto e i requisiti dell’applicazione. Ci sono anche potenziali problemi relativi alla privacy, alla raccolta in tempo reale, alla standardizzazione e alle competenze tecniche. Inoltre, la natura ristretta di molte attività TinyML semplifica l’etichettatura accurata dei dati con la giusta comprensione. I partecipanti potrebbero aver bisogno di un contesto completo per fornire annotazioni affidabili.\nPertanto, mentre il crowdsourcing può funzionare bene in molti casi, le esigenze specializzate di TinyML introducono sfide uniche per i dati. È richiesta un’attenta pianificazione per linee guida, targeting e controllo di qualità. Per alcune applicazioni, il crowdsourcing potrebbe essere fattibile, ma altre potrebbero richiedere più lavoro per la raccolta dati più mirati per ottenere dati di training pertinenti e di alta qualità.\n\n\n5.3.4 Dati Sintetici\nLa generazione di dati sintetici può essere utile per affrontare alcune delle limitazioni della raccolta dati. Comporta la creazione di dati che non sono stati originariamente catturati o osservati, ma vengono generati utilizzando algoritmi, simulazioni o altre tecniche per assomigliare ai dati del mondo reale. Come mostrato in Figura 5.5, i dati sintetici vengono uniti ai dati storici e quindi utilizzati come input per l’addestramento del modello. È diventato uno strumento prezioso in vari campi, in particolare quando i dati del mondo reale sono scarsi, costosi o eticamente difficili (ad esempio, TinyML). Varie tecniche, come le “Generative Adversarial Network” (GAN) [reti generative avversarie], possono produrre dati sintetici di alta qualità quasi indistinguibili dai dati reali. Queste tecniche hanno fatto notevoli progressi, rendendo la generazione di dati sintetici sempre più realistica e affidabile.\nPotrebbe essere necessario disporre di più dati del mondo reale per l’analisi o l’addestramento di modelli di apprendimento automatico in molti domini, in particolare quelli emergenti. I dati sintetici possono colmare questa lacuna producendo grandi volumi di dati che imitano scenari del mondo reale. Ad esempio, rilevare il suono di un vetro che si rompe potrebbe essere difficile nelle applicazioni di sicurezza in cui un dispositivo TinyML sta cercando di identificare le effrazioni. La raccolta di dati del mondo reale richiederebbe la rottura di numerose finestre, il che è poco pratico e costoso.\nInoltre, avere un set di dati diversificato è fondamentale nell’apprendimento automatico, in particolare nel deep learning. I dati sintetici possono aumentare i set di dati esistenti introducendo varianti, migliorando così la robustezza dei modelli. Ad esempio, SpecAugment è un’eccellente tecnica di aumento dei dati per i sistemi di “Automatic Speech Recognition” (ASR).\nAnche la privacy e la riservatezza sono grandi problemi. I set di dati contenenti informazioni sensibili o personali sollevano problemi di privacy quando vengono condivisi o utilizzati. I dati sintetici, essendo generati artificialmente, non hanno questi legami diretti con individui reali, consentendo un utilizzo più sicuro preservando al contempo le proprietà statistiche essenziali.\nLa generazione di dati sintetici, in particolare una volta stabiliti i meccanismi di generazione, può essere un’alternativa più conveniente. I dati sintetici eliminano la necessità di rompere più finestre per raccogliere dati rilevanti nello scenario applicativo di sicurezza di cui sopra.\nMolti casi d’uso embedded riguardano situazioni uniche, come gli impianti di produzione, che sono difficili da simulare. I dati sintetici consentono ai ricercatori il controllo completo sul processo di generazione dei dati, consentendo la creazione di scenari o condizioni specifici che sono difficili da catturare nella vita reale.\nSebbene i dati sintetici offrano numerosi vantaggi, è essenziale utilizzarli giudiziosamente. Bisogna fare attenzione a garantire che i dati generati rappresentino accuratamente le distribuzioni sottostanti del mondo reale e non introducano distorsioni indesiderate.\n\n\n\n\n\n\nFigura 5.5: Aumento delle dimensioni dei dati di training con la generazione di dati sintetici. Fonte: AnyLogic.\n\n\n\n\n\n\n\n\n\nEsercizio 5.3: Dati Sintetici\n\n\n\n\n\nScopriamo la generazione di dati sintetici utilizzando le Generative Adversarial Network (GAN) su dati tabellari. Adotteremo un approccio pratico, immergendoci nel funzionamento del modello CTGAN e applicandolo al set di dati Synthea dal dominio sanitario. Dalla pre-elaborazione dei dati al training e valutazione del modello, procederemo passo dopo passo, imparando come creare dati sintetici, valutarne la qualità e sbloccare il potenziale delle GAN per l’aumento dei dati e le applicazioni del mondo reale.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#archiviazione-dati",
    "href": "contents/data_engineering/data_engineering.it.html#archiviazione-dati",
    "title": "5  Data Engineering",
    "section": "5.4 Archiviazione Dati",
    "text": "5.4 Archiviazione Dati\nL’approvvigionamento e l’archiviazione dei dati vanno di pari passo e i dati devono essere archiviati in un formato che faciliti l’accesso e l’elaborazione. A seconda del caso d’uso, possono essere utilizzati vari tipi di sistemi di archiviazione dati per archiviare i set di dati. Alcuni esempi sono mostrati in Tabella 5.1.\n\n\n\nTabella 5.1: Panoramica comparativa del database, del data warehouse e del data lake.\n\n\n\n\n\n\n\n\n\n\nDatabase\nData Warehouse\nData Lake\n\n\n\n\nScopo\nOperativo e transazionale\nAnalitico\n\n\nTipo di dati\nStrutturato\nStrutturato, semi-strutturato e/o non strutturato\n\n\nScala\nDa piccoli a grandi volumi di dati\nGrandi volumi di dati integrati Grandi volumi di dati diversi\n\n\nEsempi\nMySQL\nGoogle BigQuery, Amazon Redshift, Microsoft Azure Synapse, Google Cloud Storage, AWS S3, Azure Data Lake Storage\n\n\n\n\n\n\nI dati archiviati sono spesso accompagnati da metadati, definiti come “dati sui dati”. Forniscono informazioni contestuali dettagliate sui dati, come mezzi di creazione dei dati, ora di creazione, licenza di utilizzo dei dati allegata, ecc. Ad esempio, Hugging Face ha Dataset Cards. Per promuovere un uso responsabile dei dati, i creatori di dataset dovrebbero rivelare potenziali “bias” [pregiudizi] tramite le “dataset cards” [schede dei dataset]. Queste schede possono istruire gli utenti sui contenuti e le limitazioni di un dataset. Le schede forniscono anche un contesto essenziale sull’uso appropriato del dataset evidenziando bias [pregiudizi] e altri dettagli importanti. Avere questo tipo di metadati può anche consentire un rapido recupero se strutturato correttamente. Una volta che il modello è stato sviluppato e distribuito su dispositivi edge, i sistemi di archiviazione possono continuare a memorizzare dati in arrivo, aggiornamenti del modello o risultati analitici.\nData Governance: Con una grande quantità di archiviazione dati, è anche fondamentale disporre di policy e pratiche (ad esempio, “governance” [gestione] dei dati) che aiutino a gestire i dati durante il loro ciclo di vita, dall’acquisizione allo smaltimento. La governance dei dati descrive il modo in cui i dati vengono gestiti e include l’adozione di decisioni chiave in merito al loro accesso e controllo. Figura 5.6 illustra i diversi domini coinvolti nella governance dei dati. Implica l’esercizio dell’autorità e l’assunzione di decisioni sui dati per mantenerne la qualità, garantire la conformità, mantenere la sicurezza e ricavarne valore. La governance dei dati è resa operativa sviluppando politiche, incentivi e sanzioni, coltivando una cultura che percepisce i dati come un bene prezioso. Procedure specifiche e autorità assegnate vengono implementate per salvaguardare la qualità dei dati e monitorarne l’utilizzo e i rischi correlati.\nLa governance dei dati utilizza tre approcci integrativi: pianificazione e controllo, organizzativo e basato sul rischio.\n\nL’approccio di pianificazione e controllo, comune nell’IT, allinea business e tecnologia attraverso cicli annuali e continui aggiustamenti, concentrandosi su una governance verificabile e basata su policy.\nL’approccio organizzativo enfatizza la struttura, stabilendo ruoli autorevoli come Chief Data Officer e garantendo responsabilità e rendicontazione nella governance.\nL’approccio basato sul rischio, intensificato dai progressi dell’IA, si concentra sull’identificazione e la gestione dei rischi intrinseci nei dati e negli algoritmi. Affronta in particolare i problemi specifici dell’IA attraverso valutazioni regolari e strategie di gestione proattiva del rischio, consentendo azioni incidentali e preventive per mitigare gli impatti indesiderati degli algoritmi.\n\n\n\n\n\n\n\nFigura 5.6: Una panoramica del framework di governance dei dati. Fonte: StarCIO..\n\n\n\nEcco alcuni esempi di governance dei dati in diversi settori:\n\nMedicina: Gli Health Information Exchanges (HIE) [scambi di informazioni sanitarie] consentono la condivisione di informazioni sanitarie tra diversi operatori sanitari per migliorare l’assistenza ai pazienti. Implementano rigorose pratiche di governance dei dati per mantenere l’accuratezza, l’integrità, la privacy e la sicurezza dei dati, rispettando normative come l’Health Insurance Portability and Accountability Act (HIPAA). Le policy di governance assicurano che i dati dei pazienti siano condivisi solo con entità autorizzate e che i pazienti possano controllare l’accesso alle proprie informazioni.\nFinanza: Basilea III Framework è un quadro normativo internazionale per le banche. Garantisce che le banche stabiliscano policy, pratiche e responsabilità chiare per la gestione dei dati, assicurandone accuratezza, completezza e tempestività. Non solo consente alle banche di soddisfare la conformità normativa, ma previene anche le crisi finanziarie gestendo i rischi in modo più efficace.\nGoverno: Le agenzie governative che gestiscono i dati dei cittadini, i registri pubblici e le informazioni amministrative implementano la governance dei dati per gestire i dati in modo trasparente e sicuro. Il sistema di previdenza sociale negli Stati Uniti e il sistema Aadhar in India sono buoni esempi di tali sistemi di governance.\n\nConsiderazioni speciali sull’archiviazione dei dati per TinyML\nFormati di Archiviazione Audio Efficienti: I sistemi di individuazione delle parole chiave necessitano di formati di archiviazione audio specializzati per consentire una rapida ricerca delle parole chiave nei dati audio. I formati tradizionali come WAV e MP3 archiviano forme d’onda audio complete, che richiedono un’elaborazione estesa per la ricerca. L’individuazione delle parole chiave utilizza un archivio compresso ottimizzato per la ricerca basata su frammenti. Un approccio consiste nell’archiviazione di caratteristiche acustiche compatte anziché audio grezzo. Tale flusso di lavoro implicherebbe:\n\nEstrazione di Caratteristiche Acustiche: I coefficienti Mel-frequency cepstral (MFCC) rappresentano comunemente importanti caratteristiche audio.\nCreazione di Embedding: Gli “embedding” trasformano le caratteristiche acustiche estratte in spazi vettoriali continui, consentendo un’archiviazione dei dati più compatta e rappresentativa. Questa rappresentazione è essenziale per convertire dati ad alta dimensionalità, come l’audio, in un formato più gestibile ed efficiente per l’elaborazione e l’archiviazione.\nQuantizzazione vettoriale: Questa tecnica rappresenta dati ad alta dimensionalità, come gli embedding, con vettori a bassa dimensionalità, riducendo le esigenze di archiviazione. Inizialmente, un codebook viene generato dai dati di training per definire un set di vettori di codice che rappresentano i vettori di dati originali. Successivamente, ogni vettore di dati viene abbinato alla “codeword” più vicina in base al codebook, garantendo una perdita minima di informazioni.\nArchiviazione sequenziale: L’audio viene frammentato in frame brevi e le feature [caratteristiche] quantizzate (o embedded) per ogni frame vengono archiviate in sequenza per mantenere l’ordine temporale, preservando la coerenza e il contesto dei dati audio.\n\nQuesto formato consente di decodificare le feature frame per frame per la corrispondenza delle parole chiave. La ricerca delle caratteristiche è più rapida della decompressione dell’audio completo.\nSelective Network Output Storage: [Archiviazione selettiva dell’output di rete] Un’altra tecnica per ridurre l’archiviazione consiste nell’eliminare le caratteristiche audio intermedie archiviate durante l’addestramento ma non richieste durante l’inferenza. La rete viene eseguita su audio completo durante l’addestramento. Tuttavia, solo gli output finali vengono archiviati durante l’inferenza.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#elaborazione-dei-dati",
    "href": "contents/data_engineering/data_engineering.it.html#elaborazione-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.5 Elaborazione dei Dati",
    "text": "5.5 Elaborazione dei Dati\nIl “Data processing” elaborazione dei dati si riferisce ai passaggi necessari per trasformare i dati grezzi in un formato adatto per l’inserimento negli algoritmi di apprendimento automatico. È una fase cruciale in qualsiasi flusso di lavoro ML, ma spesso trascurata. Con un’elaborazione dei dati adeguata, è probabile che i modelli ML raggiungano prestazioni ottimali. Figura 5.7 mostra una ripartizione dell’allocazione del tempo di uno scienziato dei dati, evidenziando la parte significativa spesa per la pulizia e l’organizzazione dei dati (%60).\n\n\n\n\n\n\nFigura 5.7: Ripartizione delle attività dei “Data scientist” in base al tempo impiegato. Fonte: Forbes.\n\n\n\nUna corretta pulizia dei dati è un passaggio cruciale che influisce direttamente sulle prestazioni del modello. I dati del mondo reale sono spesso sporchi, contengono errori, valori mancanti, rumore, anomalie e incongruenze. La pulizia dei dati comporta il rilevamento e la correzione di questi problemi per preparare dati di alta qualità per la modellazione. Selezionando attentamente le tecniche appropriate, i data scientist possono migliorare l’accuratezza del modello, ridurre l’overfitting e addestrare gli algoritmi per apprendere pattern più solidi. Nel complesso, un’elaborazione dei dati ponderata consente ai sistemi di apprendimento automatico di scoprire meglio le informazioni e di fare previsioni dai dati del mondo reale.\nI dati spesso provengono da fonti diverse e possono essere non strutturati o semi-strutturati. Pertanto, elaborarli e standardizzarli è essenziale, assicurando che aderiscano a un formato uniforme. Tali trasformazioni possono includere:\n\nNormalizzazione di variabili numeriche\nCodifica di variabili categoriali\nUtilizzo di tecniche come la riduzione della dimensionalità\n\nLa convalida dei dati svolge un ruolo più ampio rispetto alla garanzia di aderenza a determinati standard, come impedire che i valori di temperatura scendano sotto lo zero assoluto. Questi problemi si verificano in TinyML perché i sensori potrebbero funzionare male o produrre temporaneamente letture errate; tali transienti non sono rari. Pertanto, è fondamentale rilevare gli errori nei dati in anticipo prima che si propaghino attraverso la pipeline dei dati. Rigorosi processi di convalida, tra cui la verifica delle pratiche di annotazione iniziali, il rilevamento di valori anomali e la gestione dei valori mancanti tramite tecniche come l’imputazione della media, contribuiscono direttamente alla qualità dei set di dati. Ciò, a sua volta, influisce sulle prestazioni, la correttezza e la sicurezza dei modelli addestrati su di essi. Diamo un’occhiata a Figura 5.8 per un esempio di pipeline di elaborazione dei dati. Nel contesto di TinyML, il Multilingual Spoken Words Corpus (MSWC) è un esempio di pipeline di elaborazione dei dati, flussi di lavoro sistematici e automatizzati per la trasformazione, l’archiviazione e l’elaborazione dei dati. I dati di input (che sono una raccolta di brevi registrazioni) attraversano diverse fasi di elaborazione, come l’allineamento audio-parola e l’estrazione di parole chiave. Semplificando il flusso di dati, dai dati grezzi ai set di dati utilizzabili, le pipeline di dati migliorano la produttività e facilitano lo sviluppo rapido di modelli di apprendimento automatico. MSWC è una raccolta ampia e in continua espansione di registrazioni audio di parole pronunciate in 50 lingue diverse, utilizzate collettivamente da oltre 5 miliardi di persone. Questo set di dati è destinato allo studio accademico e all’uso aziendale in aree come l’identificazione di parole chiave e la ricerca basata sul parlato. È concesso in licenza aperta con Creative Commons Attribution 4.0 per un ampio utilizzo.\n\n\n\n\n\n\nFigura 5.8: Una panoramica della pipeline di elaborazione dati del Multilingual Spoken Words Corpus (MSWC). Fonte: Mazumder et al. (2021).\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. «Multilingual spoken words corpus». In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\n\nIl MSWC ha utilizzato un metodo di allineamento forzato per estrarre automaticamente singole registrazioni di parole per addestrare modelli di individuazione delle parole chiave dal progetto Common Voice, che presenta registrazioni a livello di frase in crowdsourcing. L’allineamento forzato si riferisce a metodi di lunga data nell’elaborazione del parlato che prevedono quando fenomeni del parlato come sillabe, parole o frasi iniziano e finiscono all’interno di una registrazione audio. Nei dati MSWC, le registrazioni in crowdsourcing spesso presentano rumori di sottofondo, come elettricità statica e vento. A seconda dei requisiti del modello, questi rumori possono essere rimossi o mantenuti intenzionalmente.\nMantenere l’integrità dell’infrastruttura dati è uno lavoro continuo. Ciò comprende archiviazione dei dati, sicurezza, gestione degli errori e rigoroso controllo delle versioni. Gli aggiornamenti periodici sono fondamentali, soprattutto in ambiti dinamici come l’individuazione delle parole chiave, per adattarsi alle tendenze linguistiche in evoluzione e alle integrazioni dei dispositivi.\nC’è un boom nelle pipeline di elaborazione dati, comunemente presenti nelle toolchain delle operazioni ML, di cui parleremo nel capitolo MLOps. In breve, questi includono framework come MLOps di Google Cloud. Fornisce metodi per l’automazione e il monitoraggio in tutte le fasi della costruzione del sistema ML, tra cui integrazione, test, rilascio, distribuzione e gestione dell’infrastruttura. Diversi meccanismi si concentrano sull’elaborazione dati, parte integrante di questi sistemi.\n\n\n\n\n\n\nEsercizio 5.4: Elaborazione dei Dati\n\n\n\n\n\nEsploriamo due progetti significativi nell’elaborazione dei dati vocali e nell’apprendimento automatico. MSWC è un vasto set di dati audio con oltre 340.000 parole chiave e 23,4 milioni di esempi parlati di 1 secondo. Viene utilizzato in varie applicazioni come dispositivi voice-enabled e automazione dei call center. Il progetto Few-Shot Keyword Spotting introduce un nuovo approccio per l’individuazione delle parole chiave in diverse lingue, ottenendo risultati impressionanti con dati di addestramento minimi. Esamineremo il set di dati MSWC, impareremo come strutturarlo in modo efficace e poi addestreremo un modello di individuazione di parole chiave con la tecnica “few-shot” [https://www.ibm.com/it-it/topics/few-shot-learning]. Cominciamo!",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#etichettatura-dei-dati",
    "href": "contents/data_engineering/data_engineering.it.html#etichettatura-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.6 Etichettatura dei Dati",
    "text": "5.6 Etichettatura dei Dati\nIl “Data labeling” etichettatura dei dati è importante per creare set di dati di training di alta qualità per modelli di apprendimento automatico. Le etichette forniscono informazioni di base, consentendo ai modelli di apprendere relazioni tra input e output desiderati. Questa sezione copre considerazioni chiave per la selezione di tipi di etichette, formati e contenuti per acquisire le informazioni necessarie per le attività. Discute approcci di annotazione comuni, dall’etichettatura manuale al crowdsourcing ai metodi assistiti dall’intelligenza artificiale, e le “best practice” per garantire la qualità delle etichette tramite formazione, linee guida e controlli di qualità. Sottolineiamo anche il trattamento etico degli annotatori umani. Viene anche esplorata l’integrazione dell’intelligenza artificiale per accelerare e aumentare l’annotazione umana. Comprendere le esigenze, le sfide e le strategie di etichettatura è essenziale per costruire dataset affidabili e utili per addestrare sistemi di apprendimento automatico performanti e affidabili.\n\n5.6.1 Tipi di Etichette\nLe etichette contengono informazioni su attività o concetti chiave. Figura 5.9 include alcuni tipi di etichette comuni: una “classification label” [etichetta di classificazione] viene utilizzata per categorizzare le immagini con etichette (etichettando un’immagine con “dog” [cane] e presenta un cane); un “bounding box” [riquadro delimitatore] identifica la posizione dell’oggetto (disegnando un riquadro attorno al cane); una “segmentation map” [mappa di segmentazione] classifica gli oggetti a livello di pixel (evidenziando il cane con un colore distinto); una “caption” [didascalia] fornisce annotazioni descrittive (descrivendo le azioni, la posizione, il colore, ecc. del cane); e una “transcript” [trascrizione] denota il contenuto audio. La scelta del formato dell’etichetta dipende dal caso d’uso e dai vincoli di risorse, poiché etichette più dettagliate richiedono un lavoro maggiore per la raccolta (Johnson-Roberson et al. 2017).\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, e Ram Vasudevan. 2017. «Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?» In 2017 IEEE International Conference on Robotics and Automation (ICRA), 746–53. Singapore, Singapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\n\n\n\n\n\nFigura 5.9: Una panoramica dei tipi di etichette comuni.\n\n\n\nA meno che non si concentri sull’apprendimento auto-supervisionato, un set di dati fornirà probabilmente etichette che affrontano una o più attività di interesse. Date le loro limitazioni di risorse uniche, i creatori di set di dati devono considerare quali informazioni le etichette dovrebbero catturare e come possono ottenere praticamente le etichette necessarie. I creatori devono prima decidere quali tipi di etichette di contenuto dovrebbero catturare. Ad esempio, un creatore interessato al rilevamento delle auto vorrebbe etichettare le auto nel suo dataset. Tuttavia, potrebbe anche considerare se raccogliere simultaneamente etichette per altre attività per cui il set di dati potrebbe essere potenzialmente utilizzato, come il rilevamento dei pedoni.\nInoltre, gli annotatori possono fornire metadati per le informazioni su come il set di dati rappresenta diverse caratteristiche di interesse (cfr. Sezione 5.9). Il dataset Common Voice, ad esempio, include vari tipi di metadati che forniscono informazioni sugli oratori, sulle registrazioni e sulla qualità del set di dati per ciascuna lingua rappresentata (Ardila et al. 2020). Includono suddivisioni demografiche che mostrano il numero di registrazioni per fascia di età e genere del parlante. Questo ci consente di vedere chi ha contribuito alle registrazioni per ogni lingua. Includono anche statistiche come la durata media delle registrazioni e il numero totale di ore di registrazioni convalidate. Queste forniscono informazioni sulla natura e le dimensioni dei set di dati per ogni lingua. Inoltre, le metriche di controllo qualità come la percentuale di registrazioni convalidate sono utili per sapere quanto siano completi e puliti i set di dati. I metadati includono anche suddivisioni demografiche normalizzate scalate al 100% per il confronto tra le lingue. Questo evidenzia le differenze di rappresentazione tra lingue con risorse più elevate e più basse.\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, e Gregor Weber. 2020. «Common Voice: A Massively-Multilingual Speech Corpus». In Proceedings of the Twelfth Language Resources and Evaluation Conference, 4218–22. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\nSuccessivamente, i creatori devono determinare il formato di tali etichette. Ad esempio, un creatore interessato al rilevamento delle auto potrebbe scegliere tra etichette di classificazione binaria che indicano se è presente un’auto, riquadri di delimitazione che mostrano le posizioni generali di tutte le auto o etichette di segmentazione pixel per pixel che mostrano la posizione esatta di ogni auto. La scelta del formato dell’etichetta può dipendere dal caso d’uso e dai vincoli di risorse, poiché le etichette più dettagliate sono in genere più costose e richiedono più tempo per essere acquisite.\n\n\n5.6.2 Metodi di Annotazione\nGli approcci comuni all’annotazione includono etichettatura manuale, crowdsourcing e tecniche semi-automatiche. L’etichettatura manuale da parte di esperti produce alta qualità ma necessita di maggiore scalabilità. Il crowdsourcing consente ai non esperti di distribuire annotazioni, spesso tramite piattaforme dedicate (Sheng e Zhang 2019). Metodi debolmente supervisionati e programmatici possono ridurre il lavoro manuale generando etichette in modo euristico o automatico (Ratner et al. 2018).\n\nSheng, Victor S., e Jing Zhang. 2019. «Machine Learning with Crowdsourcing: A Brief Summary of the Past Research and Future Directions». Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 9837–43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, e Christopher Ré. 2018. «Snorkel MeTaL: Weak Supervision for Multi-Task Learning». In Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning. ACM. https://doi.org/10.1145/3209889.3209898.\nDopo aver deciso il contenuto e il formato desiderati per le etichette, i creatori iniziano il processo di annotazione. Per raccogliere un gran numero di etichette da annotatori umani, i creatori si affidano spesso a piattaforme di annotazione dedicate, che possono metterli in contatto con team di annotatori umani. Quando utilizzano queste piattaforme, i creatori potrebbero aver bisogno di maggiori informazioni sui background degli annotatori e sui livelli di esperienza con argomenti di interesse. Tuttavia, alcune piattaforme offrono l’accesso ad annotatori con competenze specifiche (ad esempio, medici).\n\n\n\n\n\n\nEsercizio 5.5: Etichette Autoprodotte\n\n\n\n\n\nEsploriamo Wake Vision, un set di dati completo progettato per il rilevamento di persone con TinyML. Questo set di dati deriva da un set di dati più ampio e generico, Open Images (Kuznetsova et al. 2020), e specificamente adattato per il rilevamento binario di persone.\nIl processo di trasformazione comporta il filtraggio e la rietichettatura delle etichette e dei riquadri di delimitazione esistenti in Open Images utilizzando una pipeline automatizzata. Questo metodo non solo consente di risparmiare tempo e risorse, ma garantisce anche che il set di dati soddisfi i requisiti specifici delle applicazioni TinyML.\nInoltre, generiamo metadati per confrontare la correttezza e la robustezza dei modelli in scenari difficili.\nCominciamo!\n\n\n\n\n\nKuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. «The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale». International journal of computer vision 128 (7): 1956–81.\n\n\n5.6.3 Garantire la Qualità dell’Etichetta\nNon vi è alcuna garanzia che le etichette dei dati siano effettivamente corrette. Figura 5.10 mostra alcuni esempi di casi di etichettatura rigida: alcuni errori derivano da immagini sfocate che le rendono difficili da identificare (l’immagine della rana), e altri derivano da una mancanza di conoscenza del dominio (il caso della cicogna nera). È possibile che nonostante le migliori istruzioni fornite agli etichettatori, etichettino ancora in modo errato alcune immagini (Northcutt, Athalye, e Mueller 2021). Strategie come controlli di qualità, formazione degli annotatori e raccolta di più etichette per ciascun elemento possono aiutare a garantire la qualità delle etichette. Per attività ambigue, più annotatori possono aiutare a identificare i punti dati controversi e quantificare i livelli di disaccordo.\n\n\n\n\n\n\nFigura 5.10: Alcuni esempi di casi di etichettatura rigida. Fonte: Northcutt, Athalye, e Mueller (2021).\n\n\nNorthcutt, Curtis G, Anish Athalye, e Jonas Mueller. 2021. «Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks». arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite.\n\n\nQuando si lavora con annotatori umani, è importante offrire un compenso equo e dare priorità al trattamento etico, poiché gli annotatori possono essere sfruttati o danneggiati durante il processo di etichettatura (Perrigo, 2023). Ad esempio, se è probabile che un set di dati contenga contenuti inquietanti, gli annotatori potrebbero trarre vantaggio dall’avere la possibilità di visualizzare le immagini in scala di grigi (Google, s.d.).\n\nGoogle. s.d. «Information quality content moderation». https://blog.google/documents/83/.\n\n\n5.6.4 Annotazione assistita dall’intelligenza artificiale\nIl ML ha una domanda insaziabile di dati. Pertanto, sono necessari più dati. Ciò solleva la questione di come possiamo ottenere più dati etichettati. Invece di generare e curare sempre i dati manualmente, possiamo fare affidamento sui modelli di intelligenza artificiale esistenti per etichettare i set di dati in modo più rapido ed economico, anche se spesso con una qualità inferiore rispetto all’annotazione umana. Questo può essere fatto in vari modi come mostrato in Figura 5.11, tra cui i seguenti:\n\nPre-annotazione: I modelli di intelligenza artificiale possono generare etichette preliminari per un set di dati utilizzando metodi come l’apprendimento semi-supervisionato (Chapelle, Scholkopf, e Zien 2009), che gli esseri umani possono poi esaminare e correggere. Questo può far risparmiare una notevole quantità di tempo, soprattutto per set di dati di grandi dimensioni.\nApprendimento attivo: I modelli di intelligenza artificiale possono identificare i dati più informativi in un dataset, che possono quindi essere riordinati per priorità per l’annotazione umana. Questo può aiutare a migliorare la qualità del set di dati etichettato riducendo al contempo il tempo di annotazione complessivo.\nControllo qualità: I modelli di intelligenza artificiale possono identificare e segnalare potenziali errori nelle annotazioni umane, contribuendo a garantire l’accuratezza e la coerenza del set di dati etichettato.\n\n\nChapelle, O., B. Scholkopf, e A. Zien Eds. 2009. «Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]». IEEE Trans. Neural Networks 20 (3): 542–42. https://doi.org/10.1109/tnn.2009.2015974.\nEcco alcuni esempi di come l’annotazione assistita dall’intelligenza artificiale è stata proposta come utile:\n\nImmagini mediche: L’annotazione assistita dall’intelligenza artificiale etichetta le immagini mediche, come scansioni MRI (Magnetic Resonance Imaging) e raggi X (Krishnan, Rajpurkar, e Topol 2022). Annotare attentamente i set di dati medici è estremamente impegnativo, soprattutto su larga scala, poiché gli esperti del settore sono scarsi e diventano costosi. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale per diagnosticare malattie e altre condizioni mediche in modo più accurato ed efficiente.\nAuto a guida autonoma: L’annotazione assistita dall’intelligenza artificiale viene utilizzata per etichettare immagini e video di auto a guida autonoma. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale per identificare oggetti sulla strada, come altri veicoli, pedoni e segnali stradali.\nSocial media: L’annotazione assistita dall’intelligenza artificiale etichetta i post sui social media come immagini e video. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale a identificare e classificare diversi tipi di contenuti, come notizie, pubblicità e post personali.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, e Eric J. Topol. 2022. «Self-supervised learning in medicine and healthcare». Nat. Biomed. Eng. 6 (12): 1346–52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\n\n\n\n\nFigura 5.11: Strategie per acquisire ulteriori dati di addestramento etichettati. Fonte: Standford AI Lab.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#controllo-della-versione-dei-dati",
    "href": "contents/data_engineering/data_engineering.it.html#controllo-della-versione-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.7 Controllo della Versione dei Dati",
    "text": "5.7 Controllo della Versione dei Dati\nI sistemi di produzione sono costantemente inondati da volumi di dati fluttuanti e in aumento, che determinano la rapida comparsa di numerose repliche di dati. Questi dati in aumento servono come base per l’addestramento di modelli di apprendimento automatico. Ad esempio, un’azienda di vendita globale impegnata nella previsione delle vendite riceve continuamente dati sul comportamento dei consumatori. Allo stesso modo, i sistemi sanitari che formulano modelli predittivi per la diagnosi delle malattie acquisiscono costantemente nuovi dati sui pazienti. Le applicazioni TinyML, come l’individuazione delle parole chiave, sono molto affamate di dati per quanto riguarda la quantità di dati generati. Di conseguenza, è fondamentale un monitoraggio meticoloso delle versioni dei dati e delle prestazioni del modello corrispondente.\nIl “Data Version Control” [controllo delle versioni dei dati] offre una metodologia strutturata per gestire in modo efficiente alterazioni e versioni di set di dati. Facilita il monitoraggio delle modifiche, conserva più versioni e garantisce riproducibilità e tracciabilità nei progetti incentrati sui dati. Inoltre, il controllo delle versioni dei dati offre la versatilità di rivedere e utilizzare versioni specifiche in base alle necessità, garantendo che ogni fase dell’elaborazione dei dati e dello sviluppo del modello possa essere riesaminata e verificata in modo preciso e semplice. Ha una varietà di usi pratici -\nGestione del Rischio: Il controllo della versione dei dati consente trasparenza e responsabilità monitorando le versioni del set di dati.\nCollaborazione ed Efficienza: Un facile accesso a diverse versioni del set di dati in un unico posto può migliorare la condivisione dei dati di controllo specifici e consentire una collaborazione efficiente.\nRiproducibilità: Il controllo della versione dei dati consente di monitorare le prestazioni dei modelli riguardanti diverse versioni dei dati, e quindi di abilitare la riproducibilità.\nConcetti Chiave\n\nCommit: È un’istantanea immutabile dei dati in un momento specifico, che rappresenta una versione univoca. Ogni commit è associato a un identificatore univoco per consentire\nBranch: I “branch” [rami] consentono a sviluppatori e specialisti dei data di discostarsi dalla linea di sviluppo principale e continuare a lavorare in modo indipendente senza influenzare altri rami. Ciò è particolarmente utile quando si sperimentano nuove funzionalità o modelli, consentendo sviluppo e sperimentazione paralleli senza il rischio di danneggiare il ramo principale stabile.\nMerge: I “Merge” [unioni] aiutano a integrare le modifiche da rami diversi mantenendo l’integrità dei dati.\n\nCon il controllo della versione dei dati in atto, possiamo tracciare le modifiche mostrate in Figura 5.12, riprodurre i risultati precedenti ripristinando le versioni precedenti e collaborare in modo sicuro ramificando e isolando le modifiche.\n\n\n\n\n\n\nFigura 5.12: Data versioning.\n\n\n\nSistemi di Data Version Control più Diffusi\nDVC: È un’abbreviazione di “Data Version Control” ed è uno strumento open source e leggero che funziona su Git Hub e supporta tutti i tipi di formati di dati. Può integrarsi perfettamente nel flusso di lavoro se Git viene utilizzato per gestire il codice. Cattura le versioni dei dati e dei modelli nei commit Git mentre li archivia in locale o sul cloud (ad esempio, AWS, Google Cloud, Azure). Questi dati e modelli (ad esempio, artefatti di ML) sono definiti nei file di metadati, che vengono aggiornati a ogni commit. Può consentire il monitoraggio delle metriche dei modelli su diverse versioni dei dati.\nlakeFS: È uno strumento open source che supporta il controllo della versione dei dati sui “data lake”. Supporta molte operazioni simili a git, come i “branch” e il “merge” dei dati, nonché il ripristino delle versioni precedenti dei dati. Ha anche una funzionalità UI unica, che semplifica notevolmente l’esplorazione e la gestione dei dati.\nGit LFS: È utile per il controllo della versione dei dataset di dimensioni ridotte. Utilizza le funzionalità di “branch” e “merge” native di Git, ma è limitato nel tracciamento delle metriche, nel ripristino delle versioni precedenti o nell’integrazione con i “data lake”.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#ottimizzazione-dei-dati-per-lia-embedded",
    "href": "contents/data_engineering/data_engineering.it.html#ottimizzazione-dei-dati-per-lia-embedded",
    "title": "5  Data Engineering",
    "section": "5.8 Ottimizzazione dei Dati per l’IA Embedded",
    "text": "5.8 Ottimizzazione dei Dati per l’IA Embedded\nI creatori che lavorano su sistemi embedded potrebbero avere priorità insolite quando puliscono i loro dataset. Da un lato, i modelli potrebbero essere sviluppati per casi d’uso insolitamente specifici, che richiedono un filtraggio intensivo dei dataset. Mentre altri modelli di linguaggio naturale possono essere in grado di trasformare qualsiasi discorso in testo, un modello per un sistema embedded può essere incentrato su un singolo compito limitato, come il rilevamento di una parola chiave. Di conseguenza, i creatori possono filtrare in modo aggressivo grandi quantità di dati perché devono affrontare un determinato compito. Un sistema di intelligenza artificiale embedded può anche essere legato a specifici dispositivi hardware o ambienti. Ad esempio, un modello video potrebbe dover elaborare immagini da un singolo tipo di telecamera, che verrà montata solo sui campanelli nei quartieri residenziali. In questo scenario, i creatori possono scartare le immagini se provengono da un diverso tipo di telecamera, mostrano il tipo sbagliato di scenario o sono state scattate dall’altezza o dall’angolazione sbagliate.\nD’altra parte, ci si aspetta spesso che i sistemi di IA embedded forniscano prestazioni particolarmente accurate in contesti imprevedibili del mondo reale. Ciò può portare i creatori a progettare set di dati per rappresentare variazioni nei potenziali input e promuovere la robustezza del modello. Di conseguenza, possono definire un ambito ristretto per il loro progetto ma poi puntare a una copertura approfondita entro quei limiti. Ad esempio, i creatori del modello del campanello menzionato sopra potrebbero provare a coprire le variazioni nei dati derivanti da:\n\nQuartieri geograficamente, socialmente e architettonicamente diversi\nDiversi tipi di illuminazione artificiale e naturale\nDiverse stagioni e condizioni meteorologiche\nOstruzioni (ad esempio gocce di pioggia o scatole di consegna che oscurano la visuale della telecamera)\n\nCome descritto sopra, i creatori possono prendere in considerazione il crowdsourcing o la generazione sintetica di dati per includere queste varianti.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#sec-data-transparency",
    "href": "contents/data_engineering/data_engineering.it.html#sec-data-transparency",
    "title": "5  Data Engineering",
    "section": "5.9 Trasparenza dei Dati",
    "text": "5.9 Trasparenza dei Dati\nFornendo una documentazione chiara e dettagliata, i creatori possono aiutare gli sviluppatori a capire come utilizzare al meglio i loro set di dati. Diversi gruppi hanno suggerito formati di documentazione standardizzati per i set di dati, come Data Cards (Pushkarna, Zaldivar, e Kjartansson 2022), datasheet (Gebru et al. 2021), data statement (Bender e Friedman 2018), o Data Nutrition Labels (Holland et al. 2020). Quando rilasciano un dataset, i creatori possono descrivere quali tipi di dati hanno raccolto, come li hanno raccolti ed etichettati e quali tipi di casi d’uso potrebbero essere adatti o meno al set di dati. Quantitativamente, potrebbe essere opportuno mostrare quanto bene il set di dati rappresenti gruppi diversi (ad esempio, gruppi di genere diversi, telecamere diverse).\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, e Kate Crawford. 2021. «Datasheets for datasets». Commun. ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\nBender, Emily M., e Batya Friedman. 2018. «Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science». Transactions of the Association for Computational Linguistics 6 (dicembre): 587–604. https://doi.org/10.1162/tacl_a_00041.\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, e Kasia Chmielinski. 2020. «The Dataset Nutrition Label: A Framework to Drive Higher Data Quality Standards». In Data Protection and Privacy. Hart Publishing. https://doi.org/10.5040/9781509932771.ch-001.\nFigura 5.13 mostra un esempio di una scheda dati per un set di dati di computer vision (CV). Include alcune informazioni di base sul set di dati e istruzioni su come utilizzarlo, inclusi i “bias” noti.\n\n\n\n\n\n\nFigura 5.13: Data card che descrive un dataset CV. Fonte: Pushkarna, Zaldivar, e Kjartansson (2022).\n\n\nPushkarna, Mahima, Andrew Zaldivar, e Oddur Kjartansson. 2022. «Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI». In 2022 ACM Conference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nTenere traccia della provenienza dei dati, essenzialmente le origini e il viaggio di ogni dato attraverso la pipeline dei dati, non è solo una buona pratica, ma un requisito essenziale per la qualità. La provenienza dei dati contribuisce in modo significativo alla trasparenza dei sistemi di machine learning. I sistemi trasparenti semplificano l’analisi dei dati, consentendo una migliore identificazione e rettifica di errori, bias o incongruenze. Ad esempio, se un modello di ML addestrato su dati medici non è performante in aree specifiche, tracciare la provenienza può aiutare a identificare se il problema riguarda i metodi di raccolta dati, i gruppi demografici rappresentati nei dati o altri fattori. Questo livello di trasparenza non aiuta solo a eseguire il debug del sistema, ma svolge anche un ruolo cruciale nel migliorare la qualità complessiva dei dati. Migliorando l’affidabilità e la credibilità del set di dati, la provenienza dei dati migliora anche le prestazioni del modello e la sua accettabilità tra gli utenti finali.\nQuando si produce la documentazione, i creatori devono anche specificare come gli utenti possono accedere al dataset e come questo verrà mantenuto nel tempo. Ad esempio, gli utenti potrebbero dover sottoporsi a una formazione o ricevere un’autorizzazione speciale dai creatori prima di accedere a un set di dati di informazioni protette, come con molti dataset medici. In alcuni casi, gli utenti potrebbero non accedere direttamente ai dati. Devono invece inviare il loro modello per essere addestrato sull’hardware dei creatori del set di dati, seguendo una configurazione di apprendimento “federato” (Aledhari et al. 2020). I creatori possono anche descrivere per quanto tempo il dataset rimarrà accessibile, come gli utenti possono inviare feedback su eventuali errori che scoprono e se ci sono piani per aggiornare il set di dati.\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, e Fahad Saeed. 2020. «Federated Learning: A Survey on Enabling Technologies, Protocols, and Applications». #IEEE_O_ACC# 8: 140699–725. https://doi.org/10.1109/access.2020.3013541.\nAlcune leggi e normative promuovono anche la trasparenza dei dati attraverso nuovi requisiti per le organizzazioni:\n\nIl “General Data Protection Regulation (GDPR)” nell’Unione Europea: Stabilisce requisiti rigorosi per l’elaborazione e la protezione dei dati personali dei cittadini dell’UE. Impone policy sulla privacy in linguaggio semplice che spiegano chiaramente quali dati vengono raccolti, perché vengono utilizzati, per quanto tempo vengono archiviati e con chi vengono condivisi. Il GDPR impone inoltre che le informative sulla privacy debbano includere dettagli sulla base giuridica per l’elaborazione, i trasferimenti di dati, i periodi di conservazione, i diritti di accesso e cancellazione e le informazioni di contatto per i responsabili del trattamento dei dati.\nIl “California’s Consumer Privacy Act” (CCPA): Il CCPA richiede policy sulla privacy chiare e diritti di esclusione per vendere dati personali. In modo significativo, stabilisce anche i diritti dei consumatori di essere interpellati per la divulgazione dei propri dati specifici. Le aziende devono fornire copie delle informazioni personali raccolte e dettagli su come vengono utilizzate, quali categorie vengono raccolte e cosa ricevono le terze parti. I consumatori possono identificare dati che ritengono debbano essere più accurati. La legge rappresenta un importante passo avanti nel potenziamento dell’accesso ai dati personali.\n\nGarantire la trasparenza dei dati presenta diverse sfide, soprattutto perché richiede molto tempo e risorse finanziarie. I sistemi di dati sono anche piuttosto complessi e la trasparenza completa può richiedere tempo. La trasparenza completa può anche sopraffare i consumatori con troppi dettagli. Infine, è anche importante bilanciare il compromesso tra trasparenza e privacy.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#licenze",
    "href": "contents/data_engineering/data_engineering.it.html#licenze",
    "title": "5  Data Engineering",
    "section": "5.10 Licenze",
    "text": "5.10 Licenze\nMolti dataset di alta qualità provengono da fonti proprietarie o contengono informazioni protette da copyright. Ciò introduce le licenze come una competenza legale impegnativa. Le aziende desiderose di addestrare sistemi di ML devono impegnarsi in trattative per ottenere licenze che garantiscano l’accesso legale a questi dataset. Inoltre, i termini delle licenze possono imporre restrizioni sulle applicazioni dei dati e sui metodi di condivisione. Il mancato rispetto di queste licenze può avere gravi conseguenze.\nAd esempio, ImageNet, uno dei dataset più ampiamente utilizzati per la ricerca sulla visione artificiale, è un caso emblematico. La maggior parte delle sue immagini è stata ottenuta da fonti online pubbliche senza esplicita autorizzazione, suscitando preoccupazioni etiche (Prabhu e Birhane, 2020). L’accesso al set di dati ImageNet per le aziende richiede la registrazione e l’adesione ai suoi termini di utilizzo, che limitano l’uso commerciale (ImageNet, 2021). I principali attori come Google e Microsoft investono in modo significativo nella concessione di licenze per i set di dati per migliorare i loro sistemi di visione di ML. Tuttavia, il fattore costo limita l’accessibilità per i ricercatori di aziende più piccole con budget limitati.\nIl dominio legale della concessione di licenze per i dati ha visto casi importanti che aiutano a definire i parametri di utilizzo corretto. Un esempio importante è Authors Guild, Inc. contro Google, Inc. Questa causa del 2005 sosteneva che il progetto di scansione di libri di Google violava i diritti d’autore visualizzando frammenti senza autorizzazione. Tuttavia, i tribunali alla fine si sono pronunciati a favore di Google, sostenendo il “fair use” [correttezza] in base alla natura trasformativa della creazione di un indice ricercabile e della visualizzazione di estratti limitati di testo. Questo precedente fornisce alcune basi legali per sostenere che le protezioni del “fair use” si applicano all’indicizzazione di set di dati e alla generazione di campioni rappresentativi per l’apprendimento automatico. Tuttavia, le restrizioni di licenza rimangono vincolanti, quindi un’analisi completa dei termini di licenza è fondamentale. Il caso dimostra perché le negoziazioni con i fornitori di dati sono importanti per consentire un utilizzo legale entro limiti accettabili.\nNuove Normative sui Dati e le Loro Implicazioni\nAnche le nuove normative sui dati hanno un impatto sulle pratiche di licenza. Il panorama legislativo si sta evolvendo con normative come l’Artificial Intelligence Act dell’UE, che è pronto a regolamentare lo sviluppo e l’uso dei sistemi di intelligenza artificiale all’interno dell’Unione Europea (UE). Questa legislazione:\n\nClassifica i sistemi di IA in base al rischio.\nImpone prerequisiti di sviluppo e utilizzo.\nSottolinea la qualità dei dati, la trasparenza, la supervisione umana e la responsabilità.\n\nInoltre, l’EU Act affronta le dimensioni etiche e le sfide operative in settori quali sanità e finanza. Gli elementi chiave includono il divieto di sistemi di intelligenza artificiale che presentano rischi “inaccettabili”, condizioni rigorose per sistemi ad alto rischio e obblighi minimi per sistemi di intelligenza artificiale a “rischio limitato”. Il proposto “European AI Board” supervisionerà e garantirà l’implementazione di una regolamentazione efficiente.\nProblemi nell’Assemblaggio di Dataset di Training ML\nProblemi complessi di licenza relativi a dati proprietari, leggi sul copyright e normative sulla privacy limitano le opzioni per l’assemblaggio dei set di dati di training ML. Tuttavia, espandere l’accessibilità tramite licenze più aperte o collaborazioni di dati pubblico-private potrebbe accelerare notevolmente il progresso del settore e gli standard etici.\nA volte, alcune parti di un dataset potrebbero dover essere rimosse o oscurate per rispettare gli accordi di utilizzo dei dati o proteggere informazioni sensibili. Ad esempio, un set di dati di informazioni utente potrebbe contenere nomi, dettagli di contatto e altri dati identificativi che potrebbero dover essere rimossi dal set di dati; questo avviene molto tempo dopo che il set di dati è già stato attivamente reperito e utilizzato per l’addestramento dei modelli. Analogamente, un dataset che include contenuti protetti da copyright o segreti commerciali potrebbe dover filtrare tali parti prima di essere distribuito. Leggi come il General Data Protection Regulation (GDPR), il California Consumer Privacy Act (CCPA) e L’Amended Act on the Protection of Personal Information (APPI) sono state approvate per garantire il diritto all’oblio. Queste normative impongono legalmente ai fornitori di modelli di cancellare i dati degli utenti su richiesta.\nI raccoglitori e i fornitori di dati devono essere in grado di adottare misure appropriate per de-identificare o filtrare qualsiasi informazione proprietaria, concessa in licenza, riservata o regolamentata, se necessario. A volte, gli utenti possono richiedere esplicitamente che i loro dati vengano rimossi.\nLa possibilità di aggiornare il set di dati rimuovendo i dati consentirà ai creatori di rispettare gli obblighi legali ed etici relativi al loro utilizzo e alla privacy. Tuttavia, la capacità di rimuovere i dati presenta alcune limitazioni importanti. Dobbiamo considerare che alcuni modelli potrebbero essere già stati addestrati sul dataset e non esiste un modo chiaro o noto per eliminare l’effetto di un particolare campione di dati dalla rete addestrata. Non esiste un meccanismo di cancellazione. Quindi, ciò solleva la questione: il modello dovrebbe essere riaddestrato da zero ogni volta che viene rimosso un campione? Questa è un’opzione costosa. Una volta che i dati sono stati utilizzati per addestrare un modello, la semplice rimozione dal set di dati originale potrebbe non eliminare completamente il suo impatto sul comportamento del modello. Sono necessarie nuove ricerche sugli effetti della rimozione dei dati sui modelli già addestrati e se sia necessario un ri-addestramento completo per evitare di conservare artefatti di dati eliminati. Ciò presenta una considerazione importante quando si bilanciano gli obblighi di licenza dei dati con l’efficienza e la praticità in un sistema di ML in evoluzione e distribuito.\nLa licenza del dataset è un dominio poliedrico che interseca tecnologia, etica e legge. Comprendere queste complessità diventa fondamentale per chiunque crei set di dati durante l’ingegneria dei dati, man mano che il mondo si evolve.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#conclusione",
    "href": "contents/data_engineering/data_engineering.it.html#conclusione",
    "title": "5  Data Engineering",
    "section": "5.11 Conclusione",
    "text": "5.11 Conclusione\nI dati sono il componente fondamentale dei sistemi di intelligenza artificiale. Senza dati di qualità, anche gli algoritmi di apprendimento automatico più avanzati falliranno. L’ingegneria dei dati comprende il processo end-to-end di raccolta, archiviazione, elaborazione e gestione dei dati per alimentare lo sviluppo di modelli di apprendimento automatico. Si inizia con la definizione chiara del problema principale e degli obiettivi, che guidano una raccolta dati efficace. I dati possono essere reperiti da diversi mezzi, tra cui dataset esistenti, web scraping, crowdsourcing e generazione di dati sintetici. Ogni approccio comporta compromessi tra costi, velocità, privacy e specificità. Una volta raccolti i dati, un’etichettatura ponderata tramite annotazione manuale o assistita dall’intelligenza artificiale consente la creazione di set di dati di training di alta qualità. Un’archiviazione adeguata in database, “warehouse” o “lake” facilita l’accesso e l’analisi. I metadati forniscono dettagli contestuali sui dati. L’elaborazione dei dati trasforma i dati grezzi in un formato pulito e coerente per lo sviluppo di modelli di apprendimento automatico. In tutta questa pipeline, la trasparenza attraverso la documentazione e il tracciamento della provenienza è fondamentale per l’etica, la verificabilità e la riproducibilità. I protocolli di licenza dei dati regolano anche l’accesso e l’uso legale dei dati. Le principali sfide nell’ingegneria dei dati includono rischi per la privacy, lacune di rappresentazione, restrizioni legali sui dati proprietari e la necessità di bilanciare vincoli concorrenti come velocità e qualità. Progettando attentamente dati di training di alta qualità, i professionisti dell’apprendimento automatico possono sviluppare sistemi di intelligenza artificiale accurati, robusti e responsabili, tra cui applicazioni embedded e TinyML.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#sec-data-engineering-resource",
    "href": "contents/data_engineering/data_engineering.it.html#sec-data-engineering-resource",
    "title": "5  Data Engineering",
    "section": "5.12 Risorse",
    "text": "5.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nData Engineering: Overview.\nFeature engineering.\nData Standards: Speech Commands.\nCrowdsourcing Data for the Long Tail.\nReusing and Adapting Existing Datasets.\nResponsible Data Collection.\nRilevamento Dati Anomali:\n\nAnomaly Detection: Overview.\nAnomaly Detection: Challenges.\nAnomaly Detection: Datasets.\nAnomaly Detection: Using Autoencoders.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 5.1\nEsercizio 5.2\nEsercizio 5.3\nEsercizio 5.4\nEsercizio 5.5\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html",
    "href": "contents/frameworks/frameworks.it.html",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "",
    "text": "6.1 Introduzione\nI framework di machine learning [apprendimento automatico] forniscono gli strumenti e l’infrastruttura per creare, addestrare e distribuire in modo efficiente modelli di apprendimento automatico. In questo capitolo esploreremo l’evoluzione e le capacità chiave dei principali framework come TensorFlow (TF), PyTorch e framework specializzati per dispositivi embedded. Ci immergeremo nei componenti come grafi computazionali, algoritmi di ottimizzazione, accelerazione hardware e altro che consentono agli sviluppatori di creare rapidamente modelli performanti. Comprendere questi framework è essenziale per sfruttare la potenza del deep learning in tutto lo spettro, dal cloud ai dispositivi edge [periferici].\nI framework di apprendimento automatico gestiscono gran parte della complessità dello sviluppo di modelli tramite API di alto livello e linguaggi specifici per dominio che consentono ai professionisti di creare rapidamente modelli combinando componenti e astrazioni predefiniti. Ad esempio, framework come TensorFlow e PyTorch forniscono API Python per definire architetture di reti neurali utilizzando livelli, ottimizzatori, set di dati e altro. Ciò consente un’iterazione rapida rispetto alla codifica di ogni dettaglio del modello partendo da zero.\nUna capacità chiave offerta da questi framework è rappresentata dai motori di training distribuiti che possono scalare l’addestramento del modello su cluster di GPU e TPU. Ciò rende possibile il training di modelli all’avanguardia con miliardi o trilioni di parametri su vasti set di dati. I framework si integrano anche con hardware specializzato come le GPU NVIDIA per accelerare ulteriormente il training tramite ottimizzazioni come la parallelizzazione ed efficienti operazioni matriciali.\nInoltre, i framework semplificano il deploy [distribuzione] di modelli finiti in produzione tramite strumenti come TensorFlow Serving per il model serving scalabile e TensorFlow Lite per l’ottimizzazione su dispositivi mobili ed edge. Altre capacità preziose includono visualizzazione, tecniche di ottimizzazione del modello come quantizzazione e potatura e monitoraggio delle metriche durante il training.\nI principali framework open source come TensorFlow, PyTorch e MXNet alimentano gran parte della ricerca e dello sviluppo dell’IA oggi. Offerte commerciali come Amazon SageMaker e Microsoft Azure Machine Learning integrano questi framework open source con funzionalità proprietarie e strumenti aziendali.\nGli ingegneri e i professionisti del machine learning sfruttano questi framework robusti per concentrarsi su attività di alto valore come architettura del modello, progettazione delle feature e ottimizzazione degli iperparametri anziché sull’infrastruttura. L’obiettivo è creare e distribuire modelli performanti che risolvano in modo efficiente i problemi del mondo reale.\nIn questo capitolo, esploreremo i principali framework cloud odierni e il modo in cui hanno adattato modelli e strumenti specificamente per la distribuzione embedded ed edge. Confronteremo modelli di programmazione, hardware supportato, capacità di ottimizzazione e altro ancora per comprendere appieno in che modo i framework consentono un apprendimento automatico scalabile dal cloud all’edge.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#evoluzione-dei-framework",
    "href": "contents/frameworks/frameworks.it.html#evoluzione-dei-framework",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.2 Evoluzione dei Framework",
    "text": "6.2 Evoluzione dei Framework\nI framework di apprendimento automatico si sono evoluti in modo significativo per soddisfare le diverse esigenze dei professionisti del machine learning e i progressi nelle tecniche di intelligenza artificiale. Qualche decennio fa, la creazione e l’addestramento di modelli di apprendimento automatico richiedevano un’ampia codifica e infrastruttura di basso livello. Oltre alla necessità di una codifica di basso livello, la prima ricerca sulle reti neurali era limitata da dati e potenza di calcolo insufficienti. Tuttavia, i framework di apprendimento automatico si sono evoluti notevolmente nell’ultimo decennio per soddisfare le crescenti esigenze dei professionisti e i rapidi progressi nelle tecniche di deep learning [apprendimento profondo]. Il rilascio di grandi set di dati come ImageNet (Deng et al. 2009) e i progressi nel calcolo parallelo con GPU hanno sbloccato il potenziale per reti neurali molto più profonde.\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, e Fei-Fei Li. 2009. «ImageNet: A large-scale hierarchical image database». In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–55. IEEE. https://doi.org/10.1109/cvpr.2009.5206848.\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. «Theano: A Python framework for fast computation of mathematical expressions». https://arxiv.org/abs/1605.02688.\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, e Trevor Darrell. 2014. «Caffe: Convolutional Architecture for Fast Feature Embedding». In Proceedings of the 22nd ACM international conference on Multimedia, 675–78. ACM. https://doi.org/10.1145/2647868.2654889.\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. «ImageNet Classification with Deep Convolutional Neural Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\nChollet, François. 2018. «Introduction to keras». March 9th.\n\nTokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, e Hiroyuki Yamazaki Vincent. 2019. «Chainer: A Deep Learning Framework for Accelerating the Research Cycle». In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 5:1–6. ACM. https://doi.org/10.1145/3292500.3330756.\n\nSeide, Frank, e Amit Agarwal. 2016. «Cntk: Microsoft’s Open-Source Deep-Learning Toolkit». In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2135–35. ACM. https://doi.org/10.1145/2939672.2945397.\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. «PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation». In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, e Roman Garnett, 8024–35. ACM. https://doi.org/10.1145/3620665.3640366.\nI primi framework di apprendimento automatico, Theano di Team et al. (2016) e Caffe di Jia et al. (2014), sono stati sviluppati da istituzioni accademiche. Theano è stato creato dal Montreal Institute for Learning Algorithms, mentre Caffe è stato sviluppato dal Berkeley Vision and Learning Center. Nel crescente interesse per il deep learning dovuto alle prestazioni all’avanguardia di AlexNet Krizhevsky, Sutskever, e Hinton (2012) sul dataset ImageNet, aziende private e singole persone hanno iniziato a sviluppare framework di ML, dando vita a Keras di Chollet (2018), Chainer di Tokui et al. (2019), TensorFlow di Google (Yu et al. 2018), CNTK di Microsoft (Seide e Agarwal 2016) e PyTorch di Facebook (Ansel et al. 2024).\nMolti di questi framework ML possono essere suddivisi in framework di alto livello, di basso livello e di grafi computazionali statici e dinamici. I framework di alto livello forniscono un livello di astrazione più elevato rispetto a quelli di basso livello. I framework di alto livello hanno funzioni e moduli predefiniti per attività ML comuni, come la creazione, l’addestramento e la valutazione di modelli ML comuni, la preelaborazione dei dati, le funzionalità di progettazione e la visualizzazione dei dati, che i framework di basso livello non hanno. Pertanto, i framework di alto livello possono risultare più facili da usare ma sono meno personalizzabili rispetto a quelli di basso livello (ad esempio, gli utenti di framework di basso livello possono definire livelli personalizzati, funzioni “loss” [di perdita], algoritmi di ottimizzazione, ecc.). Esempi di framework di alto livello sono TensorFlow/Keras e PyTorch. Esempi di framework ML di basso livello includono TensorFlow con API di basso livello, Theano, Caffe, Chainer e CNTK.\nFramework come Theano e Caffe utilizzavano grafi computazionali statici, che richiedevano la definizione anticipata dell’architettura completa del modello, limitandone così la flessibilità. Al contrario, i grafici dinamici vengono costruiti al volo per uno sviluppo più iterativo. Intorno al 2016, framework come PyTorch e TensorFlow 2.0 hanno iniziato ad adottare grafici dinamici, offrendo maggiore flessibilità per lo sviluppo del modello. Discuteremo di questi concetti e dettagli più avanti nella sezione Training dell’IA.\nLo sviluppo di questi framework ha suscitato un’esplosione di dimensioni e complessità del modello nel tempo, dai primi perceptron multistrato e reti convoluzionali ai moderni trasformatori con miliardi o trilioni di parametri. Nel 2016, i modelli ResNet di He et al. (2016) hanno raggiunto un’accuratezza ImageNet record con oltre 150 livelli e 25 milioni di parametri. Poi, nel 2020, il modello linguistico GPT-3 di OpenAI (Brown et al. 2020) ha spinto i parametri a un sorprendente numero di 175 miliardi utilizzando il parallelismo del modello nei framework per addestrare migliaia di GPU e TPU.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. «Deep Residual Learning for Image Recognition». In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nOgni generazione di framework ha sbloccato nuove capacità che hanno alimentato il progresso:\n\nTheano e TensorFlow (2015) hanno introdotto grafi computazionali e differenziazione automatica per semplificare la creazione di modelli.\nCNTK (2016) ha aperto la strada a un addestramento distribuito efficiente combinando parallelismo di modelli e dati.\nPyTorch (2016) ha fornito programmazione imperativa e grafici dinamici per una sperimentazione flessibile.\nTensorFlow 2.0 (2019) ha impostato di default l’esecuzione Eager per intuitività e debug.\nTensorFlow Graphics (2020) ha aggiunto strutture dati 3D per gestire nuvole di punti e mesh.\n\nNegli ultimi anni, i framework sono convergenti. Figura 6.1 mostra che TensorFlow e PyTorch sono diventati i framework ML più dominanti, rappresentando oltre il 95% dei framework ML utilizzati nella ricerca e nella produzione. Keras è stato integrato in TensorFlow nel 2019; Preferred Networks ha trasferito Chainer a PyTorch nel 2019; e Microsoft ha smesso di sviluppare attivamente CNTK nel 2022 per supportare PyTorch su Windows.\n\n\n\n\n\n\nFigura 6.1: Popolarità dei framework ML negli Stati Uniti misurata dalle ricerche web di Google. Fonte: Google.\n\n\n\nUn approccio unico non funziona bene in tutto lo spettro, dal cloud ai piccoli dispositivi edge. Diversi framework rappresentano varie filosofie sull’esecuzione di grafici, API dichiarative rispetto a quelle imperative e altro ancora. Le dichiarative definiscono cosa dovrebbe fare il programma, mentre le imperative si concentrano su come dovrebbe essere fatto passo dopo passo. Ad esempio, TensorFlow utilizza l’esecuzione di grafici e la modellazione in stile dichiarativo, mentre PyTorch adotta l’esecuzione rapida e la modellazione imperativa per una maggiore flessibilità con Python. Ogni approccio comporta dei compromessi che discuteremo in Sezione 6.3.7.\nGli attuali framework avanzati consentono ai professionisti di sviluppare e distribuire modelli sempre più complessi, un fattore chiave dell’innovazione nel campo dell’intelligenza artificiale. Questi framework continuano a evolversi ed espandere le loro capacità per la prossima generazione di machine learning. Per capire come questi sistemi continuano a evolversi, approfondiremo TensorFlow come esempio di come il framework sia cresciuto in complessità nel tempo.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#sec-deep_dive_into_tensorflow",
    "href": "contents/frameworks/frameworks.it.html#sec-deep_dive_into_tensorflow",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.3 Approfondimento su TensorFlow",
    "text": "6.3 Approfondimento su TensorFlow\nTensorFlow è stato sviluppato dal team di Google Brain ed è stato rilasciato come libreria software open source il 9 novembre 2015. È stato progettato per il calcolo numerico utilizzando grafici di flusso di dati e da allora è diventato popolare per un’ampia gamma di applicazioni di apprendimento automatico e deep learning.\nTensorFlow è un framework di training e inferenza che fornisce funzionalità integrate per gestire tutto, dalla creazione e training del modello alla distribuzione, come mostrato in Figura 6.2. Sin dal suo sviluppo iniziale, l’ecosistema TensorFlow è cresciuto fino a includere molte diverse “varietà” di TensorFlow, ciascuna pensata per consentire agli utenti di supportare ML su diverse piattaforme. In questa sezione, discuteremo principalmente solo del pacchetto core.\n\n6.3.1 Ecosistema TF\n\nTensorFlow Core: pacchetto principale con cui interagiscono la maggior parte degli sviluppatori. Fornisce una piattaforma completa e flessibile per definire, addestrare e distribuire modelli di apprendimento automatico. Include tf.keras come API di alto livello.\nTensorFlow Lite: progettato per distribuire modelli leggeri su dispositivi mobili, embedded ed edge. Offre strumenti per convertire i modelli TensorFlow in un formato più compatto adatto a dispositivi con risorse limitate e fornisce modelli pre-addestrati ottimizzati per dispositivi mobili.\nTensorFlow Lite Micro: progettato per eseguire modelli di apprendimento automatico su microcontrollori con risorse minime. Funziona senza la necessità di supporto del sistema operativo, librerie C o C++ standard o allocazione dinamica della memoria, utilizzando solo pochi kilobyte di memoria.\nTensorFlow.js: libreria JavaScript che consente l’addestramento e la distribuzione di modelli di apprendimento automatico direttamente nel browser o su Node.js. Fornisce inoltre strumenti per il porting di modelli TensorFlow pre-addestrati nel formato browser-friendly.\nTensorFlow su dispositivi edge (Coral): piattaforma di componenti hardware e strumenti software di Google che consente l’esecuzione di modelli TensorFlow su dispositivi edge, sfruttando Edge TPU per l’accelerazione.\nTensorFlow Federated (TFF): framework per l’apprendimento automatico e altri calcoli su dati decentralizzati. TFF facilita l’apprendimento “federato”, consentendo l’addestramento del modello su molti dispositivi senza centralizzare i dati.\nTensorFlow Graphics: libreria per l’utilizzo di TensorFlow per svolgere attività correlate alla grafica, tra cui l’elaborazione di forme 3D e nuvole di punti, utilizzando il deep learning.\nTensorFlow Hub: repository di componenti di modelli di apprendimento automatico riutilizzabili che consente agli sviluppatori di riutilizzare componenti di modelli pre-addestrati, facilitando l’apprendimento per trasferimento e la composizione del modello.\nTensorFlow Serving: framework progettato per servire e distribuire modelli di apprendimento automatico per l’inferenza in ambienti di produzione. Fornisce strumenti per il versioning e l’aggiornamento dinamico dei modelli distribuiti senza interruzione del servizio.\nTensorFlow Extended (TFX): piattaforma end-to-end progettata per distribuire e gestire pipeline di apprendimento automatico in ambienti di produzione. TFX comprende validazione dei dati, pre-elaborazione, addestramento del modello, convalida e componenti di servizio.\n\n\n\n\n\n\n\nFigura 6.2: Panoramica dell’architettura di TensorFlow 2.0. Fonte: Tensorflow.\n\n\n\nTensorFlow è stato sviluppato per affrontare le limitazioni di DistBelief (Yu et al. 2018)—il framework in uso presso Google dal 2011 al 2015—offrendo flessibilità lungo tre direttrici: 1) definizione di nuovi livelli [livelli], 2) perfezionamento degli algoritmi di training e 3) definizione di nuovi algoritmi di training. Per comprendere quali limitazioni di DistBelief hanno portato allo sviluppo di TensorFlow, faremo prima una breve panoramica dell’architettura del server dei parametri utilizzata da DistBelief (Dean et al. 2012).\n\nYu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, et al. 2018. «Dynamic control flow in large-scale machine learning». In Proceedings of the Thirteenth EuroSys Conference, 265–83. ACM. https://doi.org/10.1145/3190508.3190551.\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. «Large Scale Distributed Deep Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1232–40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\nL’architettura Parameter Server (PS) è un design popolare per distribuire il training di modelli di apprendimento automatico, in particolare reti neurali profonde, su più macchine. L’idea fondamentale è di separare l’archiviazione e la gestione dei parametri del modello dal calcolo utilizzato per aggiornare tali parametri. In genere, i server dei parametri gestiscono l’archiviazione e la gestione dei parametri del modello, suddividendoli su più server. I processi worker eseguono le attività di calcolo, tra cui l’elaborazione dei dati e il calcolo dei gradienti, che vengono poi inviati ai server dei parametri per l’aggiornamento.\nStorage: I processi del server dei parametri stateful [con stato] gestivano l’archiviazione e la gestione dei parametri del modello. Data l’ampia scala dei modelli e la natura distribuita del sistema, questi parametri erano condivisi tra più server dei parametri. Ogni server manteneva una parte dei parametri del modello, rendendolo \"stateful\" poiché doveva mantenere e gestire questo stato durante il processo di training.\nComputation: I processi worker, che potevano essere eseguiti in parallelo, erano senza stato e puramente computazionali. Elaboravano dati e calcolavano gradienti senza mantenere alcuno stato o memoria a lungo termine (M. Li et al. 2014). I worker non conservavano informazioni tra le diverse attività. Invece, comunicavano periodicamente con i server dei parametri per recuperare i parametri più recenti e restituire i gradienti calcolati.\n\nLi, Mu, David G. Andersen, Alexander J. Smola, e Kai Yu. 2014. «Communication Efficient Distributed Machine Learning with the Parameter Server». In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, a cura di Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, e Kilian Q. Weinberger, 19–27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\n\n\n\n\nEsercizio 6.1: TensorFlow Core\n\n\n\n\n\nAndiamo a comprendere in modo completo gli algoritmi di apprendimento automatico di base utilizzando TensorFlow e le loro applicazioni pratiche nell’analisi dei dati e nella modellazione predittiva. Inizieremo con la regressione lineare per prevedere i tassi di sopravvivenza dal set di dati del Titanic. Poi, utilizzando TensorFlow, costruiremo classificatori per identificare diverse specie di fiori in base ai loro attributi. Successivamente, utilizzeremo l’algoritmo K-Means e la sua applicazione nella segmentazione dei set di dati in cluster coesi. Infine, applicheremo modelli hidden [nascosti] di Markov (HMM) per prevedere i modelli meteorologici.\n\n\n\n\n\n\n\n\n\n\nEsercizio 6.2: TensorFlow Lite\n\n\n\n\n\nQui vedremo come costruire un modello di apprendimento automatico in miniatura per microcontrollori. Costruiremo una mini rete neurale semplificata per apprendere dai dati anche con risorse limitate e ottimizzata per l’implementazione riducendo il nostro modello per un uso efficiente sui microcontrollori. TensorFlow Lite, una potente tecnologia derivata da TensorFlow, riduce i modelli per dispositivi minuscoli e aiuta ad abilitare funzionalità sul dispositivo come il riconoscimento delle immagini nei dispositivi smart [intelligenti]. Viene utilizzato nell’edge computing per consentire analisi e decisioni più rapide nei dispositivi che elaborano i dati localmente.\n\n\n\n\nDistBelief e la sua architettura definita sopra sono stati fondamentali per abilitare il deep learning distribuito in Google, ma hanno anche introdotto delle limitazioni che hanno motivato lo sviluppo di TensorFlow:\n\n\n6.3.2 Grafico di Calcolo Statico\nI parametri del modello sono distribuiti su vari server di parametri nell’architettura del server di parametri. Poiché DistBelief è stato progettato principalmente per il paradigma della rete neurale, i parametri corrispondevano a una struttura di rete neurale fissa. Se il computation graph [grafico di calcolo] fosse dinamico, la distribuzione e il coordinamento dei parametri diventerebbero significativamente più complicati. Ad esempio, una modifica nel grafico potrebbe richiedere l’inizializzazione di nuovi parametri o la rimozione di quelli esistenti, complicando le attività di gestione e sincronizzazione dei server di parametri. Ciò ha reso più difficile implementare modelli al di fuori del framework neurale o modelli che richiedevano grafici di calcolo dinamici.\nTensorFlow è stato progettato come un framework di calcolo più generale che esprime il calcolo come un grafico del flusso di dati. Ciò consente una più ampia varietà di modelli e algoritmi di apprendimento automatico al di fuori delle reti neurali e fornisce flessibilità nel perfezionamento dei modelli.\n\n\n6.3.3 Usabilità & Distribuzione\nIl modello del server dei parametri delinea i ruoli (nodi worker e server dei parametri) ed è ottimizzato per i deployment [distribuzioni] dei data center, che potrebbero essere ottimali solo per alcuni casi d’uso. Ad esempio, questa divisione introduce overhead o complessità sui dispositivi edge o in altri ambienti non data center.\nTensorFlow è stato creato per funzionare su più piattaforme, dai dispositivi mobili e edge all’infrastruttura cloud. Mirava anche a essere più leggero e intuitivo per gli sviluppatori e a fornire facilità d’uso tra il training locale e quello distribuito.\n\n\n6.3.4 Progettazione dell’Architettura\nInvece di utilizzare l’architettura del server dei parametri, TensorFlow distribuisce i task [attività] su un cluster. Queste attività sono processi denominati che possono comunicare su una rete e ciascuna può eseguire la struttura principale di TensorFlow, il grafico del flusso di dati e l’interfaccia con vari dispositivi di elaborazione (come CPU o GPU). Questo grafico [grafo] è una rappresentazione diretta in cui i nodi simboleggiano le operazioni di elaborazione e gli edge rappresentano i tensori (dati) che scorrono tra queste operazioni.\nNonostante l’assenza di server di parametri tradizionali, alcuni “task PS” memorizzano e gestiscono parametri che ricordano i server di parametri di altri sistemi. I task rimanenti, che di solito gestiscono calcoli, elaborazione dati e gradienti, sono denominati “task worker”. I task PS di TensorFlow possono eseguire qualsiasi calcolo rappresentabile dal grafico del flusso di dati, il che significa che non sono limitati solo all’archiviazione dei parametri e il calcolo può essere distribuito. Questa capacità li rende significativamente più versatili e offre agli utenti il potere di programmare i task PS utilizzando l’interfaccia TensorFlow standard, la stessa che userebbero per definire i loro modelli. Come accennato in precedenza, la struttura dei grafici del flusso di dati li rende anche intrinsecamente buoni per il parallelismo, consentendo l’elaborazione di grandi set di dati.\n\n\n6.3.5 Funzionalità Native & Keras\nTensorFlow include librerie per aiutare gli utenti a sviluppare e distribuire più modelli specifici per i casi d’uso e, poiché questo framework è open source, questo elenco continua a crescere. Queste librerie affrontano l’intero ciclo di vita dello sviluppo ML: preparazione dei dati, creazione di modelli, distribuzione e IA responsabile.\nUno dei maggiori vantaggi di TensorFlow è la sua integrazione con Keras, anche se, come vedremo nella prossima sezione, Pytorch ha recentemente aggiunto un’integrazione Keras. Keras è un altro framework ML creato per essere estremamente intuitivo e, di conseguenza, ha un alto livello di astrazione. Parleremo di Keras più approfonditamente più avanti in questo capitolo. Tuttavia, quando si discute della sua integrazione con TensorFlow, è importante notare che era stato originariamente creato per essere indipendente dal backend. Ciò significa che gli utenti potrebbero astrarre queste complessità, offrendo un modo più pulito e intuitivo per definire e addestrare modelli senza preoccuparsi di problemi di compatibilità con diversi backend. Gli utenti di TensorFlow hanno evidenziato alcuni problemi sull’usabilità e la leggibilità dell’API di TensorFlow, quindi, man mano che TF acquisiva importanza, ha integrato Keras come API di alto livello. Questa integrazione ha offerto grandi vantaggi agli utenti di TensorFlow poiché ha introdotto una leggibilità e una portabilità più intuitive dei modelli, sfruttando comunque le potenti funzionalità di backend, il supporto di Google e l’infrastruttura per distribuire i modelli su varie piattaforme.\n\n\n\n\n\n\nEsercizio 6.3: Esplorazione di Keras: Creazione, Addestramento e Valutazione di Reti Neurali\n\n\n\n\n\nQui, impareremo come utilizzare Keras, un’API di reti neurali di alto livello, per lo sviluppo e l’addestramento (training) di modelli. Esploreremo l’API funzionale per la creazione di modelli concisi, comprenderemo le classi “loss” e metriche per la valutazione dei modelli e utilizzeremo gli ottimizzatori nativi per aggiornare i parametri del modello durante l’addestramento. Inoltre, scopriremo come definire layer e metriche personalizzati su misura per le nostre esigenze. Infine, esamineremo i cicli di addestramento di Keras per semplificare il processo di addestramento delle reti neurali su grandi set di dati. Questa conoscenza ci consentirà di costruire e ottimizzare modelli di reti neurali in varie applicazioni di machine learning e intelligenza artificiale.\n\n\n\n\n\n\n6.3.6 Limitazioni e Sfide\nTensorFlow è uno dei framework di deep learning più popolari, ma ha dovuto affrontare critiche e debolezze, principalmente legate all’usabilità e all’utilizzo delle risorse. Sebbene vantaggioso, il ritmo rapido degli aggiornamenti tramite il supporto di Google ha talvolta portato a problemi di retrocompatibilità, funzioni deprecate e documentazione instabile. Inoltre, anche con l’implementazione di Keras, la sintassi e la curva di apprendimento di TensorFlow possono risultare difficili per i nuovi utenti. Un’altra critica importante di TensorFlow è il suo elevato overhead e consumo di memoria dovuto alla gamma di librerie integrate e al supporto. Sebbene le versioni ridotte possano risolvere alcuni di questi problemi, potrebbero comunque essere limitate in ambienti con risorse limitate.\n\n\n6.3.7 PyTorch & TensorFlow\nPyTorch e TensorFlow si sono affermati come leader nel settore. Entrambi i framework offrono funzionalità robuste ma differiscono per filosofie di progettazione, facilità d’uso, ecosistema e capacità di distribuzione.\nFilosofia di Progettazione e Paradigma di Programmazione: PyTorch utilizza un grafo computazionale dinamico denominato eager execution [esecuzione rapida]. Ciò lo rende intuitivo e facilita il debug poiché le operazioni vengono eseguite immediatamente e possono essere ispezionate al volo. Al contrario, le versioni precedenti di TensorFlow erano incentrate su un grafo computazionale statico, che richiedeva la definizione completa del grafico prima dell’esecuzione. Tuttavia, TensorFlow 2.0 ha introdotto la “eager execution” per default, rendendolo più allineato con PyTorch. La natura dinamica di PyTorch e l’approccio basato su Python hanno consentito la sua semplicità e flessibilità, in particolare per la prototipazione rapida. L’approccio grafico statico di TensorFlow nelle sue versioni precedenti aveva una curva di apprendimento più ripida; l’introduzione di TensorFlow 2.0, con la sua integrazione Keras come API di alto livello, ha semplificato notevolmente il processo di sviluppo.\nDeployment: PyTorch è fortemente favorito negli ambienti di ricerca, ma la distribuzione dei modelli PyTorch in contesti di produzione è sempre stata un problema. Tuttavia, la distribuzione è diventata più fattibile con l’introduzione di TorchScript, lo strumento TorchServe e PyTorch Mobile. TensorFlow si distingue per la sua forte scalabilità e capacità di distribuzione, in particolare su piattaforme embedded e mobili con TensorFlow Lite. TensorFlow Serving e TensorFlow.js facilitano ulteriormente la distribuzione in vari ambienti, conferendogli così una portata più ampia nell’ecosistema.\nPrestazioni: Entrambi i framework offrono un’accelerazione hardware efficiente per le loro operazioni. Tuttavia, TensorFlow ha un flusso di lavoro di ottimizzazione leggermente più robusto, come il compilatore XLA (Accelerated Linear Algebra), che può aumentare ulteriormente le prestazioni. Il suo grafo computazionale statico era anche vantaggioso per alcune ottimizzazioni nelle prime versioni.\nEcosistema: PyTorch ha un ecosistema in crescita con strumenti come TorchServe per servire modelli e librerie come TorchVision, TorchText e TorchAudio per domini specifici. Come abbiamo detto prima, TensorFlow ha un ecosistema ampio e maturo. TensorFlow Extended (TFX) fornisce una piattaforma end-to-end per distribuire pipeline di apprendimento automatico di produzione. Altri strumenti e librerie includono TensorFlow Lite, TensorFlow Lite Micro, TensorFlow.js, TensorFlow Hub e TensorFlow Serving.\nTabella 6.1 fornisce un’analisi comparativa:\n\n\n\nTabella 6.1: Confronto tra PyTorch e TensorFlow.\n\n\n\n\n\n\n\n\n\n\nAspetto\nPytorch\nTensorFlow\n\n\n\n\nFilosofia di Progettazione\nGrafo computazionale dinamico (eager execution)\nGrafo computazionale statico (prime versioni); Esecuzione rapida in TensorFlow 2.0\n\n\nDeployment\nTradizionalmente impegnativa; Migliorata con TorchScript e TorchServe\nScalabile, specialmente su piattaforme embedded con TensorFlow Lite\n\n\nPrestazioni e Ottimizzazione\nAccelerazione GPU efficiente\nOttimizzazione robusta con compilatore XLA\n\n\nEcosistema\nTorchServe, TorchVision, TorchText, TorchAudio, PyTorch Mobile\nTensorFlow Extended (TFX), TensorFlow Lite, TensorFlow Lite Micro TensorFlow.js, TensorFlow Hub, TensorFlow Serving\n\n\nFacilità d’uso\nPreferito per il suo approccio Pythonic e la prototipazione rapida\nCurva di apprendimento inizialmente ripida; Semplificato con Keras in TensorFlow 2.0",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#componenti-di-base-del-framework",
    "href": "contents/frameworks/frameworks.it.html#componenti-di-base-del-framework",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.4 Componenti di Base del Framework",
    "text": "6.4 Componenti di Base del Framework\nDopo aver introdotto i popolari framework di machine learning e aver fornito un confronto di alto livello, questa sezione presenterà le funzionalità principali che formano la struttura di questi framework. Tratterà la struttura speciale chiamata tensori, che questi framework utilizzano per gestire più facilmente dati multidimensionali complessi. Si vedrà anche come questi framework rappresentano diversi tipi di architetture di reti neurali e le loro operazioni richieste tramite grafi computazionali. Inoltre, si vedrà come offrono strumenti che rendono lo sviluppo di modelli di machine learning più astratto ed efficiente, come caricatori di dati, algoritmi di ottimizzazione delle perdite implementate, tecniche di differenziazione efficienti e la capacità di accelerare il processo di training su acceleratori hardware.\n\n6.4.1 Strutture Dati Tensoriali\nPer comprendere i tensori, partiamo dai concetti familiari dell’algebra lineare. Come mostrato in Figura 6.4, i vettori possono essere rappresentati come una pila di numeri in un array unidimensionale. Le matrici seguono la stessa idea e si possono pensare a loro come a molti vettori impilati l’uno sull’altro, rendendoli bidimensionali. I tensori di dimensioni superiori funzionano allo stesso modo. Un tensore tridimensionale è semplicemente un insieme di matrici impilate l’una sull’altra in una direzione aggiuntiva. Pertanto, vettori e matrici possono essere considerati casi speciali di tensori con dimensioni 1D e 2D, rispettivamente.\n\n\n\n\n\n\nFigura 6.3: Visualizzazione della Struttura Dati del Tensore.\n\n\n\nI tensori offrono una struttura flessibile che può rappresentare dati in dimensioni superiori. Ad esempio, per rappresentare i dati di un’immagine, i pixel in ogni posizione di un’immagine sono strutturati come matrici. Tuttavia, le immagini non sono rappresentate da una sola matrice di valori di pixel; in genere hanno tre canali in cui ogni canale è una matrice contenente valori di pixel che rappresentano l’intensità di rosso, verde o blu. Insieme, questi canali creano un’immagine colorata. Senza i tensori, archiviare tutte queste informazioni da più matrici può risultare complesso. Con i tensori, è facile contenere i dati dell’immagine in un singolo tensore tridimensionale, con ogni numero che rappresenta un certo valore di colore in una posizione specifica nell’immagine.\n\n\n\n\n\n\nFigura 6.4: Visualizzazione della struttura dell’immagine colorata che può essere facilmente memorizzata come un Tensore 3D. Credito: Niklas Lang\n\n\n\nNon finisce qui. Se volessimo archiviare una serie di immagini, potremmo usare un tensore quadridimensionale, in cui la nuova dimensione rappresenta immagini diverse. Ciò significa che si stanno archiviando più immagini, ciascuna con tre matrici che rappresentano i tre canali del colore. Questo dà un’idea dell’utilità dei tensori quando si gestiscono dati multidimensionali in modo efficiente.\nI tensori hanno anche un attributo unico che consente ai framework di calcolare automaticamente i gradienti, semplificando l’implementazione di modelli complessi e algoritmi di ottimizzazione. Nel machine learning, come discusso nel Capitolo 3, la backpropagation richiede di prendere la derivata delle equazioni. Una delle caratteristiche principali dei tensori in PyTorch e TensorFlow è la loro capacità di tracciare i calcoli e calcolare i gradienti. Ciò è fondamentale per la backpropagation nelle reti neurali. Ad esempio, in PyTorch, si può usare l’attributo requires_grad, che consente di calcolare e memorizzare automaticamente i gradienti durante il “backward pass”, facilitando il processo di ottimizzazione. Analogamente, in TensorFlow, tf.GradientTape registra le operazioni per la differenziazione automatica.\nSi consideri questa semplice equazione matematica che si vuole differenziare. Matematicamente, il calcolo del gradiente si effettua nel modo seguente:\nDato: \\[\ny = x^2\n\\]\nLa derivata di \\(y\\) rispetto a \\(x\\) è: \\[\n\\frac{dy}{dx} = 2x\n\\]\nQuando \\(x = 2\\): \\[\n\\frac{dy}{dx} = 2*2 = 4\n\\]\nIl gradiente di \\(y\\) rispetto a \\(x\\), con \\(x = 2\\), è 4.\nUna potente caratteristica dei tensori in PyTorch e TensorFlow è la loro capacità di calcolare facilmente le derivate (gradienti). Ecco gli esempi di codice corrispondenti in PyTorch e TensorFlow:\n\nPyTorchTensorFlow\n\n\nimport torch\n\n# Create a tensor with gradient tracking\nx = torch.tensor(2.0, requires_grad=True)\n\n# Define a simple function\ny = x ** 2\n\n# Compute the gradient\ny.backward()\n\n# Print the gradient\nprint(x.grad)\n\n# Output\ntensor(4.0)\n\n\nimport tensorflow as tf\n\n# Create a tensor with gradient tracking\nx = tf.Variable(2.0)\n\n# Define a simple function\nwith tf.GradientTape() as tape:\n    y = x ** 2\n\n# Compute the gradient\ngrad = tape.gradient(y, x)\n\n# Print the gradient\nprint(grad)\n\n# Output\ntf.Tensor(4.0, shape=(), dtype=float32)\n\n\n\nQuesta differenziazione automatica è una potente funzionalità dei tensori in framework come PyTorch e TensorFlow, che semplifica l’implementazione e l’ottimizzazione di modelli complessi di apprendimento automatico.\n\n\n6.4.2 Grafi computazionali\n\nDefinizione di Grafico\nI grafi computazionali sono una componente chiave di framework di deep learning come TensorFlow e PyTorch. Ci consentono di esprimere architetture di reti neurali complesse in modo efficiente e differenziato. Un grafo computazionale è costituito da un grafo aciclico diretto (directed acyclic graph, DAG) in cui ogni nodo rappresenta un’operazione o una variabile e gli spigoli rappresentano le dipendenze dei dati tra di essi.\nÈ importante differenziare i grafi computazionali dai diagrammi di reti neurali, come quelli per i perceptron multistrato (multilayer perceptrons, MLP), che rappresentano nodi e layer. I diagrammi di reti neurali, come illustrato nel Capitolo 3, visualizzano l’architettura e il flusso di dati attraverso nodi e layer, fornendo una comprensione intuitiva della struttura del modello. Al contrario, i grafi computazionali forniscono una rappresentazione di basso livello delle operazioni matematiche sottostanti e delle dipendenze dei dati necessarie per implementare e addestrare queste reti.\nAd esempio, un nodo potrebbe rappresentare un’operazione di moltiplicazione di matrici, prendendo due matrici di input (o tensori) e producendo una matrice di output (o tensore). Per visualizzarlo, si consideri il semplice esempio in Figura 7.3. Il grafo aciclico orientato sopra calcola \\(z = x \\times y\\), dove ogni variabile è composta solo da numeri.\n\n\n\n\n\n\nFigura 6.5: Esempio elementare di un grafo computazionale.\n\n\n\nFramework come TensorFlow e PyTorch creano grafi computazionali per implementare le architetture delle reti neurali che in genere rappresentiamo con diagrammi. Quando si definisce un layer di rete neurale nel codice (ad esempio, un “layer denso” in TensorFlow), il framework costruisce un grafo computazionale che include tutte le operazioni necessarie (come moltiplicazione di matrici, addizione e funzioni di attivazione) e le relative dipendenze dai dati. Questo grafo consente al framework di gestire in modo efficiente il flusso di dati, ottimizzare l’esecuzione delle operazioni e calcolare automaticamente i gradienti per l’addestramento. Internamente, i grafi computazionali rappresentano astrazioni per layer comuni come quelli convoluzionali, di pooling, ricorrenti e densi, con dati che includono attivazioni, pesi e bias rappresentati in tensori. Questa rappresentazione consente un calcolo efficiente, sfruttando la struttura del grafico per parallelizzare le operazioni e applicare ottimizzazioni.\nAlcuni livelli comuni che i grafi computazionali potrebbero implementare includono layer convoluzionali, di attenzione, ricorrenti e densi. I layer fungono da astrazioni di livello superiore che definiscono calcoli specifici in cima alle operazioni di base rappresentate nel grafo. Ad esempio, un layer Denso esegue la moltiplicazione e l’addizione di matrici tra tensori di input, peso e bias. È importante notare che un layer opera su tensori come input e output; il layer stesso non è un tensore. Alcune differenze chiave tra layer e tensori sono:\n\nI layer contengono stati come pesi e bias. I tensori sono senza stato, contengono solo dati.\nI layer possono modificare lo stato interno durante l’addestramento. I tensori sono immutabili/di sola lettura.\nI layer sono astrazioni di livello superiore. I tensori sono a un livello inferiore e rappresentano direttamente dati e operazioni matematiche.\nI layer definiscono modelli di calcolo fissi. I tensori scorrono tra i livelli durante l’esecuzione.\nI layer vengono utilizzati indirettamente durante la creazione di modelli. I tensori scorrono tra i livelli durante l’esecuzione.\n\nQuindi, mentre i tensori sono una struttura dati fondamentale che i layer consumano e producono, i layer hanno funzionalità aggiuntive per definire operazioni parametrizzate e addestramento. Mentre un layer configura le operazioni tensoriali in background, il layer rimane distinto dagli oggetti tensoriali. L’astrazione del layer rende la creazione e l’addestramento di reti neurali molto più intuitive. Questa astrazione consente agli sviluppatori di creare modelli impilando insieme questi layer senza implementare la logica del layer. Ad esempio, la chiamata di tf.keras.layers.Conv2D in TensorFlow crea un layer convoluzionale. Il framework gestisce il calcolo delle convoluzioni, la gestione dei parametri, ecc. Ciò semplifica lo sviluppo del modello, consentendo agli sviluppatori di concentrarsi sull’architettura anziché sulle implementazioni di basso livello. Le astrazioni dei layer utilizzano implementazioni altamente ottimizzate per le prestazioni. Consentono inoltre la portabilità, poiché la stessa architettura può essere eseguita su backend hardware diversi come GPU e TPU.\nInoltre, i grafi computazionali includono funzioni di attivazione come ReLU, sigmoide e tanh che sono essenziali per le reti neurali e molti framework le forniscono come astrazioni standard. Queste funzioni introducono non linearità che consentono ai modelli di approssimare funzioni complesse. I framework le forniscono come operazioni semplici e predefinite che possono essere utilizzate durante la costruzione di modelli, ad esempio if.nn.relu in TensorFlow. Questa astrazione consente flessibilità, poiché gli sviluppatori possono facilmente scambiare le funzioni di attivazione per ottimizzare le prestazioni. Le attivazioni predefinite sono inoltre ottimizzate dal framework per un’esecuzione più rapida.\nNegli ultimi anni, modelli come ResNets e MobileNets sono emersi come architetture popolari, con i framework attuali che li pre-confezionano come grafi computazionali. Invece di preoccuparsi dei dettagli, gli sviluppatori possono utilizzarli come punto di partenza, personalizzandoli secondo necessità sostituendo i layer. Ciò semplifica e velocizza lo sviluppo del modello, evitando di reinventare le architetture da zero. I modelli predefiniti includono implementazioni ben collaudate e ottimizzate che garantiscono buone prestazioni. Il loro design modulare consente inoltre di trasferire le funzionalità apprese a nuove attività tramite apprendimento tramite trasferimento. Queste architetture predefinite forniscono i mattoni ad alte prestazioni per creare rapidamente modelli robusti.\nQueste astrazioni di layer, funzioni di attivazione e architetture predefinite fornite dai framework costituiscono un grafo computazionale. Quando un utente definisce un layer in un framework (ad esempio, tf.keras.layers.Dense()), il framework configura nodi e bordi del grafo computazionale per rappresentare tale layer. I parametri del layer come pesi e bias diventano variabili nel grafo. I calcoli del layer diventano nodi operativi (come x e y nella figura sopra). Quando si chiama una funzione di attivazione come tf.nn.relu(), il framework aggiunge un nodo operativo ReLU al grafo. Le architetture predefinite sono solo sottografi preconfigurati che possono essere inseriti nel grafo del modello. Quindi, la definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i livelli, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafo.\nCostruiamo implicitamente un grafo computazionale quando definiamo un’architettura di rete neurale in un framework. Il framework utilizza questo grafo per determinare le operazioni da eseguire durante l’addestramento e l’inferenza. I grafi computazionali offrono diversi vantaggi rispetto al codice grezzo e questa è una delle funzionalità principali offerte da un buon framework di ML:\n\nRappresentazione esplicita del flusso di dati e delle operazioni\nCapacità di ottimizzare il grafo prima dell’esecuzione\nDifferenziazione automatica per il training\nAgnosticismo linguistico: il grafo può essere tradotto per essere eseguito su GPU, TPU, ecc.\nPortabilità: il grafo può essere serializzato, salvato e ripristinato in seguito\n\nI grafi computazionali sono i componenti fondamentali dei framework di ML. La definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i layer, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafico. I compilatori e gli ottimizzatori del framework operano su questo grafo per generare codice eseguibile. Le astrazioni forniscono un’API intuitiva per gli sviluppatori per la creazione di grafi computazionali. Sotto, ci sono ancora grafi! Quindi, anche se non si possono manipolare direttamente i grafi come utente del framework, consentono di eseguire ad alto livello e in modo efficiente le specifiche del modello. Le astrazioni semplificano la creazione del modello, mentre i grafi computazionali la rendono possibile.\n\n\nGrafi Statici vs. Dinamici\nI framework di deep learning hanno tradizionalmente seguito uno dei due approcci per esprimere grafi computazionali.\nGrafi statici (declare-then-execute): Con questo modello, l’intero grafo computazionale deve essere definito in anticipo prima di eseguirlo. Tutte le operazioni e le dipendenze dei dati devono essere specificate durante la fase di dichiarazione. TensorFlow originariamente seguiva questo approccio statico: i modelli venivano definiti in un contesto separato e poi veniva creata una sessione per eseguirli. Il vantaggio dei grafi statici è che consentono un’ottimizzazione più aggressiva poiché il framework può vedere il grafo completo. Tuttavia, tende anche a essere meno flessibile per la ricerca e l’interattività. Le modifiche al grafo richiedono la nuova dichiarazione del modello completo.\nPer esempio:\nx = tf.placeholder(tf.float32)\ny = tf.matmul(x, weights) + biases\nIn questo esempio, x è un segnaposto per i dati di input e y è il risultato di un’operazione di moltiplicazione di matrici seguita da un’addizione. Il modello è definito in questa fase di dichiarazione, in cui tutte le operazioni e le variabili devono essere specificate in anticipo.\nUna volta definito l’intero grafo, il framework lo compila e lo ottimizza. Ciò significa che i passaggi computazionali sono definitivamente “scolpiti” e il framework può applicare varie ottimizzazioni per migliorare l’efficienza e le prestazioni. Quando in seguito si esegue il grafo, si forniscono i tensori di input effettivi e le operazioni predefinite vengono eseguite nella sequenza ottimizzata.\nQuesto approccio è simile alla creazione di un progetto in cui ogni dettaglio è pianificato prima dell’inizio della costruzione. Sebbene ciò consenta potenti ottimizzazioni, significa anche che qualsiasi modifica al modello richiede la ridefinizione dell’intero grafo da zero.\nGrafi dinamici (define-by-run): A differenza della dichiarazione (di tutto) prima e dell’esecuzione poi, il grafo viene creato dinamicamente durante l’esecuzione. Non esiste una fase di dichiarazione separata: le operazioni vengono eseguite immediatamente come definite. Questo stile è imperativo e flessibile, facilitando la sperimentazione.\nPyTorch utilizza grafi dinamici, creandoli al volo mentre avviene l’esecuzione. Ad esempio, si consideri il seguente frammento di codice, in cui il grafo viene creato durante l’esecuzione:\nx = torch.randn(4,784)\ny = torch.matmul(x, weights) + biases\nL’esempio sopra non ha fasi separate di compilazione/build/esecuzione. Le operazioni definiscono ed eseguono immediatamente. Con i grafi dinamici, la definizione è intrecciata con l’esecuzione, fornendo un flusso di lavoro più intuitivo e interattivo. Tuttavia, lo svantaggio è che c’è meno potenziale di ottimizzazione poiché il framework vede solo il grafo mentre viene creato.\nDi recente, la distinzione si è offuscata poiché i framework adottano entrambe le modalità. TensorFlow 2.0 passa automaticamente alla modalità di grafo dinamico, consentendo agli utenti di lavorare con quelli statici quando necessario. La dichiarazione dinamica offre flessibilità e facilità d’uso, rendendo i framework più intuitivi, mentre i grafi statici forniscono vantaggi di ottimizzazione a scapito dell’interattività. Il framework ideale bilancia questi approcci. Tabella 6.2 confronta i pro e i contro dei grafi di esecuzione statici e dinamici:\n\n\n\nTabella 6.2: Confronto tra Grafi di Esecuzione Statici (Declare-then-execute) e Dinamici (Define-by-run), evidenziandone i rispettivi pro e contro.\n\n\n\n\n\n\n\n\n\n\nGrafo di esecuzione\nPro\nContro\n\n\n\n\nStatico (Declare-then-execute)\n\nAbilita le ottimizzazioni del grafo visualizzando il modello completo in anticipo\nPuò esportare e distribuire grafici congelati\nIl grafo è impacchettato indipendentemente dal codice\n\n\nMeno flessibile per la ricerca e l’iterazione\nLe modifiche richiedono la ricostruzione del grafo\nL’esecuzione ha fasi di compilazione ed esecuzione separate\n\n\n\nDinamico (Define-by-run)\n\nStile imperativo intuitivo come il codice Python\nAlterna la creazione del grafo con l’esecuzione\nFacile da modificare i grafi\nIl debug si adatta perfettamente al flusso di lavoro\n\n\nPiù difficile da ottimizzare senza un grafo completo\nPossibili rallentamenti dalla creazione del grafo durante l’esecuzione\nPuò richiedere più memoria\n\n\n\n\n\n\n\n\n\n\n6.4.3 Tool della Pipeline dei Dati\nI grafi computazionali possono essere validi solo quanto i dati da cui apprendono e su cui lavorano. Pertanto, alimentare i dati di training in modo efficiente è fondamentale per ottimizzare le prestazioni della “deep neural network” [rete neurale profonda], sebbene spesso venga trascurata come una delle funzionalità principali. Molti framework di IA moderni forniscono pipeline specializzate per acquisire, elaborare e aumentare i set di dati per il training del modello.\n\nLoader dei Dati\nAl centro di queste pipeline ci sono i “data loader”, che gestiscono esempi di training di lettura da fonti come file, database e storage di oggetti. I data loader facilitano il caricamento e la pre-elaborazione efficienti dei dati, cruciali per i modelli di deep learning. Ad esempio, la pipeline di caricamento dati tf.data di TensorFlow è progettata per gestire questo processo. A seconda dell’applicazione, i modelli di deep learning richiedono diversi formati di dati come file CSV o cartelle di immagini. Alcuni formati popolari includono:\n\nCSV, un formato versatile e semplice spesso utilizzato per dati tabellari.\nTFRecord: Formato proprietario di TensorFlow, ottimizzato per le prestazioni.\nParquet: Storage a colonne, che offre compressione e recupero dati efficienti.\nJPEG/PNG: Comunemente utilizzato per dati di immagini.\nWAV/MP3: Formati prevalenti per dati audio.\n\nEsempi di batch di data loader per sfruttare il supporto di vettorizzazione nell’hardware. Il “batching” si riferisce al raggruppamento di più dati per l’elaborazione simultanea, sfruttando le capacità di calcolo vettorizzate di hardware come le GPU. Sebbene le dimensioni tipiche dei batch siano comprese tra 32 e 512 esempi, la dimensione ottimale spesso dipende dall’ingombro di memoria dei dati e dai vincoli hardware specifici. I loader avanzati possono trasmettere in streaming set di dati virtualmente illimitati da dischi e archivi cloud. Trasmettono in streaming grandi dataset da dischi o reti anziché caricarli completamente in memoria, consentendo dimensioni illimitate.\nI data loader possono anche mescolare i dati tra “epoche” per la randomizzazione e le funzionalità di preelaborazione in parallelo con l’addestramento del modello per accelerarne il processo. Mescolare casualmente l’ordine degli esempi tra epoche di training riduce il bias e migliora la generalizzazione.\nI data loader supportano anche strategie di “caching” e “prefetching” per ottimizzare la distribuzione dei dati per un addestramento del modello rapido e fluido. Il caching [memorizzazione nella cache] dei batch preelaborati consente di riutilizzarli in modo efficiente durante più fasi di addestramento ed elimina l’elaborazione ridondante. Il prefetching, al contrario, comporta il precaricamento dei batch successivi, assicurando che il modello non resti mai inattivo in attesa di dati.\n\n\n\n6.4.4 Data Augmentation\nFramework di apprendimento automatico come TensorFlow e PyTorch forniscono strumenti per semplificare e snellire il processo di “data augmentation” [aumento dei dati], migliorando l’efficienza dell’espansione sintetica dei set di dati. Questi framework offrono funzionalità integrate per applicare trasformazioni casuali, come capovolgimento, ritaglio, rotazione, modifica del colore e aggiunta di rumore per le immagini. Per i dati audio, gli aumenti comuni comportano la miscelazione di clip con rumore di fondo o la modulazione di velocità, tono e volume.\nIntegrando gli strumenti di “augmentation” nella pipeline di dati, i framework consentono di applicare queste trasformazioni al volo durante ogni epoca di addestramento. Questo approccio incrementa la variazione nella distribuzione dei dati di addestramento, riducendo così l’overfitting e migliorando la generalizzazione del modello. L’uso di “data loader” performanti in combinazione con ampie capacità di “augmentation” consente ai professionisti di alimentare in modo efficiente set di dati massicci e vari alle reti neurali.\nQueste pipeline di dati “hands-off” rappresentano un miglioramento significativo in termini di usabilità e produttività. Consentono agli sviluppatori di concentrarsi maggiormente sull’architettura del modello e meno sulla manipolazione dei dati durante l’addestramento di modelli di deep learning.\n\n\n6.4.5 Funzioni Loss e Algoritmi di Ottimizzazione\nL’addestramento di una rete neurale è fondamentalmente un processo iterativo che cerca di minimizzare una funzione di loss [perdita]. L’obiettivo è di mettere a punto i pesi e i parametri del modello per produrre previsioni vicine alle vere etichette target. I framework di apprendimento automatico hanno notevolmente semplificato questo processo offrendo funzioni di loss [perdita] e algoritmi di ottimizzazione.\nI framework di apprendimento automatico forniscono funzioni di perdita implementate che sono necessarie per quantificare la differenza tra le previsioni del modello e i valori reali. Diversi set di dati richiedono una diversa funzione di perdita per funzionare correttamente, poiché tale funzione indica al computer l’“obiettivo” a cui mirare. Le funzioni di perdita comunemente utilizzate includono il “Mean Squared Error (MSE)” [errore quadratico medio] per le attività di regressione, la “Cross-Entropy Loss” per le attività di classificazione, e la Kullback-Leibler (KL) per i modelli probabilistici. Ad esempio, tf.keras.losses di TensorFlow contiene una serie di queste funzioni di perdita comunemente utilizzate.\nGli algoritmi di ottimizzazione vengono utilizzati per trovare in modo efficiente il set di parametri del modello che minimizzano la funzione di perdita, assicurando che il modello funzioni bene sui dati di training e si generalizzi a nuovi dati. I framework moderni sono dotati di implementazioni efficienti di diversi algoritmi di ottimizzazione, molti dei quali sono varianti della “discesa del gradiente” con metodi stocastici e tassi di apprendimento adattivo. Alcuni esempi di queste varianti sono Stochastic Gradient Descent, Adagrad, Adadelta e Adam. L’implementazione di tali varianti è fornita in tf.keras.optimizers. Ulteriori informazioni con esempi chiari sono disponibili nella sezione Training dell’IA.\n\n\n6.4.6 Supporto al Training del Modello\nÈ richiesta una fase di compilazione prima di addestrare un modello di rete neurale definito. Durante questa fase, l’architettura di alto livello della rete neurale viene trasformata in un formato eseguibile ottimizzato. Questo processo comprende diverse fasi. La prima fase consiste nel costruire il grafo computazionale, che rappresenta tutte le operazioni matematiche e il flusso di dati all’interno del modello. Ne abbiamo discusso in precedenza.\nDurante l’addestramento, l’attenzione è rivolta all’esecuzione del grafo computazionale. A ogni parametro all’interno del grafo, come pesi e bias, viene assegnato un valore iniziale. A seconda del metodo di inizializzazione scelto, questo valore potrebbe essere casuale o basato su una logica predefinita.\nIl passaggio critico successivo è l’allocazione della memoria. La memoria essenziale è riservata alle operazioni del modello sia su CPU che su GPU, garantendo un’elaborazione efficiente dei dati. Le operazioni del modello vengono poi mappate sulle risorse hardware disponibili, in particolare GPU o TPU, per accelerare l’elaborazione. Una volta completata la compilazione, il modello viene preparato per l’addestramento.\nIl processo di addestramento impiega vari strumenti per migliorare l’efficienza. L’elaborazione batch è comunemente utilizzata per massimizzare la produttività computazionale. Tecniche come la vettorizzazione consentono operazioni su interi array di dati anziché procedere elemento per elemento, il che aumenta la velocità. Ottimizzazioni come la “kernel fusion” (fare riferimento al capitolo Ottimizzazioni) amalgamano più operazioni in un’unica azione, riducendo al minimo il sovraccarico computazionale. Le operazioni possono anche essere segmentate in fasi, facilitando l’elaborazione simultanea di diversi mini-batch in varie parti.\nI framework eseguono costantemente il checkpoint dello stato, preservando le versioni intermedie del modello durante l’addestramento. Ciò garantisce che i progressi vengano recuperati in caso di interruzione e che l’addestramento possa essere ripreso dall’ultimo checkpoint. Inoltre, il sistema monitora attentamente le prestazioni del modello rispetto a un set di dati di convalida. Se il modello inizia a sovradimensionarsi (se le sue prestazioni sul set di convalida diminuiscono), l’addestramento viene automaticamente interrotto, conservando risorse computazionali e tempo.\nI framework di ML incorporano una combinazione di compilazione del modello, metodi di elaborazione batch avanzati e utilità come il checkpoint e l’arresto anticipato. Queste risorse gestiscono gli aspetti complessi delle prestazioni, consentendo ai professionisti di concentrarsi sullo sviluppo e l’addestramento del modello. Di conseguenza, gli sviluppatori sperimentano sia velocità che facilità quando utilizzano le capacità delle reti neurali.\n\n\n6.4.7 Validazione e Analisi\nDopo aver addestrato i modelli di deep learning, i framework forniscono utilità per valutare le prestazioni e ottenere informazioni sul funzionamento dei modelli. Questi strumenti consentono una sperimentazione e un debug disciplinati.\n\nMetriche di Valutazione\nI framework includono implementazioni di comuni metriche di valutazione per la convalida:\n\nAccuratezza: Frazione di previsioni corrette complessive. Sono ampiamente utilizzate per la classificazione.\nPrecisione: Delle previsioni positive, quante erano positive. Utile per set di dati sbilanciati.\nRichiamo: Dei positivi effettivi, quanti ne abbiamo previsti correttamente? Misura della Completezza.\nPunteggio F1: Media armonica di precisione e richiamo. Combina entrambe le metriche.\nAUC-ROC - Area sotto la curva ROC. Sono utilizzate per l’analisi della soglia di classificazione.\nMAP - Mean Average Precision. Valuta le previsioni classificate nel recupero/rilevamento.\nMatrice di Confusione: Matrice che mostra i veri positivi, i veri negativi, i falsi positivi e i falsi negativi. Fornisce una visione più dettagliata delle prestazioni di classificazione.\n\nQueste metriche quantificano le prestazioni del modello sui dati di convalida per il confronto.\n\n\nVisualizzazione\nGli strumenti di visualizzazione forniscono informazioni sui modelli:\n\nCurve di perdita: Tracciano la perdita di training e validazione nel tempo per individuare l’overfitting.\nLoss curves [Griglie di attivazione]: Illustrano le funzionalità apprese dai filtri convoluzionali.\nProjection [Proiezione]: Riduce la dimensionalità per una visualizzazione intuitiva.\nPrecision-recall curves [Curve di richiamo della precisione]: Valutano i compromessi di classificazione.\n\nStrumenti come TensorBoard per TensorFlow e TensorWatch per PyTorch consentono metriche e visualizzazioni in tempo reale durante il training.\n\n\n\n6.4.8 Programmazione differenziabile\nI metodi di addestramento per il machine learning come la backpropagation si basano sulla modifica della funzione di perdita rispetto alla modifica dei pesi (che essenzialmente è la definizione di derivata). Pertanto, la capacità di addestrare rapidamente ed efficientemente grandi modelli di machine learning si basa sulla capacità del computer di prendere derivate. Ciò rende la programmazione differenziabile uno degli elementi più importanti di un framework di apprendimento automatico.\nPossiamo utilizzare quattro metodi principali per far sì che i computer prendano derivate. Innanzitutto, possiamo calcolare manualmente le derivate a mano e inserirle nel computer. Questo diventerebbe rapidamente un incubo con molti layer di reti neurali se dovessimo calcolare manualmente tutte le derivate nei passaggi di backpropagation. Un altro metodo è la differenziazione simbolica utilizzando sistemi di computer algebrici come Mathematica, che può introdurre un layer di inefficienza, poiché è necessario un livello di astrazione per prendere le derivate. Le derivate numeriche, la pratica di approssimare i gradienti utilizzando metodi di differenze finite, soffrono di molti problemi, tra cui elevati costi computazionali e dimensioni della griglia più grandi, che portano a molti errori. Ciò porta alla differenziazione automatica, che sfrutta le funzioni primitive che i computer utilizzano per rappresentare le operazioni per ottenere una derivata esatta. Con la differenziazione automatica, la complessità computazionale del calcolo del gradiente è proporzionale al calcolo della funzione stessa. Le complessità della differenziazione automatica non sono gestite dagli utenti finali al momento, ma le risorse per saperne di più possono essere trovate ampiamente, ad esempio qui. La differenziazione automatica e la programmazione differenziabile di oggi sono onnipresenti e vengono eseguite in modo efficiente e automatico dai moderni framework di machine learning.\n\n\n6.4.9 Accelerazione Hardware\nLa tendenza a formare e distribuire continuamente modelli di apprendimento automatico più grandi ha reso necessario il supporto dell’accelerazione hardware per le piattaforme di machine-learning. Figura 6.6 mostra il gran numero di aziende che offrono acceleratori hardware in diversi domini, come il machine learning “Very Low Power” e quello “Embedded”. I “deep layer” delle reti neurali richiedono molte moltiplicazioni di matrici, che attraggono hardware in grado di calcolare rapidamente e in parallelo tali operazioni. In questo panorama, due architetture hardware, GPU e TPU, sono emerse come scelte principali per l’addestramento di modelli di apprendimento automatico.\nL’uso di acceleratori hardware è iniziato con AlexNet, che ha aperto la strada a lavori futuri per utilizzare le GPU come acceleratori hardware per l’addestramento di modelli di visione artificiale. Le GPU, o “Graphics Processing Units” [unità di elaborazione grafica], eccellono nella gestione di molti calcoli contemporaneamente, il che le rende ideali per le operazioni matriciali centrali per l’addestramento delle reti neurali. La loro architettura, progettata per il rendering della grafica, è perfetta per le operazioni matematiche richieste nell’apprendimento automatico. Sebbene siano molto utili per le attività di apprendimento automatico e siano state implementate in molte piattaforme hardware, le GPU sono comunque di uso generale in quanto possono essere utilizzate per altre applicazioni.\nD’altro canto, le Tensor Processing Units (TPU) sono unità hardware progettate specificamente per le reti neurali. Si concentrano sull’operazione di “moltiplicazione e accumulazione” (MAC) e il loro hardware è costituito da una grande matrice hardware che contiene elementi che calcolano in modo efficiente l’operazione MAC. Questo concetto, chiamato systolic array architecture, è stato ideato da Kung e Leiserson (1979), ma ha dimostrato di essere una struttura utile per calcolare in modo efficiente i prodotti matriciali e altre operazioni all’interno delle reti neurali (come le convoluzioni).\n\nKung, Hsiang Tsung, e Charles E Leiserson. 1979. «Systolic arrays (for VLSI)». In Sparse Matrix Proceedings 1978, 1:256–82. Society for industrial; applied mathematics Philadelphia, PA, USA.\nSebbene le TPU possano ridurre drasticamente i tempi di addestramento, presentano anche degli svantaggi. Ad esempio, molte operazioni all’interno dei framework di apprendimento automatico (principalmente TensorFlow in questo caso, poiché la TPU si integra direttamente con esso) non sono supportate dalle TPU. Non possono inoltre supportare operazioni personalizzate dai framework di apprendimento automatico e la progettazione della rete deve essere strettamente allineata alle capacità hardware.\nOggi, le GPU NVIDIA dominano il training, supportate da librerie software come CUDA, cuDNN e TensorRT. I framework includono anche ottimizzazioni per massimizzare le prestazioni su questi tipi di hardware, come l’eliminazione di connessioni non importanti e la fusione di layer. La combinazione di queste tecniche con l’accelerazione hardware fornisce una maggiore efficienza. Per l’inferenza, l’hardware si sta spostando sempre di più verso ASIC e SoC ottimizzati. Le TPU di Google accelerano i modelli nei data center, mentre Apple, Qualcomm, la famiglia NVIDIA Jetson e altri ora producono chip “mobili” incentrati sull’intelligenza artificiale.\n\n\n\n\n\n\nFigura 6.6: Aziende che offrono acceleratori hardware di ML. Fonte: Gradient Flow.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#sec-ai_frameworks-advanced",
    "href": "contents/frameworks/frameworks.it.html#sec-ai_frameworks-advanced",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.5 Funzionalità Avanzate",
    "text": "6.5 Funzionalità Avanzate\nOltre a fornire gli strumenti essenziali per il training di modelli di apprendimento automatico, i framework offrono anche funzionalità avanzate. Queste funzionalità includono la distribuzione del training su diverse piattaforme hardware, la facile messa a punto di grandi modelli pre-addestrati e l’esemplificazione del “federated learning”. L’implementazione di queste funzionalità in modo indipendente sarebbe altamente complessa e richiederebbe molte risorse, ma i framework semplificano questi processi, rendendo le tecniche avanzate di apprendimento automatico più accessibili.\n\n6.5.1 Training distribuito\nPoiché i modelli di apprendimento automatico sono diventati più grandi nel corso degli anni, è diventato essenziale per i modelli di grandi dimensioni utilizzare più nodi di elaborazione nel processo di training. Questo processo, l’apprendimento distribuito, ha consentito maggiori capacità di training, ma ha anche imposto problemi nell’implementazione.\nPossiamo considerare tre diversi modi per distribuire il lavoro di training dei modelli di apprendimento automatico su più nodi di elaborazione. Il partizionamento dei dati di input (o parallelismo dei dati) si riferisce a più processori che eseguono lo stesso modello su diverse partizioni di input. Questa è l’implementazione più semplice ed è disponibile per molti framework di machine learning. La distribuzione più impegnativa del lavoro è rappresentata dal parallelismo del modello, che si riferisce a più nodi di elaborazione che lavorano su parti diverse del modello, e dal parallelismo del modello pipelined, che si riferisce a più nodi di elaborazione che lavorano su diversi layer del modello sullo stesso input. Gli ultimi due menzionati qui sono aree di ricerca attive.\nI framework di ML che supportano l’apprendimento distribuito includono TensorFlow (tramite il suo modulo tf.distribute), PyTorch (tramite i suoi moduli torch.nn.DataParallel e torch.nn.DistributedDataParallel) e MXNet (tramite la sua API gluon).\n\n\n6.5.2 Conversione del Modello\nI modelli di machine learning hanno vari metodi per essere rappresentati e utilizzati in diversi framework e per diversi tipi di dispositivi. Ad esempio, un modello può essere convertito per essere compatibile con i framework di inferenza all’interno del dispositivo mobile. Il formato di default per i modelli TensorFlow sono i file di checkpoint contenenti pesi e architetture, necessari per riaddestrare i modelli. Tuttavia, i modelli vengono in genere convertiti nel formato TensorFlow Lite per la distribuzione mobile. TensorFlow Lite utilizza una rappresentazione compatta del “flat buffer” e ottimizzazioni per un’inferenza rapida su hardware mobile, eliminando tutto il bagaglio non necessario associato ai metadati di addestramento, come le strutture dei file di checkpoint.\nLe ottimizzazioni del modello come la quantizzazione (vedere il capitolo Ottimizzazioni) possono ottimizzare ulteriormente i modelli per architetture target come i dispositivi mobili. Ciò riduce la precisione di pesi e attivazioni a uint8 o a int8 per un ingombro ridotto e un’esecuzione più rapida con acceleratori hardware supportati. Per la quantizzazione post-training, il convertitore di TensorFlow gestisce automaticamente analisi e conversione.\nFramework come TensorFlow semplificano la distribuzione di modelli addestrati su dispositivi IoT mobili ed embedded tramite API di conversione semplici per il formato TFLite e la quantizzazione. La conversione pronta all’uso consente un’inferenza ad alte prestazioni su dispositivi mobili senza l’onere dell’ottimizzazione manuale. Oltre a TFLite, altri target comuni includono TensorFlow.js per la distribuzione Web, TensorFlow Serving per i servizi cloud e TensorFlow Hub per l’apprendimento tramite trasferimento. Le utility di conversione di TensorFlow gestiscono questi scenari per semplificare i flussi di lavoro end-to-end.\nUlteriori informazioni sulla conversione dei modelli in TensorFlow sono linkate qui.\n\n\n6.5.3 AutoML, No-Code/Low-Code ML\nIn molti casi, l’apprendimento automatico può avere una barriera d’ingresso relativamente alta rispetto ad altri campi. Per addestrare e distribuire con successo modelli, è necessario avere una comprensione critica di una varietà di discipline, dalla scienza dei dati (elaborazione dei dati, pulizia dei dati), strutture di modelli (ottimizzazione degli iperparametri, architettura delle reti neurali), hardware (accelerazione, elaborazione parallela) e altro a seconda del problema in questione. La complessità di questi problemi ha portato all’introduzione di framework come AutoML, che cerca di rendere “l’apprendimento automatico disponibile anche a chi non è esperto di apprendimento automatico” e di “automatizzare la ricerca nell’apprendimento automatico”. Hanno creato AutoWEKA, che aiuta nel complesso processo di selezione degli iperparametri, e Auto-sklearn e Auto-pytorch, un’estensione di AutoWEKA nelle popolari librerie sklearn e PyTorch.\nMentre questi sforzi per automatizzare parti delle attività di apprendimento automatico sono in corso, altri si sono concentrati sulla semplificazione dei modelli tramite l’implementazione di apprendimento automatico “no-code” [senza codice]/low-code [a basso codice], utilizzando un’interfaccia drag-and-drop con un’interfaccia utente di facile navigazione. Aziende come Apple, Google e Amazon hanno già creato queste piattaforme di facile utilizzo per consentire agli utenti di costruire modelli di apprendimento automatico che possono essere integrati nel loro ecosistema.\nQuesti passaggi per rimuovere le barriere all’ingresso continuano a democratizzare il machine learning, semplificano l’accesso per i principianti e semplificano il flusso di lavoro per gli esperti.\n\n\n6.5.4 Metodi di Apprendimento Avanzati\n\nIl Transfer Learning\nIl “transfer learning” è la pratica di utilizzare le conoscenze acquisite da un modello pre-addestrato per addestrare e migliorare le prestazioni di un modello per un’attività diversa. Ad esempio, modelli come MobileNet e ResNet vengono addestrati sul set di dati ImageNet. Per fare ciò, si può congelare il modello pre-addestrato, utilizzandolo come un estrattore di feature per addestrare un modello molto più piccolo costruito sopra l’estrazione di feature. Si può anche mettere a punto l’intero modello per adattarlo al nuovo compito. I framework di apprendimento automatico semplificano il caricamento di modelli pre-addestrati, il congelamento di layer specifici e l’addestramento di layer personalizzati in cima. Semplificano questo processo fornendo API intuitive e un facile accesso a grandi repository di modelli pre-addestrati.\nL’apprendimento tramite trasferimento presenta delle sfide, come l’incapacità del modello modificato di svolgere le sue attività originali dopo l’apprendimento tramite trasferimento. Articoli come “Learning without Forgetting” di Z. Li e Hoiem (2018) cercano di affrontare queste sfide e sono stati implementati nelle moderne piattaforme di apprendimento automatico.\n\nLi, Zhizhong, e Derek Hoiem. 2018. «Learning without Forgetting». IEEE Trans. Pattern Anal. Mach. Intell. 40 (12): 2935–47. https://doi.org/10.1109/tpami.2017.2773081.\n\n\nIl Federated Learning\nIl “Federated learning” di McMahan et al. (2017) è una forma di elaborazione distribuita che prevede l’addestramento di modelli su dispositivi personali anziché la centralizzazione dei dati su un singolo server (Figura 6.7). Inizialmente, un modello globale di base viene addestrato su un server centrale per essere distribuito a tutti i dispositivi. Utilizzando questo modello di base, i dispositivi calcolano individualmente i gradienti e li inviano all’hub centrale. Intuitivamente, questo trasferisce i parametri del modello anziché i dati stessi. L’apprendimento federato migliora la privacy mantenendo i dati sensibili sui dispositivi locali e condividendo gli aggiornamenti del modello solo con un server centrale. Questo metodo è particolarmente utile quando si gestiscono dati sensibili o quando un’infrastruttura su larga scala non è praticabile.\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, e Blaise Agüera y Arcas. 2017. «Communication-Efficient Learning of Deep Networks from Decentralized Data». In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, a cura di Aarti Singh e Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\n\n\n\n\nFigura 6.7: Un approccio con server centralizzato al “federated learning”. Fonte: NVIDIA.\n\n\n\nTuttavia, il federated learning deve affrontare sfide come garantire l’accuratezza dei dati, gestire dati non-IID (independent and identically distributed) [indipendenti e distribuiti in modo identico], gestire la produzione di dati non bilanciata e superare il sovraccarico della comunicazione e l’eterogeneità dei dispositivi. Anche i problemi di privacy e sicurezza, come gli attacchi di inversione del gradiente, pongono sfide significative.\nI framework di apprendimento automatico semplificano l’implementazione dell’apprendimento federato fornendo gli strumenti e le librerie necessarie. Ad esempio, TensorFlow Federated (TFF) offre un framework open source per supportare l’apprendimento federato. TFF consente agli sviluppatori di simulare e implementare algoritmi di apprendimento federato, offrendo un core federato per operazioni di basso livello e API di alto livello per attività federate comuni. Si integra perfettamente con TensorFlow, consentendo l’uso di modelli e ottimizzatori TensorFlow in un ambiente federato. TFF supporta tecniche di aggregazione sicure per migliorare la privacy e consente la personalizzazione degli algoritmi di apprendimento federato. Sfruttando questi strumenti, gli sviluppatori possono distribuire in modo efficiente il training, perfezionare i modelli pre-addestrati e gestire le complessità intrinseche dell’apprendimento federato.\nSono stati sviluppati anche altri programmi open source come Flower per semplificare l’implementazione dell’apprendimento federato con vari framework di machine learning.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#specializzazione-del-framework",
    "href": "contents/frameworks/frameworks.it.html#specializzazione-del-framework",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.6 Specializzazione del Framework",
    "text": "6.6 Specializzazione del Framework\nFinora abbiamo parlato in generale dei framework di ML. Tuttavia, in genere, i framework sono ottimizzati in base alle capacità computazionali e ai requisiti applicativi dell’ambiente target, che vanno dal cloud all’edge ai dispositivi minuscoli. La scelta del framework giusto è fondamentale in base all’ambiente target per la distribuzione. Questa sezione fornisce una panoramica dei principali tipi di framework di IA su misura per ambienti cloud, edge e TinyML per aiutare a comprendere le somiglianze e le differenze tra questi ecosistemi.\n\n6.6.1 Cloud\nI framework di IA basati su cloud presuppongono l’accesso a un’ampia potenza di calcolo, memoria e risorse di archiviazione nel cloud. In genere supportano sia il training che l’inferenza. I framework di IA basati su cloud sono adatti per applicazioni in cui i dati possono essere inviati al cloud per l’elaborazione, come servizi di IA basati su cloud, analisi di dati su larga scala e applicazioni Web. I framework di IA cloud più diffusi includono quelli che abbiamo menzionato in precedenza, come TensorFlow, PyTorch, MXNet, Keras, ecc. Questi framework utilizzano GPU, TPU, training distribuito e AutoML per fornire IA scalabile. Concetti come model serving, MLOps e AIOps sono correlati all’operatività dell’IA nel cloud. L’IA cloud alimenta servizi come Google Cloud AI e consente il “transfer learning” tramite modelli pre-addestrati.\n\n\n6.6.2 Edge\nI framework Edge AI sono pensati per distribuire modelli di IA su dispositivi IoT, smartphone e server edge. I framework Edge AI sono ottimizzati per dispositivi con risorse di calcolo moderate, bilanciando potenza e prestazioni. I framework Edge AI sono ideali per applicazioni che richiedono elaborazione in tempo reale o quasi reale, tra cui robotica, veicoli autonomi e dispositivi intelligenti. I principali framework Edge AI includono TensorFlow Lite, PyTorch Mobile, CoreML e altri. Impiegano ottimizzazioni come compressione del modello, quantizzazione ed architetture di reti neurali efficienti. Il supporto hardware include CPU, GPU, NPU e acceleratori come Edge TPU. Edge AI consente casi d’uso come visione mobile, riconoscimento vocale e rilevamento di anomalie in tempo reale.\n\n\n6.6.3 Embedded\nI framework TinyML sono specializzati per distribuire modelli AI su dispositivi con risorse estremamente limitate, in particolare microcontrollori e sensori all’interno dell’ecosistema IoT. I framework TinyML sono progettati per dispositivi con risorse limitate, enfatizzando memoria minima e consumo energetico. I framework TinyML sono specializzati per casi d’uso su dispositivi IoT con risorse limitate per applicazioni di manutenzione predittiva, riconoscimento dei gesti e monitoraggio ambientale. I principali framework TinyML includono TensorFlow Lite Micro, uTensor e ARM NN. Ottimizzano modelli complessi per adattarli a kilobyte di memoria tramite tecniche come l’addestramento consapevole della quantizzazione e la precisione ridotta. TinyML consente il rilevamento intelligente su dispositivi alimentati a batteria, consentendo l’apprendimento collaborativo tramite apprendimento federato. La scelta del framework implica il bilanciamento delle prestazioni del modello e dei vincoli computazionali della piattaforma target, che sia cloud, edge o TinyML. Tabella 6.3 confronta i principali framework di IA negli ambienti cloud, edge e TinyML:\n\n\n\nTabella 6.3: Confronto dei tipi di framework per Cloud AI, Edge AI e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nTipo di framework\nEsempi\nTecnologie chiave\nCasi d’uso\n\n\n\n\nCloud AI\nTensorFlow, PyTorch, MXNet, Keras\nGPU, TPU, addestramento distribuito, AutoML, MLOps\nServizi cloud, app Web, analisi di big data\n\n\nEdge AI\nTensorFlow Lite, PyTorch Mobile, Core ML\nOttimizzazione del modello, compressione, quantizzazione, architetture NN efficienti\nApp mobili, sistemi autonomi, elaborazione in tempo reale\n\n\nTinyML\nTensorFlow Lite Micro, uTensor, ARM NN\nTraining consapevole della quantizzazione, precisione ridotta, ricerca di architettura neurale\nSensori IoT, dispositivi indossabili, manutenzione predittiva, riconoscimento dei gesti\n\n\n\n\n\n\nDifferenze principali:\n\nCloud AI sfrutta un’enorme potenza di calcolo per modelli complessi utilizzando GPU/TPU e training distribuito.\nEdge AI ottimizza i modelli per l’esecuzione locale su dispositivi edge con risorse limitate.\nTinyML adatta i modelli a una memoria estremamente bassa e calcola ambienti come i microcontrollori.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#sec-ai_frameworks_embedded",
    "href": "contents/frameworks/frameworks.it.html#sec-ai_frameworks_embedded",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.7 Framework di Intelligenza Artificiale Embedded",
    "text": "6.7 Framework di Intelligenza Artificiale Embedded\n\n6.7.1 Vincoli di Risorse\nI sistemi embedded affrontano gravi limitazioni di risorse che pongono sfide uniche quando si distribuiscono modelli di machine learning rispetto alle piattaforme di elaborazione tradizionali. Ad esempio, le unità microcontrollore (MCU) comunemente utilizzate nei dispositivi IoT hanno spesso:\n\nRAM varia da decine di kilobyte a pochi megabyte. Il popolare MCU ESP8266 ha circa 80 KB di RAM a disposizione degli sviluppatori. Ciò contrasta con 8 GB o più su laptop e desktop tipici odierni.\nMemoria Flash varia da centinaia di kilobyte a pochi megabyte. Il microcontrollore Arduino Uno fornisce solo 32 KB di archiviazione del codice. I computer standard odierni hanno un’archiviazione su disco nell’ordine dei terabyte.\nPotenza di elaborazione da pochi MHz a circa 200 MHz. L’ESP8266 funziona a 80 MHz. Questo è di diversi ordini di grandezza più lento delle CPU multi-core multi-GHz nei server e nei laptop di fascia alta.\n\nQuesti vincoli rigorosi spesso rendono impossibile l’addestramento di modelli di apprendimento automatico direttamente sui microcontrollori. La RAM limitata impedisce la gestione di grandi set di dati per il training. L’uso di energia per l’addestramento esaurirebbe rapidamente anche i dispositivi alimentati a batteria. Al contrario, i modelli vengono addestrati su sistemi ricchi di risorse e distribuiti su microcontrollori per un’inferenza ottimizzata. Ma anche l’inferenza pone delle sfide:\n\nDimensioni del Modello: I modelli di intelligenza artificiale sono troppo grandi per adattarsi a dispositivi IoT ed embedded. Ciò richiede tecniche di compressione del modello, come quantizzazione, potatura e “knowledge distillation” [distillazione della conoscenza]. Inoltre, come vedremo, molti dei framework utilizzati dagli sviluppatori di intelligenza artificiale hanno grandi quantità di overhead e librerie integrate che i sistemi embedded non possono supportare.\nComplessità delle Attività: Con solo decine di KB o pochi MB di RAM, i dispositivi IoT e i sistemi embedded sono limitati nella complessità delle attività che possono gestire. Le attività che richiedono grandi set di dati o algoritmi sofisticati, ad esempio LLM, che verrebbero eseguiti senza problemi su piattaforme di elaborazione tradizionali potrebbero non essere fattibili su sistemi embedded senza compressione o altre tecniche di ottimizzazione a causa delle limitazioni di memoria.\nArchiviazione ed Elaborazione dei Dati: I sistemi embedded spesso elaborano i dati in tempo reale e potrebbero archiviarne solo piccole quantità localmente. Al contrario, i sistemi di elaborazione tradizionali possono contenere ed elaborare grandi set di dati in memoria, consentendo un’analisi più rapida delle operazioni sui dati e aggiornamenti in tempo reale.\nSicurezza e Privacy: La poca memoria limita anche la complessità degli algoritmi e dei protocolli di sicurezza, la crittografia dei dati, le protezioni da reverse engineering e altro che può essere implementato sul dispositivo. Ciò potrebbe rendere alcuni dispositivi IoT più vulnerabili agli attacchi.\n\nDi conseguenza, le ottimizzazioni software specializzate e i framework ML su misura per i microcontrollori devono funzionare entro questi stretti limiti delle risorse. Tecniche di ottimizzazione intelligenti come quantizzazione, potatura e distillazione della conoscenza comprimono i modelli per adattarli alla memoria limitata (vedere la sezione Ottimizzazioni). Gli insegnamenti tratti dalla ricerca di architettura neurale aiutano a guidare la progettazione dei modelli.\nI miglioramenti hardware come gli acceleratori ML dedicati sui microcontrollori aiutano anche ad alleviare i vincoli. Ad esempio, Hexagon DSP di Qualcomm accelera i modelli TensorFlow Lite sui chip mobili Snapdragon. Google Edge TPU racchiude le prestazioni ML in un piccolo ASIC per dispositivi edge. ARM Ethos-U55 offre un’inferenza efficiente sui microcontrollori di classe Cortex-M. Questi chip ML personalizzati sbloccano funzionalità avanzate per applicazioni con risorse limitate.\nA causa della potenza di elaborazione limitata, è quasi sempre impossibile addestrare modelli di intelligenza artificiale su IoT o sistemi embedded. Invece, i modelli vengono addestrati su potenti computer tradizionali (spesso con GPU) e poi distribuiti sul dispositivo embedded per l’inferenza. TinyML si occupa specificamente di questo, assicurando che i modelli siano sufficientemente leggeri per l’inferenza in tempo reale su questi dispositivi limitati.\n\n\n6.7.2 Framework e Librerie\nI framework di intelligenza artificiale embedded sono strumenti software e librerie progettati per abilitare funzionalità di intelligenza artificiale e ML su sistemi embedded. Questi framework sono essenziali per portare l’intelligenza artificiale su dispositivi IoT, robotica e altre piattaforme di edge computing e sono progettati per funzionare dove risorse di elaborazione, memoria e consumo energetico sono limitati.\n\n\n6.7.3 Sfide\nSebbene i sistemi embedded rappresentino un’enorme opportunità per l’implementazione dell’apprendimento automatico per abilitare capacità intelligenti in edge, questi ambienti con risorse limitate pongono sfide significative. A differenza dei tipici ambienti cloud o desktop ricchi di risorse computazionali, i dispositivi embedded introducono gravi limitazioni in termini di memoria, potenza di elaborazione, efficienza energetica e hardware specializzato. Di conseguenza, le tecniche e i framework di apprendimento automatico esistenti progettati per cluster di server con risorse abbondanti non si traducono direttamente nei sistemi embedded. Questa sezione svela alcune delle sfide e delle opportunità per i sistemi embedded e i framework ML.\n\nEcosistema Frammentato\nLa mancanza di un framework ML unificato ha portato a un ecosistema altamente frammentato. Gli ingegneri di aziende come STMicroelectronics, NXP Semiconductors e Renesas hanno dovuto sviluppare soluzioni personalizzate su misura per le loro specifiche architetture di microcontrollori e DSP. Questi framework ad hoc richiedevano un’ampia ottimizzazione manuale per ogni piattaforma hardware di basso livello. Ciò ha reso estremamente difficile il porting dei modelli, richiedendo la riqualificazione per nuove architetture Arm, RISC-V o proprietarie.\n\n\nEsigenze Hardware Disparate\nSenza un framework condiviso, non esisteva un modo standard per valutare le capacità dell’hardware. Fornitori come Intel, Qualcomm e NVIDIA crearono soluzioni integrate, combinando modelli e migliorando software e hardware. Ciò rese difficile discernere i motivi del guadagni di prestazioni, se fosse merito dei nuovi progetti di chip come i core x86 a basso consumo di Intel o le ottimizzazioni software. Era necessario un framework standard affinché i fornitori potessero valutare le capacità del loro hardware in modo equo e riproducibile.\n\n\nMancanza di Portabilità\nCon strumenti standardizzati, adattare modelli addestrati in framework comuni come TensorFlow o PyTorch per funzionare in modo efficiente sui microcontrollori era più facile. Richiedeva una traduzione manuale dispendiosa, in termini di tempo, dei modelli per l’esecuzione su DSP specializzati di aziende come CEVA o core Arm M-series a basso consumo. Nessuno strumento immediato consentiva l’implementazione portatile su diverse architetture.\n\n\nInfrastruttura Incompleta\nL’infrastruttura per supportare i flussi di lavoro di sviluppo dei modelli chiave doveva essere migliorata. È necessario un maggiore supporto per le tecniche di compressione per adattare modelli di grandi dimensioni a budget di memoria limitati. Mancavano strumenti per la quantizzazione per ridurre la precisione per un’inferenza più rapida. Le API standardizzate per l’integrazione nelle applicazioni erano incomplete. Mancavano funzionalità essenziali come il debugging sul dispositivo, le metriche e la profilazione delle prestazioni. Queste lacune hanno aumentato i costi e la difficoltà dello sviluppo ML embedded.\n\n\nNessun Benchmark Standard\nSenza benchmark unificati, non esisteva un modo standard per valutare e confrontare le capacità di diverse piattaforme hardware di fornitori come NVIDIA, Arm e Ambiq Micro.. Le valutazioni esistenti si basavano su benchmark proprietari pensati per mostrare i punti di forza di specifici chip. Ciò rendeva impossibile misurare i miglioramenti hardware in modo oggettivo, imparziale e imparziale. Il capitolo Benchmarking AI affronta questo argomento in modo più dettagliato.\n\n\nTest Minimi del Mondo Reale\nGran parte dei benchmark si basava su dati sintetici. Testare rigorosamente i modelli su applicazioni embedded nel mondo reale era difficile senza set di dati e benchmark standardizzati, sollevando dubbi su come le dichiarazioni sulle prestazioni si sarebbero tradotte in un utilizzo nel mondo reale. Erano necessari test più approfonditi per convalidare i chip in casi di utilizzo reali.\nLa mancanza di framework e infrastrutture condivisi ha rallentato l’adozione di TinyML, ostacolandone l’integrazione nei prodotti embedded. I recenti framework standard hanno iniziato ad affrontare questi problemi attraverso una migliore portabilità, profilazione delle prestazioni e supporto per il benchmarking. Tuttavia, è ancora necessaria un’innovazione continua per consentire un’implementazione fluida e conveniente dell’IA nei dispositivi edge.\n\n\nRiepilogo\nL’assenza di framework, benchmark e infrastrutture standardizzati per ML embedded ne ha tradizionalmente ostacolato l’adozione. Tuttavia, sono stati compiuti recenti progressi nello sviluppo di framework condivisi come TensorFlow Lite Micro e suite di benchmark come MLPerf Tiny che mirano ad accelerare la proliferazione di soluzioni TinyML. Tuttavia, superare la frammentazione e la difficoltà dell’implementazione embedded rimane un processo in corso.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#esempi",
    "href": "contents/frameworks/frameworks.it.html#esempi",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.8 Esempi",
    "text": "6.8 Esempi\nIl deployment [distribuzione] di machine learning su microcontrollori e altri dispositivi embedded richiede spesso librerie software e framework appositamente ottimizzati per funzionare entro vincoli rigorosi di memoria, elaborazione e potenza. Esistono diverse opzioni per eseguire l’inferenza su hardware con risorse limitate, ciascuna con il proprio approccio all’ottimizzazione dell’esecuzione del modello. Questa sezione esplorerà le caratteristiche chiave e i principi di progettazione alla base di TFLite Micro, TinyEngine e CMSIS-NN, fornendo informazioni su come ogni framework affronta il complesso problema dell’esecuzione di reti neurali molto accurata ma efficiente sui microcontrollori. Mostrerà inoltre diversi approcci per l’implementazione di framework TinyML efficienti.\nTabella 6.4 riassume le principali differenze e somiglianze tra questi tre framework di inferenza di apprendimento automatico specializzati per sistemi embedded e microcontrollori.\n\n\n\nTabella 6.4: Confronto dei framework: TensorFlow Lite Micro, TinyEngine e CMSIS-NN\n\n\n\n\n\n\n\n\n\n\n\nFramework\nTensorFlow Lite Micro\nTinyEngine\nCMSIS-NN\n\n\n\n\nApproccio\nBasato su interprete\nCompilazione statica\nKernel di reti neurali ottimizzati\n\n\nFocus sull’hardware\nDispositivi embedded generali\nMicrocontrollori\nProcessori ARM Cortex-M\n\n\nSupporto aritmetico\nVirgola mobile\nVirgola mobile, virgola fissa\nVirgola mobile, virgola fissa\n\n\nSupporto del modello\nModelli di rete neurale generale\nModelli co-progettati con TinyNAS\nTipi di livelli di rete neurale comuni\n\n\nImpronta del codice\nPiù grande grazie all’inclusione di interprete e operazioni\nPiccola, include solo le operazioni necessarie per il modello\nNativamente leggera\n\n\nLatenza\nPiù alta grazie a overhead di interpretazione\nMolto bassa grazie al modello compilato\nfocalizzato sulla bassa latenza\n\n\nGestione della memoria\nGestita dinamicamente da interprete\nOttimizzazione a livello di modello\nStrumenti per un’allocazione efficiente\n\n\nApproccio di ottimizzazione\nAlcune funzionalità di e generazione del codice\nKernel specializzati, fusione di operatori\nOttimizzazioni di assemblaggio specifiche dell’architettura\n\n\nPrincipali vantaggi\nFlessibilità, portabilità, facile aggiornamento dei modelli\nMassimizza le prestazioni, ottimizza l’utilizzo della memoria\nAccelerazione hardware, API standardizzata, portabilità\n\n\n\n\n\n\nNe comprenderemo ciascuno in modo più dettagliato nelle sezioni seguenti.\n\n6.8.1 Interprete\nTensorFlow Lite Micro (TFLM) è un framework di inferenza di apprendimento automatico progettato per dispositivi embedded con risorse limitate. Utilizza un interprete per caricare ed eseguire modelli di apprendimento automatico, il che fornisce flessibilità e facilità di aggiornamento dei modelli sul campo (David et al. 2021).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\nGli interpreti tradizionali spesso hanno un overhead di branching [diramazione] significativo, che può ridurre le prestazioni. Tuttavia, l’interpretazione del modello di machine learning trae vantaggio dall’efficienza dei kernel di lunga durata, in cui ogni runtime del kernel è relativamente grande e aiuta a mitigare l’overhead dell’interprete.\nUn’alternativa a un motore di inferenza basato su interprete è quella di generare codice nativo da un modello durante l’esportazione. Ciò può migliorare le prestazioni, ma sacrifica portabilità e flessibilità, poiché il codice generato deve essere ricompilato per ogni piattaforma target e deve essere sostituito completamente per modificare un modello.\nTFLM bilancia la semplicità della compilazione del codice e la flessibilità di un approccio basato su interprete includendo alcune funzionalità di generazione del codice. Ad esempio, la libreria può essere costruita esclusivamente da file sorgenti, offrendo gran parte della semplicità della compilazione associata alla generazione di codice, pur mantenendo i vantaggi di un framework che esegue il modello interpretandolo.\nUn approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice per l’inferenza di apprendimento automatico su dispositivi embedded:\n\nFlessibilità: I modelli possono essere aggiornati sul campo senza ricompilare l’intera applicazione.\nPortabilità: L’interprete può essere utilizzato per eseguire modelli su diverse piattaforme target senza dover effettuare il porting del codice.\nEfficienza della Memoria: L’interprete può condividere il codice su più modelli, riducendo l’utilizzo della memoria.\nFacilità di sviluppo: Gli interpreti sono più facili da sviluppare e gestire rispetto ai generatori di codice.\n\nTensorFlow Lite Micro è un framework potente e flessibile per l’inferenza di apprendimento automatico su dispositivi embedded. Il suo approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice, tra cui flessibilità, portabilità, efficienza della memoria e facilità di sviluppo.\n\n\n6.8.2 Basati su Compilatore\nTinyEngine è un framework di inferenza ML progettato specificamente per microcontrollori con risorse limitate. Utilizza diverse ottimizzazioni per consentire l’esecuzione di reti neurali molto accurate entro i vincoli rigorosi di memoria, elaborazione e archiviazione sui microcontrollori (Lin et al. 2020).\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\nMentre framework di inferenza come TFLite Micro utilizzano interpreti per eseguire il grafo della rete neurale in modo dinamico in fase di esecuzione, ciò aggiunge un overhead significativo per quanto riguarda l’utilizzo della memoria per archiviare metadati, latenza di interpretazione e mancanza di ottimizzazioni. Tuttavia, TFLite sostiene che l’overhead è piccolo. TinyEngine elimina questo overhead utilizzando un approccio di generazione del codice. Analizza il grafo di rete durante la compilazione e genera codice specializzato per eseguire solo quel modello. Questo codice viene compilato in modo nativo nel binario dell’applicazione, evitando i costi di interpretazione in fase di esecuzione.\nI framework ML convenzionali pianificano la memoria per layer, cercando di ridurre al minimo l’utilizzo per ogni layer separatamente. TinyEngine esegue la pianificazione a livello di modello anziché analizzare l’utilizzo della memoria tra i layer. Assegna una dimensione di buffer comune in base alle esigenze massime di memoria di tutti i layer. Questo buffer viene quindi condiviso in modo efficiente tra i layer per aumentare il riutilizzo dei dati.\nTinyEngine è inoltre specializzato nei kernel per ogni layer tramite tecniche come operatori di tiling, unrolling e fusing. Ad esempio, genererà kernel di calcolo unrolled [srotolato] con il numero di loop necessari per una convoluzione 3x3 o 5x5. Questi kernel specializzati estraggono le massime prestazioni dall’hardware del microcontrollore. Utilizza convoluzioni depthwise [in profondità] ottimizzate per ridurre al minimo le allocazioni di memoria calcolando l’output di ogni canale posizionato sui dati del canale di input. Questa tecnica sfrutta la natura separabile dei canali delle convoluzioni depthwise per ridurre le dimensioni di picco della memoria.\nCome TFLite Micro, il binario TinyEngine compilato include solo le operazioni necessarie per un modello specifico anziché tutte le operazioni possibili. Ciò si traduce in un footprint binario molto piccolo, mantenendo basse le dimensioni del codice per i dispositivi con limiti di memoria.\nUna differenza tra TFLite Micro e TinyEngine è che quest’ultimo è co-progettato con “TinyNAS”, un metodo di ricerca di architettura per modelli di microcontrollori simile al NAS differenziale per microcontrollori. L’efficienza di TinyEngine consente di esplorare modelli più grandi e accurati tramite NAS. Fornisce inoltre feedback a TinyNAS su quali modelli possono rientrare nei vincoli hardware.\nAttraverso varie tecniche personalizzate, come la compilazione statica, la pianificazione basata sul modello, kernel specializzati e la co-progettazione con NAS, TinyEngine consente un’inferenza di deep learning ad alta precisione entro i vincoli di risorse rigorosi dei microcontrollori.\n\n\n6.8.3 Libreria\nCMSIS-NN, acronimo di Cortex Microcontroller Software Interface Standard for Neural Networks, è una libreria software ideata da ARM. Offre un’interfaccia standardizzata per distribuire l’inferenza di reti neurali su microcontrollori e sistemi embedded, concentrandosi sull’ottimizzazione per i processori ARM Cortex-M (Lai, Suda, e Chandra 2018).\n\nLai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. «Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus». ArXiv preprint abs/1801.06601. https://arxiv.org/abs/1801.06601.\nKernel di Reti Neurali: CMSIS-NN ha kernel altamente efficienti che gestiscono operazioni fondamentali di reti neurali come convoluzione, pooling, layer completamente connessi e funzioni di attivazione. Si rivolge a un’ampia gamma di modelli di reti neurali supportando l’aritmetica a virgola mobile e fissa. Quest’ultima è particolarmente utile per i dispositivi con risorse limitate in quanto riduce i requisiti di memoria e di calcolo (Quantization).\nAccelerazione Hardware: CMSIS-NN sfrutta la potenza delle istruzioni SIMD (Single Instruction, Multiple Data) disponibili su molti processori Cortex-M. Ciò consente l’elaborazione parallela di più elementi di dati all’interno di una singola istruzione, aumentando così l’efficienza computazionale. Alcuni processori Cortex-M dispongono di estensioni di Digital Signal Processing (DSP) che CMSIS-NN può sfruttare per l’esecuzione accelerata della rete neurale. La libreria include anche ottimizzazioni a livello di assembly su misura per specifiche architetture di microcontrollori per migliorare ulteriormente le prestazioni.\nAPI standardizzata: CMSIS-NN offre un’API coerente e astratta che protegge gli sviluppatori dalle complessità dei dettagli hardware di basso livello. Ciò semplifica l’integrazione dei modelli di rete neurale nelle applicazioni. Può anche comprendere strumenti o utilità per convertire i formati di modelli di rete neurale più diffusi in un formato compatibile con CMSIS-NN.\nGestione della Memoria: CMSIS-NN fornisce funzioni per un’allocazione e una gestione efficienti della memoria, il che è fondamentale nei sistemi embedded in cui le risorse di memoria sono scarse. Garantisce un utilizzo ottimale della memoria durante l’inferenza e, in alcuni casi, consente operazioni in loco per ridurre il sovraccarico di memoria.\nPortabilità: CMSIS-NN è progettato per la portabilità su vari processori Cortex-M. Questo consente agli sviluppatori di scrivere codice che possa funzionare su diversi microcontrollori senza modifiche significative.\nBassa Latenza: CMSIS-NN riduce al minimo la latenza di inferenza, rendendolo una scelta ideale per applicazioni in tempo reale in cui è fondamentale prendere decisioni rapide.\nEfficienza Energetica: La libreria è progettata con un focus sull’efficienza energetica, rendendola adatta per dispositivi alimentati a batteria e con vincoli energetici.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#scelta-del-framework-giusto",
    "href": "contents/frameworks/frameworks.it.html#scelta-del-framework-giusto",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.9 Scelta del Framework Giusto",
    "text": "6.9 Scelta del Framework Giusto\nLa scelta del framework di machine learning giusto per una determinata applicazione richiede un’attenta valutazione di modelli, hardware e considerazioni software. Analizzando questi tre aspetti (modelli, hardware e software), gli ingegneri di ML possono selezionare il framework ottimale e personalizzarlo in base alle esigenze per applicazioni ML su dispositivo efficienti e performanti. L’obiettivo è bilanciare complessità del modello, limitazioni hardware e integrazione software per progettare una pipeline ML su misura per dispositivi embedded e edge.\n\n\n\n\n\n\nFigura 6.8: Confronto tra Framework TensorFlow - Generale. Fonte: TensorFlow.\n\n\n\n\n6.9.1 Modello\nTensorFlow supporta molte più operazioni (op) rispetto a TensorFlow Lite e TensorFlow Lite Micro, in quanto viene solitamente utilizzato per la ricerca o l’implementazione cloud, che richiedono un numero elevato di operatori e una maggiore flessibilità (vedere Figura 6.8). TensorFlow Lite supporta operazioni selezionate per il training sul dispositivo, mentre TensorFlow Micro no. TensorFlow Lite supporta anche forme dinamiche e training consapevole della quantizzazione, mentre TensorFlow Micro no. Al contrario, TensorFlow Lite e TensorFlow Micro offrono strumenti e supporto di quantizzazione nativi, dove la quantizzazione si riferisce alla trasformazione di un programma ML in una rappresentazione approssimata con operazioni di precisione inferiore disponibili.\n\n\n6.9.2 Software\n\n\n\n\n\n\nFigura 6.9: Confronto tra Framework TensorFlow - Software. Fonte: TensorFlow.\n\n\n\nTensorFlow Lite Micro non supporta il sistema operativo, mentre TensorFlow e TensorFlow Lite sì, per ridurre il sovraccarico di memoria, velocizzare i tempi di avvio e consumare meno energia (vedere Figura 6.9). TensorFlow Lite Micro può essere utilizzato insieme a sistemi operativi in tempo reale (RTOS) come FreeRTOS, Zephyr e Mbed OS. TensorFlow Lite e TensorFlow Lite Micro supportano la mappatura della memoria del modello, consentendo l’accesso diretto ai modelli dalla memoria flash anziché caricarli nella RAM, cosa che TensorFlow non fa. TensorFlow e TensorFlow Lite supportano la “accelerator delegation” per pianificare il codice su diversi acceleratori, mentre TensorFlow Lite Micro no, poiché i sistemi embedded tendono ad avere una gamma limitata di acceleratori specializzati.\n\n\n6.9.3 Hardware\n\n\n\n\n\n\nFigura 6.10: Confronto tra Framework TensorFlow - Hardware. Fonte: TensorFlow.\n\n\n\nTensorFlow Lite e TensorFlow Lite Micro hanno dimensioni binarie di base e footprint di memoria significativamente più piccoli rispetto a TensorFlow (vedere Figura 6.10). Ad esempio, un tipico binario TensorFlow Lite Micro è inferiore a 200 KB, mentre TensorFlow è molto più grande. Ciò è dovuto agli ambienti con risorse limitate dei sistemi embedded. TensorFlow supporta x86, TPU e GPU come NVIDIA, AMD e Intel. TensorFlow Lite supporta i processori Arm Cortex-A e x86 comunemente utilizzati su telefoni cellulari e tablet. Quest’ultimo è privo di tutta la logica di training non necessaria per l’implementazione sul dispositivo. TensorFlow Lite Micro fornisce supporto per core Arm Cortex M focalizzati sui microcontrollori come M0, M3, M4 e M7, nonché DSP come Hexagon e SHARC e MCU come STM32, NXP Kinetis, Microchip AVR.\n\n\n6.9.4 Altri Fattori\nLa selezione del framework di IA appropriato è essenziale per garantire che i sistemi embedded possano eseguire in modo efficiente i modelli di IA. Diversi fattori chiave oltre a modelli, hardware e software dovrebbero essere presi in considerazione quando si valutano i framework IA per i sistemi embedded. Altri fattori chiave da considerare quando si sceglie un framework di apprendimento automatico sono prestazioni, scalabilità, facilità d’uso, integrazione con strumenti di ingegneria dei dati, integrazione con strumenti di ottimizzazione dei modelli e supporto della community. Comprendendo questi fattori, si possono prendere decisioni informate e massimizzare il potenziale delle iniziative di machine-learning.\n\nPrestazioni\nLe prestazioni sono fondamentali nei sistemi embedded in cui le risorse di calcolo sono limitate. Valutare la capacità del framework di ottimizzare l’inferenza del modello per l’hardware embedded. La quantizzazione del modello e il supporto dell’accelerazione hardware sono cruciali per ottenere un’inferenza efficiente.\n\n\nScalabilità\nLa scalabilità è essenziale quando si considera la potenziale crescita di un progetto di IA embedded. Il framework dovrebbe supportare l’implementazione di modelli su vari dispositivi embedded, dai microcontrollori ai processori più potenti. Dovrebbe inoltre gestire senza problemi sia le distribuzioni su piccola che su larga scala.\n\n\nIntegrazione con Strumenti di Data Engineering\nGli strumenti di ingegneria dei dati sono essenziali per la pre-elaborazione dei dati e la gestione della pipeline. Un framework di intelligenza artificiale ideale per sistemi embedded dovrebbe integrarsi perfettamente con questi strumenti, consentendo un’efficiente acquisizione dei dati, trasformazione e addestramento del modello.\n\n\nIntegrazione con Strumenti di Ottimizzazione del Modello\nL’ottimizzazione del modello garantisce che i modelli di intelligenza artificiale siano adatti per la distribuzione embedded. Valutare se il framework si integra con strumenti di ottimizzazione del modello come TensorFlow Lite Converter o ONNX Runtime per facilitare la quantizzazione del modello e la riduzione delle dimensioni.\n\n\nFacilità d’Uso\nLa facilità d’uso di un framework di IA ha un impatto significativo sull’efficienza dello sviluppo. Un framework con un’interfaccia intuitiva e una documentazione chiara riduce la curva di apprendimento degli sviluppatori. Si dovrebbe considerare se il framework supporta API di alto livello, consentendo agli sviluppatori di concentrarsi sulla progettazione del modello piuttosto che sui dettagli di implementazione di basso livello. Questo fattore è incredibilmente importante per i sistemi embedded, che hanno meno funzionalità di quelle a cui gli sviluppatori tipici potrebbero essere abituati.\n\n\nSupporto della Community\nIl supporto della community gioca un altro fattore essenziale. I framework con community attive e coinvolte spesso hanno basi di codice ben mantenute, ricevono aggiornamenti regolari e forniscono forum preziosi per la risoluzione dei problemi. Di conseguenza, anche il supporto della community gioca un ruolo nella facilità d’uso perché garantisce che gli sviluppatori abbiano accesso a una vasta gamma di risorse, tra cui tutorial e progetti di esempio. Il supporto della community fornisce una certa garanzia che il framework continuerà a essere supportato per futuri aggiornamenti. Ci sono solo pochi framework che soddisfano le esigenze di TinyML. TensorFlow Lite Micro è il più popolare e ha il maggior supporto della comunità.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#tendenze-future-nei-framework-ml",
    "href": "contents/frameworks/frameworks.it.html#tendenze-future-nei-framework-ml",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.10 Tendenze Future nei Framework ML",
    "text": "6.10 Tendenze Future nei Framework ML\n\n6.10.1 Decomposizione\nAttualmente, lo stack del sistema ML è costituito da quattro astrazioni come mostrato in Figura 6.11, vale a dire (1) grafi computazionali, (2) programmi tensoriali, (3) librerie e runtime e (4) primitive hardware.\n\n\n\n\n\n\nFigura 6.11: Quattro astrazioni negli attuali stack dei sistemi ML. Fonte: TVM.\n\n\n\nCiò ha portato a confini verticali (ad esempio, tra i livelli di astrazione) e orizzontali (ad esempio, approcci basati sulla libreria rispetto a quelli basati sulla compilazione per il calcolo dei tensori), che ostacolano l’innovazione per il ML. Il lavoro futuro nei framework ML può guardare alla rottura di questi confini. A dicembre 2021 è stato proposto Apache TVM Unity, che mirava a facilitare le interazioni tra i diversi livelli di astrazione (nonché le persone dietro di essi, come scienziati ML, ingegneri ML e ingegneri hardware) e a co-ottimizzare le decisioni in tutti e quattro i livelli di astrazione.\n\n\n6.10.2 Compilatori e Librerie ad Alte Prestazioni\nCon l’ulteriore sviluppo dei framework ML, continueranno a emergere compilatori e librerie ad alte prestazioni. Alcuni esempi attuali includono TensorFlow XLA e CUTLASS di Nvidia, che accelerano le operazioni di algebra lineare nei grafi computazionali, e TensorRT di Nvidia, che accelera e ottimizza l’inferenza.\n\n\n6.10.3 ML per Framework ML\nPossiamo anche usare il ML per migliorare i framework di ML in futuro. Alcuni usi correnti di ML per framework ML includono:\n\nOttimizzazione degli iperparametri tramite tecniche quali ottimizzazione bayesiana, ricerca casuale e ricerca a griglia\nNeural Architecture Search (NAS) per cercare automaticamente architetture di rete ottimali\nAutoML, che come descritto in Sezione 6.5, automatizza la pipeline ML.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#conclusione",
    "href": "contents/frameworks/frameworks.it.html#conclusione",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.11 Conclusione",
    "text": "6.11 Conclusione\nIn sintesi, la selezione del framework di machine learning ottimale richiede una valutazione approfondita di varie opzioni in base a criteri quali usabilità, supporto della community, prestazioni, compatibilità hardware e capacità di conversione del modello. Non esiste una soluzione adatta a tutti, poiché il framework giusto dipende da vincoli e casi d’uso specifici.\nAbbiamo prima introdotto la necessità di framework di apprendimento automatico come TensorFlow e PyTorch. Questi framework offrono funzionalità quali tensori per la gestione di dati multidimensionali, grafi computazionali per la definizione e l’ottimizzazione delle operazioni del modello e una suite di strumenti tra cui funzioni di perdita, ottimizzatori e caricatori di dati che semplificano lo sviluppo del modello.\nLe funzionalità avanzate migliorano ulteriormente l’usabilità di questi framework, consentendo attività come la messa a punto di grandi modelli pre-addestrati e la facilitazione del “federated learning”. Queste funzionalità sono fondamentali per sviluppare modelli di apprendimento automatico sofisticati in modo efficiente.\nI framework di intelligenza artificiale embedded, come TensorFlow Lite Micro, forniscono strumenti specializzati per la distribuzione di modelli su piattaforme con risorse limitate. TensorFlow Lite Micro, ad esempio, offre strumenti di ottimizzazione completi, tra cui la mappatura della quantizzazione e le ottimizzazioni del kernel, per garantire prestazioni elevate su piattaforme basate su microcontrollori come i processori Arm Cortex-M e RISC-V. I framework creati appositamente per hardware specializzato come CMSIS-NN su processori Cortex-M possono massimizzare ulteriormente le prestazioni ma sacrificare la portabilità. I framework integrati dei fornitori di processori adattano lo stack alle loro architetture, liberando il pieno potenziale dei loro chip ma ci si blocca nel loro ecosistema.\nIn definitiva, la scelta del framework giusto implica la ricerca della migliore corrispondenza tra le sue capacità e i requisiti della piattaforma target. Ciò richiede un bilanciamento tra esigenze di prestazioni, vincoli hardware, complessità del modello e altri fattori. Una valutazione approfondita dei modelli e dei casi d’uso previsti e la valutazione delle opzioni rispetto alle metriche chiave guideranno gli sviluppatori nella selezione del framework ideale per le loro applicazioni di machine learning.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#sec-ai-frameworks-resource",
    "href": "contents/frameworks/frameworks.it.html#sec-ai-frameworks-resource",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.12 Risorse",
    "text": "6.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nFrameworks overview.\nEmbedded systems software.\nInference engines: TF vs. TFLite.\nTF flavors: TF vs. TFLite vs. TFLite Micro.\nTFLite Micro:\n\nTFLite Micro Big Picture.\nTFLite Micro Interpreter.\nTFLite Micro Model Format.\nTFLite Micro Memory Allocation.\nTFLite Micro NN Operations.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 6.1\nEsercizio 6.2\nEsercizio 6.3\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html",
    "href": "contents/training/training.it.html",
    "title": "7  Addestramento dell’IA",
    "section": "",
    "text": "7.1 Introduzione\nIl training è fondamentale per sviluppare sistemi di intelligenza artificiale accurati e utili tramite il machine learning. Il training crea un modello di apprendimento automatico che può essere generalizzato a dati nuovi e inediti anziché memorizzare gli esempi dell’addestramento. Ciò avviene inserendo dati di training in algoritmi che apprendono pattern da questi esempi regolando i parametri interni.\nGli algoritmi riducono al minimo una “funzione loss” [perdita], che confronta le loro previsioni sui dati di training con le etichette o le soluzioni note, guidando l’apprendimento. Un training efficace richiede spesso set di dati rappresentativi di alta qualità sufficientemente grandi da catturare la variabilità nei casi d’uso del mondo reale.\nRichiede inoltre la scelta di un algoritmo adatto all’attività, che si tratti di una rete neurale per la visione artificiale, un algoritmo di apprendimento di rinforzo per il controllo robotico o un metodo basato su alberi per la previsione categoriale. È necessaria un’attenta messa a punto per la struttura del modello, come la profondità e la larghezza della rete neurale e i parametri di apprendimento come la dimensione del passo e la forza della regolarizzazione.\nSono importanti anche le tecniche per prevenire l’overfitting, come le penalità di regolarizzazione nonché la convalida con dati trattenuti. L’overfitting può verificarsi quando un modello si adatta troppo ai dati di training, non riuscendo a generalizzare con i nuovi dati. Ciò può accadere se il modello è troppo complesso o è stato addestrato troppo a lungo.\nPer evitare l’overfitting, le tecniche di regolarizzazione possono aiutare a vincolare il modello. Un metodo di regolarizzazione consiste nell’aggiungere un termine di penalità alla funzione di perdita che scoraggia la complessità, come la norma L2 dei pesi. Questo penalizza i valori dei parametri elevati. Un’altra tecnica è il dropout, in cui una percentuale di neuroni viene impostata casualmente a zero durante l’addestramento. Ciò riduce il co-adattamento dei neuroni.\nI metodi di validazione aiutano anche a rilevare ed evitare l’overfitting. Una parte dei dati di training viene tenuta fuori dal ciclo di training come un set di validazione. Il modello viene valutato su questi dati. Se l’errore di convalida aumenta mentre l’errore di training diminuisce, si verifica un overfitting. Il training può quindi essere interrotto in anticipo o regolarizzato in modo più forte. La regolarizzazione e la convalida consentono ai modelli di addestrarsi alla massima capacità senza overfitting [sovra-adattare] i dati di training.\nIl training richiede notevoli risorse di elaborazione, in particolare per le reti neurali profonde (deep) utilizzate nella visione artificiale, nell’elaborazione del linguaggio naturale e in altre aree. Queste reti hanno milioni di pesi regolabili che devono essere regolati tramite un training esteso. I miglioramenti hardware e le tecniche di training distribuite hanno consentito di addestrare reti neurali sempre più grandi che possono raggiungere prestazioni di livello umano in alcune attività.\nIn sintesi, alcuni punti chiave sul training:\nGuideremo attraverso questi dettagli nelle restanti sezioni. Comprendere come sfruttare in modo efficace dati, algoritmi, ottimizzazione dei parametri e generalizzazione attraverso il training è essenziale per sviluppare sistemi di intelligenza artificiale capaci e distribuibili che funzionino in modo robusto nel mondo reale.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#introduzione",
    "href": "contents/training/training.it.html#introduzione",
    "title": "7  Addestramento dell’IA",
    "section": "",
    "text": "I Dati sono cruciali: I modelli di machine learning apprendono dagli esempi nei dati di training. Dati più rappresentativi e di qualità elevata portano a migliori prestazioni del modello. I dati devono essere elaborati e formattati per il training.\nGli algoritmi imparano dai dati: Diversi algoritmi (reti neurali, alberi decisionali, ecc.) hanno approcci diversi per trovare dei pattern nei dati. È importante scegliere l’algoritmo giusto per l’attività.\nL’addestramento affina i parametri del modello: L’addestramento del modello regola i parametri interni per trovare pattern nei dati. I modelli avanzati come le reti neurali hanno molti pesi regolabili. L’addestramento regola iterativamente i pesi per ridurre al minimo una funzione di perdita.\nLa generalizzazione è l’obiettivo: Un modello che sovra-adatta i dati di addestramento non generalizzerà bene. Le tecniche di regolarizzazione (dropout, early stopping arresto anticipato, ecc.) riducono il sovra-adattamento. I dati di validazione vengono utilizzati per valutare la generalizzazione.\nL’addestramento richiede risorse di elaborazione: L’addestramento di modelli complessi richiede una notevole potenza di elaborazione e tempo. I miglioramenti hardware e il training distribuito su GPU/TPU hanno consentito dei progressi.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#matematica-delle-reti-neurali",
    "href": "contents/training/training.it.html#matematica-delle-reti-neurali",
    "title": "7  Addestramento dell’IA",
    "section": "7.2 Matematica delle Reti Neurali",
    "text": "7.2 Matematica delle Reti Neurali\nIl deep learning ha rivoluzionato l’apprendimento automatico e l’intelligenza artificiale, consentendo ai computer di apprendere modelli complessi e prendere decisioni intelligenti. La rete neurale è al centro della rivoluzione del deep learning e, come discusso nella sezione 3, “Avvio al Deep Learning”, è un pilastro in alcuni di questi progressi.\nLe reti neurali sono costituite da semplici funzioni stratificate l’una sull’altra. Ogni layer acquisisce alcuni dati, esegue alcuni calcoli e li passa al layer successivo. Questi layer apprendono progressivamente funzionalità di alto livello utili per le attività che la rete è addestrata a svolgere. Ad esempio, in una rete addestrata per il riconoscimento delle immagini, il layer di input può acquisire valori di pixel, mentre i layer successivi possono rilevare forme semplici come i bordi. I layer successivi possono rilevare forme più complesse come nasi, occhi, ecc. Il layer di output finale classifica l’immagine nel suo complesso.\nLa rete, in una rete neurale, si riferisce al modo in cui questi layer sono connessi. L’output di ogni layer è considerato un set di neuroni, che sono collegati ai neuroni nei layer successivi, formando una “rete”. Il modo in cui questi neuroni interagiscono è determinato dai pesi tra di loro, che modellano le forze sinaptiche simili a quelle di un neurone del cervello. La rete neurale viene addestrata regolando questi pesi. Concretamente, i pesi vengono inizialmente impostati in modo casuale, quindi viene immesso l’input, l’output viene confrontato con il risultato desiderato e, infine, i pesi vengono modificati per migliorare la rete. Questo processo viene ripetuto finché la rete non riduce al minimo in modo affidabile la perdita (loss), indicando di aver appreso i pattern nei dati.\nCome viene definito matematicamente questo processo? Formalmente, le reti neurali sono modelli matematici costituiti da operazioni lineari e non lineari alternate, parametrizzate da un set di pesi apprendibili che vengono addestrati per minimizzare una qualche funzione di perdita (loss). Questa funzione di perdita misura quanto è buono il nostro modello per quanto riguarda l’adattamento dei nostri dati di addestramento e produce un valore numerico quando viene valutato sul nostro modello rispetto ai dati di addestramento. L’addestramento delle reti neurali comporta la valutazione ripetuta della funzione di perdita su molti dati diversi per misurare quanto è buono il nostro modello, quindi la modifica continua dei pesi del nostro modello utilizzando la backpropagation in modo che la perdita diminuisca, ottimizzando infine il modello per adattarlo ai nostri dati.\n\n7.2.1 Notazione delle Reti Neurali\nEntrando nei dettagli, il nucleo di una rete neurale può essere visto come una sequenza di operazioni alternate lineari e non lineari, come mostrato in Figura 7.1:\n\n\n\n\n\n\nFigura 7.1: Diagramma della rete neurale. Fonte: astroML.\n\n\n\nLa rete neurale funziona prendendo un vettore di input \\(x_i\\) e passandolo attraverso una serie di layer, ognuno dei quali esegue operazioni lineari e non lineari. L’output della rete a ogni layer \\(A_j\\) può essere rappresentato come:\n\\[\nA_j = f\\left(\\sum_{i=1}^{N} w_{ij} x_i\\right)\n\\]\nDove:\n\n\\(N\\) - Il numero totale di feature di input.\n\\(x_{i}\\) - La singola feature di input, dove \\(i\\) varia da \\(1\\) a \\(N\\).\n\\(w_{ij}\\) - I pesi che collegano il neurone \\(i\\) in uno layer al neurone \\(j\\) nel layer successivo, che vengono aggiustati durante l’addestramento.\n\\(f(\\theta)\\) - La funzione di attivazione non lineare applicata a ogni layer (ad esempio, ReLU, softmax, ecc.).\n\\(A_{j}\\) - L’output della rete neurale a ogni layer \\(j\\), dove \\(j\\) indica il numero del layer.\n\nNel contesto di Figura 7.1, \\(x_1, x_2, x_3, x_4,\\) e \\(x_5\\) rappresentano le caratteristiche di input. Ogni neurone di input \\(x_i\\) corrisponde a una feature dei dati di input. Le frecce dal layer di input al layer nascosto indicano le connessioni tra i neuroni di input e i neuroni nascosti, con ogni connessione associata a un peso \\(w_{ij}\\).\nIl layer nascosto è costituito dai neuroni \\(a_1, a_2, a_3,\\) e \\(a_4\\), ognuno dei quali riceve input da tutti i neuroni nello layer di input. I pesi \\(w_{ij}\\) collegano i neuroni di input ai neuroni nascosti. Ad esempio, \\(w_{11}\\) è il peso che collega l’input \\(x_1\\) al neurone nascosto \\(a_1\\).\nIl numero di nodi in ogni layer e il numero totale di layer insieme definiscono l’architettura della rete neurale. Nel primo layer (layer di input), il numero di nodi corrisponde alla dimensionalità dei dati di input, mentre nell’ultimo layer (layer di output), il numero di nodi corrisponde alla dimensionalità dell’output. Il numero di nodi nei layer intermedi può essere impostato arbitrariamente, consentendo flessibilità nella progettazione dell’architettura di rete.\nI pesi, che determinano il modo in cui ogni layer della rete neurale interagisce con gli altri, sono matrici di numeri reali. Inoltre, ogni layer in genere include un vettore di bias [polarizzazione], ma qui lo ignoriamo per semplicità. La matrice dei pesi \\(W_j\\) che collega il layer \\(j-1\\) al layer \\(j\\) ha le dimensioni:\n\\[\nW_j \\in \\mathbb{R}^{d_j \\times d_{j-1}}\n\\]\ndove \\(d_j\\) è il numero di nodi nel layer \\(j\\) e \\(d_{j-1}\\) è il numero di nodi nel layer precedente \\(j-1\\).\nL’output finale \\(y_k\\) della rete si ottiene applicando un’altra funzione di attivazione \\(g(\\theta)\\) alla somma ponderata degli output del layer nascosto:\n\\[\ny = g\\left(\\sum_{j=1}^{M} w_{jk} A_j\\right)\n\\]\nDove:\n\n\\(M\\) - Il numero di neuroni nascosti nel layer finale prima dell’output.\n\\(w_{jk}\\) - Il peso tra il neurone nascosto \\(a_j\\) e il neurone di output \\(y_k\\).\n\\(g(\\theta)\\) - La funzione di attivazione applicata alla somma ponderata degli output del layer nascosto.\n\nLa nostra rete neurale, come definita, esegue una sequenza di operazioni lineari e non lineari sui dati di input (\\(x_{i}\\)) per ottenere previsioni (\\(y_{i}\\)), che si spera siano una buona risposta a ciò che vogliamo che la rete neurale faccia sull’input (ad esempio, classificare se l’immagine di input è un gatto o meno). La nostra rete neurale può quindi essere rappresentata succintamente come una funzione \\(N\\) che accetta un input \\(x \\in \\mathbb{R}^{d_0}\\) parametrizzato da \\(W_1, ..., W_n\\) e produce l’output finale \\(y\\):\n\\[\ny = N(x; W_1, ..., W_n) \\quad \\text{where } A_0 = x\n\\]\nQuesta equazione indica che la rete inizia con l’input \\(A_0 = x\\) e calcola iterativamente \\(A_j\\) a ogni layer utilizzando i parametri \\(W_j\\) fino a produrre l’output finale \\(y\\) al layer di output.\nSuccessivamente vedremo come valutare questa rete neurale rispetto ai dati di addestramento introducendo una funzione di perdita.\n\n\n\n\n\n\nNota\n\n\n\nPerché sono necessarie le operazioni non lineari? Se avessimo solo layer lineari, l’intera rete sarebbe equivalente a un singolo layer lineare costituito dal prodotto degli operatori lineari. Quindi, le funzioni non lineari svolgono un ruolo chiave nella potenza delle reti neurali poiché migliorano la capacità della rete neurale di adattare le funzioni.\n\n\n\n\n\n\n\n\nNota\n\n\n\nAnche le convoluzioni sono operatori lineari e possono essere convertite in una moltiplicazione di matrici.\n\n\n\n\n7.2.2 Funzione Loss come Misura della Bontà di Adattamento Rispetto ai Dati di Addestramento\nDopo aver definito la nostra rete neurale, ci vengono forniti alcuni dati di addestramento, ovvero un set di punti \\({(x_j, y_j)}\\) per \\(j=1 \\rightarrow M\\), dove \\(M\\) è il numero totale di campioni nel set di dati e \\(j\\) indicizza ogni campione. Vogliamo valutare quanto è buona la nostra rete neurale nell’adattare questi dati. Per fare ciò, introduciamo una funzione di perdita, ovvero una funzione che prende l’output della rete neurale su un particolare punto dati \\(\\hat{y_j} = N(x_j; W_1, ..., W_n)\\) e lo confronta con la “etichetta” di quel particolare dato (il corrispondente \\(y_j\\)) e restituisce un singolo scalare numerico (ovvero un numero reale) che rappresenta quanto è “bene” la rete neurale adatta quel particolare dato; la misura finale di quanto è buona la rete neurale sull’intero set di dati è quindi solo la media delle perdite su tutti i dati.\nEsistono molti tipi diversi di funzioni di perdita; ad esempio, nel caso della classificazione delle immagini, potremmo usare la funzione di “cross-entropy loss” [perdita di entropia incrociata], che ci dice quanto bene si confrontano due vettori che rappresentano le previsioni di classificazione (ad esempio, se la nostra previsione prevede che un’immagine sia più probabilmente un cane, ma l’etichetta dice che è un gatto, restituirà una “perdita” elevata, che indica un cattivo adattamento).\nMatematicamente, una funzione di perdita è una funzione che prende due vettori con valori reali, uno che rappresenta gli output previsti della rete neurale e l’altro che rappresenta le etichette vere, e restituisce un singolo scalare numerico che rappresenta l’errore o la “perdita”.\n\\[\nL: \\mathbb{R}^{d_{n}} \\times \\mathbb{R}^{d_{n}} \\longrightarrow \\mathbb{R}\n\\]\nPer un singolo esempio di training, la perdita è data da:\n\\[\nL(N(x_j; W_1, ..., W_n), y_j)\n\\]\ndove \\(\\hat{y}_j = N(x_j; W_1, ..., W_n)\\) è l’output previsto della rete neurale per l’input \\(x_j\\), and \\(y_j\\) è la vera etichetta.\nLa perdita totale nell’intero set di dati, \\(L_{full}\\), viene quindi calcolata come la perdita media in tutti i dati di training:\n\nFunzione di Perdita per l’Ottimizzazione del Modello di Rete Neurale su Dataset \\[\nL_{full} = \\frac{1}{M} \\sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)\n\\]\n\n\n\n7.2.3 Addestramento di Reti Neurali con Discesa del Gradiente\nOra che possiamo misurare quanto bene la nostra rete si adatta ai dati di training, possiamo ottimizzare i pesi della rete neurale per ridurre al minimo questa perdita. In questo contesto, stiamo denotando \\(W_i\\) come pesi per ogni layer \\(i\\) nella rete. Ad alto livello, modifichiamo i parametri delle matrici a valori reali \\(W_i\\) per ridurre al minimo la funzione di perdita \\(L_{full}\\). Nel complesso, il nostro obiettivo matematico è\n\nObiettivo dell’Addestramento della Rete Neurale \\[\nmin_{W_1, ..., W_n} L_{full}\n\\] \\[\n= min_{W_1, ..., W_n} \\frac{1}{M} \\sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)\n\\]\n\nQuindi, come ottimizziamo questo obiettivo? Ricordiamo dal calcolo che la minimizzazione di una funzione può essere eseguita prendendo la derivata della funzione relativa ai parametri di input e modificando i parametri nella direzione del gradiente. Questa tecnica è chiamata a “discesa del gradiente” e concretamente comporta il calcolo della derivata della funzione di perdita \\(L_{full}\\) relativa a \\(W_1, ..., W_n\\) per ottenere un gradiente per questi parametri per fare un passo avanti, poi aggiornare questi parametri nella direzione del gradiente. Quindi, possiamo addestrare la nostra rete neurale utilizzando la discesa del gradiente, che applica ripetutamente la regola di aggiornamento.\n\nRegola di Aggiornamento della Discesa del Gradiente \\[\nW_i := W_i - \\lambda \\frac{\\partial L_{full}}{\\partial W_i} \\mbox{ for } i=1..n\n\\]\n\n\n\n\n\n\n\nNota\n\n\n\nIn pratica, il gradiente viene calcolato su un mini-batch di punti dati per migliorare l’efficienza computazionale. Questo processo è chiamato “discesa del gradiente stocastico” o “discesa del gradiente batch”.\n\n\nDove \\(\\lambda\\) è la dimensione del passo o il tasso di apprendimento delle nostre modifiche, nell’addestramento della nostra rete neurale, eseguiamo ripetutamente il passaggio precedente fino alla convergenza, o quando la perdita non diminuisce più. Figura 7.2 illustra questo processo: vogliamo raggiungere il punto minimo, il che si ottiene seguendo il gradiente (come illustrato con le frecce blu nella figura). Questo precedente approccio è noto come discesa del gradiente completa poiché stiamo calcolando la derivata relativa a tutti i dati di addestramento e solo dopo eseguiamo un singolo passaggio del gradiente; un approccio più efficiente è quello di calcolare il gradiente relativo solo a un batch casuale di dati e poi eseguire un passaggio, un processo noto come discesa del gradiente batch o discesa del gradiente stocastica (Robbins e Monro 1951), che è più efficiente poiché ora eseguiamo molti più passi per passaggio di tutti i dati di addestramento. Successivamente, tratteremo la matematica alla base del calcolo del gradiente della funzione di perdita relativa a \\(W_i\\), un processo noto come backpropagation.\n\nRobbins, Herbert, e Sutton Monro. 1951. «A Stochastic Approximation Method». The Annals of Mathematical Statistics 22 (3): 400–407. https://doi.org/10.1214/aoms/1177729586.\n\n\n\n\n\n\nFigura 7.2: Discesa del gradiente. Fonte: Towards Data Science.\n\n\n\n\n\n7.2.4 Backpropagation\nL’addestramento delle reti neurali comporta ripetute applicazioni dell’algoritmo di discesa del gradiente, che prevede il calcolo della derivata della funzione di perdita rispetto alle \\(W_i\\). Come calcoliamo la derivata della perdita relativa alle \\(W_i\\), dato che le \\(W_i\\) sono funzioni annidate l’una dell’altra in una rete neurale profonda? Il trucco è sfruttare la regola della catena: possiamo calcolare la derivata della perdita relativa alle \\(W_i\\) applicando ripetutamente la regola della catena in un processo completo noto come backpropagation. In particolare, possiamo calcolare i gradienti calcolando la derivata della perdita relativa agli output dell’ultimo layer, poi usarla progressivamente per calcolare la derivata della perdita relativa a ciascun layer precedente a quello di input. Questo processo inizia dalla fine della rete (il layer più vicino all’output) e procede all’indietro, e quindi prende il nome di backpropagation.\nAnalizziamolo. Possiamo calcolare la derivata della perdita relativa agli output di ciascun layer della rete neurale utilizzando applicazioni ripetute della regola della catena.\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{n}} = \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}\n\\]\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{n-1}} = \\frac{\\partial A_{n-1}}{\\partial L_{n-1}} \\frac{\\partial L_{n}}{\\partial A_{n-1}} \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}  \n\\]\no più in generale\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{i}} = \\frac{\\partial A_{i}}{\\partial L_{i}} \\frac{\\partial L_{i+1}}{\\partial A_{i}} ... \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}  \n\\]\n\n\n\n\n\n\nNota\n\n\n\nIn quale ordine dovremmo eseguire questo calcolo? Da una prospettiva computazionale, è preferibile eseguire i calcoli dalla fine alla parte frontale. (ad esempio: prima si calcola \\(\\frac{\\partial L_{full}}{\\partial A_{n}}\\), poi i termini precedenti, anziché iniziare dal centro) poiché ciò evita di materializzare e calcolare grandi jacobiani. Questo perché \\(\\ \\frac {\\partial L_{full}}{\\partial A_{n}}\\) è un vettore; quindi, qualsiasi operazione di matrice che include questo termine ha un output che è compresso per essere un vettore. Quindi, eseguire il calcolo dalla fine evita grandi moltiplicazioni matrice-matrice assicurando che i prodotti intermedi siano vettori.\n\n\n\n\n\n\n\n\nNota\n\n\n\nNella nostra notazione, assumiamo che le attivazioni intermedie \\(A_{i}\\) siano vettori colonna, anziché vettori riga, quindi la regola della catena è \\(\\frac{\\partial L}{\\partial L_{i}} = \\frac{\\partial L_{i+1}}{\\partial L_{i}} ... \\frac{\\partial L}{\\partial L_{n}}\\) piuttosto che \\(\\frac{\\partial L}{\\partial L_{i}} = \\frac{\\partial L}{\\partial L_{n}} ... \\frac{\\partial L_{i+1}}{\\partial L_{i}}\\)\n\n\nDopo aver calcolato la derivata della perdita relativa all’output di ogni layer, possiamo facilmente ottenere la derivata della perdita relativa ai parametri, utilizzando di nuovo la regola della catena:\n\\[\n\\frac{\\partial L_{full}}{W_{i}} = \\frac{\\partial L_{i}}{\\partial W_{i}} \\frac{\\partial L_{full}}{\\partial L_{i}}\n\\]\nEd è in definitiva così che le derivate dei pesi dei layer vengono calcolate usando la backpropagation! Come appare concretamente in un esempio specifico? Di seguito, esaminiamo un esempio specifico di una semplice rete neurale a 2 layer su un’attività di regressione usando una funzione di perdita MSE con input a 100 dimensioni e uno layer nascosto a 30 dimensioni:\n\nEsempio di backpropagation\nSupponiamo di avere una rete neurale a due layer \\[\nL_1 = W_1 A_{0}\n\\] \\[\nA_1 = ReLU(L_1)\n\\] \\[\nL_2 = W_2 A_{1}\n\\] \\[\nA_2 = ReLU(L_2)\n\\] \\[\nNN(x) = \\mbox{Let } A_{0} = x \\mbox{ then output } A_2\n\\] dove \\(W_1 \\in \\mathbb{R}^{30 \\times 100}\\) e \\(W_2 \\in \\mathbb{R}^{1 \\times 30}\\). Inoltre, supponiamo di utilizzare la funzione di perdita MSE: \\[\nL(x, y) = (x-y)^2\n\\] Vogliamo calcolare \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_i} \\mbox{ for } i=1,2\n\\] Notare quanto segue: \\[\n\\frac{\\partial L(x, y)}{\\partial x} = 2 \\times (x-y)\n\\] \\[\n\\frac{\\partial ReLU(x)}{\\partial x} \\delta  = \\left\\{\\begin{array}{lr}\n0 & \\text{for } x \\leq 0 \\\\\n1 & \\text{for } x \\geq 0 \\\\\n\\end{array}\\right\\} \\odot \\delta\n\\] \\[\n\\frac{\\partial WA}{\\partial A} \\delta = W^T \\delta\n\\] \\[\n\\frac{\\partial WA}{\\partial W} \\delta = \\delta A^T\n\\] Quindi abbiamo \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_2} = \\frac{\\partial L_2}{\\partial W_2} \\frac{\\partial A_2}{\\partial L_2} \\frac{\\partial L(NN(x), y)}{\\partial A_2}\n\\] \\[\n= (2L(NN(x) - y) \\odot ReLU'(L_2)) A_1^T\n\\] and \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial A_1}{\\partial L_1} \\frac{\\partial L_2}{\\partial A_1} \\frac{\\partial A_2}{\\partial L_2} \\frac{\\partial L(NN(x), y)}{\\partial A_2}\n\\] \\[\n= [ReLU'(L_1) \\odot (W_2^T [2L(NN(x) - y) \\odot ReLU'(L_2)])] A_0^T\n\\]\n\n\n\n\n\n\n\nConsiglio\n\n\n\nRicontrollare il lavoro assicurandosi che le forme siano corrette!\n\nTutti i prodotti di Hadamard (\\(\\odot\\)) dovrebbero operare su tensori della stessa forma\nTutte le moltiplicazioni di matrici dovrebbero operare su matrici che condividono una dimensione comune (ad esempio, m per n, n per k)\nTutti i gradienti relativi ai pesi dovrebbero avere la stessa forma delle stesse matrici dei pesi\n\n\n\nL’intero processo di backpropagation può essere complesso, specialmente per reti molto profonde. Fortunatamente, framework di machine learning come PyTorch supportano la differenziazione automatica, che esegue la backpropagation. In questi framework, dobbiamo semplicemente specificare il passaggio in avanti e le derivate ci verranno calcolate automaticamente. Tuttavia, è utile comprendere il processo teorico che avviene internamente in questi framework di apprendimento automatico.\n\n\n\n\n\n\nNota\n\n\n\nCome visto sopra, le attivazioni intermedie \\(A_i\\) vengono riutilizzate nella backpropagation. Per migliorare le prestazioni, queste attivazioni vengono memorizzate nella cache dal passaggio in avanti per evitare di essere ricalcolate. Tuttavia, le attivazioni devono essere mantenute in memoria tra i passaggi in avanti e indietro, il che comporta un maggiore utilizzo della memoria. Se la rete e le dimensioni del batch sono grandi, ciò potrebbe causare problemi di memoria. Analogamente, le derivate rispetto agli output di ogni layer vengono memorizzate nella cache per evitare il ricalcolo.\n\n\n\n\n\n\n\n\nEsercizio 7.1: Reti Neurali con Backpropagation e Discesa del Gradiente\n\n\n\n\n\nScoprire la matematica dietro le potenti reti neurali! Il deep learning potrebbe sembrare magico, ma è radicato nei principi matematici. In questo capitolo, abbiamo scomposto la notazione delle reti neurali, le funzioni di perdita e la potente tecnica della backpropagation. Ora, prepariamoci a implementare questa teoria con questi notebook Colab. Immergersi nel cuore di come le reti neurali apprendono. Si vedrà la matematica dietro la backpropagation e la discesa del gradiente, aggiornando quei pesi passo dopo passo.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#grafi-del-calcolo-differenziabili",
    "href": "contents/training/training.it.html#grafi-del-calcolo-differenziabili",
    "title": "7  Addestramento dell’IA",
    "section": "7.3 Grafi del Calcolo Differenziabili",
    "text": "7.3 Grafi del Calcolo Differenziabili\nIn generale, la discesa del gradiente stocastico mediante backpropagation può essere eseguita su qualsiasi grafo computazionale che un utente può definire, a condizione che le operazioni del calcolo siano differenziabili. Pertanto, le librerie generiche di deep learning come PyTorch e Tensorflow consentono agli utenti di specificare il loro processo computazionale (ad esempio, reti neurali) come grafo computazionale. La backpropagation viene eseguita automaticamente tramite differenziazione automatica quando la discesa del gradiente stocastico viene eseguita su questi grafi computazionali. Inquadrare l’addestramento dell’IA come un problema di ottimizzazione su grafi di calcolo differenziabili è un modo generale per comprendere cosa sta accadendo internamente con i sistemi di deep learning.\nLa struttura raffigurata in Figura 7.3 mostra un segmento di un grafo computazionale differenziabile. In questo grafo, l’input ‘x’ viene elaborato tramite una serie di operazioni: viene prima moltiplicato per una matrice di pesi ‘W’ (MatMul), poi aggiunto a un bias ‘b’ (Add) e infine passato a una funzione di attivazione, Rectified Linear Unit (ReLU). Questa sequenza di operazioni ci fornisce l’output C. La natura differenziabile del grafo significa che ogni operazione ha un gradiente ben definito. La differenziazione automatica, come implementata nei framework ML, sfrutta questa proprietà per calcolare in modo efficiente i gradienti della perdita rispetto a ciascun parametro nella rete (ad esempio, ‘W’ e ‘b’).\n\n\n\n\n\n\nFigura 7.3: Grafo Computazionale. Fonte: TensorFlow.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#dati-di-training",
    "href": "contents/training/training.it.html#dati-di-training",
    "title": "7  Addestramento dell’IA",
    "section": "7.4 Dati di Training",
    "text": "7.4 Dati di Training\nPer consentire un training efficace della rete neurale, i dati disponibili devono essere suddivisi in set di training, di validazione e di test. Il set di training viene utilizzato per addestrare i parametri del modello. Il set di validazione valuta il modello durante il training per ottimizzare gli iperparametri e prevenire l’overfitting. Il set di test fornisce una valutazione finale imparziale delle prestazioni del modello addestrato.\nMantenere chiare suddivisioni tra training, validation e test con dati rappresentativi è fondamentale per addestrare, ottimizzare e valutare correttamente i modelli per ottenere le migliori prestazioni nel mondo reale. A tal fine, scopriremo le insidie o gli errori comuni che le persone commettono quando creano queste suddivisioni dei dati.\nTabella 7.1 confronta le differenze tra le suddivisioni dei dati di training, validazione e test:\n\n\n\nTabella 7.1: Confronto tra suddivisioni di dati di training, validazione e test.\n\n\n\n\n\n\n\n\n\n\nSuddivisione\nScopo\nDimensioni tipiche\n\n\n\n\nSet di addestramento\nAddestrare i parametri del modello\n60-80% dei dati totali\n\n\nSet di validazione\nValutare il modello durante l’addestramento per ottimizzare gli iperparametri e prevenire l’overfitting\n∼20% dei dati totali\n\n\nSet di test\nFornire una valutazione imparziale del modello finale addestrato\n∼20% dei dati totali\n\n\n\n\n\n\n\n7.4.1 Suddivisioni di Dataset\n\nSet di Training\nIl set di training viene utilizzato per addestrare il modello. È il sottoinsieme più grande, in genere il 60-80% dei dati totali. Il modello vede e impara dai dati di addestramento per fare previsioni. È necessario un set di training sufficientemente grande e rappresentativo affinché il modello apprenda efficacemente i pattern sottostanti.\n\n\nSet di Validazione\nIl set di validazione valuta il modello durante l’addestramento, in genere dopo ogni epoca. Solitamente, il 20% dei dati viene assegnato a questo set. Il modello non impara né aggiorna i suoi parametri in base ai dati di validazione. Vengono usati per ottimizzare gli iperparametri e apportare altre modifiche per migliorare l’addestramento. Il monitoraggio di metriche come perdita e accuratezza sul set di validazione impedisce l’overfitting solo sui dati di addestramento.\n\n\nSet di Test\nIl set di test agisce come un dataset che il modello non ha visto durante l’addestramento. Viene utilizzato per fornire una valutazione imparziale del modello addestrato finale. In genere, il 20% dei dati è riservato ai test. Mantenere un set di test “hold-out” [esterno] è fondamentale per ottenere una stima accurata di come il modello addestrato si comporterebbe su dati non ancora visti del mondo reale. La mancanza di dati dal set di test deve essere evitata a tutti i costi.\nLe proporzioni relative dei set di training, validazione e test possono variare in base alle dimensioni dei dati e all’applicazione. Tuttavia, seguire le linee guida generali per una suddivisione 60/20/20 è un buon punto di partenza. Un’attenta suddivisione dei dati garantisce che i modelli siano adeguatamente addestrati, ottimizzati e valutati per ottenere le prestazioni migliori.\nVideo 7.1 spiega come suddividere correttamente il dataset in set di training, validazione e test, assicurando un processo di training ottimale.\n\n\n\n\n\n\nVideo 7.1: Train/Dev/Test Sets\n\n\n\n\n\n\n\n\n\n7.4.2 Errori e Insidie Comuni\n\nDati di Training Insufficienti\nAssegnare troppo pochi dati al set di training è un errore comune quando si suddividono i dati, il che può avere un impatto significativo sulle prestazioni del modello. Se il set di training è troppo piccolo, il modello non avrà campioni sufficienti per apprendere in modo efficace i veri pattern nei dati. Ciò comporta un’elevata varianza e impedisce al modello di generalizzare bene ai nuovi dati.\nAd esempio, se si addestra un modello di classificazione delle immagini per riconoscere cifre scritte a mano, fornire solo 10 o 20 immagini per classe di cifre sarebbe del tutto inadeguato. Il modello avrebbe bisogno di più esempi per catturare le ampie varianze negli stili di scrittura, rotazioni, larghezze dei tratti e altre varianti.\nCome regola generale, la dimensione del training set dovrebbe essere di almeno centinaia o migliaia di esempi affinché la maggior parte degli algoritmi di apprendimento automatico funzioni in modo efficace. A causa dell’elevato numero di parametri, il set di training spesso deve essere di decine o centinaia di migliaia per le reti neurali profonde, in particolare quelle che utilizzano layer convoluzionali.\nDati di training insufficienti si manifestano in genere in sintomi quali alti tassi di errore su set di validazione/test, bassa accuratezza del modello, alta varianza e overfitting su campioni di set di training di piccole dimensioni. La soluzione è raccogliere più dati di training di qualità. Le tecniche di data augmentation possono anche aiutare ad aumentare virtualmente le dimensioni dei dati di training per immagini, audio, ecc.\nÈ importante considerare attentamente la complessità del modello e la difficoltà del problema quando si assegnano i campioni di training per garantire che siano disponibili dati sufficienti affinché il modello possa apprendere correttamente. Si consiglia inoltre di seguire le linee guida sulle dimensioni minime dei set di training per diversi algoritmi. Sono necessari più dati di training per mantenere il successo complessivo di qualsiasi applicazione di machine learning.\nSi consideri Figura 7.4 dove proviamo a classificare/suddividere i dati in due categorie (qui, per colore): a sinistra, l’overfitting è rappresentato da un modello che ha appreso troppo bene le sfumature nei dati di training (o il set di dati era troppo piccolo o abbiamo eseguito il modello per troppo tempo), facendo sì che segua il rumore insieme al segnale, come indicato dalle eccessive curve della linea. Il lato destro mostra l’underfitting, dove la semplicità del modello gli impedisce di catturare la struttura sottostante del dataset, con conseguente linea che non si adatta bene ai dati. Il grafico centrale rappresenta un adattamento ideale, dove il modello bilancia bene tra generalizzazione e adattamento, catturando la tendenza principale dei dati senza essere influenzato da valori anomali. Sebbene il modello non sia un adattamento perfetto (manca di alcuni punti), ci interessa di più la sua capacità di riconoscere modelli generali piuttosto che valori anomali idiosincratici.\n\n\n\n\n\n\nFigura 7.4: Adattamento dei dati: overfitting, right fit e underfitting. Fonte: MathWorks.\n\n\n\nFigura 7.5 il processo di adattamento dei dati nel tempo. Durante l’addestramento, cerchiamo il “punto ottimale” tra underfitting e overfitting. Inizialmente, quando il modello non ha avuto abbastanza tempo per apprendere i pattern nei dati, ci troviamo nella zona di underfitting, indicata da alti tassi di errore sul set di convalida (da ricordare che il modello è addestrato sul set di addestramento e testiamo la sua generalizzabilità sul set di convalida o sui dati che non ha mai visto prima). A un certo punto, raggiungiamo un minimo globale per i tassi di errore e idealmente vogliamo interrompere l’addestramento lì. Se continuiamo l’addestramento, il modello inizierà a “memorizzare” o a conoscere i dati troppo bene, tanto che il tasso di errore inizierà a risalire, poiché il modello non riuscirà a generalizzare a dati che non ha mai visto prima.\n\n\n\n\n\n\nFigura 7.5: Adattamento dei dati nel tempo. Fonte: IBM.\n\n\n\nIl Video 7.2 fornisce una panoramica di bias e varianza e la relazione tra i due concetti e l’accuratezza del modello.\n\n\n\n\n\n\nVideo 7.2: Bias/Varianza\n\n\n\n\n\n\n\n\nPerdita di Dati Tra Set\nIl “data leakage” [perdita di dati] si riferisce al trasferimento involontario di informazioni tra i set di training, convalida e test. Ciò viola il presupposto fondamentale che le divisioni siano reciprocamente esclusive. La perdita di dati porta a risultati di valutazione seriamente compromessi e metriche di prestazioni gonfiate.\nUn modo comune in cui si verifica la perdita di dati è se alcuni campioni del set di test vengono inavvertitamente inclusi nei dati di training. Quando si valuta il set di test, il modello ha già visto alcuni dati, il che fornisce punteggi eccessivamente ottimistici. Ad esempio, se il 2% dei dati di test trapelano nel set di training di un classificatore binario, può comportare un aumento della precisione fino al 20%!\nSe le divisioni dei dati non vengono eseguite con attenzione, possono verificarsi forme di perdita più sottili. Se le divisioni non vengono randomizzate e mescolate correttamente, i campioni che sono vicini tra loro nel set di dati potrebbero finire nella stessa divisione, portando a distorsioni della distribuzione. Ciò crea una fuga di informazioni basata sulla prossimità nel set di dati.\nUn altro caso è quando i set di dati hanno campioni collegati, intrinsecamente connessi, come grafici, reti o dati di serie temporali. La suddivisione “ingenua” può isolare nodi o intervalli di tempo connessi in set diversi. I modelli possono fare ipotesi non valide basate su informazioni parziali.\nPer prevenire la perdita di dati è necessario creare una solida separazione tra le suddivisioni: nessun campione dovrebbe esistere in più di una suddivisione. Il mescolamento e la suddivisione randomizzata aiutano a creare divisioni robuste. Le tecniche di “cross-validation” [validazione incrociata] possono essere utilizzate per una valutazione più rigorosa. Rilevare la perdita è difficile, ma i segnali rivelatori includono modelli che funzionano molto meglio sui dati di test rispetto a quelli di validazione.\nLa perdita di dati compromette gravemente la validità della valutazione perché il modello ha già visto parzialmente i dati di test. Nessuna quantità di messa a punto o architetture complesse può sostituire le suddivisioni nette dei dati. È meglio essere prudenti e creare una separazione completa tra le suddivisioni per evitare questo errore fondamentale nelle pipeline di machine learning.\n\n\nSet di Validazione Piccolo o Non Rappresentativo\nIl set di validazione viene utilizzato per valutare le prestazioni del modello durante l’addestramento e per ottimizzare gli iperparametri. Per valutazioni affidabili e stabili, il set di validazione deve essere sufficientemente ampio e rappresentativo della distribuzione dei dati reali. Tuttavia, ciò può rendere più impegnativa la selezione e l’ottimizzazione del modello.\nAd esempio, se il set di validazione contiene solo 100 campioni, le metriche calcolate avranno un’elevata varianza. A causa del rumore, l’accuratezza può variare fino al 5-10% tra le epoche. Questo rende difficile sapere se un calo nell’accuratezza della validazione è dovuto a un overfitting o a una varianza naturale. Con un set di validazione più ampio, diciamo 1000 campioni, le metriche saranno molto più stabili.\nInoltre, se il set di validazione non è rappresentativo, forse mancano alcune sottoclassi, la capacità stimata del modello potrebbe essere gonfiata. Ciò potrebbe portare a scelte di iperparametri scadenti o a interruzioni premature dell’addestramento. I modelli selezionati in base a tali set di validazione distorti non si generalizzano bene ai dati reali.\nUna buona regola pratica è che la dimensione del set di convalida dovrebbe essere di almeno diverse centinaia di campioni e fino al 10-20% del set di addestramento, lasciando comunque campioni sufficienti per l’addestramento. Le divisioni dovrebbero anche essere stratificate, il che significa che le proporzioni di classe nel set di validazione dovrebbero corrispondere a quelle nel set di dati completo, soprattutto se si lavora con set di dati sbilanciati. Un set di validazione più ampio che rappresenti le caratteristiche dei dati originali è essenziale per una corretta selezione e messa a punto del modello.\n\n\nRiutilizzo del Set di Test Più Volte\nIl set di test è progettato per fornire una valutazione imparziale del modello completamente addestrato solo una volta alla fine del processo di sviluppo del modello. Riutilizzare il set di test più volte durante lo sviluppo per la valutazione del modello, la messa a punto degli iperparametri, la selezione del modello, ecc., può causare un overfitting sui dati di test. Invece, si deve riservare il set di test per una valutazione finale del modello completamente addestrato, trattandolo come una scatola nera per simularne le prestazioni su dati reali. Questo approccio fornisce metriche affidabili per determinare se il modello è pronto per la distribuzione in produzione.\nSe il set di test viene riutilizzato come parte del processo di validazione, il modello potrebbe iniziare a vedere e imparare dai campioni di test. Questo, insieme all’ottimizzazione intenzionale o meno delle prestazioni del modello sul set di test, può gonfiare artificialmente metriche come l’accuratezza.\nAd esempio, supponiamo che il set di test venga utilizzato ripetutamente per la selezione del modello su 5 architetture. In tal caso, il modello potrebbe raggiungere il 99% di accuratezza del test memorizzando i campioni anziché apprendere modelli generalizzabili. Tuttavia, quando implementati nel mondo reale, l’accuratezza dei nuovi dati potrebbe scendere del 60%.\nLa prassi migliore è interagire con il set di test solo una volta alla fine per segnalare metriche imparziali su come il modello finale ottimizzato si comporterebbe nel mondo reale. Durante lo sviluppo del modello, il set di convalida dovrebbe essere utilizzato per tutte le attività di ottimizzazione dei parametri, selezione del modello, arresto anticipato e simili. È importante riservare una parte, come il 20-30% dell’intero set di dati, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l’ottimizzazione o la selezione del modello durante lo sviluppo.\nNon mantenere un set “hold-out” non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrità finale sull’efficacia nel mondo reale. Mantenere la completa separazione di addestramento/validazione dal set di test è essenziale per ottenere stime accurate delle prestazioni del modello. Anche piccole deviazioni da un singolo utilizzo del set di test potrebbero falsare positivamente i risultati e le metriche, fornendo una visione eccessivamente ottimistica dell’efficacia nel mondo reale.\n\n\nStesse Suddivisioni dei Dati negli Esperimenti\nQuando si confrontano diversi modelli di machine learning o si sperimentano varie architetture e iperparametri, utilizzare le stesse suddivisioni dei dati per l’addestramento, la validazione e il test nei diversi esperimenti può introdurre distorsioni e invalidare le comparazioni.\nSe le stesse suddivisioni vengono riutilizzate, i risultati della valutazione potrebbero essere più bilanciati e misurare accuratamente quale modello funziona meglio. Ad esempio, una certa suddivisione casuale dei dati potrebbe favorire il modello A rispetto al modello B indipendentemente dagli algoritmi. Riutilizzare questa suddivisione causerà quindi distorsioni a favore del modello A.\nInvece, le suddivisioni dei dati dovrebbero essere randomizzate o mescolate per ogni iterazione sperimentale. Ciò garantisce che la casualità nel campionamento delle suddivisioni non conferisca un vantaggio ingiusto a nessun modello.\nCon diverse suddivisioni per esperimento, la valutazione diventa più solida. Ogni modello viene testato su un’ampia gamma di set di test estratti casualmente dalla popolazione complessiva, attenuando la variazione e rimuovendo la correlazione tra i risultati.\nLa prassi corretta è quella di impostare un “seed” casuale prima di suddividere i dati per ogni esperimento. La suddivisione dovrebbe avvenire dopo il rimescolamento/ricampionamento come parte della pipeline sperimentale. Eseguire confronti sulle stesse suddivisioni viola l’ipotesi i.i.d (indipendenti e identicamente distribuite) richiesta per la validità statistica.\nLe suddivisioni univoche sono essenziali per confronti di modelli equi. Sebbene richieda un’elaborazione più intensiva, l’allocazione randomizzata per esperimento rimuove la distorsione del campionamento e consente un benchmarking valido. Ciò evidenzia le vere differenze nelle prestazioni del modello indipendentemente dalle caratteristiche di una particolare suddivisione.\n\n\nMancata Stratificazione delle Suddivisioni\nQuando si suddividono i dati in set di training, validazione e test, la mancata stratificazione delle suddivisioni può comportare una rappresentazione non uniforme delle classi target tra le suddivisioni e introdurre un bias di campionamento. Ciò è particolarmente problematico per i set di dati sbilanciati.\nLa suddivisione stratificata implica il campionamento dei dati in modo che la proporzione di classi di output sia approssimativamente preservata in ogni suddivisione. Ad esempio, se si esegue una suddivisione training-test 70/30 su un set di dati con campioni negativi al 60% e positivi al 40%, la stratificazione garantisce esempi negativi al ~60% e positivi al ~40% sia nei set di training che nei set di test.\nSenza stratificazione, la casualità potrebbe comportare che la suddivisione di training abbia campioni positivi al 70% mentre il test ha campioni positivi al 30%. Il modello addestrato su questa distribuzione di training distorta non si generalizzerà bene. Lo squilibrio delle classi compromette anche le metriche del modello come l’accuratezza.\nLa stratificazione funziona meglio quando viene eseguita utilizzando etichette, sebbene proxy come il clustering possano essere utilizzati per l’apprendimento non supervisionato. Diventa essenziale per set di dati altamente distorti con classi rare che potrebbero essere facilmente omesse dalle suddivisioni.\nLibrerie come Scikit-Learn hanno metodi di suddivisione stratificati nativi. Non utilizzarli potrebbe inavvertitamente introdurre bias di campionamento e danneggiare le prestazioni del modello sui gruppi minoritari. Dopo aver eseguito le suddivisioni, il bilanciamento complessivo delle classi dovrebbe essere esaminato per garantire una rappresentazione uniforme tra le suddivisioni.\nLa stratificazione fornisce un set di dati bilanciato sia per l’addestramento del modello che per la valutazione. Sebbene la semplice suddivisione casuale sia facile, tenendo conto delle esigenze di stratificazione, specialmente per dati sbilanciati nel mondo reale, si traduce in uno sviluppo e una valutazione del modello più solidi.\n\n\nIgnorare le Dipendenze delle Serie Temporali\nI dati delle serie temporali hanno una struttura temporale intrinseca con osservazioni dipendenti dal contesto passato. Suddividere ingenuamente i dati delle serie temporali in set di training e test senza tenere conto di questa dipendenza porta a perdite di dati e bias di lookahead.\nAd esempio, suddividere semplicemente una serie temporale nel primo 70% di training e nell’ultimo 30% come dati di test contaminerà i dati di training con dati futuri. Il modello può usare queste informazioni per “sbirciare” in avanti durante il training.\nCiò si traduce in una valutazione eccessivamente ottimistica delle prestazioni del modello. Il modello può sembrare che preveda il futuro in modo accurato, ma in realtà ha appreso implicitamente in base ai dati futuri, il che non si traduce in prestazioni nel mondo reale.\nDovrebbero essere utilizzate tecniche di validazione incrociata delle serie temporali appropriate, come il concatenamento in avanti, per preservare l’ordine e la dipendenza. Il set di test dovrebbe contenere solo dati da una finestra temporale futura a cui il modello non è stato esposto per il training.\nNon tenere conto delle relazioni temporali porta a ipotesi di causalità non valide. Se i dati di training contengono dati futuri, il modello potrebbe anche dover imparare come estrapolare ulteriormente le previsioni.\nMantenere il flusso temporale degli eventi ed evitare il bias di lookahead è fondamentale per addestrare e testare correttamente i modelli di serie temporali. Ciò garantisce che possano davvero prevedere modelli futuri e non solo memorizzare i dati di training passati.\n\n\nNessun Dato Non Visto per la Valutazione Finale\nUn errore comune quando si suddividono i dati è non metterne da parte una porzione solo per la valutazione finale del modello completato. Tutti i dati vengono utilizzati per training, validazione e set di test durante lo sviluppo.\nQuesto non lascia dati non visti per ottenere una stima imparziale di come il modello finale ottimizzato si comporterebbe nel mondo reale. Le metriche sul set di test utilizzate durante lo sviluppo potrebbero riflettere solo parzialmente le reali capacità del modello.\nAd esempio, scelte come l’arresto anticipato e l’ottimizzazione degli iperparametri sono spesso ottimizzate in base alle prestazioni del set di test. Questo accoppia il modello ai dati di test. È necessario un set di dati non visto per interrompere questo accoppiamento e ottenere metriche reali del mondo reale.\nLa “best practice” è quella di riservare una parte, come il 20-30% del set di dati completo, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l’ottimizzazione o la selezione del modello durante lo sviluppo.\nIl salvataggio di alcuni dati non visti consente di valutare il modello completamente addestrato come una scatola nera su dati del mondo reale. Questo fornisce metriche affidabili per decidere se il modello è pronto per la distribuzione in produzione.\nNon mantenere un set “hold-out” non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrità finale sull’efficacia nel mondo reale.\n\n\nSovra-ottimizzazione del Set di Validazione\nIl set di validazione è pensato per guidare il processo di training del modello, non per fungere da dati di training aggiuntivi. L’eccessiva ottimizzazione del set di validazione per massimizzare le metriche delle prestazioni lo tratta più come un set di training secondario, portando a metriche gonfiate e scarsa generalizzazione.\nAd esempio, tecniche come l’ottimizzazione estensiva degli iperparametri o l’aggiunta di incrementi di dati mirati a migliorare l’accuratezza della convalida possono far sì che il modello si adatti troppo ai dati di validazione. Il modello può raggiungere un’accuratezza di validazione del 99% ma solo un’accuratezza di test del 55%.\nAnalogamente, riutilizzare il set di validazione per un arresto anticipato può anche ottimizzare il modello specificamente per quei dati. L’arresto alle migliori prestazioni di validazione sovra-adatta il rumore e le fluttuazioni causate dalle piccole dimensioni di validazione.\nIl set di validazione funge da proxy per ottimizzare e selezionare i modelli. Tuttavia, l’obiettivo rimane massimizzare le prestazioni dei dati del mondo reale, non il set di validazione. Ridurre al minimo la perdita o l’errore sui dati di validazione non si traduce automaticamente in una buona generalizzazione.\nUn buon approccio è quello di mantenere l’uso del set di validazione al minimo: gli iperparametri possono essere regolati grossolanamente prima sui dati di training, ad esempio. Il set di validazione guida il training ma non dovrebbe influenzare o alterare il modello stesso. È uno strumento diagnostico, non di ottimizzazione.\nQuando si valutano le prestazioni sul set di validazione, bisogna fare attenzione a non sovra-adattare. Sono necessari dei compromessi per costruire modelli che funzionino bene sulla popolazione complessiva e non siano eccessivamente regolati sui campioni di validazione.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#algoritmi-di-ottimizzazione",
    "href": "contents/training/training.it.html#algoritmi-di-ottimizzazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.5 Algoritmi di Ottimizzazione",
    "text": "7.5 Algoritmi di Ottimizzazione\nStochastic gradient descent (SGD) è un algoritmo di ottimizzazione semplice ma potente per l’addestramento di modelli di machine learning. Funziona stimando il gradiente della funzione di perdita relativa ai parametri del modello utilizzando un singolo esempio di addestramento e poi aggiornando i parametri nella direzione che riduce la perdita.\nSebbene concettualmente semplice, SGD necessita di alcune aree di miglioramento. Innanzitutto, scegliere un tasso di apprendimento appropriato può essere difficile: troppo piccolo e i progressi sono molto lenti; troppo grande e i parametri possono oscillare e non convergere. In secondo luogo, SGD tratta tutti i parametri in modo uguale e indipendente, il che potrebbe non essere l’ideale in tutti i casi. Infine, SGD vanilla [standard] utilizza solo informazioni sul gradiente di primo ordine, il che si traduce in progressi lenti su problemi mal condizionati.\n\n7.5.1 Ottimizzazioni\nNel corso degli anni, sono state proposte varie ottimizzazioni per accelerare e migliorare l’SGD vanilla. Ruder (2016) fornisce un’eccellente panoramica dei diversi ottimizzatori. In breve, diverse tecniche di ottimizzazione SGD comunemente utilizzate includono:\n\nRuder, Sebastian. 2016. «An overview of gradient descent optimization algorithms». ArXiv preprint abs/1609.04747 (settembre). http://arxiv.org/abs/1609.04747v2.\nMomentum: Accumula un vettore di velocità in direzioni di gradiente persistente attraverso le iterazioni. Ciò aiuta ad accelerare i progressi smorzando le oscillazioni e mantiene i progressi in direzioni coerenti.\nNesterov Accelerated Gradient (NAG): Una variante di momentum che calcola i gradienti in “look ahead” anziché nella posizione del parametro corrente. Questo aggiornamento anticipatorio impedisce l’overshooting mentre il momentum mantiene il progresso accelerato.\nAdagrad: Un algoritmo di velocità di apprendimento adattivo che mantiene una velocità di apprendimento per parametro ridotta proporzionalmente alla somma storica dei gradienti di ciascun parametro. Aiuta a eliminare la necessità di regolare manualmente i tassi di apprendimento (Duchi, Hazan, e Singer 2010).\n\nDuchi, John C., Elad Hazan, e Yoram Singer. 2010. «Adaptive Subgradient Methods for Online Learning and Stochastic Optimization». In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, a cura di Adam Tauman Kalai e Mehryar Mohri, 257–69. Omnipress. http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.\n\nZeiler, Matthew D. 2012. «ADADELTA: An Adaptive Learning Rate Method», dicembre, 119–49. https://doi.org/10.1002/9781118266502.ch6.\nAdadelta: Una modifica ad Adagrad limita la finestra dei gradienti passati accumulati, riducendo così il decadimento aggressivo dei tassi di apprendimento (Zeiler 2012).\nRMSProp: Divide il tasso di apprendimento per una media esponenzialmente decrescente dei gradienti quadrati. Ciò ha un effetto di normalizzazione simile ad Adagrad ma non accumula i gradienti nel tempo, evitando un rapido decadimento dei tassi di apprendimento (Hinton 2017).\n\nHinton, Geoffrey. 2017. «Overview of Minibatch Gradient Descent». University of Toronto; University Lecture.\n\nKingma, Diederik P., e Jimmy Ba. 2014. «Adam: A Method for Stochastic Optimization». A cura di Yoshua Bengio e Yann LeCun, dicembre. http://arxiv.org/abs/1412.6980v9.\nAdam: Combinazione di momentum e rmsprop dove rmsprop modifica il tasso di apprendimento in base alla media delle recenti ampiezze dei gradienti. Mostra un progresso iniziale molto rapido e regola automaticamente le dimensioni dei passi (Kingma e Ba 2014).\nAMSGrad: Una variante di Adam che assicura una convergenza stabile mantenendo il massimo dei gradienti quadratici passati, impedendo al tasso di apprendimento di aumentare durante l’addestramento (Reddi, Kale, e Kumar 2019).\n\nReddi, Sashank J., Satyen Kale, e Sanjiv Kumar. 2019. «On the Convergence of Adam and Beyond». arXiv preprint arXiv:1904.09237, aprile. http://arxiv.org/abs/1904.09237v1.\nTra questi metodi, Adam è ampiamente considerato l’algoritmo di ottimizzazione di riferimento per molte attività di deep-learning. Supera costantemente SGD vanilla in termini di velocità di addestramento e prestazioni. Altri ottimizzatori potrebbero essere più adatti in alcuni casi, in particolare per modelli più semplici.\n\n\n7.5.2 Compromessi\nTabella 7.2 è una tabella di pro e contro per alcuni dei principali algoritmi di ottimizzazione per l’addestramento di reti neurali:\n\n\n\nTabella 7.2: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione.\n\n\n\n\n\n\n\n\n\n\nAlgoritmo\nPro\nContro\n\n\n\n\nMomentum\n\nConvergenza più rapida dovuta all’accelerazione lungo i gradienti\nMinore oscillazione rispetto a SGD vanilla\n\n\nRichiede la messa a punto del parametro momentum\n\n\n\nNesterov Accelerated Gradient (NAG)\n\nPiù veloce dello slancio standard in alcuni casi\nGli aggiornamenti anticipati impediscono il superamento\n\n\nPiù complesso da comprendere intuitivamente\n\n\n\nAdagrad\n\nElimina la necessità di regolare manualmente i tassi di apprendimento\nFunziona bene su gradienti radi\n\n\nIl tasso di apprendimento potrebbe decadere troppo rapidamente su gradienti densi\n\n\n\nAdadelta\n\nDecadimento del tasso di apprendimento meno aggressivo rispetto ad Adagrad\n\n\nAncora sensibile al valore iniziale del tasso di apprendimento\n\n\n\nRMSProp\n\nRegola automaticamente i tassi di apprendimento\nFunziona bene nella pratica\n\n\nNessun aspetto negativo importante\n\n\n\nAdam\n\nCombinazione di momentum e tassi di apprendimento adattivo\nConvergenza efficiente e veloce\n\n\nPrestazioni di generalizzazione leggermente peggiori in alcuni casi\n\n\n\nAMSGrad\n\nMiglioramento di Adam che affronta il problema della generalizzazione\n\n\nNon è stato utilizzato/testato così ampiamente come Adam\n\n\n\n\n\n\n\n\n\n7.5.3 Algoritmi di Benchmarking\nNon esiste un singolo metodo migliore per tutti i tipi di problemi. Ciò significa che abbiamo bisogno di un benchmarking completo per identificare l’ottimizzatore più efficace per set di dati e modelli specifici. Le prestazioni di algoritmi come Adam, RMSProp e Momentum variano a seconda delle dimensioni del batch, dei programmi di apprendimento, dell’architettura del modello, della distribuzione dei dati e della regolarizzazione. Queste variazioni sottolineano l’importanza di valutare ogni ottimizzatore in diverse condizioni.\nPrendiamo ad esempio Adam, che spesso eccelle nelle attività di visione artificiale, a differenza di RMSProp, che potrebbe mostrare una migliore generalizzazione in determinate attività di elaborazione del linguaggio naturale. La forza di Momentum risiede nella sua accelerazione in scenari con direzioni di gradiente coerenti, mentre i tassi di apprendimento adattivo di Adagrad sono più adatti per problemi di gradiente sparso.\nQuesta vasta gamma di interazioni tra ottimizzatori dimostra la difficoltà di dichiarare un singolo algoritmo universalmente superiore. Ogni ottimizzatore ha punti di forza unici, rendendo fondamentale valutare vari metodi per scoprire empiricamente le loro condizioni di applicazione ottimali.\nUn approccio di benchmarking completo dovrebbe valutare la velocità di convergenza e fattori come errore di generalizzazione, stabilità, sensibilità degli iperparametri ed efficienza computazionale, tra gli altri. Ciò comporta il monitoraggio delle curve di apprendimento di training e convalida su più esecuzioni e il confronto degli ottimizzatori su vari set di dati e modelli per comprenderne i punti di forza e di debolezza.\nAlgoPerf, introdotto da Dürr et al. (2021), risponde alla necessità di un sistema di benchmarking robusto. Questa piattaforma valuta le prestazioni dell’ottimizzatore utilizzando criteri quali curve di loss [perdita] di training, errore di generalizzazione, sensibilità agli iperparametri ed efficienza computazionale. AlgoPerf testa vari metodi di ottimizzazione, tra cui Adam, LAMB e Adafactor, su diversi tipi di modelli come CNN e RNN/LSTM su set di dati stabiliti. Utilizza la “containerizzazione” e la raccolta automatica di metriche per ridurre al minimo le incongruenze e consente esperimenti controllati su migliaia di configurazioni, fornendo una base affidabile per confrontare gli ottimizzatori.\n\nDürr, Marc, Gunnar Nissen, Kurt-Wolfram Sühs, Philipp Schwenkenbecher, Christian Geis, Marius Ringelstein, Hans-Peter Hartung, et al. 2021. «CSF Findings in Acute NMDAR and LGI1 Antibody–Associated Autoimmune Encephalitis». Neurology Neuroimmunology &amp; Neuroinflammation 8 (6). https://doi.org/10.1212/nxi.0000000000001086.\nLe informazioni ottenute da AlgoPerf e benchmark simili sono inestimabili per guidare la scelta ottimale o la messa a punto degli ottimizzatori. Abilitando valutazioni riproducibili, questi benchmark contribuiscono a una comprensione più approfondita delle prestazioni di ciascun ottimizzatore, aprendo la strada a innovazioni future e progressi accelerati nel settore.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#ottimizzazione-degli-iperparametri",
    "href": "contents/training/training.it.html#ottimizzazione-degli-iperparametri",
    "title": "7  Addestramento dell’IA",
    "section": "7.6 Ottimizzazione degli Iperparametri",
    "text": "7.6 Ottimizzazione degli Iperparametri\nGli iperparametri sono impostazioni importanti nei modelli di machine learning che incidono notevolmente sulle prestazioni finali dei modelli. A differenza di altri parametri del modello che vengono appresi durante l’addestramento, gli iperparametri vengono specificati dai “data scientist” o dagli ingegneri del machine learning prima dell’addestramento del modello.\nLa scelta dei valori degli iperparametri corretti consente ai modelli di apprendere pattern dai dati in modo efficace. Alcuni esempi di iperparametri chiave negli algoritmi di apprendimento automatico includono:\n\nReti neurali: Velocità di apprendimento, dimensione del batch, numero di unità nascoste, funzioni di attivazione\nMacchine a vettori di supporto: Forza di regolarizzazione, tipo di kernel e parametri\nRandom forest: Numero di alberi, profondità dell’albero\nK-means: Numero di cluster\n\nIl problema è che non ci sono regole pratiche affidabili per scegliere configurazioni ottimali degli iperparametri: in genere si devono provare valori diversi e valutare le prestazioni. Questo processo è chiamato “hyperparameter tuning” ottimizzazione degli iperparametri.\nNei primi anni del moderno deep learning, i ricercatori erano ancora alle prese con problemi di convergenza instabile e lenta. I punti dolenti comuni includevano perdite di training che fluttuavano selvaggiamente, gradienti che esplodevano o svanivano e un’ampia serie di tentativi ed errori necessari per addestrare le reti in modo affidabile. Di conseguenza, un punto focale iniziale era l’utilizzo di iperparametri per controllare l’ottimizzazione del modello. Ad esempio, tecniche seminali come la normalizzazione batch consentivano una convergenza più rapida del modello regolando gli aspetti dello spostamento interno delle covariate. I metodi di velocità di apprendimento adattivo hanno anche mitigato la necessità di estese pianificazioni manuali. Questi affrontavano problemi di ottimizzazione durante l’addestramento, come la divergenza incontrollata del gradiente. Le velocità di apprendimento adattate con attenzione sono anche il fattore di controllo primario per ottenere una convergenza rapida e stabile anche oggi.\nCon l’espansione esponenziale della capacità computazionale negli anni successivi, modelli molto più grandi potevano essere addestrati senza cadere preda di problemi di pura ottimizzazione numerica. L’attenzione si è spostata verso la generalizzazione, sebbene una convergenza efficiente fosse un prerequisito fondamentale. Tecniche all’avanguardia come “Transformers” hanno introdotto miliardi di parametri. A tali dimensioni, gli iperparametri relativi a capacità, regolarizzazione, ensembling [raggruppamento], ecc., hanno assunto un ruolo centrale per la messa a punto, anziché solo le metriche di convergenza grezze.\nLa lezione è che comprendere l’accelerazione e la stabilità del processo di ottimizzazione stesso costituisce il lavoro di base. Schemi di inizializzazione, dimensioni dei batch, decadimenti di peso e altri iperparametri di training rimangono indispensabili oggi. Dominare una convergenza rapida e impeccabile consente ai professionisti di espandere la propria attenzione sulle esigenze emergenti relative alla messa a punto di parametri quali accuratezza, robustezza ed efficienza su larga scala.\n\n7.6.1 Algoritmi di Ricerca\nQuando si tratta del processo critico di ottimizzazione degli iperparametri, ci sono diversi algoritmi sofisticati su cui gli specialisti del machine learning si affidano per cercare sistematicamente nel vasto spazio di possibili configurazioni dei modelli. Alcuni degli algoritmi di ricerca degli iperparametri più importanti includono:\n\nGrid Search: Il metodo di ricerca più elementare, in cui si definisce manualmente una griglia di valori da controllare per ogni iperparametro. Ad esempio, controllando velocità di apprendimento = [0.01, 0.1, 1] e dimensioni batch = [32, 64, 128]. Il vantaggio principale è la semplicità, ma può portare a un’esplosione esponenziale nello spazio di ricerca, rendendolo dispendioso in termini di tempo. È più adatto per l’ottimizzazione di un piccolo numero di parametri.\nRandom Search: Invece di definire una griglia, si selezionano casualmente valori per ogni iperparametro da un intervallo o set predefinito. Questo metodo è più efficiente nell’esplorazione di un vasto spazio di iperparametri perché non richiede una ricerca esaustiva. Tuttavia, potrebbe comunque non trovare parametri ottimali poiché non esplora sistematicamente tutte le possibili combinazioni.\nBayesian Optimization: Questo è un approccio probabilistico avanzato per l’esplorazione adattiva basato su una funzione surrogata per modellare le prestazioni su iterazioni. È semplice ed efficiente: trova iperparametri altamente ottimizzati in meno passaggi di valutazione. Tuttavia, richiede un maggiore investimento nella configurazione (Snoek, Larochelle, e Adams 2012).\nEvolutionary Algorithms: Questi algoritmi imitano i principi della selezione naturale. Generano popolazioni di combinazioni di iperparametri e le evolvono nel tempo in base alle prestazioni. Questi algoritmi offrono solide capacità di ricerca più adatte per superfici di risposta complesse. Tuttavia, sono necessarie molte iterazioni per una convergenza ragionevole.\nPopulation Based Training (PBT): Un metodo che ottimizza gli iperparametri addestrando più modelli in parallelo, consentendo loro di condividere e adattare configurazioni di successo durante l’addestramento, combinando elementi di ricerca casuale e algoritmi evolutivi (Jaderberg et al. 2017).\nNeural Architecture Search: Un approccio alla progettazione di architetture ad alte prestazioni per reti neurali. Tradizionalmente, gli approcci NAS utilizzano una qualche forma di apprendimento di rinforzo per proporre architetture di reti neurali, che vengono poi ripetutamente valutate (Zoph e Le 2016).\n\n\nSnoek, Jasper, Hugo Larochelle, e Ryan P. Adams. 2012. «Practical Bayesian Optimization of Machine Learning Algorithms». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 2960–68. https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html.\n\nJaderberg, Max, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, et al. 2017. «Population Based Training of Neural Networks». arXiv preprint arXiv:1711.09846, novembre. http://arxiv.org/abs/1711.09846v2.\n\nZoph, Barret, e Quoc V. Le. 2016. «Neural Architecture Search with Reinforcement Learning», novembre, 367–92. https://doi.org/10.1002/9781394217519.ch17.\n\n\n7.6.2 Implicazioni di Sistema\nLa messa a punto degli iperparametri può avere un impatto significativo sul tempo di convergenza durante l’addestramento del modello, influenzando direttamente il runtime complessivo. I valori corretti per gli iperparametri chiave di training sono cruciali per un’efficiente convergenza del modello. Ad esempio, la velocità di apprendimento dell’iperparametro controlla la dimensione del passo durante l’ottimizzazione della discesa del gradiente. Impostando correttamente uno scheduling della velocità di apprendimento assicura che l’algoritmo di ottimizzazione converga rapidamente verso un buon minimo. Una velocità di apprendimento troppo bassa porta a una convergenza dolorosamente lenta, mentre un valore troppo grande causa una fluttuazione selvaggia delle perdite. Una messa a punto corretta assicura un rapido movimento verso pesi e bias ottimali.\nAnalogamente, la dimensione del batch per la discesa del gradiente stocastica influisce sulla stabilità della convergenza. La giusta dimensione del batch attenua le fluttuazioni negli aggiornamenti dei parametri per avvicinarsi più rapidamente al minimo. Sono necessarie più dimensioni del batch per evitare una convergenza rumorosa, mentre le dimensioni maggiori del batch non riescono a generalizzare e rallentano la convergenza a causa di aggiornamenti dei parametri meno frequenti. La messa a punto degli iperparametri per una convergenza più rapida e una durata di addestramento ridotta ha implicazioni dirette sui costi e sui requisiti di risorse per il ridimensionamento dei sistemi di machine learning:\n\nCosti computazionali inferiori: Tempi di convergenza più brevi significano costi computazionali inferiori per i modelli di training. Il training ML sfrutta spesso grandi istanze di cloud computing come cluster GPU e TPU che comportano pesanti costi orari. Ridurre al minimo i tempi di training riduce direttamente questo costo di noleggio delle risorse, che tende a dominare i budget ML per le organizzazioni. Un’iterazione più rapida consente inoltre agli esperti di dati di sperimentare più liberamente all’interno dello stesso budget.\nTempo di training ridotto: Un tempo di training ridotto sblocca opportunità per addestrare più modelli utilizzando lo stesso budget computazionale. Gli iperparametri ottimizzati estendono ulteriormente le risorse disponibili, consentendo alle aziende di sviluppare e sperimentare più modelli con vincoli di risorse per massimizzare le prestazioni.\nEfficienza delle risorse: Un training più rapido consente di allocare istanze di calcolo più piccole nel cloud poiché i modelli richiedono l’accesso alle risorse per una durata più breve. Ad esempio, un job di training di un’ora consente di utilizzare istanze GPU meno potenti rispetto a un training di più ore, che richiede un accesso di elaborazione sostenuto su intervalli più lunghi. Ciò consente di risparmiare sui costi, soprattutto per carichi di lavoro di grandi dimensioni.\n\nCi sono anche altri vantaggi. Ad esempio, una convergenza più rapida riduce la pressione sui team di ingegneria ML in merito al provisioning delle risorse di training. Le semplici routine di riaddestramento del modello possono utilizzare risorse meno potenti anziché richiedere l’accesso a code ad alta priorità per cluster GPU di livello di produzione vincolati, liberando risorse di distribuzione per altre applicazioni.\n\n\n7.6.3 Gli Auto Tuner\nData la sua importanza, esiste un’ampia gamma di offerte commerciali per aiutare con l’ottimizzazione degli iperparametri. Toccheremo brevemente due esempi: uno incentrato sull’ottimizzazione per ML su scala cloud e l’altro per modelli di apprendimento automatico mirati ai microcontrollori. Tabella 7.3 delinea le principali differenze:\n\n\n\nTabella 7.3: Confronto di piattaforme di ottimizzazione per diversi casi d’uso di machine learning.\n\n\n\n\n\n\n\n\n\n\n\nPiattaforma\nCaso d’Uso Target\nTecniche di ottimizzazione\nVantaggi\n\n\n\n\nVertex AI di Google\nApprendimento automatico su scala cloud\nOttimizzazione bayesiana, addestramento Population-Based\nNasconde la complessità, consentendo modelli rapidi e pronti per l’implementazione con ottimizzazione iperparametrica all’avanguardia\n\n\nEON Tuner di Edge Impulse\nModelli di microcontrollori (TinyML)\nOttimizzazione bayesiana\nAdatta i modelli per dispositivi con risorse limitate, semplifica l’ottimizzazione per l’implementazione embedded\n\n\n\n\n\n\n\nBigML\nSono disponibili diverse piattaforme commerciali di auto-tuning per risolvere questo problema. Una soluzione è Vertex AI Cloud di Google, che offre un ampio supporto integrato per tecniche di ottimizzazione all’avanguardia.\nUna delle funzionalità più importanti della piattaforma di apprendimento automatico gestita da Vertex AI di Google è l’ottimizzazione efficiente e integrata degli iperparametri per lo sviluppo del modello. Per addestrare con successo modelli ML performanti è necessario identificare configurazioni ottimali per un set di iperparametri esterni che determinano il comportamento del modello, ponendo un problema di ricerca ad alta dimensione impegnativo. Vertex AI semplifica questo processo tramite strumenti di Automated Machine Learning (AutoML).\nIn particolare, gli scienziati dei dati possono sfruttare i motori di ottimizzazione degli iperparametri di Vertex AI fornendo un set di dati etichettato e scegliendo un tipo di modello come un classificatore di reti neurali o Random Forest. Vertex avvia un job di “Hyperparameter Search” in modo trasparente sul backend, gestendo completamente il provisioning delle risorse, l’addestramento del modello, il monitoraggio delle metriche e l’analisi dei risultati automaticamente utilizzando algoritmi di ottimizzazione avanzati.\nInternamente, Vertex AutoML impiega varie strategie di ricerca per esplorare in modo intelligente le configurazioni di iperparametri più promettenti in base ai risultati delle valutazioni precedenti. Tra queste, l’ottimizzazione bayesiana è offerta in quanto fornisce un’efficienza di campionamento superiore, richiedendo meno iterazioni di training per ottenere una qualità del modello ottimizzata rispetto ai metodi standard di Grid Search o di Random Search. Per spazi di ricerca di architettura neurale più complessi, Vertex AutoML utilizza il Population-Based Training, che addestra simultaneamente più modelli e regola dinamicamente i loro iperparametri sfruttando le prestazioni di altri modelli nella popolazione, analogamente ai principi di selezione naturale.\nVertex AI democratizza le tecniche di ricerca di iperparametri all’avanguardia su scala cloud per tutti gli sviluppatori ML, astraendo la complessità di esecuzione e di orchestrazione sottostante. Gli utenti si concentrano esclusivamente sul loro set di dati, sui requisiti del modello e sugli obiettivi di accuratezza, mentre Vertex gestisce il ciclo di ottimizzazione, l’allocazione delle risorse, il training del modello, il monitoraggio dell’accuratezza e l’archiviazione degli artefatti internamente. Il risultato è ottenere modelli ML ottimizzati e pronti per la distribuzione più velocemente per il problema target.\n\n\nTinyML\nEdge Impulse’s Efficient On-device Neural Network Tuner (EON Tuner) è uno strumento di ottimizzazione automatizzata degli iperparametri progettato per sviluppare modelli di apprendimento automatico per microcontrollori. Semplifica il processo di sviluppo del modello trovando automaticamente la migliore configurazione di rete neurale per un’implementazione efficiente e accurata su dispositivi con risorse limitate.\nLa funzionalità chiave di EON Tuner è la seguente. Innanzitutto, gli sviluppatori definiscono gli iperparametri del modello, come numero di layer, nodi per layer, funzioni di attivazione e pianificazione della velocità di “annealing” [https://it.wikipedia.org/wiki/Ricottura_simulata] dell’apprendimento. Questi parametri costituiscono lo spazio di ricerca che verrà ottimizzato. Successivamente, viene selezionata la piattaforma del microcontrollore target, fornendo vincoli hardware embedded. L’utente può anche specificare obiettivi di ottimizzazione, come la riduzione dell’ingombro di memoria, la riduzione della latenza, la riduzione del consumo energetico o la massimizzazione della precisione.\nCon lo spazio di ricerca definito e gli obiettivi di ottimizzazione, EON Tuner sfrutta l’ottimizzazione degli iperparametri bayesiani per esplorare in modo intelligente possibili configurazioni. Ogni configurazione potenziale viene automaticamente implementata come specifica di modello completa, addestrata e valutata per metriche di qualità. Il processo continuo bilancia esplorazione e sfruttamento per arrivare a impostazioni ottimizzate su misura per l’architettura del chip scelta dallo sviluppatore e i requisiti di prestazioni.\nEON Tuner libera gli esperti di machine learning dal processo iterativo esigente di messa a punto manuale dei modelli, regolando automaticamente i modelli per il deployment embedded. Lo strumento si integra perfettamente nel flusso di lavoro Edge Impulse, portando i modelli dal concetto a implementazioni ottimizzate in modo efficiente sui microcontrollori. L’esperienza racchiusa in EON Tuner per quanto riguarda l’ottimizzazione del modello ML per i microcontrollori garantisce che sia gli sviluppatori principianti che quelli esperti possano rapidamente iterare per ottenere modelli adatti alle esigenze del loro progetto.\n\n\n\n\n\n\nEsercizio 7.2: Ottimizzazione degli Iperparametri\n\n\n\n\n\nPrepariamoci a scoprire i segreti della messa a punto degli iperparametri e portiamo i modelli PyTorch al livello successivo! Gli iperparametri sono come i quadranti e le manopole nascosti che controllano i superpoteri di apprendimento del modello. In questo notebook Colab, si collaborerà con Ray Tune per trovare le combinazioni perfette di iperparametri. Scopriamo come definire quali valori cercare, impostare il codice di training per l’ottimizzazione e lasciare che Ray Tune faccia il grosso del lavoro. Alla fine, si diventerà professionisti della messa a punto degli iperparametri!\n\n\n\n\nVideo 7.3 spiega l’organizzazione sistematica del processo di ottimizzazione degli iperparametri.\n\n\n\n\n\n\nVideo 7.3: Iperparametro",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#regolarizzazione",
    "href": "contents/training/training.it.html#regolarizzazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.7 Regolarizzazione",
    "text": "7.7 Regolarizzazione\nLa regolarizzazione è una tecnica critica per migliorare le prestazioni e la generalizzabilità dei modelli di machine learning in impostazioni applicate. Si riferisce alla limitazione matematica o alla penalizzazione della complessità del modello per evitare il sovra-adattamento dei dati di training. Senza regolarizzazione, i modelli ML complessi sono inclini al sovra-adattamento del set di dati e alla memorizzazione di peculiarità e rumore nel set di training anziché all’apprendimento di modelli significativi. Possono raggiungere un’elevata accuratezza di training ma hanno prestazioni scadenti quando valutano nuovi input non ancora visti.\nLa regolarizzazione aiuta ad affrontare questo problema ponendo vincoli che favoriscono modelli più semplici e più generalizzabili che non si agganciano a errori di campionamento. Tecniche come la regolarizzazione L1/L2 penalizzano direttamente valori di parametri elevati durante il training, costringendo il modello a utilizzare i parametri più piccoli che possono spiegare adeguatamente il segnale. Le regole di arresto anticipato interrompono il training quando le prestazioni del set di validazione smettono di migliorare, prima che il modello inizi a sovra-adattarsi.\nUna regolarizzazione appropriata è fondamentale quando si distribuiscono modelli a nuove popolazioni di utenti e ambienti in cui sono probabili cambiamenti di distribuzione. Ad esempio, un modello irregolare di rilevamento delle frodi addestrato presso una banca potrebbe funzionare inizialmente, ma accumulare debiti tecnici nel tempo man mano che emergono nuovi pattern di frode.\nLa regolarizzazione di reti neurali complesse offre anche vantaggi computazionali: modelli più piccoli richiedono meno “data augmentation”, potenza di calcolo e archiviazione dei dati. La regolarizzazione consente anche sistemi di intelligenza artificiale più efficienti, in cui accuratezza, robustezza e gestione delle risorse sono attentamente bilanciate rispetto alle limitazioni del set di addestramento.\nDiverse potenti tecniche di regolarizzazione sono comunemente utilizzate per migliorare la generalizzazione del modello. L’architettura della strategia ottimale richiede la comprensione di come ogni metodo influisce sull’apprendimento e sulla complessità del modello.\n\n7.7.1 L1 e L2\nDue delle forme di regolarizzazione più ampiamente utilizzate sono la regolarizzazione L1 e la L2. Entrambe penalizzano la complessità del modello aggiungendo un termine extra alla funzione di costo ottimizzata durante l’addestramento. Questo termine cresce all’aumentare dei parametri del modello.\nLa regolarizzazione L2, nota anche come “ridge regression” [https://it.wikipedia.org/wiki/Regolarizzazione_di_Tichonov], aggiunge la somma delle grandezze al quadrato di tutti i parametri moltiplicata per un coefficiente α. Questa penalità quadratica riduce i valori dei parametri estremi in modo più aggressivo rispetto alle tecniche L1. L’implementazione richiede solo la modifica della funzione di costo e la messa a punto di α.\n\\[R_{L2}(\\Theta) = \\alpha \\sum_{i=1}^{n}\\theta_{i}^2\\]\nDove:\n\n\\(R_{L2}(\\Theta)\\) - Il termine di regolarizzazione L2 che viene aggiunto alla funzione di costo\n\\(\\alpha\\) - L’iperparametro di regolarizzazione L2 che controlla la forza della regolarizzazione\n\\(\\theta_{i}\\) - L’i-esimo parametro del modello\n\\(n\\) - Il numero di parametri nel modello\n\\(\\theta_{i}^2\\) - Il quadrato di ciascun parametro\n\nE la funzione di costo regolarizzata L2 completa è:\n\\[J(\\theta) = L(\\theta) + R_{L2}(\\Theta)\\]\nDove:\n\n\\(L(\\theta)\\) - La funzione di costo non regolarizzata originale\n\\(J(\\theta)\\) - La nuova funzione di costo regolarizzata\n\nSia la regolarizzazione L1 che L2 penalizzano i pesi elevati nella rete neurale. Tuttavia, la differenza fondamentale tra la regolarizzazione L1 e L2 è che la regolarizzazione L2 penalizza i quadrati dei parametri anziché i valori assoluti. Questa differenza fondamentale ha un impatto considerevole sui pesi regolarizzati risultanti. La regolarizzazione L1, o regressione LASSO [https://it.wikipedia.org/wiki/Regolarizzazione_(matematica)], utilizza la somma assoluta delle grandezze anziché il quadrato moltiplicato per α. La penalizzazione del valore assoluto dei pesi induce scarsità poiché il gradiente degli errori estrapola linearmente mentre i termini dei pesi tendono a zero; questo è diverso dalla penalizzazione del valore al quadrato dei pesi, dove la penalità si riduce man mano che i pesi tendono a 0. Inducendo scarsità nel vettore dei parametri, la regolarizzazione L1 esegue automaticamente la selezione delle feature, impostando i pesi delle feature irrilevanti a zero. A differenza della regolarizzazione L2, la L1 porta alla scarsità poiché i pesi sono impostati su 0; nella regolarizzazione L2, i pesi sono impostati su un valore molto vicino a 0 ma generalmente non raggiungono mai esattamente 0. La regolarizzazione L1 incoraggia la scarsità ed è stata utilizzata in alcuni lavori per addestrare reti sparse che potrebbero essere più efficienti in termini di hardware (Hoefler et al. 2021).\n\nHoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, e Alexandra Peste. 2021. «Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks», gennaio. http://arxiv.org/abs/2102.00554v1.\n\\[R_{L1}(\\Theta) = \\alpha \\sum_{i=1}^{n}||\\theta_{i}||\\]\nDove:\n\n\\(R_{L1}(\\Theta)\\) - Il termine di regolarizzazione L1 che viene aggiunto alla funzione di costo\n\\(\\alpha\\) - L’iperparametro di regolarizzazione L1 che controlla la forza della regolarizzazione\n\\(\\theta_{i}\\) - L’i-esimo parametro del modello\n\\(n\\) - Il numero di parametri nel modello\n\\(||\\theta_{i}||\\) - La norma L1, che assume il valore assoluto di ciascun parametro\n\nE la funzione di costo regolarizzata L1 completa è:\n\\[J(\\theta) = L(\\theta) + R_{L1}(\\Theta)\\]\nDove:\n\n\\(L(\\theta)\\) - La funzione di costo non regolarizzata originale\n\\(J(\\theta)\\) - La nuova funzione di costo regolarizzata\n\nLa scelta tra L1 e L2 dipende dalla complessità del modello prevista e dalla necessità o meno di una selezione di feature intrinseche. Entrambi richiedono una messa a punto iterativa su un set di validazione per selezionare l’iperparametro α ottimale.\nVideo 7.4 e Video 7.5 spiegano come funziona la regolarizzazione.\n\n\n\n\n\n\nVideo 7.4: Regolarizzazione\n\n\n\n\n\n\nVideo 7.5 spiega come la regolarizzazione può aiutare a ridurre l’overfitting del modello per migliorare le prestazioni.\n\n\n\n\n\n\nVideo 7.5: Perché la Regolarizzazione Riduce l’Overfitting\n\n\n\n\n\n\n\n\n7.7.2 Dropout\nUn altro metodo di regolarizzazione ampiamente adottato è “dropout” (Srivastava et al. 2014). Durante l’addestramento, dropout imposta casualmente una frazione \\(p\\) di output del nodo o attivazioni nascoste a zero. Questo incoraggia una maggiore distribuzione delle informazioni su più nodi anziché affidarsi a un piccolo numero di nodi. Al momento della previsione, viene utilizzata l’intera rete neurale, con attivazioni intermedie scalate di \\(1 - p\\) per mantenere le ampiezze di output. Le ottimizzazioni GPU semplificano l’implementazione efficiente di dropout tramite framework come PyTorch e TensorFlow.\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, e Ruslan Salakhutdinov. 2014. «Dropout: a simple way to prevent neural networks from overfitting.» J. Mach. Learn. Res. 15 (1): 1929–58. https://doi.org/10.5555/2627435.2670313.\nSiamo più precisi. Durante l’addestramento con dropout, l’output di ogni nodo \\(a_i\\) viene passato attraverso una maschera di dropout \\(r_i\\) prima di essere utilizzato dal layer successivo:\n\\[ ã_i = r_i \\odot a_i \\]\nDove:\n\n\\(a_i\\) - output del nodo \\(i\\)\n\\(ã_i\\) - output del nodo \\(i\\) dopo il dropout\n\\(r_i\\) - variabile casuale di Bernoulli indipendente con probabilità \\(1 - p\\) di essere 1\n\\(\\odot\\) - moltiplicazione elemento per elemento\n\nPer capire come funziona il dropout, è importante sapere che la maschera di dropout \\(r_i\\) è basata sulle variabili casuali di Bernoulli. Una variabile casuale di Bernoulli assume un valore di 1 con probabilità \\(1-p\\) (mantenendo l’attivazione) e un valore di 0 con probabilità \\(p\\) (dropping [perdendo] l’attivazione). Ciò significa che l’attivazione di ciascun nodo viene mantenuta o eliminata indipendentemente durante l’addestramento. Questa maschera di dropout \\(r_i\\) imposta casualmente una frazione \\(p\\) di attivazioni a 0 durante l’addestramento, costringendo la rete a creare rappresentazioni ridondanti.\nAl momento del test, la maschera di dropout viene rimossa e le attivazioni vengono ridimensionate di \\(1 - p\\) per mantenere le ampiezze di output previste:\n\\[ a_i^{test} = (1 - p)  a_i\\]\nDove:\n\n\\(a_i^{test}\\) - output del nodo al momento del test\n\\(p\\) - la probabilità di effettuare il dropping [eliminare] di un nodo.\n\nL’iperparametro chiave è \\(p\\), la probabilità di eliminare ogni nodo, spesso impostata tra 0.2 e 0.5. Le reti più grandi tendono a trarre vantaggio da un dropout maggiore, mentre le reti più piccole rischiano di non adattarsi se vengono eliminati troppi nodi. Tentativi ed errori combinati con il monitoraggio delle prestazioni di validazione aiutano a regolare il livello di dropout.\nVideo 7.6 discute l’intuizione alla base della tecnica di regolarizzazione del dropout e il suo funzionamento.\n\n\n\n\n\n\nVideo 7.6: Dropout\n\n\n\n\n\n\n\n\n7.7.3 Arresto Anticipato\nL’intuizione alla base di “early stopping” arresto anticipato implica il monitoraggio delle prestazioni del modello su un set di validazione “held-out” [esterno] in epoche di addestramento. Inizialmente, gli aumenti nell’idoneità del set di addestramento accompagnano i guadagni nell’accuratezza della validazione man mano che il modello rileva modelli generalizzabili. Dopo un certo punto, tuttavia, il modello inizia a sovradimensionarsi, agganciandosi a peculiarità e rumore nei dati di addestramento che non si applicano più in generale. Le prestazioni di validazione raggiungono il picco e poi si degradano se l’addestramento continua. Le regole di “arresto anticipato” interrompono l’addestramento a questo picco per evitare il sovradimensionamento. Questa tecnica dimostra come le pipeline ML debbano monitorare il feedback del sistema, non solo massimizzare incondizionatamente le prestazioni su un set di addestramento statico. Lo stato del sistema evolve e gli endpoint ottimali cambiano.\nPertanto, i metodi formali di arresto anticipato richiedono il monitoraggio di una metrica come l’accuratezza o la perdita di validazione dopo ogni epoca. Le curve comuni mostrano rapidi guadagni iniziali che si riducono gradualmente, alla fine raggiungendo un plateau e diminuiscono leggermente man mano che si verifica il sovradimensionamento. Il punto di arresto ottimale è spesso compreso tra 5 e 15 epoche oltre il picco, a seconda dei “patient threshold” [limiti della pazienza!]. Il monitoraggio di più metriche può migliorare il segnale poiché esiste una varianza tra le misure.\nLe semplici regole di arresto anticipato si interrompono immediatamente alla prima degradazione post-picco. Metodi più robusti introducono un parametro di “pazienza”, ovvero il numero di epoche di degradazione consentite prima dell’arresto. Ciò evita di interrompere prematuramente l’addestramento a causa di fluttuazioni transitorie. Le finestre di “pazienza” tipiche vanno da 50 a 200 batch di validazione. Finestre più ampie comportano il rischio di overfitting. Le strategie di ottimizzazione formali possono determinare la “pazienza” ottimale.\n\n\n\n\n\n\nEsercizio 7.3: Regolarizzazione\n\n\n\n\n\nCombattere l’Overfitting: Scoprire i Segreti della Regolarizzazione! L’overfitting è come se il modello memorizzasse le risposte a un test, per poi fallire l’esame reale. Le tecniche di regolarizzazione sono le guide di studio che aiutano il modello a generalizzare e ad affrontare nuovi problemi. In questo notebook Colab, impareremo come ottimizzare i parametri di regolarizzazione per risultati ottimali utilizzando la regolarizzazione L1 e L2, il dropout e l’arresto anticipato.\n\n\n\n\nVideo 7.7 tratta alcuni altri metodi di regolarizzazione che possono ridurre l’overfitting del modello.\n\n\n\n\n\n\nVideo 7.7: Altri Metodi di Regolarizzazione",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#funzioni-di-attivazione",
    "href": "contents/training/training.it.html#funzioni-di-attivazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.8 Funzioni di Attivazione",
    "text": "7.8 Funzioni di Attivazione\nLe funzioni di attivazione svolgono un ruolo cruciale nelle reti neurali. Introducono comportamenti non lineari che consentono alle reti neurali di modellare pattern complessi. Le funzioni di attivazione elemento per elemento vengono applicate alle somme ponderate che arrivano a ciascun neurone nella rete. Senza funzioni di attivazione, le reti neurali sarebbero ridotte a modelli di regressione lineare.\nIdealmente, le funzioni di attivazione possiedono alcune qualità desiderabili:\n\nNon lineari: Consentono di modellare relazioni complesse tramite trasformazioni non lineari della somma degli input.\nDifferenziabili: Devono avere derivate prime ben definite per abilitare la retropropagazione e l’ottimizzazione basata sul gradiente durante l’addestramento.\nLimitazione dell’Intervallo: Limitano il segnale di output, impedendo un’esplosione. Ad esempio, la sigmoide schiaccia gli input a (0,1).\n\nInoltre, proprietà come efficienza computazionale, monotonicità e fluidità rendono alcune attivazioni più adatte di altre in base all’architettura di rete e alla complessità del problema.\nEsamineremo brevemente alcune delle funzioni di attivazione più ampiamente adottate e i loro punti di forza e limiti. Forniremo anche linee guida per la selezione di funzioni appropriate abbinate ai vincoli del sistema ML e alle esigenze dei casi d’uso.\n\n7.8.1 Sigmoide\nL’attivazione sigmoide applica una curva a forma di S schiacciante che lega strettamente l’output tra 0 e 1. Ha la forma matematica:\n\\[ sigmoid(x) = \\frac{1}{1+e^{-x}} \\]\nLa trasformazione esponenziale consente alla funzione di passare gradualmente da quasi 0 a quasi 1 quando l’input passa da molto negativo a molto positivo. L’aumento monotono copre l’intero intervallo (0,1).\nLa funzione sigmoide presenta diversi vantaggi. Fornisce sempre un gradiente uniforme per la retropropagazione e il suo output è limitato tra 0 e 1, il che aiuta a prevenire valori “esplosivi” durante l’addestramento. Inoltre, ha una semplice formula matematica che è facile da calcolare.\nTuttavia, la funzione sigmoide presenta anche alcuni svantaggi. Tende a saturarsi a valori di input estremi, il che può causare la “scomparsa” dei gradienti, rallentando o addirittura interrompendo il processo di apprendimento. Inoltre, la funzione non è centrata sullo zero, il che significa che i suoi output non sono distribuiti simmetricamente attorno allo zero, il che può portare ad aggiornamenti inefficienti durante l’addestramento.\n\n\n7.8.2 Tanh\nAnche Tanh o “tangente iperbolica” assume una forma a S ma è centrata sullo zero, il che significa che il valore medio dell’output è 0.\n\\[ tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\]\nLa trasformazione numeratore/denominatore sposta l’intervallo da (0,1) in Sigmoide a (-1, 1) in tanh.\nLa maggior parte dei pro/contro sono condivisi con la Sigmoide, ma Tanh evita alcuni problemi di saturazione dell’output essendo centrata. Tuttavia, soffre ancora di gradienti che svaniscono con molti layer.\n\n\n7.8.3 ReLU\nLa Rectified Linear Unit (ReLU) introduce un semplice comportamento di soglia con la sua forma matematica:\n\\[ ReLU(x) = max(0, x) \\]\nLascia tutti gli input positivi invariati mentre taglia tutti i valori negativi a 0. Questa attivazione sparsa e il calcolo economico rendono ReLU ampiamente favorito rispetto a sigmoide/tanh.\nFigura 7.6 dimostra le 3 funzioni di attivazione di cui abbiamo discusso sopra in confronto a una funzione lineare:\n\n\n\n\n\n\nFigura 7.6: Funzioni di Attivazione Comuni. Fonte: AI Wiki.\n\n\n\n\n\n7.8.4 Softmax\nLa funzione di attivazione softmax è generalmente utilizzata come ultimo layer per le attività di classificazione per normalizzare il vettore del valore di attivazione in modo che i suoi elementi sommino a 1. Questo è utile per le attività di classificazione in cui vogliamo imparare a prevedere probabilità specifiche per classe di un input particolare, nel qual caso la probabilità cumulativa tra le classi è uguale a 1. La funzione di attivazione softmax è definita come\n\\[\\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K\\]\n\n\n7.8.5 Pro e Contro\nTabella 7.4 sono i pro e i contro riassuntivi di queste varie funzioni di attivazione standard:\n\n\n\nTabella 7.4: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione.\n\n\n\n\n\n\n\n\n\n\nAttivazione\nPro\nContro\n\n\n\n\nSigmoide\n\nGradiente uniforme per il backdrop [sfondo]\nOutput limitato tra 0 e 1\n\n\nLa saturazione elimina i gradienti\nNon centrato sullo zero\n\n\n\nTanh\n\nGradiente più uniforme della sigmoide\nOutput centrato sullo zero [-1, 1]\n\n\nSoffre ancora di problemi di gradiente evanescente\n\n\n\nReLU\n\nEfficiente dal punto di vista computazionale\nIntroduce la “sparsity” [scarsità]\nEvita gradienti evanescenti\n\n\nUnità “ReLU morenti”\nNon limitato\n\n\n\nSoftmax\n\nUtilizzato per l’ultimo livello per normalizzare gli output in modo che siano una distribuzione\nIn genere utilizzato per attività di classificazione\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsercizio 7.4: Funzioni di Attivazione\n\n\n\n\n\nSblocchiamo la potenza delle funzioni di attivazione! Questi piccoli “muletti” matematici sono ciò che rende le reti neurali così incredibilmente flessibili. In questo notebook Colab, ci si cimenterà con funzioni come Sigmoid, tanh e la superstar ReLU. Guardiamo come trasformano gli input e scopriamo quale funziona meglio in diverse situazioni. È la chiave per costruire reti neurali in grado di affrontare problemi complessi!",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#inizializzazione-dei-pesi",
    "href": "contents/training/training.it.html#inizializzazione-dei-pesi",
    "title": "7  Addestramento dell’IA",
    "section": "7.9 Inizializzazione dei Pesi",
    "text": "7.9 Inizializzazione dei Pesi\nLa corretta inizializzazione dei pesi in una rete neurale prima dell’addestramento è un passaggio fondamentale che ha un impatto diretto sulle prestazioni del modello. L’inizializzazione casuale dei pesi a valori molto grandi o molto piccoli può portare a problemi come gradienti che svaniscono/esplodono, convergenza lenta dell’addestramento o intrappolati in minimi locali scadenti. La corretta inizializzazione del peso accelera la convergenza del modello durante l’addestramento e comporta implicazioni per le prestazioni del sistema al momento dell’inferenza negli ambienti di produzione. Alcuni aspetti chiave sono:\n\nTempo di Accuratezza più Rapido: Un’inizializzazione attentamente calibrata porta a una convergenza più rapida, che si traduce nel raggiungimento da parte dei modelli di traguardi di accuratezza target in anticipo nel ciclo di training. Ad esempio, l’inizializzazione Xavier potrebbe ridurre il tempo di accuratezza del 20% rispetto a un’inizializzazione casuale errata. Poiché l’addestramento è in genere la fase più dispendiosa in termini di tempo e calcolo, ciò migliora direttamente la velocità e la produttività del sistema ML.\nEfficienza del Ciclo di Iterazione del Modello: Se i modelli vengono addestrati più rapidamente, il tempo di risposta complessivo per le iterazioni di sperimentazione, valutazione e progettazione del modello diminuisce in modo significativo. I sistemi hanno maggiore flessibilità per esplorare architetture, pipeline di dati, ecc., entro determinati intervalli di tempo.\nImpatto sulle Epoche di Addestramento Necessarie: Il processo di addestramento viene eseguito per più epoche, con ogni passaggio completo attraverso i dati che rappresenta un’epoca. Una buona inizializzazione può ridurre le epoche necessarie per far convergere le curve di perdita e accuratezza sul set di addestramento del 10-30%. Ciò significa risparmi tangibili sui costi di risorse e infrastruttura.\nEffetto sugli Iperparametri di Addestramento: I parametri di inizializzazione del peso interagiscono fortemente con determinati iperparametri di regolarizzazione che governano le dinamiche di addestramento, come i programmi di velocità di apprendimento e le probabilità di abbandono. Trovare la giusta combinazione di impostazioni non è banale. Un’inizializzazione appropriata semplifica questa ricerca.\n\nL’inizializzazione dei pesi ha vantaggi a cascata per l’efficienza ingegneristica dell’apprendimento automatico e un overhead di risorse di sistema ridotto al minimo. È una tattica facilmente trascurata che ogni professionista dovrebbe padroneggiare. La scelta di quale tecnica di inizializzazione del peso utilizzare dipende da fattori come l’architettura del modello (numero di layer, pattern di connettività, ecc.), le funzioni di attivazione e il problema specifico da risolvere. Nel corso degli anni, i ricercatori hanno sviluppato e verificato empiricamente diverse strategie di inizializzazione mirate alle comuni architetture di reti neurali, di cui parleremo qui.\n\n7.9.1 Inizializzazione Uniforme e Normale\nQuando si inizializzano pesi in modo casuale, vengono comunemente utilizzate due distribuzioni di probabilità standard: uniforme e Gaussiana (normale). La distribuzione uniforme imposta una probabilità uguale che i parametri di peso iniziali rientrino in qualsiasi punto entro i limiti minimi e massimi impostati. Ad esempio, i limiti potrebbero essere -1 e 1, portando a una distribuzione uniforme dei pesi tra questi limiti. La distribuzione gaussiana, d’altra parte, concentra la probabilità attorno a un valore medio, seguendo la forma di una curva a campana. La maggior parte dei valori di peso si raggrupperà nella regione della media specificata, con meno campioni verso le estremità. Il parametro di deviazione standard controlla la distribuzione attorno alla media.\nLa scelta tra inizializzazione uniforme o normale dipende dall’architettura di rete e dalle funzioni di attivazione. Per reti poco profonde, si consiglia una distribuzione normale con una deviazione standard relativamente piccola (ad esempio, 0.01). La curva a campana impedisce valori di peso elevati che potrebbero innescare l’instabilità di addestramento in reti piccole. Per reti più profonde, una distribuzione normale con deviazione standard più elevata (diciamo 0.5 o superiore) o una distribuzione uniforme può essere preferita per tenere conto dei problemi di gradiente evanescente su molti layer. La maggiore diffusione determina una maggiore differenziazione tra i comportamenti dei neuroni. La messa a punto dei parametri di distribuzione di inizializzazione è fondamentale per una convergenza stabile e rapida del modello. Il monitoraggio dei trend di “loss” [perdita] di addestramento può diagnosticare i problemi per modificare i parametri in modo iterativo.\n\n\n7.9.2 Inizializzazione Xavier\nProposta da Glorot e Bengio (2010), questa tecnica di inizializzazione è progettata appositamente per le funzioni di attivazione sigmoide e tanh. Queste attivazioni saturate possono causare gradienti evanescenti o esplosivi durante la retro-propagazione su molti layer.\n\nGlorot, Xavier, e Yoshua Bengio. 2010. «Understanding the difficulty of training deep feedforward neural networks.» In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249–56. http://proceedings.mlr.press/v9/glorot10a.html.\nIl metodo Xavier imposta in modo intelligente la varianza della distribuzione dei pesi in base al numero di input e output per ciascun layer. L’intuizione è che questo bilancia il flusso di informazioni e gradienti in tutta la rete. Ad esempio, si consideri un layer con 300 unità di input e 100 unità di output. Inserendo questo nella formula varianza = 2/(#inputs + #outputs) si ottiene una varianza di 2/(300+100) = 0.01.\nIl campionamento dei pesi iniziali da una distribuzione uniforme o normale centrata su 0 con questa varianza fornisce una convergenza di addestramento molto più fluida per reti sigmoide/tanh profonde. I gradienti sono ben condizionati, impedendo la scomparsa o la crescita esponenziale.\n\n\n7.9.3 Inizializzazione He\nCome proposto da He et al. (2015), questa tecnica di inizializzazione è adattata alle funzioni di attivazione ReLU (Rectified Linear Unit). Le ReLU introducono il problema del neurone morente in cui le unità rimangono bloccate e producono solo 0 se inizialmente ricevono forti input negativi. Ciò rallenta e ostacola l’addestramento.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2015. «Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification». In 2015 IEEE International Conference on Computer Vision (ICCV), 1026–34. IEEE. https://doi.org/10.1109/iccv.2015.123.\n“He” supera questo problema campionando i pesi da una distribuzione con un set di varianza basato solo sul numero di input per layer, ignorando gli output. Ciò mantiene i segnali in arrivo sufficientemente piccoli da attivare le ReLU nel loro regime lineare dall’inizio, evitando unità morte. Per un layer con 1024 input, la formula varianza = 2/1024 = 0.002 mantiene la maggior parte dei pesi concentrati strettamente attorno a 0.\nQuesta inizializzazione specializzata consente alle reti ReLU di convergere in modo efficiente fin dall’inizio. La scelta tra Xavier e He deve corrispondere alla funzione di attivazione della rete prevista.\n\n\n\n\n\n\nEsercizio 7.5: Inizializzazione dei Pesi\n\n\n\n\n\nFacciamo partire la rete neurale col piede giusto con l’inizializzazione dei pesi! Il modo in cui si impostano quei pesi iniziali può fare la differenza nell’addestramento del modello. Si immagini di accordare gli strumenti di un’orchestra prima del concerto. In questo notebook Colab, si imparerà che la giusta strategia di inizializzazione può far risparmiare tempo, migliorare le prestazioni del modello e rendere il percorso di deep-learning molto più fluido.\n\n\n\n\nVideo 7.8 sottolinea l’importanza di selezionare deliberatamente i valori di peso iniziale rispetto a scelte casuali.\n\n\n\n\n\n\nVideo 7.8: Inizializzazione dei Pesi",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#colli-di-bottiglia-del-sistema",
    "href": "contents/training/training.it.html#colli-di-bottiglia-del-sistema",
    "title": "7  Addestramento dell’IA",
    "section": "7.10 “Colli di Bottiglia” del Sistema",
    "text": "7.10 “Colli di Bottiglia” del Sistema\nCome introdotto in precedenza, le reti neurali comprendono operazioni lineari (moltiplicazioni di matrici) intervallate da funzioni di attivazione non lineari elemento per elemento. La parte computazionalmente più costosa delle reti neurali sono le trasformazioni lineari, in particolare le moltiplicazioni di matrici tra ogni layer. Questi layer lineari mappano le attivazioni dal layer precedente a uno spazio dimensionale superiore che funge da input per la funzione di attivazione del layer successivo.\n\n7.10.1 Complessità a Runtime della Moltiplicazione di Matrici\n\nMoltiplicazioni di Layer vs. Attivazioni\nLa maggior parte del calcolo nelle reti neurali deriva dalle moltiplicazioni di matrici tra layer. Si consideri un layer di rete neurale con una dimensione di input di \\(M\\) = 500 e una dimensione di output di \\(N\\) = 1000; la moltiplicazione di matrici richiede \\(O(N \\cdot M) = O(1000 \\cdot 500) = 500,000\\) operazioni di moltiplicazione-accumulazione (MAC) tra quei layer.\nConfronta questo col layer precedente, che aveva \\(M\\) = 300 input, che richiedevano \\(O(500 \\cdot 300) = 150,000\\) operazioni. Man mano che le dimensioni dei layer aumentano, i requisiti computazionali aumentano quadraticamente con la dimensione del layer. I calcoli totali su \\(L\\) layer possono essere espressi come \\(\\sum_{l=1}^{L-1} O\\big(N^{(l)} \\cdot M^{(l-1)}\\big)\\), dove il calcolo richiesto per ogni layer dipende dal prodotto delle dimensioni di input e output delle matrici che vengono moltiplicate.\nOra, confrontando la moltiplicazione della matrice con la funzione di attivazione, che richiede solo \\(O(N) = 1000\\) non linearità elemento per elemento per \\(N = 1000\\) output, possiamo vedere le trasformazioni lineari che dominano le attivazioni computazionalmente.\nQueste grandi moltiplicazioni di matrici influiscono sulle scelte hardware, sulla latenza dell’inferenza e sui vincoli di potenza per le applicazioni di reti neurali nel mondo reale. Ad esempio, un tipico layer DNN potrebbe richiedere 500,000 moltiplicazioni-accumulazioni rispetto a solo 1000 attivazioni non lineari, dimostrando un aumento di 500x nelle operazioni matematiche.\nQuando si addestrano reti neurali, in genere utilizziamo la discesa del gradiente in mini-batch, operando su piccoli batch di dati contemporaneamente. Considerando una dimensione batch di \\(B\\) esempi di addestramento, l’input per la moltiplicazione di matrice diventa una matrice \\(M \\times B\\), mentre l’output è una matrice \\(N \\times B\\).\n\n\nMini-batch\nNell’addestramento delle reti neurali, dobbiamo stimare ripetutamente il gradiente della funzione di perdita rispetto ai parametri di rete (ad esempio, pesi e bias). Questo gradiente indica in quale direzione i parametri devono essere aggiornati per ridurre al minimo la perdita. Come introdotto in precedenza, eseguiamo aggiornamenti su un batch di dati a ogni aggiornamento, noto anche come discesa del gradiente stocastico o “discesa del gradiente mini-batch”.\nL’approccio più semplice è stimare il gradiente in base a un singolo esempio di addestramento, calcolare l’aggiornamento dei parametri, riassettare tutto e ripetere per l’esempio successivo. Tuttavia, ciò comporta aggiornamenti dei parametri molto piccoli e frequenti che possono essere computazionalmente inefficienti e potrebbero dover essere più accurati in termini di convergenza a causa della stocasticità dell’utilizzo di un solo dato per un aggiornamento del modello.\nInvece, la discesa del gradiente in mini-batch bilancia la stabilità della convergenza e l’efficienza computazionale. Invece di calcolare il gradiente su singoli esempi, stimiamo il gradiente in base a piccoli “mini-batch” di dati, solitamente tra 8 e 256 esempi in pratica.\nCiò fornisce una stima del gradiente rumorosa ma coerente che porta a una convergenza più stabile. Inoltre, l’aggiornamento dei parametri deve essere eseguito solo una volta per mini-batch anziché una volta per ogni esempio, riducendo il sovraccarico computazionale.\nRegolando la dimensione del mini-batch, possiamo controllare il compromesso tra la fluidità della stima (i batch più grandi sono generalmente migliori) e la frequenza degli aggiornamenti (i batch più piccoli consentono aggiornamenti più frequenti). Le dimensioni del mini-batch sono solitamente potenze di 2, quindi possono sfruttare in modo efficiente il parallelismo tra i core GPU.\nQuindi, il calcolo totale esegue una moltiplicazione di matrici \\(N \\times M\\) per \\(M \\times B\\), producendo \\(O(N \\cdot M \\cdot B)\\) operazioni in virgola mobile. Come esempio numerico, \\(N=1000\\) unità nascoste, \\(M=500\\) unità di input e una dimensione del batch \\(B=64\\) equivale a 1000 x 500 x 64 = 32 milioni di moltiplicazioni-accumulazioni per iterazione di training!\nAl contrario, le funzioni di attivazione vengono applicate elemento per elemento alla matrice di output \\(N \\times B\\), richiedendo solo \\(O(N \\cdot B)\\) calcoli. Per \\(N=1000\\) e \\(B=64\\), si tratta di sole 64,000 non linearità, ovvero 500 volte meno lavoro della moltiplicazione di matrici.\nMan mano che aumentiamo le dimensioni del batch per sfruttare appieno hardware parallelo come le GPU, la discrepanza tra la moltiplicazione di matrici e il costo della funzione di attivazione aumenta ulteriormente. Ciò rivela come l’ottimizzazione delle operazioni di algebra lineare offra enormi guadagni di efficienza.\nPertanto, la moltiplicazione di matrici è fondamentale nell’analisi di dove e come le reti neurali impiegano i calcoli. Ad esempio, le moltiplicazioni di matrici spesso rappresentano oltre il 90% della latenza di inferenza e del tempo di addestramento nelle comuni reti neurali convoluzionali e ricorrenti.\n\n\nOttimizzazione della Moltiplicazione di Matrici\nDiverse tecniche migliorano l’efficienza delle operazioni generali matrice-matrice densa/sparsa e matrice-vettore per migliorare l’efficienza complessiva. Alcuni metodi chiave sono:\n\nSfruttamento di librerie matematiche ottimizzate come cuBLAS per l’accelerazione GPU\nAbilitazione di formati di precisione inferiore come FP16 o INT8 dove l’accuratezza lo consente\nUtilizzo di Tensor Processing Unit con moltiplicazione di matrici in hardware\nCalcoli consapevoli della sparsità e formati di archiviazione dati per sfruttare i parametri zero\nApprossimazione delle moltiplicazioni di matrici con algoritmi come le Fast Fourier Transform\nProgettazione dell’architettura del modello per ridurre le larghezze e le attivazioni degli layer\nQuantizzazione, pruning [potatura], distillazione e altre tecniche di compressione\nParallelizzazione del calcolo sull’hardware disponibile\nRisultati di caching/pre-calcolo ove possibile per ridurre le operazioni ridondanti\n\nLe potenziali tecniche di ottimizzazione sono vaste, data la porzione sproporzionata di tempo che i modelli trascorrono nella matematica di matrici e vettori. Anche i miglioramenti incrementali velocizzano i tempi di esecuzione e riducono il consumo di energia. Trovare nuovi modi per migliorare queste primitive di algebra lineare rimane un’area di ricerca attiva allineata con le future esigenze di machine learning. Ne parleremo in dettaglio nei capitoli Ottimizzazioni e Accelerazione IA.\n\n\n\n7.10.2 Calcolo vs. Collo di Bottiglia della Memoria\nA questo punto, la moltiplicazione matrice-matrice è l’operazione matematica fondamentale alla base delle reti neurali. Sia l’addestramento che l’inferenza per le reti neurali utilizzano ampiamente queste operazioni di moltiplicazione di matrici. L’analisi mostra che oltre il 90% dei requisiti computazionali nelle reti neurali attuali derivano da moltiplicazioni di matrici. Di conseguenza, le prestazioni della moltiplicazione di matrici hanno un’enorme influenza sul tempo complessivo di addestramento o inferenza del modello.\n\nAddestramento vs. Inferenza\nMentre l’addestramento e l’inferenza si basano ampiamente sulle prestazioni della moltiplicazione di matrici, i loro profili computazionali precisi differiscono. In particolare, l’inferenza della rete neurale tende a essere più legata al calcolo rispetto all’addestramento per una dimensione di batch equivalente. La differenza fondamentale risiede nel passaggio di backpropagation, che è richiesto solo durante l’addestramento. La backpropagation implica una sequenza di operazioni di moltiplicazione di matrici per calcolare i gradienti rispetto alle attivazioni su ogni layer della rete. Tuttavia, è fondamentale che qui non sia necessaria alcuna larghezza di banda di memoria aggiuntiva: gli input, gli output e i gradienti vengono letti/scritti dalla cache o dai registri.\nDi conseguenza, l’addestramento mostra intensità aritmetiche inferiori, con calcoli del gradiente limitati dall’accesso alla memoria anziché dai FLOP (Floating Point Operations Per Second), una misura delle prestazioni computazionali che indica quanti calcoli in virgola mobile un sistema può eseguire al secondo. Al contrario, la propagazione in avanti domina l’inferenza della rete neurale, che corrisponde a una serie di moltiplicazioni matrice-matrice. Senza una retrospettiva del gradiente che richiede molta memoria, le dimensioni dei batch più grandi spingono facilmente l’inferenza a essere estremamente limitata dal calcolo. Le elevate intensità aritmetiche misurate mostrano questo. I tempi di risposta possono essere critici per alcune applicazioni di inferenza, costringendo il fornitore dell’applicazione a utilizzare una dimensione di batch inferiore per soddisfare questi requisiti di tempo di risposta, riducendo così l’efficienza dell’hardware; quindi, le inferenze potrebbero vedere un utilizzo inferiore dell’hardware.\nLe implicazioni sono che il provisioning hardware e i compromessi tra larghezza di banda e FLOP differiscono a seconda che un sistema miri al training o all’inferenza. I server ad alta produttività e bassa latenza per l’inferenza dovrebbero enfatizzare la potenza di calcolo anziché la memoria, mentre i cluster di training richiedono un’architettura più bilanciata.\nTuttavia, la moltiplicazione di matrici mostra un’interessante tensione: la larghezza di banda della memoria dell’hardware sottostante o le capacità di throughput aritmetico possono vincolarla. La capacità del sistema di recuperare e fornire dati matriciali rispetto alla sua capacità di eseguire operazioni di calcolo determina questa direzione.\nQuesto fenomeno ha impatti profondi; l’hardware deve essere progettato giudiziosamente e devono essere prese in considerazione le ottimizzazioni del software. Ottimizzare e bilanciare il calcolo rispetto alla memoria per alleviare questo collo di bottiglia della moltiplicazione di matrici è fondamentale per un training un deployment efficienti del modello.\nInfine, la dimensione del batch può avere un impatto sui tassi di convergenza durante l’addestramento della rete neurale, un’altra considerazione importante. Ad esempio, ci sono generalmente rendimenti decrescenti nei benefici della convergenza con dimensioni di batch estremamente grandi (ad esempio, &gt; 16384). Al contrario, dimensioni di batch estremamente grandi possono essere sempre più vantaggiose da una prospettiva di intensità hardware/aritmetica; l’utilizzo di batch così grandi potrebbe non tradursi in una convergenza più rapida rispetto al tempo a causa dei loro benefici decrescenti per la convergenza. Questi compromessi fanno parte delle decisioni di progettazione fondamentali per i sistemi per il tipo di ricerca basata sull’apprendimento automatico.\n\n\nDimensione del Batch\nLa dimensione del batch utilizzata durante l’addestramento e l’inferenza della rete neurale ha un impatto significativo sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria. In concreto, la dimensione del batch si riferisce al numero di campioni propagati assieme attraverso la rete in un passaggio avanti/indietro. La moltiplicazione di matrici equivale a dimensioni di matrice maggiori.\nIn particolare, diamo un’occhiata all’intensità aritmetica della moltiplicazione di matrici durante l’addestramento della rete neurale. Questa misura il rapporto tra operazioni computazionali e trasferimenti di memoria. La moltiplicazione di due matrici di dimensione \\(N \\times M\\) e \\(M \\times B\\) richiede \\(N \\times M \\times B\\) operazioni di moltiplicazione-accumulo, ma solo trasferimenti di \\(N \\times M + M \\times B\\) elementi di matrice.\nMan mano che aumentiamo la dimensione del batch \\(B\\), il numero di operazioni aritmetiche cresce più velocemente dei trasferimenti di memoria. Ad esempio, con una dimensione del batch di 1, abbiamo bisogno di \\(N \\times M\\) operazioni e \\(N + M\\) trasferimenti, dando un rapporto di intensità aritmetica di circa \\(\\frac{N \\times M}{N+M}\\). Ma con una dimensione del batch di grandi dimensioni di 128, il rapporto di intensità diventa \\(\\frac{128 \\times N \\times M}{N \\times M + M \\times 128} \\approx 128\\). L’utilizzo di una dimensione del batch più grande sposta il calcolo complessivo da vincolato alla memoria a più vincolato al calcolo. L’addestramento IA utilizza grandi dimensioni del batch ed è generalmente limitato dalle massime prestazioni di calcolo aritmetiche, ovvero l’Applicazione 3 in Figura 7.7.\nPertanto, la moltiplicazione di matrici in batch è molto più intensiva dal punto di vista computazionale rispetto al limite di accesso alla memoria. Ciò ha implicazioni per la progettazione hardware e le ottimizzazioni software, che tratteremo in seguito. L’intuizione chiave è che possiamo modificare in modo significativo il profilo computazionale e i colli di bottiglia posti dall’addestramento e dall’inferenza della rete neurale regolando la dimensione del batch.\n\n\n\n\n\n\nFigura 7.7: Modello a profilo a di tetto per il training di IA.\n\n\n\n\n\nCaratteristiche Hardware\nL’hardware moderno come CPU e GPU è altamente ottimizzato per la produttività computazionale piuttosto che per la larghezza di banda della memoria. Ad esempio, le GPU H100 Tensor Core di fascia alta possono fornire oltre 60 TFLOPS di prestazioni a doppia precisione, ma forniscono solo fino a 3 TB/s di larghezza di banda della memoria. Ciò significa che c’è uno squilibrio di quasi 20 volte tra unità aritmetiche e accesso alla memoria; di conseguenza, per hardware come gli acceleratori GPU, i carichi di lavoro di addestramento della rete neurale devono essere resi il più intensivi possibile dal punto di vista computazionale per utilizzare appieno le risorse disponibili.\nCiò motiva ulteriormente la necessità di utilizzare batch di grandi dimensioni durante l’addestramento. Quando si utilizza un batch di piccole dimensioni, la moltiplicazione della matrice è limitata dalla larghezza di banda della memoria, sottoutilizzando le abbondanti risorse di elaborazione. Tuttavia, possiamo spostare il collo di bottiglia verso l’elaborazione e ottenere un’intensità aritmetica molto più elevata con batch sufficientemente grandi. Ad esempio, potrebbero essere necessari batch di 256 o 512 campioni per saturare una GPU di fascia alta. Lo svantaggio è che batch più grandi forniscono aggiornamenti dei parametri meno frequenti, il che può influire sulla convergenza. Tuttavia, il parametro funge da importante manopola di sintonizzazione per bilanciare le limitazioni di memoria e quelle di elaborazione.\nPertanto, date le architetture di elaborazione-memoria sbilanciate dell’hardware moderno, l’impiego di batch di grandi dimensioni è essenziale per alleviare i colli di bottiglia e massimizzare la produttività. Come accennato, anche il software e gli algoritmi successivi devono adattarsi a tali dimensioni di batch, poiché dimensioni di batch più grandi possono avere rendimenti decrescenti verso la convergenza della rete. L’utilizzo di dimensioni di batch molto piccole può portare a un utilizzo non ottimale dell’hardware, limitando in ultima analisi l’efficienza del training. L’aumento di dimensioni dei batch di grandi dimensioni è un argomento di ricerca esplorato in vari lavori che mirano a eseguire una training su larga scala (You et al. 2017).\n\nYou, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, e Kurt Keutzer. 2017. «ImageNet Training in Minutes», settembre. http://arxiv.org/abs/1709.05011v10.\n\n\nArchitetture dei Modelli\nL’architettura della rete neurale influisce anche sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria maggiore durante l’esecuzione. I trasformatori e gli MLP sono molto più vincolati al calcolo rispetto alle reti neurali convoluzionali CNN. Ciò deriva dai tipi di operazioni di moltiplicazione di matrici coinvolte in ciascun modello. I trasformatori si basano sull’auto-attenzione, moltiplicando grandi matrici di attivazione per enormi matrici di parametri per correlare gli elementi. Gli MLP impilano layer completamente connessi, richiedendo anche grandi moltiplicazioni matriciali.\nAl contrario, i layer convoluzionali nelle CNN hanno una finestra scorrevole che riutilizza attivazioni e parametri nell’input, il che significa che sono necessarie meno operazioni matriciali univoche. Tuttavia, le convoluzioni richiedono l’accesso ripetuto a piccole parti di input e lo spostamento di somme parziali per popolare ciascuna finestra. Sebbene le operazioni aritmetiche nelle convoluzioni siano intense, questo spostamento di dati e la manipolazione del buffer impongono enormi overhead di accesso alla memoria. Le CNN comprendono diverse fasi a strati, quindi gli output intermedi devono materializzarsi frequentemente nella memoria.\nDi conseguenza, l’addestramento CNN tende a essere più vincolato alla larghezza di banda della memoria rispetto al limite aritmetico in confronto a Transformers e MLP. Pertanto, il profilo di moltiplicazione della matrice e, a sua volta, il collo di bottiglia posto, varia in modo significativo in base alla scelta del modello. Hardware e sistemi devono essere progettati con un appropriato equilibrio di larghezza di banda di elaborazione-memoria a seconda dell’implementazione del modello target. I modelli che si basano maggiormente sull’attenzione e sui layer MLP richiedono una maggiore produttività aritmetica rispetto alle CNN, il che richiede un’elevata larghezza di banda della memoria.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#parallelizzazione-del-training",
    "href": "contents/training/training.it.html#parallelizzazione-del-training",
    "title": "7  Addestramento dell’IA",
    "section": "7.11 Parallelizzazione del Training",
    "text": "7.11 Parallelizzazione del Training\nL’addestramento delle reti neurali comporta richieste di calcolo e memoria intensive. L’algoritmo di backpropagation per il calcolo dei gradienti e l’aggiornamento dei pesi consiste in ripetute moltiplicazioni di matrici e operazioni aritmetiche sull’intero set di dati. Ad esempio, un passaggio di backpropagation scala in complessità temporale con \\(O(num\\_parameters \\times batch\\_size \\times sequence\\_length)\\).\nI requisiti di calcolo aumentano rapidamente con l’aumento delle dimensioni del modello in parametri e layer. Inoltre, l’algoritmo richiede l’archiviazione di output di attivazione e parametri del modello per la fase di backward, che cresce con le dimensioni del modello.\nI modelli più grandi non possono adattarsi e addestrarsi su un singolo dispositivo acceleratore come una GPU e l’ingombro di memoria diventa proibitivo. Pertanto, dobbiamo parallelizzare l’addestramento del modello su più dispositivi per fornire elaborazione e memoria sufficienti per addestrare reti neurali all’avanguardia.\nCome mostrato in Figura 7.8, i due approcci principali sono il parallelismo dei dati, che replica il modello su più dispositivi suddividendo i dati di input in batch, e il parallelismo del modello, che suddivide l’architettura del modello stesso su diversi dispositivi. Tramite il training in parallelo, possiamo sfruttare maggiori risorse aggregate di elaborazione e memoria per superare le limitazioni del sistema e accelerare i carichi di lavoro di deep learning.\n\n\n\n\n\n\nFigura 7.8: Parallelismo dei dati e parallelismo del modello.\n\n\n\n\n7.11.1 Parallelismo dei Dati\nLa parallelizzazione dei dati è un approccio comune per parallelizzare il training di apprendimento automatico su più unità di elaborazione, come GPU o risorse di elaborazione distribuite. Il set di dati di addestramento è suddiviso in batch nel parallelismo dei dati e un’unità di elaborazione separata elabora ogni batch. I parametri del modello vengono poi aggiornati in base ai gradienti calcolati dall’elaborazione di ogni batch. Ecco una descrizione dettagliata della parallelizzazione dei dati per il training ML:\n\nDivisione del Dataset: Il set di dati di addestramento è suddiviso in batch più piccoli, ciascuno contenente un sottoinsieme degli esempi di training.\nReplica del Modello: Il modello di rete neurale è replicato su tutte le unità di elaborazione e ogni unità di elaborazione ha la sua copia.\nCalcolo Parallelo: Ogni unità di elaborazione prende un batch diverso e calcola in modo indipendente i passaggi in forward e backward. Durante il passaggio forward [in avanti], il modello fa delle previsioni sui dati di input. La funzione di loss [perdita] determina i gradienti per i parametri del modello durante il passaggio backward [all’indietro].\nAggregazione dei Gradienti: Dopo l’elaborazione dei rispettivi batch, i gradienti di ogni unità di elaborazione vengono aggregati. I metodi di aggregazione comuni includono la sommatoria o la media dei gradienti.\nAggiornamento dei Parametri: I gradienti aggregati aggiornano i parametri del modello. L’aggiornamento può essere eseguito utilizzando algoritmi di ottimizzazione come SGD o varianti come Adam.\nSincronizzazione: Dopo l’aggiornamento, tutte le unità di elaborazione sincronizzano i parametri del modello, assicurandosi che ciascuna ne abbia la versione più recente.\n\nI passaggi precedenti vengono ripetuti per diverse iterazioni o fino alla convergenza.\nPrendiamo un esempio specifico. Abbiamo 256 dimensioni di batch e 8 GPU; ogni GPU riceverà un micro-batch di 32 campioni. I loro passaggi forward e backward calcolano perdite e gradienti solo in base ai 32 campioni locali. I gradienti vengono aggregati tra i dispositivi con un server dei parametri o una libreria di comunicazioni collettiva per ottenere il gradiente effettivo per il batch globale. Gli aggiornamenti dei pesi avvengono indipendentemente su ogni GPU in base a questi gradienti. Dopo un numero configurato di iterazioni, i pesi aggiornati si sincronizzano e si equalizzano tra i dispositivi prima di passare alle iterazioni successive.\nIl parallelismo dei dati è efficace quando il modello è grande e il set di dati è sostanziale, poiché consente l’elaborazione parallela di diverse parti dei dati. È ampiamente utilizzato in framework e librerie di deep learning che supportano il training distribuito, come TensorFlow e PyTorch. Tuttavia, per garantire una parallelizzazione efficiente, è necessario prestare attenzione a gestire problemi come l sovraccarico della comunicazione, bilanciamento del carico e sincronizzazione.\n\n\n7.11.2 Parallelismo del Modello\nIl parallelismo del modello si riferisce alla distribuzione del modello di rete neurale su più dispositivi anziché alla replica del modello completo come il parallelismo dei dati. Ciò è particolarmente utile quando un modello è troppo grande per essere inserito nella memoria di una singola GPU o di un dispositivo acceleratore. Sebbene ciò potrebbe non essere specificamente applicabile per casi d’uso embedded o TinyML poiché la maggior parte dei modelli è relativamente piccola, è comunque utile saperlo.\nNell’addestramento parallelo del modello, diverse parti o layer del modello vengono assegnati a dispositivi separati. Le attivazioni di input e gli output intermedi vengono partizionati e passati tra questi dispositivi durante i passaggi forward e backward per coordinare i calcoli del gradiente tra le partizioni del modello.\nIl “footprint” [impronta] di memoria e le operazioni di calcolo vengono distribuite suddividendo l’architettura del modello su più dispositivi anziché concentrarsi su uno. Ciò consente l’addestramento di modelli molto grandi con miliardi di parametri che altrimenti supererebbero la capacità di un singolo dispositivo. Esistono diversi modi in cui possiamo eseguire il partizionamento:\n\nParallelismo di Layer: I layer consecutivi sono distribuiti su dispositivi diversi. Ad esempio, il dispositivo 1 contiene i layer 1-3; il dispositivo 2 contiene i layer 4-6. Le attivazioni di output dal layer 3 verrebbero trasferite al dispositivo 2 per avviare i layer successivi per i calcoli della fase di forward.\nParallelismo a Livello di Filtro: Nei layer convoluzionali, i filtri di output possono essere suddivisi tra più dispositivi. Ogni dispositivo calcola gli output di attivazione per un sottoinsieme di filtri, che vengono concatenati prima di propagarsi ulteriormente.\nParallelismo Spaziale: Le immagini di input vengono divise spazialmente, quindi ogni dispositivo elabora una determinata regione come il quarto in alto a sinistra delle immagini. Le regioni di output si combinano poi per formare l’output completo.\n\nInoltre, le combinazioni ibride possono suddividere il modello a livello di layer e i dati in batch. Il tipo appropriato di parallelismo del modello dipende dai vincoli specifici dell’architettura neurale e dalla configurazione hardware. Ottimizzare il partizionamento e la comunicazione per la topologia del modello è fondamentale per ridurre al minimo il sovraccarico.\nTuttavia, poiché le parti del modello vengono eseguite su dispositivi fisicamente separati, devono comunicare e sincronizzare i loro parametri durante ogni fase di addestramento. La fase di backward deve garantire che gli aggiornamenti del gradiente si propaghino accuratamente tra le partizioni del modello. Quindi, il coordinamento e l’interconnessione ad alta velocità tra i dispositivi sono fondamentali per ottimizzare le prestazioni dell’addestramento parallelo. Sono necessari dei buoni protocolli di partizionamento e comunicazione per ridurre al minimo il sovraccarico di trasferimento.\n\n\n7.11.3 Confronto\nRiassumendo, Tabella 7.5 illustra alcune delle caratteristiche chiave per confrontare il parallelismo dei dati e quello dei modelli.\n\n\n\nTabella 7.5: Confronto tra parallelismo dei dati e parallelismo del modello.\n\n\n\n\n\n\n\n\n\n\nCaratteristica\nParallelismo dei dati\nParallelismo del modello\n\n\n\n\nDefinizione\nDistribuisce i dati tra i dispositivi con repliche\nDistribuisce il modello tra i dispositivi\n\n\nObiettivo\nAccelera il training tramite il ridimensionamento del calcolo\nAbilita un training del modello più ampio\n\n\nMetodo di Ridimensionamento\nDispositivi/workers in scala\nDimensioni modello in scala\n\n\nVincolo Principale\nDimensione del modello per ogni dispositivo\nOverhead di coordinamento dispositivo\n\n\nRequisiti Hardware\nPiù GPU/TPU\nSpesso interconnessione specializzata\n\n\nDifficoltà Principale\nSincronizzazione dei parametri\nPartizionamento e comunicazione complicati\n\n\nTipi\nN/D\nPer livello, per filtro, spaziale\n\n\nComplessità del Codice\nModifiche minime\nIntervento più significativa sul modello\n\n\nLibrerie Popolari\nHorovod, PyTorch Distributed\nMesh TensorFlow",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#conclusione",
    "href": "contents/training/training.it.html#conclusione",
    "title": "7  Addestramento dell’IA",
    "section": "7.12 Conclusione",
    "text": "7.12 Conclusione\nIn questo capitolo abbiamo trattato le basi fondamentali che consentono un training efficace dei modelli di intelligenza artificiale. Abbiamo esplorato concetti matematici come funzioni di perdita, backpropagation e discesa del gradiente che rendono possibile l’ottimizzazione delle reti neurali. Abbiamo anche discusso tecniche pratiche per sfruttare i dati di training, la regolarizzazione, la messa a punto degli iperparametri, l’inizializzazione dei pesi e strategie di parallelizzazione distribuita che migliorano convergenza, generalizzazione e scalabilità.\nQueste metodologie costituiscono il fondamento attraverso cui è stato raggiunto il successo del deep learning nell’ultimo decennio. Padroneggiare questi fondamenti prepara i professionisti a progettare sistemi e perfezionare modelli su misura per il loro contesto. Tuttavia, man mano che modelli e set di dati crescono in modo esponenziale, i sistemi di training devono ottimizzare parametri come tempo, costo e “carbon footprint” [impatto ambientale]. Il ridimensionamento hardware tramite grosse warehouse consente un throughput computazionale enorme, ma le ottimizzazioni relative a efficienza e specializzazione saranno fondamentali. Tecniche software come compressione e sfruttamento delle matrici sparse possono aumentare i guadagni hardware. Ne discuteremo diverse nei prossimi capitoli.\nNel complesso, i fondamenti trattati in questo capitolo preparano i professionisti a costruire, perfezionare e distribuire modelli. Tuttavia, le competenze interdisciplinari che abbracciano teoria, sistemi e hardware differenzieranno gli esperti in grado di portare l’IA al livello successivo in modo sostenibile e responsabile, come richiesto dalla società. Comprendere l’efficienza insieme all’accuratezza costituisce l’approccio ingegneristico bilanciato necessario per addestrare sistemi intelligenti che si integrano senza problemi in molti contesti del mondo reale.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#sec-ai-training-resource",
    "href": "contents/training/training.it.html#sec-ai-training-resource",
    "title": "7  Addestramento dell’IA",
    "section": "7.13 Risorse",
    "text": "7.13 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nThinking About Loss.\nMinimizing Loss.\nTraining, Validation, and Test Data.\nContinuous Training:\n\nRetraining Trigger.\nData Processing Overview.\nData Ingestion.\nData Validation.\nData Transformation.\nTraining with AutoML.\nContinuous Training with Transfer Learning.\nContinuous Training Use Case Metrics.\nContinuous Training Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 7.1\nVideo 7.2\nVideo 7.3\nVideo 7.4\nVideo 7.5\nVideo 7.6\nVideo 7.7\nVideo 7.8\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 7.1\nEsercizio 7.2\nEsercizio 7.3\nEsercizio 7.5\nEsercizio 7.4\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html",
    "href": "contents/efficient_ai/efficient_ai.it.html",
    "title": "8  IA Efficiente",
    "section": "",
    "text": "8.1 Introduzione\nI modelli di training possono consumare molta energia, a volte equivalente all’impatto ambientale di processi industriali considerevoli. Tratteremo alcuni di questi dettagli sulla sostenibilità nel capitolo Sostenibilità dell’IA. Dal punto di vista dell’implementazione, se questi modelli non sono ottimizzati per l’efficienza, possono esaurire rapidamente le batterie dei dispositivi, richiedere una memoria eccessiva o non soddisfare le esigenze di elaborazione in tempo reale. In questo capitolo, miriamo a chiarire le sfumature dell’efficienza, gettando le basi per un’esplorazione completa nei capitoli successivi.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#la-necessità-di-unia-efficiente",
    "href": "contents/efficient_ai/efficient_ai.it.html#la-necessità-di-unia-efficiente",
    "title": "8  IA Efficiente",
    "section": "8.2 La Necessità di un’IA Efficiente",
    "text": "8.2 La Necessità di un’IA Efficiente\nL’efficienza assume connotazioni diverse a seconda di dove si verificano i calcoli dell’IA. Rivediamo Cloud, Edge e TinyML (come discusso in Sistemi di ML) e distinguiamoli in termini di efficienza. Figura 8.1 fornisce un confronto generale delle tre diverse piattaforme.\n\n\n\n\n\n\nFigura 8.1: Cloud, Mobile e TinyML. Fonte: Schizas et al. (2022).\n\n\nSchizas, Nikolaos, Aristeidis Karras, Christos Karras, e Spyros Sioutas. 2022. «TinyML for Ultra-Low Power AI and Large Scale IoT Deployments: A Systematic Review». Future Internet 14 (12): 363. https://doi.org/10.3390/fi14120363.\n\n\nIA Cloud: I modelli IA tradizionali vengono spesso eseguiti in data center su larga scala dotati di potenti GPU e TPU (Barroso, Hölzle, e Ranganathan 2019). Qui, l’efficienza riguarda l’ottimizzazione delle risorse di calcolo, la riduzione dei costi e la garanzia di elaborazione e restituzione tempestive dei dati. Tuttavia, fare affidamento sul cloud introduce latenza, soprattutto quando si ha a che fare con flussi di dati di grandi dimensioni che richiedono caricamento, elaborazione e download.\n\nBarroso, Luiz André, Urs Hölzle, e Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\nLi, En, Liekang Zeng, Zhi Zhou, e Xu Chen. 2020. «Edge AI: On-demand Accelerating Deep Neural Network Inference via Edge Computing». IEEE Trans. Wireless Commun. 19 (1): 447–57. https://doi.org/10.1109/twc.2019.2946140.\nIA Edge: L’edge computing avvicina l’intelligenza artificiale alla fonte dei dati, elaborando le informazioni direttamente su dispositivi locali come smartphone, fotocamere o macchine industriali (Li et al. 2020). Qui, l’efficienza comprende risposte rapide in tempo reale e ridotte esigenze di trasmissione dei dati. Tuttavia, i vincoli sono più severi: questi dispositivi, sebbene più potenti dei microcontrollori, hanno una potenza di calcolo limitata rispetto alle configurazioni cloud.\nTinyML: TinyML supera i limiti consentendo ai modelli di intelligenza artificiale di funzionare su microcontrollori o ambienti con risorse estremamente limitate. La differenza di prestazioni del processore e della memoria tra TinyML e i sistemi cloud o mobili può essere di diversi ordini di grandezza (Warden e Situnayake 2019). L’efficienza in TinyML consiste nell’assicurare che i modelli siano sufficientemente leggeri da adattarsi a questi dispositivi, consumino il minimo di energia (fondamentale per i dispositivi alimentati a batteria) e continuino a svolgere le loro attività in modo efficace.\n\nWarden, Pete, e Daniel Situnayake. 2019. Tinyml: Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers. O’Reilly Media.\nLo spettro da Cloud a TinyML rappresenta un passaggio da vaste risorse di elaborazione centralizzate ad ambienti distribuiti, localizzati e limitati. Passando dall’uno all’altro, i problemi e le strategie relative all’efficienza evolvono, sottolineando la necessità di approcci specializzati su misura per ogni scenario. Dopo aver stabilito la necessità di un’intelligenza artificiale efficiente, in particolare nel contesto di TinyML, passeremo all’esplorazione delle metodologie ideate per rispondere a queste sfide. Le sezioni seguenti delineano i concetti principali che approfondiremo in seguito. Dimostreremo l’ampiezza e la profondità dell’innovazione necessarie per ottenere un’intelligenza artificiale efficiente mentre esploriamo queste strategie.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#architetture-di-modelli-efficienti",
    "href": "contents/efficient_ai/efficient_ai.it.html#architetture-di-modelli-efficienti",
    "title": "8  IA Efficiente",
    "section": "8.3 Architetture di Modelli Efficienti",
    "text": "8.3 Architetture di Modelli Efficienti\nSelezionare un’architettura del modello ottimale è tanto cruciale quanto ottimizzarla. Negli ultimi anni, i ricercatori hanno compiuto passi da gigante nell’esplorazione di architetture innovative che possono avere intrinsecamente meno parametri pur mantenendo prestazioni elevate.\nMobileNet: MobileNet sono modelli di applicazioni di visione mobile ed embedded efficienti (Howard et al. 2017). L’idea chiave che ha portato al loro successo sono le convoluzioni separabili in profondità, che riducono significativamente il numero di parametri e calcoli nella rete. MobileNetV2 e V3 migliorano ulteriormente questo design introducendo residui invertiti e colli di bottiglia lineari.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. «SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and 0.5 MB model size». ArXiv preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\nSqueezeNet: SqueezeNet è una classe di modelli ML noti per le sue dimensioni ridotte senza sacrificare la precisione. Ciò si ottiene utilizzando un “modulo fire” che riduce il numero di canali di input a filtri 3x3, riducendo così i parametri (Iandola et al. 2016). Inoltre, impiega il downsampling [sottocampionamento] ritardato per aumentare la precisione mantenendo una mappa delle feature più ampia.\nVarianti ResNet: L’architettura Residual Network (ResNet) consente l’introduzione di connessioni skip o scorciatoie (He et al. 2016). Alcune varianti di ResNet sono progettate per essere più efficienti. Ad esempio, ResNet-SE incorpora il meccanismo “squeeze and excitation” per ricalibrare le feature map (Hu, Shen, e Sun 2018), mentre ResNeXt offre convoluzioni raggruppate per l’efficienza (Xie et al. 2017).\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. «Deep Residual Learning for Image Recognition». In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nHu, Jie, Li Shen, e Gang Sun. 2018. «Squeeze-and-Excitation Networks». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7132–41. IEEE. https://doi.org/10.1109/cvpr.2018.00745.\n\nXie, Saining, Ross Girshick, Piotr Dollar, Zhuowen Tu, e Kaiming He. 2017. «Aggregated Residual Transformations for Deep Neural Networks». In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1492–1500. IEEE. https://doi.org/10.1109/cvpr.2017.634.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-model-compression",
    "href": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-model-compression",
    "title": "8  IA Efficiente",
    "section": "8.4 Compressione Efficiente del Modello",
    "text": "8.4 Compressione Efficiente del Modello\nI metodi di compressione dei modelli sono essenziali per portare i modelli di apprendimento profondo su dispositivi con risorse limitate. Queste tecniche riducono le dimensioni dei modelli, il consumo energetico e le richieste di elaborazione senza perdere significativamente la precisione. Ad alto livello, i metodi possono essere categorizzati nei seguenti metodi fondamentali:\nPruning: L’Abbiamo menzionato un paio di volte nei capitoli precedenti, ma non l’abbiamo ancora formalmente introdotta. Il pruning è simile alla potatura dei rami di un albero. Questo è stato pensato per la prima volta nel documento Optimal Brain Damage (LeCun, Denker, e Solla 1989) ed è stato successivamente reso popolare nel contesto del deep learning da Han, Mao, e Dally (2016). Determinati pesi o interi neuroni vengono rimossi dalla rete nella potatura in base a criteri specifici. Questo può ridurre significativamente le dimensioni del modello. In Sezione 9.2.1 esploreremo due delle principali strategie di potatura, quella strutturata e quella non-strutturata. Figura 8.2 è un esempio di potatura della rete neurale, in cui la rimozione di alcuni nodi negli strati interni (in base a criteri specifici) riduce il numero di rami tra i nodi e, a sua volta, le dimensioni del modello.\n\nLeCun, Yann, John Denker, e Sara Solla. 1989. «Optimal brain damage». Adv Neural Inf Process Syst 2.\n\nHan, Song, Huizi Mao, e William J. Dally. 2016. «Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding». https://arxiv.org/abs/1510.00149.\n\n\n\n\n\n\nFigura 8.2: Neural Network Pruning.\n\n\n\nQuantizzazione: La quantizzazione è il processo di limitazione di un input da un set ampio a un output in un set più piccolo, principalmente nel deep learning; ciò significa ridurre il numero di bit che rappresentano i pesi e i bias del modello. Ad esempio, l’utilizzo di rappresentazioni a 16 o 8 bit anziché a 32 bit può ridurre la dimensione del modello e velocizzare i calcoli, con un piccolo compromesso in termini di accuratezza. Esploreremo questi aspetti più in dettaglio in Sezione 9.3.4. Figura 8.3 mostra un esempio di quantizzazione mediante arrotondamento al numero più vicino. La conversione da virgola mobile a 32 bit a 16 bit riduce l’utilizzo della memoria del 50%. Passare da un intero a 32 bit a uno a 8 bit riduce l’utilizzo della memoria del 75%. Mentre la perdita di precisione numerica e, di conseguenza, di prestazioni del modello è minima, l’efficienza nell’utilizzo della memoria è significativa.\n\n\n\n\n\n\nFigura 8.3: Diverse forme di quantizzazione.\n\n\n\nKnowledge Distillation: La “distillazione della conoscenza” comporta l’addestramento di un modello più piccolo (studente) per replicare il comportamento di un modello più grande (insegnante). L’idea è quella di trasferire la conoscenza dal modello ingombrante a quello leggero. Quindi, il modello più piccolo raggiunge prestazioni vicine alla sua controparte più grande ma con parametri significativamente inferiori. Esploreremo la “distillazione della conoscenza” in modo più dettagliato in Sezione 9.2.2.1.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#hardware-di-inferenza-efficiente",
    "href": "contents/efficient_ai/efficient_ai.it.html#hardware-di-inferenza-efficiente",
    "title": "8  IA Efficiente",
    "section": "8.5 Hardware di Inferenza Efficiente",
    "text": "8.5 Hardware di Inferenza Efficiente\nNel capitolo Training, abbiamo discusso il processo di training dei modelli di intelligenza artificiale. Ora, dal punto di vista dell’efficienza, è importante notare che il training è un’attività che richiede molte risorse e molto tempo, spesso richiede hardware potente e impiega da ore a settimane per essere completato. L’inferenza, d’altra parte, deve essere il più veloce possibile, soprattutto nelle applicazioni in tempo reale. È qui che entra in gioco un hardware di inferenza efficiente. Ottimizzando l’hardware specificamente per le attività di inferenza, possiamo ottenere tempi di risposta rapidi e un funzionamento efficiente dal punto di vista energetico, il che è particolarmente cruciale per i dispositivi edge e i sistemi embedded.\nTPU (Tensor Processing Unit): Le TPU sono ASIC (Application-Specific Integrated Circuits) personalizzati da Google per accelerare i carichi di lavoro di apprendimento automatico (Jouppi et al. 2017). Sono ottimizzate per le operazioni tensoriali, offrono un throughput elevato per l’aritmetica a bassa precisione e sono progettate specificamente per il machine learning delle reti neurali. Le TPU accelerano significativamente l’addestramento e l’inferenza del modello rispetto alle GPU/CPU generiche. Questo potenziamento si traduce in un addestramento più rapido dei modelli e in capacità di inferenza in tempo reale o quasi reale, fondamentali per applicazioni come la ricerca vocale e la realtà aumentata.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\nLe Edge TPU sono una versione più piccola e a basso consumo delle TPU di Google, studiate appositamente per i dispositivi edge. Forniscono un’inferenza ML veloce sul dispositivo per i modelli TensorFlow Lite. Le Edge TPU consentono un’inferenza a bassa latenza e ad alta efficienza su dispositivi edge come smartphone, dispositivi IoT e sistemi embedded. Le capacità di IA possono essere implementate in applicazioni in tempo reale senza comunicare con un server centrale, risparmiando così larghezza di banda e riducendo la latenza. Si consideri la tabella in Figura 8.4. Mostra le differenze di prestazioni tra l’esecuzione di modelli diversi su CPU rispetto a un acceleratore Coral USB. L’acceleratore Coral USB è un accessorio della piattaforma Coral AI di Google che consente agli sviluppatori di collegare le Edge TPU ai computer Linux. L’esecuzione dell’inferenza sulle Edge TPU è stata da 70 a 100 volte più veloce rispetto alle CPU.\n\n\n\n\n\n\nFigura 8.4: Confronto delle prestazioni tra acceleratore e CPU in diverse configurazioni hardware. Desktop CPU: 64-bit Intel(R) Xeon(R) E5–1650 v4 @ 3.60GHz. Embedded CPU: Quad-core Cortex-A53 @ 1.5GHz, †Dev Board: Quad-core Cortex-A53 @ 1.5GHz + Edge TPU. Fonte: TensorFlow Blog.\n\n\n\nAcceleratori NN (Neural Network): Gli acceleratori di reti neurali a funzione fissa sono acceleratori hardware progettati esplicitamente per i calcoli di reti neurali. Possono essere chip standalone o far parte di una soluzione di system-on-chip (SoC) più ampia. Ottimizzando l’hardware per le operazioni specifiche richieste dalle reti neurali, come moltiplicazioni di matrici e convoluzioni, gli acceleratori NN possono ottenere tempi di inferenza più rapidi e consumi energetici inferiori rispetto alle CPU e alle GPU per uso generico. Sono particolarmente utili nei dispositivi TinyML con vincoli di potenza o termici, come smartwatch, micro-droni o robotica.\nMa questi sono solo gli esempi più comuni. Stanno emergendo diversi altri tipi di hardware che hanno il potenziale per offrire vantaggi significativi per l’inferenza. Questi includono, ma non solo, hardware neuromorfico, elaborazione fotonica, ecc. In Sezione 10.3, esploreremo questi aspetti in modo più dettagliato.\nUn hardware efficiente per l’inferenza velocizza il processo, risparmia energia, prolunga la durata della batteria e può funzionare in condizioni di tempo reale. Man mano che l’intelligenza artificiale viene integrata in innumerevoli applicazioni, dalle telecamere intelligenti agli assistenti vocali, il ruolo dell’hardware ottimizzato diventerà sempre più importante. Sfruttando questi componenti hardware specializzati, sviluppatori e ingegneri possono portare la potenza dell’intelligenza artificiale a dispositivi e situazioni che prima erano impensabili.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-numerics",
    "href": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-numerics",
    "title": "8  IA Efficiente",
    "section": "8.6 Matematica Efficiente",
    "text": "8.6 Matematica Efficiente\nL’apprendimento automatico, e in particolare il deep learning, comporta enormi quantità di elaborazione. I modelli possono avere milioni o miliardi di parametri, spesso addestrati su vasti set di dati. Ogni operazione, ogni moltiplicazione o addizione, richiede risorse di elaborazione. Pertanto, la precisione dei numeri utilizzati in queste operazioni può avere un impatto significativo sulla velocità di elaborazione, sul consumo di energia e sui requisiti di memoria. È qui che entra in gioco il concetto di numeri efficienti.\n\n8.6.1 Formati Numerici\nEsistono molti tipi diversi di numeri. I numeri hanno una lunga storia nei sistemi di elaborazione.\nFloating point: Noto come “virgola mobile” a precisione singola, FP32 utilizza 32 bit per rappresentare un numero, incorporandone segno, esponente e mantissa. Comprendere come i numeri in virgola mobile sono rappresentati in modo approfondito è fondamentale per comprendere le varie ottimizzazioni possibili nei calcoli numerici. Il bit del segno determina se il numero è positivo o negativo, l’esponente controlla l’intervallo di valori che possono essere rappresentati e la mantissa determina la precisione del numero. La combinazione di questi componenti consente ai numeri in virgola mobile di rappresentare un’ampia gamma di valori con vari gradi di precisione.\nVideo 8.1 fornisce una panoramica completa di questi tre componenti principali, segno, esponente e mantissa, e di come funzionano insieme per rappresentare i numeri in virgola mobile.\n\n\n\n\n\n\nVideo 8.1: Numeri in Virgola Mobile\n\n\n\n\n\n\nFP32 è ampiamente adottato in molti framework di deep learning e bilancia accuratezza e requisiti computazionali. È prevalente nella fase di training per molte reti neurali grazie alla sua sufficiente precisione nel catturare dettagli minuti durante gli aggiornamenti dei pesi. Noto anche come virgola mobile a mezza precisione, FP16 utilizza 16 bit per rappresentare un numero, inclusi il segno, l’esponente e la frazione. Offre un buon equilibrio tra precisione e risparmio di memoria. FP16 è particolarmente popolare nella training di deep learning su GPU che supportano l’aritmetica a precisione mista, combinando i vantaggi di velocità di FP16 con la precisione di FP32 quando necessario.\nDiversi altri formati numerici rientrano in una classe esotica. Un esempio esotico è BF16 o Brain Floating Point. È un formato numerico a 16 bit progettato esplicitamente per applicazioni di deep learning. È un compromesso tra FP32 e FP16, che mantiene l’esponente a 8 bit di FP32 riducendo la mantissa a 7 bit (rispetto alla mantissa a 23 bit di FP32). Questa struttura dà priorità al range rispetto alla precisione. BF16 ha ottenuto risultati di training paragonabili in accuratezza a FP32, utilizzando significativamente meno memoria e risorse computazionali (Kalamkar et al. 2019). Ciò lo rende adatto non solo per l’inferenza, ma anche per il training di reti neurali profonde.\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019. «A Study of BFLOAT16 for Deep Learning Training». https://arxiv.org/abs/1905.12322.\nMantenendo l’esponente a 8 bit di FP32, BF16 offre un range simile, che è fondamentale per le attività di deep learning in cui determinate operazioni possono generare numeri molto grandi o molto piccoli. Allo stesso tempo, troncando la precisione, BF16 consente requisiti di memoria e computazionali ridotti rispetto a FP32. BF16 è emerso come una promettente via di mezzo nel panorama dei formati numerici per il deep learning, fornendo un’alternativa efficiente ed efficace ai formati FP32 e FP16 più tradizionali.\nFigura 8.5 mostra tre diversi formati in virgola mobile: Float32, Float16 e BFloat16.\n\n\n\n\n\n\nFigura 8.5: Tre formati a virgola mobile.\n\n\n\nIntero: Si tratta di rappresentazioni di numeri interi che utilizzano 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocità e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attività di inferenza, in particolare sui dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione è spesso accettabile, dati i guadagni di efficienza. Una forma estrema di numeri interi è per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno dei due valori: +1 o -1.\nLarghezze di bit variabili: Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezze di bit estremamente basse possono offrire accelerazioni significative e ridurre ulteriormente il consumo di energia. Sebbene permangano dei problemi nel mantenere l’accuratezza del modello con una quantizzazione così drastica, si continuano a fare progressi in quest’area.\nL’efficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano più pervasivi, soprattutto in ambienti reali con risorse limitate, l’attenzione su una numerica efficiente continuerà a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, è possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocità, memoria ed energia. Tabella 8.1 riassume questi compromessi.\n\n\n\nTabella 8.1: Confronto dei livelli di precisione nel deep learning.\n\n\n\n\n\n\n\n\n\n\nPrecisione\nPro\nContro\n\n\n\n\nFP32 (virgola mobile a 32 bit)\n\nPrecisione standard utilizzata nella maggior parte dei framework di deep learning.\nElevata accuratezza grazie all’ampia capacità di rappresentazione.\nAdatto per il training\n\n\nElevato utilizzo di memoria.\nTempi di inferenza più lenti rispetto ai modelli quantizzati.\nMaggiore consumo energetico.\n\n\n\nFP16 (virgola mobile a 16 bit)\n\nRiduce l’utilizzo di memoria rispetto a FP32.\nVelocizza i calcoli su hardware che supporta FP16.\nSpesso utilizzato nel training a precisione mista per bilanciare velocità e accuratezza.\n\n\nMinore capacità di rappresentazione rispetto a FP32.\nRischio di instabilità numerica in alcuni modelli o livelli.\n\n\n\nINT8 (intero a 8 bit)\n\nImpronta di memoria notevolmente ridotta rispetto alle rappresentazioni in virgola mobile.\nInferenza più rapida se l’hardware supporta i calcoli INT8.\nAdatto a molti scenari di quantizzazione post-training.\n\n\nLa quantizzazione può comportare una certa perdita di accuratezza.\nRichiede una calibrazione attenta durante la quantizzazione per ridurre al minimo il degrado della precisione.\n\n\n\nINT4 (intero a 4 bit)\n\nUtilizzo di memoria ancora inferiore rispetto a INT8.\nUlteriore potenziale di accelerazione per l’inferenza.\n\n\nRischio di perdita di precisione più elevato rispetto a INT8.\nLa calibrazione durante la quantizzazione diventa più critica.\n\n\n\nBinario\n\nIngombro di memoria minimo (solo 1 bit per parametro).\nInferenza estremamente rapida grazie alle operazioni bit a bit.\nEfficienza energetica.\n\n\nCalo significativo della precisione per molte attività.\nDinamiche di training complesse grazie alla quantizzazione estrema.\n\n\n\nTernario\n\nBasso utilizzo di memoria ma leggermente superiore a quello binario.\nOffre una via di mezzo tra rappresentazione ed efficienza.\n\n\nL’accuratezza potrebbe essere ancora inferiore a quella dei modelli di precisione più elevata.\nLe dinamiche di addestramento possono essere complesse.\n\n\n\n\n\n\n\n\n\n8.6.2 Vantaggi dell’Efficienza\nL’efficienza numerica è importante per i carichi di lavoro di machine learning per diversi motivi:\nEfficienza Computazionale: I calcoli ad alta precisione (come FP32 o FP64) possono essere lenti e richiedere molte risorse. Ridurre la precisione numerica può ottenere tempi di calcolo più rapidi, specialmente su hardware specializzato che supporta una precisione inferiore.\nEfficienza della Memoria: I requisiti di archiviazione diminuiscono con una precisione numerica ridotta. Ad esempio, FP16 richiede metà della memoria di FP32. Ciò è fondamentale quando si distribuiscono modelli su dispositivi edge con memoria limitata o si lavora con modelli di grandi dimensioni.\nEfficienza Energetica: I calcoli a precisione inferiore spesso consumano meno energia, il che è particolarmente importante per i dispositivi alimentati a batteria.\nIntroduzione del Rumore: È interessante notare che il rumore introdotto utilizzando una precisione inferiore può talvolta fungere da regolarizzatore, contribuendo a prevenire l’overfitting in alcuni modelli.\nAccelerazione Hardware: Molti acceleratori di IA e GPU moderni sono ottimizzati per operazioni di precisione inferiore, sfruttando i vantaggi dell’efficienza di tali numeri.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#valutazione-dei-modelli",
    "href": "contents/efficient_ai/efficient_ai.it.html#valutazione-dei-modelli",
    "title": "8  IA Efficiente",
    "section": "8.7 Valutazione dei Modelli",
    "text": "8.7 Valutazione dei Modelli\nVale la pena notare che i vantaggi e i compromessi effettivi possono variare in base all’architettura specifica della rete neurale, al set di dati, all’attività e all’hardware utilizzato. Prima di decidere una precisione numerica, è consigliabile eseguire esperimenti per valutare l’impatto sull’applicazione desiderata.\n\n8.7.1 Metriche di Efficienza\nUna profonda comprensione dei metodi di valutazione dei modelli è importante per guidare questo processo in modo sistematico. Quando si valuta l’efficacia e l’idoneità dei modelli di intelligenza artificiale per varie applicazioni, le metriche di efficienza vengono in primo piano.\nI FLOP (Floating Point Operations), introdotti in Training, misurano le esigenze computazionali di un modello. Ad esempio, una moderna rete neurale come BERT ha miliardi di FLOP, che potrebbero essere gestibili su un potente server cloud ma sarebbero gravosi su uno smartphone. FLOP più elevati possono portare a tempi di inferenza più prolungati e a un notevole consumo di energia, soprattutto su dispositivi senza acceleratori hardware specializzati. Quindi, per applicazioni in tempo reale come lo streaming video o i giochi, potrebbero essere più desiderabili modelli con FLOP più bassi.\nL’Utilizzo della Memoria riguarda la quantità di spazio di archiviazione richiesta dal modello, che influisce sia sullo spazio di archiviazione del dispositivo che sulla RAM. Si prenda in considerazione l’implementazione di un modello su uno smartphone: un modello che occupa diversi gigabyte di spazio non solo consuma prezioso spazio di archiviazione, ma potrebbe anche essere più lento a causa della necessità di caricare grandi pesi nella memoria. Ciò diventa particolarmente cruciale per dispositivi edge come telecamere di sicurezza o droni, dove impronte di memoria minime sono vitali per l’archiviazione e l’elaborazione rapida dei dati.\nIl Consumo Energetico diventa particolarmente cruciale per i dispositivi che si basano sulle batterie. Ad esempio, un monitor sanitario indossabile che utilizza un modello ad alto consumo energetico potrebbe esaurire la batteria in poche ore, rendendolo poco pratico per il monitoraggio continuo. L’ottimizzazione dei modelli per un basso consumo energetico diventa essenziale mentre ci muoviamo verso un’era dominata dai dispositivi IoT, dove molti dispositivi funzionano a batteria.\nIl Tempo di Inferenza riguarda la rapidità con cui un modello può produrre risultati. In applicazioni come la guida autonoma, dove decisioni in frazioni di secondo fanno la differenza tra sicurezza e calamità, i modelli devono funzionare rapidamente. Se il modello di un’auto a guida autonoma impiega anche solo pochi secondi in più per riconoscere un ostacolo, le conseguenze potrebbero essere disastrose. Quindi, garantire che il tempo di inferenza di un modello sia allineato con le richieste in tempo reale della sua applicazione è fondamentale.\nIn sostanza, queste metriche di efficienza sono più che dei numeri che stabiliscono dove e come un modello può essere distribuito in modo efficace. Un modello potrebbe vantare un’elevata accuratezza, ma se i suoi FLOP, l’utilizzo della memoria, il consumo energetico o il tempo di inferenza lo rendono inadatto alla piattaforma prevista o agli scenari del mondo reale, la sua utilità pratica diventa limitata.\n\n\n8.7.2 Confronti di Efficienza\nIl panorama dei modelli di machine learning è vasto, con ogni modello che offre un set unico di punti di forza e considerazioni di implementazione. Sebbene le cifre di accuratezza grezza o le velocità di training e inferenza possano essere parametri di riferimento allettanti, forniscono un quadro incompleto. Un’analisi comparativa più approfondita rivela diversi fattori critici che influenzano l’idoneità di un modello per le applicazioni TinyML. Spesso, incontriamo il delicato equilibrio tra accuratezza ed efficienza. Ad esempio, mentre un modello di deep learning e denso e una variante MobileNet leggera potrebbero eccellere nella classificazione delle immagini, le loro richieste di calcolo potrebbero essere ad estremi opposti. Questa differenziazione è particolarmente pronunciata quando si confrontano le distribuzioni su server cloud con risorse abbondanti rispetto ai limitati dispositivi TinyML. In molti scenari del mondo reale, i guadagni marginali in termini di accuratezza potrebbero essere oscurati dalle inefficienze di un modello ad alta intensità di risorse richieste.\nInoltre, la scelta del modello ottimale non è sempre universale, ma spesso dipende dalle specifiche di un’applicazione. Ad esempio, un modello che eccelle in scenari di rilevamento di oggetti generali potrebbe avere difficoltà in ambienti di nicchia, come il rilevamento di difetti di fabbricazione in una fabbrica. Questa adattabilità, o la sua mancanza, può influenzare l’utilità reale di un modello.\nUn’altra considerazione importante è la relazione tra la complessità del modello e i suoi vantaggi pratici. Prendiamo gli assistenti attivati tramite comando vocale, come “Alexa” o “OK Google”. Mentre un modello complesso potrebbe dimostrare una comprensione marginalmente superiore del parlato dell’utente se è più lento a rispondere rispetto a una controparte più semplice, l’esperienza utente potrebbe essere compromessa. Pertanto, l’aggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.\nUn’altra considerazione importante è la relazione tra la complessità del modello e i suoi vantaggi pratici. Prendiamo gli assistenti vocali come “Alexa” o “OK Google”. Mentre un modello complesso potrebbe dimostrare una comprensione leggermente superiore del parlato dell’utente se è più lento a rispondere rispetto a una controparte più semplice, l’esperienza utente potrebbe essere compromessa. Pertanto, l’aggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.\nInoltre, mentre i set di dati di riferimento, come ImageNet (Russakovsky et al. 2015), COCO (Lin et al. 2014), Visual Wake Words (Wang e Zhan 2019), Google Speech Commands (Warden 2018), ecc. forniscono una metrica di prestazioni standardizzata, potrebbero non catturare la diversità e l’imprevedibilità dei dati del mondo reale. Due modelli di riconoscimento facciale con punteggi di riferimento simili potrebbero mostrare competenze diverse quando si trovano di fronte a background etnici diversi o condizioni di illuminazione difficili. Tali disparità sottolineano l’importanza di robustezza e coerenza tra dati diversi. Ad esempio, Figura 8.6 dal set di dati Dollar Street mostra immagini di stufe su redditi mensili estremi. Le stufe hanno forme e livelli tecnologici diversi in diverse regioni e livelli di reddito. Un modello che non è addestrato su set di dati diversi potrebbe funzionare bene su un benchmark ma fallire nelle applicazioni del mondo reale. Quindi, se un modello fosse addestrato solo su immagini di stufe trovate nei paesi ricchi, non riuscirebbe a riconoscere le stufe delle regioni più povere.\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. «ImageNet Large Scale Visual Recognition Challenge». Int. J. Comput. Vision 115 (3): 211–52. https://doi.org/10.1007/s11263-015-0816-y.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, e C Lawrence Zitnick. 2014. «Microsoft coco: Common objects in context». In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740–55. Springer.\n\nWang, LingFeng, e YaQing Zhan. 2019. «A conceptual peer review model for arXiv and other preprint databases». Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\nWarden, Pete. 2018. «Speech commands: A dataset for limited-vocabulary speech recognition». arXiv preprint arXiv:1804.03209.\n\n\n\n\n\n\nFigura 8.6: Diversi tipi di stufe. Fonte: Immagini di stufe di Dollar Street.\n\n\n\nIn sostanza, un’analisi comparativa approfondita trascende le metriche numeriche. È una valutazione olistica intrecciata con applicazioni del mondo reale, costi e le intricate sottigliezze che ogni modello porta con sé. Ecco perché avere parametri di riferimento e metriche standard ampiamente stabiliti e adottati dalla comunità diventa importante.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#conclusione",
    "href": "contents/efficient_ai/efficient_ai.it.html#conclusione",
    "title": "8  IA Efficiente",
    "section": "8.8 Conclusione",
    "text": "8.8 Conclusione\nL’intelligenza artificiale efficiente è fondamentale mentre ci spingiamo verso un’implementazione più ampia e diversificata del machine learning nel mondo reale. Questo capitolo ha fornito una panoramica, esplorando le varie metodologie e considerazioni alla base del raggiungimento di un’intelligenza artificiale efficiente, a partire dall’esigenza fondamentale, dalle somiglianze e dalle differenze tra i sistemi cloud, Edge e TinyML.\nAbbiamo esaminato le architetture dei modelli efficienti e la loro utilità per l’ottimizzazione. Le tecniche di compressione dei modelli come pruning, quantizzazione e distillazione della conoscenza esistono per aiutare a ridurre le richieste di calcolo e l’ingombro della memoria senza influire in modo significativo sulla precisione. Hardware specializzati come TPU e acceleratori NN offrono chip ottimizzati per le operazioni di rete neurale e il flusso di dati. I numeri efficienti bilanciano precisione ed efficienza, consentendo ai modelli di ottenere prestazioni robuste utilizzando risorse minime. Esploreremo questi argomenti in modo approfondito e dettagliato nei capitoli successivi.\nInsieme, questi formano un quadro olistico per un’intelligenza artificiale efficiente. Ma il viaggio non finisce qui. Il raggiungimento di un’intelligenza efficiente in modo ottimale richiede ricerca e innovazione continue. Man mano che i modelli diventano più sofisticati, i set di dati crescono e le applicazioni si diversificano in domini specializzati, l’efficienza deve evolversi di pari passo. La misura dell’impatto nel mondo reale richiede parametri di riferimento adatti e metriche standardizzate che vadano oltre le semplicistiche cifre dell’accuratezza.\nInoltre, l’intelligenza artificiale efficiente si espande oltre l’ottimizzazione tecnologica e comprende costi, impatto ambientale e considerazioni etiche per il bene della società in senso più ampio. Man mano che l’intelligenza artificiale permea i settori e la vita quotidiana, una prospettiva completa sull’efficienza sostiene il suo progresso sostenibile e responsabile. I capitoli successivi si baseranno su questi concetti fondamentali, fornendo approfondimenti concreti e norme pratiche per lo sviluppo e l’implementazione di soluzioni di intelligenza artificiale efficienti.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-ai-resource",
    "href": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-ai-resource",
    "title": "8  IA Efficiente",
    "section": "8.9 Risorse",
    "text": "8.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nDeploying on Edge Devices: challenges and techniques.\nModel Evaluation.\nContinuous Evaluation Challenges for TinyML.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html",
    "href": "contents/optimizations/optimizations.it.html",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "",
    "text": "9.1 Introduzione\nAbbiamo strutturato questo capitolo in tre livelli. Innanzitutto, in Sezione 9.2 esaminiamo l’importanza e le metodologie per ridurre la complessità dei parametri dei modelli senza compromettere le loro capacità di inferenza. Vengono discusse tecniche come il “pruning” [potatura] e la distillazione della conoscenza, offrendo spunti su come i modelli possono essere compressi e semplificati mantenendo, o addirittura migliorando, le loro prestazioni.\nScendendo di un livello, in Sezione 9.3, studiamo il ruolo della precisione numerica nei calcoli dei modelli e come la sua modifica influisce sulle sue dimensioni, velocità e precisione. Esamineremo i vari formati numerici e come l’aritmetica a precisione ridotta può essere sfruttata per ottimizzare i modelli per la distribuzione embedded.\nInfine, man mano che scendiamo più in basso e ci avviciniamo all’hardware, in Sezione 9.4, esploreremo il panorama della progettazione congiunta hardware-software, esplorando come i modelli possono essere ottimizzati adattandoli alle caratteristiche e alle capacità specifiche dell’hardware target. Discuteremo di come i modelli possono essere adattati per sfruttare efficacemente le risorse hardware disponibili.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#introduzione",
    "href": "contents/optimizations/optimizations.it.html#introduzione",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "",
    "text": "Figura 9.1: Tre livelli da coprire.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#sec-model_ops_representation",
    "href": "contents/optimizations/optimizations.it.html#sec-model_ops_representation",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.2 Rappresentazione Efficiente del Modello",
    "text": "9.2 Rappresentazione Efficiente del Modello\nIl primo passo per l’ottimizzazione del modello inizia in un territorio familiare per la maggior parte dei professionisti del ML: la rappresentazione efficiente del modello viene spesso affrontata per la prima volta al livello più alto di astrazione della parametrizzazione, ovvero l’architettura stessa del modello.\nLa maggior parte dei professionisti del ML tradizionali progetta modelli con un obiettivo generale di alto livello in mente, che si tratti di classificazione delle immagini, rilevamento di persone o individuazione di parole chiave come menzionato in precedenza in questo testo. I loro progetti in genere finiscono per adattarsi naturalmente ad alcuni vincoli soft dovuti a risorse di elaborazione limitate durante lo sviluppo, ma in genere questi progetti non sono a conoscenza di vincoli successivi, come quelli richiesti se il modello deve essere distribuito su un dispositivo più limitato anziché sul cloud.\nIn questa sezione, discuteremo di come i professionisti possono sfruttare i principi della progettazione congiunta hardware-software anche nell’architettura di alto livello di un modello per rendere i loro modelli compatibili con i dispositivi edge. Da quelli più consapevoli dell’hardware a quelli meno consapevoli a questo livello di modifica, discutiamo alcune delle strategie più comuni per una parametrizzazione efficiente del modello: pruning, compressione e architetture edge-friendly. Abbiamo già parlato di pruning e compressione del modello in Sezione 8.4; questa sezione andrà oltre le definizioni per fornire una comprensione tecnica del loro funzionamento.\n\n9.2.1 Il Pruning\n\nPanoramica\nIl model pruning [potatura] è una tecnica di apprendimento automatico che riduce le dimensioni e la complessità di un modello di rete neurale, mantenendone il più possibile le capacità predittive. L’obiettivo della potatura è quello di rimuovere componenti ridondanti o non essenziali del modello, tra cui connessioni tra neuroni, singoli neuroni o persino interi layer della rete.\nQuesto processo in genere comporta l’analisi del modello di machine learning per identificare e rimuovere pesi, nodi o layer che hanno scarso impatto sugli output del modello. Potando selettivamente un modello in questo modo, il numero totale di parametri può essere ridotto in modo significativo senza cali sostanziali nell’accuratezza del modello. Il modello compresso risultante richiede meno memoria e risorse di calcolo per l’addestramento e l’esecuzione, consentendo tempi di inferenza più rapidi.\nIl pruning del modello è particolarmente utile quando si distribuiscono modelli di apprendimento automatico su dispositivi con risorse di calcolo limitate, come telefoni cellulari o sistemi TinyML. La tecnica facilita la distribuzione di modelli più grandi e complessi su questi dispositivi riducendo le loro richieste di risorse. Inoltre, i modelli più piccoli richiedono meno dati per generalizzare bene e sono meno inclini all’overfitting [sovradattamento]. Fornendo un modo efficiente per semplificare i modelli, la potatura dei modelli è diventata una tecnica fondamentale per ottimizzare le reti neurali nell’apprendimento automatico.\nEsistono diverse tecniche di potatura comuni utilizzate nell’apprendimento automatico, tra cui la potatura strutturata, la potatura non strutturata, la potatura iterativa, la potatura bayesiana e persino la potatura casuale. Oltre a potare i pesi, si possono anche potare le attivazioni. La potatura di attivazioni prende di mira specificamente neuroni o filtri che si attivano raramente o hanno un’attivazione complessivamente bassa. Esistono numerosi altri metodi, come la potatura di sensibilità e movimento. Per un elenco completo dei metodi, si consiglia al lettore di leggere il seguente articolo: “A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations” (2023).\nQuindi, come si scelgono i metodi di potatura? Esistono molte varianti di tecniche di potatura, ciascuna delle quali varia l’euristica di ciò che dovrebbe essere mantenuto e potato dal modello, nonché il numero di volte in cui cui deve essere eseguita. Tradizionalmente, la potatura avviene dopo che il modello è completamente addestrato, dove il modello potato può subire una lieve perdita di accuratezza. Tuttavia, come discuteremo più avanti, recenti scoperte hanno trovato che la potatura può essere utilizzata durante l’addestramento (ad esempio, in modo iterativo) per identificare rappresentazioni del modello più efficienti e accurate.\n\n\nPotatura Strutturata\nIniziamo con la “potatura strutturata”, una tecnica che riduce le dimensioni di una rete neurale eliminando intere sotto-strutture specifiche del modello mantenendone la struttura generale. Rimuove interi neuroni/canali o layer in base a criteri di importanza. Ad esempio, per una rete neurale convoluzionale (CNN), potrebbero essere determinate istanze di filtro o canali. Per reti completamente connesse, potrebbero essere i neuroni stessi mantenendo la piena connettività o persino l’eliminazione di interi layer del modello che sono considerati insignificanti. Questo tipo di potatura spesso porta a reti sparse regolari e strutturate che sono compatibili con l’hardware.\nSono iniziate a emergere le “best practice” su come pensare alla potatura strutturata. Ci sono tre componenti principali:\n\n1. Strutture Candidate per il Pruning\nData la varietà di approcci, diverse strutture all’interno di una rete neurale vengono potate in base a criteri specifici. Le strutture primarie per la potatura includono neuroni, canali e talvolta interi layer, ognuno con le sue implicazioni e metodologie uniche. L’obiettivo di ogni approccio è garantire che il modello ridotto mantenga il più possibile la capacità predittiva del modello originale, migliorando al contempo l’efficienza computazionale e riducendo le dimensioni.\nQuando i neuroni vengono potati, rimuoviamo interi neuroni insieme ai loro pesi e bias associati, riducendo così la larghezza del layer. Questo tipo di potatura viene spesso utilizzato in layer completamente connessi.\nLa potatura del canale, che viene applicata prevalentemente nelle reti neurali convoluzionali (CNN), comporta l’eliminazione di interi canali o filtri, il che a sua volta riduce la profondità delle mappe delle feature e influisce sulla capacità della rete di estrarre determinate feature dai dati di input. Ciò è particolarmente cruciale nelle attività di elaborazione delle immagini in cui l’efficienza computazionale è fondamentale.\nInfine, la potatura dei layer adotta un approccio più aggressivo rimuovendo interi layer della rete. Ciò riduce significativamente la profondità della rete e quindi la sua capacità di plasmare pattern e gerarchie complesse nei dati. Questo approccio richiede un attento equilibrio per garantire che la capacità predittiva del modello non venga indebitamente compromessa.\nFigura 9.2 mostra la differenza tra la potatura di canale/filtro e quella del layer. Quando potiamo un canale, dobbiamo riconfigurare l’architettura del modello per adattarla ai cambiamenti strutturali. Una modifica consiste nel cambiare il numero di canali di input nel layer successivo (qui, il terzo e il layer più profondo): modificando le profondità dei filtri applicati al layer con il canale potato. D’altra parte, la potatura di un intero layer (rimuovendo tutti i canali nel layer) richiede modifiche più drastiche. Quella principale riguarda la modifica delle connessioni tra i layer rimanenti per sostituire o bypassare il layer potato. Nel nostro caso, riconfiguriamo per connettere il primo e l’ultimo layer. In tutti i casi di potatura, dobbiamo mettere a punto la nuova struttura per regolare i pesi.\n\n\n\n\n\n\nFigura 9.2: Potatura del canale e quella del layer.\n\n\n\n\n\n2. Stabilire un criterio per il Pruning\nStabilire criteri ben definiti per determinare quali strutture specifiche potare da un modello di rete neurale è una componente cruciale del processo di “pruning” del modello. L’obiettivo principale qui è identificare e rimuovere i componenti che contribuiscono meno alle capacità predittive del modello, mantenendo al contempo le strutture integrali per preservare l’accuratezza.\nUna strategia ampiamente adottata ed efficace per potare sistematicamente le strutture si basa sul calcolo di punteggi di importanza per singoli componenti come neuroni, filtri, canali o layer. Questi punteggi servono come metriche quantitative per valutare la significatività di ciascuna struttura e il suo effetto sull’output del modello.\nEsistono diverse tecniche per assegnare questi punteggi sull’importanza:\n\nPruning Basato sulla Magnitudo del peso: Questo approccio assegna punteggi di importanza a una struttura valutando la magnitudo aggregata dei pesi associati. Le strutture con magnitudo del peso complessivo inferiore sono considerate meno critiche per le prestazioni della rete.\nPruning Basato sul Bradiente: Questa tecnica utilizza i gradienti della funzione di los [perdita] rispetto ai pesi associati a una struttura. Le strutture con magnitudo del gradiente cumulativo basso, che indica un impatto minimo sulla perdita quando alterato, sono le candidate principali per la potatura.\nPruning Basato sull’Attivazione: Questo metodo tiene traccia della frequenza con cui un neurone o un filtro viene attivato memorizzando queste informazioni in un parametro chiamato contatore delle attivazioni. Ogni volta che la struttura viene attivata, il contatore viene incrementato. Un conteggio di attivazione basso suggerisce che la struttura è meno rilevante.\nPruning Basato sull’Espansione di Taylor: Questo approccio approssima la modifica nella funzione di perdita derivante dalla rimozione di un dato peso. Valutando la perturbazione della perdita cumulativa derivante dalla rimozione di tutti i pesi associati a una struttura, è possibile identificare le strutture con un impatto trascurabile sulla perdita, rendendole candidate idonee per la potatura.\n\nL’idea è di misurare, direttamente o indirettamente, il contributo di ogni componente all’output del modello. Le strutture con un’influenza minima in base ai criteri definiti vengono potate per prime. Ciò consente una potatura selettiva e ottimizzata che comprime al massimo i modelli preservando al contempo la capacità predittiva. In generale, è importante valutare l’impatto della rimozione di particolari strutture sull’output del modello, con lavori recenti come (Rachwan et al. 2022) e (Lubana e Dick 2020) che studiano combinazioni di tecniche come la potatura basata sulla magnitudine e la potatura basata sul gradiente.\n\nRachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, e Stephan Günnemann. 2022. «Winning the lottery ahead of time: Efficient early network pruning». In International Conference on Machine Learning, 18293–309. PMLR.\n\nLubana, Ekdeep Singh, e Robert P Dick. 2020. «A gradient flow framework for analyzing network pruning». arXiv preprint arXiv:2009.11839.\n\n\n3. Selezione di una strategia di potatura\nOra che abbiamo capito alcune tecniche per determinare l’importanza delle strutture all’interno di una rete neurale, il passo successivo è decidere come applicare queste intuizioni. Ciò comporta la selezione di una strategia di potatura appropriata, che stabilisce come e quando le strutture identificate vengono rimosse e come il modello viene messo a punto per mantenere le sue prestazioni. Esistono due principali strategie di potatura strutturata: quella iterativa e la one-shot.\nLa potatura iterativa rimuove gradualmente le strutture attraverso più cicli di potatura seguiti da messa a punto. In ogni ciclo, un piccolo set di strutture viene potato in base a criteri di importanza. Il modello viene poi messo a punto, consentendogli di adattarsi senza problemi ai cambiamenti strutturali prima della successiva iterazione di potatura. Questo approccio graduale e ciclico impedisce bruschi cali di accuratezza. Consente al modello di adattarsi lentamente man mano che le strutture vengono ridotte attraverso le iterazioni.\nConsideriamo una situazione in cui desideriamo potare i 6 canali meno efficaci (in base ad alcuni criteri specifici) da una rete neurale convoluzionale. In Figura 9.3, mostriamo un processo di potatura semplificato eseguito su 3 iterazioni. In ogni iterazione, eliminiamo solo 2 canali. La rimozione dei canali comporta un degrado della precisione. Nella prima iterazione, la precisione scende da 0.995 a 0.971. Tuttavia, dopo aver perfezionato il modello sulla nuova struttura, siamo in grado di recuperare dalla perdita di prestazioni, portando la precisione a 0.992. Poiché i cambiamenti strutturali sono minori e graduali, la rete può adattarsi più facilmente a essi. Eseguendo lo stesso processo altre 2 volte, finiamo con una precisione finale di 0.991 (una perdita di solo lo 0.4% rispetto all’originale) e una riduzione del 27% nel numero di canali. Pertanto, la potatura iterativa ci consente di mantenere le prestazioni beneficiando di una maggiore efficienza computazionale dovuta alla riduzione delle dimensioni del modello.\n\n\n\n\n\n\nFigura 9.3: Potatura iterativa.\n\n\n\nLa potatura one-shot adotta un approccio più aggressivo, potando una grande porzione di strutture simultaneamente in un’unica operazione in base a criteri di importanza predefiniti. Segue un’ampia messa a punto per recuperare l’accuratezza del modello. Sebbene più rapida, questa strategia aggressiva può degradare l’accuratezza se il modello non riesce a recuperare durante la messa a punto.\nLa scelta tra queste strategie comporta la valutazione di fattori quali dimensioni del modello, quanto è sparso il target, calcolo disponibile e perdite di accuratezza accettabili. La potatura one-shot può comprimere rapidamente i modelli, ma quella iterativa può consentire una migliore conservazione dell’accuratezza per un livello target di potatura. In pratica, la strategia è personalizzata in base ai vincoli del caso d’uso. L’obiettivo generale è quello di generare una strategia ottimale che rimuova la ridondanza, ottenga guadagni di efficienza tramite la potatura e metta a punto il modello per stabilizzare l’accuratezza a un livello accettabile per l’implementazione.\nOra si consideri la stessa rete che avevamo nell’esempio di potatura iterativa. Mentre nel processo iterativo abbiamo potato 2 canali alla volta, nella potatura one-shot poteremo i 6 canali contemporaneamente (Figura 9.4). La rimozione simultanea del 27% del canale della rete altera significativamente la struttura, causando un calo della precisione da 0.995 a 0.914. Date le modifiche principali, la rete non è in grado di adattarsi correttamente durante la messa a punto e la precisione è salita a 0.943, un degrado del 5% rispetto alla precisione della rete non potata. Mentre le strutture finali nei processi di potatura iterativa e di potatura one-shot sono identiche, la prima è in grado di mantenere prestazioni elevate mentre la seconda subisce degradi significativi.\n\n\n\n\n\n\nFigura 9.4: Potatura one-shot.\n\n\n\n\n\n\nVantaggi della Potatura Strutturata\nLa potatura strutturata offre una miriade di vantaggi che soddisfano vari aspetti dell’implementazione e dell’utilizzo del modello, specialmente in ambienti in cui le risorse computazionali sono limitate.\n\nEfficienza Computazionale: Eliminando intere strutture, come neuroni o canali, si riduce significativamente il carico computazionale durante le fasi di training e inferenza, consentendo così previsioni più rapide del modello e convergenza del training. Inoltre, la rimozione delle strutture riduce intrinsecamente il “footprint” [impronta] di memoria del modello, assicurando che richieda meno spazio di archiviazione e memoria durante il funzionamento, il che è particolarmente vantaggioso in ambienti con limiti di memoria come i sistemi TinyML.\nEfficienza Hardware: La potatura strutturata spesso si traduce in modelli più adatti all’implementazione su hardware specializzato, come i Field-Programmable Gate Arrays (FPGA) o Application-Specific Integrated Circuits (ASIC), a causa della regolarità e la semplicità dell’architettura potata. Con requisiti di elaborazione ridotti, si traduce in un consumo energetico inferiore, fondamentale per i dispositivi alimentati a batteria e i metodi di elaborazione sostenibili.\nManutenzione e Distribuzione: Il modello ridotto, sebbene più piccolo, mantiene la sua forma architettonica originale, che può semplificare la pipeline di distribuzione e garantire la compatibilità con i sistemi e i framework esistenti. Inoltre, con meno parametri e strutture più semplici, il modello potato diventa più facile da gestire e monitorare negli ambienti di produzione, riducendo potenzialmente le spese generali associate alla manutenzione e agli aggiornamenti del modello. Più avanti, quando approfondiremo MLOps, questa necessità diventerà evidente.\n\n\n\nPotatura non Strutturata\nIl “pruning” non-strutturato è, come suggerisce il nome, la potatura del modello senza riguardo alla sotto-struttura specifica del modello. Come accennato in precedenza, offre una maggiore aggressività nella potatura e può raggiungere maggiori diradazione del modello mantenendo la precisione, dati meno vincoli su ciò che può e non può essere potato. In genere, la potatura non-strutturata post-training consiste in un criterio di importanza per i singoli parametri/pesi del modello, potatura/rimozione dei pesi che scendono al di sotto dei criteri e una successiva messa a punto facoltativa per provare a recuperare la precisione persa durante la rimozione dei pesi.\nLa potatura non-strutturata presenta alcuni vantaggi rispetto a quella strutturata: la rimozione di singoli pesi anziché di intere sotto-strutture del modello spesso porta in pratica a minori diminuzioni della precisione del modello. Inoltre, in genere determinare il criterio di importanza per un singolo peso è molto più semplice che per un’intera sotto-struttura di parametri nella potatura strutturata, rendendo la prima preferibile nei casi in cui tale overhead è difficile o poco chiaro da calcolare. Analogamente, il processo effettivo di potatura strutturata è generalmente meno flessibile, poiché la rimozione di singoli pesi è generalmente più semplice della rimozione di intere sotto-strutture e della garanzia che il modello funzioni ancora.\nLa potatura non strutturata, pur offrendo il potenziale per una significativa riduzione delle dimensioni del modello e una migliore implementabilità, porta con sé problemi legati alla gestione di rappresentazioni sparse e alla garanzia dell’efficienza computazionale. È particolarmente utile in scenari in cui è fondamentale ottenere la massima compressione possibile del modello e in cui l’ambiente di distribuzione può gestire in modo efficiente i calcoli sparsi.\nTabella 9.1 fornisce un confronto conciso tra potatura strutturata e la non-strutturata. In questa tabella, gli aspetti relativi alla natura e all’architettura del modello potato (Definizione, Regolarità del modello e Livello di compressione) sono raggruppati insieme, seguiti dagli aspetti relativi alle considerazioni computazionali (Efficienza computazionale e Compatibilità hardware) e terminando con gli aspetti relativi all’implementazione e all’adattamento del modello potato (Complessità di implementazione e Complessità di messa a punto). Entrambe le strategie di potatura offrono vantaggi e problemi unici, come mostrato in Tabella 9.1, e la selezione tra di esse dovrebbe essere influenzata da requisiti specifici del progetto e della distribuzione.\n\n\n\nTabella 9.1: Confronto tra potatura strutturata e non-strutturata.\n\n\n\n\n\n\n\n\n\n\nAspetto\nPotatura strutturata\nPotatura non strutturata\n\n\n\n\nDefinizione\nPotatura di intere strutture (ad esempio, neuroni, canali, layer) all’interno della rete\nPotatura di singoli pesi o neuroni, con conseguenti matrici sparse o strutture di rete non regolari\n\n\nRegolarità del Modello\nMantiene un’architettura di rete regolare e strutturata\nSi traduce in architetture di rete irregolari e sparse\n\n\nLivello di Compressione\nPuò offrire una compressione del modello limitata rispetto alla potatura non-strutturata\nPuò ottenere una compressione del modello più elevata grazie alla potatura a grana fine\n\n\nEfficienza Computazionale\nIn genere più efficiente computazionalmente grazie al mantenimento di strutture regolari\nPuò essere inefficiente dal punto di vista computazionale a causa di matrici di peso sparse, a meno che non venga utilizzato hardware/software specializzato\n\n\nCompatibilità Hardware\nIn genere più compatibile con vari hardware grazie alle strutture regolari\nPotrebbe richiedere hardware che gestisca in modo efficiente i calcoli sparsi per ottenere vantaggi\n\n\nComplessità di Implementazione\nSpesso più semplice da implementare e gestire grazie al mantenimento della struttura della rete\nPuò essere complesso da gestire e calcolare a causa delle rappresentazioni sparse\n\n\nComplessità di Messa a Punto Fine\nPotrebbe richiedere strategie di messa a punto fine meno complesse dopo la potatura\nPotrebbe richiedere strategie di riaddestramento o messa a punto fine più complesse dopo la potatura\n\n\n\n\n\n\nIn Figura 9.5 abbiamo esempi che illustrano le differenze tra potatura non-strutturata e strutturata. Osservare che la potatura non-strutturata può portare a modelli che non rispettano più le garanzie strutturali di alto livello delle loro controparti originali non potate: la rete di sinistra non è più una rete completamente connessa dopo la potatura. La potatura strutturata, d’altro canto, mantiene quelle invarianti: al centro, la rete completamente connessa viene potata in modo che resti ancora completamente connessa; allo stesso modo, la CNN mantiene la sua struttura convoluzionale, sebbene con meno filtri.\n\n\n\n\n\n\nFigura 9.5: Potatura non-strutturata e strutturata. Fonte: Qi et al. (2021).\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang, e Honggang Zhang. 2021. «An efficient pruning scheme of deep neural networks for Internet of Things applications». EURASIP Journal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\n\n\nIpotesi del Biglietto della Lotteria\nLa potatura si è evoluta da una tecnica puramente post-addestramento che comportava un costo per una certa accuratezza, a un potente approccio di meta-apprendimento applicato durante l’addestramento per ridurre la complessità del modello. Questo progresso a sua volta migliora l’efficienza di calcolo, memoria e latenza sia nell’addestramento che nell’inferenza.\nUna scoperta rivoluzionaria che ha catalizzato questa evoluzione è stata l’ipotesi del biglietto della lotteria di Frankle e Carbin (2019). Il loro lavoro afferma che all’interno di reti neurali dense esistono sotto-reti sparse, denominate “biglietti vincenti”, che possono eguagliare o addirittura superare le prestazioni del modello originale quando addestrate in isolamento. In particolare, questi biglietti vincenti, quando inizializzati utilizzando gli stessi pesi della rete originale, possono raggiungere una convergenza e un’accuratezza di addestramento altrettanto elevate su un dato compito. Vale la pena sottolineare che hanno scoperto empiricamente l’ipotesi del biglietto della lotteria, che è stata successivamente formalizzata.\n\nFrankle, Jonathan, e Michael Carbin. 2019. «The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks». In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\nL’intuizione alla base di questa ipotesi è che, durante il processo di addestramento di una rete neurale, molti neuroni e connessioni diventano ridondanti o non importanti, in particolare con l’inclusione di tecniche di addestramento che incoraggiano la ridondanza come il “dropout” [abbandono]. L’identificazione, la potatura e l’inizializzazione di questi “biglietti vincenti” consentono un addestramento più rapido e modelli più efficienti, poiché contengono le informazioni essenziali per la decisione del modello per l’attività. Inoltre, come generalmente noto con la teoria del “bias-variance tradeoff” [compromesso tra bias e varianza], questi biglietti soffrono meno di sovra-parametrizzazione e quindi si generalizzano meglio piuttosto che sovra-adattarsi all’attività.\nIn Figura 9.6 abbiamo un esempio che mostra esperimenti di potatura e addestramento su una LeNet completamente connessa su una varietà di rapporti di potatura. Nel grafico a sinistra, si nota come una potatura pesante riveli una sotto-rete più efficiente (in verde) che è il 21,1% delle dimensioni della rete originale (in blu). La sotto-rete raggiunge una maggiore accuratezza e in modo più rapido rispetto alla versione non potata (la linea verde è sopra la linea blu). Tuttavia, la potatura ha un limite (punto ottimale) e un’ulteriore potatura produrrà degradi delle prestazioni e alla fine scenderà al di sotto delle prestazioni della versione non potata (nota come le sotto-reti rossa, viola e marrone diminuiscono gradualmente nelle prestazioni di accuratezza) a causa della significativa perdita nel numero di parametri.\n\n\n\n\n\n\nFigura 9.6: Esperimenti sull’ipotesi del biglietto della lotteria.\n\n\n\nPer scoprire questi biglietti vincenti della lotteria all’interno di una rete neurale, viene seguito un processo sistematico. Questo processo, illustrato in Figura 9.7 (a sinistra), prevede l’addestramento iterativo, la potatura e la reinizializzazione della rete. I passaggi seguenti delineano questo approccio:\n\nInizializzare i pesi della rete a valori casuali.\nAddestrare la rete finché non converge alle prestazioni desiderate.\nEliminare una percentuale di rami con i valori di peso più bassi.\nReinizializzare la rete con gli stessi valori casuali del passaggio 1.\nRipetere i passaggi 2-4 più volte o finché la precisione non peggiora in modo significativo.\n\nAlla fine, ci si ritrova con una rete potata (Figura 9.7 lato destro), che è una sotto-rete di quella di partenza. La sotto-rete dovrebbe avere una struttura significativamente più piccola, pur mantenendo un livello di precisione comparabile.\n\n\n\n\n\n\nFigura 9.7: Trovare la sottorete del biglietto vincente.\n\n\n\n\n\nProblemi e Limitazioni\nNon c’è niente di gratuito con le ottimizzazioni di potatura, con alcune scelte che comportano sia miglioramenti che costi da considerare. Di seguito, discutiamo alcuni compromessi che gli esperti devono considerare.\n\nGestione di Matrici di Peso Sparse: Una matrice di peso sparsa è una matrice in cui molti degli elementi sono pari a zero. La potatura non strutturata spesso produce matrici di peso sparse, in cui molti pesi vengono potati a zero. Sebbene ciò riduca le dimensioni del modello, introduce anche diversi problemi. L’inefficienza computazionale può sorgere perché l’hardware standard è ottimizzato per operazioni di matrice densa. Senza ottimizzazioni che sfruttano la sparsità, i risparmi computazionali derivanti dalla potatura possono essere persi. Sebbene le matrici sparse possano essere archiviate senza formati specializzati, sfruttare efficacemente la loro sparsità richiede una gestione attenta per evitare di sprecare risorse. Algoritmicamente, la navigazione in strutture sparse richiede di saltare in modo efficiente le voci zero, il che aggiunge complessità al calcolo e agli aggiornamenti del modello.\nQualità vs. Riduzione delle Dimensioni: Una sfida fondamentale sia nella potatura strutturata che in quella non-strutturata è bilanciare la riduzione delle dimensioni con il mantenimento o il miglioramento delle prestazioni predittive. È essenziale stabilire criteri di potatura robusti, sia per rimuovere intere strutture (potatura strutturata) sia singoli pesi (potatura non strutturata). Questi criteri di potatura scelti devono identificare accuratamente gli elementi la cui rimozione ha un impatto minimo sulle prestazioni. Spesso è necessaria un’attenta sperimentazione per garantire che il modello potato rimanga efficiente mantenendo al contempo le sue prestazioni predittive.\nFine-Tuning e Riaddestramento: La messa a punto post-potatura è fondamentale sia nella potatura strutturata che in quella non-strutturata per recuperare le prestazioni perse e stabilizzare il modello. La sfida comprende la determinazione dell’estensione, della durata e della natura del processo di messa a punto, che può essere influenzato dal metodo di potatura e dal grado di potatura applicato.\nCompatibilità ed Efficienza Hardware: Particolarmente pertinenti alla potatura non-strutturata, la compatibilità e l’efficienza hardware diventano critiche. La potatura non strutturata spesso si traduce in matrici di peso sparse, che potrebbero non essere gestite in modo efficiente da un certo hardware, annullando potenzialmente i vantaggi computazionali della potatura (vedere Figura 9.8). Garantire che i modelli potati, in particolare quelli risultanti dall’eliminazione non-strutturata, siano scalabili, compatibili ed efficienti sull’hardware target è una considerazione importante.\nConsiderazioni Legali ed Etiche: Ultimo ma non meno importante, il rispetto delle linee guida legali ed etiche è importante, soprattutto in ambiti con conseguenze significative. I metodi di potatura devono essere sottoposti a rigorosi processi di validazione, test e potenzialmente certificazione per garantire la conformità alle normative e agli standard pertinenti, sebbene al momento non esistano standard formali e “best practice” che siano esaminati e convalidati da entità terze. Ciò è particolarmente cruciale in applicazioni ad alto rischio come l’intelligenza artificiale medica e la guida autonoma, dove i cali di qualità dovuti a ottimizzazioni simili alla potatura possono essere pericolosi per la vita. Inoltre, le considerazioni etiche si estendono oltre la sicurezza fino all’equità e all’uguaglianza; un recente lavoro di (Tran et al. 2022) ha rivelato che la potatura può avere un impatto sproporzionato sulle persone di colore, sottolineando la necessità di una valutazione etica completa nel processo di potatura.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, e Rakshit Naidu. 2022. «Pruning has a disparate impact on model accuracy». Adv Neural Inf Process Syst 35: 17652–64.\n\n\n\n\n\n\nFigura 9.8: Matrice dei pesi sparsi.\n\n\n\n\n\n\n\n\n\nEsercizio 9.1: Pruning\n\n\n\n\n\nSi immagini che la rete neurale sia un cespuglio gigante e troppo cresciuto. La potatura è come tagliare strategicamente i rami per renderla più forte ed efficiente! Nel Colab, si imparerà come fare questa potatura in TensorFlow. La comprensione di questi concetti fornirà le basi per vedere come la potatura rende i modelli abbastanza piccoli da poter essere eseguiti sul telefono!\n\n\n\n\n\n\n\n9.2.2 Compressione del Modello\nLe tecniche di compressione del modello sono fondamentali per distribuire modelli di deep learning su dispositivi con risorse limitate. Queste tecniche mirano a creare modelli più piccoli ed efficienti che preservino le prestazioni predittive dei modelli originali.\n\nDistillazione della Conoscenza\nUna tecnica popolare è la knowledge distillation (KD) distillazione della conoscenza, che trasferisce la conoscenza da un modello “insegnante” ampio e complesso a un modello “studente” più piccolo. L’idea chiave è addestrare il modello studente a imitare gli output dell’insegnante. Il concetto di KD è stato reso popolare per la prima volta da Hinton (2005).\n\nHinton, Geoffrey. 2005. «Van Nostrand’s Scientific Encyclopedia». Wiley. https://doi.org/10.1002/0471743984.vse0673.\n\nPanoramica e Vantaggi\nLa distillazione della conoscenza implica il trasferimento della conoscenza da un modello insegnante ampio e complesso a un modello studente più piccolo. L’idea di base è quella di utilizzare gli output dell’insegnante, noti come soft targets, per guidare il training del modello studente. A differenza dei tradizionali “hard targets” (le vere etichette), quelli soft sono le distribuzioni di probabilità sulle classi che il modello insegnante prevede. Queste distribuzioni forniscono informazioni più complete sulle relazioni tra le classi, il che può aiutare il modello studente ad apprendere in modo più efficace.\nAbbiamo imparato che la funzione softmax converte gli output grezzi di un modello in una distribuzione di probabilità sulle classi. Una tecnica chiave in KD è la scalatura della temperatura, che viene applicata alla funzione softmax degli output del modello insegnante. Introducendo un parametro di temperatura, la distribuzione può essere regolata: una temperatura più alta produce probabilità più soft, il che significa che le differenze tra le probabilità di classe diventano meno estreme. Questo effetto di ammorbidimento determina una distribuzione più uniforme, in cui la fiducia del modello nella classe più probabile è ridotta e altre classi hanno probabilità più elevate, diverse da zero. Ciò è prezioso per il modello studente perché gli consente di apprendere non solo dalla classe più probabile, ma anche dalle probabilità relative di tutte le classi, catturando modelli sottili che potrebbero essere persi se addestrati solo su obiettivi difficili. Pertanto, la scalabilità della temperatura facilita il trasferimento di conoscenze più sfumate dal modello insegnante a quello studente.\nLa funzione di perdita nella distillazione della conoscenza in genere combina due componenti: una perdita di distillazione e una perdita di classificazione. La perdita di distillazione, spesso calcolata utilizzando la divergenza di Kullback-Leibler (KL), misura la differenza tra gli soft target prodotti dal modello insegnante e gli output del modello studente, incoraggiando lo studente a imitare le previsioni dell’insegnante. Nel frattempo, la perdita di classificazione assicura che il modello studente preveda correttamente le etichette vere in base ai dati originali. Insieme, queste due componenti aiutano lo studente modello a conservare le conoscenze dell’insegnante, rispettando al contempo le etichette di verità di base.\nQuesti componenti, quando configurati e armonizzati abilmente, consentono al modello studente di assimilare la conoscenza del modello insegnante, creando un percorso verso modelli più piccoli, efficienti e robusti, che mantengono la capacità predittiva delle loro controparti più grandi. Figura 9.9 visualizza la procedura di training della “knowledge distillation”. Notare come i logit o le soft label del modello insegnante vengono utilizzati per fornire una perdita di distillazione da cui il modello studente può imparare.\n\n\n\n\n\n\nFigura 9.9: Processo di training della distillazione della conoscenza. Fonte: IntelLabs (2023).\n\n\nIntelLabs. 2023. «Knowledge Distillation - Neural Network Distiller». https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\n\n\nSfide\nTuttavia, KD presenta una serie unica di sfide e considerazioni che ricercatori e professionisti devono affrontare attentamente. Una delle sfide è nella messa a punto meticolosa degli iperparametri, come il parametro “temperatura” nella funzione softmax e la ponderazione tra la distillazione e la perdita di classificazione nella funzione obiettivo. Raggiungere un equilibrio che sfrutti efficacemente gli output ammorbiditi del modello insegnante mantenendo al contempo la fedeltà alle etichette dei dati reali non è banale e può avere un impatto significativo sulle prestazioni e sulle capacità di generalizzazione del modello studente.\nInoltre, l’architettura del modello studente stesso pone un problema considerevole. Progettare un modello compatto per soddisfare i vincoli di calcolo e memoria, pur essendo in grado di assimilare le conoscenze essenziali dal modello insegnante, richiede una comprensione sfumata della capacità del modello e dei compromessi intrinseci coinvolti nella compressione. Il modello studente deve essere attentamente progettato per navigare nella dicotomia di dimensioni e prestazioni, assicurando che la conoscenza distillata venga catturata e utilizzata in modo significativo. Inoltre, la scelta del modello dell’insegnante, che influenza intrinsecamente la qualità e la natura della conoscenza da trasferire, è importante e introduce un ulteriore livello di complessità nel processo KD.\nQueste sfide sottolineano la necessità di un approccio completo e sfumato all’implementazione di KD, assicurando che i modelli degli studenti risultanti siano sia efficienti che efficaci nei loro contesti operativi.\n\n\n\nFattorizzazione di Matrici di Basso Rango\nSimile nel tema dell’approssimazione, la Low-Rank Matrix Factorization (LRMF) fattorizzazione di matrici di basso rango è una tecnica matematica utilizzata in algebra lineare e analisi dei dati per approssimare una matrice data scomponendola in due o più matrici di dimensione inferiore. L’idea fondamentale è di esprimere una matrice di grandi dimensioni come prodotto di matrici di rango inferiore, il che può aiutare a ridurre la complessità dei dati preservandone la struttura essenziale. Matematicamente, data una matrice \\(A \\in \\mathbb{R}^{m \\times n}\\), LRMF cercare le matrici \\(U \\in \\mathbb{R}^{m \\times k}\\) e \\(V \\in \\mathbb{R}^{k \\times n}\\) tali che \\(A \\approx UV\\), dove \\(k\\) è il rango ed è in genere molto più piccolo di \\(m\\) e \\(n\\).\n\nBackground e Benefici\nUno dei primo lavori nel campo della fattorizzazione di matrici, in particolare nel contesto dei sistemi di raccomandazione, è il documento di Koren, Bell, e Volinsky (2009). Gli autori esaminano vari modelli di fattorizzazione, fornendo approfondimenti sulla loro efficacia nel catturare i modelli sottostanti nei dati e nel migliorare l’accuratezza predittiva nel filtraggio collaborativo. LRMF è stato ampiamente applicato nei sistemi di raccomandazione (come Netflix, Facebook, ecc.), dove la matrice di interazione utente-elemento è fattorizzata per catturare fattori latenti corrispondenti alle preferenze dell’utente e agli attributi dell’elemento.\n\nKoren, Yehuda, Robert Bell, e Chris Volinsky. 2009. «Matrix Factorization Techniques for Recommender Systems». Computer 42 (8): 30–37. https://doi.org/10.1109/mc.2009.263.\nIl vantaggio principale della “fattorizzazione di matrici di basso rango” risiede nella sua capacità di ridurre la dimensionalità dei dati come mostrato in Figura 9.10, dove ci sono meno parametri da memorizzare, rendendola più efficiente dal punto di vista computazionale e riducendo i requisiti di archiviazione a costo di un po’ di elaborazione aggiuntiva. Ciò può portare a calcoli più rapidi e rappresentazioni di dati più compatte, il che è particolarmente prezioso quando si ha a che fare con grandi set di dati. Inoltre, può aiutare nella riduzione del rumore e può rivelare modelli e relazioni sottostanti nei dati.\nFigura 9.10 illustra la diminuzione della parametrizzazione abilitata dalla fattorizzazione di matrici di basso rango. Osservare come la matrice \\(M\\) può essere approssimata dal prodotto delle matrici \\(L_k\\) e \\(R_k^T\\). Per intuizione, la maggior parte dei layer completamente connessi nelle reti sono archiviati come matrice di proiezione \\(M\\), che richiede il caricamento di \\(m \\times n\\) parametri durante il calcolo. Tuttavia, scomponendola e approssimandola come prodotto di due matrici di rango inferiore, abbiamo bisogno di archiviare solo \\(m \\times k + k\\times n\\) parametri in termini di archiviazione, sostenendo al contempo un costo di calcolo aggiuntivo per la moltiplicazione delle matrici. Finché \\(k &lt; n/2\\), questa fattorizzazione ha meno parametri totali da archiviare, aggiungendo un calcolo di runtime \\(O(mkn)\\) (Gu 2023).\n\nGu, Ivy. 2023. «Deep Learning Model Compression (ii) by Ivy Gu Medium». https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\n\n\n\n\nFigura 9.10: Fattorizzazione di matrici di basso rango. Fonte: The Clever Machine.\n\n\n\n\n\nSfide\nMa professionisti e ricercatori incontrano una serie di problemi e considerazioni che richiedono una particolare attenzione e approcci strategici. Come con qualsiasi tecnica di compressione lossy [con perdita], potremmo perdere informazioni durante questo processo di approssimazione: scegliere il rango corretto che bilanci le informazioni perse e i costi computazionali è altrettanto complicato e aggiunge un ulteriore iperparametro da regolare.\nLa fattorizzazione di matrici di basso rango è uno strumento prezioso per la riduzione della dimensionalità e per adattare il calcolo ai dispositivi edge ma, come altre tecniche, deve essere attentamente regolata in base al modello e all’attività da svolgere. Una sfida fondamentale risiede nella gestione della complessità computazionale inerente a LRMF, soprattutto quando si hanno a che fare con dati ad alta dimensionalità e su larga scala. L’onere computazionale, in particolare nel contesto di applicazioni in tempo reale e set di dati massicci, rimane un ostacolo significativo per un utilizzo efficace di LRMF.\nInoltre, l’enigma della scelta del rango ottimale \\(k\\) per la fattorizzazione introduce un ulteriore livello di complessità. La selezione di \\(k\\) implica intrinsecamente un compromesso tra accuratezza dell’approssimazione e semplicità del modello, e l’identificazione di un rango che bilanci abilmente questi obiettivi contrastanti spesso richiede una combinazione di competenza di dominio, convalida empirica e, a volte, approcci euristici. La sfida è ulteriormente amplificata quando i dati comprendono rumore o quando la struttura intrinseca di basso rango non è pronunciata, rendendo la determinazione di un \\(k\\) adatto ancora più sfuggente.\nLa gestione di dati mancanti o sparsi, un evento comune in applicazioni come i sistemi di raccomandazione, pone un’altra sfida sostanziale. Le tecniche tradizionali di fattorizzazione delle matrici, come la Singular Value Decomposition (SVD), non sono direttamente applicabili alle matrici con voci mancanti, rendendo necessario lo sviluppo e l’applicazione di algoritmi specializzati in grado di fattorizzare matrici incomplete mitigando al contempo i rischi di overfitting alle voci osservate. Ciò spesso comporta l’incorporazione di termini di regolarizzazione o la limitazione della fattorizzazione in modi specifici, il che a sua volta introduce ulteriori iperparametri che devono essere selezionati giudiziosamente.\nInoltre, in scenari in cui i dati evolvono o crescono nel tempo, sviluppare modelli LRMF in grado di adattarsi a nuovi dati senza richiedere una completa rifattorizzazione è un’impresa critica ma impegnativa. Gli algoritmi di fattorizzazione di matrici incrementali e online cercano di risolvere questo problema consentendo l’aggiornamento delle matrici fattorizzate all’arrivo di nuovi dati, ma garantire stabilità, accuratezza ed efficienza computazionale in queste impostazioni dinamiche rimane un compito intricato. Ciò è particolarmente impegnativo nello spazio di TinyML, in cui la ridistribuzione dei rami per i modelli aggiornati può essere piuttosto impegnativa.\n\n\n\nDecomposizione dei Tensori\nAbbiamo visto in Sezione 6.4.1 che i tensori sono strutture flessibili, comunemente utilizzate dai framework ML, che possono rappresentare dati in dimensioni superiori. Similmente alla fattorizzazione di matrici di basso rango, i modelli più complessi possono memorizzare pesi in dimensioni superiori, come i tensori. La decomposizione tensoriale è l’analogo di dimensioni superiori della fattorizzazione di matrici, in cui un tensore modello viene scomposto in componenti di rango inferiore (cfr. Figura 9.11). Questi componenti di rango inferiore sono più facili da calcolare e memorizzare, ma possono soffrire degli stessi problemi menzionati sopra, come la perdita di informazioni e la necessità di una messa a punto sfumata degli iperparametri. Matematicamente, dato un tensore \\(\\mathcal{A}\\), la decomposizione tensoriale cerca di rappresentare \\(\\mathcal{A}\\) come una combinazione di tensori più semplici, facilitando una rappresentazione compressa che approssima i dati originali riducendo al minimo la perdita di informazioni.\nIl lavoro di Tamara G. Kolda e Brett W. Bader, “Tensor Decompositions and Applications” (2009), si distingue come un articolo fondamentale nel campo delle decomposizioni tensoriali. Gli autori forniscono una panoramica completa di vari metodi di decomposizione tensoriale, esplorandone i fondamenti matematici, gli algoritmi e un’ampia gamma di applicazioni, che vanno dall’elaborazione del segnale al data mining. Naturalmente, il motivo per cui ne stiamo discutendo è perché ha un enorme potenziale per i miglioramenti delle prestazioni del sistema, in particolare nello spazio di TinyML, dove la produttività e i risparmi di memoria sono fondamentali per la fattibilità delle distribuzioni.\n\n\n\n\n\n\nFigura 9.11: Decomposizione dei Tensori. Fonte: Xinyu (s.d.).\n\n\nXinyu, Chen. s.d.\n\n\n\n\n\n\n\n\nEsercizio 9.2: Compressione di Modelli Scalabili con TensorFlow\n\n\n\n\n\nQuesto Colab si addentra in una tecnica per comprimere i modelli mantenendo un’elevata accuratezza. L’idea chiave è quella di addestrare un modello con un termine di penalità extra che incoraggia il modello a essere più comprimibile. Quindi, il modello viene codificato utilizzando uno schema di codifica speciale che si allinea con questa penalità. Questo approccio consente di ottenere modelli compressi che funzionano altrettanto bene dei modelli originali ed è utile per distribuire modelli su dispositivi con risorse limitate come telefoni cellulari e dispositivi edge.\n\n\n\n\n\n\n\n9.2.3 Modelli Progettati per l’Edge\nOra raggiungiamo l’altro estremo del gradiente hardware-software, dove prendiamo decisioni specifiche sull’architettura del modello direttamente in base alla conoscenza dei dispositivi edge su cui desideriamo implementare.\nCome spiegato nelle sezioni precedenti, i dispositivi edge sono vincolati specificamente da limitazioni di memoria e calcoli parallelizzabili: in quanto tali, se ci sono requisiti critici di velocità di inferenza, i calcoli devono essere sufficientemente flessibili da soddisfare i vincoli hardware, qualcosa che può essere progettato a livello di architettura del modello. Inoltre, cercare di stipare grandi modelli SOTA ML su dispositivi edge anche dopo potatura e compressione è generalmente irrealizzabile puramente a causa delle dimensioni: la complessità del modello stesso deve essere scelta con più sfumature per adattarsi più fattibilmente al dispositivo. Gli sviluppatori di Edge ML hanno affrontato questa sfida architettonica sia attraverso la progettazione di architetture di modelli edge ML su misura sia attraverso la Neural Architecture Search (NAS) [ricerca di architettura neurale] avente il dispositivo come target, che può generare in modo più sistematico architetture fattibili di modelli su dispositivo.\n\nTecniche di Progettazione del Modello\nUn design di architettura edge friendly, comunemente utilizzato nel deep learning per l’elaborazione delle immagini, è quello delle convoluzioni separabili in profondità. Consiste in due fasi distinte: la prima è la convoluzione in profondità, in cui ogni canale di input viene convoluto in modo indipendente con il proprio set di filtri apprendibili, come mostrato in Figura 9.12. Questa fase riduce la complessità computazionale in modo significativo rispetto alle convoluzioni standard, poiché riduce drasticamente il numero di parametri e calcoli coinvolti. La seconda fase è la convoluzione puntuale, che combina l’output dei canali di convoluzione in profondità tramite una convoluzione 1x1, creando interazioni tra canali. Questo approccio offre diversi vantaggi. I vantaggi includono dimensioni ridotte del modello, tempi di inferenza più rapidi e spesso una migliore generalizzazione grazie al minor numero di parametri, rendendolo adatto ad applicazioni mobili ed embedded. Tuttavia, le convoluzioni separabili in profondità potrebbero non catturare interazioni spaziali complesse in modo efficace come le convoluzioni standard e potrebbero richiedere più profondità (livelli) per raggiungere lo stesso livello di potenza rappresentativa, portando potenzialmente a tempi di addestramento più lunghi. Tuttavia, la loro efficienza in termini di parametri e calcolo le rende una scelta popolare nelle moderne architetture di reti neurali convoluzionali.\n\n\n\n\n\n\nFigura 9.12: Convoluzioni separabili in profondità. Fonte: Hegde (2023).\n\n\nHegde, Sumant. 2023. «An Introduction to Separable Convolutions - Analytics Vidhya». https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\n\n\nArchitetture di Modello di Esempio\nIn quest’ottica, diverse architetture recenti sono state, fin dall’inizio, progettate specificamente per massimizzare la precisione in un’implementazione edge, in particolare SqueezeNet, MobileNet ed EfficientNet.\n\nSqueezeNet di Iandola et al. (2016), ad esempio, utilizza un’architettura compatta con convoluzioni 1x1 e moduli “fire” per ridurre al minimo il numero di parametri mantenendo al contempo una forte accuratezza.\nMobileNet di Howard et al. (2017), d’altra parte, impiega le suddette convoluzioni separabili in profondità per ridurre sia il calcolo che le dimensioni del modello.\nEfficientNet di Tan e Le (2023) adotta un approccio diverso ottimizzando il ridimensionamento della rete (ovvero variando la profondità, la larghezza e la risoluzione di una rete) e il ridimensionamento composto, una variazione più sfumata del ridimensionamento della rete, per ottenere prestazioni superiori con meno parametri.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. «SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and 0.5 MB model size». ArXiv preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nTan, Mingxing, e Quoc V. Le. 2023. «Demystifying Deep Learning». Wiley. https://doi.org/10.1002/9781394205639.ch6.\nQuesti modelli sono essenziali nel contesto dell’edge computing in cui la limitazione di potenza di elaborazione e di memoria richiede modelli leggeri ma efficaci in grado di eseguire in modo efficiente attività quali il riconoscimento delle immagini, il rilevamento di oggetti e altro ancora. I loro principi di progettazione mostrano l’importanza di un’architettura di modelli intenzionalmente personalizzata per l’edge computing, in cui prestazioni ed efficienza devono rientrare nei vincoli.\n\n\nSemplificazione della Ricerca di Architetture di Modelli\nInfine, per affrontare la sfida di trovare architetture di modelli efficienti che siano compatibili con i dispositivi edge, i ricercatori hanno sviluppato pipeline sistematizzate che semplificano la ricerca di progetti performanti. Due framework degni di nota in questo spazio sono TinyNAS di J. Lin et al. (2020) e MorphNet di Gordon et al. (2018), che automatizzano il processo di ottimizzazione delle architetture di reti neurali per l’implementazione edge.\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, e Edward Choi. 2018. «MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1586–95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\nTinyNAS è un innovativo framework di ricerca di architetture neurali introdotto nel documento MCUNet, progettato per scoprire in modo efficiente architetture di reti neurali leggere per dispositivi edge con risorse computazionali limitate. Sfruttando l’apprendimento per rinforzo e uno spazio di ricerca compatto di micromoduli neurali, TinyNAS ottimizza sia l’accuratezza che la latenza, consentendo l’implementazione di modelli di deep learning su microcontrollori, dispositivi IoT e altre piattaforme con risorse limitate. Nello specifico, TinyNAS, in combinazione con un ottimizzatore di rete, TinyEngine, genera diversi spazi di ricerca ridimensionando la risoluzione di input e la larghezza del modello, poi raccoglie la distribuzione FLOP di calcolo delle reti soddisfacenti all’interno dello spazio di ricerca per valutarne la priorità. TinyNAS si basa sul presupposto che uno spazio di ricerca che ospita FLOP più elevati con vincoli di memoria possa produrre modelli di accuratezza più elevata, cosa che gli autori hanno verificato in pratica nel loro lavoro. In termini di prestazioni empiriche, TinyEngine ha ridotto l’utilizzo di memoria di picco dei modelli di circa 3.4 volte e ha accelerato l’inferenza da 1.7 a 3.3 volte rispetto a TFLite e a CMSIS-NN.\nAnalogamente, MorphNet è un framework di ottimizzazione delle reti neurali progettato per rimodellare e trasformare automaticamente l’architettura delle reti neurali profonde, ottimizzandole per requisiti di distribuzione specifici. Ciò avviene in due fasi: in primo luogo, sfrutta un set di operazioni di morphing della rete personalizzabili, come l’ampliamento o l’approfondimento dei layer, per regolare dinamicamente la struttura della rete. Queste operazioni consentono alla rete di adattarsi a vari vincoli computazionali, tra cui dimensioni del modello, latenza e obiettivi di accuratezza, che sono estremamente diffusi nell’utilizzo dell’edge computing. Nella seconda fase, MorphNet utilizza un approccio basato sull’apprendimento di rinforzo per cercare la permutazione ottimale delle operazioni di morphing, bilanciando efficacemente il compromesso tra dimensioni del modello e prestazioni. Questo metodo innovativo consente ai professionisti del deep learning di adattare automaticamente le architetture delle reti neurali a requisiti hardware e applicativi specifici, garantendo un’implementazione efficiente ed efficace su diverse piattaforme.\nTinyNAS e MorphNet rappresentano alcuni dei numerosi progressi significativi nel campo dell’ottimizzazione sistematica delle reti neurali, consentendo di scegliere e generare sistematicamente architetture per adattarsi perfettamente ai vincoli del problema.\n\n\n\n\n\n\nEsercizio 9.3: Modelli Progettati per l’Edge\n\n\n\n\n\nSi Immagini di costruire un piccolo robot in grado di identificare diversi fiori. Deve essere intelligente, ma anche piccolo ed efficiente dal punto di vista energetico! Nel mondo dei “Modelli Progettati per l’Edge”, abbiamo appreso tecniche come le convoluzioni separabili in profondità e architetture come SqueezeNet, MobileNet ed EfficientNet, tutte progettate per concentrare l’intelligenza in modelli compatti. Ora, vediamo queste idee in azione con alcuni xColab:\nSqueezeNet in Action: Forse piacerebbe un Colab che mostra come addestrare un modello SqueezeNet su un set di dati di immagini di fiori. Ciò dimostrerebbe le sue piccole dimensioni e come impara a riconoscere i pattern nonostante la sua efficienza.\n\nMobileNet Exploration: Ci si è mai chiesto se quei piccoli modelli di immagini sono buoni quanto quelli grandi? Scopriamolo! In questo Colab, mettiamo a confronto MobileNet, il campione dei pesi leggeri, con un modello di classificazione delle immagini classico. Li faremo gareggiare per la velocità, misureremo le loro esigenze di memoria e vedremo chi vincerà per accuratezza. Preparatevi per una battaglia di cervelli di immagini!",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#sec-model_ops_numerics",
    "href": "contents/optimizations/optimizations.it.html#sec-model_ops_numerics",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.3 Rappresentazione Numerica Efficiente",
    "text": "9.3 Rappresentazione Numerica Efficiente\nLa rappresentazione numerica implica una miriade di considerazioni, tra cui, ma non solo, la precisione dei numeri, i loro formati di codifica e le operazioni aritmetiche facilitate. Implica invariabilmente una vasta gamma di diversi compromessi, in cui i professionisti sono incaricati di destreggiarsi tra accuratezza numerica ed efficienza computazionale. Ad esempio, mentre i numeri a bassa precisione possono offrire il fascino di un utilizzo di memoria ridotto e calcoli accelerati, presentano contemporaneamente sfide relative alla stabilità numerica e al potenziale degrado dell’accuratezza del modello.\n\nMotivazione\nEmerge l’imperativo per una rappresentazione numerica efficiente, in particolare perché l’ottimizzazione efficiente del modello da sola non è sufficiente quando si adattano i modelli per l’implementazione su dispositivi edge a bassa potenza che operano con vincoli rigorosi.\nOltre a ridurre al minimo le richieste di memoria, l’enorme potenziale di una rappresentazione numerica efficiente risiede, ma non è limitato a, queste modalità fondamentali. Riducendo l’intensità computazionale, la matematica efficiente può amplificare la velocità computazionale, consentendo di elaborare modelli più complessi su dispositivi a bassa potenza. Ridurre la precisione in bit di pesi e attivazioni su modelli fortemente sovra-parametrizzati consente la condensazione delle dimensioni del modello per dispositivi edge senza danneggiare significativamente l’accuratezza predittiva del modello. Con l’onnipresenza delle reti neurali nei modelli, la matematica efficiente ha un vantaggio unico nello sfruttare la struttura a layer delle NN per variare la precisione numerica tra i layer, riducendo al minimo la precisione nei layer resistenti e preservando una maggiore precisione in quelli sensibili.\nIn questa sezione, approfondiremo il modo in cui i professionisti possono sfruttare i principi della progettazione congiunta hardware-software ai livelli più bassi di un modello per facilitare la compatibilità con i dispositivi edge. Iniziando con un’introduzione ai numeri, esamineremo le sue implicazioni per la memoria del dispositivo e la complessità computazionale. Successivamente, intraprenderemo una discussione sui compromessi implicati nell’adozione di questa strategia, seguita da un’analisi approfondita di un metodo fondamentale della matematica efficiente: la quantizzazione.\n\n\n9.3.1 Le Basi\n\nI Tipi\nI dati numerici, il fondamento su cui si basano i modelli di apprendimento automatico, si manifestano in due forme principali. Si tratta di numeri interi e numeri in virgola mobile.\nNumeri Interi: Numeri interi, privi di componenti frazionarie, (ad esempio, -3, 0, 42) sono fondamentali negli scenari che richiedono valori discreti. Ad esempio, in ML, le etichette di classe in un’attività di classificazione potrebbero essere rappresentate come numeri interi, dove “gatto”, “cane” e “uccello” potrebbero essere codificati rispettivamente come 0, 1 e 2.\nNumeri in virgola mobile: Comprendendo numeri reali, (ad esempio, -3.14, 0.01, 2.71828) consentono la rappresentazione di valori con componenti frazionarie. Nei parametri del modello ML, i pesi potrebbero essere inizializzati con piccoli valori a virgola mobile, ad esempio 0.001 o -0.045, per avviare il processo di training. Attualmente, ci sono 4 popolari formati di precisione discussi di seguito.\nLarghezze di bit variabili: Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezza di bit estremamente ridotta possono offrire accelerazioni significative e ridurre ulteriormente il consumo energetico. Sebbene permangano dei problemi nel mantenere l’accuratezza del modello con una quantizzazione così drastica, si continuano a fare progressi in quest’area.\n\n\nPrecisione\nLa precisione, che delinea l’esattezza con cui un numero è rappresentato, si biforca tipicamente in singola, doppia, mezza e negli ultimi anni sono emerse numerose altre precisioni per supportare meglio e in modo efficiente le attività di apprendimento automatico sull’hardware sottostante.\nDoppia precisione (Float64): Allocando 64 bit, la doppia precisione (ad esempio, 3.141592653589793) fornisce una precisione elevata, sebbene richieda più memoria e più risorse di calcolo. Nei calcoli scientifici, dove la precisione è fondamentale, variabili come π potrebbero essere rappresentate con Float64.\nSingola precisione (Float32): Con 32 bit a disposizione, la singola precisione (ad esempio, 3.1415927) raggiunge un equilibrio tra precisione numerica e risparmio della memoria. In ML, Float32 potrebbe essere impiegato per memorizzare i pesi durante l’addestramento per mantenere un livello ragionevole di precisione.\nHalf Precision (Float16): Limitata a 16 bit, la half precision (ad esempio, 3.14) riduce l’utilizzo della memoria e può velocizzare i calcoli, sebbene sacrifichi l’accuratezza e l’intervallo numerico. In ML, specialmente durante l’inferenza su dispositivi con risorse limitate, Float16 potrebbe essere utilizzato per ridurre l’impronta di memoria del modello.\nBfloat16: Brain Floating-Point Format o Bfloat16, impiega anche 16 bit ma li alloca in modo diverso rispetto a FP16: 1 bit per il segno, 8 bit per l’esponente (che si traduce nello stesso intervallo numerico di float32) e 7 bit per la frazione. Questo formato, sviluppato da Google, dà priorità a un intervallo di esponenti più ampio rispetto alla precisione, rendendolo particolarmente utile nelle applicazioni di apprendimento profondo in cui l’intervallo dinamico è cruciale.\nFigura 9.13 illustra le differenze tra i tre formati a virgola mobile: Float32, Float16 e BFloat16.\n\n\n\n\n\n\nFigura 9.13: Tre formati a virgola mobile.\n\n\n\nIntero: Le rappresentazioni di numeri interi sono realizzate utilizzando 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocità e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attività di inferenza, in particolare su dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione è spesso accettabile dati i guadagni di efficienza. Una forma estrema di numeri interi è per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno di due valori: +1 o -1.\nÈ possibile fare riferimento a Sezione 8.6.1 per una tabella di confronto tra i compromessi dei diversi tipi numerici.\n\n\nCodifica e Archiviazione Numerica\nLa codifica numerica, l’arte di trasformare i numeri in un formato utilizzabile dal computer e la loro successiva memorizzazione sono fondamentali per l’efficienza computazionale. Ad esempio, i numeri in virgola mobile potrebbero essere codificati utilizzando lo standard IEEE 754, che ripartisce i bit tra i componenti segno, esponente e frazione, consentendo così la rappresentazione di una vasta gamma di valori con un singolo formato. Esistono alcuni nuovi formati in virgola mobile IEEE che sono stati definiti specificamente per i carichi di lavoro AI:\n\nbfloat16- Un formato in virgola mobile a 16 bit introdotto da Google. Ha 8 bit per esponente, 7 bit per mantissa e 1 bit per segno. Offre un compromesso di precisione ridotto tra float a 32 bit e interi a 8 bit. Supportato su molti acceleratori hardware.\nposit - Un formato configurabile che può rappresentare diversi livelli di precisione in base ai bit esponente. È più efficiente dei numeri binari in virgola mobile IEEE 754. Ha una gamma dinamica e una precisione regolabili.\nFlexpoint - Un formato introdotto da Intel che può regolare dinamicamente la precisione tra livelli o all’interno di un layer. Consente di adattare la precisione all’accuratezza e ai requisiti hardware.\nBF16ALT - Un formato a 16 bit proposto da ARM come alternativa a bfloat16. Utilizza un bit aggiuntivo nell’esponente per evitare overflow/underflow.\nTF32 - Introdotto da Nvidia per le GPU Ampere. Utilizza 10 bit per l’esponente invece di 8 bit come FP32. Migliora le prestazioni di training del modello mantenendo l’accuratezza.\nFP8 - Formato a virgola mobile a 8 bit che mantiene 6 bit per la mantissa e 2 bit per l’esponente. Consente una gamma dinamica migliore rispetto agli interi.\n\nGli obiettivi principali di questi nuovi formati sono di fornire alternative di precisione inferiore ai float a 32 bit per una migliore efficienza computazionale e prestazioni sugli acceleratori AI, mantenendo al contempo l’accuratezza del modello. Offrono diversi compromessi in termini di precisione, portata e costo/complessità di implementazione.\n\n\n\n9.3.2 Vantaggi dell’Efficienza\nCome visto in Sezione 8.6.2, l’efficienza numerica è importante per i carichi di lavoro di apprendimento automatico per una serie di motivi. L’efficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano più pervasivi, soprattutto in ambienti reali con risorse limitate, l’attenzione su una numerica efficiente continuerà a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, è possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocità, memoria ed energia.\n\n\n9.3.3 Sfumature della Rappresentazione Numerica\nCi sono diverse sfumature con le rappresentazioni numeriche per ML che richiedono di avere una comprensione sia degli aspetti teorici che pratici della rappresentazione numerica, nonché una profonda consapevolezza dei requisiti e dei vincoli specifici del dominio applicativo.\n\nUtilizzo della Memoria\nL’impronta di memoria dei modelli ML, in particolare quelli di notevole complessità e profondità, può essere sostanziale, ponendo quindi una sfida significativa sia nelle fasi di training che di deployment. Ad esempio, una rete neurale profonda con 100 milioni di parametri, rappresentata utilizzando Float32 (32 bit o 4 byte per parametro), richiederebbe circa 400 MB di memoria solo per l’archiviazione dei pesi del modello. Ciò non tiene conto dei requisiti di memoria aggiuntivi durante il training per l’archiviazione di gradienti, stati dell’ottimizzatore e cache di passaggio forward [in avanti], che possono amplificare ulteriormente l’utilizzo della memoria, potenzialmente mettendo a dura prova le risorse su determinati hardware, in particolare dispositivi edge con capacità di memoria limitata.\nLa scelta della rappresentazione numerica ha un impatto ulteriore sull’utilizzo della memoria e sull’efficienza computazionale. Ad esempio, l’utilizzo di Float64 per i pesi del modello raddoppierebbe i requisiti di memoria rispetto a Float32 e potrebbe potenzialmente aumentare anche il tempo di elaborazione. Per una matrice di peso con dimensioni [1000, 1000], Float64 consumerebbe circa 8 MB di memoria, mentre Float32 la ridurrebbe a circa 4 MB. Pertanto, la selezione di un formato numerico appropriato è fondamentale per ottimizzare sia la memoria che l’efficienza computazionale.\n\n\nComplessità Computazionale\nLa precisione numerica ha un impatto diretto sulla complessità computazionale, influenzando il tempo e le risorse necessarie per eseguire operazioni aritmetiche. Ad esempio, le operazioni che utilizzano Float64 generalmente consumano più risorse computazionali rispetto alle loro controparti Float32 o Float16 (vedere Figura 9.14). Nel regno del ML, dove i modelli potrebbero dover elaborare milioni di operazioni (ad esempio, moltiplicazioni e addizioni in operazioni di matrice durante passaggi in forward e backward), anche piccole differenze nella complessità computazionale per operazione possono aggregarsi in un impatto sostanziale sui tempi di training e inferenza. Come mostrato in Figura 9.15, i modelli quantizzati possono essere molte volte più veloci delle loro versioni non-quantizzate.\n\n\n\n\n\n\nFigura 9.14: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Mark Horowitz, Stanford University.\n\n\n\n\n\n\n\n\n\nFigura 9.15: Velocità di tre diversi modelli in forma normale e quantizzata.\n\n\n\nOltre ai tempi di esecuzione puri, c’è anche una preoccupazione per l’efficienza energetica. Non tutti i calcoli numerici sono creati uguali dal punto di vista dell’hardware sottostante. Alcune operazioni numeriche sono più efficienti dal punto di vista energetico di altre. Ad esempio, Figura 9.16 di seguito mostra che l’addizione di interi è molto più efficiente dal punto di vista energetico della moltiplicazione di interi.\n\n\n\n\n\n\nFigura 9.16: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Isscc (2014).\n\n\nIsscc. 2014. «Computing’s energy problem (and what we can do about it)». https://ieeexplore.ieee.org/document/6757323.\n\n\n\n\nCompatibilità Hardware\nGarantire la compatibilità e le prestazioni ottimizzate su diverse piattaforme hardware è un’altra sfida nella rappresentazione numerica. Hardware diversi, come CPU, GPU, TPU e FPGA, hanno capacità e ottimizzazioni diverse per gestire diverse precisioni numeriche. Ad esempio, alcune GPU potrebbero essere ottimizzate per i calcoli Float32, mentre altre potrebbero fornire accelerazioni per Float16. Sviluppare e ottimizzare modelli ML in grado di sfruttare le capacità numeriche specifiche di hardware diversi, garantendo al contempo che il modello mantenga la sua accuratezza e robustezza, richiede un’attenta considerazione e potenzialmente ulteriori sforzi di sviluppo e test.\n\n\nCompromessi di Precisione e Accuratezza\nIl compromesso tra precisione numerica e accuratezza del modello è una sfida “sfumata” nella rappresentazione numerica. L’utilizzo di numeri a bassa precisione, come Float16, potrebbe risparmiare memoria e velocizzare i calcoli, ma può anche introdurre problemi come errore di quantizzazione e intervallo numerico ridotto. Ad esempio, addestrare un modello con Float16 potrebbe introdurre problemi nella rappresentazione di valori di gradiente molto piccoli, potenzialmente influenzando la convergenza e la stabilità del processo di addestramento. Inoltre, in alcune applicazioni, come simulazioni scientifiche o calcoli finanziari, in cui l’elevata precisione è fondamentale, l’uso di numeri a bassa precisione potrebbe non essere consentito a causa del rischio di accumulare errori significativi.\n\n\nEsempi di Compromessi\nPer comprendere e apprezzare le sfumature, prendiamo in considerazione alcuni esempi di casi d’uso. Attraverso questi, ci renderemo conto che la scelta della rappresentazione numerica non è semplicemente una decisione tecnica, ma strategica, che influenza l’acume predittivo del modello, le sue esigenze computazionali e la sua implementabilità in diversi ambienti computazionali. In questa sezione esamineremo un paio di esempi per comprendere meglio i compromessi con i numeri e come si collegano al mondo reale.\n\nVeicoli Autonomi\nNel dominio dei veicoli autonomi, i modelli ML vengono impiegati per interpretare i dati dei sensori e prendere decisioni in tempo reale. I modelli devono elaborare dati ad alta dimensionalità da vari sensori (ad esempio, LiDAR, telecamere, radar) ed eseguire numerosi calcoli entro un intervallo di tempo limitato per garantire un funzionamento sicuro e reattivo del veicolo. Quindi i compromessi qui includerebbero:\n\nUtilizzo della Memoria: L’archiviazione e l’elaborazione di dati dei sensori ad alta risoluzione, specialmente in formati a virgola mobile, possono consumare una quantità di memoria sostanziale.\nComplessità Computazionale: L’elaborazione in tempo reale richiede calcoli efficienti, in cui numeri di precisione più elevata potrebbero impedire l’esecuzione tempestiva delle azioni di controllo.\n\n\n\nApplicazioni Sanitarie Mobili\nLe applicazioni sanitarie mobili spesso utilizzano modelli ML per attività come il riconoscimento delle attività, il monitoraggio della salute o l’analisi predittiva, operando nell’ambiente con risorse limitate dei dispositivi mobili. I compromessi in questo caso includerebbero:\n\nCompromessi di Precisione e Accuratezza: L’impiego di numeri a bassa precisione per conservare risorse potrebbe influire sull’accuratezza delle previsioni sanitarie o delle rilevazioni di anomalie, il che potrebbe avere implicazioni significative per la salute e la sicurezza degli utenti.\nCompatibilità Hardware: I modelli devono essere ottimizzati per diversi hardware mobili, garantendo un funzionamento efficiente su un’ampia gamma di dispositivi con diverse capacità di calcolo numerico.\n\n\n\nSistemi di Trading ad Alta Frequenza (HFT)\nI sistemi HFT sfruttano i modelli ML per prendere decisioni di trading rapide basate su dati di mercato in tempo reale. Questi sistemi richiedono risposte a bassissima latenza per capitalizzare le opportunità di trading di breve durata.\n\nComplessità Computazionale: I modelli devono elaborare e analizzare vasti flussi di dati di mercato con una latenza minima, dove anche lievi ritardi, potenzialmente introdotti da numeri a precisione più elevata, possono comportare opportunità perse.\nCompromessi di Precisione e Accuratezza: I calcoli finanziari spesso richiedono un’elevata precisione numerica per garantire valutazioni accurate dei prezzi e dei rischi, ponendo sfide nel bilanciamento tra efficienza computazionale e accuratezza numerica.\n\n\n\nSistemi di Sorveglianza Basati su Edge\nI sistemi di sorveglianza distribuiti su dispositivi edge, come le telecamere di sicurezza, utilizzano modelli ML per attività come rilevamento di oggetti, riconoscimento di attività e rilevamento di anomalie, spesso operando con vincoli di risorse rigorosi.\n\nUtilizzo della Memoria: L’archiviazione di modelli pre-addestrati e l’elaborazione di feed video in tempo reale richiedono un utilizzo efficiente della memoria, il che può essere impegnativo con numeri ad alta precisione.\nCompatibilità Hardware: Garantire che i modelli possano funzionare in modo efficiente su dispositivi edge con diverse capacità hardware e ottimizzazioni per diverse precisioni numeriche è fondamentale per una distribuzione diffusa.\n\n\n\nSimulazioni Scientifiche\nI modelli ML vengono sempre più utilizzati nelle simulazioni scientifiche, come la modellazione climatica o le simulazioni di dinamica molecolare, per migliorare le capacità predittive e ridurre le richieste di calcolo.\n\nCompromessi di Precisione e Accuratezza: Le simulazioni scientifiche spesso richiedono un’elevata precisione numerica per garantire risultati accurati e affidabili, il che può entrare in conflitto con il desiderio di ridurre le richieste di calcolo tramite numeri a bassa precisione.\nComplessità Computazionale: I modelli devono gestire ed elaborare dati di simulazione complessi e ad alta dimensionalità in modo efficiente per garantire risultati tempestivi e consentire simulazioni su larga scala o di lunga durata.\n\nQuesti esempi illustrano diversi scenari in cui le sfide della rappresentazione numerica nei modelli ML sono palesemente manifestate. Ogni sistema presenta un set unico di requisiti e vincoli, che richiedono strategie e soluzioni personalizzate per affrontare i problemi dell’utilizzo della memoria, della complessità computazionale, dei compromessi tra precisione e accuratezza e della compatibilità hardware.\n\n\n\n\n9.3.4 Quantizzazione\nLa quantizzazione è prevalente in vari domini scientifici e tecnologici e comporta essenzialmente la mappatura o la limitazione di un set o intervallo continuo in una controparte discreta per ridurre al minimo il numero di bit richiesti.\n\nAnalisi Iniziale\nIniziamo la nostra incursione nella quantizzazione con una breve analisi di un importante utilizzo della quantizzazione.\nNel signal processing [elaborazione del segnale], l’onda sinusoidale continua (mostrata in Figura 9.17) può essere quantizzata in valori discreti tramite un processo noto come campionamento. Questo è un concetto fondamentale nell’elaborazione del segnale digitale ed è cruciale per convertire segnali analogici (come l’onda sinusoidale continua) in una forma digitale che possa essere elaborata dai computer. L’onda sinusoidale è un esempio prevalente grazie alla sua natura periodica e regolare, il che la rende uno strumento utile per spiegare concetti come frequenza, ampiezza, fase e, naturalmente, quantizzazione.\n\n\n\n\n\n\nFigura 9.17: Onda Sinusoidale.\n\n\n\nNella versione quantizzata mostrata in Figura 9.18, l’onda sinusoidale continua (Figura 9.17) viene campionata a intervalli regolari (in questo caso, ogni \\(\\frac{\\pi}{4}\\) radianti) e solo questi valori campionati vengono rappresentati nella versione digitale del segnale. Le linee graduali tra i punti mostrano un modo per rappresentare il segnale quantizzato in una forma costante a tratti. Questo è un esempio semplificato di come funziona la conversione analogico-digitale, in cui un segnale continuo viene mappato su un set discreto di valori, consentendone la rappresentazione e l’elaborazione digitale.\n\n\n\n\n\n\nFigura 9.18: Onda Sinusoidale Quantizzata.\n\n\n\nTornando al contesto del Machine Learning (ML), la quantizzazione si riferisce al processo di limitazione dei possibili valori che i parametri numerici (come pesi e bias) possono assumere in un set discreto, riducendo così la precisione dei parametri e, di conseguenza, l’ingombro di memoria del modello. Se implementata correttamente, la quantizzazione può ridurre le dimensioni del modello fino a 4 volte e migliorare la latenza e la produttività dell’inferenza fino a 2-3 volte. Figura 9.19 illustra l’impatto che la quantizzazione ha sulle dimensioni di modelli diversi: ad esempio, un modello di classificazione delle immagini come ResNet-v2 può essere compresso da 180 MB a 45 MB con quantizzazione a 8 bit. In genere, la perdita di accuratezza del modello è inferiore all’1% con una quantizzazione ben fatta. L’accuratezza può spesso essere recuperata riaddestrando il modello quantizzato con tecniche di addestramento consapevoli della quantizzazione. Pertanto, questa tecnica è emersa come molto importante nell’implementazione di modelli ML in ambienti con risorse limitate, come dispositivi mobili, dispositivi IoT e piattaforme di edge computing, dove le risorse computazionali (memoria e potenza di elaborazione) sono limitate.\n\n\n\n\n\n\nFigura 9.19: Effetto della quantizzazione sulle dimensioni del modello. Fonte: HarvardX.\n\n\n\nEsistono diverse dimensioni della quantizzazione, come uniformità, stocasticità (o determinismo), simmetria, granularità (tra layer/canali/gruppi o persino all’interno dei canali), considerazioni sulla calibrazione dell’intervallo (statico o dinamico) e metodi di messa a punto (QAT, PTQ, ZSQ). Esaminiamo questi di seguito.\n\n\n\n9.3.5 I Tipi\n\nQuantizzazione Uniforme\nLa quantizzazione uniforme implica la mappatura di valori continui o ad alta precisione su una rappresentazione a precisione inferiore utilizzando una scala uniforme. Ciò significa che l’intervallo tra ogni possibile valore quantizzato è coerente. Ad esempio, se i pesi di un layer di rete neurale sono quantizzati su numeri interi a 8 bit (valori tra 0 e 255), un peso con un valore in virgola mobile di 0.56 potrebbe essere mappato su un valore intero di 143, presupponendo una mappatura lineare tra le scale originale e quantizzata. Grazie all’uso di pipeline matematiche intere o a virgola fissa, questa forma di quantizzazione consente il calcolo sul dominio quantizzato senza la necessità di dequantizzare in anticipo.\nIl processo per implementare la quantizzazione uniforme inizia con la scelta di un intervallo di numeri reali da quantizzare. Il passaggio successivo consiste nel selezionare una funzione di quantizzazione e mappare i valori reali sugli interi rappresentabili dalla larghezza di bit della rappresentazione quantizzata. Ad esempio, una scelta popolare per una funzione di quantizzazione è:\n\\[\nQ(r)=Int(r/S) - Z\n\\]\ndove \\(Q\\) è l’operatore di quantizzazione, \\(r\\) è un input a valore reale (nel nostro caso, un’attivazione o un peso), \\(S\\) è un fattore di scala a valore reale e \\(Z\\) è un punto zero intero. La funzione Int mappa un valore reale in un valore intero tramite un’operazione di arrotondamento. Tramite questa funzione, abbiamo mappato in modo efficace i valori reali \\(r\\) in alcuni valori interi, ottenendo livelli quantizzati uniformemente distanziati.\nQuando i professionisti hanno la necessità di recuperare i valori originali di precisione più elevata, i valori reali \\(r\\) possono essere recuperati dai valori quantizzati tramite un’operazione nota come dequantizzazione. Nell’esempio sopra, ciò significherebbe eseguire la seguente operazione sul nostro valore quantizzato:\n\\[\n\\bar{r} = S(Q(r) + Z)\n\\]\nCome discusso, una certa precisione nel valore reale viene persa dalla quantizzazione. In questo caso, il valore recuperato \\(\\bar{r}\\) non corrisponderà esattamente a \\(r\\) a causa dell’operazione di arrotondamento. Questo è un importante compromesso da notare; tuttavia, in molti utilizzi riusciti della quantizzazione, la perdita di precisione può essere trascurabile e l’accuratezza del test rimane elevata. Nonostante ciò, la quantizzazione uniforme continua a essere la scelta di fatto attuale per la sua semplicità e l’efficiente mappatura all’hardware.\n\n\nQuantizzazione Non-Uniforme\nLa quantizzazione non uniforme, d’altro canto, non mantiene un intervallo coerente tra i valori quantizzati. Questo approccio potrebbe essere utilizzato per allocare più possibili valori discreti in regioni in cui i valori dei parametri sono più densamente popolati, preservando così maggiori dettagli dove sono più necessari. Ad esempio, nelle distribuzioni a campana di pesi con lunghe code, un set di pesi in un modello si trova prevalentemente all’interno di un certo intervallo; quindi, più livelli di quantizzazione potrebbero essere assegnati a tale intervallo per preservare dettagli più fini, consentendoci di acquisire meglio le informazioni. Tuttavia, una delle principali debolezze della quantizzazione non uniforme è che richiede la dequantizzazione prima di calcoli di precisione più elevata a causa della sua non uniformità, limitando la sua capacità di accelerare il calcolo rispetto alla quantizzazione uniforme.\nIn genere, una quantizzazione non uniforme basata su regole utilizza una distribuzione logaritmica di passaggi e livelli esponenzialmente crescenti anziché linearmente. Un altra tipologia popolare risiede nella quantizzazione basata su codice binario in cui i vettori di numeri reali vengono quantizzati in vettori binari con un fattore di scala. In particolare, non esiste una soluzione in forma chiusa per minimizzare gli errori tra il valore reale e il valore non uniformemente quantizzato, quindi la maggior parte delle quantizzazioni in questo campo si basa su soluzioni euristiche. Ad esempio, un lavoro recente di Xu et al. (2018) formula la quantizzazione non uniforme come un problema di ottimizzazione in cui i passaggi/livelli di quantizzazione nel quantizzatore \\(Q\\) vengono regolati per ridurre al minimo la differenza tra il tensore originale e la controparte quantizzata.\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, e Hongbin Zha. 2018. «Alternating Multi-bit Quantization for Recurrent Neural Networks». In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\\[\n\\min_Q ||Q(r)-r||^2\n\\]\nInoltre, i quantizzatori addestrabili lo possono essere congiuntamente con parametri di modello e i passaggi/livelli di quantizzazione sono generalmente addestrati con ottimizzazione iterativa o discesa del gradiente. Inoltre, il clustering è stato utilizzato per alleviare la perdita di informazioni dalla quantizzazione. Sebbene in grado di catturare livelli di dettaglio più elevati, gli schemi di quantizzazione non uniformi possono essere difficili da implementare in modo efficiente su hardware di calcolo generale, rendendoli meno preferiti ai metodi che utilizzano la quantizzazione uniforme.\n\n\n\n\n\n\nFigura 9.20: Uniformità della Quantizzazione. Fonte: Gholami et al. (2021).\n\n\n\n\n\nQuantizzazione Stocastica\nA differenza dei due approcci precedenti che generano mappature deterministiche, c’è un po’ di lavoro che esplora l’idea della quantizzazione stocastica per l’addestramento consapevole della quantizzazione e l’addestramento a precisione ridotta. Questo approccio mappa numeri fluttuanti verso l’alto o verso il basso con una probabilità associata alla grandezza dell’aggiornamento del peso. La speranza generata dall’intuizione di alto livello è che un tale approccio probabilistico possa consentire a una rete neurale di esplorare di più, rispetto alla quantizzazione deterministica. Presumibilmente, abilitare un arrotondamento stocastico potrebbe consentire alle reti neurali di sfuggire agli ottimi locali, aggiornando così i propri parametri. Di seguito sono riportati due esempi di funzioni di mappatura stocastica:\n\n\n\n\n\n\n\nFigura 9.21: Funzioni di quantizzazione Intera e Binaria.\n\n\n\n\n\nQuantizzazione “Zero Shot”\nLa quantizzazione Zero-shot si riferisce al processo di conversione di un modello di deep learning a precisione completa direttamente in un modello quantizzato a bassa precisione senza la necessità di alcun riaddestramento o messa a punto sul modello quantizzato. Il vantaggio principale di questo approccio è la sua efficienza, in quanto elimina il processo, spesso dispendioso in termini di tempo e risorse, del riaddestramento post-quantizzazione. Sfruttando tecniche che anticipano e riducono al minimo gli errori di quantizzazione, la quantizzazione zero-shot mantiene l’accuratezza originale del modello anche dopo averne ridotto la precisione numerica. È particolarmente utile per i provider di “Machine Learning as a Service (MLaaS)” che mirano ad accelerare la distribuzione dei carichi di lavoro dei propri clienti senza dover accedere ai loro set di dati.\n\n\n\n9.3.6 Calibrazione\nLa calibrazione è il processo di selezione dell’intervallo di clipping [ritaglio] più efficace [\\(\\alpha\\), \\(\\beta\\)] per pesi e attivazioni da quantizzare. Ad esempio, si consideri la quantizzazione delle attivazioni che originariamente hanno un intervallo in virgola mobile tra -6 e 6 a interi a 8 bit. Prendere solo i valori minimi e massimi possibili di interi a 8 bit (da -128 a 127) come intervallo di quantizzazione, potrebbe non essere il più efficace. Invece, la calibrazione implicherebbe il passaggio di un set di dati rappresentativo e quindi l’utilizzo di questo intervallo osservato per la quantizzazione.\nEsistono molti metodi di calibrazione, ma alcuni comunemente utilizzati includono:\n\nMax: Utilizza il valore assoluto massimo visualizzato durante la calibrazione. Tuttavia, questo metodo è suscettibile di dati anomali. Notare come in Figura 9.22, abbiamo un cluster anomalo intorno a 2.1, mentre il resto è raggruppato attorno a valori più piccoli.\nEntropia: Utilizza la divergenza KL per ridurre al minimo la perdita di informazioni tra i valori originali in virgola mobile e i valori che potrebbero essere rappresentati dal formato quantizzato. Questo è il metodo predefinito utilizzato da TensorRT.\nPercentile: Imposta l’intervallo su un percentile della distribuzione dei valori assoluti osservati durante la calibrazione. Ad esempio, una calibrazione del 99% taglierebbe l’1% dei valori di magnitudine più grandi.\n\n\n\n\n\n\n\nFigura 9.22: Attivazioni di input nel layer 3 in ResNet50. Fonte: @Wu, Judd, e Isaev (2020).\n\n\n\nÈ importante notare che la qualità della calibrazione può fare la differenza tra un modello quantizzato che conserva la maggior parte della sua accuratezza e uno che si degrada in modo significativo. Quindi, è un passaggio essenziale nel processo di quantizzazione. Quando si sceglie un intervallo di calibrazione, ci sono due tipi: simmetrico e asimmetrico.\n\nQuantizzazione Simmetrica\nLa quantizzazione simmetrica mappa i valori reali su un intervallo di clipping simmetrico centrato su 0. Ciò comporta la scelta di un intervallo [\\(\\alpha\\), \\(\\beta\\)] dove \\(\\alpha = -\\beta\\). Ad esempio, un intervallo simmetrico si baserebbe sui valori min/max dei valori reali in modo tale che:\n\\[\n\\alpha = \\beta = max(abs(r_{max}), abs(r_{min}))\n\\]\nGli intervalli di clipping simmetrici sono i più ampiamente adottati nella pratica in quanto hanno il vantaggio di un’implementazione più semplice. In particolare, la mappatura da zero a zero nell’intervallo di clipping (talvolta chiamata “azzeramento del punto zero”) può portare a una riduzione del costo computazionale durante l’inferenza (Wu, Judd, e Isaev 2020).\n\n\nQuantizzazione Asimmetrica\nLa quantizzazione asimmetrica mappa i valori reali in un intervallo di clipping asimmetrico che non è necessariamente centrato sullo 0, come mostrato in Figura 9.23 a destra. Comporta la scelta di un intervallo [\\(\\alpha\\), \\(\\beta\\)] dove \\(\\alpha \\neq -\\beta\\). Ad esempio, selezionando un intervallo basato sui valori reali minimi e massimi, o dove \\(\\alpha = r_{min}\\) and \\(\\beta = r_{max}\\), si crea un intervallo asimmetrico. In genere, la quantizzazione asimmetrica produce intervalli di clipping più stretti rispetto a quella simmetrica, il che è importante quando i pesi e le attivazioni target sono sbilanciati, ad esempio, l’attivazione dopo la ReLU ha sempre valori non negativi. Nonostante produca intervalli di clipping più stretti, la quantizzazione asimmetrica è meno preferita di quella simmetrica in quanto non azzera sempre il valore dello zero reale.\n\n\n\n\n\n\nFigura 9.23: (a)simmetria della Quantizzazione. Fonte: Gholami et al. (2021).\n\n\n\n\n\nGranularità\nDopo aver deciso il tipo di intervallo di clipping, è essenziale restringerlo per consentire a un modello di mantenere la massima accuratezza possibile. Daremo un’occhiata alle reti neurali convoluzionali come nostro modo di esplorare metodi che ottimizzano la granularità degli intervalli di clipping per la quantizzazione. L’attivazione di input di un layer nella nostra CNN subisce una convoluzione con più filtri convoluzionali. Ogni filtro convoluzionale può possedere un intervallo di valori univoco. Si noti come in Figura 9.24 l’intervallo per il Filtro 1 sia molto più piccolo di quello per il Filtro 3. Di conseguenza, una caratteristica distintiva degli approcci di quantizzazione è la precisione con cui l’intervallo di clipping [α,β] viene determinato per i pesi.\n\n\n\n\n\n\nFigura 9.24: Granularità di quantizzazione: intervalli variabili. Fonte: Gholami et al. (2021).\n\n\n\n\nQuantizzazione a Layer: Questo approccio determina l’intervallo di clipping considerando tutti i pesi nei filtri convoluzionali di un layer. Quindi, lo stesso intervallo di clipping viene utilizzato per tutti i filtri convoluzionali. È il più semplice da implementare e, come tale, spesso si traduce in una precisione non ottimale a causa dell’ampia varietà di intervalli diversi tra i filtri. Ad esempio, un kernel convoluzionale con un intervallo di parametri più ristretto perde la sua risoluzione di quantizzazione a causa di un altro kernel nello stesso layer che ha un intervallo più ampio.\nGroupwise Quantization: Questo approccio raggruppa diversi canali all’interno di un layer per calcolare l’intervallo di clipping. Questo metodo può essere utile quando la distribuzione dei parametri su una singola convoluzione/attivazione varia molto. In pratica, questo metodo è stato utile in Q-BERT (Shen et al. 2020) per quantizzare i modelli Transformer (Vaswani et al. 2017) costituiti da layer di attenzione completamente connessi. Lo svantaggio di questo approccio è il costo aggiuntivo di contabilizzazione di diversi fattori di scala.\nChannelwise Quantization: Questo metodo popolare utilizza un intervallo fisso per ogni filtro convoluzionale che è indipendente dagli altri canali. Poiché a ogni canale viene assegnato un fattore di scala dedicato, questo metodo garantisce una risoluzione di quantizzazione più elevata e spesso si traduce in una maggiore accuratezza.\nSub-channelwise Quantization: Portando la quantizzazione canale per canale all’estremo, questo metodo determina l’intervallo di clipping rispetto a qualsiasi gruppo di parametri in una convoluzione o in un layer completamente connesso. Potrebbe comportare un overhead considerevole poiché è necessario tenere conto di diversi fattori di scala quando si elabora una singola convoluzione o un layer completamente connesso.\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, e Kurt Keutzer. 2020. «Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT». Proceedings of the AAAI Conference on Artificial Intelligence 34 (05): 8815–21. https://doi.org/10.1609/aaai.v34i05.6409.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, e Illia Polosukhin. 2017. «Attention is all you need». Adv Neural Inf Process Syst 30.\nTra questi, la quantizzazione canale per canale è lo standard corrente utilizzato per quantizzare i kernel convoluzionali, poiché consente la regolazione degli intervalli di clipping per ogni singolo kernel con overhead trascurabile.\n\n\nQuantizzazione Statica e Dinamica\nDopo aver determinato il tipo e la granularità dell’intervallo di clipping, gli esperti devono decidere quando gli intervalli vengono determinati nei loro algoritmi di calibrazione dell’intervallo. Esistono due approcci per quantizzare le attivazioni: quantizzazione statica e quella dinamica.\nLa quantizzazione statica è l’approccio più frequentemente utilizzato. In questo, l’intervallo di clipping è precalcolato e statico durante l’inferenza. Non aggiunge alcun sovraccarico computazionale, ma, di conseguenza, comporta una minore accuratezza rispetto alla quantizzazione dinamica. Un metodo popolare per implementarlo è eseguire una serie di input di calibrazione per calcolare l’intervallo tipico di attivazioni (Jacob et al. 2018; Yao et al. 2021).\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. «Quantization and training of neural networks for efficient integer-arithmetic-only inference». In Proceedings of the IEEE conference on computer vision and pattern recognition, 2704–13.\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, et al. 2021. «Hawq-v3: Dyadic neural network quantization». In International Conference on Machine Learning, 11875–86. PMLR.\nLa quantizzazione dinamica è un approccio alternativo che calcola dinamicamente l’intervallo per ogni mappa di attivazione durante il runtime. L’approccio richiede calcoli in tempo reale che potrebbero avere un sovraccarico molto elevato. In questo modo, la quantizzazione dinamica spesso raggiunge la massima accuratezza poiché l’intervallo viene calcolato specificamente per ogni input.\nTra i due, il calcolo dell’intervallo in modo dinamico è solitamente molto costoso, quindi la maggior parte dei professionisti utilizzerà spesso la quantizzazione statica.\n\n\n\n9.3.7 Tecniche\nLe due tecniche prevalenti per la quantizzazione dei modelli sono la “Post Training Quantization” e la “Quantization-Aware Training”.\nPost Training Quantization: La quantizzazione post-addestramento (PTQ) è una tecnica di quantizzazione in cui il modello viene quantizzato dopo essere stato addestrato. Il modello viene addestrato in virgola mobile e poi i pesi e le attivazioni vengono quantizzati come fase di post-elaborazione. Questo è l’approccio più semplice e non richiede l’accesso ai dati di addestramento. Diversamente la “Quantization-Aware Training (QAT), PTQ” imposta direttamente i parametri di quantizzazione del peso e dell’attivazione, rendendolo poco costoso e adatto a situazioni con dati limitati o non etichettati. Tuttavia, non riaggiustare i pesi dopo la quantizzazione, specialmente nella quantizzazione a bassa precisione, può portare a un comportamento molto diverso e quindi a una minore accuratezza. Per affrontare questo problema, sono state sviluppate tecniche come la correzione della distorsione, l’equalizzazione degli intervalli di peso e i metodi di arrotondamento adattivo. PTQ può essere applicato anche in scenari zero-shot, in cui non sono disponibili dati di addestramento o di test. Questo metodo è stato reso ancora più efficiente per avvantaggiare modelli linguistici di grandi dimensioni che richiedono molta elaborazione e memoria. Di recente, è stata sviluppata SmoothQuant, una soluzione PTQ senza training, che preserva l’accuratezza ed è di uso generale che consente la quantizzazione di peso a 8 bit e attivazione a 8 bit per LLM, dimostrando un’accelerazione fino a 1.56x e una riduzione della memoria di 2x per LLM con una perdita trascurabile di accuratezza (Xiao et al. 2022).\nIn PTQ, un modello pre-addestrato subisce un processo di calibrazione, come mostrato in Figura 9.25. La calibrazione comporta l’utilizzo di un set di dati separato noto come dati di calibrazione, un sottoinsieme specifico dei dati di training riservato alla quantizzazione per aiutare a trovare gli intervalli di clipping e i fattori di scala appropriati.\n\n\n\n\n\n\nFigura 9.25: Quantizzazione e Calibrazione Post-Training. Fonte: Gholami et al. (2021).\n\n\n\nQuantization-Aware Training: L’addestramento consapevole della quantizzazione (QAT) è una messa a punto del modello PTQ. Il modello viene addestrato in modo consapevole della quantizzazione, consentendogli di adattarsi agli effetti della quantizzazione. Ciò produce una migliore accuratezza con l’inferenza quantizzata. La quantizzazione di un modello di rete neurale addestrato con metodi come PTQ introduce perturbazioni che possono deviare il modello dal suo punto di convergenza originale. Ad esempio, Krishnamoorthi ha dimostrato che anche con la quantizzazione per canale, reti come MobileNet non raggiungono la precisione di base con int8 “Post Training Quantization (PTQ)” e richiedono “Quantization-Aware Training (QAT)” (Krishnamoorthi 2018). Per risolvere questo problema, QAT riaddestra il modello con parametri quantizzati, impiegando passaggi forward e backward in virgola mobile ma quantizzando i parametri dopo ogni aggiornamento del gradiente. La gestione dell’operatore di quantizzazione non differenziabile è fondamentale; un metodo ampiamente utilizzato è lo “Straight Through Estimator (STE)”, che approssima l’operazione di arrotondamento come una funzione identità. Sebbene esistano altri metodi e varianti, STE rimane il più comunemente utilizzato per la sua efficacia pratica. In QAT, un modello pre-addestrato viene quantizzato e poi messo a punto utilizzando i dati di addestramento per regolare i parametri e recuperare il degrado della precisione, come mostrato in Figura 9.26. Il processo di calibrazione viene spesso condotto parallelamente al processo di messa a punto per QAT.\n\n\n\n\n\n\nFigura 9.26: Quantization-Aware Training. Fonte: Gholami et al. (2021).\n\n\nGholami, Dong Kim, Mahoney Yao, e Keutzer. 2021. «A Survey of Quantization Methods for Efficient Neural Network Inference)». ArXiv preprint. https://arxiv.org/abs/2103.13630.\n\n\nLa “Quantization-Aware Training” funge da estensione naturale della “Post-Training Quantization”. Dopo la quantizzazione iniziale eseguita da PTQ, QAT viene utilizzata per perfezionare e mettere a punto ulteriormente i parametri quantizzati: vedere come in Figura 9.27, il modello PTQ subisce un ulteriore passaggio, QAT. Comporta un processo di riqualificazione in cui il modello viene esposto a ulteriori iterazioni di training utilizzando i dati originali. Questo approccio di training dinamico consente al modello di adattare e regolare i suoi parametri, compensando il degrado delle prestazioni causato dalla quantizzazione.\n\n\n\n\n\n\nFigura 9.27: PTQ e QAT. Fonte: «The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training» (s.d.).\n\n\n«The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training». s.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nFigura 9.28 mostra l’accuratezza relativa di diversi modelli dopo PTQ e QAT. In quasi tutti i casi, QAT produce un’accuratezza migliore di PTQ. Si consideri ad esempio EfficientNet b0. Dopo PTQ, l’accuratezza scende dal 76.85% a 72.06%. Ma quando applichiamo QAT, l’accuratezza rimbalza al 76.95% (con persino un leggero miglioramento rispetto all’accuratezza originale).\n\n\n\n\n\n\nFigura 9.28: Accuratezza relativa di PTQ e QAT. Fonte: Wu, Judd, e Isaev (2020).\n\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nPost Training Quantization\nQuantization-Aware Training\nDynamic Quantization\n\n\n\n\nPro\n\n\n\n\n\nSemplicità\n✓\n✗\n✗\n\n\nPreservazione della precisione\n✗\n✓\n✓\n\n\nAdattabilità\n✗\n✗\n✓\n\n\nPrestazioni Ottimizzate\n✗\n✓\nPotenzialmente\n\n\nContro\n\n\n\n\n\nDegrado della Precisione\n✓\n✗\nPotenzialmente\n\n\nSovraccarico Computazionale\n✗\n✓\n✓\n\n\nComplessità di Implementazione\n✗\n✓\n✓\n\n\nCompromessi\n\n\n\n\n\nVelocità vs. Precisione\n✓\n✗\n✗\n\n\nPrecisione vs. Costo\n✗\n✓\n✗\n\n\nAdattabilità vs. Overhead\n✗\n✗\n✓\n\n\n\n\n\n9.3.8 Pesi vs. Attivazioni\nQuantizzazione del peso: Comporta la conversione dei pesi continui o ad alta precisione di un modello in pesi a bassa precisione, come la conversione dei pesi Float32 in pesi INT8 (interi) quantizzati - in Figura 9.29, la quantizzazione del peso avviene nel secondo passaggio (quadrati rossi) quando moltiplichiamo gli input. Ciò riduce le dimensioni del modello, riducendo così la memoria richiesta per archiviare il modello e le risorse computazionali necessarie per eseguire l’inferenza. Ad esempio, si consideri una matrice di pesi in un layer di rete neurale con pesi Float32 come [0.215, -1.432, 0.902, …]. Attraverso la quantizzazione del peso, questi potrebbero essere mappati su valori INT8 come [27, -183, 115, …], riducendo significativamente la memoria richiesta per memorizzarli.\n\n\n\n\n\n\nFigura 9.29: Quantizzazione del peso e dell’attivazione. Fonte: HarvardX.\n\n\n\nQuantizzazione dell’Attivazione: Comporta la quantizzazione dei valori di attivazione (output dei livelli) durante l’inferenza del modello. Ciò può ridurre le risorse computazionali richieste durante l’inferenza, ma introduce ulteriori problemi nel mantenimento dell’accuratezza del modello a causa della ridotta precisione dei calcoli intermedi. Ad esempio, in una rete neurale convoluzionale (CNN), le mappe di attivazione (mappe delle feature) prodotte dai layer convoluzionali, originariamente in Float32, potrebbero essere quantizzate su INT8 durante l’inferenza per accelerare il calcolo, in particolare su hardware ottimizzato per l’aritmetica degli interi. Inoltre, un lavoro recente ha esplorato l’uso della quantizzazione del “Activation-aware Weight Quantization” per la compressione e l’accelerazione LLM, che comporta la protezione di solo l’1% dei pesi salienti più importanti osservando le attivazioni, non i pesi (Lin et al. 2023).\n\n\n9.3.9 Compromessi\nLa quantizzazione introduce invariabilmente un compromesso tra dimensioni/prestazioni del modello e accuratezza. Sebbene riduca significativamente l’ingombro della memoria e possa accelerare l’inferenza, specialmente su hardware ottimizzato per aritmetica a bassa precisione, la precisione ridotta può degradare l’accuratezza del modello.\nDimensioni del Modello: Un modello con pesi rappresentati come Float32 quantizzato a INT8 può teoricamente ridurre le dimensioni del modello di un fattore 4, consentendone l’implementazione su dispositivi con memoria limitata. Le dimensioni di grandi modelli linguistici si stanno sviluppando a un ritmo più veloce della memoria GPU negli ultimi anni, portando a un grande divario tra domanda e offerta di memoria. Figura 9.30 illustra la recente tendenza del divario crescente tra le dimensioni del modello (linea rossa) e la memoria dell’acceleratore (linea gialla). Le tecniche di quantizzazione e compressione del modello possono aiutare a colmare il divario\n\n\n\n\n\n\nFigura 9.30: Dimensioni del modello vs. memoria dell’acceleratore. Fonte: Xiao et al. (2022).\n\n\nXiao, Seznec Lin, Demouth Wu, e Han. 2022. «SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models». ArXiv preprint. https://arxiv.org/abs/2211.10438.\n\n\nVelocità di Inferenza: La quantizzazione può anche accelerare l’inferenza, poiché l’aritmetica a precisione inferiore è computazionalmente meno costosa. Ad esempio, alcuni acceleratori hardware, come Edge TPU di Google, sono ottimizzati per l’aritmetica INT8 e possono eseguire l’inferenza in modo significativamente più rapido con modelli quantizzati INT8 rispetto alle loro controparti in virgola mobile. La riduzione della memoria dalla quantizzazione aiuta a ridurre la quantità di trasmissione dei dati, risparmiando memoria e velocizzando il processo. Figura 9.31 confronta l’aumento della produttività e la riduzione della memoria della larghezza di banda per diversi tipi di dati sulla NVIDIA Turing GPU.\n\n\n\n\n\n\nFigura 9.31: Vantaggi dei tipi di dati a precisione inferiore. Fonte: Wu, Judd, e Isaev (2020).\n\n\nWu, Zhang Judd, e Micikevicius Isaev. 2020. «Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation)». ArXiv preprint. https://arxiv.org/abs/2004.09602.\n\n\nPrecisione: La riduzione della precisione numerica post-quantizzazione può portare a un degrado della precisione del modello, che potrebbe essere accettabile in alcune applicazioni (ad esempio, classificazione delle immagini) ma non in altre (ad esempio, diagnosi medica). Pertanto, dopo la quantizzazione, il modello richiede in genere una ricalibrazione o una messa a punto per mitigare la perdita di accuratezza. Inoltre, un lavoro recente ha esplorato l’uso di Activation-aware Weight Quantization (Lin et al. 2023) che si basa sull’osservazione che proteggere solo l’1% dei pesi salienti può ridurre notevolmente l’errore di quantizzazione.\n\n\n9.3.10 Quantizzazione e Potatura\nPruning [potatura] e quantizzazione funzionano bene insieme ed è stato scoperto che il pruning non ostacola la quantizzazione. In effetti, il pruning può aiutare a ridurre l’errore di quantizzazione. Intuitivamente, ciò è dovuto al pruning che riduce il numero di pesi da quantizzare, riducendo così l’errore accumulato dalla quantizzazione. Ad esempio, una AlexNet non potata ha 60 milioni di pesi da quantizzare mentre una AlexNet potata ha solo 6.7 milioni di pesi da quantizzare. Questa significativa riduzione dei pesi aiuta a ridurre l’errore tra la quantizzazione dell’AlexNet non potato rispetto all’AlexNet potato. Inoltre, studi recenti hanno scoperto che il pruning consapevole della quantizzazione genera modelli più efficienti dal punto di vista computazionale rispetto al pruning o alla quantizzazione da soli; in genere, ha prestazioni simili o migliori in termini di efficienza computazionale rispetto ad altre tecniche di ricerca dell’architettura neurale come l’ottimizzazione bayesiana (Hawks et al. 2021).\n\n\n\n\n\n\nFigura 9.32: Precisione rispetto al tasso di compressione con diversi metodi di compressione. Fonte: Han, Mao, e Dally (2015).\n\n\nHan, Song, Huizi Mao, e William J Dally. 2015. «Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding». arXiv preprint arXiv:1510.00149.\n\n\n\n\n9.3.11 Quantizzazione Edge-aware\nLa quantizzazione non solo riduce le dimensioni del modello, ma consente anche calcoli più rapidi e consuma meno energia, rendendola fondamentale per lo sviluppo per edge. I dispositivi edge in genere hanno vincoli di risorse rigidi con elaborazione, memoria e potenza, impossibili da soddisfare per molti dei modelli deep NN profondi odierni. Inoltre, i processori edge non supportano le operazioni in virgola mobile, rendendo la quantizzazione intera particolarmente importante per chip come GAP-8, un SoC RISC-V per l’inferenza edge con un acceleratore CNN dedicato, che supporta solo l’aritmetica intera.\nUna piattaforma hardware che utilizza la quantizzazione è il gruppo ARM Cortex-M di core di processori ARM RISC a 32 bit. Sfruttano la quantizzazione a virgola fissa con fattori di scala di potenza di due, in modo che la quantizzazione e la de-quantizzazione possano essere eseguite in modo efficiente tramite spostamento di bit. Inoltre, Google Edge TPU, la soluzione emergente di Google per l’esecuzione di inferenze in periferia, è progettata per dispositivi piccoli e a bassa potenza e può supportare solo l’aritmetica a 8 bit. Molti modelli di reti neurali complesse che potevano essere distribuiti solo su server a causa delle loro elevate esigenze di elaborazione possono ora essere eseguiti su dispositivi edge grazie ai recenti progressi (ad esempio metodi di quantizzazione) nel campo dell’edge computing.\nOltre a essere una tecnica indispensabile per molti processori edge, la quantizzazione ha anche apportato notevoli miglioramenti ai processori non edge, incoraggiando tali processori a soddisfare i requisiti del Service Level Agreement (SLA) come la latenza del 99° percentile.\nPertanto, la quantizzazione combinata con una logica efficiente a bassa precisione e acceleratori dedicati di deep learning, è stata una forza trainante cruciale per l’evoluzione di tali processori edge.\nVideo 9.1 è una lezione sulla quantizzazione e sui diversi metodi di quantizzazione.\n\n\n\n\n\n\nVideo 9.1: Quantizzazione",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#sec-model_ops_hw",
    "href": "contents/optimizations/optimizations.it.html#sec-model_ops_hw",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.4 Implementazione Hardware Efficiente",
    "text": "9.4 Implementazione Hardware Efficiente\nL’implementazione hardware efficiente trascende la selezione di componenti adatti; richiede una comprensione olistica di come il software interagirà con le architetture sottostanti. L’essenza del raggiungimento delle massime prestazioni nelle applicazioni TinyML non risiede solo nell’affinare gli algoritmi per l’hardware, ma anche nell’assicurare che l’hardware sia strategicamente adattato per supportare questi algoritmi. Questa sinergia tra hardware e software è fondamentale. Mentre esaminiamo più a fondo le complessità dell’implementazione hardware efficiente, il significato di un approccio di progettazione congiunta, in cui hardware e software vengono sviluppati in tandem, diventa sempre più evidente. Questa sezione fornisce una panoramica delle tecniche di come l’hardware e le interazioni tra hardware e software possono essere ottimizzati per migliorare le prestazioni dei modelli.\n\n9.4.1 Ricerca di Architettura Neurale Basata sull’Hardware\nConcentrarsi solo sulla precisione durante l’esecuzione della ricerca di architettura neurale porta a modelli esponenzialmente complessi e che richiedono memoria e capacità di elaborazione crescenti. Ciò ha portato a vincoli hardware che limitano lo sfruttamento dei modelli di apprendimento profondo al loro pieno potenziale. Progettare manualmente l’architettura del modello è ancora più difficile se si considerano la varietà e le limitazioni dell’hardware. Ciò ha portato alla creazione di Hardware-aware Neural Architecture Search che incorpora le contrazioni hardware nella loro ricerca e ottimizza lo spazio di ricerca per un hardware e una precisione specifici. HW-NAS può essere categorizzato in base a come ottimizza per l’hardware. Esploreremo brevemente queste categorie e lasceremo dei link a documenti correlati per il lettore interessato.\n\nConfigurazione Single Target, Fixed Platfrom\nL’obiettivo qui è trovare la migliore architettura in termini di precisione ed efficienza hardware per un hardware target fisso. Per un hardware specifico, ad esempio Arduino Nicla Vision, questa categoria di HW-NAS cercherà l’architettura che ottimizza precisione, latenza, consumo energetico, ecc.\n\nStrategia di Ricerca Hardware-aware\nQui, la ricerca è un problema di ottimizzazione multi-obiettivo, in cui sia l’accuratezza che il costo dell’hardware guidano l’algoritmo di ricerca per trovare l’architettura più efficiente (Tan et al. 2019; Cai, Zhu, e Han 2019; B. Wu et al. 2019).\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, e Quoc V. Le. 2019. «MnasNet: Platform-aware Neural Architecture Search for Mobile». In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2820–28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\nCai, Han, Ligeng Zhu, e Song Han. 2019. «ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware». In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, e Yangqing Jia. 2019. «FBNet: Hardware-aware Efficient ConvNet Design via Differentiable Neural Architecture Search». In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10734–42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nSpazio di Ricerca Hardware-aware\nQui, lo spazio di ricerca è limitato alle architetture che funzionano bene sull’hardware specifico. Questo può essere ottenuto misurando le prestazioni degli operatori (operatore Conv, operatore Pool, …) o definendo un set di regole che limitano lo spazio di ricerca. (L. L. Zhang et al. 2020)\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, e Yunxin Liu. 2020. «Fast Hardware-Aware Neural Architecture Search». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\n\nConfigurazioni Single Target, Multiple Platform\nAlcuni hardware possono avere configurazioni diverse. Ad esempio, gli FPGA hanno blocchi logici configurabili (CLB) che possono essere configurati dal firmware. Questo metodo consente all’HW-NAS di esplorare diverse configurazioni. (Hu et al. 2023; Ho Yoon et al. 2012)\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song, Jun Yeong Seok, Kyung Jean Yoon, et al. 2012. «Frontiers in Electronic Materials». Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nTarget Multipli\nQuesta categoria mira a ottimizzare un singolo modello per più hardware. Questo può essere utile per lo sviluppo di dispositivi mobili in quanto può ottimizzare diversi modelli di telefoni. (Chu et al. 2021; Hu et al. 2023)\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. «Discovering Multi-Hardware Mobile Models via Architecture Search». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, e Jian Shi. 2023. «Halide Perovskite Semiconductors». Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nEsempi di “Hardware-Aware Neural Architecture Search”\n\nTinyNAS\nTinyNAS adotta un approccio in due fasi per trovare un’architettura ottimale per il modello tenendo a mente i vincoli del microcontrollore specifico.\nInnanzitutto, TinyNAS genera più spazi di ricerca variando la risoluzione di input del modello e il numero di canali dei layer. Quindi, TinyNAS sceglie uno spazio di ricerca in base ai FLOP (operazioni in virgola mobile al secondo) di ogni spazio di ricerca. Gli spazi con una probabilità maggiore di contenere architetture con un numero elevato di FLOP producono modelli con maggiore accuratezza: confrontare la linea rossa con la linea nera in Figura 9.33. Poiché un numero maggiore di FLOP significa che il modello ha una maggiore capacità di calcolo, è più probabile che il modello abbia una maggiore accuratezza.\nPoi, TinyNAS esegue un’operazione di ricerca sullo spazio scelto per trovare l’architettura ottimale per i vincoli specifici del microcontrollore. (J. Lin et al. 2020)\n\n\n\n\n\n\nFigura 9.33: Precisione degli spazi di ricerca. Fonte: J. Lin et al. (2020).\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\n\n\n\nTopology-Aware NAS\nSi concentra sulla creazione e l’ottimizzazione di uno spazio di ricerca allineato alla topologia hardware del dispositivo. (T. Zhang et al. 2020)\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Helen Li, e Yiran Chen. 2020. «AutoShrink: A Topology-Aware NAS for Discovering Efficient Neural Architecture». In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 6829–36. AAAI Press. https://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\n\n9.4.2 Sfide nella “Hardware-Aware Neural Architecture Search”\nSebbene HW-NAS abbia un potenziale elevato per trovare architetture ottimali per TinyML, presenta alcuni problemi. Le metriche hardware come latenza, consumo energetico e utilizzo dell’hardware sono più difficili da valutare rispetto alle metriche di accuratezza o di perdita. Spesso richiedono strumenti specializzati per misure precise. Inoltre, l’aggiunta di tutte queste metriche porta a uno spazio di ricerca molto più grande. Ciò fa sì che HW-NAS sia dispendioso in termini di tempo e denaro. Deve essere applicato a ogni hardware per risultati ottimali, tra le altre cose, il che significa che se si deve distribuire il modello su più dispositivi, la ricerca deve essere condotta più volte e produrrà modelli diversi, a meno che non si ottimizzi per tutti, il che significa una minore accuratezza. Infine, l’hardware cambia frequentemente e potrebbe essere necessario eseguire HW-NAS su ogni versione.\n\n\n9.4.3 Ottimizzazioni del Kernel\nLe ottimizzazioni del kernel sono modifiche apportate al kernel per migliorare le prestazioni dei modelli di apprendimento automatico su dispositivi con risorse limitate. Separeremo le ottimizzazioni del kernel in due tipi.\n\nOttimizzazioni del kernel Generali\nQueste sono ottimizzazioni del kernel da cui tutti i dispositivi possono trarre vantaggio. Forniscono tecniche per convertire il codice in istruzioni più efficienti.\n\n“Srotolamento” del Loop\nInvece di avere un loop con “loop control” (incrementando il contatore, si controlla la condizione di terminazione del loop), il loop può essere srotolato e il sovraccarico del “loop control” può essere omesso. Questo può anche fornire ulteriori opportunità di parallelismo che potrebbero non essere possibili con la struttura con loop. Questo può essere particolarmente utile per loop stretti, in cui il corpo del loop è un piccolo numero di istruzioni con molte iterazioni.\n\n\nBlocking\nIl Blocking viene utilizzato per rendere più efficienti i modelli di accesso alla memoria. Se abbiamo tre calcoli, il primo e l’ultimo devono accedere alla cache A e il secondo deve accedere alla cache B, il “blocking” ferma i primi due calcoli per ridurre il numero di letture di memoria necessarie.\n\n\nTiling\nAnalogamente al blocking, il tiling [piastrellatura] divide i dati e il calcolo in blocchi, ma si estende oltre i miglioramenti della cache. Il tiling crea partizioni di calcolo indipendenti che possono essere eseguite in parallelo, il che può comportare significativi miglioramenti delle prestazioni.\n\n\nLibrerie Kernel Ottimizzate\nQuesto comprende lo sviluppo di kernel ottimizzati che sfruttano appieno un hardware specifico. Un esempio è la libreria CMSIS-NN, che è una raccolta di kernel di reti neurali efficienti sviluppati per ottimizzare le prestazioni e ridurre al minimo l’ingombro di memoria dei modelli sui processori Arm Cortex-M, comuni sui dispositivi edge IoT. Il kernel sfrutta più capacità hardware dei processori Cortex-M come Single Instruction Multiple Data (SIMD), Floating Point Unit (FPU) e M-Profile Vector Extensions (MVE). Queste ottimizzazioni rendono più efficienti le operazioni comuni come le moltiplicazioni di matrici, aumentando le prestazioni delle operazioni del modello sui processori Cortex-M. (Lai, Suda, e Chandra 2018)\n\nLai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. «CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs». https://arxiv.org/abs/1801.06601.\n\n\n\n\n9.4.4 Compute-in-Memory (CiM)\nQuesto è un esempio di progettazione congiunta di algoritmo e hardware. CiM è un paradigma di elaborazione che esegue calcoli all’interno della memoria. Pertanto, le architetture CiM consentono di eseguire operazioni direttamente sui dati archiviati, senza la necessità di spostare i dati avanti e indietro tra unità di elaborazione e memoria separate. Questo paradigma di progettazione è particolarmente utile in scenari in cui lo spostamento dei dati è una fonte primaria di consumo energetico e latenza, come nelle applicazioni TinyML su dispositivi edge. Figura 9.34 è un esempio di utilizzo di CiM in TinyML: l’individuazione delle parole chiave richiede un processo sempre attivo che cerca determinate parole di attivazione (come “Hey, Siri”). Data la natura ad alta intensità di risorse di questa attività, l’integrazione di CiM per il modello di rilevamento delle parole chiave sempre attivo può migliorare l’efficienza.\nAttraverso la progettazione congiunta di algoritmo e hardware, gli algoritmi possono essere ottimizzati per sfruttare le caratteristiche uniche delle architetture CiM e, l’hardware CiM può essere personalizzato o configurato per supportare meglio i requisiti di elaborazione e le caratteristiche degli algoritmi. Ciò si ottiene utilizzando le proprietà analogiche delle celle di memoria, come l’addizione e la moltiplicazione nella DRAM. (Zhou et al. 2021)\n\n\n\n\n\n\nFigura 9.34: CiM per l’individuazione delle parole chiave. Fonte: Zhou et al. (2021).\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat, Xavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian, Manuel Le Gallo, e Paul N. Whatmough. 2021. «AnalogNets: Ml-hw Co-Design of Noise-robust TinyML Models and Always-On Analog Compute-in-Memory Accelerator». https://arxiv.org/abs/2111.06503.\n\n\n\n\n9.4.5 Ottimizzazione dell’Accesso alla Memoria\nDispositivi diversi possono avere gerarchie di memorie diverse. L’ottimizzazione per la gerarchia di memoria specifica nell’hardware specifico può portare a grandi miglioramenti delle prestazioni riducendo le costose operazioni di lettura e scrittura nella memoria. L’ottimizzazione del flusso di dati può essere ottenuta ottimizzando il riutilizzo dei dati all’interno di un singolo layer e tra più layer. Questa ottimizzazione del flusso di dati può essere adattata alla gerarchia di memoria specifica dell’hardware, il che può portare a maggiori vantaggi rispetto alle ottimizzazioni generali per diversi hardware.\n\nSfruttamento dei Dati Sparsi\nIl Pruning [potatura] è un approccio fondamentale per comprimere i modelli e renderli compatibili con dispositivi con risorse limitate. Ciò si traduce in modelli sparsi in cui molti pesi sono 0. Pertanto, sfruttare questa diradazione può portare a miglioramenti significativi nelle prestazioni. Sono stati creati degli strumenti per ottenere esattamente questo. RAMAN, è un acceleratore TinyML sparse progettato per l’inferenza su dispositivi edge. RAMAN sovrappone le attivazioni di input e output sullo stesso spazio di memoria, riducendo i requisiti di archiviazione fino al 50%. (Krishna et al. 2023)\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh Dwivedi, André van Schaik, Mahesh Mehendale, e Chetan Singh Thakur. 2023. «RAMAN: A Re-configurable and Sparse TinyML Accelerator for Inference on Edge». https://arxiv.org/abs/2306.06493.\n\n\nFramework di Ottimizzazione\nI framework di ottimizzazione sono stati introdotti per sfruttare le capacità specifiche dell’hardware per accelerare il software. Un esempio di tale framework è hls4ml: Figura 9.35 fornisce una panoramica del flusso di lavoro del framework. Questo flusso di lavoro di co-progettazione software-hardware open source aiuta a interpretare e tradurre algoritmi di machine learning per l’implementazione con tecnologie FPGA e ASIC. Funzionalità quali ottimizzazione di rete, nuove API Python, potatura consapevole della quantizzazione e flussi di lavoro FPGA end-to-end sono integrate nel framework hls4ml, sfruttando unità di elaborazione parallele, gerarchie di memoria e set di istruzioni specializzati per ottimizzare i modelli per hardware edge. Inoltre, hls4ml è in grado di tradurre algoritmi di apprendimento automatico direttamente nel firmware FPGA.\n\n\n\n\n\n\nFigura 9.35: workflow del framework hls4ml. Fonte: Fahim et al. (2021).\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, et al. 2021. «hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices». https://arxiv.org/abs/2103.05579.\n\n\nUn altro framework per FPGA che si concentra su un approccio olistico è CFU Playground (Prakash et al. 2023)\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. «CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs». In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nHardware Costruito Attorno al Software\nIn un approccio contrastante, l’hardware può essere progettato su misura attorno ai requisiti software per ottimizzare le prestazioni per un’applicazione specifica. Questo paradigma crea hardware specializzato per adattarsi meglio alle specifiche del software, riducendo così il sovraccarico computazionale e migliorando l’efficienza operativa. Un esempio di questo approccio è un’applicazione di riconoscimento vocale di (Kwon e Park 2021). Il documento propone una struttura in cui le operazioni di pre-elaborazione, tradizionalmente gestite dal software, sono assegnate ad un hardware progettato su misura. Questa tecnica è stata ottenuta introducendo la logica resistore-transistor in un modulo audio a circuito inter-integrato per il windowing e l’acquisizione di dati audio grezzi nell’applicazione di riconoscimento vocale. Di conseguenza, questa “delega” delle operazioni di pre-elaborazione ha portato a una riduzione del carico computazionale sul software, mostrando un’applicazione pratica della creazione di hardware attorno al software per migliorare l’efficienza e le prestazioni.\n\n\n\n\n\n\nFigura 9.36: Delega dell’elaborazione dei dati a un FPGA. Fonte: Kwon e Park (2021).\n\n\nKwon, Jisu, e Daejin Park. 2021. «Hardware/Software Co-Design for TinyML Voice-Recognition Application on Resource Frugal Edge Devices». Applied Sciences 11 (22): 11073. https://doi.org/10.3390/app112211073.\n\n\n\n\nSplitNet\nLi SplitNet sono state introdotte nel contesto dei sistemi Head-Mounted. Distribuiscono il carico di lavoro delle Deep Neural Network (DNN) tra i sensori della telecamera e un aggregatore. Ciò è particolarmente interessante nel contesto di TinyML. Il framework SplitNet è un NAS split-aware per trovare l’architettura di rete neurale ottimale per ottenere una buona accuratezza, dividere il modello tra i sensori e l’aggregatore e ridurre al minimo la comunicazione tra i sensori e l’aggregatore. Figura 9.37 dimostra come le SplitNet (in rosso) ottengano una maggiore accuratezza per una latenza inferiore (in esecuzione su ImageNet) rispetto ad altri approcci, come l’esecuzione del DNN sul sensore (All-on-sensor; in verde) o sul cellulare (All-on-aggregator; in blu). La comunicazione minima è importante in TinyML dove la memoria è fortemente limitata, in questo modo i sensori conducono parte dell’elaborazione sui loro chip e poi inviano solo le informazioni necessarie all’aggregatore. Durante i test su ImageNet, SplitNets è stato in grado di ridurre la latenza di un ordine di grandezza sui dispositivi di visione artificiale montati sulla testa [occhiali o visori]. Ciò può essere utile quando il sensore ha il suo chip. (Dong et al. 2022)\n\n\n\n\n\n\nFigura 9.37: Le SplitNet rispetto ad altri approcci. Fonte: Dong et al. (2022).\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T. Kung, e Ziyun Li. 2022. «SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12549–59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\n\n\nHardware Specifico per il “Data Augmentation”\nOgni dispositivo edge può possedere caratteristiche di sensore uniche, che portano a specifici modelli di rumore che possono influire sulle prestazioni del modello. Un esempio sono i dati audio, in cui sono prevalenti le variazioni derivanti dalla scelta del microfono. Applicazioni come le Keyword Spotting possono sperimentare miglioramenti sostanziali incorporando dati registrati da dispositivi simili a quelli destinati all’implementazione. La messa a punto dei modelli esistenti può essere impiegata per adattare i dati in modo preciso alle caratteristiche distintive del sensore.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#supporto-software-e-framework",
    "href": "contents/optimizations/optimizations.it.html#supporto-software-e-framework",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.5 Supporto Software e Framework",
    "text": "9.5 Supporto Software e Framework\nSebbene tutte le tecniche sopra menzionate come pruning, quantizzazione e numeri efficienti siano ben note, rimarrebbero poco pratiche e inaccessibili senza un ampio supporto software. Ad esempio, la quantizzazione diretta di pesi e attivazioni in un modello richiederebbe la modifica manuale della definizione del modello e l’inserimento di operazioni di quantizzazione. Allo stesso modo, la potatura diretta dei pesi del modello richiede la manipolazione dei tensori dei pesi. Tali approcci noiosi diventano impraticabili su larga scala.\nSenza l’ampia innovazione software nei framework, negli strumenti di ottimizzazione e nell’integrazione hardware, la maggior parte di queste tecniche rimarrebbe teorica o praticabile solo per gli esperti. Senza API del framework e automazione per semplificare l’applicazione di queste ottimizzazioni, non verrebbero adottate. Il supporto software le rende accessibili al pubblico e sblocca vantaggi concreti. Inoltre, problemi come la messa a punto degli iperparametri per la potatura, la gestione del compromesso tra dimensioni del modello e accuratezza e la garanzia della compatibilità con i dispositivi target pongono ostacoli che gli sviluppatori devono superare.\n\n9.5.1 API Native di Ottimizzazione\nI principali framework di machine learning come TensorFlow, PyTorch e MXNet forniscono librerie e API per consentire l’applicazione di tecniche comuni di ottimizzazione dei modelli senza richiedere implementazioni personalizzate. Ad esempio, TensorFlow offre il TensorFlow Model Optimization Toolkit che contiene moduli come:\n\nquantization - Applica un training che tiene conto della quantizzazione per convertire i modelli in virgola mobile in una precisione inferiore come int8 con una perdita di accuratezza minima. Gestisce la quantizzazione del peso e dell’attivazione.\nsparsity - Fornisce API di potatura per indurre la “sparsità” e rimuovere connessioni non necessarie in modelli come le reti neurali. Può potare pesi, livelli, ecc.\nclustering - Supporta la compressione del modello raggruppando i pesi per tassi di compressione più elevati.\n\nQueste API consentono agli utenti di abilitare tecniche di ottimizzazione come la quantizzazione e la potatura senza modificare direttamente il codice del modello. È possibile configurare parametri come i tassi di “sparsità” del target, le larghezze di bit di quantizzazione, ecc. Allo stesso modo, PyTorch fornisce torch.quantization per convertire i modelli in rappresentazioni di precisione inferiore. TorchTensor e TorchModule formano le classi di base per il supporto della quantizzazione. Offre inoltre torch.nn.utils.prune per la potatura nativa dei modelli. MXNet offre layer gluon.contrib che aggiungono funzionalità di quantizzazione come l’arrotondamento a punto fisso e l’arrotondamento stocastico di pesi/attivazioni durante l’addestramento. Ciò consente di includere facilmente la quantizzazione nei modelli gluon.\nIl vantaggio principale delle ottimizzazioni integrate è che gli utenti possono applicarle senza dover reimplementare tecniche complesse. Ciò rende i modelli ottimizzati accessibili a un’ampia gamma di professionisti. Garantisce inoltre che le best practice siano seguite basandosi sulla ricerca e sull’esperienza nell’implementazione dei metodi. Man mano che emergono nuove ottimizzazioni, i framework si sforzano di fornire supporto nativo e API ove possibile per abbassare ulteriormente la barriera verso un ML efficiente. La disponibilità di questi strumenti è fondamentale per un’adozione diffusa.\n\n\n9.5.2 Strumenti di Ottimizzazione Automatizzata\nGli strumenti di ottimizzazione automatizzati forniti dai framework possono analizzare i modelli e applicare automaticamente ottimizzazioni come quantizzazione, potatura e fusione degli operatori per rendere il processo più semplice e accessibile senza un’eccessiva messa a punto manuale. In effetti, questo si basa sulla sezione precedente. Ad esempio, TensorFlow fornisce il TensorFlow Model Optimization Toolkit che contiene moduli come:\n\nQuantizationAwareTraining - Quantizza automaticamente pesi e attivazioni in un modello per ridurre la precisione come UINT8 o INT8 con una perdita di accuratezza minima. Inserisce nodi di quantizzazione falsi durante l’addestramento in modo che il modello possa imparare a essere compatibile con la quantizzazione.\nPruning - Rimuove automaticamente le connessioni non necessarie in un modello in base all’analisi dell’importanza del peso. Può potare interi filtri in livelli convoluzionali o “attention head” [teste di attenzione] nei trasformatori. Gestisce il ri-addestramento iterativo per recuperare qualsiasi perdita di accuratezza.\nGraphOptimizer - Applica ottimizzazioni grafiche come la fusione degli operatori per consolidare le operazioni e ridurre la latenza di esecuzione, in particolare per l’inferenza. In Figura 9.38, si può vedere l’originale (Source Graph) a sinistra e come le sue operazioni vengono trasformate (consolidate) a destra. Notare come Block1 in Source Graph abbia 3 passaggi separati (Convolution, BiasAdd e Activation), che vengono poi consolidati insieme in Block1 su Optimized Graph.\n\n\n\n\n\n\n\nFigura 9.38: GraphOptimizer. Fonte: Wess et al. (2020).\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, e Anvesh Nookala. 2020. «ANNETTE: Accurate Neural Network Execution Time Estimation with Stacked Models». IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nQuesti moduli automatizzati richiedono solo all’utente di fornire il modello originale in virgola mobile e di gestire la pipeline di ottimizzazione end-to-end, inclusa qualsiasi riqualificazione per ripristinare la precisione. Anche altri framework come PyTorch offrono un crescente supporto all’automazione, ad esempio tramite torch.quantization.quantize_dynamic. L’ottimizzazione automatizzata rende l’apprendimento automatico efficiente accessibile ai professionisti senza competenze di ottimizzazione.\n\n\n9.5.3 Librerie di Ottimizzazione Hardware\nLibrerie hardware come TensorRT e TensorFlow XLA consentono di ottimizzare i modelli per l’hardware target tramite tecniche di cui abbiamo discusso in precedenza.\n\nQuantizzazione: Ad esempio, TensorRT e TensorFlow Lite supportano entrambi la quantizzazione dei modelli durante la conversione nel loro formato. Ciò fornisce accelerazioni sui SoC mobili con supporto INT8/INT4.\nOttimizzazione del Kernel: ad esempio, TensorRT esegue l’auto-tuning per ottimizzare i kernel CUDA in base all’architettura GPU per ogni layer nel grafo del modello. Ciò estrae la massima produttività.\nFusione degli Operatori: TensorFlow XLA esegue una fusione aggressiva per creare un binario ottimizzato per le TPU. Sui dispositivi mobili, framework come NCNN supportano anche operatori fusi [unificati].\nCodice Specifico per l’Hardware: Le librerie vengono utilizzate per generare codice binario ottimizzato specializzato per l’hardware target. Per esempio, TensorRT usa librerie Nvidia CUDA/cuDNN che sono ottimizzate manualmente per ogni architettura GPU. Questa codifica specifica per hardware è fondamentale per le prestazioni. Sui dispositivi TinyML, questo può significare codice assembly ottimizzato per una CPU Cortex M4, ad esempio. I fornitori forniscono CMSIS-NN e altre librerie.\nOttimizzazioni del Layout dei Dati: Possiamo sfruttare in modo efficiente la gerarchia della memoria di hardware come cache e registri tramite tecniche come riorganizzazione tensore/peso, tiling e riutilizzo. Ad esempio, TensorFlow XLA ottimizza i layout dei buffer per massimizzare l’utilizzo della TPU. Questo aiuta qualsiasi sistema con limiti di memoria.\nOttimizzazione Basata sulla Profilazione: Possiamo usare strumenti di profilazione per identificare i colli di bottiglia. Ad esempio, regolare i livelli di fusione del kernel in base alla profilazione della latenza. Sui SoC mobili, fornitori come Qualcomm forniscono profiler in SNPE per trovare opportunità di ottimizzazione nelle CNN. Questo approccio basato sui dati è importante per le prestazioni.\n\nIntegrando i modelli di framework con queste librerie hardware tramite pipeline di conversione ed esecuzione, gli sviluppatori di ML possono ottenere significativi incrementi di velocità e guadagni di efficienza da ottimizzazioni di basso livello su misura per l’hardware target. La stretta integrazione tra software e hardware è fondamentale per consentire un’implementazione performante delle applicazioni di ML, in particolare su dispositivi mobili e TinyML.\n\n\n9.5.4 Visualizzazione delle Ottimizzazioni\nL’implementazione di tecniche di ottimizzazione del modello senza visibilità degli effetti sul modello può essere impegnativa. Strumenti dedicati o strumenti di visualizzazione possono fornire informazioni critiche e utili sulle modifiche del modello e aiutano a tracciare il processo di ottimizzazione. Consideriamo le ottimizzazioni che abbiamo considerato in precedenza, come la potatura per la “sparsity” [diradazione] e la quantizzazione.\n\nSparsità\nAd esempio, si considerino le ottimizzazioni di sparsity. Gli strumenti di visualizzazione di sparsity possono fornire informazioni critiche sui modelli potati, mappando esattamente quali pesi sono stati rimossi. Ad esempio, le mappe di calore di sparsity possono utilizzare gradienti di colore per indicare la percentuale di pesi potati in ogni layer di una rete neurale. I layer con percentuali di potatura più elevate appaiono più scuri (cfr. Figura 9.39). Questo identifica quali layer sono stati semplificati di più tramite potatura (Souza 2020).\n\n\n\n\n\n\nFigura 9.39: Mappa “termica” della rete sparsa. Fonte: Numenta.\n\n\n\nI grafici di tendenza possono anche tracciare la scarsità nei successivi round di potatura: possono mostrare una rapida potatura iniziale seguita da incrementi più graduali. Il tracciamento della diradazione globale corrente insieme a statistiche come la diradazione media, minima e massima per ogli layer in tabelle o grafici fornisce una panoramica della composizione del modello. Per una rete convoluzionale di esempio, questi strumenti potrebbero rivelare che il primo layer di convoluzione viene potato del 20% mentre quello di classificazione finale viene potato del 70% data la sua ridondanza. La diradazione del modello globale può aumentare dal 10% dopo la potatura iniziale al 40% dopo cinque round.\nRendendo i dati di diradazione visivamente accessibili, i professionisti possono comprendere meglio esattamente come il loro modello viene ottimizzato e quali aree vengono interessate. La visibilità consente loro di mettere a punto e controllare il processo di potatura per una determinata architettura.\nLa visualizzazione della diradazione trasforma la potatura in una tecnica trasparente anziché in un’operazione “black-box”.\n\n\nQuantizzazione\nLa conversione di modelli in precisioni numeriche inferiori tramite quantizzazione introduce errori che possono influire sulla precisione del modello se non vengono monitorati e affrontati correttamente. La visualizzazione delle distribuzioni degli errori di quantizzazione fornisce informazioni preziose sugli effetti dei numeri di precisione ridotti applicati a diverse parti di un modello. Per questo, è possibile generare istogrammi degli errori di quantizzazione per pesi e attivazioni. Questi istogrammi possono rivelare la forma della distribuzione degli errori, se assomigliano a una distribuzione gaussiana o contengono valori anomali e picchi significativi. Figura 9.40 mostra le distribuzioni di diversi metodi di quantizzazione. Valori anomali elevati possono indicare problemi con particolari layer che gestiscono la quantizzazione. Il confronto degli istogrammi tra layer evidenzia eventuali aree problematiche che si distinguono con errori anormalmente elevati.\n\n\n\n\n\n\nFigura 9.40: Errori di Quantizzazione. Fonte: Kuzmin et al. (2022).\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, e Tijmen Blankevoort. 2022. «FP8 Quantization: The Power of the Exponent». https://arxiv.org/abs/2208.09225.\n\n\nLe visualizzazioni di attivazione sono importanti anche per rilevare problemi di overflow. Con la mappatura a colori delle attivazioni prima e dopo la quantizzazione, tutti i valori spinti al di fuori degli intervalli previsti diventano visibili. Ciò rivela problemi di saturazione e troncamento che potrebbero alterare le informazioni che fluiscono attraverso il modello. Il rilevamento di questi errori consente di ricalibrare le attivazioni per evitare la perdita di informazioni (Mandal 2022). Figura 9.41 è una mappatura a colori dei kernel convoluzionali AlexNet.\n\n\n\n\n\n\nFigura 9.41: Mappatura a colori delle attivazioni. Fonte: Krizhevsky, Sutskever, e Hinton (2017).\n\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2017. «ImageNet classification with deep convolutional neural networks». A cura di F. Pereira, C. J. Burges, L. Bottou, e K. Q. Weinberger. Commun. ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\nAltre tecniche, come il tracciamento dell’errore di quantizzazione quadratico medio complessivo a ogni passaggio del processo di addestramento consapevole della quantizzazione, identificano fluttuazioni e divergenze. Picchi improvvisi nel grafico di tracciamento possono indicare punti in cui la quantizzazione sta interrompendo l’addestramento del modello. Il monitoraggio di questa metrica crea intuizione sul comportamento del modello in fase di quantizzazione. Insieme, queste tecniche trasformano la quantizzazione in un processo trasparente. Le intuizioni empiriche consentono ai professionisti di valutare correttamente gli effetti della quantizzazione. Individuano le aree dell’architettura del modello o del processo di training da ricalibrare in base ai problemi di quantizzazione osservati. Ciò aiuta a ottenere modelli quantizzati numericamente stabili e accurati.\nFornire questi dati consente ai professionisti di valutare correttamente l’impatto della quantizzazione e identificare potenziali aree problematiche del modello da ricalibrare o riprogettare per renderlo più adatto alla quantizzazione. Questa analisi empirica sviluppa l’intuizione sul raggiungimento di una quantizzazione ottimale.\nGli strumenti di visualizzazione possono fornire approfondimenti che aiutano i professionisti a comprendere meglio gli effetti delle ottimizzazioni sui loro modelli. La visibilità consente di correggere i problemi in anticipo prima che l’accuratezza o le prestazioni siano influenzate in modo significativo. Aiuta anche ad applicare le ottimizzazioni in modo più efficace per modelli specifici. Queste analisi di ottimizzazione aiutano a sviluppare l’intuizione quando si trasferiscono i modelli a rappresentazioni più efficienti.\n\n\n\n9.5.5 Conversione e Distribuzione del Modello\nUna volta che i modelli sono stati ottimizzati con successo in framework come TensorFlow e PyTorch, sono necessarie piattaforme specializzate di conversione e deployment [distribuzione] del modello per colmare il divario con l’esecuzione sui dispositivi target.\nTensorFlow Lite - La piattaforma di TensorFlow per convertire i modelli in un formato leggero ottimizzato per dispositivi mobili, embedded ed edge. Supporta ottimizzazioni come quantizzazione, fusione del kernel e rimozione di operazioni inutilizzate. I modelli possono essere eseguiti utilizzando kernel TensorFlow Lite ottimizzati sull’hardware del dispositivo. Fondamentale per la distribuzione mobile e TinyML.\nONNX Runtime - Esegue la conversione e l’inferenza per i modelli nel formato “open ONNX”. Fornisce kernel ottimizzati, supporta acceleratori hardware come GPU e distribuzione multipiattaforma dal cloud all’edge. Consente la distribuzione indipendente dal framework. Figura 9.42 è una mappa di interoperabilità ONNX, inclusi i principali framework più diffusi.\n\n\n\n\n\n\nFigura 9.42: Interoperabilità di ONNX. Fonte: TowardsDataScience.\n\n\n\nPyTorch Mobile - Consente l’esecuzione dei modelli PyTorch su iOS e Android convertendoli in rappresentazioni ottimizzate per dispositivi mobili. Fornisce implementazioni mobili efficienti di operazioni come convoluzione e funzioni speciali ottimizzate per hardware mobile.\nQueste piattaforme si integrano con driver hardware, sistemi operativi e librerie di acceleratori sui dispositivi per eseguire modelli in modo efficiente utilizzando l’ottimizzazione hardware. Inoltre, delegano le operazioni ad acceleratori ML dedicati, ove presenti. La disponibilità di queste piattaforme di distribuzione collaudate e robuste colma il divario tra l’ottimizzazione dei modelli nei framework e la distribuzione effettiva su miliardi di dispositivi. Consentono agli utenti di concentrarsi sullo sviluppo del modello anziché sulla creazione di runtime mobili personalizzati. L’innovazione continua per supportare nuovi hardware e ottimizzazioni in queste piattaforme è fondamentale per le ottimizzazioni di ML diffuse.\nFornendo queste pipeline di distribuzione ottimizzate, l’intero flusso di lavoro, dal training al deployment [distribuzione] del dispositivo, può sfruttare le ottimizzazioni del modello per fornire applicazioni ML performanti. Questa infrastruttura software end-to-end ha contribuito a guidare l’adozione di ML sul dispositivo.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#conclusione",
    "href": "contents/optimizations/optimizations.it.html#conclusione",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.6 Conclusione",
    "text": "9.6 Conclusione\nIn questo capitolo abbiamo discusso l’ottimizzazione del modello nell’ambito software-hardware. Ci siamo immersi in una rappresentazione efficiente del modello, dove abbiamo trattato le sfumature della potatura strutturata e non-strutturata e altre tecniche per la compressione del modello come la distillazione della conoscenza e la decomposizione di matrice e tensore. Ci siamo anche immersi brevemente nella progettazione del modello specifico per l’edge a livello di parametri e architettura del modello, esplorando argomenti come modelli specifici per l’edge e NAS basati sull’hardware.\nAbbiamo quindi esplorato rappresentazioni numeriche efficienti, dove abbiamo trattato le basi della matematica, codifiche numeriche e archiviazione, vantaggi della matematica efficiente e le sfumature della rappresentazione numerica con utilizzo della memoria, complessità computazionale, compatibilità hardware e scenari di compromesso. Abbiamo concluso concentrandoci su un elemento fondamentale della matematica efficiente: la quantizzazione, dove abbiamo esaminato la sua storia, calibrazione, tecniche e interazione con la potatura.\nInfine, abbiamo esaminato come possiamo apportare ottimizzazioni specifiche per l’hardware che abbiamo. Abbiamo esplorato come possiamo trovare architetture modello su misura per l’hardware, apportare ottimizzazioni nel kernel per gestire meglio il modello e framework creati per sfruttare al meglio l’hardware. Abbiamo anche esaminato come possiamo fare il contrario e creare hardware attorno al nostro software specifico e abbiamo parlato di come suddividere le reti per l’esecuzione su più processori disponibili sul dispositivo edge.\nComprendendo il quadro completo dei gradi di libertà all’interno dell’ottimizzazione del modello sia lontano che vicino all’hardware e i compromessi da considerare quando si implementano questi metodi, i professionisti possono sviluppare una pipeline più ponderata per comprimere i loro carichi di lavoro sui dispositivi edge.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#sec-model-optimizations-resource",
    "href": "contents/optimizations/optimizations.it.html#sec-model-optimizations-resource",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.7 Risorse",
    "text": "9.7 Risorse\nEcco un elenco curato di risorse per supportare sia gli studenti che gli insegnanti nel loro percorso di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e aggiungeremo nuovi esercizi nel prossimo futuro.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nQuantizzazione:\n\nQuantization: Part 1.\nQuantization: Part 2.\nPost-Training Quantization (PTQ).\nQuantization-Aware Training (QAT).\n\nPruning:\n\nPruning: Part 1.\nPruning: Part 2.\n\nKnowledge Distillation.\nClustering.\nNeural Architecture Search (NAS):\n\nNAS overview.\nNAS: Part 1.\nNAS: Part 2.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 9.1\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 9.1\nEsercizio 9.2\nEsercizio 9.3\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html",
    "href": "contents/hw_acceleration/hw_acceleration.it.html",
    "title": "10  Accelerazione IA",
    "section": "",
    "text": "10.1 Introduzione\nProbabilmente avrete notato la crescente domanda di integrazione dell’apprendimento automatico nei dispositivi di uso quotidiano, come gli smartphone nelle nostre tasche, gli elettrodomestici intelligenti e persino i veicoli autonomi. Portare le funzionalità di ML in questi ambienti del mondo reale è entusiasmante, ma comporta una serie di sfide. A differenza dei potenti server dei data center, questi dispositivi edge hanno risorse di elaborazione limitate, il che rende difficile eseguire modelli complessi in modo efficace.\nL’accelerazione hardware specializzata è la chiave per rendere possibile l’apprendimento automatico ad alte prestazioni su dispositivi edge con risorse limitate. Quando parliamo di accelerazione hardware, ci riferiamo all’uso di chip e architetture personalizzati progettati per gestire il pesante lavoro delle operazioni di ML, alleggerendo il carico del processore principale. Nelle reti neurali, alcune delle attività più impegnative riguardano le moltiplicazioni di matrici durante l’inferenza. Gli acceleratori hardware sono progettati per ottimizzare queste operazioni, spesso offrendo accelerazioni da 10 a 100 volte superiori rispetto alle CPU per uso generico. Questo tipo di accelerazione è ciò che rende fattibile l’esecuzione di modelli di reti neurali avanzate su dispositivi limitati da dimensioni, peso e potenza, e di fare tutto in tempo reale.\nIn questo capitolo, esamineremo più da vicino le diverse tecniche di accelerazione hardware disponibili per l’apprendimento automatico embedded e i compromessi che derivano da ciascuna opzione. L’obiettivo è fornire una solida comprensione di come funzionano queste tecniche, in modo che si possano prendere decisioni informate quando si tratta di scegliere l’hardware giusto e ottimizzare il software. Alla fine, sarete ben equipaggiati per sviluppare capacità di apprendimento automatico ad alte prestazioni su dispositivi edge, anche con i loro vincoli.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#background-e-basi",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#background-e-basi",
    "title": "10  Accelerazione IA",
    "section": "10.2 Background e Basi",
    "text": "10.2 Background e Basi\n\n10.2.1 Background Storico\nLe origini dell’accelerazione hardware risalgono agli anni ’60, con l’avvento dei coprocessori matematici in virgola mobile per eliminare i calcoli dalla CPU principale. Un primo esempio è stato il chip Intel 8087 rilasciato nel 1980 per accelerare le operazioni in virgola mobile per il processore 8086. Ciò ha stabilito la pratica di utilizzare processori specializzati per gestire in modo efficiente carichi di lavoro ad alta intensità di calcolo.\nNegli anni ’90, sono emerse le prime Graphics Processing Units (GPU) [Unità di elaborazione grafica] per elaborare rapidamente pipeline grafiche per rendering e giochi. La GeForce 256 di Nvidia nel 1999 è stata una delle prime GPU programmabili in grado di eseguire algoritmi software personalizzati. Le GPU esemplificano acceleratori a funzione fissa specifici per dominio e si sono evolute in acceleratori programmabili paralleli.\nNegli anni 2000, le GPU sono state applicate all’elaborazione generica in GPGPU. La loro elevata larghezza di banda di memoria e la produttività computazionale le hanno rese adatte a carichi di lavoro ad alta intensità di calcolo. Ciò ha incluso innovazioni nell’uso di GPU per accelerare il training di modelli di deep learning come AlexNet nel 2012.\nNegli ultimi anni, le Tensor Processing Unit (TPU) di Google rappresentano ASIC personalizzati specificamente progettati per la moltiplicazione di matrici nel deep learning. Durante l’inferenza, i loro core tensoriali ottimizzati raggiungono TeraOPS/watt più elevati rispetto a CPU o GPU. L’innovazione continua include tecniche di compressione del modello come pruning e quantizzazione per adattare reti neurali più grandi su dispositivi edge.\nQuesta evoluzione dimostra come l’accelerazione hardware si sia concentrata sulla risoluzione di colli di bottiglia ad alta intensità di calcolo, dalla matematica in virgola mobile alla grafica alla moltiplicazione di matrici per ML. Comprendere questa storia fornisce un contesto cruciale per gli acceleratori AI specializzati odierni.\n\n\n10.2.2 La Necessità di Accelerazione\nL’evoluzione dell’accelerazione hardware è strettamente legata alla storia più ampia dell’informatica. Centrale in questa storia è il ruolo dei transistor, i mattoni fondamentali dell’elettronica moderna. I transistor agiscono come piccoli interruttori che possono accendersi o spegnersi, consentendo i calcoli complessi che guidano tutto, dalle semplici calcolatrici ai modelli avanzati di apprendimento automatico. Nei primi decenni, la progettazione dei chip era governata dalla legge di Moore, che prevedeva che il numero di transistor su un circuito integrato sarebbe raddoppiato approssimativamente ogni due anni, e dal Dennard Scaling, che osservava che man mano che i transistor diventavano più piccoli, le loro prestazioni (velocità) aumentavano, mentre la densità di potenza (potenza per unità di area) rimaneva costante. Queste due leggi sono state mantenute durante l’era single-core. Figura 10.1 mostra le tendenze di diverse metriche dei microprocessori. Come indica la figura, il Dennard Scaling fallisce intorno alla metà degli anni 2000; si noti come la velocità di clock (frequenza) rimanga pressoché costante anche se il numero di transistor continua ad aumentare.\nTuttavia, come descrive Patterson e Hennessy (2016), i vincoli tecnologici alla fine hanno imposto una transizione all’era multicore, con chip contenenti più core di elaborazione per offrire guadagni in termini di prestazioni. Le limitazioni di potenza hanno impedito un ulteriore ridimensionamento, il che ha portato al “silicio scuro” (Dark Silicon), in cui non tutte le aree del chip potevano essere attive simultaneamente (Xiu 2019).\n\nPatterson, David A, e John L Hennessy. 2016. Computer organization and design ARM edition: The hardware software interface. Morgan kaufmann.\n\nXiu, Liming. 2019. «Time Moore: Exploiting Moore’s Law From The Perspective of Time». IEEE Solid-State Circuits Mag. 11 (1): 39–55. https://doi.org/10.1109/mssc.2018.2882285.\n“Dark silicon” si riferisce a parti del chip che non possono essere alimentate simultaneamente a causa di limitazioni termiche e di potenza. In sostanza, con l’aumento della densità dei transistor, la quota del chip che poteva essere utilizzata attivamente senza surriscaldarsi o superare i budget di potenza si è ridotta.\nQuesto fenomeno ha comportato che, sebbene i chip avessero più transistor, non tutti potevano essere operativi simultaneamente, limitando i potenziali guadagni in termini di prestazioni. Questa crisi energetica ha reso necessario un passaggio all’era degli acceleratori, con unità hardware specializzate su misura per attività specifiche per massimizzare l’efficienza. L’esplosione dei carichi di lavoro dell’intelligenza artificiale ha ulteriormente spinto la domanda di acceleratori personalizzati. I fattori abilitanti includevano nuovi linguaggi di programmazione, strumenti software e progressi nella produzione.\n\n\n\n\n\n\nFigura 10.1: Tendenze dei Microprocessori. Fonte: Karl Rupp.\n\n\n\nFondamentalmente, gli acceleratori hardware vengono valutati in base a Prestazioni, Potenza e Area di silicio (PPA); la natura dell’applicazione target, sia essa legata alla memoria o al calcolo, influenza notevolmente la progettazione. Ad esempio, i carichi di lavoro legati alla memoria richiedono un’elevata larghezza di banda e un accesso a bassa latenza, mentre le applicazioni legate al calcolo richiedono la massima produttività di elaborazione.\n\n\n10.2.3 Principi Generali\nLa progettazione di acceleratori hardware specializzati comporta la gestione di compromessi complessi tra prestazioni, efficienza energetica, area di silicio e ottimizzazioni specifiche del carico di lavoro. Questa sezione delinea considerazioni e metodologie fondamentali per raggiungere un equilibrio ottimale in base ai requisiti dell’applicazione e ai vincoli hardware.\n\nPrestazioni entro i Budget di Potenza\nPer capire come raggiungere il giusto equilibrio tra prestazioni e budget di potenza, è importante definire prima alcuni concetti chiave che svolgono un ruolo cruciale in questo processo. Le prestazioni si riferiscono in generale alla capacità complessiva di un sistema di completare efficacemente le attività di calcolo entro determinati vincoli. Uno dei componenti chiave delle prestazioni è il throughput, ovvero la velocità con cui vengono elaborate queste attività, comunemente misurata in “floating point operations per second (FLOPS)” [operazioni in virgola mobile al secondo ] o frame al secondo (FPS). Il throughput dipende fortemente dal parallelismo, ovvero la capacità dell’hardware di eseguire più operazioni contemporaneamente, e dalla frequenza di clock, ovvero la velocità con cui il processore esegue ciclicamente queste operazioni. Un throughput più elevato in genere comporta prestazioni migliori, ma aumenta anche il consumo di energia all’aumentare dell’attività.\nLa semplice massimizzazione del throughput non è sufficiente; anche l’efficienza dell’hardware è importante. L’efficienza è la misura di quante operazioni vengono eseguite per watt di potenza consumata, riflettendo la relazione tra lavoro di calcolo e consumo di energia. In scenari in cui la potenza è un fattore limitante, come nei dispositivi edge, ottenere un’elevata efficienza è fondamentale. Per aiutare a ricordare come questi concetti si interconnettono, considerare le seguenti relazioni:\n\nPrestazioni = Throughput * Efficienza\nThroughput ~= Parallelismo * Frequenza di Clock\nEfficienza = Operazioni / Watt\n\nGli acceleratori hardware mirano a massimizzare le prestazioni entro budget di potenza stabiliti. Ciò richiede un attento bilanciamento del parallelismo, della frequenza di clock del chip, della tensione di esercizio, dell’ottimizzazione del carico di lavoro e di altre tecniche per massimizzare le operazioni per watt.\nAd esempio, le GPU raggiungono un throughput elevato tramite architetture massivamente parallele. Tuttavia, la loro efficienza è inferiore a quella dei circuiti integrati specifici per applicazione (ASIC) personalizzati come il TPU di Google, che ottimizzano per un carico di lavoro specifico.\n\n\nGestione dell’Area e dei Costi del Silicio\nLa dimensione dell’area di un chip ha un impatto diretto sul suo costo di produzione. Per capirne il motivo, è utile conoscere un po’ il processo di produzione.\nI chip vengono creati da grandi e sottili fette di materiale semiconduttore note come wafer. Durante la produzione, ogni wafer viene suddiviso in blocchi più piccoli chiamati “die”, e ogni die contenente i circuiti per un singolo chip. Dopo che il wafer è stato elaborato, viene tagliato in questi singoli die, che vengono poi confezionati per formare i chip finali utilizzati nei dispositivi elettronici.\nI die più grandi richiedono più materiale e sono più inclini a difetti, il che può ridurre la resa, il che significa che vengono prodotti meno chip utilizzabili da ogni wafer. Mentre i produttori possono scalare i progetti combinando più die più piccoli in un singolo pacchetto (pacchetti multi-die), ciò aggiunge complessità e costi al processo di confezionamento e produzione.\nLa quantità di area di silicio necessaria su un die dipende da diversi fattori:\n\nRisorse di Calcolo, ad esempio numero di core, memoria, cache\nNodo del Processo di Produzione, transistor più piccoli consentono una maggiore densità\nModello di Programmazione, acceleratori programmati richiedono maggiore flessibilità\n\nLa progettazione dell’acceleratore implica la compressione delle massime prestazioni entro questi vincoli di area del silicio. Tecniche come la potatura e la compressione aiutano ad adattare modelli più grandi al chip senza superare lo spazio disponibile.\n\n\nOttimizzazioni Specifiche del Carico di Lavoro\nLa progettazione di acceleratori hardware efficaci richiede di adattare l’architettura alle esigenze specifiche del carico di lavoro target. Diversi tipi di carichi di lavoro, che siano in AI, grafica o robotica, hanno caratteristiche uniche che stabiliscono come l’acceleratore dovrebbe essere ottimizzato.\nAlcune delle considerazioni chiave quando si ottimizza l’hardware per carichi di lavoro specifici includono:\n\nMemoria vs Limiti di Calcolo: I carichi di lavoro vincolati alla memoria richiedono una maggiore larghezza di banda di memoria, mentre le app vincolate al calcolo necessitano di un throughput [produttività] aritmetico.\nLocalità dei Dati: Lo spostamento dei dati dovrebbe essere ridotto al minimo per l’efficienza. La memoria vicina al calcolo aiuta.\nOperazioni a Livello di Bit: I tipi di dati a bassa precisione come INT8/INT4 ottimizzano la densità di calcolo.\nParallelismo dei Dati: Più unità di calcolo replicate consentono l’esecuzione parallela.\nPipelining: L’esecuzione sovrapposta delle operazioni aumenta la produttività.\n\nLa comprensione delle caratteristiche del carico di lavoro consente un’accelerazione personalizzata. Ad esempio, le reti neurali convoluzionali utilizzano operazioni di “finestra scorrevole” mappate in modo ottimale su array spaziali di elementi di elaborazione.\nGrazie alla comprensione di questi compromessi architettonici, i progettisti possono prendere decisioni informate sull’architettura dell’acceleratore hardware, assicurandosi che fornisca le migliori prestazioni possibili per l’uso previsto.\n\n\nProgettazione Hardware Sostenibile\nNegli ultimi anni, la sostenibilità dell’IA è diventata una preoccupazione urgente, guidata da due fattori chiave: la scala crescente dei carichi di lavoro dell’IA e il consumo energetico associato.\nInnanzitutto, le dimensioni dei modelli e dei set di dati dell’IA sono cresciute rapidamente. Ad esempio, in base alle tendenze di elaborazione dell’IA di OpenAI, la quantità di elaborazione utilizzata per addestrare modelli all’avanguardia raddoppia ogni 3,5 mesi. Questa crescita esponenziale richiede enormi risorse di elaborazione nei data center.\nIn secondo luogo, l’uso di energia per l’addestramento e l’inferenza dell’IA presenta problemi di sostenibilità. I data center che eseguono applicazioni di IA consumano molta energia, contribuendo a elevate emissioni di carbonio. Si stima che l’addestramento di un grande modello di IA possa avere un’impronta di carbonio di 626.000 libbre di CO2 equivalente, quasi 5 volte le emissioni di un’auto media nel corso della sua vita.\nPer affrontare queste sfide, la progettazione hardware sostenibile si concentra sull’ottimizzazione dell’efficienza energetica senza compromettere le prestazioni. Ciò comporta lo sviluppo di acceleratori specializzati che riducono al minimo il consumo di energia massimizzando al contempo la produttività computazionale.\nParleremo di IA sostenibile in un capitolo successivo, dove ne discuteremo più in dettaglio.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#sec-aihw",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#sec-aihw",
    "title": "10  Accelerazione IA",
    "section": "10.3 Tipi di acceleratori",
    "text": "10.3 Tipi di acceleratori\nGli acceleratori hardware possono assumere molte forme. Possono esistere come widget (come il Neural Engine nel chip Apple M1) o come interi chip appositamente progettati per svolgere molto bene determinate attività. Questa sezione esaminerà i processori per carichi di lavoro di apprendimento automatico lungo lo spettro che va dagli ASIC altamente specializzati alle CPU più generiche.\nCi concentriamo prima sull’hardware personalizzato appositamente progettato per l’intelligenza artificiale per comprendere le ottimizzazioni più estreme possibili quando vengono rimossi i vincoli di progettazione. Questo stabilisce un limite massimo per prestazioni ed efficienza. Poi prendiamo in considerazione progressivamente architetture più programmabili e adattabili, discutendo di GPU e FPGA. Queste fanno compromessi nella personalizzazione per mantenere la flessibilità. Infine, trattiamo le CPU generiche che sacrificano le ottimizzazioni per un carico di lavoro particolare in cambio di una programmabilità versatile tra le applicazioni.\nStrutturando l’analisi lungo questo spettro, miriamo a illustrare i compromessi fondamentali tra utilizzo, efficienza, programmabilità e flessibilità nella progettazione dell’acceleratore. Il punto di equilibrio ottimale dipende dai vincoli e dai requisiti dell’applicazione target. Questa prospettiva dello spettro fornisce un quadro per ragionare sulle scelte hardware per l’apprendimento automatico e sulle capacità richieste a ciascun livello di specializzazione.\nFigura 10.2 illustra la complessa interazione tra flessibilità, prestazioni, diversità funzionale e area di progettazione dell’architettura. Nota come l’ASIC si trovi nell’angolo in basso a destra, con area minima, flessibilità e consumo energetico e prestazioni massime, a causa della sua natura altamente specializzata per l’applicazione. Un compromesso chiave è la diversità funzionale rispetto alle prestazioni: le architetture per uso generico possono servire applicazioni diverse, ma le loro prestazioni applicative sono degradate rispetto alle architetture più personalizzate.\nLa progressione inizia con l’opzione più specializzata, gli ASIC appositamente progettati per l’intelligenza artificiale, per basare la nostra comprensione sulle massime ottimizzazioni possibili prima di espanderci ad architetture più generalizzabili. Questo approccio strutturato chiarisce lo spazio di progettazione dell’acceleratore.\n\n\n\n\n\n\nFigura 10.2: Compromessi di Progettazione. Fonte: El-Rayis (2014).\n\n\nEl-Rayis, A. O. 2014. «Reconfigurable architectures for the next generation of mobile device telecommunications systems». : https://www.researchgate.net/publication/292608967.\n\n\n\n10.3.1 Application-Specific Integrated Circuits (ASIC)\nUn “circuito integrato specifico per applicazione” (ASIC) è un tipo di circuito integrato (IC) progettato su misura per un’applicazione o un carico di lavoro specifico, anziché per un uso generico. A differenza di CPU e GPU, gli ASIC non supportano più applicazioni o carichi di lavoro. Piuttosto, sono ottimizzati per eseguire un singolo compito in modo estremamente efficiente. Google TPU è un esempio di ASIC.\nGli ASIC raggiungono questa efficienza adattando ogni aspetto del design del chip, ovvero le porte logiche sottostanti, i componenti elettronici, l’architettura, la memoria, l’I/O e il processo di produzione, specificamente per l’applicazione target. Questo livello di personalizzazione consente di rimuovere qualsiasi logica o funzionalità non necessaria richiesta per il calcolo generale. Il risultato è un IC che massimizza le prestazioni e l’efficienza energetica sul carico di lavoro desiderato. I guadagni di efficienza derivanti dall’hardware specifico per applicazione sono così sostanziali che queste aziende incentrate sul software dedicano enormi risorse ingegneristiche alla progettazione di ASIC personalizzati.\nL’ascesa di algoritmi di apprendimento automatico più complessi ha reso i vantaggi prestazionali abilitati dall’accelerazione hardware personalizzata un fattore di differenziazione competitiva chiave, anche per le aziende tradizionalmente concentrate sull’ingegneria del software. Gli ASIC sono diventati un investimento ad alta priorità per i principali provider cloud che mirano a offrire un calcolo AI più veloce.\n\nVantaggi\nGrazie alla loro natura personalizzata, gli ASIC offrono vantaggi significativi rispetto ai processori generici come CPU e GPU. I principali vantaggi includono quanto segue.\n\nPrestazioni ed efficienza massimizzate\nIl vantaggio più fondamentale degli ASIC è la massimizzazione delle prestazioni e dell’efficienza energetica personalizzando l’architettura hardware specificamente per l’applicazione target. Ogni transistor e aspetto della progettazione è ottimizzato per il carico di lavoro desiderato: non è necessaria alcuna logica o sovraccarico non necessario per supportare il calcolo generico.\nAd esempio, le Tensor Processing Units (TPU) di Google contengono architetture su misura esattamente per le operazioni di moltiplicazione di matrici utilizzate nelle reti neurali. Per progettare gli ASIC TPU, i team di ingegneria di Google devono definire chiaramente le specifiche del chip, scrivere la descrizione dell’architettura utilizzando linguaggi di descrizione hardware come Verilog, sintetizzare il design per mapparlo sui componenti hardware e posizionare e instradare con cura transistor e collegamenti in base alle regole di progettazione del processo di fabbricazione. Questo complesso processo di progettazione, noto come “very-large-scale integration” (VLSI) [integrazione su larga scala ], consente loro di creare un IC ottimizzato per carichi di lavoro di apprendimento automatico.\nDi conseguenza, gli ASIC TPU raggiungono un’efficienza di oltre un ordine di grandezza superiore nelle operazioni per watt rispetto alle GPU per uso generico sui carichi di lavoro di apprendimento automatico massimizzando le prestazioni e riducendo al minimo il consumo energetico tramite un design hardware full-stack personalizzato.\n\n\nMemoria On-Chip Specializzata\nGli ASIC incorporano memoria on-chip, come SRAM (Static Random Access Memory) e cache specificamente ottimizzate per fornire dati alle unità di elaborazione. La SRAM è un tipo di memoria più veloce e affidabile della DRAM (Dynamic Random Access Memory) perché non deve essere aggiornata periodicamente. Tuttavia, richiede più transistor per bit di dati, il che la rende più ingombrante e costosa da produrre rispetto alla DRAM.\nLa SRAM è ideale per la memoria on-chip, dove la velocità è fondamentale. Il vantaggio di avere grandi quantità di SRAM on-chip ad alta larghezza di banda è che i dati possono essere archiviati vicino agli elementi di elaborazione, consentendo un rapido accesso. Ciò fornisce enormi vantaggi in termini di velocità rispetto all’accesso alla DRAM off-chip, che, sebbene di capacità maggiore, può essere fino a 100 volte più lenta. Ad esempio, il system-on-a-chip M1 di Apple contiene una speciale SRAM a bassa latenza per accelerare le prestazioni del suo hardware di machine learning Neural Engine.\nLa località dei dati e l’ottimizzazione della gerarchia di memoria sono fondamentali per un throughput elevato e un basso consumo energetico. Tabella 10.1 mostra “Numeri che Tutti Dovrebbero Conoscere”, di Jeff Dean.\n\n\n\nTabella 10.1: Confronto della latenza delle operazioni di elaborazione e di rete.\n\n\n\n\n\n\n\n\n\nOperazione\nLatenza\n\n\n\n\nRiferimento alla cache L1\n0,5 ns\n\n\nBranch mispredict\n5 ns\n\n\nRiferimento alla cache L2\n7 ns\n\n\nBlocco/sblocco mutex\n25 ns\n\n\nRiferimento alla memoria principale\n100 ns\n\n\nComprimere 1K byte con Zippy\n3.000 ns (3 us)\n\n\nInviare 1 KB byte su una rete da 1 Gbps\n10.000 ns (10 us)\n\n\nLeggere 4 KB casualmente da SSD\n150.000 ns (150 us)\n\n\nLeggere 1 MB in sequenza dalla memoria\n250.000 ns (250 us)\n\n\nAndata e ritorno all’interno dello stesso data center\n500.000 ns (0,5 ms)\n\n\nLeggere 1 MB in sequenza da SSD\n1.000.000 ns (1 ms)\n\n\nRicerca su disco\n10.000.000 ns (10 ms)\n\n\nLeggere 1 MB in sequenza da disco\n20.000.000 ns (20 ms)\n\n\nInviare un pacchetto CA → Paesi Bassi → CA\n150.000.000 ns (150 ms)\n\n\n\n\n\n\n\n\nTipi di Dati e Operazioni Personalizzati\nA differenza dei processori generici, gli ASIC possono essere progettati per supportare in modo nativo tipi di dati personalizzati come INT4 o bfloat16, ampiamente utilizzati nei modelli di ML. Ad esempio, l’architettura GPU Ampere di Nvidia ha un bfloat16 dedicato ai Tensor Core per accelerare i carichi di lavoro AI. I tipi di dati a bassa precisione consentono una maggiore densità aritmetica e prestazioni. Per ulteriori dettagli fare riferimento a Sezione 8.6. Gli ASIC possono anche incorporare direttamente operazioni non standard negli algoritmi ML come operazioni primitive, ad esempio, il supporto nativo di funzioni di attivazione come ReLU rende l’esecuzione più efficiente.\n\n\nParallelismo Elevato\nLe architetture ASIC possono sfruttare un parallelismo più elevato ottimizzato per il carico di lavoro del target rispetto alle CPU o GPU generiche. Un maggior numero di unità di calcolo personalizzate per l’applicazione significa più operazioni eseguite simultaneamente. Gli ASIC altamente paralleli raggiungono un throughput enorme per carichi di lavoro paralleli di dati come l’inferenza di reti neurali.\n\n\nNodi di Processo Avanzati\nI processi di produzione all’avanguardia consentono di impacchettare più transistor in aree di die più piccole, aumentando la densità. Gli ASIC progettati specificamente per applicazioni ad alto volume possono ammortizzare meglio i costi dei nodi.\n\n\n\nSvantaggi\n\nTempistiche di Progettazione Lunghe\nIl processo di progettazione e validazione di un ASIC può richiedere 2-3 anni. La sintesi dell’architettura utilizzando linguaggi di descrizione hardware, la definizione del layout del chip e la fabbricazione del chip su nodi di processo avanzati comportano lunghi cicli di sviluppo. Ad esempio, per realizzare un chip da 7 nm, i team devono definire attentamente le specifiche, scrivere l’architettura in HDL, sintetizzare le porte logiche, posizionare i componenti, instradare tutte le interconnessioni e finalizzare il layout da inviare per la fabbricazione. Questa “Very Large-Scale Integration (VLSI)” significa che la progettazione e la produzione di ASIC possono tradizionalmente richiedere 2-5 anni.\nCi sono alcuni motivi chiave per cui le lunghe tempistiche di progettazione degli ASIC, spesso 2-3 anni, possono essere difficili per i carichi di lavoro di apprendimento automatico:\n\nGli algoritmi ML si evolvono rapidamente: Nuove architetture di modelli, tecniche di training e ottimizzazioni di rete emergono continuamente. Ad esempio, i Transformers sono diventati estremamente popolari nell’NLP negli ultimi anni. Quando un ASIC termina il tapeout, l’architettura ottimale per un carico di lavoro potrebbe essere cambiata.\nI dataset crescono rapidamente: Gli ASIC progettati per determinate dimensioni di modello o tipi di dati possono diventare sottodimensionati rispetto alla domanda. Ad esempio, i modelli di linguaggio naturale stanno aumentando esponenzialmente con più dati e parametri. Un chip progettato per BERT potrebbe non supportare GPT-3.\nLe applicazioni ML cambiano frequentemente: L’attenzione del settore cambia tra visione artificiale, parlato, NLP, sistemi di raccomandazione, ecc. Un ASIC ottimizzato per la classificazione delle immagini potrebbe avere meno rilevanza in pochi anni.\nCicli di progettazione più rapidi con GPU/FPGA: Gli acceleratori programmabili come le GPU possono adattarsi molto più rapidamente aggiornando le librerie software e i framework. I nuovi algoritmi possono essere implementati senza modifiche hardware.\nEsigenze di time-to-market: Ottenere un vantaggio competitivo in ML richiede di sperimentare e implementare rapidamente nuove idee. Attendere diversi anni per un ASIC è diverso da un’iterazione rapida.\n\nIl ritmo dell’innovazione in ML deve essere adattato meglio alla scala temporale pluriennale per lo sviluppo di ASIC. Sono necessari notevoli sforzi ingegneristici per estendere la durata di vita di ASIC tramite architetture modulari, ridimensionamento dei processi, compressione dei modelli e altre tecniche. Tuttavia, la rapida evoluzione di ML rende l’hardware a funzione fissa una sfida.\n\n\nElevati Costi di Progettazione Non Ricorrenti\nI costi fissi per portare un ASIC dalla progettazione alla produzione ad alto volume possono essere molto dispendiosi in termini di capitale, spesso decine di milioni di dollari. La fabbricazione di fotomaschere per il tape-out dei chip in nodi di processo avanzati, il packaging e il lavoro di progettazione una tantum sono costosi. Ad esempio, un solo tape-out del chip da 7 nm potrebbe costare milioni. L’elevato “non-recurring engineering (NRE)” [investimento di progettazione non ricorrente] riduce la fattibilità dell’ASIC ai casi di utilizzo della produzione ad alto volume in cui il costo iniziale può essere ammortizzato.\n\n\nIntegrazione e Programmazione Complesse\nGli ASIC richiedono un ampio lavoro di integrazione software, inclusi driver, compilatori, supporto del sistema operativo e strumenti di debug. Hanno anche bisogno di competenza nel packaging elettrico e termico. Inoltre, programmare in modo efficiente le architetture ASIC può comportare sfide come il partizionamento del carico di lavoro e la pianificazione su molte unità parallele. La natura personalizzata richiede notevoli sforzi di integrazione per trasformare l’hardware grezzo in acceleratori completamente operativi.\nMentre gli ASIC forniscono enormi guadagni di efficienza nelle applicazioni target adattando ogni aspetto della progettazione hardware a un’attività specifica, la loro natura fissa comporta compromessi in termini di flessibilità e costi di sviluppo rispetto agli acceleratori programmabili, che devono essere soppesati in base all’applicazione.\n\n\n\n\n10.3.2 Field-Programmable Gate Array (FPGA)\nGli FPGA sono circuiti integrati programmabili che possono essere riconfigurati per diverse applicazioni. La loro natura personalizzabile offre vantaggi per accelerare gli algoritmi AI rispetto agli ASIC fissi o alle GPU inflessibili. Mentre Google, Meta e NVIDIA stanno valutando di installare gli ASIC nei data center, Microsoft ha distribuito gli FPGA nei suoi data center (Putnam et al. 2014) nel 2011 per servire in modo efficiente diversi carichi di lavoro.\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei Cao, Xuegong Zhou, et al. 2021. «MRI-based brain tumor segmentation using FPGA-accelerated neural network». BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\nGli FPGA hanno trovato ampia applicazione in vari campi, tra cui l’imaging medico, la robotica e la finanza, dove eccellono nella gestione di attività di machine learning ad alta intensità di calcolo. Nell’imaging medico, un esempio illustrativo è l’applicazione degli FPGA per la segmentazione dei tumori cerebrali, un processo tradizionalmente dispendioso in termini di tempo e soggetto a errori. Rispetto alle implementazioni tradizionali di GPU e CPU, gli FPGA hanno dimostrato rispettivamente miglioramenti delle prestazioni di oltre 5 e 44 volte e guadagni di 11 e 82 volte in termini di efficienza energetica, evidenziando il loro potenziale per applicazioni esigenti (Xiong et al. 2021).\n\nVantaggi\nGli FPGA offrono diversi vantaggi rispetto alle GPU e agli ASIC per accelerare i carichi di lavoro di apprendimento automatico.\n\nFlessibilità Tramite “Reconfigurable Fabric”\nIl vantaggio principale degli FPGA è la capacità di riconfigurare il “fabric” [tessuto] sottostante per implementare architetture personalizzate ottimizzate per diversi modelli, a differenza degli ASIC a funzione fissa. Ad esempio, le società di trading quantitativo utilizzano gli FPGA per accelerare i loro algoritmi perché cambiano frequentemente e il basso costo NRE degli FPGA è più fattibile rispetto acquistare i nuovi ASIC. Figura 10.3 contiene una tabella che confronta tre diversi FPGA.\n\n\n\n\n\n\nFigura 10.3: Confronto di FPGA. Fonte: Gwennap (s.d.).\n\n\nGwennap, Linley. s.d. «Certus-NX Innovates General-Purpose FPGAs».\n\n\nGli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono fornite una quantità base di queste risorse e gli ingegneri programmano i chip compilando il codice HDL in flussi di bit che riorganizzano la struttura in diverse configurazioni. Questo rende gli FPGA adattabili man mano che gli algoritmi evolvono.\nSebbene gli FPGA possano non raggiungere le massime prestazioni ed efficienza degli ASIC specifici per il carico di lavoro, la loro programmabilità offre maggiore flessibilità man mano che gli algoritmi cambiano. Questa adattabilità rende gli FPGA una scelta interessante per accelerare le applicazioni di machine learning in evoluzione.\n\n\nParallelismo e Pipeline Personalizzati\nLe architetture FPGA possono sfruttare il parallelismo spaziale e il pipelining adattando la progettazione hardware per rispecchiare il parallelismo nei modelli ML. Ad esempio, la piattaforma FPGA HARPv2 di Intel suddivide i layer di una rete convoluzionale MNIST su elementi di elaborazione separati per massimizzare la produttività. Sugli FPGA sono possibili anche modelli paralleli unici come le valutazioni di “ensemble” ad albero. Pipeline profonde con buffering e flusso di dati ottimizzati possono essere personalizzate in base alla struttura e ai tipi di dati di ogni modello. Questo livello di parallelismo e pipeline su misura non è fattibile sulle GPU.\n\n\nMemoria On-Chip a Bassa Latenza\nGrandi quantità di memoria on-chip ad alta larghezza di banda consentono l’archiviazione localizzata per pesi e attivazioni. Ad esempio, gli FPGA Xilinx Versal contengono 32 MB di blocchi RAM a bassa latenza e interfacce DDR4 a doppio canale per la memoria esterna. Avvicinare fisicamente la memoria alle unità di elaborazione riduce la latenza di accesso. Ciò fornisce significativi vantaggi di velocità rispetto alle GPU che attraversano PCIe (Peripheral Component Interconnect Express) o altri bus di sistema per raggiungere la memoria GDDR6 off-chip.\n\n\nSupporto Nativo per Bassa Precisione\nUn vantaggio fondamentale degli FPGA è la capacità di implementare in modo nativo qualsiasi larghezza di bit per unità aritmetiche, come INT4 o bfloat16, utilizzate nei modelli ML quantizzati. Ad esempio, gli FPGA Stratix 10 NX di Intel hanno core INT8 dedicati che possono raggiungere fino a 143 INT8 TOPS (Tera Operations Per Second) a ~1 TOPS/W (Tera Operations Per Second per Watt) Intel Stratix 10 NX FPGA. TOPS è una misura di prestazioni simile a FLOPS, ma mentre FLOPS misura calcoli in virgola mobile, TOPS misura il numero di operazioni intere che un sistema può eseguire al secondo. Le larghezze di bit inferiori, come INT8 o INT4, aumentano la densità aritmetica e le prestazioni. Gli FPGA possono persino supportare la sintonizzazione a precisione mista o dinamica in fase di esecuzione.\n\n\n\nSvantaggi\n\nThroughput di Picco Inferiore Rispetto agli ASIC\nGli FPGA non possono eguagliare i numeri di throughput grezzi degli ASIC, personalizzati per un modello e una precisione specifici. I sovraccarichi del “fabric” riconfigurabile rispetto all’hardware a funzione fissa comportano prestazioni di picco inferiori. Ad esempio, i pod TPU v5e consentono di collegare fino a 256 chip con oltre 100 petaOps (Peta Operations Per Second) di prestazioni INT8, mentre gli FPGA possono offrire fino a 143 INT8 TOPS o 286 INT4 TOPS Intel Stratix 10 NX FPGA. PetaOps rappresenta quadrilioni di operazioni al secondo, mentre TOPS misura trilioni, evidenziando la capacità di throughput molto maggiore dei pod TPU rispetto agli FPGA.\nQuesto perché gli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono forniti con una quantità stabilita di queste risorse. Per programmare gli FPGA, gli ingegneri scrivono codice HDL e lo compilano in flussi di bit che riorganizzano il “fabric”, che ha sovraccarichi intrinseci rispetto a un ASIC appositamente progettato per un calcolo.\n\n\nComplessità di Programmazione\nPer ottimizzare le prestazioni FPGA, gli ingegneri devono programmare le architetture in linguaggi di descrizione hardware di basso livello come Verilog o VHDL. Ciò richiede competenza nella progettazione hardware e cicli di sviluppo più lunghi rispetto a framework software di livello superiore come TensorFlow. Massimizzare l’utilizzo può essere difficile nonostante i progressi nella sintesi di alto livello da C/C++.\n\n\nSovraccarichi di Riconfigurazione\nLa modifica delle configurazioni FPGA richiede il ricaricamento di un nuovo flusso di bit, che ha costi di latenza e dimensioni di archiviazione considerevoli. Ad esempio, la riconfigurazione parziale su FPGA Xilinx può richiedere centinaia di millisecondi. Questo rende impossibile lo scambio dinamico di architetture in tempo reale. L’archiviazione del flusso di bit consuma anche memoria on-chip.\n\n\nGuadagni in diminuzione sui nodi avanzati\nSebbene i nodi di processo più piccoli siano molto vantaggiosi per gli ASIC, offrono meno vantaggi per gli FPGA. A 7 nm e al di sotto, effetti come variazione di processo, vincoli termici e invecchiamento hanno un impatto sproporzionato sulle prestazioni degli FPGA. Anche le spese generali della struttura configurabile riducono i guadagni rispetto agli ASIC a funzione fissa.\n\n\n\n\n10.3.3 Digital Signal Processor (DSP)\nIl primo core di elaborazione del segnale digitale è stato costruito nel 1948 da Texas Instruments (The Evolution of Audio DSPs). Tradizionalmente, i DSP avrebbero avuto una logica per accedere direttamente ai dati digitali/audio nella memoria, eseguire un’operazione aritmetica (moltiplica-addiziona-accumula-MAC era una delle operazioni più comuni) e quindi scrivere il risultato nella memoria. Il DSP avrebbe incluso componenti analogici specializzati per recuperare i dati digitali/audio.\nUna volta entrati nell’era degli smartphone, i DSP hanno iniziato a comprendere attività più sofisticate. Richiedevano Bluetooth, Wi-Fi e connettività cellulare. Anche i media sono diventati molto più complessi. Oggi, è raro avere chip interi dedicati solo al DSP, ma un System on Chip includerebbe DSP e CPU per uso generico. Ad esempio, l’Hexagon Digital Signal Processor di Qualcomm afferma di essere un “processore di livello mondiale con funzionalità sia CPU che DSP per supportare le esigenze di elaborazione profondamente integrate della piattaforma mobile per funzioni sia multimediali che modem”. Google Tensors, il chip nei telefoni Google Pixel, include anche CPU e motori DSP specializzati.\n\nVantaggi\nI DSP offrono vantaggi architettonici in termini di throughput della matematica vettoriale, accesso alla memoria a bassa latenza, efficienza energetica e supporto per diversi tipi di dati, rendendoli adatti all’accelerazione ML embedded.\n\nArchitettura Ottimizzata per la Matematica Vettoriale\nI DSP contengono percorsi dati specializzati, file di registro e istruzioni ottimizzati specificamente per le operazioni di matematica vettoriale comunemente utilizzate nei modelli di apprendimento automatico. Ciò include motori di prodotto scalare, unità MAC e funzionalità SIMD su misura per calcoli vettoriali/matriciali. Ad esempio, il DSP CEVA-XM6 (“Ceva SensPro fonde AI e Vector DSP”) ha unità vettoriali a 512 bit per accelerare le convoluzioni. Questa efficienza sui carichi di lavoro di matematica vettoriale va ben oltre le CPU generiche.\n\n\nMemoria On-Chip a Bassa Latenza\nI DSP integrano grandi quantità di memoria SRAM veloce su chip per conservare i dati localmente per l’elaborazione. Avvicinare fisicamente la memoria alle unità di calcolo riduce la latenza di accesso. Ad esempio, il DSP SHARC+ di Analog contiene 10 MB di SRAM su chip. Questa memoria locale ad alta larghezza di banda offre vantaggi di velocità per le applicazioni in tempo reale.\n\n\nEfficienza Energetica\nI DSP sono progettati per fornire elevate prestazioni per watt su carichi di lavoro di segnali digitali. Percorsi dati efficienti, parallelismo e architetture di memoria consentono trilioni di operazioni matematiche al secondo entro budget di potenza mobili ridotti. Ad esempio, il DSP Hexagon di Qualcomm può fornire 4 trilioni di operazioni al secondo (TOPS) consumando watt minimi.\n\n\nSupporto per Matematica a Virgola Mobile e Intera\nA differenza delle GPU che eccellono in precisione singola o dimezzata, i DSP possono supportare nativamente tipi di dati a virgola mobile e intera a 8/16 bit utilizzati nei modelli ML. Alcuni DSP supportano l’accelerazione del prodotto scalare a precisione INT8 per reti neurali quantizzate.\n\n\n\nSvantaggi\nI DSP fanno compromessi architettonici che limitano il throughput di picco, la precisione e la capacità del modello rispetto ad altri acceleratori AI. Tuttavia, i loro vantaggi in termini di efficienza energetica e matematica intera li rendono una valida opzione di edge computing. Quindi, mentre i DSP offrono alcuni vantaggi rispetto alle CPU, presentano anche delle limitazioni per i carichi di lavoro di apprendimento automatico:\n\nThroughput di Picco Inferiore Rispetto ad ASIC/GPU\nI DSP non possono eguagliare il throughput computazionale grezzo delle GPU o degli ASIC personalizzati progettati specificamente per l’apprendimento automatico. Ad esempio, l’ASIC Cloud AI 100 di Qualcomm fornisce 480 TOPS su INT8, mentre il loro DSP Hexagon fornisce 10 TOPS. I DSP non hanno il massiccio parallelismo delle unità GPU SM.\n\n\nPrestazioni a Doppia Precisione più Lente\nLa maggior parte dei DSP deve essere ottimizzata per la virgola mobile di precisione più elevata necessaria in alcuni modelli ML. I loro motori di prodotto scalare si concentrano su INT8/16 e FP32, che forniscono una migliore efficienza energetica. Tuttavia, la produttività in virgola mobile a 64 bit è molto più bassa, il che può limitare l’utilizzo nei modelli che richiedono un’elevata precisione.\n\n\nCapacità del Modello Limitata\nLa limitata memoria on-chip dei DSP limita le dimensioni del modello che possono eseguire. Grandi modelli di deep learning con centinaia di megabyte di parametri supererebbero la capacità delle SRAM on-chip. I DSP sono più adatti per modelli di piccole e medie dimensioni destinati a dispositivi edge.\n\n\nComplessità di Programmazione\nLa programmazione efficiente delle architetture DSP richiede competenza nella programmazione parallela e nell’ottimizzazione dei modelli di accesso ai dati. Le loro microarchitetture specializzate hanno una curva di apprendimento più ripida rispetto ai framework software di alto livello, rendendo lo sviluppo più complesso.\n\n\n\n\n10.3.4 Graphics Processing Unit (GPU)\nIl termine “graphics processing unit” [unità di elaborazione grafica] esiste almeno dagli anni ’80. C’è sempre stata una richiesta di hardware grafico nelle console per videogiochi (elevata richiesta, doveva avere un costo relativamente basso) e nelle simulazioni scientifiche (richiesta inferiore, ma risoluzione più alta, poteva avere un prezzo elevato).\nIl termine è stato reso popolare, tuttavia, nel 1999 quando NVIDIA ha lanciato la GeForce 256, mirando principalmente al settore di mercato dei giochi per PC (Lindholm et al. 2008). Man mano che i giochi per PC diventavano più sofisticati, le GPU NVIDIA diventavano più programmabili. Presto, gli utenti si resero conto che potevano sfruttare questa programmabilità, eseguire vari carichi di lavoro non correlati alla grafica sulle GPU e trarre vantaggio dall’architettura sottostante. E così, alla fine degli anni 2000, le GPU divennero unità di elaborazione grafica per uso generale o GP-GPU.\n\nLindholm, Erik, John Nickolls, Stuart Oberman, e John Montrym. 2008. «NVIDIA Tesla: A Unified Graphics and Computing Architecture». IEEE Micro 28 (2): 39–55. https://doi.org/10.1109/mm.2008.31.\nIn seguito a questo cambiamento, altri importanti attori come Intel con la sua Arc Graphics e AMD con la sua serie Radeon RX hanno anche evoluto le loro GPU per supportare una gamma più ampia di applicazioni oltre al rendering grafico tradizionale. Questa espansione delle capacità delle GPU ha aperto nuove possibilità, in particolare nei campi che richiedono un’enorme potenza di calcolo.\nUn esempio lampante di questo potenziale è la recente ricerca rivoluzionaria condotta da OpenAI (Brown et al. 2020) con GPT-3, un modello di linguaggio con 175 miliardi di parametri. L’addestramento di un modello così massiccio, che avrebbe richiesto mesi su CPU convenzionali, è stato completato in pochi giorni utilizzando potenti GPU, dimostrando l’impatto trasformativo delle GPU nell’accelerazione di complesse attività di apprendimento automatico.\n\nVantaggi\n\nElevata Capacità di Elaborazione\nIl vantaggio principale delle GPU è la loro capacità di eseguire calcoli in virgola mobile paralleli massivi ottimizzati per la computer grafica e l’algebra lineare (Raina, Madhavan, e Ng 2009). Le GPU moderne come la A100 di Nvidia offrono fino a 19,5 teraflop di prestazioni FP32 con 6912 core CUDA e 40 GB di memoria grafica strettamente accoppiati a 1,6 TB/s di larghezza di banda della memoria grafica.\n\nRaina, Rajat, Anand Madhavan, e Andrew Y. Ng. 2009. «Large-scale deep unsupervised learning using graphics processors». In Proceedings of the 26th Annual International Conference on Machine Learning, a cura di Andrea Pohoreckyj Danyluk, Léon Bottou, e Michael L. Littman, 382:873–80. ACM International Conference Proceeding Series. ACM. https://doi.org/10.1145/1553374.1553486.\nQuesta capacità di elaborazione grezza deriva dall’architettura “Streaming Multiprocessor” (SM) altamente parallela, pensata per carichi di lavoro paralleli ai dati (Zhihao Jia, Zaharia, e Aiken 2019). Ogni SM contiene centinaia di core scalari ottimizzati per la matematica float32/64. Con migliaia di SM su un chip, le GPU sono appositamente progettate per la moltiplicazione di matrici e le operazioni vettoriali utilizzate in tutte le reti neurali.\nAd esempio, l’ultima GPU H100 di Nvidia fornisce 4000 TFLOP di FP8, 2000 TFLOP di FP16, 1000 TFLOP di TF32, 67 TFLOP di FP32 e 34 TFLOP di prestazioni di elaborazione FP64, che possono accelerare notevolmente l’addestramento di grandi batch su modelli come BERT, GPT-3 e altre architetture di trasformatori. Il parallelismo scalabile delle GPU è fondamentale per accelerare il deep learning computazionalmente intensivo.\n\n\nEcosistema Software Maturo\nNvidia fornisce ampie librerie di runtime come cuDNN e cuBLAS che sono altamente ottimizzate per primitive di deep learning. Framework come TensorFlow e PyTorch si integrano con queste librerie per abilitare l’accelerazione GPU senza programmazione diretta. Queste librerie sono basate su CUDA, la piattaforma di elaborazione parallela e il modello di programmazione di Nvidia.\nCUDA (Compute Unified Device Architecture) è il framework sottostante che consente a queste librerie di alto livello di interagire con l’hardware della GPU. Fornisce agli sviluppatori un accesso di basso livello alle risorse della GPU, consentendo calcoli e ottimizzazioni personalizzate che sfruttano appieno le capacità di elaborazione parallela della GPU. Utilizzando CUDA, gli sviluppatori possono scrivere software che sfruttano l’architettura della GPU per attività di elaborazione ad alte prestazioni.\nQuesto ecosistema consente di sfruttare rapidamente le GPU ad alto livello tramite Python senza competenze di programmazione GPU. Flussi di lavoro e astrazioni noti forniscono una comoda rampa di accesso per scalare gli esperimenti di deep learning. La maturità del software integra i vantaggi della produttività.\n\n\nAmpia Disponibilità\nLe economie di scala dell’elaborazione grafica rendono le GPU ampiamente accessibili nei data center, nelle piattaforme cloud come AWS e GCP e nelle workstation desktop. La loro disponibilità negli ambienti di ricerca ha fornito una comoda piattaforma di sperimentazione e innovazione nel ML. Ad esempio, quasi tutti i risultati di deep learning all’avanguardia hanno coinvolto l’accelerazione GPU per merito di questa ubiquità. L’ampio accesso integra la maturità del software per rendere le GPU l’acceleratore ML standard.\n\n\nArchitettura Programmabile\nSebbene non siano flessibili come gli FPGA, le GPU offrono programmabilità tramite linguaggi CUDA e shader per personalizzare i calcoli. Gli sviluppatori possono ottimizzare i modelli di accesso ai dati, creare nuove operazioni e regolare le precisioni per modelli e algoritmi in evoluzione.\n\n\n\nSvantaggi\nSebbene le GPU siano diventate l’acceleratore standard per il deep learning, la loro architettura presenta alcuni svantaggi importanti.\n\nMeno Efficienti degli ASIC Custom\nL’affermazione “Le GPU sono meno efficienti degli ASIC” potrebbe scatenare un acceso dibattito nel campo ML/AI e far esplodere questo libro.\nIn genere, le GPU sono percepite come meno efficienti degli ASIC perché questi ultimi sono realizzati su misura per attività specifiche e quindi possono funzionare in modo più efficiente nativamente. Con la loro architettura generica, le GPU sono intrinsecamente più versatili e programmabili, soddisfacendo un ampio spettro di attività computazionali oltre a ML/AI.\nTuttavia, le GPU moderne si sono evolute per includere un supporto hardware specializzato per operazioni AI essenziali, come la moltiplicazione di matrici generalizzata (GEMM) e altre operazioni di matrice, supporto nativo per la quantizzazione e supporto nativo per la potatura, che sono fondamentali per l’esecuzione efficace dei modelli ML. Questi miglioramenti hanno notevolmente migliorato l’efficienza delle GPU per le attività AI al punto che possono rivaleggiare con le prestazioni degli ASIC per determinate applicazioni.\nDi conseguenza, le GPU contemporanee sono convergenti, incorporando capacità specializzate simili ad ASIC all’interno di un framework di elaborazione flessibile e di uso generale. Questa adattabilità ha offuscato i confini tra i due tipi di hardware. Le GPU offrono un forte equilibrio tra specializzazione e programmabilità che si adatta bene alle esigenze dinamiche della ricerca e sviluppo ML/AI.\n\n\nElevate Esigenze di Larghezza di Banda di Memoria\nL’architettura massicciamente parallela richiede un’enorme larghezza di banda di memoria per alimentare migliaia di core. Ad esempio, la GPU Nvidia A100 richiede 1.6 TB/sec per saturare completamente il suo computer. Le GPU si affidano ad ampi bus di memoria a 384 bit per RAM GDDR6 ad alta larghezza di banda, ma anche la GDDR6 più veloce raggiunge il massimo a circa 1 TB/sec. Questa dipendenza dalla DRAM esterna comporta latenza e sovraccarico di potenza.\n\n\nComplessità di Programmazione\nSebbene strumenti come CUDA siano utili, la mappatura e il partizionamento ottimali dei carichi di lavoro ML nell’architettura GPU massivamente parallela rimangono una sfida, il raggiungimento di un utilizzo elevato e della località della memoria richiede una messa a punto di basso livello (Zhe Jia et al. 2018). Astrazioni come TensorFlow possono tralasciare le prestazioni.\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, e Daniele P. Scarpazza. 2018. «Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking». ArXiv preprint. https://arxiv.org/abs/1804.06826.\n\n\nMemoria On-Chip Limitata\nLe GPU hanno cache di memoria on-chip relativamente piccole rispetto ai grandi requisiti di working set dei modelli ML durante l’addestramento. Si basano su un accesso ad alta larghezza di banda alla DRAM esterna, che gli ASIC riducono al minimo con una grande SRAM on-chip.\n\n\nArchitettura Fissa\nA differenza degli FPGA, l’architettura fondamentale della GPU non può essere modificata dopo la produzione. Questo vincolo limita l’adattamento a nuovi carichi di lavoro o layer ML. Il confine CPU-GPU crea anche overhead di spostamento dei dati.\n\n\n\n\n10.3.5 Central Processing Unit (CPU)\nIl termine CPU ha una lunga storia che risale al 1955 (Weik 1955) mentre la prima CPU a microprocessore, l’Intel 4004, è stata inventata nel 1971 (Chi ha inventato il microprocessore?). I compilatori traducono linguaggi di programmazione di alto livello come Python, Java o C per assemblare istruzioni (x86, ARM, RISC-V, ecc.) che le CPU devono elaborare. Il set di istruzioni che una CPU comprende è chiamato “instruction set architecture” (ISA), che definisce i comandi che il processore può eseguire direttamente. Deve essere concordato sia dall’hardware che dal software ci gira sopra.\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital Computing Systems. Ballistic Research Laboratories.\nUna panoramica degli sviluppi significativi nelle CPU:\n\nEra del Single-core (anni ’50-2000): Questa era è nota per i miglioramenti microarchitettonici aggressivi. Tecniche come l’esecuzione speculativa (esecuzione di un’istruzione prima che quella precedente fosse finita), “out-of-order execution” [esecuzione fuori ordine] (riordinamento delle istruzioni per renderle più efficaci) e “wider issue widths” [larghezze di emissione più ampie] (esecuzione di più istruzioni contemporaneamente) sono state implementate per aumentare la produttività delle istruzioni. Anche il termine “System on Chip” ha avuto origine in questa era, poiché diversi componenti analogici (componenti progettati con transistor) e componenti digitali (componenti progettati con linguaggi di descrizione hardware mappati su transistor) sono stati inseriti sulla stessa piattaforma per realizzare un’attività.\nEra Multicore (anni 2000): Guidata dalla diminuzione della legge di Moore, questa è caratterizzata dall’aumento del numero di core all’interno di una CPU. Ora, le attività possono essere suddivise su più core diversi, ognuno con il proprio percorso dati e unità di controllo. Molti dei problemi di quest’epoca riguardavano come condividere determinate risorse, quali risorse condividere e come mantenere coerenza e consistenza in tutti i core.\nUn Mare di acceleratori (anni 2010): Ancora una volta, spinta dalla diminuzione della legge di Moore, quest’epoca è caratterizzata dal delegare le attività più complicate su acceleratori (widget) collegati al datapath principale nelle CPU. È comune vedere acceleratori dedicati a vari carichi di lavoro di intelligenza artificiale, nonché elaborazione di immagini/digitali e crittografia. In queste progettazioni, le CPU sono spesso descritte più come giudici, che decidono quali attività devono essere elaborate piuttosto che eseguire l’elaborazione stessa. Qualsiasi attività potrebbe comunque essere eseguita sulla CPU anziché sugli acceleratori, ma la CPU sarebbe generalmente più lenta. Tuttavia, il costo di progettazione e programmazione dell’acceleratore è diventato un ostacolo non banale che ha suscitato interesse per le librerie specifiche per la progettazione (DSL).\nPresenza nei data center: Sebbene sentiamo spesso dire che le GPU dominano il mercato dei data center, le CPU sono comunque adatte per attività che non possiedono intrinsecamente un elevato grado di parallelismo. Le CPU spesso gestiscono attività seriali e di piccole dimensioni e coordinano il data center.\nSull’edge: Dati i vincoli più rigidi sulle risorse sull’edge, le CPU edge spesso implementano solo un sottoinsieme delle tecniche sviluppate nell’era single-core perché queste ottimizzazioni tendono a essere pesanti in termini di consumo di energia e area. Le CPU edge mantengono comunque un datapath relativamente semplice con capacità di memoria limitate.\n\nTradizionalmente, le CPU sono state sinonimo di elaborazione generica, un termine che è cambiato anche perché il carico di lavoro “medio” che un consumatore esegue cambia nel tempo. Ad esempio, i componenti in virgola mobile erano un tempo considerati riservati alla “elaborazione scientifica”, di solito venivano implementati come un coprocessore (un componente modulare che funzionava con il datapath) e raramente distribuiti ai consumatori medi. Confrontate questo atteggiamento con quello odierno, in cui le FPU sono integrate in ogni datapath.\n\nVantaggi\nSebbene la produttività in sé sia limitata, le CPU per uso generico offrono vantaggi pratici di accelerazione AI.\n\nProgrammabilità Generale\nLe CPU supportano carichi di lavoro diversi oltre al ML, offrendo una programmabilità flessibile per uso generico. Questa versatilità deriva dai loro set di istruzioni standardizzati e dagli ecosistemi di compilatori maturi, che consentono di eseguire qualsiasi applicazione, dai database e server Web alle pipeline analitiche (Hennessy e Patterson 2019).\n\nHennessy, John L., e David A. Patterson. 2019. «A new golden age for computer architecture». Commun. ACM 62 (2): 48–60. https://doi.org/10.1145/3282307.\nQuesto evita la necessità di acceleratori ML dedicati e consente di sfruttare l’infrastruttura basata su CPU esistenti per la distribuzione ML di base. Ad esempio, i server X86 di fornitori come Intel e AMD possono eseguire framework ML comuni utilizzando pacchetti Python e TensorFlow insieme ad altri carichi di lavoro aziendali.\n\n\nEcosistema Software Maturo\nPer decenni, librerie matematiche altamente ottimizzate come BLAS, LAPACK e FFTW hanno sfruttato istruzioni vettorializzate e multithreading su CPU (Dongarra 2009). I principali framework ML come PyTorch, TensorFlow e SciKit-Learn sono progettati per integrarsi perfettamente con questi kernel matematici di CPU.\n\nDongarra, Jack J. 2009. «The evolution of high performance computing on system z». IBM J. Res. Dev. 53: 3–4.\nI fornitori di hardware come Intel e AMD forniscono anche librerie di basso livello per ottimizzare completamente le prestazioni per primitive di deep learning (accelerazione dell’inferenza AI su CPU). Questo ecosistema software robusto e maturo consente di distribuire rapidamente ML su infrastrutture di CPU esistenti.\n\n\nAmpia Disponibilità\nLe economie di scala della produzione di CPU, guidate dalla domanda in molti mercati come PC, server e dispositivi mobili, le rendono disponibili ovunque. Le CPU Intel, ad esempio, hanno alimentato la maggior parte dei server per decenni (Ranganathan 2011). Questa ampia disponibilità nei data center riduce i costi hardware per l’implementazione di ML di base.\n\nRanganathan, Parthasarathy. 2011. «From Microprocessors to Nanostores: Rethinking Data-Centric Systems». Computer 44 (1): 39–48. https://doi.org/10.1109/mc.2011.18.\nAnche i piccoli dispositivi embedded in genere integrano una certa CPU, consentendo l’inferenza edge. L’ubiquità riduce la necessità di acquistare acceleratori ML specializzati in molte situazioni.\n\n\nBasso Consumo per L’inferenza\nOttimizzazioni come ARM Neon e le estensioni vettoriali Intel AVX forniscono un throughput di numeri interi e in virgola mobile a basso consumo ottimizzato per carichi di lavoro “a raffica” come l’inferenza (Ignatov et al. 2018). Sebbene più lenta delle GPU, l’inferenza CPU può essere implementata in ambienti con vincoli energetici. Ad esempio, le CPU Cortex-M di ARM ora offrono oltre 1 TOPS di prestazioni INT8 sotto 1 W, consentendo l’individuazione di parole chiave e applicazioni di visione su dispositivi edge (ARM).\n\n\n\nSvantaggi\nPur offrendo alcuni vantaggi, le CPU per uso generico presentano anche delle limitazioni per i carichi di lavoro AI.\n\nThroughput Inferiore Rispetto agli Acceleratori\nLe CPU non dispongono delle architetture specializzate per l’elaborazione parallela massiva che GPU e altri acceleratori forniscono. Il loro design per uso generico riduce il throughput computazionale per le operazioni matematiche altamente parallelizzabili comuni nei modelli ML (N. P. Jouppi et al. 2017a).\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nNon Ottimizzato per il Parallelismo dei Dati\nLe architetture delle CPU non sono specificamente ottimizzate per i carichi di lavoro paralleli dei dati inerenti all’AI (Sze et al. 2017). Assegnano un’area di silicio sostanziale alla decodifica delle istruzioni, all’esecuzione speculativa, alla memorizzazione nella cache e al controllo del flusso che fornisce pochi vantaggi per le operazioni su array utilizzate nelle reti neurali (accelerazione dell’inferenza AI sulle CPU). Tuttavia, le CPU moderne sono dotate di istruzioni vettoriali come AVX-512 specificamente per accelerare determinate operazioni chiave come la moltiplicazione matriciale.\nI multiprocessori di streaming GPU, ad esempio, dedicano la maggior parte dei transistor alle unità a virgola mobile anziché alla logica di predizione di diramazione complessa. Questa specializzazione consente un utilizzo molto più elevato per la matematica ML.\n\n\nMaggiore Latenza della Memoria\nLe CPU soffrono di una latenza maggiore nell’accesso alla memoria principale rispetto alle GPU e ad altri acceleratori (DDR). Tecniche come il tiling e il caching possono aiutare, ma la separazione fisica dalla RAM off-chip crea colli di bottiglia nei carichi di lavoro ML ad alta intensità di dati. Ciò sottolinea la necessità di architetture di memoria specializzate nell’hardware ML.\n\n\nInefficienza Energetica in Caso di Carichi di Lavoro Pesanti\nSebbene sia adatto per l’inferenza intermittente, il mantenimento di una produttività quasi di picco per l’addestramento comporta un consumo energetico inefficiente sulle CPU, in particolare sulle CPU mobili (Ignatov et al. 2018). Gli acceleratori ottimizzano esplicitamente il flusso di dati, la memoria e il calcolo per carichi di lavoro ML sostenuti. Le CPU sono inefficienti dal punto di vista energetico per l’addestramento di modelli di grandi dimensioni.\n\n\n\n\n10.3.6 Confronto\nTabella 10.2 confronta i diversi tipi di funzionalità hardware.\n\n\n\nTabella 10.2: Confronto di diversi acceleratori hardware per carichi di lavoro AI.\n\n\n\n\n\n\n\n\n\n\n\nAcceleratore\nDescrizione\nPrincipali vantaggi\nPrincipali svantaggi\n\n\n\n\nASIC\nIC personalizzati progettati per carichi di lavoro target come l’inferenza AI\n\nMassimizza le prestazioni/watt.\nOttimizzato per le operazioni tensoriali\nMemoria on-chip a bassa latenza\n\n\nL’architettura fissa manca di flessibilità\nElevato costo NRE\nLunghi cicli di progettazione\n\n\n\nFPGA\nFabric riconfigurabile con logica programmabile e routing\n\nArchitettura flessibile\nAccesso alla memoria a bassa latenza\n\n\nPrestazioni/watt inferiori rispetto agli ASIC\nProgrammazione complessa\n\n\n\nGPU\nOriginariamente per la grafica, ora utilizzate per l’accelerazione della rete neurale\n\nElevata produttività\nScalabilità parallela\nEcosistema software con CUDA\n\n\nNon efficienti dal punto di vista energetico come gli ASIC\nRichiede un’elevata larghezza di banda della memoria\n\n\n\nCPU\nProcessori per uso generico\n\nProgrammabilità\nDisponibilità ubiqua\n\n\nPrestazioni inferiori per carichi di lavoro AI\n\n\n\n\n\n\n\nIn generale, le CPU forniscono una baseline prontamente disponibile, le GPU offrono un’accelerazione ampiamente accessibile, gli FPGA offrono programmabilità e gli ASIC massimizzano l’efficienza per funzioni fisse. La scelta ottimale dipende dalla scala, dal costo, dalla flessibilità e da altri requisiti dell’applicazione target.\nSebbene inizialmente sviluppati per l’implementazione del data center, Google ha anche profuso notevoli sforzi nello sviluppo di TPU Edge. Questi TPU Edge mantengono l’ispirazione degli array sistolici [https://it.wikipedia.org/wiki/Array_sistolico], ma sono adattati alle risorse limitate accessibili all’edge.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#co-progettazione-hardware-software",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#co-progettazione-hardware-software",
    "title": "10  Accelerazione IA",
    "section": "10.4 Co-Progettazione Hardware-Software",
    "text": "10.4 Co-Progettazione Hardware-Software\nLa co-progettazione hardware-software si basa sul principio secondo cui i sistemi AI raggiungono prestazioni ed efficienza ottimali quando i componenti hardware e software sono progettati in stretta integrazione. Ciò comporta un ciclo di progettazione iterativo e collaborativo in cui l’architettura hardware e gli algoritmi software vengono sviluppati e perfezionati contemporaneamente con un feedback continuo tra i team.\nAd esempio, un nuovo modello di rete neurale può essere prototipato su una piattaforma di accelerazione basata su FPGA per ottenere dati sulle prestazioni reali all’inizio del processo di progettazione. Questi risultati forniscono un feedback ai progettisti hardware su potenziali ottimizzazioni e agli sviluppatori software su perfezionamenti del modello o framework per sfruttare meglio le capacità hardware. Questo livello di sinergia è difficile da raggiungere con la pratica comune di software sviluppato in modo indipendente per essere distribuito su hardware fisso.\nLa progettazione congiunta è fondamentale per i sistemi di intelligenza artificiale embedded che affrontano notevoli vincoli di risorse come budget di potenza ridotti, memoria e capacità di elaborazione limitate e requisiti di latenza in tempo reale. La stretta integrazione tra sviluppatori di algoritmi e architetti hardware aiuta a sbloccare le ottimizzazioni in tutto lo stack per soddisfare queste restrizioni. Le tecniche di abilitazione includono miglioramenti algoritmici come la ricerca e il pruning [potatura] dell’architettura neurale e progressi hardware come flussi di dati specializzati e gerarchie di memoria.\nRiunendo la progettazione hardware e software, anziché svilupparli separatamente, è possibile realizzare ottimizzazioni olistiche che massimizzano prestazioni ed efficienza. Le sezioni successive forniscono maggiori dettagli su specifici approcci di progettazione congiunta.\n\n10.4.1 La Necessità della Progettazione Congiunta\nDiversi fattori chiave rendono essenziale un approccio di progettazione congiunta hardware-software collaborativo per la creazione di sistemi di intelligenza artificiale efficienti.\n\nAumento delle Dimensioni e della Complessità del Modello\nI modelli di intelligenza artificiale all’avanguardia sono cresciuti rapidamente in termini di dimensioni, abilitati dai progressi nella progettazione dell’architettura neurale e dalla disponibilità di grandi set di dati. Ad esempio, il modello linguistico GPT-3 contiene 175 miliardi di parametri (Brown et al. 2020), che richiedono enormi risorse di calcolo per l’addestramento. Questa esplosione nella complessità del modello richiede una progettazione congiunta per sviluppare hardware e algoritmi efficienti in tandem. Tecniche come la compressione del modello (Cheng et al. 2018) e la quantizzazione devono essere co-ottimizzate con l’architettura hardware.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nCheng, Yu, Duo Wang, Pan Zhou, e Tao Zhang. 2018. «Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges». IEEE Signal Process Mag. 35 (1): 126–36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nVincoli della Distribuzione Embedded\nL’implementazione di applicazioni AI su dispositivi edge come telefoni cellulari o elettrodomestici intelligenti introduce vincoli significativi su energia, memoria e area di silicio (Sze et al. 2017). Abilitare l’inferenza in tempo reale con queste restrizioni richiede la co-esplorazione di ottimizzazioni hardware come flussi di dati specializzati e compressione con progettazione efficiente di reti neurali e tecniche di potatura. La co-progettazione massimizza le prestazioni entro rigidi vincoli di distribuzione.\n\n\nRapida Evoluzione degli Algoritmi AI\nL’intelligenza artificiale si sta evolvendo rapidamente, con nuove architetture di modelli, metodologie di training e framework software che emergono costantemente. Ad esempio, i Transformers sono diventati di recente molto popolari per l’NLP (Young et al. 2018). Per tenere il passo con queste innovazioni algoritmiche è necessaria una progettazione congiunta hardware-software per adattare le piattaforme ed evitare rapidamente il debito tecnico accumulato.\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, e Erik Cambria. 2018. «Recent Trends in Deep Learning Based Natural Language Processing [Review Article]». IEEE Comput. Intell. Mag. 13 (3): 55–75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nInterazioni Complesse Hardware-Software\nMolte interazioni e compromessi sottili tra scelte architettoniche hardware e ottimizzazioni software hanno un impatto significativo sull’efficienza complessiva. Ad esempio, tecniche come il partizionamento tensoriale e il batching influenzano il parallelismo e i modelli di accesso ai dati influenzano l’utilizzo della memoria. La progettazione congiunta fornisce una prospettiva multilivello per svelare queste dipendenze.\n\n\nNecessità di Specializzazione\nI carichi di lavoro dell’intelligenza artificiale traggono vantaggio da operazioni specializzate come matematica a bassa precisione e gerarchie di memoria personalizzate. Ciò motiva l’incorporazione di hardware personalizzato su misura per algoritmi di reti neurali piuttosto che affidarsi esclusivamente a software flessibile in esecuzione su hardware generico (Sze et al. 2017). Tuttavia, lo stack software deve mirare esplicitamente alle operazioni hardware personalizzate per realizzare i vantaggi.\n\n\nRichiesta di Maggiore Efficienza\nCon la crescente complessità del modello, si verificano rendimenti decrescenti e spese generali derivanti dall’ottimizzazione del solo hardware o software in isolamento (Putnam et al. 2014). Si presentano inevitabili compromessi che richiedono un’ottimizzazione globale su più livelli. La progettazione congiunta di hardware e software fornisce grandi guadagni di efficienza composti.\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. «A reconfigurable fabric for accelerating large-scale datacenter services». ACM SIGARCH Computer Architecture News 42 (3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\n\n10.4.2 Principi di Progettazione Congiunta Hardware-Software\nL’architettura hardware e lo stack software devono essere strettamente integrati e co-ottimizzati per creare sistemi di intelligenza artificiale efficienti e ad alte prestazioni. Nessuno dei due può essere progettato in isolamento; massimizzare le loro sinergie richiede un approccio olistico noto come progettazione congiunta hardware-software.\nL’obiettivo principale è adattare le capacità hardware in modo che corrispondano agli algoritmi e ai carichi di lavoro eseguiti dal software. Ciò richiede un ciclo di feedback tra architetti hardware e sviluppatori software per convergere su soluzioni ottimizzate. Diverse tecniche consentono un’efficace co-progettazione:\n\nOttimizzazione Software Consapevole dell’Hardware\nLo stack software può essere ottimizzato per sfruttare meglio le capacità hardware:\n\nParallelismo: Parallelizzare i calcoli matriciali come convoluzione o layer di attenzione per massimizzare la produttività sui motori vettoriali.\nOttimizzazione della Memoria: Ottimizzare i layout dei dati per migliorare la località della cache in base alla profilazione hardware. Ciò massimizza il riutilizzo e riduce al minimo l’accesso DRAM costoso.\nCompressione: Utilizzare la sparsity [diradazione] nei modelli per ridurre lo spazio di archiviazione e risparmiare sui calcoli tramite operazioni di zero-skipping.\nOperazioni Personalizzate: Incorporare operazioni specializzate come INT4 a bassa precisione o bfloat16 nei modelli per sfruttare al meglio il supporto hardware dedicato.\nMappatura del Flusso di Dati: Mappare esplicitamente le fasi del modello alle unità di calcolo per ottimizzare lo spostamento dei dati sull’hardware.\n\n\n\nSpecializzazione Hardware Algorithm-Driven\nL’hardware può essere adattato alle caratteristiche degli algoritmi ML:\n\nTipi di Dati Personalizzati: Supportare INT8/4 o bfloat16 a bassa precisione nell’hardware per una maggiore densità aritmetica.\nMemoria su Chip: Aumentare la larghezza di banda SRAM e ridurre la latenza di accesso per adattarla ai pattern di accesso alla memoria del modello.\nOperazioni Specifiche del Dominio: Aggiungere unità hardware per funzioni ML chiave come FFT o moltiplicazione di matrici per ridurre latenza ed energia.\nProfilazione del Modello: Utilizzare la simulazione e la profilazione del modello per identificare hotspot computazionali e ottimizzare l’hardware.\n\nLa chiave è il feedback collaborativo: le informazioni dalla profilazione dell’hardware guidano le ottimizzazioni del software, mentre i progressi algoritmici informano la specializzazione dell’hardware. Questo miglioramento reciproco fornisce guadagni di efficienza moltiplicativa rispetto agli sforzi isolati.\n\n\nCo-esplorazione Algoritmo-Hardware\nUna potente tecnica di co-progettazione prevede l’esplorazione congiunta di innovazioni nelle architetture di reti neurali e nella progettazione custom dell’hardware. A powerful co-design technique involves jointly exploring innovations in neural network architectures and custom hardware design. Ciò consente di trovare abbinamenti ideali su misura per i rispettivi punti di forza (Sze et al. 2017).\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, e Joel S. Emer. 2017. «Efficient Processing of Deep Neural Networks: A Tutorial and Survey». Proc. IEEE 105 (12): 2295–2329. https://doi.org/10.1109/jproc.2017.2761740.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. «Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2704–13. IEEE. https://doi.org/10.1109/cvpr.2018.00286.\n\nGale, Trevor, Erich Elsen, e Sara Hooker. 2019. «The state of sparsity in deep neural networks». ArXiv preprint abs/1902.09574. https://arxiv.org/abs/1902.09574.\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, e Paulius Micikevicius. 2021. «Accelerating Sparse Deep Neural Networks». CoRR abs/2104.08378. https://arxiv.org/abs/2104.08378.\nAd esempio, il passaggio ad architetture mobili come MobileNets (Howard et al. 2017) è stato guidato dai vincoli dei dispositivi edge come dimensioni del modello e latenza. La quantizzazione (Jacob et al. 2018) e le tecniche di pruning [potatura] (Gale, Elsen, e Hooker 2019) che hanno reso questi modelli efficienti sono diventate possibili grazie ad acceleratori hardware con supporto nativo per interi a bassa precisione e supporto per potatura (Mishra et al. 2021).\nI modelli basati sull’attenzione hanno prosperato su GPU e ASIC massivamente paralleli, dove il loro calcolo si mappa bene nello spazialmente, al contrario delle architetture RNN, che si basano sull’elaborazione sequenziale. La co-evoluzione di algoritmi e hardware ha evidenziato nuove capacità.\nUna co-esplorazione efficace richiede una stretta collaborazione tra ricercatori di algoritmi e architetti hardware. La prototipazione rapida su FPGA (C. Zhang et al. 2015) o simulatori di intelligenza artificiale specializzati consente una rapida valutazione di diverse coppie di architetture di modelli e progetti hardware pre-silicio.\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, e Jason Optimizing Cong. 2015. «FPGA-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 ACM». In SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA, 15:161–70.\nAd esempio, l’architettura TPU di Google si è evoluta con ottimizzazioni verso i modelli TensorFlow per massimizzare le prestazioni sulla classificazione delle immagini. Questo stretto ciclo di feedback ha prodotto modelli su misura per la TPU che sarebbero stati improbabili in isolamento.\nGli studi hanno mostrato guadagni di prestazioni ed efficienza da 2 a 5 volte superiori con la co-esplorazione algoritmo-hardware rispetto agli sforzi isolati di ottimizzazione di algoritmi o hardware (Suda et al. 2016). Parallelizzare lo sviluppo congiunto riduce anche i “time-to-deployment” [tempi di distribuzione].\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma, Sarma Vrudhula, Jae-sun Seo, e Yu Cao. 2016. «Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks». In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 16–25. ACM. https://doi.org/10.1145/2847263.2847276.\nNel complesso, esplorare le strette interdipendenze tra innovazione del modello e progressi hardware crea opportunità che devono essere visibili quando vengono affrontate in sequenza. Questa progettazione sinergica congiunta produce soluzioni maggiori della somma delle loro parti.\n\n\n\n10.4.3 Sfide\nSebbene la progettazione collaborativa possa migliorare l’efficienza, l’adattabilità e il time-to-market, presenta anche sfide ingegneristiche e organizzative.\n\nAumento dei Costi di Prototipazione\nÈ richiesta una prototipazione più estesa per valutare diverse accoppiate hardware-software. La necessità di prototipi rapidi e iterativi su FPGA o emulatori aumenta il sovraccarico della validazione. Ad esempio, Microsoft ha scoperto che erano necessari più prototipi per la progettazione collaborativa di un acceleratore AI rispetto alla progettazione sequenziale (Fowers et al. 2018).\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. «A Configurable Cloud-Scale DNN Processor for Real-Time AI». In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), 1–14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nOstacoli Organizzativi e di Team\nLa progettazione collaborativa richiede uno stretto coordinamento tra gruppi hardware e software tradizionalmente scollegati. Ciò potrebbe causare problemi di comunicazione o priorità e pianificazioni non allineate. Anche la navigazione di diversi flussi di lavoro di progettazione è impegnativa. Potrebbe esistere una certa inerzia organizzativa nell’adottare pratiche integrate.\n\n\nComplessità di Simulazione e Modellazione\nCatturare interazioni sottili tra layer hardware e software per la simulazione e la modellazione congiunte aggiunge una complessità significativa. Le astrazioni complete “cross-layer” sono difficili da costruire quantitativamente prima dell’implementazione, rendendo più difficile quantificare in anticipo le ottimizzazioni olistiche.\n\n\nRischi di Eccessiva Specializzazione\nUna progettazione congiunta rigorosa comporta il rischio di adattare eccessivamente le ottimizzazioni agli algoritmi correnti, sacrificando la generalità. Ad esempio, l’hardware ottimizzato esclusivamente per i modelli Transformer potrebbe avere prestazioni inferiori con le tecniche future. Mantenere la flessibilità richiede lungimiranza.\n\n\nProblemi sui Cambiamenti\nGli ingegneri che hanno familiarità con le consolidate pratiche di progettazione hardware o software discrete potrebbero accettare solo flussi di lavoro collaborativi familiari. Nonostante i vantaggi a lungo termine, i progetti potrebbero incontrare attriti nella transizione alla progettazione congiunta.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#software-per-hardware-ai",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#software-per-hardware-ai",
    "title": "10  Accelerazione IA",
    "section": "10.5 Software per Hardware AI",
    "text": "10.5 Software per Hardware AI\nAcceleratori hardware specializzati come GPU, TPU e FPGA sono essenziali per fornire applicazioni di intelligenza artificiale ad alte prestazioni. Tuttavia, è necessario un ampio stack software per sfruttare efficacemente queste piattaforme hardware, che coprano l’intero ciclo di vita di sviluppo e distribuzione. Framework e librerie costituiscono la spina dorsale dell’hardware AI, offrendo set di codice, algoritmi e funzioni pre-costruiti e robusti, specificamente ottimizzati per eseguire varie attività AI su hardware diversi. Sono progettati per semplificare le complessità dell’utilizzo dell’hardware da zero, che può richiedere molto tempo ed essere soggetto a errori. Il software svolge un ruolo importante:\n\nFornendo astrazioni di programmazione e modelli come CUDA e OpenCL per mappare i calcoli sugli acceleratori.\nIntegrando gli acceleratori in framework di deep learning popolari come TensorFlow e PyTorch.\nOttimizzando l’intero stack hardware-software con compilatori e tool.\nCon piattaforme di simulazione per modellare insieme hardware e software.\nCon l’infrastruttura per gestire la distribuzione sugli acceleratori.\n\nQuesto vasto ecosistema software è importante quanto l’hardware nel fornire applicazioni AI performanti ed efficienti. Questa sezione fornisce una panoramica degli strumenti disponibili a ogni livello dello stack per consentire agli sviluppatori di creare ed eseguire sistemi AI basati sull’accelerazione hardware.\n\n10.5.1 Modelli di Programmazione\nI modelli di programmazione forniscono astrazioni per mappare calcoli e dati su acceleratori hardware eterogenei:\n\nCUDA: Modello di programmazione parallela di Nvidia per sfruttare le GPU utilizzando estensioni a linguaggi come C/C++. Consente di avviare kernel su core GPU (Luebke 2008).\nOpenCL: Standard aperto per scrivere programmi che spaziano tra CPU, GPU, FPGA e altri acceleratori. Specifica un framework di elaborazione eterogeneo (Munshi 2009).\nOpenGL/WebGL: Interfacce di programmazione grafica 3D in grado di mappare codice generico su core GPU (Segal e Akeley 1999).\nVerilog/VHDL: “Hardware description languages (HDL)” [Linguaggi di descrizione hardware] utilizzati per configurare FPGA come acceleratori AI specificando circuiti digitali (Gannot e Ligthart 1994).\nTVM: Un framework di compilazione che fornisce un frontend Python per ottimizzare e mappare modelli di deep learning su diversi backend hardware (Chen et al. 2018).\n\n\nLuebke, David. 2008. «CUDA: Scalable parallel programming for high-performance scientific computing». In 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 836–38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\nMunshi, Aaftab. 2009. «The OpenCL specification». In 2009 IEEE Hot Chips 21 Symposium (HCS), 1–314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\nSegal, Mark, e Kurt Akeley. 1999. «The OpenGL graphics system: A specification (version 1.1)».\n\nGannot, G., e M. Ligthart. 1994. «Verilog HDL based FPGA design». In International Verilog HDL Conference, 86–92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. «TVM: An automated End-to-End optimizing compiler for deep learning». In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\nLe sfide principali includono l’espressione del parallelismo, la gestione della memoria tra dispositivi e l’abbinamento di algoritmi alle capacità hardware. Le astrazioni devono bilanciare la portabilità con la possibilità di personalizzazione hardware. I modelli di programmazione consentono agli sviluppatori di sfruttare gli acceleratori senza competenze hardware. Questi dettagli sono discussi nella sezione AI frameworks section.\n\n\n\n\n\n\nEsercizio 10.1: Software per hardware AI - TVM\n\n\n\n\n\nAbbiamo imparato che l’hardware AI sofisticato ha bisogno di un software speciale per fare magie. TVM è come un traduttore super intelligente, che trasforma il codice in istruzioni che gli acceleratori capiscono. In questo Colab, useremo TVM per creare un acceleratore finto chiamato VTA che esegue la moltiplicazione di matrici super velocemente. Pronti a vedere come il software alimenta l’hardware?\n\n\n\n\n\n\n10.5.2 Librerie e Runtime\nLibrerie e runtime specializzati forniscono astrazioni software per accedere e massimizzare l’utilizzo degli acceleratori AI:\n\nLibrerie Matematiche: Implementazioni altamente ottimizzate di primitive di algebra lineare come GEMM, FFT, convoluzioni, ecc., su misura per l’hardware target. Nvidia cuBLAS, Intel MKL e librerie di elaborazione Arm sono esempi.\nIntegrazioni di Framework: Librerie per accelerare framework di deep learning come TensorFlow, PyTorch e MXNet su hardware supportato. Ad esempio, cuDNN accelera le CNN sulle GPU Nvidia.\nRuntime: Software per gestire l’esecuzione dell’acceleratore, tra cui pianificazione, sincronizzazione, gestione della memoria e altre attività. Nvidia TensorRT è un ottimizzatore di inferenza e runtime.\nDriver e firmware: Software di basso livello per interfacciarsi con l’hardware, inizializzare i dispositivi e gestire l’esecuzione. Fornitori come Xilinx forniscono driver per le loro schede acceleratrici.\n\nAd esempio, gli integratori PyTorch utilizzano librerie cuDNN e cuBLAS per accelerare l’addestramento sulle GPU Nvidia. Il runtime TensorFlow XLA ottimizza e compila modelli per acceleratori come le TPU. I driver inizializzano i dispositivi e delegano le operazioni.\nLe sfide includono il partizionamento e la pianificazione efficienti dei carichi di lavoro su dispositivi eterogenei come nodi multi-GPU. I runtime devono anche ridurre al minimo il sovraccarico dei trasferimenti di dati e della sincronizzazione.\nLibrerie, runtime e driver forniscono i mattoni ottimizzati che gli sviluppatori di deep learning possono sfruttare per le prestazioni dell’acceleratore senza competenze di programmazione hardware. La loro ottimizzazione è essenziale per le distribuzioni.\n\n\n10.5.3 Ottimizzazione dei Compilatori\nL’ottimizzazione dei compilatori è fondamentale per estrarre le massime prestazioni ed efficienza dagli acceleratori hardware per i carichi di lavoro AI. Applicano ottimizzazioni che spaziano tra modifiche algoritmiche, trasformazioni a livello di grafico e generazione di codice di basso livello.\n\nOttimizzazione degli Algoritmi: Tecniche come quantizzazione, potatura e ricerca di architettura neurale per migliorare l’efficienza del modello e abbinare le capacità hardware.\nOttimizzazioni dei Grafi: Ottimizzazioni a livello di grafo come fusione degli operatori, riscrittura e trasformazioni di layout per ottimizzare le prestazioni sull’hardware target.\nGenerazione di Codice: Generazione di codice di basso livello ottimizzato per acceleratori da modelli e framework di alto livello.\n\nAd esempio, lo stack di compilatori “open” TVM applica la quantizzazione per un modello BERT che ha come target le GPU Arm. Fonde le operazioni di convoluzione puntuale e trasforma il layout dei pesi per ottimizzare l’accesso alla memoria. Infine, emette codice OpenGL ottimizzato per eseguire il carico di lavoro GPU.\nLe ottimizzazioni chiave del compilatore includono la massimizzazione del parallelismo, il miglioramento della località e del riutilizzo dei dati, la riduzione al minimo dell’ingombro della memoria e lo sfruttamento delle operazioni hardware personalizzate. I compilatori creano e ottimizzano i carichi di lavoro di machine learning in modo olistico su componenti hardware come CPU, GPU e altri acceleratori.\nTuttavia, la mappatura efficiente di modelli complessi introduce sfide come il partizionamento efficiente dei carichi di lavoro su dispositivi eterogenei. I compilatori a livello di produzione richiedono anche molto tempo per la messa a punto su carichi di lavoro rappresentativi. Tuttavia, l’ottimizzazione dei compilatori è per sfruttare tutte le capacità degli acceleratori AI.\n\n\n10.5.4 Simulazione e Modellazione\nIl software di simulazione è importante nella progettazione congiunta hardware-software. Consente accoppiare la modellazione di architetture hardware e stack software proposti:\n\nSimulazione Hardware: Piattaforme come Gem5 consentono la simulazione dettagliata di componenti hardware come pipeline, cache, interconnessioni e gerarchie di memoria. Gli ingegneri possono modellare le modifiche hardware senza prototipazione fisica (Binkert et al. 2011).\nSimulazione Software: Stack di compilatori come TVM supportano la simulazione di carichi di lavoro di machine learning per stimare le prestazioni sulle architetture hardware target. Questo aiuta con le ottimizzazioni software.\nCo-simulazione: Piattaforme unificate come SCALE-Sim (Samajdar et al. 2018) integrano la simulazione hardware e software in un unico strumento. Ciò consente un’analisi “what-if” per quantificare gli impatti a livello di sistema delle ottimizzazioni cross-layer all’inizio del ciclo di progettazione.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. «The gem5 simulator». ACM SIGARCH Computer Architecture News 39 (2): 1–7. https://doi.org/10.1145/2024716.2024718.\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, e Tushar Krishna. 2018. «Scale-sim: Systolic cnn accelerator simulator». ArXiv preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\nAd esempio, un progetto di acceleratore AI basato su FPGA potrebbe essere simulato utilizzando il linguaggio di descrizione hardware Verilog e sintetizzato in un modello Gem5. Verilog è adatto per descrivere la logica digitale e le interconnessioni dell’architettura dell’acceleratore. Verilog consente al progettista di specificare i datapath [percorsi dati], la logica di controllo, le memorie on-chip e altri componenti implementati nella struttura FPGA. Una volta completato il progetto Verilog, può essere sintetizzato in un modello che simula il comportamento dell’hardware, ad esempio utilizzando il simulatore Gem5. Gem5 è utile per questa attività perché consente la modellazione di sistemi completi, inclusi processori, cache, bus e acceleratori personalizzati. Gem5 supporta l’interfacciamento dei modelli Verilog dell’hardware alla simulazione, consentendo la modellazione unificata del sistema.\nIl modello di acceleratore FPGA sintetizzato potrebbe quindi avere carichi di lavoro ML simulati utilizzando TVM compilato su di esso all’interno dell’ambiente Gem5 per una modellazione unificata. TVM consente la compilazione ottimizzata di modelli di ML su hardware eterogeneo come FPGA. L’esecuzione di carichi di lavoro compilati con TVM sull’acceleratore all’interno della simulazione Gem5 fornisce un modo integrato per convalidare e perfezionare la progettazione hardware, lo stack software e l’integrazione di sistema prima di realizzare fisicamente l’acceleratore su un FPGA reale.\nQuesto tipo di co-simulazione fornisce stime di metriche complessive come throughput, latenza e potenza per guidare la progettazione congiunta prima della costosa prototipazione fisica. Aiutano anche con le ottimizzazioni di partizionamento tra hardware e software per guidare i compromessi di progettazione.\nTuttavia, la precisione nella modellazione di interazioni sottili di basso livello tra componenti è limitata. Le simulazioni quantificate sono stime ma non possono sostituire completamente i prototipi fisici e i test. Tuttavia, la simulazione e la modellazione unificate forniscono preziose informazioni iniziali sulle opportunità di ottimizzazione a livello di sistema durante il processo di co-progettazione.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#benchmarking-dellhardware-ai",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#benchmarking-dellhardware-ai",
    "title": "10  Accelerazione IA",
    "section": "10.6 Benchmarking dell’Hardware AI",
    "text": "10.6 Benchmarking dell’Hardware AI\nIl benchmarking è un processo critico che quantifica e confronta le prestazioni di varie piattaforme hardware progettate per accelerare le applicazioni di intelligenza artificiale. Guida le decisioni di acquisto, l’attenzione allo sviluppo e gli sforzi di ottimizzazione delle prestazioni per i produttori di hardware e gli sviluppatori di software.\nIl capitolo sul benchmarking esplora questo argomento in modo molto dettagliato, spiegando perché è diventato una parte indispensabile del ciclo di sviluppo dell’hardware AI e come influisce sul più ampio panorama tecnologico. Qui, esamineremo brevemente i concetti principali, ma consigliamo di fare riferimento al capitolo per maggiori dettagli.\nSuite di benchmarking come MLPerf, Fathom e AI Benchmark offrono una serie di test standardizzati utilizzabili su diverse piattaforme hardware. Queste suite misurano le prestazioni dell’acceleratore AI su varie reti neurali e attività di apprendimento automatico, dalla classificazione di immagini di base all’elaborazione complessa del linguaggio. Fornendo un terreno comune per il confronto, aiutano a garantire che le dichiarazioni sulle prestazioni siano coerenti e verificabili. Questi “tool” vengono applicati non solo per guidare lo sviluppo dell’hardware, ma anche per garantire che lo stack software sfrutti appieno il potenziale dell’architettura sottostante.\n\nMLPerf: Include un ampio set di benchmark che coprono sia l’addestramento (Mattson et al. 2020) che l’inferenza (Reddi et al. 2020) per una gamma di attività di machine learning.\nFathom: Si concentra sulle operazioni principali nei modelli di deep learning, enfatizzandone l’esecuzione su diverse architetture (Adolf et al. 2016).\nAI Benchmark: Mira a dispositivi mobili e consumer, valutando le prestazioni dell’IA nelle applicazioni per utenti finali (Ignatov et al. 2018).\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. «MLPerf Inference Benchmark». In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. «Fathom: Reference workloads for modern deep learning methods». In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, e Luc Van Gool. 2018. «AI Benchmark: Running deep neural networks on Android smartphones», 0–0.\nI benchmark hanno anche metriche delle prestazioni che sono misure quantificabili utilizzate per valutare l’efficacia degli acceleratori di IA. Queste metriche forniscono una visione completa delle capacità di un acceleratore e vengono utilizzate per guidare il processo di progettazione e selezione per i sistemi di IA. Le metriche comuni comprendono:\n\nThroughput: Solitamente misurato in operazioni al secondo, questo parametro indica il volume di calcoli che un acceleratore può gestire.\nLatenza: Il ritardo temporale tra input e output in un sistema è fondamentale per le attività di elaborazione in tempo reale.\nEfficienza Energetica: Calcolato come elaborazione per watt, che rappresenta il compromesso tra prestazioni e consumo energetico.\nEfficienza dei Costi: Valuta il costo operativo in relazione alle prestazioni, un parametro essenziale per le distribuzioni attente al budget.\nPrecisione: Nelle attività di inferenza, la precisione dei calcoli è fondamentale e talvolta bilanciata rispetto alla velocità.\nScalabilità: La capacità del sistema di mantenere i guadagni in termini di prestazioni man mano che il carico computazionale aumenta.\n\nI risultati del benchmark forniscono informazioni che vanno oltre i semplici numeri: possono rivelare colli di bottiglia nello stack software e nell’hardware. Ad esempio, i benchmark possono mostrare come l’aumento delle dimensioni del batch migliori l’utilizzo della GPU fornendo più parallelismo o come le ottimizzazioni del compilatore aumentino le prestazioni della TPU. Questi insegnamenti consentono un’ottimizzazione continua (Zhihao Jia, Zaharia, e Aiken 2019).\n\nJia, Zhihao, Matei Zaharia, e Alex Aiken. 2019. «Beyond Data and Model Parallelism for Deep Neural Networks». In Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019, a cura di Ameet Talwalkar, Virginia Smith, e Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand Jayarajan, Amar Phanishayee, Bianca Schroeder, e Gennady Pekhimenko. 2018. «Benchmarking and Analyzing Deep Neural Network Training». In 2018 IEEE International Symposium on Workload Characterization (IISWC), 88–100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\nIl benchmarking standardizzato fornisce una valutazione quantificata e comparabile degli acceleratori AI per informare la progettazione, l’acquisto e l’ottimizzazione. Tuttavia, anche la convalida delle prestazioni nel mondo reale rimane essenziale (H. Zhu et al. 2018).",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#sfide-e-soluzioni",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#sfide-e-soluzioni",
    "title": "10  Accelerazione IA",
    "section": "10.7 Sfide e Soluzioni",
    "text": "10.7 Sfide e Soluzioni\nGli acceleratori AI offrono notevoli miglioramenti delle prestazioni, ma spesso è necessario migliorare i significativi problemi di portabilità e compatibilità nella loro integrazione nel più ampio panorama AI. Il nocciolo della questione risiede nella diversità dell’ecosistema AI: esiste una vasta gamma di acceleratori, framework e linguaggi di programmazione per l’apprendimento automatico, ognuno con le sue caratteristiche e requisiti unici.\n\n10.7.1 Problemi di Portabilità/Compatibilità\nGli sviluppatori incontrano spesso difficoltà nel trasferire i loro modelli AI da un ambiente hardware a un altro. Ad esempio, un modello di machine learning sviluppato per un ambiente desktop in Python utilizzando il framework PyTorch, ottimizzato per una GPU Nvidia, potrebbe non essere facilmente trasferito a un dispositivo più vincolato come Arduino Nano 33 BLE. Questa complessità deriva da nette differenze nei requisiti di programmazione: Python e PyTorch sul desktop rispetto a un ambiente C++ su un Arduino, per non parlare del passaggio dall’architettura x86 ad ARM ISA.\nQueste divergenze evidenziano la complessità della portabilità all’interno dei sistemi AI. Inoltre, il rapido progresso negli algoritmi e nei modelli di intelligenza artificiale implica che gli acceleratori hardware debbano adattarsi continuamente, creando un obiettivo mobile per la compatibilità. L’assenza di standard e interfacce universali aggrava il problema, rendendo difficile l’implementazione di soluzioni di intelligenza artificiale in modo coerente su vari dispositivi e piattaforme.\n\nSoluzioni e Strategie\nPer affrontare questi ostacoli, il settore dell’intelligenza artificiale si sta muovendo verso diverse soluzioni:\n\nIniziative di Standardizzazione\nOpen Neural Network Exchange (ONNX) è in prima linea in questa ricerca, proponendo un ecosistema aperto e condiviso che promuove l’intercambiabilità dei modelli. ONNX facilita l’uso di modelli di intelligenza artificiale su vari framework, consentendo ai modelli addestrati in un ambiente di essere distribuiti in modo efficiente in un altro, riducendo significativamente la necessità di riscritture o modifiche che richiedono molto tempo.\n\n\nFramework Multipiattaforma\nA complemento degli sforzi di standardizzazione, framework multipiattaforma come TensorFlow Lite e PyTorch Mobile sono stati sviluppati specificamente per creare coesione tra diversi ambienti di calcolo che vanno dai desktop ai dispositivi mobili ed embedded. Questi framework offrono versioni semplificate e leggere delle loro versioni principali, garantendo compatibilità e integrità funzionale su diversi tipi di hardware senza sacrificare le prestazioni. Ciò garantisce che gli sviluppatori possano creare applicazioni con la certezza che funzioneranno su molti dispositivi, colmando un divario che tradizionalmente ha rappresentato una sfida considerevole nello sviluppo dell’intelligenza artificiale.\n\n\nPiattaforme Indipendenti dall’Hardware\nL’ascesa delle piattaforme indipendenti dall’hardware ha anche svolto un ruolo importante nella democratizzazione dell’uso dell’IA. Creando ambienti in cui le applicazioni di IA possono essere eseguite su vari acceleratori, queste piattaforme eliminano l’onere della codifica specifica per l’hardware dagli sviluppatori. Questa astrazione semplifica il processo di sviluppo e apre nuove possibilità per l’innovazione e l’implementazione delle applicazioni, libere dai vincoli delle specifiche hardware.\n\n\nStrumenti di Compilazione Avanzati\nInoltre, l’avvento di strumenti di compilazione avanzati come TVM, un compilatore di tensori end-to-end, offre un percorso ottimizzato attraverso la giungla delle diverse architetture hardware. TVM fornisce agli sviluppatori i mezzi per mettere a punto modelli di machine learning per un ampio spettro di substrati computazionali, garantendo prestazioni ottimali ed evitando la regolazione manuale del modello ogni volta che si verifica uno spostamento nell’hardware sottostante.\n\n\nCollaborazione tra Comunità e Settore\nLa collaborazione tra comunità open source e consorzi di settore non può essere sottovalutata. Questi organismi collettivi sono fondamentali per la formazione di standard condivisi e best practice a cui tutti gli sviluppatori e i produttori possono aderire. Tale collaborazione promuove un ecosistema AI più unificato e sinergico, riducendo significativamente la prevalenza di problemi di portabilità e spianando la strada verso l’integrazione e l’avanzamento dell’AI globale. Attraverso questi lavori combinati, l’AI si sta muovendo costantemente verso un futuro in cui la distribuzione di modelli senza soluzione di continuità su varie piattaforme diventa uno standard piuttosto che un’eccezione.\nRisolvere le sfide della portabilità è fondamentale per il campo dell’IA per realizzare il pieno potenziale degli acceleratori hardware in un panorama tecnologico dinamico e diversificato. Richiede uno sforzo concertato da parte dei produttori di hardware, degli sviluppatori di software e degli enti normativi per creare un ambiente più interoperabile e flessibile. Con innovazione e collaborazione continue, la comunità dell’IA può aprire la strada a un’integrazione e a un’implementazione senza soluzione di continuità dei modelli di IA su molte piattaforme.\n\n\n\n\n10.7.2 Problemi di Consumo Energetico\nIl consumo energetico è un problema cruciale nello sviluppo e nel funzionamento degli acceleratori AI dei data center, come le unità di elaborazione grafica (GPU) e le unità di elaborazione tensoriale (TPU) (N. P. Jouppi et al. 2017b) (Norrie et al. 2021) (N. Jouppi et al. 2023). Questi potenti componenti sono la spina dorsale dell’infrastruttura AI contemporanea, ma le loro elevate richieste di energia contribuiscono all’impatto ambientale della tecnologia e aumentano significativamente i costi operativi. Man mano che le esigenze di elaborazione dei dati diventano più complesse, con la crescente popolarità dell’AI e del deep learning, c’è una richiesta pressante di GPU e TPU in grado di fornire la potenza di calcolo necessaria in modo più efficiente. L’impatto di tali progressi è duplice: possono ridurre l’impatto ambientale di queste tecnologie e ridurre i costi di esecuzione delle applicazioni AI.\n\n———, et al. 2017b. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, e David Patterson. 2021. «The Design Process for Google’s Training Chips: Tpuv2 and TPUv3». IEEE Micro 41 (2): 56–63. https://doi.org/10.1109/mm.2021.3058217.\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, et al. 2023. «TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings». In Proceedings of the 50th Annual International Symposium on Computer Architecture. ISCA ’23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\nLe tecnologie hardware emergenti sono sul punto di rivoluzionare l’efficienza energetica in questo settore. L’informatica fotonica, ad esempio, utilizza la luce anziché l’elettricità per trasportare informazioni, offrendo la promessa di un’elaborazione ad alta velocità con una frazione del consumo energetico. Analizziamo più approfonditamente questa e altre tecnologie innovative nella sezione “Tecnologie Hardware Emergenti”, esplorando il loro potenziale per affrontare le attuali sfide del consumo energetico.\nAi margini della rete, gli acceleratori AI sono progettati per elaborare dati su dispositivi come smartphone, sensori IoT e dispositivi indossabili intelligenti. Questi dispositivi spesso funzionano con gravi limitazioni di potenza, rendendo necessario un attento bilanciamento tra prestazioni e consumo energetico. Un modello AI ad alte prestazioni può fornire risultati rapidi, ma a costo di esaurire rapidamente la durata della batteria e aumentare la produzione termica, il che può influire sulla funzionalità e sulla durata del dispositivo. La posta in gioco è più alta per i dispositivi distribuiti in aree remote o difficili da raggiungere, dove non è possibile garantire un’alimentazione costante, il che sottolinea la necessità di soluzioni a basso consumo energetico.\nI problemi di latenza aggravano ulteriormente la sfida dell’efficienza energetica ai margini. Le applicazioni AI Edge in settori quali la guida autonoma e il monitoraggio sanitario richiedono velocità, precisione e affidabilità, poiché i ritardi nell’elaborazione possono comportare gravi rischi per la sicurezza. Per queste applicazioni, gli sviluppatori devono ottimizzare sia gli algoritmi AI sia la progettazione hardware per raggiungere un equilibrio ottimale tra consumo energetico e latenza.\nQuesto sforzo di ottimizzazione non riguarda solo l’apporto di miglioramenti incrementali alle tecnologie esistenti; riguarda il ripensamento di come e dove elaboriamo le attività AI. Progettando acceleratori AI che siano sia efficienti dal punto di vista energetico sia in grado di elaborare rapidamente, possiamo garantire che questi dispositivi svolgano i loro scopi previsti senza un consumo energetico non necessario o prestazioni compromesse. Tali sviluppi potrebbero promuovere l’adozione diffusa dell’AI in vari settori, consentendo un uso più intelligente, sicuro e sostenibile della tecnologia.\n\n\n10.7.3 Superare i Vincoli delle Risorse\nAnche i vincoli di risorse rappresentano una sfida significativa per gli acceleratori Edge AI, poiché queste soluzioni hardware e software specializzate devono fornire prestazioni robuste entro i limiti dei dispositivi edge. A causa dei limiti di potenza e dimensioni, gli acceleratori Edge AI hanno spesso capacità di calcolo, memoria e archiviazione limitate (L. Zhu et al. 2023). Questa scarsità di risorse richiede un’attenta allocazione delle capacità di elaborazione per eseguire modelli di apprendimento automatico in modo efficiente.\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2023. «PockEngine: Sparse and Efficient Fine-tuning in a Pocket». In 56th Annual IEEE/ACM International Symposium on Microarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\nLin, Ji, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, e Song Han. 2023. «AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration». arXiv.\n\nLi, Yuhang, Xin Dong, e Wei Wang. 2020. «Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks». In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, e Song Han. 2020. «APQ: Joint Search for Network Architecture, Pruning and Quantization Policy». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2075–84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\nInoltre, la gestione di risorse limitate richiede approcci innovativi, tra cui la quantizzazione del modello (Lin et al. 2023) (Li, Dong, e Wang 2020), pruning (Wang et al. 2020) e l’ottimizzazione delle pipeline di inferenza. Gli acceleratori Edge AI devono trovare un delicato equilibrio tra la fornitura di funzionalità AI significative e il non esaurire le risorse disponibili, mantenendo al contempo un basso consumo energetico. Superare questi vincoli di risorse è fondamentale per garantire l’implementazione di successo dell’intelligenza artificiale ai margini, dove molte applicazioni, dall’IoT ai dispositivi mobili, si basano sull’uso efficiente di risorse hardware limitate per fornire un processo decisionale intelligente e in tempo reale.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#tecnologie-emergenti",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#tecnologie-emergenti",
    "title": "10  Accelerazione IA",
    "section": "10.8 Tecnologie Emergenti",
    "text": "10.8 Tecnologie Emergenti\nFinora abbiamo discusso la tecnologia hardware AI nel contesto della progettazione dell’architettura von Neumann convenzionale e dell’implementazione basata su CMOS. Questi chip AI specializzati offrono vantaggi come una maggiore produttività ed efficienza energetica, ma si basano sui principi di elaborazione tradizionali. La crescita inarrestabile della domanda di potenza di elaborazione AI sta guidando le innovazioni nei metodi di integrazione per l’hardware AI.\nSono emersi due approcci principali per massimizzare la densità di elaborazione, l’integrazione su “scala wafer” e le architetture basate su “chiplet”, di cui parleremo in questa sezione. Guardando molto più avanti, esamineremo le tecnologie emergenti che divergono dalle architetture convenzionali e adottano approcci fondamentalmente diversi per l’elaborazione specializzata AI.\nAlcuni di questi paradigmi non convenzionali includono l’elaborazione neuromorfica, che imita le reti neurali biologiche; l’elaborazione quantistica, che sfrutta gli effetti della meccanica quantistica; e l’elaborazione ottica, che utilizza fotoni anziché elettroni. Oltre ai nuovi substrati di elaborazione, le nuove tecnologie dei dispositivi stanno consentendo ulteriori guadagni attraverso una migliore memoria e interconnessione.\nEsempi includono i “memristor” [https://it.wikipedia.org/wiki/Memristore] per l’elaborazione in memoria e la nanofotonica per la comunicazione fotonica integrata. Insieme, queste tecnologie offrono il potenziale per miglioramenti di ordini di grandezza in termini di velocità, efficienza e scalabilità rispetto all’attuale hardware AI. Esamineremo questi aspetti in questa sezione.\n\n10.8.1 Metodi di Integrazione\nI metodi di integrazione si riferiscono agli approcci utilizzati per combinare e interconnettere i vari componenti di elaborazione e memoria di un chip o sistema AI. Collegando strettamente gli elementi di elaborazione chiave, l’integrazione cerca di massimizzare le prestazioni, l’efficienza energetica e la densità.\nIn passato, l’elaborazione AI veniva eseguita principalmente su CPU e GPU costruite utilizzando metodi di integrazione convenzionali. Questi componenti discreti venivano fabbricati separatamente e collegati insieme su una scheda. Tuttavia, questa integrazione poco stretta crea colli di bottiglia, come i sovraccarichi dei trasferimento di dati.\nCon l’aumento dei carichi di lavoro AI, aumenta la domanda di una più stretta integrazione tra elementi di elaborazione, memoria e comunicazione. Alcuni fattori chiave dell’integrazione includono:\n\nRiduzione al minimo dello spostamento dei dati: Una stretta integrazione riduce la latenza e l’energia per lo spostamento dei dati tra i componenti. Ciò migliora l’efficienza.\nPersonalizzazione: Adattare tutti i componenti del sistema ai carichi di lavoro AI consente ottimizzazioni in tutto lo stack hardware.\nParallelismo: L’integrazione di molti elementi di elaborazione consente un calcolo parallelo massiccio.\nDensità: Una più stretta integrazione consente di impacchettare più transistor e memoria in una determinata area.\nCosto: Le economie di scala derivanti da grandi sistemi integrati possono ridurre i costi.\n\nIn risposta, nuove tecniche di produzione come la fabbricazione su scala di wafer e il confezionamento avanzato consentono ora livelli di integrazione molto più elevati. L’obiettivo è creare complessi di elaborazione AI unificati e specializzati, su misura per il deep learning e altri algoritmi AI. Un’integrazione più stretta è fondamentale per fornire le prestazioni e l’efficienza necessarie per la prossima generazione di AI.\n\nAI su Scala Wafer\nL’intelligenza artificiale su “wafer-scale” adotta un approccio estremamente integrato, producendo un intero wafer di silicio come un gigantesco chip. Ciò differisce drasticamente dalle CPU e GPU convenzionali, che tagliano ogni wafer in molti chip singoli più piccoli. Figura 10.4 mostra un confronto tra Cerebras Wafer Scale Engine 2, che è il chip più grande mai costruito, e la GPU più grande. Mentre alcune GPU possono contenere miliardi di transistor, impallidiscono comunque rispetto alla scala di un chip delle dimensioni di un wafer con oltre un trilione di transistor.\nL’approccio su scala di wafer diverge anche dai progetti system-on-chip più modulari che hanno ancora componenti discreti che comunicano tramite bus. Invece, l’intelligenza artificiale su scala di wafer consente la personalizzazione completa e la stretta integrazione di elaborazione, memoria e interconnessioni nell’intero di die.\n\n\n\n\n\n\nFigura 10.4: Wafer-scale vs. GPU. Fonte: Cerebras.\n\n\n\nProgettando il wafer come un’unità logica integrata, il trasferimento dati tra gli elementi è ridotto al minimo. Ciò fornisce una latenza e un consumo energetico inferiori rispetto ai design discreti system-on-chip o chiplet. Mentre i chiplet possono offrire flessibilità mescolando e abbinando i componenti, la comunicazione tra chiplet è impegnativa. La natura monolitica dell’integrazione su scala wafer elimina questi colli di bottiglia nella comunicazione tra chip.\nTuttavia, la scala ultra-large pone anche difficoltà per la producibilità e la resa con i design su scala wafer. Difetti in qualsiasi regione del wafer possono rendere (alcune parti del) chip inutilizzabile. Sono necessarie tecniche di litografia specializzate per produrre tali matrici di grandi dimensioni. Quindi, l’integrazione su scala wafer persegue i massimi guadagni in termini di prestazioni dall’integrazione ma richiede il superamento di sostanziali sfide di fabbricazione.\nVideo 10.1 fornisce ulteriore contesto sui chip AI su scala wafer.\n\n\n\n\n\n\nVideo 10.1: Wafer-scale AI Chips\n\n\n\n\n\n\n\n\nChiplet per AI\nIl design chiplet si riferisce a un’architettura semiconduttrice in cui un singolo circuito integrato (IC) è costruito da più componenti più piccoli e individuali noti come chiplet. Ogni chiplet è un blocco funzionale autonomo, in genere specializzato per un’attività o funzionalità specifica. Questi chiplet sono quindi interconnessi su un substrato o un package più grande per creare un sistema coeso. Figura 10.5 illustra questo concetto. Per l’hardware AI, i chiplet consentono di combinare diversi tipi di chip ottimizzati per attività come moltiplicazione di matrici, spostamento di dati, I/O analogico e memorie specializzate. Questa integrazione eterogenea differisce notevolmente dall’integrazione wafer-scale, in cui tutta la logica è prodotta come un unico chip monolitico. Aziende come Intel e AMD hanno adottato design chiplet per le loro CPU.\nI chiplet sono interconnessi utilizzando tecniche di packaging avanzate come interposer di substrato ad alta densità, impilamento 2.5D/3D e packaging a livello di wafer. Ciò consente di combinare chiplet realizzati con diversi nodi di processo, memorie specializzate e vari motori AI ottimizzati.\n\n\n\n\n\n\nFigura 10.5: Partizionamento chiplet.. Fonte: Vivet et al. (2021).\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar Fuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021. «IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management». IEEE J. Solid-State Circuits 56 (1): 79–97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nEcco alcuni vantaggi chiave dell’uso di chiplet per l’intelligenza artificiale:\n\nFlessibilità: I chiplet consentono la combinazione di diversi tipi di chip, nodi di processo e memorie su misura per ogni funzione. Questo è più modulare rispetto a un design fisso su scala wafer.\nResa: I chiplet più piccoli hanno una resa maggiore rispetto a un gigantesco chip su scala wafer. I difetti sono contenuti nei singoli chiplet.\nCosto: Sfrutta le capacità di produzione esistenti anziché richiedere nuovi processi specializzati. Riduce i costi riutilizzando la fabbricazione assestata.\nCompatibilità: Può integrarsi con architetture di sistema più convenzionali come PCIe e interfacce di memoria DDR standard.\n\nTuttavia, i chiplet devono anche affrontare sfide di integrazione e prestazioni:\n\nDensità inferiore rispetto alla scala wafer, poiché i chiplet sono limitati in termini di dimensioni.\nLatenza aggiuntiva durante la comunicazione tra chiplet rispetto all’integrazione monolitica. Richiede ottimizzazione per interconnessioni a bassa latenza.\nIl packaging avanzato aggiunge complessità rispetto all’integrazione su scala wafer, sebbene ciò sia discutibile.\n\nL’obiettivo principale dei chiplet è trovare il giusto equilibrio tra flessibilità modulare e densità di integrazione per prestazioni AI ottimali. I chiplet mirano a un’accelerazione AI efficiente pur lavorando entro i vincoli delle tecniche di produzione convenzionali. I chiplet prendono una via di mezzo tra gli estremi dell’integrazione su scala wafer e dei componenti completamente discreti. Ciò fornisce vantaggi pratici ma può sacrificare una certa densità computazionale ed efficienza rispetto a un sistema teorico a livello di wafer.\n\n\n\n10.8.2 Elaborazione Nùeuromorfica\nL’elaborazione neuromorfica è un campo emergente che mira a emulare l’efficienza e la robustezza dei sistemi neurali biologici per applicazioni di machine learning. Una differenza fondamentale rispetto alle classiche architetture di Von Neumann è la fusione di memoria ed elaborazione nello stesso circuito (Schuman et al. 2022; Marković et al. 2020; Furber 2016), come illustrato in Figura 10.6. La struttura del cervello ispira questo approccio integrato. Un vantaggio fondamentale è il potenziale per un miglioramento di ordini di grandezza nel calcolo efficiente dal punto di vista energetico rispetto all’hardware AI convenzionale. Ad esempio, le stime prevedono guadagni di 100x-1000x nell’efficienza energetica rispetto agli attuali sistemi basati su GPU per carichi di lavoro equivalenti.\n\nMarković, Danijela, Alice Mizrahi, Damien Querlioz, e Julie Grollier. 2020. «Physics for neuromorphic computing». Nature Reviews Physics 2 (9): 499–510. https://doi.org/10.1038/s42254-020-0208-2.\n\nFurber, Steve. 2016. «Large-scale neuromorphic computing systems». J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\n\n\n\n\nFigura 10.6: Confronto tra l’architettura di von Neumann e l’architettura neuromorfica. Fonte: Schuman et al. (2022).\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. «Opportunities for neuromorphic computing algorithms and applications». Nature Computational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nIntel e IBM stanno guidando gli sforzi commerciali nell’hardware neuromorfico. I chip Loihi e Loihi 2 di Intel (Davies et al. 2018, 2021) offrono core neuromorfici programmabili con apprendimento on-chip. Il dispositivo Northpole (Modha et al. 2023) di IBM comprende oltre 100 milioni di sinapsi a giunzione a tunnel magnetico e 68 miliardi di transistor. Questi chip specializzati offrono vantaggi come un basso consumo energetico per l’inferenza edge.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. «Loihi: A Neuromorphic Manycore Processor with On-Chip Learning». IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya, Gabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, e Sumedh R. Risbud. 2021. «Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook». Proc. IEEE 109 (5): 911–34. https://doi.org/10.1109/jproc.2021.3067593.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. «Neural inference at the frontier of energy, space, and time». Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\nMaass, Wolfgang. 1997. «Networks of spiking neurons: The third generation of neural network models». Neural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\nLe “Spiking neural network (SNN)” (Maass 1997) sono modelli computazionali per hardware neuromorfico. A differenza delle reti neurali profonde che comunicano tramite valori continui, le SNN utilizzano picchi discreti che sono più simili ai neuroni biologici. Questo consente un calcolo efficiente basato sugli eventi anziché un’elaborazione costante. Inoltre, le SNN considerano le caratteristiche temporali e spaziali dei dati di input. Ciò imita meglio le reti neurali biologiche, in cui la tempistica dei picchi neuronali svolge un ruolo importante. Tuttavia, l’addestramento delle SNN rimane impegnativo a causa della complessità temporale aggiunta. Figura 10.7 fornisce una panoramica della metodologia spiking: (a) Diagramma di un neurone; (b) Misura di un potenziale d’azione propagato lungo l’assone [https://it.wikipedia.org/wiki/Assone] di un neurone. Solo il potenziale d’azione è rilevabile lungo l’assone; (c) Il picco del neurone è approssimato con una rappresentazione binaria; (d) Elaborazione guidata dagli eventi; (e) Active Pixel Sensor e Dynamic Vision Sensor.\n\n\n\n\n\n\nFigura 10.7: Spiking neuromorfico. Fonte: Eshraghian et al. (2023).\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, e Wei D. Lu. 2023. «Training Spiking Neural Networks Using Lessons From Deep Learning». Proc. IEEE 111 (9): 1016–54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nSi può anche guardare Video 10.2 linkato di seguito per una spiegazione più dettagliata.\n\n\n\n\n\n\nVideo 10.2: Neuromorphic Computing\n\n\n\n\n\n\nDispositivi nanoelettronici specializzati chiamati memristor (Chua 1971) sono componenti sinaptici nei sistemi neuromorfici. I memristor agiscono come memoria non volatile con conduttanza regolabile, emulando la plasticità delle sinapsi reali. I memristor consentono l’apprendimento in situ senza trasferimenti di dati separati combinando funzioni di memoria ed elaborazione. Tuttavia, la tecnologia dei memristor deve ancora raggiungere la maturità e la scalabilità per l’hardware commerciale.\n\nChua, L. 1971. «Memristor-The missing circuit element». #IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\nL’integrazione della fotonica con il calcolo neuromorfico (Shastri et al. 2021) è emersa di recente come un’area di ricerca attiva. L’uso della luce per il calcolo e la comunicazione consente alte velocità e un consumo energetico ridotto. Tuttavia, la piena realizzazione di sistemi neuromorfici fotonici richiede il superamento di problemi di progettazione e integrazione.\nIl calcolo neuromorfico offre promettenti capacità per un’efficace inferenza edge, ma incontra ostacoli in merito ad algoritmi di addestramento, integrazione dei nanodispositivi e progettazione del sistema. La ricerca multidisciplinare in corso in informatica, ingegneria, scienza dei materiali e fisica sarà fondamentale per sbloccare il pieno potenziale di questa tecnologia per i casi d’uso dell’intelligenza artificiale.\n\n\n10.8.3 Calcolo Analogico\nIl computing analogico è un approccio emergente che utilizza segnali e componenti analogici come condensatori, induttori e amplificatori anziché la logica digitale per il calcolo. Rappresenta le informazioni come segnali elettrici continui anziché 0 e 1 discreti. Ciò consente al calcolo di riflettere direttamente la natura analogica dei dati del mondo reale, evitando errori di digitalizzazione e overhead.\nIl computing analogico ha generato un rinnovato interesse per l’hardware AI efficiente, in particolare per l’inferenza direttamente su dispositivi edge a basso consumo. I circuiti analogici, come la moltiplicazione e la sommatoria al centro delle reti neurali, possono essere utilizzati con un consumo energetico molto basso. Ciò rende l’analogico adatto per l’implementazione di modelli ML su nodi finali con vincoli energetici. Startup come Mythic stanno sviluppando acceleratori AI analogici.\nMentre il computing analogico era popolare nei primi computer, il boom della logica digitale ha portato al suo declino. Tuttavia, l’analogico è convincente per applicazioni di nicchia che richiedono estrema efficienza (Haensch, Gokmen, e Puri 2019). Contrasta con gli approcci neuromorfici digitali che utilizzano ancora picchi digitali per il calcolo. L’analogico può consentire un calcolo di precisione inferiore, ma richiede competenza nella progettazione di circuiti analogici. I compromessi su precisione, complessità di programmazione e costi di fabbricazione rimangono aree di ricerca attive.\n\nHaensch, Wilfried, Tayfun Gokmen, e Ruchir Puri. 2019. «The Next Generation of Deep Learning Hardware: Analog Computing». Proc. IEEE 107 (1): 108–22. https://doi.org/10.1109/jproc.2018.2871057.\n\nHazan, Avi, e Elishai Ezra Tsur. 2021. «Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation». Front. Neurosci. 15 (febbraio): 627221. https://doi.org/10.3389/fnins.2021.627221.\nIl calcolo neuromorfico, che emula i sistemi neurali biologici per un’inferenza ML efficiente, può utilizzare circuiti analogici per implementare i componenti e i comportamenti chiave del cervello. Ad esempio, i ricercatori hanno progettato circuiti analogici per modellare neuroni e sinapsi utilizzando condensatori, transistor e amplificatori operazionali (Hazan e Ezra Tsur 2021). I condensatori possono esibire le dinamiche di picco dei neuroni biologici, mentre gli amplificatori e i transistor forniscono una somma ponderata di input per imitare i dendriti. Le tecnologie a resistore variabile come i memristor possono realizzare sinapsi analogiche con plasticità dipendente dal tempo di picco, che può rafforzare o indebolire le connessioni in base all’attività di picco.\nStartup come SynSense hanno sviluppato chip neuromorfici analogici contenenti questi componenti biomimetici (Bains 2020). Questo approccio analogico si traduce in un basso consumo energetico e un’elevata scalabilità per i dispositivi edge rispetto alle complesse implementazioni SNN digitali.\n\nBains, Sunny. 2020. «The business of building brains». Nature Electronics 3 (7): 348–51. https://doi.org/10.1038/s41928-020-0449-1.\nTuttavia, l’addestramento di SNN analogiche sui chip rimane una sfida aperta. Nel complesso, la realizzazione analogica è una tecnica promettente per fornire l’efficienza, la scalabilità e la plausibilità biologica previste con il calcolo neuromorfico. La fisica dei componenti analogici combinata con la progettazione dell’architettura neurale potrebbe migliorare l’efficienza dell’inferenza rispetto alle reti neurali digitali convenzionali.\n\n\n10.8.4 Elettronica Flessibile\nMentre gran parte della nuova tecnologia hardware nell’area di lavoro ML si è concentrata sull’ottimizzazione e sulla creazione di sistemi più efficienti, c’è una traiettoria parallela che mira ad adattare l’hardware per applicazioni specifiche (Gates 2009; Musk et al. 2019; Tang et al. 2023; Tang, He, e Liu 2022; Kwon e Dong 2022). Una di queste strade è lo sviluppo di elettronica flessibile per casi d’uso AI.\n\nGates, Byron D. 2009. «Flexible Electronics». Science 323 (5921): 1566–67. https://doi.org/10.1126/science.1171230.\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, e Jia Liu. 2023. «Flexible braincomputer interfaces». Nature Electronics 6 (2): 109–18. https://doi.org/10.1038/s41928-022-00913-9.\n\nTang, Xin, Yichun He, e Jia Liu. 2022. «Soft bioelectronics for cardiac interfaces». Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\nL’elettronica flessibile si riferisce a circuiti elettronici e dispositivi fabbricati su substrati flessibili in plastica o polimeri anziché in silicio rigido. A differenza delle schede e dei chip rigidi convenzionali, ciò consente all’elettronica di piegarsi, torcersi e adattarsi a forme irregolari. Figura 10.8 mostra un esempio di un prototipo di dispositivo flessibile che misura in modalità wireless la temperatura corporea, che può essere integrato senza soluzione di continuità in indumenti o cerotti cutanei. La flessibilità e la piegabilità dei materiali elettronici emergenti consentono di integrarli in fattori di forma sottili e leggeri, adatti per applicazioni AI e TinyML embedded.\nL’hardware AI flessibile può adattarsi a superfici curve e funzionare in modo efficiente con budget di potenza in microwatt. La flessibilità consente inoltre fattori di forma arrotolabili o pieghevoli per ridurre al minimo l’ingombro e il peso del dispositivo, ideali per piccoli dispositivi intelligenti portatili e dispositivi indossabili che incorporano TinyML. Un altro vantaggio fondamentale dell’elettronica flessibile rispetto alle tecnologie convenzionali sono i costi di produzione inferiori e i processi di fabbricazione più semplici, che potrebbero democratizzare l’accesso a queste tecnologie. Mentre le maschere in silicio e i costi di fabbricazione in genere costano milioni di dollari, l’hardware flessibile in genere costa solo decine di centesimi per la produzione (Huang et al. 2011; Biggs et al. 2021). Il potenziale di fabbricare elettronica flessibile direttamente su pellicole di plastica utilizzando processi di stampa e rivestimento ad alta produttività può ridurre i costi e migliorare la producibilità su larga scala rispetto ai chip AI rigidi (Musk et al. 2019).\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi Sekitani, Takao Someya, e Kwang-Ting Cheng. 2011. «Pseudo-CMOS: A Design Style for Low-Cost and Robust Flexible Electronics». IEEE Trans. Electron Devices 58 (1): 141–50. https://doi.org/10.1109/ted.2010.2088127.\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony Sou, Catherine Ramsdale, Ken Williamson, Richard Price, e Scott White. 2021. «A natively flexible 32-bit Arm microprocessor». Nature 595 (7868): 532–36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\n\n\n\n\nFigura 10.8: Prototipo di dispositivo flessibile. Fonte: Jabil Circuit.\n\n\n\nIl campo è abilitato dai progressi nei semiconduttori organici e nei nanomateriali che possono essere depositati su pellicole sottili e flessibili. Tuttavia, la fabbricazione rimane impegnativa rispetto ai processi maturi del silicio. I circuiti flessibili attualmente presentano in genere prestazioni inferiori rispetto agli equivalenti rigidi. Tuttavia, promettono di trasformare l’elettronica in materiali leggeri e pieghevoli.\nI casi d’uso dell’elettronica flessibile sono adatti per l’integrazione intima con il corpo umano. Le potenziali applicazioni dell’intelligenza artificiale medica includono sensori biointegrati, “soft robot” e impianti che monitorano o stimolano il sistema nervoso in modo intelligente. In particolare, gli array di elettrodi flessibili potrebbero consentire interfacce neurali a densità più elevata e meno invasive rispetto agli equivalenti rigidi.\nPertanto, l’elettronica flessibile sta inaugurando una nuova era di dispositivi indossabili e sensori corporei, in gran parte grazie alle innovazioni nei transistor organici. Questi componenti consentono un’elettronica più leggera e pieghevole, ideale per dispositivi indossabili, pelle elettronica e dispositivi medici che si adattano al corpo.\nSono adatti per dispositivi bioelettronici in termini di biocompatibilità, aprendo la strada ad applicazioni in interfacce cerebrali e cardiache. Ad esempio, la ricerca sulle interfacce flessibili cervello-computer e sulla bioelettronica morbida per applicazioni cardiache dimostra il potenziale per applicazioni mediche di vasta portata.\nAziende e istituti di ricerca non stanno solo sviluppando e investendo grandi quantità di risorse in elettrodi flessibili, come mostrato nel lavoro di Neuralink (Musk et al. 2019). Tuttavia, stanno anche spingendo i confini per integrare modelli di apprendimento automatico nei sistemi (Kwon e Dong 2022). Questi sensori intelligenti mirano a una simbiosi fluida e duratura con il corpo umano.\n\nMusk, Elon et al. 2019. «An Integrated Brain-Machine Interface Platform With Thousands of Channels». J. Med. Internet Res. 21 (10): e16194. https://doi.org/10.2196/16194.\n\nKwon, Sun Hwa, e Lin Dong. 2022. «Flexible sensors and machine learning for heart monitoring». Nano Energy 102 (novembre): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, e P. W. C. Prasad. 2017. «Ethical Implications of User Perceptions of Wearable Devices». Sci. Eng. Ethics 24 (1): 1–28. https://doi.org/10.1007/s11948-017-9872-8.\n\nGoodyear, Victoria A. 2017. «Social media, apps and wearable technologies: Navigating ethical dilemmas and procedures». Qualitative Research in Sport, Exercise and Health 9 (3): 285–302. https://doi.org/10.1080/2159676x.2017.1303790.\n\nFarah, Martha J. 2005. «Neuroethics: The practical and the philosophical». Trends Cogn. Sci. 9 (1): 34–40. https://doi.org/10.1016/j.tics.2004.12.001.\n\nRoskies, Adina. 2002. «Neuroethics for the New Millenium». Neuron 35 (1): 21–23. https://doi.org/10.1016/s0896-6273(02)00763-8.\nEticamente, l’incorporazione di sensori intelligenti basati sull’apprendimento automatico nel corpo solleva importanti questioni. Le problematiche relative alla privacy dei dati, al consenso informato e alle implicazioni sociali a lungo termine di tali tecnologie sono al centro del lavoro in corso in neuroetica e bioetica (Segura Anaya et al. 2017; Goodyear 2017; Farah 2005; Roskies 2002). Il campo sta progredendo a un ritmo che richiede progressi paralleli nei parametri etici per guidare lo sviluppo e l’implementazione responsabili di queste tecnologie. Sebbene vi siano limitazioni e ostacoli etici da superare, le prospettive per l’elettronica flessibile sono ampie e promettono molto per la ricerca e le applicazioni future.\n\n\n10.8.5 Tecnologie delle Memorie\nLe tecnologie delle memorie sono fondamentali per l’hardware AI, ma la DDR DRAM e la SRAM convenzionali creano colli di bottiglia. I carichi di lavoro AI richiedono un’elevata larghezza di banda (&gt;1 TB/s). Le applicazioni scientifiche estreme dell’AI richiedono una latenza estremamente bassa (&lt;50 ns) per alimentare i dati alle unità di calcolo (Duarte et al. 2022), un’elevata densità (&gt;128 Gb) per archiviare grandi parametri di modelli e set di dati e un’eccellente efficienza energetica (&lt;100 fJ/b) per uso embedded (Verma et al. 2019). Sono necessarie nuove memorie per soddisfare queste esigenze. Le opzioni emergenti includono diverse nuove tecnologie:\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, e Vijay Janapa Reddi. 2022. «FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning». ArXiv preprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay, Lung-Yen Chen, Bonan Zhang, e Peter Deaville. 2019. «In-Memory Computing: Advances and Prospects». IEEE Solid-State Circuits Mag. 11 (3): 43–55. https://doi.org/10.1109/mssc.2019.2922889.\n\nLa RAM resistiva (ReRAM) può migliorare la densità con semplici array passivi. Tuttavia, permangono dei problemi legati alla variabilità (Chi et al. 2016).\nLa “Phase change memory (PCM)” [memoria a cambiamento di fase ] sfrutta le proprietà uniche del vetro calcogenuro. Le fasi cristalline e amorfe hanno resistenze diverse. L’Optane DCPMM di Intel fornisce PCM veloci (100 ns) e ad alta resistenza. Tuttavia, le sfide includono cicli di scrittura limitati e corrente di reset elevata (Burr et al. 2016).\nLo stacking 3D può anche aumentare la densità di memoria e la larghezza di banda integrando verticalmente strati di memoria con interconnessioni TSV (Loh 2008). Ad esempio, HBM fornisce interfacce larghe 1024 bit.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng, Jau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. «Recent Progress in Phase-Change?Pub _newline ?Memory Technology». IEEE Journal on Emerging and Selected Topics in Circuits and Systems 6 (2): 146–62. https://doi.org/10.1109/jetcas.2016.2547718.\n\nLoh, Gabriel H. 2008. «3D-Stacked Memory Architectures for Multi-core Processors». ACM SIGARCH Computer Architecture News 36 (3): 453–64. https://doi.org/10.1145/1394608.1382159.\nLe nuove tecnologie di memoria, con le loro innovative architetture e materiali cellulari, sono fondamentali per sbloccare il prossimo livello di prestazioni ed efficienza hardware AI. Realizzare i loro vantaggi nei sistemi commerciali rimane una sfida continua.\nL’elaborazione in-memory sta guadagnando terreno come promettente strada per ottimizzare l’apprendimento automatico e i carichi di lavoro di elaborazione ad alte prestazioni. Al centro, la tecnologia colloca l’archiviazione e l’elaborazione dei dati per migliorare l’efficienza energetica e ridurre la latenza Wong et al. (2012). Due tecnologie chiave sotto questo ombrello sono la “Resistive RAM (ReRAM)” e il “Processing-In-Memory (PIM)”.\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu, Pang-Shiu Chen, Byoungil Lee, Frederick T. Chen, e Ming-Jinn Tsai. 2012. «MetalOxide RRAM». Proc. IEEE 100 (6): 1951–70. https://doi.org/10.1109/jproc.2012.2190369.\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, e Yuan Xie. 2016. «Prime: a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory». ACM SIGARCH Computer Architecture News 44 (3): 27–39. https://doi.org/10.1145/3007787.3001140.\nReRAM (Wong et al. 2012) e PIM (Chi et al. 2016) sono le colonne portanti per l’elaborazione in memoria, l’archiviazione e l’elaborazione dei dati nella stessa posizione. ReRAM si concentra su questioni di uniformità, resistenza, conservazione, funzionamento multi-bit e scalabilità. D’altro canto, PIM coinvolge unità CPU integrate direttamente in array di memoria, specializzate per attività come la moltiplicazione di matrici, che sono centrali nei calcoli AI.\nQueste tecnologie trovano applicazioni nei carichi di lavoro AI e nell’elaborazione ad alte prestazioni, dove la sinergia di storage e calcolo può portare a significativi guadagni in termini di prestazioni. L’architettura è particolarmente utile per le attività di elaborazione intensiva comuni nei modelli di apprendimento automatico.\nMentre le tecnologie di elaborazione in memoria come ReRAM e PIM offrono interessanti prospettive di efficienza e prestazioni, presentano le loro sfide, come l’uniformità dei dati e i problemi di scalabilità in ReRAM (Imani, Rahimi, e S. Rosing 2016). Tuttavia, il campo è maturo per l’innovazione e affrontare queste limitazioni può aprire nuove frontiere nell’AI e nell’elaborazione ad alte prestazioni.\n\nImani, Mohsen, Abbas Rahimi, e Tajana S. Rosing. 2016. «Resistive Configurable Associative Memory for Approximate Computing». In Proceedings of the 2016 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1327–32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\n10.8.6 Calcolo Ottico\nNell’accelerazione dell’intelligenza artificiale, un’area di interesse in rapida crescita risiede nelle nuove tecnologie che si discostano dai paradigmi tradizionali. Alcune tecnologie emergenti menzionate sopra, come l’elettronica flessibile, il calcolo in memoria o persino il calcolo neuromorfico, stanno per diventare realtà, date le loro innovazioni e applicazioni rivoluzionarie. Una delle frontiere promettenti e all’avanguardia della prossima generazione è la tecnologia del calcolo ottico H. Zhou et al. (2022). Aziende come [LightMatter] stanno aprendo la strada all’uso della fotonica per i calcoli, utilizzando così i fotoni al posto degli elettroni per la trasmissione e il calcolo dei dati.\n\nZhou, Hailong, Jianji Dong, Junwei Cheng, Wenchan Dong, Chaoran Huang, Yichen Shen, Qiming Zhang, et al. 2022. «Photonic matrix multiplication lights up photonic accelerator and beyond». Light: Science &amp; Applications 11 (1): 30. https://doi.org/10.1038/s41377-022-00717-8.\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H. P. Pernice, Harish Bhaskaran, C. D. Wright, e Paul R. Prucnal. 2021. «Photonics for artificial intelligence and neuromorphic computing». Nat. Photonics 15 (2): 102–14. https://doi.org/10.1038/s41566-020-00754-y.\nIl calcolo ottico utilizza fotoni e dispositivi fotonici anziché i tradizionali circuiti elettronici per il calcolo e l’elaborazione dei dati. Trae ispirazione dai collegamenti di comunicazione in fibra ottica che si basano sulla luce per un trasferimento dati rapido ed efficiente (Shastri et al. 2021). La luce può propagarsi con una perdita molto inferiore rispetto agli elettroni dei semiconduttori, consentendo vantaggi intrinseci in termini di velocità ed efficienza.\nAlcuni vantaggi specifici dell’elaborazione ottica includono:\n\nAlta produttività: I fotoni possono trasmettere con larghezze di banda &gt;100 Tb/s utilizzando il multiplexing a divisione di lunghezza d’onda.\nBassa latenza: I fotoni interagiscono su scale temporali di femtosecondi, milioni di volte più velocemente dei transistor al silicio.\nParallelismo: Più segnali di dati possono propagarsi simultaneamente attraverso lo stesso mezzo ottico.\nBassa potenza: I circuiti fotonici che utilizzano guide d’onda e risonatori possono ottenere una logica e una memoria complesse con solo microwatt di potenza.\n\nTuttavia, l’elaborazione ottica deve attualmente affrontare sfide significative:\n\nMancanza di memoria ottica equivalente alla RAM elettronica\nRichiede la conversione tra domini ottici ed elettrici.\nSet limitato di componenti ottici disponibili rispetto al ricco ecosistema elettronico.\nMetodi di integrazione immaturi per combinare la fotonica con i tradizionali chip CMOS.\nModelli di programmazione complessi richiesti per gestire il parallelismo.\n\nDi conseguenza, l’elaborazione ottica è ancora in una fase di ricerca molto precoce nonostante il suo potenziale promettente. Tuttavia, le innovazioni tecniche potrebbero consentirgli di integrare l’elettronica e sbloccare guadagni di prestazioni per i carichi di lavoro AI. Aziende come Lightmatter sono pioniere nei primi acceleratori ottici AI. A lungo termine, se le sfide chiave saranno superate, potrebbe rappresentare un substrato di elaborazione rivoluzionario.\n\n\n10.8.7 Quantum Computing\nI computer quantistici sfruttano fenomeni unici della fisica quantistica, come la sovrapposizione e l’entanglement, per rappresentare ed elaborare informazioni in modi non possibili in modo classico. Invece dei bit binari, l’unità fondamentale è il bit quantistico o qubit. A differenza dei bit classici, che sono limitati a 0 o 1, i qubit possono esistere simultaneamente in una sovrapposizione di entrambi gli stati a causa degli effetti quantistici.\nAnche più qubit possono essere entangled, portando a una densità di informazioni esponenziale ma introducendo risultati probabilistici. La sovrapposizione consente il calcolo parallelo su tutti gli stati possibili, mentre l’entanglement consente correlazioni non locali tra qubit.\nGli algoritmi quantistici manipolano attentamente questi effetti meccanici quantistici intrinseci per risolvere problemi come l’ottimizzazione o la ricerca in modo più efficiente rispetto alle loro controparti classiche in teoria.\n\nTraining più rapido di reti neurali profonde sfruttando il parallelismo quantistico per operazioni di algebra lineare.\nGli algoritmi ML quantistici efficienti sfruttano le capacità uniche dei qubit.\nReti neurali quantistiche con effetti quantistici intrinseci integrati nell’architettura del modello.\nOttimizzatori quantistici che sfruttano algoritmi di “annealing” quantistica o adiabatici per problemi di ottimizzazione combinatoria.\n\nTuttavia, gli stati quantistici sono fragili e soggetti a errori che richiedono protocolli di correzione degli errori. La natura non intuitiva della programmazione quantistica introduce anche sfide non presenti nell’informatica classica.\n\nI bit quantistici rumorosi e fragili sono difficili da scalare. Il più grande computer quantistico odierno ha meno di 1000 qubit.\nInsieme limitato di porte e circuiti quantistici disponibili rispetto alla programmazione classica.\nMancanza di set di dati e benchmark per valutare l’apprendimento automatico quantistico in domini pratici.\n\nSebbene un vantaggio quantistico significativo per l’apprendimento automatico sia ancora lontano, la ricerca attiva presso aziende come D-Wave, Rigetti e IonQ sta facendo progredire l’ingegneria informatica quantistica e gli algoritmi quantistici. Le principali aziende tecnologiche come Google, IBM e Microsoft stanno esplorando attivamente l’informatica quantistica. Google ha recentemente annunciato un processore quantistico a 72 qubit chiamato Bristlecone e prevede di costruire un sistema quantistico commerciale a 49 qubit. Microsoft ha anche un programma di ricerca attivo nell’informatica quantistica topologica e collabora con la startup quantistica IonQ\nLe tecniche quantistiche potrebbero prima fare breccia nell’ottimizzazione prima di un’adozione più generalizzata dell’apprendimento automatico. La realizzazione del pieno potenziale dell’apprendimento automatico quantistico attende importanti traguardi nello sviluppo dell’hardware quantistico e nella maturità dell’ecosistema.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#tendenze-future",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#tendenze-future",
    "title": "10  Accelerazione IA",
    "section": "10.9 Tendenze Future",
    "text": "10.9 Tendenze Future\nIn questo capitolo, l’attenzione principale è stata rivolta alla progettazione di hardware specializzato ottimizzato per carichi di lavoro e algoritmi di machine learning. Questa discussione ha riguardato le architetture personalizzate di GPU e TPU per l’addestramento e l’inferenza delle reti neurali. Tuttavia, una direzione di ricerca emergente sta sfruttando l’apprendimento automatico per facilitare il processo di progettazione hardware stesso.\nIl processo di progettazione hardware comprende molte fasi complesse, tra cui specifica, modellazione di alto livello, simulazione, sintesi, verifica, prototipazione e fabbricazione. Gran parte di questo processo richiede tradizionalmente una vasta competenza umana, impegno e tempo. Tuttavia, i recenti progressi nell’apprendimento automatico stanno consentendo l’automazione e il miglioramento di parti del flusso di lavoro di progettazione hardware utilizzando tecniche di apprendimento automatico.\nEcco alcuni esempi di come l’apprendimento automatico sta trasformando la progettazione hardware:\n\nSintesi di circuiti automatizzata tramite apprendimento per rinforzo: Anziché realizzare manualmente progetti a livello di transistor, gli agenti di apprendimento automatico come l’apprendimento per rinforzo possono imparare a collegare porte logiche e generare automaticamente layout di circuiti. Ciò può accelerare il lungo processo di sintesi.\nSimulazione ed emulazione hardware basate su ML: I modelli di reti neurali profonde possono essere addestrati per prevedere come si comporterà un progetto hardware in diverse condizioni. Ad esempio, i modelli di apprendimento profondo possono essere addestrati per prevedere i conteggi dei cicli per determinati carichi di lavoro. Ciò consente una simulazione più rapida e accurata rispetto alle simulazioni RTL tradizionali.\nPianificazione automatizzata dei chip mediante algoritmi ML: La pianificazione dei chip comporta il posizionamento ottimale di diversi componenti su un die. Algoritmi evolutivi come quelli genetici e altri algoritmi ML come l’apprendimento per rinforzo vengono utilizzati per esplorare le opzioni di pianificazione. Ciò può migliorare significativamente i posizionamenti manuali di pianificazione in termini di tempi di consegna più rapidi e qualità dei posizionamenti.\nOttimizzazione dell’architettura basata su ML: Le nuove architetture hardware, come quelle per gli acceleratori ML efficienti, possono essere generate e ottimizzate automaticamente tramite la ricerca nello spazio di progettazione architettonica. Gli algoritmi di apprendimento automatico possono cercare efficacemente ampi spazi di progettazione architettonica.\n\nL’applicazione del ML all’automazione della progettazione hardware promette di rendere il processo più veloce, più economico e più efficiente. Apre possibilità di progettazione che richiederebbero più di una progettazione manuale. L’uso del ML nella progettazione hardware è un’area di ricerca attiva e di distribuzione precoce, e studieremo le tecniche coinvolte e il loro potenziale trasformativo.\n\n10.9.1 ML per l’automazione della progettazione hardware\nUna grande opportunità per l’apprendimento automatico nella progettazione hardware è l’automazione di parti del complesso e noioso flusso di lavoro di progettazione. Con “Hardware design automation (HDA)” ci si riferisce in generale all’uso di tecniche ML come l’apprendimento per rinforzo, algoritmi genetici e reti neurali per automatizzare attività come sintesi, verifica, floorplanning e altro. Ecco alcuni esempi di dove l’ML per HDA mostra una vera promessa:\n\nSintesi di circuiti automatizzata: La sintesi di circuiti comporta la conversione di una descrizione di alto livello della logica desiderata in un’implementazione di netlist a livello di gate ottimizzata. Questo processo complesso ha molte considerazioni e compromessi di progettazione. Gli agenti ML possono essere addestrati tramite l’apprendimento per rinforzo G. Zhou e Anderson (2023) per esplorare lo spazio di progettazione e produrre automaticamente sintesi ottimizzate. Startup come Symbiotic EDA stanno portando questa tecnologia sul mercato.\nAutomated chip floorplanning: Il Floorplanning si riferisce al posizionamento strategico di diversi componenti su un’area del chip. Algoritmi di ricerca come algoritmi genetici (Valenzuela e Wang 2000) e apprendimento per rinforzo (Mirhoseini et al. (2021), Agnesina et al. (2023)) possono essere utilizzati per automatizzare l’ottimizzazione il floorplan per ridurre al minimo la lunghezza dei collegamenti, il consumo di energia e altri obiettivi. Questi “floor planners” assistiti da ML automatizzati sono estremamente preziosi man mano che aumenta la complessità dei chip.\nSimulatori hardware ML: L’addestramento di modelli di reti neurali profonde per prevedere le prestazioni dei progetti hardware, poiché i simulatori possono accelerare il processo di simulazione di oltre 100 volte rispetto alle simulazioni architettoniche e RTL tradizionali.\nTraduzione automatica del codice: La conversione di linguaggi di descrizione hardware come Verilog in implementazioni RTL ottimizzate è fondamentale ma richiede molto tempo. I modelli ML possono essere addestrati per agire come agenti traduttori e automatizzare questo processo.\n\n\nZhou, Guanglei, e Jason H. Anderson. 2023. «Area-Driven FPGA Logic Synthesis Using Reinforcement Learning». In Proceedings of the 28th Asia and South Pacific Design Automation Conference, 159–65. ACM. https://doi.org/10.1145/3566097.3567894.\n\nValenzuela, Christine L, e Pearl Y Wang. 2000. «A genetic algorithm for VLSI floorplanning». In Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 1820, 2000 Proceedings 6, 671–80. Springer.\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. «A graph placement methodology for fast chip design». Nature 594 (7862): 207–12. https://doi.org/10.1038/s41586-021-03544-w.\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta, Austin Jiao, Ben Keller, Brucek Khailany, e Haoxing Ren. 2023. «AutoDMP: Automated DREAMPlace-based Macro Placement». In Proceedings of the 2023 International Symposium on Physical Design, 149–57. ACM. https://doi.org/10.1145/3569052.3578923.\nI vantaggi dell’HDA che utilizza ML sono tempi di progettazione ridotti, ottimizzazioni superiori ed esplorazione di spazi di progettazione troppo complessi per approcci manuali. Ciò può accelerare lo sviluppo hardware e portare a progetti migliori.\nLe sfide includono i limiti della generalizzazione ML, la natura black-box di alcune tecniche e compromessi sull’accuratezza. Tuttavia, la ricerca sta rapidamente avanzando per affrontare questi problemi e rendere le soluzioni HDA ML robuste e affidabili per l’uso in produzione. HDA fornisce un’importante via per ML per trasformare la progettazione hardware.\n\n\n10.9.2 Simulazione e Verifica Hardware Basate su ML\nLa simulazione e la verifica dei progetti hardware sono fondamentali prima della produzione per garantire che il progetto si comporti come previsto. Gli approcci tradizionali come la simulazione “register-transfer level” (RTL) sono complessi e richiedono molto tempo. Il ML introduce nuove opportunità per migliorare la simulazione e la verifica dell’hardware. Ecco alcuni esempi:\n\nModellazione surrogata per la simulazione: Modelli surrogati di un progetto altamente accurati possono essere creati utilizzando reti neurali. Questi modelli prevedono gli output dagli input molto più velocemente della simulazione RTL, consentendo una rapida esplorazione dello spazio di progettazione. Aziende come Ansys utilizzano questa tecnica.\nSimulatori ML: Grandi modelli di reti neurali possono essere addestrati su simulazioni RTL per imparare a imitare la funzionalità di un progetto hardware. Una volta addestrato, il modello NN può essere un simulatore altamente efficiente per test di regressione e altre attività. Graphcore ha dimostrato un’accelerazione di oltre 100 volte con questo approccio.\nVerifica formale tramite ML: La verifica formale dimostra matematicamente le proprietà di un progetto. Le tecniche di ML possono aiutare a generare proprietà di verifica e imparare a risolvere le complesse prove formali necessarie, automatizzando parti di questo processo impegnativo. Startup come Cortical.io stanno introducendo sul mercato soluzioni di verifica ML formali.\nRilevamento di bug: I modelli ML possono essere addestrati per elaborare progetti hardware e identificare potenziali problemi. Ciò aiuta i progettisti umani a ispezionare progetti complessi e a trovare bug. Facebook ha mostrato modelli di rilevamento di bug per l’hardware dei suoi server.\n\nI principali vantaggi dell’applicazione di ML alla simulazione e alla verifica sono tempi di esecuzione più rapidi per la convalida del progetto, test più rigorosi e riduzione del lavoro umano. Le sfide includono la verifica della correttezza del modello ML e la gestione dei casi limite. ML promette di accelerare significativamente i flussi di lavoro di test.\n\n\n10.9.3 ML per Architetture Hardware Efficienti\nUn obiettivo chiave è la progettazione di architetture hardware ottimizzate per prestazioni, potenza ed efficienza. ML introduce nuove tecniche per automatizzare e migliorare l’esplorazione dello spazio di progettazione dell’architettura per hardware generico e specializzato come gli acceleratori ML. Alcuni esempi promettenti sono:\n\nRicerca di architetture per hardware: Tecniche di ricerca come algoritmi evolutivi (Kao e Krishna 2020), ottimizzazione bayesiana (Reagen et al. (2017), Bhardwaj et al. (2020)), apprendimento per rinforzo (Kao, Jeong, e Krishna (2020), Krishnan et al. (2022)) possono generare automaticamente nuove architetture hardware mutando e mescolando attributi di progettazione come dimensione della cache, numero di unità parallele, larghezza di banda della memoria e così via. Ciò consente un’esplorazione efficiente di ampi spazi di progettazione.\nModellazione predittiva per l’ottimizzazione: I modelli ML possono essere addestrati per prevedere metriche di prestazioni, potenza ed efficienza hardware per una determinata architettura. Questi diventano “modelli surrogati” (Krishnan et al. 2023) per una rapida ottimizzazione ed esplorazione dello spazio sostituendo lunghe simulazioni.\nOttimizzazione dell’acceleratore specializzato: Per chip specializzati come unità di elaborazione tensore per AI, tecniche di ricerca architettura automatizzata basate su algoritmi ML (D. Zhang et al. 2022) promettono di trovare progetti rapidi ed efficienti.\n\n\nKao, Sheng-Chun, e Tushar Krishna. 2020. «Gamma: automating the HW mapping of DNN models on accelerators via genetic algorithm». In Proceedings of the 39th International Conference on Computer-Aided Design, 1–9. ACM. https://doi.org/10.1145/3400302.3415639.\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, e David Brooks. 2017. «A case for efficient accelerator design space exploration via Bayesian optimization». In 2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 1–6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel Hernández-Lobato, e Gu-Yeon Wei. 2020. «A comprehensive methodology to determine optimal coherence interfaces for many-accelerator SoCs». In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design, 145–50. ACM. https://doi.org/10.1145/3370748.3406564.\n\nKao, Sheng-Chun, Geonhwa Jeong, e Tushar Krishna. 2020. «ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning». In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 622–36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang, Izzeddin Gur, Vijay Janapa Reddi, e Aleksandra Faust. 2022. «Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration». https://arxiv.org/abs/2211.16385.\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, e Azalia Mirhoseini. 2022. «A full-stack search technique for domain optimized deep learning accelerators». In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 27–42. ASPLOS ’22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\nI vantaggi dell’utilizzo di ML includono un’esplorazione dello spazio di progettazione superiore, ottimizzazione automatizzata e riduzione dello sforzo manuale. Le sfide includono lunghi tempi di training per alcune tecniche e limitazioni degli ottimi locali. Tuttavia, ML per l’architettura hardware ha un grande potenziale per rivelare miglioramenti in termini di prestazioni ed efficienza.\n\n\n10.9.4 ML per Ottimizzare la Produzione e Ridurre i Difetti\nUna volta completata la progettazione hardware, si passa alla produzione. Tuttavia, variabilità e difetti durante la produzione possono influire su rese e qualità. Le tecniche ML vengono ora applicate per migliorare i processi di fabbricazione e ridurre i difetti. Ecco alcuni esempi:\n\nManutenzione predittiva: I modelli ML possono analizzare i dati dei sensori delle apparecchiature nel tempo e identificare segnali che prevedono le esigenze di manutenzione prima del guasto. Ciò consente una manutenzione proattiva, che può essere molto utile nel costoso processo di fabbricazione.\nOttimizzazione del processo: I modelli di apprendimento supervisionato possono essere addestrati sui dati di processo per identificare i fattori che portano a basse rese. I modelli possono quindi ottimizzare i parametri per migliorare rese, produttività o coerenza.\nPrevisione della resa: Analizzando i dati di prova da progetti realizzati utilizzando tecniche come alberi di regressione, i modelli ML possono prevedere le rese all’inizio della produzione, consentendo aggiustamenti del processo.\nRilevamento dei difetti: Le tecniche di visione artificiale ML possono essere applicate alle immagini dei progetti per identificare difetti invisibili all’occhio umano. Ciò consente un controllo di qualità di precisione e un’analisi delle cause principali.\nAnalisi proattiva dei guasti: I modelli ML possono aiutare a prevedere, diagnosticare e prevenire i problemi che portano a difetti e guasti a valle analizzando i dati di processo strutturati e non strutturati.\n\nL’applicazione del ML alla produzione consente l’ottimizzazione dei processi, il controllo di qualità in tempo reale, la manutenzione predittiva e rese più elevate. Le sfide includono la gestione di dati di produzione complessi e varianti. Ma il ML è pronto a trasformare la produzione di semiconduttori.\n\n\n10.9.5 Verso Modelli di Base per la Progettazione Hardware\nCome abbiamo visto, l’apprendimento automatico sta aprendo nuove possibilità nel flusso di lavoro di progettazione hardware, dalle specifiche alla produzione. Tuttavia, le attuali tecniche di ML hanno ancora una portata limitata e richiedono un’ampia progettazione specifica per dominio. La visione a lungo termine è lo sviluppo di sistemi di intelligenza artificiale generali che possono essere applicati con versatilità in tutte le attività di progettazione hardware.\nPer realizzare appieno questa visione, sono necessari investimenti e ricerca per sviluppare modelli di base per la progettazione hardware. Si tratta di modelli e architetture ML unificati e generici che possono apprendere complesse competenze di progettazione hardware con i dati di training e gli obiettivi corretti.\nLa realizzazione di modelli di base per la progettazione hardware end-to-end richiederà quanto segue:\n\nAccumulare grandi set di dati di alta qualità ed etichettati in tutte le fasi di progettazione hardware per addestrare i modelli di base.\nProgressi nelle tecniche ML multimodali e multi-task per gestire la diversità di dati e attività di progettazione hardware.\nInterfacce e layer di astrazione per collegare i modelli di base ai flussi e agli strumenti di progettazione esistenti.\nSviluppo di ambienti di simulazione e benchmark per addestrare e testare i modelli di base sulle capacità di progettazione hardware.\nMetodi per spiegare e interpretare le decisioni di progettazione dei modelli ML e le ottimizzazioni per attendibilità e verifica.\nTecniche di compilazione per ottimizzare i modelli di base per un’implementazione efficiente su piattaforme hardware.\n\nSebbene siano ancora in corso ricerche significative, i modelli di base rappresentano l’obiettivo a lungo termine più trasformativo per l’infusione dell’IA nel processo della progettazione hardware. Democratizzare la progettazione hardware tramite sistemi ML versatili e automatizzati promette di aprire una nuova era di progettazione di chip ottimizzata, efficiente e innovativa. Il viaggio che ci attende è pieno di sfide e opportunità aperte.\nSe sei interessato alla progettazione di architetture per computer assistite da ML (Krishnan et al. 2023), invitiamo a leggere Architecture 2.0.\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023. «ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design». In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–16. ACM. https://doi.org/10.1145/3579371.3589049.\nIn alternativa, si può guardare Video 10.3 for more details.\n\n\n\n\n\n\nVideo 10.3: Architecture 2.0",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#conclusione",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#conclusione",
    "title": "10  Accelerazione IA",
    "section": "10.10 Conclusione",
    "text": "10.10 Conclusione\nL’accelerazione hardware specializzata è diventata indispensabile per abilitare applicazioni di intelligenza artificiale performanti ed efficienti, poiché modelli e set di dati esplodono in complessità. Questo capitolo ha esaminato i limiti dei processori generici come le CPU per i carichi di lavoro di intelligenza artificiale. La loro mancanza di parallelismo e di throughput computazionale non consente di addestrare o eseguire rapidamente reti neurali profonde all’avanguardia. Queste motivazioni hanno guidato le innovazioni negli acceleratori personalizzati.\nAbbiamo esaminato GPU, TPU, FPGA e ASIC progettati specificamente per le operazioni matematiche intensive inerenti alle reti neurali. Coprendo questo spettro di opzioni, abbiamo mirato a fornire un framework per ragionare attraverso la selezione dell’acceleratore in base a vincoli relativi a flessibilità, prestazioni, potenza, costi e altri fattori.\nAbbiamo anche esplorato il ruolo del software nell’abilitazione e nell’ottimizzazione attive dell’accelerazione dell’intelligenza artificiale. Ciò abbraccia astrazioni di programmazione, framework, compilatori e simulatori. Abbiamo discusso della progettazione congiunta hardware-software come metodologia proattiva per la creazione di sistemi di intelligenza artificiale più olistici integrando strettamente l’innovazione degli algoritmi e i progressi hardware.\nMa c’è molto di più in arrivo! Frontiere entusiasmanti come l’informatica analogica, le reti neurali ottiche e l’apprendimento automatico quantistico rappresentano direzioni di ricerca attive che potrebbero sbloccare miglioramenti di ordini di grandezza in termini di efficienza, velocità e scala rispetto ai paradigmi attuali.\nIn definitiva, l’accelerazione hardware specializzata rimane indispensabile per sbloccare le prestazioni e l’efficienza necessarie per soddisfare la promessa dell’intelligenza artificiale dal cloud all’edge. Ci auguriamo che questo capitolo fornisca utili informazioni di base e approfondimenti sulla rapida innovazione che si sta verificando in questo dominio.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#sec-ai-acceleration-resource",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#sec-ai-acceleration-resource",
    "title": "10  Accelerazione IA",
    "section": "10.11 Risorse",
    "text": "10.11 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 10.1\nVideo 10.2\nVideo 10.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\n\nEsercizio 10.1\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\n\nProssimamente.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html",
    "href": "contents/benchmarking/benchmarking.it.html",
    "title": "11  Benchmarking AI",
    "section": "",
    "text": "11.1 Introduzione\nIl benchmarking fornisce le misure essenziali necessarie per guidare il progresso dell’apprendimento automatico e comprendere veramente le prestazioni del sistema. Come ha affermato il fisico Lord Kelvin, “Misurare è conoscere”. I benchmark ci consentono di conoscere quantitativamente le capacità di diversi modelli, software e hardware. Consentono agli sviluppatori di ML di misurare il tempo di inferenza, l’utilizzo della memoria, il consumo energetico e altre metriche che caratterizzano un sistema. Inoltre, i benchmark creano processi standardizzati per la misurazione, consentendo confronti equi tra diverse soluzioni.\nQuando i benchmark vengono mantenuti nel tempo, diventano fondamentali per catturare i progressi attraverso generazioni di algoritmi, set di dati e hardware. I modelli e le tecniche che stabiliscono nuovi record sui benchmark di ML da un anno all’altro dimostrano miglioramenti tangibili in ciò che è possibile per l’apprendimento automatico “on-device”. Utilizzando i benchmark per misurare, i professionisti di ML possono conoscere le capacità reali dei loro sistemi e avere la certezza che ogni passaggio rifletta un progresso autentico verso lo stato dell’arte.\nIl benchmarking ha diversi obiettivi e scopi importanti che guidano la sua implementazione per i sistemi di apprendimento automatico.\nQuesto capitolo tratterà i 3 tipi di benchmark AI, le metriche standard, gli strumenti e le tecniche che i progettisti utilizzano per ottimizzare i loro sistemi e le sfide e le tendenze nel benchmarking.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#sec-benchmarking-ai",
    "href": "contents/benchmarking/benchmarking.it.html#sec-benchmarking-ai",
    "title": "11  Benchmarking AI",
    "section": "",
    "text": "Valutazione delle prestazioni. Ciò comporta la valutazione di parametri chiave come la velocità, l’accuratezza e l’efficienza di un dato modello. Ad esempio, in un contesto TinyML, è fondamentale confrontare la rapidità con cui un assistente vocale può riconoscere i comandi, poiché ciò valuta le prestazioni in tempo reale.\nValutazione delle risorse. Ciò significa valutare l’impatto del modello sulle risorse critiche del sistema, tra cui durata della batteria, utilizzo della memoria e sovraccarico computazionale. Un esempio rilevante è il confronto del consumo della batteria di due diversi algoritmi di riconoscimento delle immagini in esecuzione su un dispositivo indossabile.\nValidazione e verifica. Il benchmarking aiuta a garantire che il sistema funzioni correttamente e soddisfi i requisiti specificati. Un modo è quello di controllare l’accuratezza di un algoritmo, come un cardiofrequenzimetro su uno smartwatch, rispetto alle letture di apparecchiature di livello medico come forma di validazione clinica.\nAnalisi competitiva. Ciò consente di confrontare le soluzioni con le offerte concorrenti sul mercato. Ad esempio, il benchmarking di un modello personalizzato di rilevamento di oggetti rispetto ai benchmark TinyML comuni come MobileNet e Tiny-YOLO.\nCredibilità. I benchmark accurati sostengono la credibilità delle soluzioni AI e delle organizzazioni che le sviluppano. Dimostrano un impegno verso trasparenza, onestà e qualità, essenziali per creare fiducia con utenti e stakeholder.\nRegolamentazione e Standardizzazione. Man mano che il settore dell’AI continua a crescere, cresce anche la necessità di regolamentazione e standardizzazione per garantire che le soluzioni AI siano sicure, etiche ed efficaci. I benchmark accurati e affidabili sono essenziali per questo quadro normativo, poiché forniscono i dati e le prove necessari per valutare la conformità con gli standard del settore e i requisiti legali.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#contesto-storico",
    "href": "contents/benchmarking/benchmarking.it.html#contesto-storico",
    "title": "11  Benchmarking AI",
    "section": "11.2 Contesto Storico",
    "text": "11.2 Contesto Storico\n\n11.2.1 Benchmark Standard\nL’evoluzione dei benchmark nell’informatica illustra vividamente l’incessante ricerca dell’eccellenza e dell’innovazione da parte del settore. Nei primi giorni dell’informatica, negli anni ’60 e ’70, i benchmark erano rudimentali e progettati per i mainframe. Ad esempio, il benchmark Whetstone, che prende il nome dal compilatore Whetstone ALGOL, è stato uno dei primi test standardizzati per misurare le prestazioni aritmetiche in virgola mobile di una CPU. Questi benchmark pionieristici hanno spinto i produttori a perfezionare le loro architetture e algoritmi per ottenere punteggi di benchmark migliori.\nGli anni ’80 hanno segnato un cambiamento significativo con l’ascesa dei personal computer. Mentre aziende come IBM, Apple e Commodore gareggiavano per quote di mercato, i benchmark sono diventati strumenti essenziali per consentire una concorrenza leale. I benchmark CPU SPEC, introdotti dalla System Performance Evaluation Cooperative (SPEC), hanno stabilito test standardizzati che consentono confronti oggettivi tra diverse macchine. Questa standardizzazione ha creato un ambiente competitivo, spingendo i produttori di chip e i creatori di sistemi a migliorare continuamente le loro offerte hardware e software.\nGli anni ’90 hanno portato l’era delle applicazioni e dei videogiochi “graphics-intensive”. La necessità di benchmark per valutare le prestazioni delle schede grafiche ha portato alla creazione di 3DMark da parte di Futuremark. Mentre i giocatori e i professionisti cercavano schede grafiche ad alte prestazioni, aziende come NVIDIA e AMD sono state spinte a una rapida innovazione, portando a importanti progressi nella tecnologia GPU come gli shader programmabili.\nGli anni 2000 hanno visto un’impennata di telefoni cellulari e dispositivi portatili come i tablet. Con la portabilità è arrivata la sfida di bilanciare prestazioni e consumo energetico. Benchmark come MobileMark di BAPCo hanno valutato velocità e durata della batteria. Ciò ha spinto le aziende a sviluppare System-on-Chip (SOC) più efficienti dal punto di vista energetico, portando all’emergere di architetture come ARM che hanno dato priorità all’efficienza energetica.\nL’attenzione dell’ultimo decennio si è spostata verso il cloud computing, i big data e l’intelligenza artificiale. I provider di servizi cloud come Amazon Web Services e Google Cloud competono su prestazioni, scalabilità e convenienza. I benchmark specifici del cloud come CloudSuite sono diventati essenziali, spingendo i provider a ottimizzare la propria infrastruttura per servizi migliori.\n\n\n11.2.2 Benchmark Personalizzati\nOltre ai benchmark standard del settore, ci sono benchmark personalizzati specificamente progettati per soddisfare i requisiti unici di una particolare applicazione o attività. Sono personalizzati in base alle esigenze specifiche dell’utente o dello sviluppatore, assicurando che le metriche delle prestazioni siano direttamente pertinenti all’uso previsto del modello o del sistema di intelligenza artificiale. I benchmark personalizzati possono essere creati da singole organizzazioni, ricercatori o sviluppatori e sono spesso utilizzati insieme ai benchmark standard del settore per fornire una valutazione completa delle prestazioni dell’intelligenza artificiale.\nAd esempio, un ospedale potrebbe sviluppare un benchmark per valutare un modello di intelligenza artificiale per prevedere la riammissione dei pazienti. Questo benchmark incorporerebbe metriche pertinenti alla popolazione di pazienti dell’ospedale, come dati demografici, anamnesi e fattori sociali. Allo stesso modo, il benchmark di rilevamento delle frodi di un istituto finanziario potrebbe concentrarsi sull’identificazione accurata delle transazioni fraudolente riducendo al minimo i falsi positivi. Nel settore automobilistico, un benchmark di veicoli autonomi potrebbe dare priorità alle prestazioni in diverse condizioni, alla risposta agli ostacoli e alla sicurezza. I rivenditori potrebbero confrontare i sistemi di raccomandazione utilizzando il tasso di clic, il tasso di conversione e la soddisfazione del cliente. Le aziende manifatturiere potrebbero confrontare i sistemi di controllo qualità in base all’identificazione dei difetti, all’efficienza e alla riduzione degli sprechi. In ogni settore, i benchmark personalizzati forniscono alle organizzazioni criteri di valutazione su misura per le loro esigenze e il loro contesto unici. Ciò consente una valutazione più significativa di quanto i sistemi di intelligenza artificiale soddisfino i requisiti.\nIl vantaggio dei benchmark personalizzati risiede nella loro flessibilità e pertinenza. Possono essere progettati per testare aspetti specifici delle prestazioni critici per il successo della soluzione di intelligenza artificiale nella sua applicazione prevista. Ciò consente una valutazione più mirata e accurata delle capacità del modello o del sistema di intelligenza artificiale. I benchmark personalizzati forniscono anche informazioni preziose sulle prestazioni delle soluzioni di intelligenza artificiale in scenari reali, il che può essere cruciale per identificare potenziali problemi e aree di miglioramento.\nNell’intelligenza artificiale, i benchmark svolgono un ruolo cruciale nel guidare il progresso e l’innovazione. Sebbene i benchmark siano stati a lungo utilizzati nell’informatica, la loro applicazione all’apprendimento automatico è relativamente recente. I benchmark incentrati sull’intelligenza artificiale forniscono metriche standardizzate per valutare e confrontare le prestazioni di diversi algoritmi, architetture di modelli e piattaforme hardware.\n\n\n11.2.3 Consenso della Comunità\nUna prerogativa fondamentale affinché un benchmark abbia un impatto è che deve riflettere le priorità e i valori condivisi della più ampia comunità di ricerca. I benchmark progettati in modo isolato rischiano di non ottenere accettazione se trascurano metriche chiave considerate importanti dai gruppi leader. Attraverso uno sviluppo collaborativo con la partecipazione aperta di laboratori accademici, aziende e altri stakeholder, i benchmark possono incorporare un contributo collettivo su capacità critiche che vale la pena misurare. Ciò aiuta a garantire che i benchmark valutino aspetti che la comunità concorda siano essenziali per far progredire il campo. Il processo di raggiungimento dell’allineamento su attività e metriche supporta di per sé la convergenza su ciò che conta di più.\nInoltre, i benchmark pubblicati con ampia co-paternità da istituzioni rispettate hanno autorità e validità che convincono la comunità ad adottarli come standard affidabili. I benchmark percepiti come distorti da particolari interessi aziendali o istituzionali generano scetticismo. Anche il coinvolgimento continuo della comunità attraverso workshop e sfide è fondamentale dopo la versione iniziale, ed è ciò che, ad esempio, ha portato al successo di ImageNet. Col progredire della ricerca, la partecipazione collettiva consente un continuo perfezionamento ed espansione dei benchmark nel tempo.\nInfine, i benchmark sviluppati dalla comunità rilasciati con accesso aperto accelerano l’adozione e l’implementazione coerente. Abbiamo condiviso codice open source, documentazione, modelli e infrastrutture per ridurre le barriere che impediscono ai gruppi di confrontare le soluzioni su un piano di parità utilizzando implementazioni standardizzate. Questa coerenza è fondamentale per confronti equi. Senza coordinamento, laboratori e aziende potrebbero implementare i benchmark in modo diverso, riducendo la riproducibilità dei risultati.\nIl consenso della comunità conferisce ai benchmark una rilevanza duratura, mentre la frammentazione confonde. Attraverso lo sviluppo collaborativo e un funzionamento trasparente, i benchmark possono diventare standard autorevoli per monitorare i progressi. Molti dei benchmark di cui parliamo in questo capitolo sono stati sviluppati e creati dalla comunità, per la comunità, ed è questo che alla fine ha portato al loro successo.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmark-ai-sistema-modello-e-dati",
    "href": "contents/benchmarking/benchmarking.it.html#benchmark-ai-sistema-modello-e-dati",
    "title": "11  Benchmarking AI",
    "section": "11.3 Benchmark AI: Sistema, Modello e Dati",
    "text": "11.3 Benchmark AI: Sistema, Modello e Dati\nLa necessità di un benchmarking completo diventa fondamentale man mano che i sistemi AI diventano più complessi e onnipresenti. In questo contesto, i benchmark sono spesso classificati in tre categorie principali: Hardware, Modello e Dati. Analizziamo perché ognuno di questi gruppi è essenziale e il significato della valutazione dell’AI da queste tre dimensioni distinte:\n\n11.3.1 Benchmark di Sistema\nI calcoli AI, in particolare quelli nel deep learning, richiedono molte risorse. L’hardware su cui vengono eseguiti questi calcoli svolge un ruolo importante nel determinare la velocità, l’efficienza e la scalabilità delle soluzioni AI. Di conseguenza, i benchmark hardware aiutano a valutare le prestazioni di CPU, GPU, TPU e altri acceleratori nelle attività AI. Comprendendone le prestazioni, gli sviluppatori possono scegliere quali piattaforme hardware si adattano meglio a specifiche applicazioni AI. Inoltre, i produttori di hardware utilizzano questi benchmark per identificare aree di miglioramento, guidando l’innovazione nei progetti di chip specifici per AI.\n\n\n11.3.2 Benchmark del Modello\nL’architettura, le dimensioni e la complessità dei modelli AI variano notevolmente. Modelli diversi hanno diverse esigenze di calcolo e offrono diversi livelli di accuratezza ed efficienza. I benchmark dei modelli aiutano a valutare le prestazioni di varie architetture AI su attività standardizzate. Forniscono informazioni sulla velocità, l’accuratezza e le richieste di risorse di diversi modelli. Eseguendo il benchmarking dei modelli, i ricercatori possono identificare le architetture più performanti per attività specifiche, guidando la comunità AI verso soluzioni più efficienti ed efficaci. Inoltre, questi benchmark aiutano a monitorare i progressi della ricerca sull’intelligenza artificiale, mostrando i progressi nella progettazione e nell’ottimizzazione dei modelli.\n\n\n11.3.3 Benchmark dei Dati\nL’intelligenza artificiale, in particolare l’apprendimento automatico, è intrinsecamente basata sui dati. La qualità, le dimensioni e la diversità dei dati influenzano l’efficacia dell’addestramento e la capacità di generalizzazione dei modelli di intelligenza artificiale. I benchmark dei dati si concentrano sui set di dati utilizzati nell’addestramento e nella valutazione dell’intelligenza artificiale. Forniscono set di dati standardizzati che la comunità può utilizzare per addestrare e testare i modelli, garantendo parità di condizioni per i confronti. Inoltre, questi benchmark evidenziano le sfide relative alla qualità, alla diversità e alla rappresentazione dei dati, spingendo la comunità ad affrontare bias e lacune nei dati di addestramento dell’intelligenza artificiale. Comprendendo i benchmark dei dati, i ricercatori possono anche valutare come i modelli potrebbero comportarsi in scenari reali, garantendo robustezza e affidabilità.\nNelle restanti sezioni, discuteremo ciascuno di questi tipi di benchmark. L’attenzione sarà rivolta a un’esplorazione approfondita dei benchmark di sistema, poiché sono fondamentali per comprendere e migliorare le prestazioni del sistema di apprendimento automatico. Parleremo brevemente dei benchmark dei modelli e dei dati per una prospettiva completa, ma l’enfasi e la maggior parte del contenuto saranno dedicati ai benchmark di sistema.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmarking-di-sistema",
    "href": "contents/benchmarking/benchmarking.it.html#benchmarking-di-sistema",
    "title": "11  Benchmarking AI",
    "section": "11.4 Benchmarking di Sistema",
    "text": "11.4 Benchmarking di Sistema\n\n11.4.1 Granularità\nIl benchmarking del sistema di apprendimento automatico fornisce un approccio strutturato e sistematico per valutare le prestazioni di un sistema in diverse dimensioni. Data la complessità dei sistemi ML, possiamo analizzare le loro prestazioni attraverso diversi livelli di granularità e ottenere una visione completa dell’efficienza del sistema, identificare potenziali colli di bottiglia e individuare le aree di miglioramento. A tal fine, nel corso degli anni si sono evoluti vari tipi di benchmark che continuano a persistere.\nFigura 11.1 illustra i diversi livelli di granularità di un sistema ML. A livello di applicazione, i benchmark end-to-end valutano le prestazioni complessive del sistema, considerando fattori come la pre-elaborazione dei dati, l’addestramento del modello e l’inferenza. Mentre a livello di modello, i benchmark si concentrano sulla valutazione dell’efficienza e dell’accuratezza di modelli specifici. Ciò include la valutazione di quanto bene i modelli si generalizzano a nuovi dati e della loro efficienza computazionale durante l’addestramento e l’inferenza. Inoltre, il benchmarking può estendersi all’infrastruttura hardware e software, esaminando le prestazioni di singoli componenti come GPU o TPU.\n\n\n\n\n\n\nFigura 11.1: Granularità del sistema ML.\n\n\n\n\nMicro Benchmark\nI micro-benchmark nell’IA sono specializzati e valutano componenti distinti o operazioni specifiche all’interno di un processo di apprendimento automatico più ampio. Questi benchmark si concentrano su singole attività, offrendo approfondimenti sulle richieste computazionali di un particolare layer di rete neurale, l’efficienza di un’unica tecnica di ottimizzazione o la produttività di una specifica funzione di attivazione. Ad esempio, i professionisti potrebbero utilizzare i micro-benchmark per misurare il tempo di calcolo richiesto da un layer convoluzionale in un modello di deep learning o per valutare la velocità di preelaborazione che alimenta i dati nel modello. Tali valutazioni granulari sono fondamentali per la messa a punto e l’ottimizzazione di aspetti discreti dei modelli di IA, assicurando che ogni componente funzioni al massimo del suo potenziale.\nQuesti tipi di microbenchmark includono lo zoom su operazioni o componenti molto specifiche della pipeline AI, come le seguenti:\n\nOperazioni Tensoriali: Librerie come cuDNN (di NVIDIA) spesso hanno benchmark per misurare le prestazioni di singole operazioni tensoriali, come convoluzioni o moltiplicazioni di matrici, che sono fondamentali per i calcoli del deep learning.\nFunzioni di Attivazione: Benchmark che misurano la velocità e l’efficienza di varie funzioni di attivazione come ReLU, Sigmoid o Tanh in isolamento.\nBenchmark di Layer: Valutazioni dell’efficienza computazionale di distinti layer di rete neurale, come blocchi LSTM o Transformer, quando si opera su dimensioni di input standardizzate.\n\nEsempio: DeepBench, introdotto da Baidu, è un buon esempio di qualcosa che valuta quanto sopra. DeepBench valuta le prestazioni delle operazioni di base nei modelli di deep learning, fornendo informazioni su come diverse piattaforme hardware gestiscono l’addestramento e l’inferenza delle reti neurali.\n\n\n\n\n\n\nEsercizio 11.1: Benchmarking di Sistema - Operazioni Tensoriali\n\n\n\n\n\nCi si è mai chiesto come mai i filtri immagine diventano così veloci? Librerie speciali come cuDNN potenziano quei calcoli su determinati hardware. In questo Colab, useremo cuDNN con PyTorch per velocizzare il filtraggio delle immagini. Lo si consideri un piccolo benchmark, che mostra come il software giusto può sbloccare la potenza della GPU!\n\n\n\n\n\n\nMacro Benchmark\nI macro benchmark forniscono una visione olistica, valutando le prestazioni end-to-end di interi modelli di apprendimento automatico o sistemi di intelligenza artificiale completi. Invece di concentrarsi sulle singole operazioni, i macro benchmark valutano l’efficacia collettiva dei modelli in scenari o attività del mondo reale. Ad esempio, un macro benchmark potrebbe valutare le prestazioni complete di un modello di apprendimento profondo che esegue la classificazione delle immagini su un set di dati come ImageNet. Ciò include la misura dell’accuratezza, della velocità di calcolo e del consumo di risorse. Allo stesso modo, si potrebbero misurare il tempo e le risorse cumulativi necessari per addestrare un modello di elaborazione del linguaggio naturale su corpora di testo estesi o valutare le prestazioni di un intero sistema di raccomandazione, dall’inserimento dei dati agli output finali specifici dell’utente.\nEsempi: Questi benchmark valutano il modello di intelligenza artificiale:\n\nMLPerf Inference (Reddi et al. 2020): Un set di benchmark standard per misurare le prestazioni di software e hardware di apprendimento automatico. MLPerf ha una suite di benchmark dedicati per scale specifiche, come MLPerf Mobile per dispositivi di classe mobile e MLPerf Tiny, che si concentra su microcontrollori e altri dispositivi con risorse limitate.\nMLMark di EEMBC: Una suite di benchmarking per valutare le prestazioni e l’efficienza energetica dei dispositivi embedded che eseguono carichi di lavoro di apprendimento automatico. Questo benchmark fornisce informazioni su come diverse piattaforme hardware gestiscono attività come il riconoscimento delle immagini o l’elaborazione audio.\nAI-Benchmark (Ignatov et al. 2019): Uno strumento di benchmarking progettato per dispositivi Android, valuta le prestazioni delle attività di intelligenza artificiale sui dispositivi mobili, comprendendo vari scenari del mondo reale come il riconoscimento delle immagini, l’analisi dei volti e il riconoscimento ottico dei caratteri.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. «MLPerf Inference Benchmark». In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nIgnatov, Andrey, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, e Luc Van Gool. 2019. «AI Benchmark: All About Deep Learning on Smartphones in 2019». In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 0–0. IEEE. https://doi.org/10.1109/iccvw.2019.00447.\n\n\nBenchmark end-to-end\nI benchmark end-to-end forniscono una valutazione completa che si estende oltre i confini del modello di IA stesso. Invece di concentrarsi esclusivamente sull’efficienza o l’accuratezza computazionale di un modello di apprendimento automatico, questi benchmark comprendono l’intera pipeline di un sistema di IA. Ciò include la pre-elaborazione iniziale dei dati, le prestazioni del modello principale, la post-elaborazione degli output del modello e altri componenti integrali come l’archiviazione e le interazioni di rete.\nLa pre-elaborazione dei dati è la prima fase in molti sistemi di IA, trasformando i dati grezzi in un formato adatto per l’addestramento o l’inferenza del modello. L’efficienza, la scalabilità e l’accuratezza di queste fasi di pre-elaborazione sono vitali per le prestazioni complessive del sistema. I benchmark end-to-end valutano questa fase, assicurando che la pulizia dei dati, la normalizzazione, l’aumento o qualsiasi altro processo di trasformazione non diventi un collo di bottiglia.\nAnche la fase di post-elaborazione è al centro dell’attenzione. Ciò comporta l’interpretazione degli output grezzi del modello, eventualmente la conversione dei punteggi in categorie significative, il filtraggio dei risultati o persino l’integrazione con altri sistemi. Nelle applicazioni del mondo reale, questa fase è fondamentale per fornire informazioni fruibili e i benchmark end-to-end ne garantiscono l’efficienza e l’efficacia.\nOltre alle operazioni di base dell’IA, altri componenti del sistema sono importanti per le prestazioni complessive e l’esperienza utente. Le soluzioni di archiviazione, basate su cloud, on-premise o ibride, possono avere un impatto significativo sui tempi di recupero e archiviazione dei dati, in particolare con vasti set di dati di IA. Allo stesso modo, le interazioni di rete, vitali per le soluzioni di IA basate su cloud o per i sistemi distribuiti, possono diventare colli di bottiglia delle prestazioni se non ottimizzate. I benchmark end-to-end valutano in modo olistico questi componenti, assicurando che l’intero sistema funzioni senza problemi, dal recupero dei dati alla consegna dell’output finale.\nAd oggi, non esistono benchmark end-to-end pubblici che tengano conto del ruolo dell’archiviazione dei dati, della rete e delle prestazioni di elaborazione. Si può sostenere che MLPerf Training and Inference si avvicini all’idea di un benchmark end-to-end, ma si concentrano esclusivamente sulle prestazioni del modello ML e non rappresentano scenari di distribuzione nel mondo reale di come i modelli vengono utilizzati sul campo. Tuttavia, forniscono un segnale molto utile che aiuta a valutare le prestazioni del sistema AI.\nData la specificità intrinseca del benchmarking end-to-end, viene in genere eseguito internamente in un’azienda “strumentando” [inserendo punti di controllo] distribuzioni di produzione reali di AI. Ciò consente agli ingegneri di avere una comprensione e una ripartizione realistiche delle prestazioni, ma data la sensibilità e la specificità delle informazioni, raramente vengono segnalate all’esterno dell’azienda.\n\n\nComprendere i Compromessi\nDiversi problemi sorgono nelle diverse fasi di un sistema di intelligenza artificiale. I micro-benchmark aiutano a mettere a punto i singoli componenti, i macro-benchmark aiutano a perfezionare le architetture o gli algoritmi del modello e i benchmark end-to-end guidano l’ottimizzazione dell’intero flusso di lavoro. Comprendendo dove si trova un problema, gli sviluppatori possono applicare ottimizzazioni mirate.\nInoltre, mentre i singoli componenti di un sistema di intelligenza artificiale potrebbero funzionare in modo ottimale in isolamento, possono emergere colli di bottiglia quando interagiscono. I benchmark end-to-end, in particolare, sono fondamentali per garantire che l’intero sistema, quando funziona collettivamente, soddisfi gli standard di prestazioni ed efficienza desiderati.\nInfine, le organizzazioni possono prendere decisioni informate su dove allocare le risorse individuando colli di bottiglia o inefficienze nelle prestazioni. Ad esempio, se i micro-benchmark rivelano inefficienze in specifiche operazioni tensoriali, gli investimenti possono essere indirizzati verso acceleratori hardware specializzati. Al contrario, se i benchmark end-to-end indicano problemi di recupero dei dati, gli investimenti potrebbero essere incanalati verso soluzioni di archiviazione migliori.\n\n\n\n11.4.2 Componenti dei Benchmark\nIn sostanza, un benchmark AI è più di un semplice test o punteggio; è un framework di valutazione completo. Per comprenderlo in modo approfondito, analizziamo i componenti tipici che compongono un benchmark AI.\n\nDataset Standardizzati\nI set di dati fungono da base per la maggior parte dei benchmark AI. Forniscono un set di dati coerente su cui i modelli vengono addestrati e valutati, garantendo parità di condizioni per i confronti.\nEsempio: ImageNet, un set di dati su larga scala contenente milioni di immagini etichettate che abbracciano migliaia di categorie, è uno standard di benchmarking popolare per le attività di classificazione delle immagini.\n\n\nAttività Predefinite\nUn benchmark dovrebbe avere un obiettivo o un compito chiaro che i modelli mirano a raggiungere. Questo compito definisce il problema che il sistema AI sta cercando di risolvere.\nEsempio: I compiti per i benchmark di elaborazione del linguaggio naturale potrebbero includere analisi del “sentiment”, riconoscimento di entità denominate o traduzione automatica.\n\n\nMetriche di Valutazione\nUna volta definito un task, i benchmark richiedono parametri per quantificare le prestazioni. Questi parametri offrono misure oggettive per confrontare diversi modelli o sistemi. Nei task di classificazione, parametri come accuratezza, precisione, richiamo e punteggio F1 sono comunemente utilizzati. Errori quadratici medi o assoluti potrebbero essere utilizzati per i task di regressione.\n\n\nBaseline e Modelli Baseline\nI benchmark spesso includono modelli “baseline” o implementazioni di riferimento. Di solito servono come punti di partenza o standard minimi di prestazione per confrontare nuovi modelli o nuove tecniche. I modelli “baseline” aiutano i ricercatori a misurare l’efficacia di nuovi algoritmi.\nNelle suite di benchmark, modelli semplici come la regressione lineare o le reti neurali di base sono spesso le baseline comuni. Queste forniscono un contesto quando si valutano modelli più complessi. Confrontando questi modelli più semplici, i ricercatori possono quantificare i miglioramenti derivanti da approcci avanzati.\nLe metriche delle prestazioni variano in base all’attività, ma ecco alcuni esempi:\n\nLe attività di classificazione utilizzano metriche come accuratezza, precisione, richiamo e punteggio F1.\nLe attività di regressione utilizzano spesso l’errore quadratico medio o l’errore assoluto medio.\n\n\n\nSpecifiche Hardware e Software\nData la variabilità introdotta da diverse configurazioni hardware e software, i benchmark spesso specificano o documentano gli ambienti hardware e software in cui vengono condotti i test.\nEsempio: Un benchmark AI potrebbe indicare che le valutazioni sono state condotte su una GPU NVIDIA Tesla V100 utilizzando TensorFlow v2.4.\n\n\nCondizioni Ambientali\nPoiché fattori esterni possono influenzare i risultati del benchmark, è essenziale controllare o documentare condizioni come temperatura, fonte di alimentazione o processi di background del sistema.\nEsempio: I benchmark AI mobili potrebbero specificare che i test sono stati condotti a temperatura ambiente con dispositivi collegati a una fonte di alimentazione per eliminare le variazioni del livello della batteria.\n\n\nRegole di Riproducibilità\nPer garantire che i benchmark siano credibili e possano essere replicati da altri nella comunità, spesso includono protocolli dettagliati che coprono tutto, dai “random seed” utilizzati agli iperparametri esatti.\nEsempio: Un benchmark per un’attività di learning di rinforzo potrebbe specificare gli episodi esatti dell’addestramento, i rapporti di esplorazione-sfruttamento e le strutture di ricompensa utilizzate.\n\n\nLinee Guida per l’Interpretazione dei Risultati\nOltre ai punteggi o alle metriche pure, i benchmark spesso forniscono linee guida o contesto per interpretare i risultati, aiutando i professionisti a comprendere le implicazioni più ampie.\nEsempio: Un benchmark potrebbe evidenziare che, sebbene il Modello A abbia ottenuto un punteggio più alto del Modello B in termini di accuratezza, offre migliori prestazioni in tempo reale, rendendolo più adatto per applicazioni sensibili al fattore tempo.\n\n\n\n11.4.3 Training vs. inferenza\nIl ciclo di vita dello sviluppo di un modello di apprendimento automatico prevede due fasi critiche: addestramento e inferenza. Training, come forse ricorderete, è il processo di apprendimento di modelli dai dati per creare il modello. L’inferenza si riferisce al modello che fa previsioni su nuovi dati non etichettati. Entrambe le fasi svolgono ruoli indispensabili ma distinti. Di conseguenza, ogni fase garantisce un rigoroso benchmarking per valutare metriche delle prestazioni come velocità, accuratezza ed efficienza computazionale.\nIl benchmarking della fase di training fornisce informazioni su come diverse architetture di modelli, valori di iperparametri e algoritmi di ottimizzazione influiscono sul tempo e sulle risorse necessarie per il training del modello. Ad esempio, il benchmarking mostra come la profondità della rete neurale influisce sul tempo di training su un dato set di dati. Il benchmarking rivela anche come acceleratori hardware come GPU e TPU possono accelerare il training.\nD’altro canto, il benchmark dell’inferenza valuta le prestazioni del modello in condizioni reali dopo la distribuzione. Le metriche chiave includono latenza, throughput, footprint di memoria e consumo energetico. Questo tipo di benchmarking determina se un modello soddisfa i requisiti della sua applicazione target in termini di tempo di risposta e vincoli del dispositivo. Tuttavia, ne discuteremo ampiamente per garantire una comprensione generale.\n\n\n11.4.4 I Benchmark del Training\nIl training [addestramento] rappresenta la fase in cui il sistema elabora e assimila dati grezzi per adattare e perfezionare i propri parametri. Pertanto, è un’attività algoritmica e comporta considerazioni a livello di sistema, tra cui pipeline di dati, archiviazione, risorse di elaborazione e meccanismi di orchestrazione. L’obiettivo è garantire che il sistema ML possa apprendere in modo efficiente dai dati, ottimizzando sia le prestazioni del modello sia l’utilizzo delle risorse del sistema.\n\nScopo\nDal punto di vista dei sistemi ML, i benchmark del training valutano quanto bene il sistema si adatta all’aumento dei volumi di dati e delle richieste di elaborazione. Si tratta di comprendere l’interazione tra hardware, software e pipeline di dati nel processo di training.\nSi consideri un sistema ML distribuito progettato per il training su vasti set di dati, come quelli utilizzati nelle raccomandazioni di prodotti di e-commerce su larga scala. Un benchmark di training valuterebbe l’efficienza con cui il sistema si adatta a più nodi, gestisce lo sharding [partizionamento] dei dati e gestisce guasti o abbandoni dei nodi durante il training.\nI benchmark di training valutano l’utilizzo di CPU, GPU, memoria e rete durante la fase di training, guidando le ottimizzazioni del sistema. Quando si addestra un modello in un sistema ML basato su cloud, è fondamentale capire come vengono utilizzate le risorse. Le GPU vengono sfruttate appieno? C’è un sovraccarico di memoria non necessario? I benchmark possono evidenziare colli di bottiglia o inefficienze nell’utilizzo delle risorse, con conseguenti risparmi sui costi e miglioramenti delle prestazioni.\nIl training di un modello ML è subordinato alla consegna tempestiva ed efficiente dei dati. I benchmark in questo contesto valuterebbero anche l’efficienza delle pipeline di dati, la velocità di preelaborazione dei dati e i tempi di recupero dell’archiviazione. Per i sistemi di analisi in tempo reale, come quelli utilizzati nel rilevamento delle frodi, la velocità con cui i dati di training vengono ingeriti, preelaborati e immessi nel modello può essere critica. I benchmark valuterebbero la latenza delle pipeline di dati, l’efficienza dei sistemi di archiviazione (come SSD rispetto a HDD) e la velocità delle attività di aumento o trasformazione dei dati.\n\n\nMetriche\nSe viste da una prospettiva di sistema, le metriche di training offrono informazioni che trascendono gli indicatori di prestazioni algoritmiche convenzionali. Queste metriche misurano l’efficacia di apprendimento del modello e misurano l’efficienza, la scalabilità e la robustezza dell’intero sistema ML durante la fase di training. Analizziamo più a fondo queste metriche e il loro significato.\nLe seguenti metriche sono spesso considerate importanti:\n\nTempo di training: Il tempo necessario per addestrare un modello da zero fino a raggiungere un livello di prestazioni soddisfacente. Misura direttamente le risorse di elaborazione necessarie per addestrare un modello. Ad esempio, il BERT di Google (Devlin et al. 2019) è un modello di elaborazione del linguaggio naturale che richiede diversi giorni per l’addestramento su un corpus enorme di dati di testo utilizzando più GPU. Il lungo tempo di training è una sfida significativa in termini di consumo di risorse e costi. In alcuni casi, i benchmark possono invece misurare la produttività del training (campioni di training per unità di tempo). La produttività può essere calcolata molto più velocemente e facilmente del tempo di addestramento, ma potrebbe oscurare le metriche che ci interessano davvero (ad esempio, il tempo di addestramento).\nScalabilità: Quanto bene il processo di addestramento può gestire gli aumenti delle dimensioni dei dati o della complessità del modello. La scalabilità può essere valutata misurando il tempo di addestramento, l’utilizzo della memoria e altri consumi di risorse all’aumentare delle dimensioni dei dati o della complessità del modello. Il modello GPT-3 di OpenAI (Brown et al. 2020) ha 175 miliardi di parametri, il che lo rende uno dei più grandi modelli linguistici esistenti. L’addestramento di GPT-3 ha richiesto notevoli sforzi ingegneristici per scalare il processo di addestramento in modo da gestire le enormi dimensioni del modello. Ciò ha comportato l’utilizzo di hardware specializzato, addestramento distribuito e altre tecniche per garantire che il modello potesse essere addestrato in modo efficiente.\nUtilizzo delle Risorse: La misura in cui il processo di addestramento utilizza le risorse di calcolo disponibili come CPU, GPU, memoria e I/O del disco. Un elevato utilizzo delle risorse può indicare un processo di training efficiente, mentre un basso utilizzo può suggerire colli di bottiglia o inefficienze. Ad esempio, il training di una rete neurale convoluzionale (CNN) per la classificazione delle immagini richiede notevoli risorse GPU. L’utilizzo di configurazioni multi-GPU e l’ottimizzazione del codice di training per l’accelerazione GPU possono migliorare notevolmente l’utilizzo delle risorse e l’efficienza del training.\nConsumo di Memoria: La quantità di memoria utilizzata dal processo di training. Il consumo di memoria può essere un fattore limitante per il training di modelli o set di dati di grandi dimensioni. Ad esempio, i ricercatori di Google hanno dovuto affrontare notevoli sfide di consumo di memoria durante il training di BERT. Il modello ha centinaia di milioni di parametri, che richiedono grandi quantità di memoria. I ricercatori hanno dovuto sviluppare tecniche per ridurre il consumo di memoria, come il checkpointing del gradiente e il parallelismo del modello.\nConsumo Energetico: L’energia consumata durante il training. Man mano che i modelli di apprendimento automatico diventano più complessi, il consumo energetico è diventato un fattore importante da considerare. Il training di grandi modelli di apprendimento automatico può consumare molta energia, e quindi molto carbonio. Ad esempio, si è stimato che l’addestramento di GPT-3 di OpenAI abbia un’impronta di carbonio equivalente a un viaggio in auto di 700.000 chilometri.\nThroughput: l numero di campioni di addestramento elaborati per unità di tempo. Un throughput [produttività] più elevato indica generalmente un processo di addestramento più efficiente. La produttività è una metrica importante da considerare quando si addestra un sistema di raccomandazione per una piattaforma di e-commerce. Una produttività elevata assicura che il modello possa elaborare rapidamente grandi volumi di dati di interazione dell’utente, il che è fondamentale per mantenere la pertinenza e l’accuratezza delle raccomandazioni. Ma è anche importante capire come bilanciare la produttività con i limiti di latenza. Pertanto, un vincolo di produttività limitato dalla latenza viene spesso imposto agli accordi sul livello di servizio per le distribuzioni di applicazioni del data center.\nCosto: Il costo della training di un modello può includere sia risorse computazionali che umane. Il costo è importante quando si considera la praticità e la fattibilità del training di modelli grandi o complessi. Si stima che l’addestramento di modelli di linguaggio grandi come GPT-3 costi milioni di dollari. Questo costo include risorse computazionali, elettriche e umane necessarie per lo sviluppo e l’addestramento del modello.\nTolleranza agli Errori e Robustezza: La capacità del processo di training di gestire guasti o errori senza bloccarsi o produrre risultati errati. Questo è importante per garantire l’affidabilità del processo di addestramento. Errori di rete o malfunzionamenti hardware possono verificarsi in uno scenario reale in cui un modello di apprendimento automatico viene addestrato su un sistema distribuito. Negli ultimi anni, è diventato abbondantemente chiaro che gli errori derivanti dalla corruzione “silenziosa” dei dati sono emersi come un problema importante. Un processo di addestramento affidabile e tollerante agli errori può recuperare da tali errori senza compromettere l’integrità del modello.\nFacilità d’Uso e Flessibilità: La facilità con cui il processo di addestramento può essere impostato e utilizzato e la sua flessibilità nella gestione di diversi tipi di dati e modelli. In aziende come Google, l’efficienza può talvolta essere misurata dal numero di anni di “Software Engineer (SWE)” risparmiati poiché ciò si traduce direttamente in impatto. La facilità d’uso e la flessibilità possono ridurre il tempo e lo sforzo necessari per addestrare un modello. TensorFlow e PyTorch sono popolari framework di apprendimento automatico che forniscono interfacce intuitive e API flessibili per la creazione e l’addestramento di modelli di machine-learning. Questi framework supportano molte architetture di modelli e sono dotati di strumenti che semplificano il processo di addestramento.\nRiproducibilità: La capacità di riprodurre i risultati del processo di training. La riproducibilità è importante per verificare la correttezza e la validità di un modello. Tuttavia, le variazioni dovute alle caratteristiche stocastiche della rete spesso rendono difficile riprodurre il comportamento preciso delle applicazioni in fase di addestramento, il che può rappresentare una sfida per il benchmarking.\n\nEseguendo il benchmarking per questi tipi di metriche, possiamo ottenere una visione completa delle prestazioni e dell’efficienza del processo di training da una prospettiva di sistema. Ciò può aiutare a identificare le aree di miglioramento e garantire che le risorse siano utilizzate in modo efficace.\n\n\nI Task\nSelezionare una manciata di task [attività] rappresentative per il benchmarking dei sistemi di machine learning è una sfida, perché l’apprendimento automatico viene applicato a vari domini con caratteristiche e requisiti unici. Ecco alcune delle sfide affrontate nella selezione di attività rappresentative:\n\nDiversità di Applicazioni: L’apprendimento automatico viene utilizzato in numerosi campi, come sanità, finanza, elaborazione del linguaggio naturale, visione artificiale e molti altri. Ogni campo ha attività specifiche che potrebbero non essere rappresentative di altri campi. Ad esempio, le attività di classificazione delle immagini nella visione artificiale potrebbero non essere importanti per il rilevamento delle frodi finanziarie.\nVariabilità nei Tipi di Dati e nella Qualità: Diverse attività richiedono tipi di dati diversi, come testo, immagini, video o dati numerici. La qualità e la disponibilità dei dati possono variare notevolmente tra le attività, rendendo difficile selezionare attività rappresentative delle sfide generali affrontate nell’apprendimento automatico.\nComplessità e Difficoltà delle Attività: La complessità delle attività varia notevolmente. Alcune sono relativamente semplici, mentre altre sono molto complesse e richiedono modelli e tecniche sofisticate. Selezionare attività rappresentative che coprano le complessità riscontrate nell’apprendimento automatico è difficile.\nProblemi Etici e di Privacy: Alcune attività possono riguardare dati sensibili o privati, come cartelle cliniche o informazioni personali. Queste attività possono presentare problemi etici e di privacy che devono essere affrontati, rendendole meno adatte come attività rappresentative per il benchmarking.\nRequisiti di Scalabilità e Risorse: Attività diverse possono avere requisiti di scalabilità e risorse diverse. Alcune attività possono richiedere ampie risorse di calcolo, mentre altre possono essere eseguite con risorse minime. Selezionare attività che rappresentino i requisiti di risorse generali nell’apprendimento automatico è difficile.\nMetriche di Valutazione: Le metriche utilizzate per valutare le prestazioni dei modelli di apprendimento automatico variano tra le attività. Alcune attività possono avere metriche di valutazione consolidate, mentre altre non hanno metriche chiare o standardizzate. Ciò può rendere difficile confrontare le prestazioni tra diverse attività.\nGeneralizzabilità dei Risultati: I risultati ottenuti dal benchmarking su un’attività specifica potrebbero non essere generalizzabili ad altre attività. Ciò significa che le prestazioni di un sistema di apprendimento automatico su un’attività selezionata potrebbero non essere indicative delle sue prestazioni su altre attività.\n\nÈ importante considerare attentamente questi fattori quando si progettano benchmark per garantire che siano significativi e pertinenti per la vasta gamma di attività incontrate nel machine learning.\n\n\nI Benchmark\nEcco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per l’addestramento di sistemi di apprendimento automatico.\nMLPerf Training Benchmark\nMLPerf è una suite di benchmark progettata per misurare le prestazioni di hardware, software e servizi di apprendimento automatico. Il benchmark di MLPerf Training (Mattson et al. 2020a) si concentra sul tempo necessario per addestrare i modelli a una metrica di qualità target. Include carichi di lavoro diversi, come classificazione delle immagini, rilevamento di oggetti, traduzione e apprendimento per rinforzo.\nMetriche:\n\nTempo di training per la qualità target\nThroughput (esempi al secondo)\nUtilizzo delle risorse (CPU, GPU, memoria, I/O del disco)\n\nDAWNBench\nDAWNBench (Coleman et al. 2019) è una suite di benchmark incentrata sul tempo di training end-to-end del deep learning e sulle prestazioni dell’inferenza. Include attività comuni come la classificazione delle immagini e la risposta alle domande.\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, e Matei Zaharia. 2019. «Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark». ACM SIGOPS Operating Systems Review 53 (1): 14–25. https://doi.org/10.1145/3352020.3352024.\nMetriche:\n\nTempo di training per la precisione target\nLatenza dell’inferenza\nCosto (in termini di risorse di cloud computing e storage)\n\nFathom\nFathom (Adolf et al. 2016) è un benchmark dell’Università di Harvard che valuta le prestazioni dei modelli di deep learning utilizzando un set diversificato di carichi di lavoro. Questi includono attività comuni come la classificazione delle immagini, il riconoscimento vocale e la modellazione del linguaggio.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. «Fathom: Reference workloads for modern deep learning methods». In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\nMetriche:\n\nOperazioni al secondo (per misurare l’efficienza computazionale)\nTempo di completamento per ogni carico di lavoro\nLarghezza di banda della memoria\n\n\n\nCaso d’Uso di Esempio\nConsideriamo uno scenario in cui vogliamo eseguire il benchmark dell’addestramento di un modello di classificazione delle immagini su una piattaforma hardware specifica.\n\nTask: L’attività consiste nell’addestrare una rete neurale convoluzionale (CNN) per la classificazione delle immagini sul set di dati CIFAR-10.\nBenchmark: Possiamo usare il benchmark di addestramento MLPerf per questa attività. Include un carico di lavoro di classificazione delle immagini pertinente alla nostra attività.\nMetriche: Misureremo le seguenti metriche:\n\n\nTempo di addestramento per raggiungere una precisione target del 90%.\nThroughput in termini di immagini elaborate al secondo.\nUtilizzo di GPU e CPU durante l’addestramento.\n\nMisurando queste metriche, possiamo valutare le prestazioni e l’efficienza del processo di addestramento sulla piattaforma hardware selezionata. Queste informazioni possono quindi essere utilizzate per identificare potenziali colli di bottiglia o aree di miglioramento.\n\n\n\n11.4.5 Benchmark di Inferenza\nL’inferenza nell’apprendimento automatico si riferisce all’uso di un modello addestrato per fare previsioni su dati nuovi e mai visti prima. È la fase in cui il modello applica le conoscenze apprese per risolvere il problema per cui è stato progettato, come la classificazione di immagini, il riconoscimento vocale o la traduzione di testo.\n\nScopo\nQuando creiamo modelli di machine learning, il nostro obiettivo finale è di distribuirli in applicazioni del mondo reale in cui possano fornire previsioni accurate e affidabili su dati nuovi e mai visti. Questo processo di utilizzo di un modello addestrato per fare previsioni è noto come inferenza. Le prestazioni reali di un modello di apprendimento automatico possono differire in modo significativo dalle sue prestazioni su set di dati di addestramento o validazione, il che rende l’inferenza di benchmarking un passaggio cruciale nello sviluppo e nell’implementazione di modelli di machine learning.\nIl benchmarking dell’inferenza ci consente di valutare quanto bene un modello di apprendimento automatico funziona in scenari del mondo reale. Questa valutazione garantisce che il modello sia pratico e affidabile quando distribuito in applicazioni, fornendo una comprensione più completa del comportamento del modello con dati reali. Inoltre, il benchmarking può aiutare a identificare potenziali colli di bottiglia o limitazioni nelle prestazioni del modello. Ad esempio, se un modello impiega troppo tempo per dedurre, potrebbe non essere pratico per applicazioni in tempo reale come la guida autonoma o gli assistenti vocali.\nL’efficienza delle risorse è un altro aspetto critico dell’inferenza, poiché può essere computazionalmente intensiva e richiedere memoria e potenza di elaborazione significative. Il benchmarking aiuta a garantire che il modello sia efficiente per quanto riguarda l’utilizzo delle risorse, il che è particolarmente importante per i dispositivi edge con capacità computazionali limitate, come smartphone o dispositivi IoT. Inoltre, il benchmarking ci consente di confrontare le prestazioni del nostro modello con quelli concorrenti o versioni precedenti dello stesso modello. Questo confronto è essenziale per prendere decisioni informate su quale modello implementare in un’applicazione specifica.\nInfine, è fondamentale garantire che le previsioni del modello non siano solo accurate, ma anche coerenti tra diversi dati. Il benchmarking aiuta a verificare l’accuratezza e la coerenza del modello, assicurando che soddisfi i requisiti dell’applicazione. Valuta inoltre la robustezza del modello, assicurando che possa gestire la variabilità dei dati del mondo reale e comunque fare previsioni accurate.\n\n\nMetriche\n\nPrecisione: La precisione è una delle metriche più importanti quando si confrontano i modelli di machine learning. Quantifica la percentuale di previsioni corrette effettuate dal modello rispetto ai valori o alle etichette reali. Ad esempio, se un modello di rilevamento dello spam riesce a classificare correttamente 95 messaggi e-mail su 100, la sua precisione verrebbe calcolata al 95%.\nLatenza: La latenza è una metrica delle prestazioni che calcola il ritardo o l’intervallo di tempo tra la ricezione dell’input e la produzione dell’output corrispondente da parte del sistema di apprendimento automatico. Un esempio che descrive chiaramente la latenza è un’applicazione di traduzione in tempo reale; se esiste un ritardo di mezzo secondo dal momento in cui un utente inserisce una frase al momento in cui l’app visualizza il testo tradotto, la latenza del sistema è di 0.5 secondi.\nLatency-Bounded Throughput: Il throughput limitato dalla latenza è una metrica preziosa che combina gli aspetti di latenza e throughput, misurando il throughput massimo di un sistema pur rispettando un vincolo di latenza specificato. Ad esempio, in un’applicazione di streaming video che utilizza un modello di apprendimento automatico per generare e visualizzare automaticamente i sottotitoli, il throughput limitato dalla latenza misurerebbe quanti frame video il sistema può elaborare al secondo (throughput) garantendo al contempo che i sottotitoli vengano visualizzati con un ritardo non superiore a 1 secondo (latenza). Questa metrica è particolarmente importante nelle applicazioni in tempo reale in cui soddisfare i requisiti di latenza è fondamentale per l’esperienza utente.\nThroughput: Il throughput valuta la capacità del sistema misurando il numero di inferenze o previsioni che un modello di apprendimento automatico può gestire entro un’unità di tempo specifica. Si consideri un sistema di riconoscimento vocale che utilizza una Recurrent Neural Network (RNN) come modello sottostante; se questo sistema riesce a elaborare e comprendere 50 diverse clip audio in un minuto, allora la sua velocità di elaborazione è di 50 clip al minuto.\nEfficienza energetica: L’efficienza energetica è una metrica che determina la quantità di energia consumata dal modello di apprendimento automatico per eseguire una singola inferenza. Un esempio lampante di ciò sarebbe un modello di elaborazione del linguaggio naturale basato su un’architettura di rete Transformer; se utilizza 0,1 Joule di energia per tradurre una frase dall’inglese al francese, la sua efficienza energetica è misurata a 0,1 Joule per inferenza.\nUtilizzo della memoria: L’utilizzo della memoria quantifica il volume di RAM necessario a un modello di apprendimento automatico per svolgere attività di inferenza. Un esempio rilevante per illustrare questo sarebbe un sistema di riconoscimento facciale basato su una CNN; se un tale sistema richiede 150 MB di RAM per elaborare e riconoscere i volti all’interno di un’immagine, il suo utilizzo della memoria è di 150 MB.\n\n\n\nI Task\nLe sfide nella scelta di attività rappresentative per il benchmarking dei sistemi di apprendimento automatico inferenziale sono, in generale, piuttosto simili alla tassonomia che abbiamo fornito per la il training. Tuttavia, per essere pignoli, discutiamone nel contesto dei sistemi di machine learning inferenziale.\n\nDiversità di Applicazioni: L’apprendimento automatico inferenziale è impiegato in numerosi domini come sanità, finanza, intrattenimento, sicurezza e altro. Ogni dominio ha attività uniche e ciò che è rappresentativo in un dominio potrebbe non esserlo in un altro. Ad esempio, un’attività di inferenza per prevedere i prezzi delle azioni nel dominio finanziario potrebbe differire dalle attività di riconoscimento delle immagini nel dominio medico.\nVariabilità nei Tipi di Dati: Diverse attività di inferenza richiedono diversi tipi di dati: testo, immagini, video, dati numerici, ecc. Assicurarsi che i benchmark affrontino l’ampia varietà di tipi di dati utilizzati nelle applicazioni del mondo reale è una sfida. Ad esempio, i sistemi di riconoscimento vocale elaborano dati audio, che sono molto diversi dai dati visivi elaborati dai sistemi di riconoscimento facciale.\nComplessità delle Attività: La complessità delle attività di inferenza può variare enormemente, da attività di classificazione di base ad attività complesse che richiedono modelli all’avanguardia. Ad esempio, distinguere tra due categorie (classificazione binaria) è in genere più semplice che rilevare centinaia di tipi di oggetti in una scena affollata.\nRequisiti in Tempo Reale: Alcune applicazioni richiedono risposte immediate o in tempo reale, mentre altre possono consentire un certo ritardo. Nella guida autonoma, il rilevamento degli oggetti in tempo reale e il processo decisionale sono fondamentali, mentre un motore di raccomandazione per un sito Web di shopping potrebbe tollerare lievi ritardi.\nProblemi di Scalabilità: Data la variabilità della scala delle applicazioni, dai dispositivi edge ai server basati su cloud, le attività devono rappresentare i diversi ambienti di elaborazione in cui si verifica l’inferenza. Ad esempio, un’attività di inferenza in esecuzione sulle risorse limitate di uno smartphone è diversa da un potente server cloud.\nDiversità delle Metriche di Valutazione: Le metriche utilizzate per valutare le prestazioni possono variare in modo significativo a seconda dell’attività. Trovare un terreno comune o una metrica universalmente accettata per attività diverse è una sfida. Ad esempio, la precisione e il “recall” [richiamo] potrebbero essere vitali per un’attività di diagnosi medica, mentre la produttività (inferenze al secondo) potrebbero essere più cruciali per le attività di elaborazione video.\nProblemi Etici e di Privacy: Esistono problemi relativi all’etica e alla privacy, soprattutto in aree sensibili come il riconoscimento facciale o l’elaborazione dei dati personali. Questi problemi possono influire sulla selezione e sulla natura delle attività utilizzate per il benchmarking. Ad esempio, l’utilizzo di dati facciali reali per il benchmarking può sollevare problemi di privacy, mentre i dati sintetici potrebbero non replicare le sfide del mondo reale.\nDiversità Hardware: Con un’ampia gamma di dispositivi, da GPU, CPU e TPU ad ASIC personalizzati utilizzati per l’inferenza, garantire che le attività siano rappresentative su hardware diversi è una sfida. Ad esempio, un’attività ottimizzata per l’inferenza su una GPU potrebbe avere prestazioni non ottimali su un dispositivo edge.\n\n\n\nI Benchmark\nEcco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per sistemi di apprendimento automatico inferenziale.\nMLPerf Inference Benchmark: MLPerf Inference è una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una varietà di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l’obiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza. Le sue metriche includono:\nMLPerf Inference è una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una varietà di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l’obiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza.\nMetriche:\n\nTempo di inferenza\nLatenza\nThroughput [Produttività]\nPrecisione\nConsumo energetico\n\nAI Benchmark: AI Benchmark è uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attività di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un’analisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware. Le sue metriche includono:\nAI Benchmark è uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e di apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attività di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un’analisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware.\nMetriche:\n\nTempo di inferenza\nLatenza\nConsumo energetico\nUtilizzo della memoria\nThroughput [Produttività]\n\nToolkit OpenVINO: Il toolkit OpenVINO fornisce uno strumento di benchmark per misurare le prestazioni dei modelli di apprendimento profondo per varie attività, come la classificazione delle immagini, il rilevamento degli oggetti e il riconoscimento facciale, su hardware Intel. Offre approfondimenti dettagliati sulle prestazioni di inferenza dei modelli su diverse configurazioni hardware. Le sue metriche includono:\nMetriche:\n\nTempo di inferenza\nThroughput [Produttività]\nLatenza\nUtilizzo di CPU e GPU\n\n\n\nCaso d’Uso di Esempio\nConsideriamo uno scenario in cui vogliamo valutare le prestazioni di inferenza di un modello di rilevamento di oggetti su uno specifico dispositivo edge.\nTask: L’attività consiste nell’eseguire il rilevamento di oggetti in tempo reale su flussi video, rilevando e identificando oggetti quali veicoli, pedoni e segnali stradali.\nBenchmark: Possiamo utilizzare AI Benchmark per questa attività in quanto valuta le prestazioni di inferenza sui dispositivi edge, il che si adatta al nostro scenario.\nMetriche: Misureremo le seguenti metriche:\n\nTempo di inferenza per elaborare ogni fotogramma video\nLatenza per generare i “bounding box” per gli oggetti rilevati\nConsumo energetico durante il processo di inferenza\nProduttività in termini di fotogrammi video elaborati al secondo\n\nMisurando queste metriche, possiamo valutare le prestazioni del modello di rilevamento di oggetti sul dispositivo edge e identificare eventuali colli di bottiglia o aree di ottimizzazione per migliorare le capacità di elaborazione in tempo reale.\n\n\n\n\n\n\nEsercizio 11.2: Benchmark di Inferenza - MLPerf\n\n\n\n\n\nPrepararsi a mettere alla prova i propri modelli di intelligenza artificiale! MLPerf è come le Olimpiadi per le prestazioni del machine learning. In questo Colab, utilizzeremo un toolkit chiamato CK per eseguire benchmark MLPerf ufficiali, misurare la velocità e l’accuratezza di un proprio modello e persino utilizzare TVM per dargli una spinta super veloce. Pronti a vedere il modello vincere la sua medaglia?\n\n\n\n\n\n\n\n11.4.6 Esempio di Benchmark\nPer illustrare correttamente i componenti di un benchmark di sistema, possiamo esaminare il benchmark di individuazione delle parole chiave in MLPerf Tiny e spiegare la motivazione alla base di ogni decisione.\n\nTask\nL’individuazione delle parole chiave è stata selezionata come attività perché è un caso d’uso comune in TinyML che è stato ben consolidato per anni. Inoltre, l’hardware tipico utilizzato per l’individuazione delle parole chiave differisce sostanzialmente dalle offerte di altri benchmark, come l’attività di riconoscimento vocale di MLPerf Inference.\n\n\nIl Dataset\nGoogle Speech Commands (Warden 2018) è stato selezionato come il miglior dataset per rappresentare l’attività. Il dataset è ben consolidato nella comunità di ricerca e ha una licenza permissiva, che consente di utilizzarlo facilmente in un benchmark.\n\nWarden, Pete. 2018. «Speech commands: A dataset for limited-vocabulary speech recognition». ArXiv preprint abs/1804.03209. https://arxiv.org/abs/1804.03209.\n\n\nModello\nIl componente principale successivo è il modello, che fungerà da carico di lavoro primario per il benchmark. Il modello dovrebbe essere ben consolidato come soluzione per l’attività selezionata piuttosto che una soluzione all’avanguardia. Il modello selezionato è un semplice modello di convoluzione separabile in profondità. Questa architettura non è la soluzione all’avanguardia per l’attività, ma è ben consolidata e non progettata per una piattaforma hardware specifica come molte soluzioni all’avanguardia. Nonostante sia un benchmark di inferenza, stabilisce anche una ricetta di training di riferimento per essere completamente riproducibile e trasparente.\n\n\nMetriche\nLa latenza è stata selezionata come metrica primaria per il benchmark, poiché i sistemi di individuazione delle parole chiave devono reagire rapidamente per mantenere la soddisfazione dell’utente. Inoltre, dato che i sistemi TinyML sono spesso alimentati a batteria, il consumo energetico viene misurato per garantire l’efficienza della piattaforma hardware. L’accuratezza del modello viene misurata anche per garantire che le ottimizzazioni applicate da un submitter, come la quantizzazione, non degradino l’accuratezza oltre una soglia.\n\n\nBenchmark Harness\nMLPerf Tiny utilizza EEMBCs EnergyRunner benchmark harness per caricare gli input nel modello e isolare e misurare il consumo energetico del dispositivo. Quando si misura il consumo energetico, è fondamentale selezionare un “harness” [imbracatura] che sia accurato ai livelli di potenza previsti dei dispositivi sottoposti a test e sufficientemente semplice da non diventare un peso per i partecipanti al benchmark.\n\n\nLa Baseline\nGli invii di baseline sono fondamentali per contestualizzare i risultati e come punto di riferimento per aiutare i partecipanti a iniziare. L’invio di base dovrebbe dare priorità alla semplicità e alla leggibilità rispetto alle prestazioni avanzate. L’individuazione della parola chiave della baseline utilizza un microcontrollore STM standard come hardware e TensorFlow Lite come microcontrollore (David et al. 2021) come framework di inferenza.\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\n\n\n\n11.4.7 Sfide e Limitazioni\nSebbene il benchmarking fornisca una metodologia strutturata per la valutazione delle prestazioni in domini complessi come l’intelligenza artificiale e l’informatica, il processo pone anche diverse sfide. Se non affrontati correttamente, questi ostacoli possono minare la credibilità e l’accuratezza dei risultati del benchmarking. Alcune delle difficoltà predominanti affrontate nel benchmarking includono quanto segue:\n\nCopertura incompleta del problema: Le attività di benchmarking potrebbero non rappresentare completamente lo spazio del problema. Ad esempio, i set di dati di classificazione delle immagini comuni come CIFAR-10 hanno una diversità limitata nei tipi di immagini. Gli algoritmi ottimizzati per tali benchmark potrebbero non riuscire a generalizzare bene con i set di dati del mondo reale.\nInsignificanza statistica: I benchmark devono avere prove e campioni di dati sufficienti per produrre risultati statisticamente significativi. Ad esempio, il benchmarking di un modello OCR su solo poche scansioni di testo potrebbe non catturare adeguatamente i suoi veri tassi di errore.\nRiproducibilità limitata: Variazioni di hardware, versioni software, basi di codice e altri fattori possono ridurre la riproducibilità dei risultati di benchmark. MLPerf affronta questo problema fornendo implementazioni di riferimento e specifiche ambientali.\nDisallineamento con gli obiettivi finali: I benchmark che si concentrano solo su metriche di velocità o accuratezza possono disallineare gli obiettivi reali come costi ed efficienza energetica. I benchmark devono riflettere tutti gli assi prestazionali critici.\nRapida obsolescenza: A causa del rapido ritmo dei progressi nell’intelligenza artificiale e nell’informatica, i benchmark e i loro set di dati possono rapidamente diventare obsoleti. Mantenere benchmark aggiornati è quindi una sfida persistente.\n\nMa di tutte queste, la sfida più importante è l’ingegneria dei benchmark.\n\nLotteria Hardware\nLa “hardware lottery” nel benchmarking dei sistemi di machine learning si riferisce alla situazione in cui il successo o l’efficienza di un modello di apprendimento automatico sono significativamente influenzati dalla compatibilità del modello con l’hardware sottostante (Chu et al. 2021). In altre parole, alcuni modelli hanno prestazioni eccezionali perché sono adatti alle caratteristiche o alle capacità specifiche dell’hardware su cui vengono eseguiti, piuttosto che perché sono modelli intrinsecamente superiori.\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. «Discovering Multi-Hardware Mobile Models via Architecture Search». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\n\n\n\n\nFigura 11.2: Hardware Lottery.\n\n\n\nAd esempio, alcuni modelli di apprendimento automatico possono essere progettati e ottimizzati per sfruttare le capacità di elaborazione parallela di acceleratori hardware specifici, come le unità di elaborazione grafica (GPU) o le unità di elaborazione tensore (TPU). Di conseguenza, questi modelli potrebbero mostrare prestazioni superiori quando vengono sottoposti a benchmark su tale hardware rispetto ad altri modelli che non sono ottimizzati per l’hardware.\nAd esempio, un articolo del 2018 ha introdotto una nuova architettura di rete neurale convoluzionale per la classificazione delle immagini che ha raggiunto un’accuratezza all’avanguardia su ImageNet. Tuttavia, l’articolo menzionava solo che il modello era stato addestrato su 8 GPU senza specificare il modello, la dimensione della memoria o altri dettagli rilevanti. Uno studio di follow-up ha cercato di riprodurre i risultati, ma ha scoperto che addestrare lo stesso modello su GPU comunemente disponibili ha ottenuto un’accuratezza inferiore del 10%, anche dopo l’ottimizzazione degli iperparametri. L’hardware originale probabilmente aveva una larghezza di banda di memoria e una potenza di calcolo molto più elevate. Come altro esempio, i tempi di addestramento per modelli linguistici di grandi dimensioni possono variare drasticamente in base alle GPU utilizzate.\nLa “hardware lottery” può introdurre sfide e bias nel benchmarking dei sistemi di apprendimento automatico, poiché le prestazioni del modello non dipendono esclusivamente dall’architettura o dall’algoritmo del modello, ma anche dalla compatibilità e dalle sinergie con l’hardware sottostante. Ciò può rendere difficile confrontare equamente diversi modelli e identificare il modello migliore in base ai suoi meriti intrinseci. Può anche portare a una situazione in cui la comunità converge su modelli che sono adatti all’hardware più diffuso del momento, trascurando potenzialmente altri modelli che potrebbero essere superiori ma incompatibili con le attuali tendenze hardware.\n\n\nBenchmark Engineering\nLa lotteria hardware si verifica quando un modello di apprendimento automatico funziona in modo eccezionalmente bene o male su una configurazione hardware specifica a causa di compatibilità o incompatibilità impreviste. Il modello non è esplicitamente progettato o ottimizzato per quell’hardware specifico dagli sviluppatori o dagli ingegneri; piuttosto, capita che si allinei o (non si allinei) con le capacità o le limitazioni dell’hardware. In questo caso, le prestazioni del modello sull’hardware sono un prodotto della coincidenza piuttosto che della progettazione.\nContrariamente alla lotteria hardware accidentale, il benchmark engineering implica l’ottimizzazione o la progettazione deliberata di un modello di apprendimento automatico per funzionare eccezionalmente bene su hardware specifico, spesso per vincere benchmark o competizioni. Questa ottimizzazione intenzionale potrebbe includere la modifica dell’architettura, degli algoritmi o dei parametri del modello per sfruttare appieno le funzionalità e le capacità dell’hardware.\n\n\nProblema\nIl benchmark engineering si riferisce alla modifica o all’ottimizzazione di un sistema di intelligenza artificiale per ottimizzare le prestazioni su test di benchmark specifici, spesso a scapito della generalizzabilità o delle prestazioni nel mondo reale. Ciò può includere la regolazione di iperparametri, dati di training o altri aspetti del sistema specificamente per ottenere punteggi elevati sulle metriche di benchmark senza necessariamente migliorare la funzionalità o l’utilità complessiva del sistema.\nLa motivazione alla base dell’ingegneria dei benchmark spesso deriva dal desiderio di ottenere punteggi di prestazioni elevate per scopi di marketing o competitivi. Punteggi di benchmark elevati possono dimostrare la superiorità di un sistema di intelligenza artificiale rispetto ai concorrenti e possono essere un argomento chiave per la vendita per potenziali utenti o investitori. Questa pressione per ottenere buoni risultati nei benchmark a volte porta a dare priorità alle ottimizzazioni specifiche del benchmark rispetto a miglioramenti più olistici del sistema.\nPuò comportare diversi rischi e sfide. Uno dei rischi principali è che il sistema di intelligenza artificiale possa funzionare meglio nelle applicazioni del mondo reale rispetto a quanto suggeriscono i punteggi di benchmark. Ciò può portare a insoddisfazione dell’utente, danni alla reputazione e potenziali problemi di sicurezza o etici. Inoltre, l’ingegneria dei benchmark può contribuire a una mancanza di trasparenza e responsabilità nella comunità dell’intelligenza artificiale, poiché può essere difficile discernere quanta parte delle prestazioni di un sistema di intelligenza artificiale sia dovuta a miglioramenti genuini rispetto a ottimizzazioni specifiche del benchmark.\nLa comunità AI deve dare priorità alla trasparenza e alla responsabilità per mitigare i rischi associati all’ingegneria dei benchmark. Ciò può includere la divulgazione di eventuali ottimizzazioni o modifiche apportate specificamente per i test di benchmark e la fornitura di valutazioni più complete dei sistemi AI che includono metriche delle prestazioni del mondo reale e punteggi di benchmark. I ricercatori e gli sviluppatori devono dare priorità a miglioramenti olistici dei sistemi AI che ne migliorino la generalizzabilità e la funzionalità in varie applicazioni anziché concentrarsi esclusivamente su ottimizzazioni specifiche del benchmark.\n\n\nProblemi\nUno dei problemi principali dell’ingegneria del benchmark è che può compromettere le prestazioni reali dei sistemi di intelligenza artificiale. Quando gli sviluppatori si concentrano sull’ottimizzazione dei loro sistemi per ottenere punteggi elevati in specifici test di benchmark, potrebbero trascurare altri importanti aspetti delle prestazioni del sistema, cruciali nelle applicazioni del mondo reale. Ad esempio, un sistema di intelligenza artificiale progettato per il riconoscimento delle immagini potrebbe essere progettato per funzionare eccezionalmente bene in un test di benchmark che include un set specifico di immagini, ma necessita di aiuto per riconoscere accuratamente immagini leggermente diverse da quelle nel set di test.\nUn’altra area di miglioramento con l’ingegneria di benchmark è che può comportare sistemi di intelligenza artificiale privi di generalizzabilità. In altre parole, mentre il sistema può funzionare bene nel test di benchmark, potrebbe aver bisogno di aiuto per gestire una vasta gamma di input o scenari. Ad esempio, un modello di intelligenza artificiale sviluppato per l’elaborazione del linguaggio naturale potrebbe essere progettato per ottenere punteggi elevati in un test di benchmark che include un tipo specifico di testo, ma non riesce a elaborare accuratamente il testo che non rientra in quel tipo specifico.\nPuò anche portare a risultati fuorvianti. Quando i sistemi di intelligenza artificiale sono progettati per funzionare bene nei test di benchmark, i risultati potrebbero non riflettere accuratamente le reali capacità del sistema. Questo può essere problematico per gli utenti o gli investitori che si affidano ai punteggi di benchmark per prendere decisioni informate su quali sistemi di intelligenza artificiale utilizzare o in cui investire. Ad esempio, un sistema di intelligenza artificiale progettato per ottenere punteggi elevati in un test di benchmark per il riconoscimento vocale potrebbe dover essere più in grado di riconoscere accuratamente il parlato in situazioni reali, portando gli utenti o gli investitori a prendere decisioni basate su informazioni imprecise.\n\n\nAttenuazione\nEsistono diversi modi per mitigare l’ingegneria dei benchmark. La trasparenza nel processo di benchmarking è fondamentale per mantenere l’accuratezza e l’affidabilità dei benchmark. Ciò implica la divulgazione chiara delle metodologie, dei set di dati e dei criteri di valutazione utilizzati nei test di benchmark, nonché di eventuali ottimizzazioni o modifiche apportate al sistema di intelligenza artificiale ai fini del benchmark.\nUn modo per ottenere trasparenza è attraverso l’uso di benchmark open source. I benchmark open source vengono resi disponibili al pubblico, consentendo a ricercatori, sviluppatori e altre parti interessate di esaminarli, criticarli e contribuire, garantendone così l’accuratezza e l’affidabilità. Questo approccio collaborativo facilita anche la condivisione delle “best practice” e lo sviluppo di benchmark più solidi e completi.\nUn esempio è MLPerf Tiny. È un framework open source progettato per semplificare il confronto di diverse soluzioni nel mondo di TinyML. Il suo design modulare consente di sostituire i componenti per il confronto o il miglioramento. Le implementazioni di riferimento, mostrate in verde e arancione in Figura 11.3, fungono da base per i risultati. TinyML spesso necessita di ottimizzazione nell’intero sistema e gli utenti possono contribuire concentrandosi su parti specifiche, come la quantizzazione. Il design modulare del benchmark consente agli utenti di mostrare i propri contributi e il vantaggio competitivo modificando un’implementazione di riferimento. In breve, MLPerf Tiny offre un modo flessibile e modulare per valutare e migliorare le applicazioni TinyML, semplificando il confronto e il miglioramento di diversi aspetti della tecnologia.\n\n\n\n\n\n\nFigura 11.3: Design modulare di MLPerf Tiny. Fonte: Mattson et al. (2020a).\n\n\n———, et al. 2020a. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nUn altro metodo per ottenere trasparenza è attraverso la revisione paritaria dei benchmark. Ciò comporta che esperti indipendenti esaminino e convalidino la metodologia, i set di dati e i risultati del benchmark per garantirne la credibilità e l’affidabilità. La revisione paritaria può fornire un mezzo prezioso per verificare l’accuratezza dei test di benchmark e contribuire a creare fiducia nei risultati.\nLa standardizzazione dei benchmark è un’altra importante soluzione per mitigare l’ingegneria dei benchmark. I benchmark standardizzati forniscono un quadro comune per la valutazione dei sistemi di intelligenza artificiale, garantendo coerenza e comparabilità tra diversi sistemi e applicazioni. Ciò può essere ottenuto sviluppando standard e “best practice” per l’intero settore per il benchmarking e tramite metriche e criteri di valutazione comuni.\nAnche la verifica da parte di terze parti dei risultati può essere preziosa per mitigare l’ingegneria dei benchmark. Ciò comporta che una terza parte indipendente verifichi i risultati di un test di benchmark per garantirne la credibilità e l’affidabilità. La verifica di terze parti può creare fiducia nei risultati e fornire un mezzo prezioso per convalidare le prestazioni e le capacità dei sistemi di intelligenza artificiale.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmarking-del-modello",
    "href": "contents/benchmarking/benchmarking.it.html#benchmarking-del-modello",
    "title": "11  Benchmarking AI",
    "section": "11.5 Benchmarking del Modello",
    "text": "11.5 Benchmarking del Modello\nIl benchmarking dei modelli di machine learning è importante per determinare l’efficacia e l’efficienza di vari algoritmi di apprendimento automatico nella risoluzione di compiti o problemi specifici. Analizzando i risultati ottenuti dal benchmarking, sviluppatori e ricercatori possono identificare i punti di forza e di debolezza dei loro modelli, portando a decisioni più informate sulla selezione del modello e su un’ulteriore ottimizzazione.\nL’evoluzione e il progresso dei modelli di apprendimento automatico sono intrinsecamente collegati alla disponibilità e alla qualità dei set di dati. Nell’apprendimento automatico, i dati fungono da materia prima che alimenta gli algoritmi, consentendo loro di apprendere, adattarsi e, in definitiva, eseguire compiti che erano tradizionalmente di dominio degli esseri umani. Pertanto, è importante comprendere questa storia.\n\n11.5.1 Contesto Storico\nI dataset di apprendimento automatico hanno una storia ricca e si sono evoluti in modo significativo nel corso degli anni, crescendo in dimensioni, complessità e diversità per soddisfare le richieste sempre crescenti del settore. Diamo un’occhiata più da vicino a questa evoluzione, partendo da uno dei primi e più iconici set di dati: MNIST.\n\nMNIST (1998)\nIl dataset MNIST, creato da Yann LeCun, Corinna Cortes e Christopher J.C. Burges nel 1998, può essere considerato una pietra miliare nella storia dei dataset di machine learning. Comprende 70.000 immagini in scala di grigi da 28x28 pixel etichettate di cifre scritte a mano (0-9). MNIST è stato ampiamente utilizzato per il benchmarking degli algoritmi nell’elaborazione delle immagini e nell’apprendimento automatico come punto di partenza per molti ricercatori e professionisti. Figura 11.4 mostra alcuni esempi di cifre scritte a mano.\n\n\n\n\n\n\nFigura 11.4: Cifre scritte a mano in MNIST. Fonte: Suvanjanprasai.\n\n\n\n\n\nImageNet (2009)\nFacciamo un salto al 2009 e vediamo l’introduzione di ImageNet, che ha segnato un balzo significativo nella scala e nella complessità dei dataset. ImageNet è composto da oltre 14 milioni di immagini etichettate che abbracciano più di 20.000 categorie. Fei-Fei Li e il suo team lo hanno sviluppato per far progredire il riconoscimento degli oggetti e la ricerca sulla visione artificiale. Il dataset è diventato sinonimo della ImageNet Large Scale Visual Recognition Challenge (ILSVRC), una competizione annuale cruciale nello sviluppo di modelli di deep learning, tra cui il famoso AlexNet nel 2012.\n\n\nCOCO (2014)\nIl Common Objects in Context (COCO) dataset (Lin et al. 2014), rilasciato nel 2014, ha ulteriormente ampliato il panorama dei set di dati di apprendimento automatico introducendo un set più ricco di annotazioni. COCO è costituito da immagini contenenti scene complesse con più oggetti e ogni immagine è annotata con riquadri di delimitazione degli oggetti, maschere di segmentazione e didascalie. Questo set di dati è stato determinante nel far progredire la ricerca nel rilevamento degli oggetti, nella segmentazione e nella didascalia delle immagini.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, e C Lawrence Zitnick. 2014. «Microsoft coco: Common objects in context». In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740–55. Springer.\nhttps://cocodataset.org/images/jpg/coco-examples.jpg\n\n\nGPT-3 (2020)\nSebbene gli esempi sopra riportati si concentrino principalmente sui dataset di immagini, si sono verificati anche sviluppi significativi nei dataset di testo. Un esempio degno di nota è GPT-3 (Brown et al. 2020), sviluppato da OpenAI. GPT-3 è un modello linguistico addestrato su testo Internet eterogeneo. Sebbene il dataset utilizzato per addestrare GPT-3 non sia disponibile al pubblico, il modello stesso, costituito da 175 miliardi di parametri, è una testimonianza della scala e della complessità dei moderni dataset e modelli di apprendimento automatico.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nPresente e Futuro\nOggi disponiamo di una pletora di dataset che abbracciano vari domini, tra cui sanità, finanza, scienze sociali e altro ancora. Le seguenti caratteristiche ci aiutano a classificare lo spazio e la crescita dei dataset di apprendimento automatico che alimentano lo sviluppo del modello.\n\nDiversità dei Set di Dati: La varietà di set di dati disponibili per ricercatori e ingegneri si è ampliata notevolmente, coprendo molti campi, tra cui l’elaborazione del linguaggio naturale, il riconoscimento delle immagini e altro ancora. Questa diversità ha alimentato lo sviluppo di modelli di apprendimento automatico specializzati, su misura per attività specifiche, come la traduzione, il riconoscimento vocale e il riconoscimento facciale.\nVolume di Dati: L’enorme volume di dati che è diventato disponibile nell’era digitale ha anche svolto un ruolo cruciale nel progresso dei modelli di apprendimento automatico. I grandi set di dati consentono ai modelli di catturare la complessità e le sfumature dei fenomeni del mondo reale, portando a previsioni più accurate e affidabili.\nQualità e Pulizia dei Dati: La qualità dei dati è un altro fattore critico che influenza le prestazioni dei modelli di apprendimento automatico. Set di dati puliti, ben etichettati e imparziali sono essenziali per modelli di addestramento solidi ed equi.\nAccesso Aperto ai Dati: La disponibilità di set di dati “open-access” ha contribuito in modo significativo anche al progresso dell’apprendimento automatico. I dati “aperti” consentono ai ricercatori di tutto il mondo di collaborare, condividere approfondimenti e basarsi sul lavoro degli altri, portando a un’innovazione più rapida e allo sviluppo di modelli più avanzati.\nProblemi di Etica e Privacy: Man mano che i set di dati crescono in dimensioni e complessità, le considerazioni etiche e i problemi di privacy diventano sempre più importanti. È in corso un dibattito sull’equilibrio tra lo sfruttamento dei dati per i progressi dell’apprendimento automatico e la protezione dei diritti alla privacy degli individui.\n\nLo sviluppo di modelli di apprendimento automatico si basa in larga misura sulla disponibilità di set di dati diversificati, grandi, di alta qualità e ad accesso libero. Mentre andiamo avanti, affrontare le considerazioni etiche e le preoccupazioni sulla privacy associate all’uso di grandi set di dati è fondamentale per garantire che le tecnologie di apprendimento automatico siano vantaggiose per la società. C’è una crescente consapevolezza che i dati agiscono come carburante per l’apprendimento automatico, guidando e alimentando lo sviluppo di modelli di apprendimento automatico. Di conseguenza, si sta ponendo maggiore attenzione sullo sviluppo dei set di dati stessi. Esploreremo questo aspetto in modo più dettagliato nella sezione del benchmarking dei dati.\n\n\n\n11.5.2 Metriche del Modello\nLa valutazione del modello di machine learning si è evoluta da un focus ristretto sulla precisione a un approccio più completo che considera una serie di fattori, da considerazioni etiche e applicabilità nel mondo reale a vincoli pratici come dimensioni ed efficienza del modello. Questo cambiamento riflette la maturazione del campo poiché i modelli di apprendimento automatico vengono sempre più applicati in scenari reali diversi e complessi.\n\nPrecisione\nLa precisione è una delle metriche più intuitive e comunemente utilizzate per valutare i modelli di apprendimento automatico. In sostanza, la precisione misura la percentuale di previsioni corrette effettuate dal modello rispetto a tutte le previsioni. Ad esempio, immaginiamo di aver sviluppato un modello di apprendimento automatico per classificare le immagini come contenenti o meno un gatto. Se testiamo questo modello su un set di dati di 100 immagini e ne identifica correttamente 90, calcoleremmo la sua precisione al 90%.\nNelle fasi iniziali dell’apprendimento automatico, la precisione era spesso la metrica principale, se non l’unica, considerata quando si valutavano le prestazioni del modello. Ciò è comprensibile, data la sua natura semplice e la facilità di interpretazione. Tuttavia, con il progredire del settore, i limiti del fare affidamento esclusivamente sulla precisione sono diventati più evidenti.\nSi consideri l’esempio di un modello di diagnosi medica con una precisione del 95%. Sebbene a prima vista possa sembrare impressionante, dobbiamo guardare più a fondo per valutare appieno le prestazioni del modello. Supponiamo che il modello non riesca a diagnosticare accuratamente condizioni gravi che, sebbene rare, possono avere gravi conseguenze; la sua elevata precisione potrebbe non essere così significativa. Un esempio pertinente di ciò è il modello di apprendimento automatico della retinopatia di Google, progettato per diagnosticare la retinopatia diabetica e l’edema maculare diabetico da fotografie della retina.\nIl modello di Google ha dimostrato livelli di precisione impressionanti in contesti di laboratorio. Tuttavia, quando è stato distribuito in ambienti clinici reali in Thailandia, ha dovuto affrontare sfide significative. Nel contesto reale, il modello ha incontrato popolazioni di pazienti diverse, qualità delle immagini variabili e una gamma di diverse condizioni mediche a cui non era stato esposto durante il suo training. Di conseguenza, le sue prestazioni avrebbero potuto essere migliori e ha fatto fatica a mantenere gli stessi livelli di accuratezza osservati in laboratorio. Questo esempio serve come un chiaro promemoria del fatto che, sebbene un’elevata accuratezza sia un attributo importante e desiderabile per un modello di diagnosi medica, deve essere valutata insieme ad altri fattori, come la capacità del modello di generalizzare a diverse popolazioni e gestire condizioni reali diverse e imprevedibili, per comprenderne veramente il valore e il potenziale impatto sull’assistenza ai pazienti.\nAllo stesso modo, se il modello funziona bene in media ma mostra significative disparità nelle prestazioni tra diversi gruppi demografici, anche questo sarebbe motivo di preoccupazione.\nL’evoluzione dell’apprendimento automatico ha quindi visto uno spostamento verso un approccio più olistico alla valutazione del modello, tenendo conto non solo dell’accuratezza, ma anche di altri fattori cruciali come la correttezza, trasparenza e applicabilità nel mondo reale. Un esempio lampante è il progetto Gender Shades del MIT Media Lab, guidato da Joy Buolamwini, che evidenzia significativi pregiudizi razziali e di genere nei sistemi commerciali di riconoscimento facciale. Il progetto ha valutato le prestazioni di tre tecnologie di riconoscimento facciale sviluppate da IBM, Microsoft e Face++. Ha scoperto che tutte presentavano dei pregiudizi, con prestazioni migliori su volti maschili e dalla pelle più chiara rispetto a volti femminili e dalla pelle più scura.\nSebbene l’accuratezza rimanga una metrica fondamentale e preziosa per la valutazione dei modelli di apprendimento automatico, è necessario un approccio più completo per valutare appieno le prestazioni di un modello. Ciò significa considerare metriche aggiuntive che tengano conto di correttezza, trasparenza e applicabilità nel mondo reale, nonché condurre test rigorosi su diversi set di dati per scoprire e mitigare eventuali potenziali pregiudizi. Il passaggio a un approccio più olistico alla valutazione del modello riflette la maturazione del campo e il suo crescente riconoscimento delle implicazioni nel mondo reale e delle considerazioni etiche associate all’implementazione di modelli di apprendimento automatico.\n\n\nCorrettezza\nLa correttezza nei modelli di apprendimento automatico è un aspetto multiforme e critico che richiede un’attenzione particolare, in particolare nelle applicazioni ad alto rischio che influenzano significativamente la vita delle persone, come nei processi di approvazione dei prestiti, nelle assunzioni e nella giustizia penale. Si riferisce al trattamento equo di tutti gli individui, indipendentemente dai loro attributi demografici o sociali come razza, genere, età o stato socioeconomico.\nAffidarsi semplicemente all’accuratezza può essere insufficiente e potenzialmente fuorviante quando si valutano i modelli. Ad esempio, si consideri un modello di approvazione dei prestiti con un tasso di accuratezza del 95%. Sebbene questa cifra possa sembrare impressionante a prima vista, non rivela come il modello si comporta nei diversi gruppi demografici. Se questo modello discrimina costantemente un gruppo particolare, la sua accuratezza è meno encomiabile e la sua correttezza viene messa in discussione.\nLa discriminazione può manifestarsi in varie forme, come la discriminazione diretta, in cui un modello utilizza esplicitamente attributi sensibili come razza o genere nel suo processo decisionale, o discriminazione indiretta, in cui variabili apparentemente neutre sono correlate ad attributi sensibili, influenzando indirettamente i risultati del modello. Un esempio infame di quest’ultimo è lo strumento COMPAS utilizzato nel sistema di giustizia penale degli Stati Uniti, che ha mostrato pregiudizi razziali nel prevedere i tassi di recidiva nonostante non utilizzasse esplicitamente la razza come variabile.\nAffrontare l’equità implica un attento esame delle prestazioni del modello tra gruppi diversi, identificando potenziali pregiudizi e rettificando le disparità attraverso misure correttive come il ribilanciamento dei set di dati, l’adeguamento dei parametri del modello e l’implementazione di algoritmi consapevoli dell’equità e della correttezza. Ricercatori e professionisti sviluppano continuamente metriche e metodologie su misura per casi d’uso specifici per valutare la correttezza e l’equità in scenari del mondo reale. Ad esempio, l’analisi di impatto disparato, la parità demografica e le pari opportunità sono alcune delle metriche impiegate per valutare l’equità/correttezza.\nInoltre, la trasparenza e l’interpretabilità dei modelli sono fondamentali per raggiungere la correttezza. Comprendere come un modello prende decisioni può rivelare potenziali pregiudizi e consentire alle parti interessate di ritenere responsabili gli sviluppatori. Strumenti open source come AI Fairness 360 di IBM e Fairness Indicators di TensorFlow sono in fase di sviluppo per facilitare le valutazioni dell’equità/correttezza e l’attenuazione dei pregiudizi nei modelli di apprendimento automatico.\nGarantire l’equità/correttezza nei modelli di apprendimento automatico, in particolare nelle applicazioni che hanno un impatto significativo sulla vita delle persone, richiede una rigorosa valutazione delle prestazioni del modello in gruppi diversi, un’attenta identificazione e attenuazione dei pregiudizi e l’implementazione di misure di trasparenza e interpretabilità. Affrontando la correttezza in modo completo, possiamo lavorare per sviluppare modelli di apprendimento automatico equi, giusti e vantaggiosi per la società.\n\n\nComplessità\n\nParameteri*\nNelle fasi iniziali del machine learning, il benchmarking dei modelli si basava spesso sui conteggi dei parametri come proxy [sostituto] per la complessità del modello. La logica era che più parametri in genere portano a un modello più complesso, che dovrebbe, a sua volta, fornire prestazioni migliori. Tuttavia, questo approccio si è dimostrato inadeguato in quanto deve tenere conto del costo computazionale associato all’elaborazione di molti parametri.\nAd esempio, GPT-3, sviluppato da OpenAI, è un modello linguistico che vanta ben 175 miliardi di parametri. Sebbene raggiunga prestazioni all’avanguardia in varie attività di elaborazione del linguaggio naturale, le sue dimensioni e le risorse computazionali necessarie per eseguirlo lo rendono poco pratico per l’implementazione in molti scenari del mondo reale, in particolare quelli con capacità computazionali limitate.\nAffidarsi ai conteggi dei parametri come proxy per la complessità del modello non riesce a considerare anche l’efficienza del modello. Se ottimizzato per l’efficienza, un modello con meno parametri potrebbe essere altrettanto efficace, se non di più, di un modello con un conteggio di parametri più elevato. Ad esempio, MobileNets, sviluppato da Google, è una famiglia di modelli progettati specificamente per dispositivi mobili ed edge. Utilizzano convoluzioni separabili in base alla profondità per ridurre il numero di parametri e i costi computazionali, pur mantenendo prestazioni competitive.\nAlla luce di queste limitazioni, il settore si è spostato verso un approccio più olistico al benchmarking dei modelli che considera i conteggi dei parametri e altri fattori cruciali come le operazioni in virgola mobile al secondo (FLOP), il consumo di memoria e la latenza. I FLOP, in particolare, sono emersi come una metrica importante in quanto forniscono una rappresentazione più accurata del carico computazionale imposto da un modello. Questo passaggio a un approccio più completo al benchmarking dei modelli riflette il riconoscimento della necessità di bilanciare prestazioni e praticità, assicurando che i modelli siano efficaci, efficienti e implementabili in scenari reali.\n\n\nFLOP\nLa dimensione di un modello di apprendimento automatico è un aspetto essenziale che influisce direttamente sulla sua usabilità in scenari pratici, soprattutto quando le risorse computazionali sono limitate. Tradizionalmente, il numero di parametri in un modello veniva spesso utilizzato come proxy per le sue dimensioni, con l’ipotesi di base che più parametri si sarebbero tradotti in prestazioni migliori. Tuttavia, questa visione semplicistica non considera il costo computazionale dell’elaborazione di questi parametri. È qui che entra in gioco il concetto di “floating-point operations per second (FLOP)” [operazioni in virgola mobile al secondo], che fornisce una rappresentazione più accurata del carico computazionale imposto da un modello.\nI FLOP misurano il numero di operazioni in virgola mobile eseguite da un modello per generare una previsione. Un modello con molti FLOP richiede risorse computazionali sostanziali per elaborare il vasto numero di operazioni, il che potrebbe renderlo poco pratico per alcune applicazioni. Al contrario, un modello con un conteggio di FLOP inferiore è più leggero e può essere facilmente distribuito in scenari in cui le risorse computazionali sono limitate.\nFigura 11.5, da (Bianco et al. 2018), mostra la relazione tra la Top-1 Accuracy su ImageNet (asse y), i G-FLOP del modello (asse x) e il conteggio dei parametri del modello (dimensione del cerchio).\n\n\n\n\n\n\nFigura 11.5: Un grafico che raffigura la top-1 Imagenet Accuracy rispetto al conteggio FLOP di un modello insieme al conteggio dei parametri del modello. La figura mostra un compromesso complessivo tra complessità e accuratezza del modello, sebbene alcune architetture del modello siano più efficienti di altre. Fonte: Bianco et al. (2018).\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, e Paolo Napoletano. 2018. «Benchmark analysis of representative deep neural network architectures». IEEE access 6: 64270–77.\n\n\nConsideriamo un esempio. BERT [Bidirectional Encoder Representations from Transformers] (Devlin et al. 2019), un popolare modello di elaborazione del linguaggio naturale, ha oltre 340 milioni di parametri, il che lo rende un modello di grandi dimensioni con elevata accuratezza e prestazioni impressionanti in varie attività. Tuttavia, le dimensioni di BERT, unite al suo elevato numero di FLOP, lo rendono un modello computazionalmente intensivo che potrebbe non essere adatto per applicazioni in tempo reale o per l’implementazione su dispositivi edge con capacità computazionali limitate.\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, e Kristina Toutanova. 2019. «BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding». In Proceedings of the 2019 Conference of the North, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423.\nAlla luce di ciò, c’è stato un crescente interesse nello sviluppo di modelli più piccoli in grado di raggiungere livelli di prestazioni simili alle loro controparti più grandi, pur essendo più efficienti nel carico computazionale. DistilBERT, ad esempio, è una versione più piccola di BERT che mantiene il 97% delle sue prestazioni, pur essendo il 40% più piccola in termini di numero di parametri. La riduzione delle dimensioni si traduce anche in un numero di FLOP inferiore, rendendo DistilBERT una scelta più pratica per scenari con risorse limitate.\nIn sintesi, mentre il conteggio dei parametri fornisce un’indicazione utile della dimensione del modello, non è una metrica completa in quanto deve considerare il costo computazionale associato all’elaborazione di questi parametri. I FLOP, d’altro canto, offrono una rappresentazione più accurata del carico computazionale di un modello e sono quindi una considerazione essenziale quando si distribuiscono modelli di apprendimento automatico in scenari reali, in particolare quando le risorse computazionali sono limitate. L’evoluzione dal basarsi esclusivamente sul conteggio dei parametri alla considerazione dei FLOP indica una maturazione nel campo, che riflette una maggiore consapevolezza dei vincoli pratici e delle sfide dell’implementazione di modelli di apprendimento automatico in contesti diversi.\n\n\nEfficienza\nAnche le metriche di efficienza, come il consumo di memoria e la latenza/capacità di elaborazione, hanno acquisito importanza. Queste metriche sono particolarmente cruciali quando si distribuiscono modelli su dispositivi edge o in applicazioni in tempo reale, poiché misurano la velocità con cui un modello può elaborare i dati e la quantità di memoria richiesta. In questo contesto, le curve di Pareto vengono spesso utilizzate per visualizzare il compromesso tra diverse metriche, aiutando le parti interessate a decidere quale modello si adatta meglio alle loro esigenze.\n\n\n\n\n11.5.3 Lezioni Apprese\nIl benchmarking dei modelli ci ha offerto diverse preziose intuizioni che possono essere sfruttate per guidare l’innovazione nei benchmark di sistema. La progressione dei modelli di apprendimento automatico è stata profondamente influenzata dall’avvento delle classifiche e dalla disponibilità open source di modelli e set di dati. Questi elementi hanno svolto il ruolo di catalizzatori significativi, spingendo l’innovazione e accelerando l’integrazione di modelli all’avanguardia negli ambienti di produzione. Tuttavia, come approfondiremo ulteriormente, questi non sono gli unici fattori che contribuiscono allo sviluppo dei benchmark di apprendimento automatico.\nLe classifiche svolgono un ruolo fondamentale nel fornire un metodo oggettivo e trasparente per ricercatori e professionisti per valutare l’efficacia di diversi modelli, classificandoli in base alle loro prestazioni nei benchmark. Questo sistema promuove un ambiente competitivo, incoraggiando lo sviluppo di modelli che non siano solo accurati ma anche efficienti. L’ImageNet Large Scale Visual Recognition Challenge (ILSVRC) ne è un ottimo esempio, con la sua classifica annuale che contribuisce in modo significativo allo sviluppo di modelli innovativi come AlexNet.\nL’accesso open source a modelli e set di dati all’avanguardia diffonde ulteriormente l’apprendimento automatico, facilitando la collaborazione tra ricercatori e professionisti in tutto il mondo. Questo accesso aperto accelera il processo di test, convalida e distribuzione di nuovi modelli in ambienti di produzione, come dimostrato dall’adozione diffusa di modelli come BERT e GPT-3 in varie applicazioni, dall’elaborazione del linguaggio naturale a compiti multimodali più complessi.\nPiattaforme di collaborazione della comunità come Kaggle hanno rivoluzionato il settore ospitando competizioni che uniscono data scientist da tutto il mondo per risolvere problemi intricati. Benchmark specifici fungono da paletti per l’innovazione e lo sviluppo di modelli.\nInoltre, la disponibilità di set di dati diversi e di alta qualità è fondamentale per l’addestramento e il test dei modelli di apprendimento automatico. Set di dati come ImageNet hanno svolto un ruolo fondamentale nell’evoluzione dei modelli di riconoscimento delle immagini, mentre ampi set di dati di testo hanno facilitato i progressi nei modelli di elaborazione del linguaggio naturale.\nInfine, è necessario supportare i contributi di istituti accademici e di ricerca. Il loro ruolo nella pubblicazione di articoli di ricerca, nella condivisione di risultati in conferenze e nella promozione della collaborazione tra varie istituzioni ha contribuito in modo significativo al progresso dei modelli e dei benchmark di apprendimento automatico.\n\nTendenze Emergenti\nMan mano che i modelli di apprendimento automatico diventano più sofisticati, lo diventano anche i benchmark necessari per valutarli in modo accurato. Ci sono diversi benchmark e dataset emergenti che stanno guadagnando popolarità grazie alla loro capacità di valutare i modelli in scenari più complessi e realistici:\nDataset Multimodali: Questi set di dati contengono più tipi di dati, come testo, immagini e audio, per rappresentare meglio le situazioni del mondo reale. Un esempio è VQA (Visual Question Answering) (Antol et al. 2015), in cui viene testata la capacità dei modelli di rispondere a domande basate su testo sulle immagini.\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, e Devi Parikh. 2015. «VQA: Visual Question Answering». In 2015 IEEE International Conference on Computer Vision (ICCV), 2425–33. IEEE. https://doi.org/10.1109/iccv.2015.279.\nValutazione di Correttezza e Bias: C’è una crescente attenzione alla creazione di benchmark che valutino l’equità/Correttezza e i bias [pregiudizi] dei modelli di apprendimento automatico. Esempi includono il toolkit AI Fairness 360, che offre un set completo di metriche e set di dati per valutare il bias nei modelli.\nGeneralizzazione Out-of-Distribution: Test di quanto bene i modelli funzionano su dati diversi dalla distribuzione di training originale. Questo valuta la capacità del modello di generalizzare a dati nuovi e inediti. Esempi di benchmark sono Wilds (Koh et al. 2021), RxRx e ANC-Bench.\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. «WILDS: A Benchmark of in-the-Wild Distribution Shifts». In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:5637–64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, e Dawn Song. 2021. «Natural Adversarial Examples». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15262–71. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, e Quoc V. Le. 2020. «Adversarial Examples Improve Image Recognition». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 816–25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\nRobustezza Avversaria: Valutazione delle prestazioni del modello in caso di attacchi avversari o perturbazioni ai dati di input. Questo testa la robustezza del modello. Esempi di benchmark sono ImageNet-A (Hendrycks et al. 2021), ImageNet-C (Xie et al. 2020) e CIFAR-10.1.\nPrestazioni nel Mondo Reale: Test di modelli su set di dati del mondo reale che corrispondono da vicino alle attività finali anziché solo su set di dati di benchmark predefiniti. Esempi sono set di dati di imaging medico per attività sanitarie o log di chat di assistenza clienti per sistemi di dialogo.\nEfficienza Energetica e di Calcolo: Benchmark che misurano le risorse di calcolo necessarie per ottenere una particolare accuratezza. Questo valuta l’efficienza del modello. Esempi sono MLPerf e Greenbench, già discussi nella sezione Benchmarking dei sistemi.\nInterpretabilità e Spiegabilità: Benchmark che valutano quanto sia facile comprendere e spiegare la logica interna e le previsioni di un modello. Esempi di parametri sono la fedeltà ai gradienti di input e la coerenza delle spiegazioni.\n\n\n\n11.5.4 Limitazioni e Sfide\nSebbene i benchmark dei modelli siano uno strumento essenziale per valutare i modelli di machine learning, è necessario affrontare diverse limitazioni e sfide per garantire che riflettano accuratamente le prestazioni in scenari reali.\nIl dataset non corrisponde a scenari reali: Spesso, i dati utilizzati nei benchmark dei modelli vengono puliti e preelaborati a tal punto che potrebbe essere necessario rappresentare accuratamente i dati che un modello incontrerebbe in applicazioni reali. Questa versione idealizzata dei dati può portare a una sovrastima delle prestazioni di un modello. Nel caso del set di dati ImageNet, le immagini sono ben etichettate e categorizzate. Tuttavia, in uno scenario reale, un modello potrebbe dover gestire immagini sfocate che potrebbero essere meglio illuminate o scattate da angolazioni scomode. Questa discrepanza può influire in modo significativo sulle prestazioni del modello.\nSim2Real Gap: Il Sim2Real Gap si riferisce alla differenza nelle prestazioni di un modello quando si passa da un ambiente simulato a un ambiente reale. Questo gap è spesso osservato nella robotica, dove un robot addestrato in un ambiente simulato ha difficoltà a svolgere compiti nel mondo reale a causa della complessità e dell’imprevedibilità degli ambienti reali. Un robot addestrato a raccogliere oggetti in un ambiente simulato potrebbe aver bisogno di aiuto per svolgere lo stesso compito nel mondo reale perché l’ambiente simulato non rappresenta accuratamente le complessità della fisica, dell’illuminazione e della variabilità degli oggetti del mondo reale.\nSfide nella Creazione di Dataset: La creazione di un set di dati per il benchmarking del modello è un’attività impegnativa che richiede un’attenta considerazione di vari fattori come qualità dei dati, diversità e rappresentazione. Come discusso nella sezione di ingegneria dei dati, garantire che i dati siano puliti, imparziali e rappresentativi dello scenario del mondo reale è fondamentale per l’accuratezza e l’affidabilità del benchmark. Ad esempio, quando si crea un set di dati per un’attività correlata all’assistenza sanitaria, è importante assicurarsi che i dati siano rappresentativi dell’intera popolazione e non distorti verso un particolare gruppo demografico. Ciò garantisce che il modello funzioni bene in diverse popolazioni di pazienti.\nI benchmark del modello sono essenziali per misurare la capacità di un’architettura di modello di risolvere un’attività fissa, ma è importante affrontare le limitazioni e le sfide ad essi associate. Ciò include il garantire che il set di dati rappresenti accuratamente scenari del mondo reale, affrontare il divario Sim2Real e superare le sfide della creazione di set di dati imparziali e rappresentativi. Affrontando queste sfide e molte altre, possiamo garantire che i benchmark del modello forniscano una valutazione più accurata e affidabile delle prestazioni di un modello in applicazioni del mondo reale.\nLo Speech Commands dataset e il suo successore MSWC sono benchmark comuni per una delle applicazioni TinyML per eccellenza, l’individuazione delle parole chiave. I comandi vocali stabiliscono metriche di errore di streaming oltre la precisione di classificazione standard top-1 più pertinenti al caso d’uso di individuazione delle parole chiave. L’utilizzo di metriche pertinenti ai casi è ciò che eleva un dataset a un benchmark del modello.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmarking-dei-dati",
    "href": "contents/benchmarking/benchmarking.it.html#benchmarking-dei-dati",
    "title": "11  Benchmarking AI",
    "section": "11.6 Benchmarking dei Dati",
    "text": "11.6 Benchmarking dei Dati\nNegli ultimi anni, l’intelligenza artificiale si è concentrata sullo sviluppo di modelli di apprendimento automatico sempre più sofisticati, come i grandi modelli linguistici. L’obiettivo è stato quello di creare modelli in grado di prestazioni di livello umano o sovrumane su un’ampia gamma di attività, addestrandoli su enormi set di dati. Questo approccio incentrato sul modello ha prodotto rapidi progressi, con modelli che hanno ottenuto risultati all’avanguardia su molti benchmark consolidati. Figura 11.6 mostra le prestazioni dei sistemi di intelligenza artificiale rispetto alle prestazioni umane (contrassegnate dalla linea orizzontale a 0) in cinque applicazioni: riconoscimento della scrittura a mano, riconoscimento vocale, riconoscimento delle immagini, comprensione della lettura e comprensione del linguaggio. Negli ultimi dieci anni, le prestazioni dell’intelligenza artificiale hanno superato quelle degli esseri umani.\nTuttavia, le crescenti preoccupazioni su questioni come pregiudizi, sicurezza e robustezza persistono anche nei modelli che raggiungono un’elevata accuratezza sui benchmark standard. Inoltre, alcuni set di dati popolari utilizzati per la valutazione dei modelli stanno iniziando a saturarsi, con modelli che raggiungono prestazioni quasi perfette su divisioni di test esistenti (Kiela et al. 2021). Come semplice esempio, ci sono immagini di test nel classico dataset di cifre scritte a mano MNIST che potrebbero sembrare indecifrabili per la maggior parte dei valutatori umani, ma a cui è stata assegnata un’etichetta quando è stato creato il set di dati: i modelli che concordano con quelle etichette potrebbero sembrare esibire prestazioni sovrumane, ma potrebbero invece catturare solo idiosincrasie del processo di etichettatura e acquisizione dalla creazione del set di dati nel 1994. Con lo stesso spirito, i ricercatori di visione artificiale ora chiedono: “Abbiamo finito con ImageNet?” (Beyer et al. 2020). Ciò evidenzia i limiti nell’approccio convenzionale incentrato sul modello di ottimizzazione dell’accuratezza su set di dati fissi tramite innovazioni architettoniche.\n\nBeyer, Lucas, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, e Aäron van den Oord. 2020. «Are we done with imagenet?» ArXiv preprint abs/2006.07159. https://arxiv.org/abs/2006.07159.\n\n\n\n\n\n\nFigura 11.6: IA e prestazioni umane. Fonte: Kiela et al. (2021).\n\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. «Dynabench: Rethinking Benchmarking in NLP». In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4110–24. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\n\nSta emergendo un paradigma alternativo chiamato IA incentrata sui dati. Invece di trattare i dati come statici e concentrarsi strettamente sulle prestazioni del modello, questo approccio riconosce che i modelli sono validi solo quanto i loro dati di training. Quindi, l’enfasi si sposta sulla cura di dataset di alta qualità che riflettano meglio la complessità del mondo reale, sviluppando benchmark di valutazione più informativi e considerando attentamente come i dati vengono campionati, preelaborati e aumentati. L’obiettivo è ottimizzare il comportamento del modello migliorando i dati anziché semplicemente ottimizzando le metriche su set di dati imperfetti. L’intelligenza artificiale incentrata sui dati esamina e migliora criticamente i dati stessi per produrre un’intelligenza artificiale utile. Ciò riflette un’importante evoluzione nella mentalità, poiché il campo affronta le carenze di un benchmarking ristretto.\nQuesta sezione esplorerà le principali differenze tra gli approcci all’intelligenza artificiale incentrati sui modelli e sui dati. Questa distinzione ha importanti implicazioni sul modo in cui eseguiamo il benchmarking dei sistemi di intelligenza artificiale. In particolare, vedremo come concentrarsi sulla qualità dei dati e sull’efficienza può migliorare direttamente le prestazioni dell’apprendimento automatico come alternativa all’ottimizzazione delle sole architetture dei modelli. L’approccio incentrato sui dati riconosce che i modelli sono validi solo quanto i loro dati di addestramento. Quindi, migliorare la cura dei dati, i benchmark di valutazione e i processi di gestione dei dati può produrre sistemi di intelligenza artificiale più sicuri, più equi e più robusti. Ripensare il benchmarking per dare priorità ai dati insieme ai modelli rappresenta un’importante evoluzione, poiché il settore si sforza di fornire un impatto affidabile nel mondo reale.\n\n11.6.1 Limitazioni dell’IA Incentrata sul Modello\nNell’era dell’IA incentrata sul modello, una caratteristica importante era lo sviluppo di architetture di modelli complesse. Ricercatori e professionisti hanno dedicato notevoli sforzi alla progettazione di modelli sofisticati e intricati nella ricerca di prestazioni superiori. Ciò ha spesso comportato l’incorporazione di livelli aggiuntivi e la messa a punto di una moltitudine di iperparametri per ottenere miglioramenti nell’accuratezza. Contemporaneamente, c’era una notevole enfasi sullo sfruttamento di algoritmi avanzati. Questi algoritmi, spesso in prima linea nelle ultime ricerche, sono stati impiegati per migliorare le prestazioni dei modelli di IA. L’obiettivo principale di questi algoritmi era ottimizzare il processo di apprendimento dei modelli, estraendo così il massimo delle informazioni dai dati di addestramento.\nSebbene l’approccio incentrato sul modello sia stato centrale per molti progressi nell’IA, ha diverse aree di miglioramento. Innanzitutto, lo sviluppo di architetture di modelli complesse può spesso portare a un overfitting. Questo è quando il modello funziona bene sui dati di addestramento ma deve generalizzare a nuovi dati mai visti. I layer aggiuntivi e la complessità possono catturare il rumore nei dati di training come se fosse un pattern reale, danneggiando le prestazioni del modello su nuovi dati.\nIn secondo luogo, affidarsi ad algoritmi avanzati può a volte oscurare la reale comprensione del funzionamento di un modello. Questi algoritmi spesso agiscono come una scatola nera, rendendo difficile interpretare il modo in cui il modello prende decisioni. Questa mancanza di trasparenza può essere un ostacolo significativo, specialmente in applicazioni critiche come sanità e finanza, dove la comprensione del processo decisionale del modello è fondamentale.\nIn terzo luogo, l’enfasi sul raggiungimento di risultati all’avanguardia su set di dati di riferimento può a volte essere fuorviante. Questi dataset devono rappresentare in modo più completo le complessità e la variabilità dei dati del mondo reale. Un modello che funziona bene su un set di dati di riferimento potrebbe non essere necessariamente generalizzato bene a dati nuovi e mai visti in un’applicazione del mondo reale. Questa discrepanza può portare a una falsa fiducia nelle capacità del modello e ostacolarne l’applicabilità pratica.\nInfine, l’approccio incentrato sul modello spesso si basa su grandi set di dati etichettati per l’addestramento. Tuttavia, ottenere tali set di dati richiede tempo e impegno in molti scenari del mondo reale. Questa dipendenza da grandi dataset limita anche l’applicabilità dell’IA in domini in cui i dati sono scarsi o costosi da etichettare.\nCome risultato delle ragioni di cui sopra, e di molte altre, la comunità dell’IA sta passando a un approccio più incentrato sui dati. Invece di concentrarsi solo sull’architettura del modello, i ricercatori stanno ora dando priorità alla cura di set di dati di alta qualità, allo sviluppo di migliori benchmark di valutazione e alla considerazione di come i dati vengono campionati e preelaborati. L’idea chiave è che i modelli sono validi solo quanto i loro dati di training. Quindi, concentrandoci sull’ottenimento dei dati giusti, potremo sviluppare sistemi di intelligenza artificiale più equi, sicuri e allineati con i valori umani. Questo cambiamento incentrato sui dati rappresenta un importante cambiamento di mentalità man mano che l’intelligenza artificiale progredisce.\n\n\n11.6.2 Verso un’Intelligenza Artificiale Incentrata sui Dati\nL’intelligenza artificiale incentrata sui dati è un paradigma che sottolinea l’importanza di dataset di alta qualità, ben etichettati e diversificati nello sviluppo di modelli di intelligenza artificiale. Contrariamente all’approccio incentrato sul modello, che si concentra sulla rifinitura e l’iterazione dell’architettura e dell’algoritmo del modello per migliorare le prestazioni, l’intelligenza artificiale incentrata sui dati dà priorità alla qualità dei dati di input come motore principale per migliorare le prestazioni del modello. I dati di alta qualità sono puliti, ben etichettati e rappresentativi degli scenari del mondo reale che il modello incontrerà. Al contrario, i dati di bassa qualità possono portare a scarse prestazioni del modello, indipendentemente dalla complessità o dalla sofisticatezza dell’architettura del modello.\nL’intelligenza artificiale incentrata sui dati pone una forte enfasi sulla pulizia e l’etichettatura dei dati. La pulizia comporta la rimozione di valori anomali, la gestione dei valori mancanti e la risoluzione di altre incongruenze nei dati. L’etichettatura, d’altro canto, comporta l’assegnazione di etichette significative e accurate ai dati. Entrambi questi processi sono fondamentali per garantire che il modello di intelligenza artificiale venga addestrato su dati accurati e pertinenti. Un altro aspetto importante dell’approccio incentrato sui dati è il “data augmentation” [l’aumento dei dati]. Ciò comporta l’aumento artificiale delle dimensioni e della diversità del set di dati applicando varie trasformazioni ai dati, come rotazione, ridimensionamento e capovolgimento delle immagini di addestramento. L’aumento dei dati aiuta a migliorare la robustezza del modello e le capacità di generalizzazione.\nCi sono diversi vantaggi nell’adottare un approccio incentrato sui dati per lo sviluppo dell’intelligenza artificiale. Innanzitutto, porta a prestazioni del modello migliorate e capacità di generalizzazione. Assicurandosi che il modello venga addestrato su dati diversi e di alta qualità, il modello può generalizzare meglio a dati nuovi e mai visti (Mattson et al. 2020b).\nInoltre, un approccio incentrato sui dati può spesso portare a modelli più semplici che sono più facili da interpretare e gestire. Questo perché l’enfasi è sui dati piuttosto che sull’architettura del modello, il che significa che i modelli più semplici possono raggiungere prestazioni elevate quando addestrati su dati di alta qualità.\nIl passaggio all’IA incentrata sui dati rappresenta un significativo cambiamento di paradigma. Dando priorità alla qualità dei dati di input, questo approccio cerca di modellare le prestazioni e le capacità di generalizzazione, portando in ultima analisi a sistemi di intelligenza artificiale più solidi e affidabili. Mentre continuiamo ad avanzare nella nostra comprensione e applicazione dell’IA, è probabile che l’approccio incentrato sui dati svolga un ruolo importante nel plasmare il futuro di questo campo.\n\n\n11.6.3 Benchmarking dei Dati\nIl benchmarking dei dati mira a valutare problemi comuni nei set di dati, come l’identificazione di errori di etichetta, caratteristiche rumorose, squilibrio di rappresentazione (ad esempio, su 1000 classi in Imagenet-1K, ci sono oltre 100 categorie che sono solo tipi di cani), squilibrio di classe (dove alcune classi hanno molti più campioni di altre), se i modelli addestrati su un dato set di dati possono generalizzare a caratteristiche fuori distribuzione o quali tipi di bias potrebbero esistere in un dato set di dati (Mattson et al. 2020b). Nella sua forma più semplice, il benchmarking dei dati mira a migliorare l’accuratezza su un set di test rimuovendo campioni di addestramento rumorosi o etichettati in modo errato mantenendo fissa l’architettura del modello. Recenti competizioni nel benchmarking dei dati hanno invitato i partecipanti a presentare nuove strategie di “augmentation” e tecniche di apprendimento attivo.\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020b. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\nLe tecniche incentrate sui dati continuano a guadagnare attenzione nel benchmarking, soprattutto perché i modelli di base sono sempre più addestrati su obiettivi auto-supervisionati. Rispetto ai set di dati più piccoli come Imagenet-1K, i set di dati più grandi comunemente usati nell’apprendimento auto-supervisionato, come Common Crawl, OpenImages e LAION-5B, contengono quantità maggiori di rumore, duplicati, bias e dati potenzialmente offensivi.\nDataComp è una competizione di dataset lanciata di recente che ha come obiettivo la valutazione di grandi corpora. DataComp si concentra sulle coppie linguaggio-immagine usate per addestrare i modelli CLIP. Il documento introduttivo rileva che quando il budget di elaborazione totale per l’addestramento è costante, i modelli CLIP più performanti nelle attività downstream, come la classificazione ImageNet, vengono addestrati solo sul 30% del pool di campioni disponibile. Ciò suggerisce che un corretto filtraggio di grandi corpora è fondamentale per migliorare l’accuratezza dei modelli di base. Analogamente, Demystifying CLIP Data (Xu et al. 2023) chiede se il successo di CLIP sia attribuibile all’architettura o al set di dati.\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, e Christoph Feichtenhofer. 2023. «Demystifying CLIP Data». ArXiv preprint abs/2309.16671. https://arxiv.org/abs/2309.16671.\nDataPerf è un altro recente lavoro incentrato sul benchmarking dei dati in varie modalità. DataPerf offre round di competizione online per stimolare il miglioramento dei dataset. L’offerta inaugurale è stata lanciata con sfide in termini di visione, parlato, acquisizione, debug e prompt di testo per la generazione di immagini.\n\n\n11.6.4 Efficienza dei Dati\nMan mano che i modelli di apprendimento automatico diventano più grandi e complessi e le risorse di elaborazione diventano più scarse di fronte alla crescente domanda, diventa difficile soddisfare i requisiti di elaborazione anche con le flotte di machine learning più grandi. Per superare queste sfide e garantire la scalabilità del sistema di apprendimento automatico, è necessario esplorare nuove opportunità che aumentino gli approcci convenzionali alla scalabilità delle risorse.\nMigliorare la qualità dei dati può essere un metodo utile per avere un impatto significativo sulle prestazioni del sistema di apprendimento automatico. Uno dei principali vantaggi del miglioramento della qualità dei dati è il potenziale di poter ridurre le dimensioni del set di dati di addestramento mantenendo o addirittura migliorando le prestazioni del modello. Questa riduzione delle dimensioni dei dati è direttamente correlata alla quantità di tempo di addestramento richiesto, consentendo così ai modelli di convergere in modo più rapido ed efficiente. Raggiungere questo equilibrio tra qualità dei dati e dimensioni del set di dati è un compito impegnativo che richiede lo sviluppo di metodi, algoritmi e tecniche sofisticati.\nPossono essere adottati diversi approcci per migliorare la qualità dei dati. Questi metodi includono e non sono limitati a quanto segue:\n\nPulizia dei Dati: Ciò comporta la gestione dei valori mancanti, la correzione degli errori e la rimozione dei valori anomali. I dati puliti assicurano che il modello non stia imparando da rumore o imprecisioni.\nInterpretabilità e Spiegabilità dei Dati: Le tecniche comuni includono LIME (Ribeiro, Singh, e Guestrin 2016), che fornisce informazioni sui limiti decisionali dei classificatori, e valori Shapley (Lundberg e Lee 2017), che stimano l’importanza dei singoli campioni nel contribuire alle previsioni di un modello.\nFeature Engineering: Trasformare o creare nuove funzionalità può migliorare significativamente le prestazioni del modello fornendo informazioni più pertinenti per l’apprendimento.\nData Augmentation: Aumentare i dati creando nuovi campioni tramite varie trasformazioni può aiutare a migliorare la robustezza e la generalizzazione del modello.\nActive Learning: Questo è un approccio di apprendimento semi-supervisionato in cui il modello interroga attivamente un “oracolo” umano per etichettare i campioni più informativi (Coleman et al. 2022). Ciò garantisce che il modello venga addestrato sui dati più rilevanti.\nRiduzione della Dimensionalità: Tecniche come PCA possono ridurre il numero di feature in un set di dati, riducendo così la complessità e il tempo di addestramento.\n\n\nRibeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. «” Why should i trust you?” Explaining the predictions of any classifier». In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135–44.\n\nLundberg, Scott M., e Su-In Lee. 2017. «A Unified Approach to Interpreting Model Predictions». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert D. Nowak, Roshan Sumbaly, Matei Zaharia, e I. Zeki Yalniz. 2022. «Similarity Search for Efficient Active Learning and Search of Rare Concepts». In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, 6402–10. AAAI Press. https://ojs.aaai.org/index.php/AAAI/article/view/20591.\nEsistono molti altri metodi in circolazione. Ma l’obiettivo è lo stesso. Affinare il set di dati e garantire che sia della massima qualità può ridurre il tempo di addestramento necessario per la convergenza dei modelli. Tuttavia, per raggiungere questo obiettivo è necessario sviluppare e implementare metodi, algoritmi e tecniche sofisticati in grado di pulire, preelaborare e aumentare i dati, mantenendo al contempo i campioni più informativi. Questa è una sfida continua che richiederà una continua ricerca e innovazione nel campo dell’apprendimento automatico.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#la-tripletta",
    "href": "contents/benchmarking/benchmarking.it.html#la-tripletta",
    "title": "11  Benchmarking AI",
    "section": "11.7 La Tripletta",
    "text": "11.7 La Tripletta\nMentre i benchmark di sistema, modello e dati sono stati tradizionalmente studiati in modo isolato, si sta diffondendo la consapevolezza che per comprendere e far progredire completamente l’IA, dobbiamo adottare una visione più olistica. Iterando tra sistemi di benchmarking, modelli e dataset insieme, potrebbero emergere nuove intuizioni che non sono evidenti quando questi componenti vengono analizzati separatamente. Le prestazioni del sistema influiscono sulla precisione del modello, le capacità del modello determinano le esigenze dei dati e le caratteristiche dei dati determinano i requisiti del sistema.\nIl benchmarking della triade di sistema, modello e dati in modo integrato porterà probabilmente a scoperte sulla progettazione congiunta dei sistemi di IA, sulle proprietà di generalizzazione dei modelli e sul ruolo della cura e della qualità dei dati nel consentire le prestazioni. Piuttosto che benchmark ristretti di singoli componenti, il futuro dell’IA richiede benchmark che valutino la relazione simbiotica tra piattaforme di elaborazione, algoritmi e dati di training. Questa prospettiva a livello di sistema sarà fondamentale per superare le attuali limitazioni e sbloccare il prossimo livello di capacità dell’IA.\nFigura 11.7 illustra i molti modi potenziali per far interagire tra loro il benchmarking dei dati, quello dei modelli e quello dell’infrastruttura di sistema. L’esplorazione di queste complesse interazioni probabilmente porterà alla scoperta di nuove opportunità di ottimizzazione e capacità di miglioramento. La tripletta di benchmark di dati, modelli e sistemi offre un ricco spazio per la progettazione congiunta e la co-ottimizzazione.\n\n\n\n\n\n\nFigura 11.7: La tripletta del Benchmarking.\n\n\n\nSebbene questa prospettiva integrata rappresenti una tendenza emergente, il settore ha ancora molto da scoprire sulle sinergie e i compromessi tra questi componenti. Mentre eseguiamo il benchmarking iterativo di combinazioni di dati, modelli e sistemi, emergeranno nuove intuizioni che rimangono nascoste quando questi elementi vengono studiati separatamente. Questo approccio di benchmarking multiforme che traccia le intersezioni di dati, algoritmi e hardware promette di essere una strada fruttuosa per importanti progressi nell’intelligenza artificiale, anche se è ancora nelle sue fasi iniziali.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmark-per-tecnologie-emergenti",
    "href": "contents/benchmarking/benchmarking.it.html#benchmark-per-tecnologie-emergenti",
    "title": "11  Benchmarking AI",
    "section": "11.8 Benchmark per Tecnologie Emergenti",
    "text": "11.8 Benchmark per Tecnologie Emergenti\nDate le loro significative differenze rispetto alle tecniche esistenti, le tecnologie emergenti possono essere particolarmente difficili da progettare per i benchmark. I benchmark standard utilizzati per le tecnologie esistenti potrebbero non evidenziare le caratteristiche chiave del nuovo approccio. Al contrario, i nuovi benchmark potrebbero essere visti come artificiosi per favorire la tecnologia emergente rispetto ad altre. Potrebbero essere così diversi dai benchmark esistenti da non poter essere compresi e perdere significato. Pertanto, i benchmark per le tecnologie emergenti devono bilanciare equità, applicabilità e facilità di confronto con quelli esistenti.\nUn esempio di tecnologia emergente in cui il benchmarking si è dimostrato particolarmente difficile è nel Neuromorphic Computing. Utilizzando il cervello come fonte di ispirazione per un’intelligenza generale scalabile, robusta ed efficiente dal punto di vista energetico, il calcolo neuromorfico (Schuman et al. 2022) incorpora direttamente meccanismi biologicamente realistici sia negli algoritmi di calcolo che nell’hardware, come le reti neurali spiking (Maass 1997) e le architetture non-von Neumann architectures per eseguirle (Davies et al. 2018; Modha et al. 2023). Da una prospettiva full-stack di modelli, tecniche di training e sistemi hardware, il calcolo neuromorfico differisce dall’hardware e dall’intelligenza artificiale convenzionali. Pertanto, esiste una sfida fondamentale nello sviluppo di benchmark equi e utili per guidare la tecnologia.\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. «Opportunities for neuromorphic computing algorithms and applications». Nature Computational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\nMaass, Wolfgang. 1997. «Networks of spiking neurons: The third generation of neural network models». Neural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. «Loihi: A Neuromorphic Manycore Processor with On-Chip Learning». IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. «Neural inference at the frontier of energy, space, and time». Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\nYik, Jason, Soikat Hasan Ahmed, Zergham Ahmed, Brian Anderson, Andreas G. Andreou, Chiara Bartolozzi, Arindam Basu, et al. 2023. «NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking». https://arxiv.org/abs/2304.04640.\nUn’iniziativa in corso per sviluppare benchmark neuromorfici standard è NeuroBench (Yik et al. 2023). Per un benchmarking adeguato del neuromorfico, NeuroBench segue principi di alto livello di inclusività attraverso l’applicabilità di attività e metriche sia alle soluzioni neuromorfiche che non neuromorfiche, attuabilità dell’implementazione utilizzando strumenti comuni e aggiornamenti iterativi per continuare a garantire la pertinenza man mano che il campo cresce rapidamente. NeuroBench e altri benchmark per le tecnologie emergenti forniscono una guida critica per le tecniche future, che potrebbero essere necessarie man mano che i limiti di scalabilità degli approcci esistenti si avvicinano.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#conclusione",
    "href": "contents/benchmarking/benchmarking.it.html#conclusione",
    "title": "11  Benchmarking AI",
    "section": "11.9 Conclusione",
    "text": "11.9 Conclusione\nCiò che viene misurato viene migliorato. Questo capitolo ha esplorato la natura multiforme del benchmarking che abbraccia sistemi, modelli e dati. Il benchmarking è importante per far progredire l’IA in quanto fornisce le misurazioni essenziali per monitorare i progressi.\nI benchmark del sistema ML consentono l’ottimizzazione attraverso metriche di velocità, efficienza e scalabilità. I benchmark del modello guidano l’innovazione attraverso attività e metriche standardizzate oltre l’accuratezza. I benchmark dei dati evidenziano problemi di qualità, equilibrio e rappresentazione.\nÈ importante notare che la valutazione di questi componenti in modo isolato presenta dei limiti. In futuro, sarà probabilmente utilizzato un benchmarking più integrato per esplorare l’interazione tra benchmark di sistema, modello e dati. Questa visione promette nuove intuizioni sulla progettazione congiunta di dati, algoritmi e infrastrutture.\nMan mano che l’IA diventa più complessa, il benchmarking completo diventa ancora più critico. Gli standard devono evolversi continuamente per misurare nuove capacità e rivelare limitazioni. Una stretta collaborazione tra settore, mondo accademico, etichette nazionali, ecc. è essenziale per sviluppare benchmark rigorosi, trasparenti e socialmente utili.\nIl benchmarking fornisce la bussola per guidare il progresso nell’IA. Misurando costantemente e condividendo apertamente i risultati, possiamo orientarci verso sistemi performanti, robusti e affidabili. Se l’IA deve soddisfare adeguatamente le esigenze sociali e umane, deve essere sottoposta a benchmarking tenendo a mente gli interessi dell’umanità. A tal fine, ci sono aree emergenti, come il benchmarking della sicurezza dei sistemi di IA, ma questo è per un altro giorno e qualcosa di cui possiamo discutere ulteriormente in “Generative AI”!\nIl benchmarking è un argomento in continua evoluzione. L’articolo The Olympics of AI: Benchmarking Machine Learning Systems copre diversi sottocampi emergenti nel benchmarking dell’IA, tra cui robotica, realtà estesa e calcolo neuromorfico che incoraggiamo il lettore ad approfondire.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#sec-benchmarking-ai-resource",
    "href": "contents/benchmarking/benchmarking.it.html#sec-benchmarking-ai-resource",
    "title": "11  Benchmarking AI",
    "section": "11.10 Risorse",
    "text": "11.10 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nPerché il benchmarking è importante?\nBenchmarking di inferenza embedded.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 11.1\nEsercizio 11.2\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html",
    "href": "contents/ondevice_learning/ondevice_learning.it.html",
    "title": "12  Apprendimento On-Device",
    "section": "",
    "text": "12.1 Introduzione\nL’apprendimento su dispositivo si riferisce all’addestramento di modelli ML direttamente sul dispositivo in cui vengono distribuiti, al contrario dei metodi tradizionali in cui i modelli vengono addestrati su server potenti e poi distribuiti sui dispositivi. Questo metodo è particolarmente rilevante per TinyML, in cui i sistemi ML sono integrati in dispositivi minuscoli e con risorse limitate.\nUn esempio di apprendimento su dispositivo può essere visto in un termostato intelligente che si adatta al comportamento dell’utente nel tempo. Inizialmente, il termostato può avere un modello generico che comprende modelli di utilizzo di base. Tuttavia, poiché è esposto a più dati, come gli orari in cui l’utente è a casa o fuori, le temperature preferite e le condizioni meteorologiche esterne, il termostato può perfezionare il suo modello direttamente sul dispositivo per fornire un’esperienza personalizzata. Tutto ciò avviene senza inviare dati a un server centrale per l’elaborazione.\nUn altro esempio è nel testo predittivo sugli smartphone. Mentre gli utenti digitano, il telefono impara dai modelli linguistici dell’utente e suggerisce parole o frasi che probabilmente verranno utilizzate in seguito. Questo apprendimento avviene direttamente sul dispositivo e il modello si aggiorna in tempo reale man mano che vengono raccolti più dati. Un esempio pratico di apprendimento su dispositivo ampiamente utilizzato è Gboard. Su un telefono Android, Gboard impara da modelli di digitazione e dettatura per migliorare l’esperienza per tutti gli utenti. L’apprendimento “On-device” è anche chiamato “apprendimento federato”. Figura 12.1 mostra il ciclo di apprendimento federato sui dispositivi mobili: A. il dispositivo impara dai modelli utente; B. gli aggiornamenti del modello locale vengono comunicati al cloud; C. il server cloud aggiorna il modello globale e invia il nuovo modello a tutti i dispositivi.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#introduzione",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#introduzione",
    "title": "12  Apprendimento On-Device",
    "section": "",
    "text": "Figura 12.1: Ciclo di apprendimento federato. Fonte: Google Research.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#vantaggi-e-limiti",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#vantaggi-e-limiti",
    "title": "12  Apprendimento On-Device",
    "section": "12.2 Vantaggi e Limiti",
    "text": "12.2 Vantaggi e Limiti\nL’apprendimento su dispositivo offre diversi vantaggi rispetto al tradizionale ML basato su cloud. Mantenendo dati e modelli sul dispositivo, elimina la necessità di costose trasmissioni di dati e risolve i problemi di privacy. Ciò consente esperienze più personalizzate e reattive, poiché il modello può adattarsi in tempo reale al comportamento dell’utente.\nTuttavia, l’apprendimento su dispositivo presenta anche dei compromessi. Le limitate risorse di elaborazione sui dispositivi dei consumatori possono rendere difficile l’esecuzione di modelli complessi in locale. Anche i set di dati sono più limitati poiché sono costituiti solo da dati generati dall’utente da un singolo dispositivo. Inoltre, l’aggiornamento dei modelli richiede l’invio di nuove versioni anziché aggiornamenti cloud senza interruzioni.\nL’apprendimento su dispositivo apre nuove possibilità abilitando l’intelligenza artificiale offline mantenendo al contempo la privacy dell’utente. Tuttavia, richiede una gestione attenta della complessità dei modelli e dei dati entro i limiti dei dispositivi dei consumatori. Trovare il giusto equilibrio tra localizzazione e offload dal cloud è fondamentale per ottimizzare le esperienze su dispositivo.\n\n12.2.1 Vantaggi\n\nPrivacy e Sicurezza dei Dati\nUno dei vantaggi significativi dell’apprendimento sul dispositivo è la maggiore privacy e sicurezza dei dati degli utenti. Ad esempio, si consideri uno smartwatch che monitora parametri sanitari sensibili come la frequenza cardiaca e la pressione sanguigna. Elaborando i dati e adattando i modelli direttamente sul dispositivo, i dati biometrici rimangono localizzati, aggirando la necessità di trasmettere dati grezzi ai server cloud dove potrebbero essere soggetti a violazioni.\nLe violazioni dei server sono tutt’altro che rare, con milioni di record compromessi ogni anno. Ad esempio, la violazione di Equifax del 2017 ha esposto i dati personali di 147 milioni di persone. Mantenendo i dati sul dispositivo, il rischio di tali esposizioni è drasticamente ridotto. L’apprendimento sul dispositivo elimina la dipendenza dall’archiviazione cloud centralizzata e protegge dall’accesso non autorizzato da varie minacce, tra cui attori malintenzionati, minacce interne ed esposizione accidentale.\nRegolamenti come l’Health Insurance Portability and Accountability Act (HIPAA) e il General Data Protection Regulation (GDPR) impongono rigorosi requisiti di riservatezza dei dati che l’apprendimento sul dispositivo affronta abilmente. Garantendo che i dati rimangano localizzati e non vengano trasferiti ad altri sistemi, l’apprendimento sul dispositivo facilita la conformità a tali regolamenti.\nL’apprendimento sul dispositivo non è solo vantaggioso per i singoli utenti; ha implicazioni significative per le organizzazioni e i settori che gestiscono dati altamente sensibili. Ad esempio, in ambito militare, l’apprendimento sul dispositivo consente ai sistemi di prima linea di adattare modelli e funzioni indipendentemente dalle connessioni ai server centrali che potrebbero essere potenzialmente compromessi. Le informazioni critiche e sensibili sono saldamente protette dalla localizzazione dell’elaborazione e dell’apprendimento dei dati. Tuttavia, ciò comporta il compromesso che i singoli dispositivi assumono più valore e possono incentivare furti o distruzioni poiché diventano gli unici vettori di modelli di intelligenza artificiale specializzati. È necessario prestare attenzione alla protezione dei dispositivi stessi durante la transizione all’apprendimento sul dispositivo.\nÈ inoltre importante preservare la privacy, la sicurezza e la conformità normativa dei dati personali e sensibili. Invece che nel cloud, i modelli di training e operativi aumentano sostanzialmente le misure di privacy a livello locale, assicurando che i dati degli utenti siano protetti da potenziali minacce.\nTuttavia, questo è solo parzialmente intuitivo perché l’apprendimento sul dispositivo potrebbe invece esporre i sistemi a nuovi attacchi alla privacy. Con preziosi riepiloghi dei dati e aggiornamenti dei modelli archiviati in modo permanente su singoli dispositivi, potrebbe essere molto più difficile proteggerli fisicamente e digitalmente rispetto a un grande cluster di elaborazione. Mentre l’apprendimento sul dispositivo riduce la quantità di dati compromessi in una qualsiasi violazione, potrebbe anche introdurre nuovi pericoli disperdendo informazioni sensibili su molti terminali decentralizzati. Le pratiche di sicurezza attente sono ancora essenziali per i sistemi “on-device”.\n\n\nNormativa di Conformità\nL’apprendimento sul dispositivo aiuta ad affrontare le principali normative sulla privacy come GDPR)e CCPA. Queste normative richiedono la localizzazione dei dati, limitando i trasferimenti di dati transfrontalieri a paesi approvati con controlli adeguati. Il GDPR impone inoltre requisiti di “privacy by design” e consenso per la raccolta dei dati. Mantenendo l’elaborazione dei dati e il training del modello localizzati sul dispositivo, i dati sensibili degli utenti non vengono trasferiti altrove. Ciò evita importanti grattacapi di conformità per le organizzazioni.\nAd esempio, un fornitore di servizi sanitari che monitora i parametri vitali dei pazienti con dispositivi indossabili deve garantire che i trasferimenti di dati transfrontalieri siano conformi a HIPAA e GDPR se utilizza il cloud. Determinare le leggi del paese applicabili e garantire le approvazioni per i flussi di dati internazionali introduce oneri legali e ingegneristici. Con l’apprendimento on-device, nessun dato lascia il dispositivo, semplificando la conformità. Il tempo e le risorse spesi per la conformità vengono ridotti in modo significativo.\nSettori come sanità, finanza e governo, che hanno dati altamente regolamentati, possono trarre grandi vantaggi dal training sul dispositivo. Localizzando i dati e l’apprendimento, i requisiti normativi di privacy e sovranità dei dati vengono soddisfatti più facilmente. Le soluzioni su dispositivo forniscono un modo efficiente per creare applicazioni di IA conformi.\nLe principali normative sulla privacy impongono restrizioni sullo spostamento transfrontaliero dei dati che l’apprendimento su dispositivo affronta intrinsecamente tramite elaborazione localizzata. Ciò riduce l’onere di conformità per le organizzazioni che lavorano con dati regolamentati.\n\n\nRiduzione della Larghezza di Banda, dei Costi e Maggiore Efficienza\nUno dei principali vantaggi dell’apprendimento su dispositivo è la significativa riduzione dell’utilizzo della larghezza di banda e dei costi associati all’infrastruttura cloud. Mantenendo i dati localizzati per l’addestramento del modello anziché trasmettere dati grezzi al cloud, l’apprendimento su dispositivo può comportare notevoli risparmi di larghezza di banda. Ad esempio, una rete di telecamere che analizzano i filmati video può ottenere significative riduzioni nel trasferimento di dati addestrando i modelli sul dispositivo anziché trasmettere in streaming tutti i filmati video al cloud per l’elaborazione.\nQuesta riduzione nella trasmissione dei dati consente di risparmiare larghezza di banda e si traduce in costi inferiori per server, reti e archiviazione dei dati nel cloud. Le grandi organizzazioni, che potrebbero spendere milioni in infrastrutture cloud per addestrare modelli di dati sui dispositivi, possono sperimentare drastiche riduzioni dei costi tramite l’apprendimento on-device. Nell’era dell’intelligenza artificiale generativa, in cui i costi sono aumentati in modo significativo, trovare modi per contenere le spese è diventato sempre più importante.\nInoltre, anche i costi energetici e ambientali della gestione di grandi server farm sono diminuiti. I data center consumano grandi quantità di energia, contribuendo alle emissioni di gas serra. Riducendo la necessità di un’ampia infrastruttura basata su cloud, l’apprendimento sui dispositivi contribuisce a mitigare l’impatto ambientale dell’elaborazione dei dati (Wu et al. 2022).\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. «Sustainable ai: Environmental implications, challenges and opportunities». Proceedings of Machine Learning and Systems 4: 795–813.\nSpecificamente per le applicazioni endpoint [finali], l’apprendimento sui dispositivi riduce al minimo il numero di chiamate API di rete necessarie per eseguire l’inferenza tramite un provider cloud. I costi cumulativi associati alla larghezza di banda e alle chiamate API possono aumentare rapidamente per le applicazioni con milioni di utenti. Al contrario, eseguire training e inferenze localmente è notevolmente più efficiente e conveniente. Con ottimizzazioni all’avanguardia, è stato dimostrato che l’apprendimento on-device riduce i requisiti di memoria del training, migliora drasticamente l’efficienza della memoria e riduce fino al 20% la latenza per iterazione (Dhar et al. 2021).\nUn altro vantaggio fondamentale dell’apprendimento sul dispositivo è la possibilità per i dispositivi IoT di adattare continuamente il loro modello ML a nuovi dati per un apprendimento continuo e permanente. I modelli sul dispositivo possono rapidamente diventare obsoleti man mano che il comportamento dell’utente, i modelli di dati e le preferenze cambiano. L’apprendimento continuo consente al modello di adattarsi in modo efficiente a nuovi dati e miglioramenti e di mantenere elevate prestazioni del modello nel tempo.\n\n\n\n12.2.2 Limitazioni\nMentre i tradizionali sistemi ML basati su cloud hanno accesso a risorse di elaborazione pressoché infinite, l’apprendimento sul dispositivo è spesso limitato nella potenza di elaborazione e di archiviazione del dispositivo edge su cui viene addestrato il modello. Per definizione, un dispositivo edge è un dispositivo con risorse di elaborazione, memoria ed energia limitate che non possono essere facilmente aumentate o diminuite. Pertanto, la dipendenza dai dispositivi edge può limitare la complessità, l’efficienza e le dimensioni dei modelli ML sul dispositivo.\n\nRisorse di elaborazione\nI tradizionali sistemi ML basati su cloud utilizzano grandi server con più GPU o TPU di fascia alta, che forniscono una potenza di calcolo e una memoria pressoché infinite. Ad esempio, servizi come Amazon Web Services (AWS) EC2 consentono di configurare cluster di istanze GPU per un training parallelo massiccio.\nAl contrario, l’apprendimento sul dispositivo è limitato dall’hardware del dispositivo edge su cui viene eseguito. I dispositivi edge si riferiscono a endpoint come smartphone, elettronica embedded e dispositivi IoT. Per definizione, questi dispositivi hanno risorse di elaborazione, memoria ed energia molto limitate rispetto al cloud.\nAd esempio, uno smartphone tipico o Raspberry Pi può avere solo pochi core CPU, pochi GB di RAM e una piccola batteria. Ancora più limitati in termini di risorse sono i dispositivi microcontrollore TinyML come Arduino Nano BLE Sense. Le risorse sono fisse su questi dispositivi e non possono essere facilmente aumentate su richiesta, come il ridimensionamento dell’infrastruttura cloud. Questa dipendenza dai dispositivi edge limita direttamente la complessità, l’efficienza e le dimensioni dei modelli che possono essere distribuiti per l’addestramento sul dispositivo:\n\nComplessità: I limiti di memoria, elaborazione e potenza limitano la progettazione dell’architettura del modello, così come il numero di layer e dei parametri.\nEfficienza: I modelli devono essere fortemente ottimizzati tramite metodi come la quantizzazione e la potatura per essere eseguiti più velocemente e consumare meno energia.\nDimensioni: I file del modello effettivo devono essere compressi il più possibile per rientrare nei limiti di archiviazione dei dispositivi edge.\n\nPertanto, mentre il cloud offre una scalabilità infinita, l’apprendimento sul dispositivo deve operare entro i rigidi vincoli di risorse dell’hardware. Ciò richiede un’attenta progettazione congiunta di modelli semplificati, metodi di addestramento e ottimizzazioni su misura specificamente per i dispositivi edge.\n\n\nDimensioni, Accuratezza e Generalizzazione del Dataset\nOltre alle risorse di elaborazione limitate, l’apprendimento sul dispositivo è anche limitato dal set di dati disponibile per i modelli di training.\nNel cloud, i modelli vengono addestrati su dataset enormi e diversi come ImageNet o Common Crawl. Ad esempio, ImageNet contiene oltre 14 milioni di immagini attentamente categorizzate in migliaia di classi.\nL’apprendimento sul dispositivo si basa invece su “data silos” più piccoli e decentralizzati, unici per ogni dispositivo. Il rullino fotografico di uno smartphone potrebbe contenere solo migliaia di foto degli interessi e degli ambienti degli utenti.\nQuesti dati decentralizzati portano alla necessità di dati IID (indipendenti e distribuiti in modo identico). Ad esempio, due amici potrebbero scattare molte foto degli stessi luoghi e oggetti, il che significa che le loro distribuzioni di dati sono altamente correlate piuttosto che indipendenti.\nMotivi per cui i dati potrebbero essere non IID nelle impostazioni sul dispositivo:\n\nEterogeneità degli utenti: Utenti diversi hanno interessi e ambienti diversi.\nDifferenze tra dispositivi: Sensori, regioni e dati demografici influenzano i dati.\nEffetti temporali: Ora del giorno, impatti stagionali sui dati.\n\nL’efficacia del ML si basa in gran parte su dati di training ampi e diversificati. Con set di dati piccoli e localizzati, i modelli on-device potrebbero non riuscire a generalizzare tra diverse popolazioni di utenti e ambienti. Ad esempio, un modello di rilevamento delle malattie addestrato solo su immagini di un singolo ospedale non si generalizzerebbe bene ad altri dati demografici dei pazienti. Le prestazioni nel mondo reale non potranno che migliorare con progressi medici estesi e diversificati. Quindi, mentre l’apprendimento basato su cloud sfrutta enormi set di dati, l’apprendimento su dispositivo si basa su “silo di dati” decentralizzati molto più piccoli, unici per ogni utente.\nI dati limitati e le ottimizzazioni richieste per l’apprendimento on-device possono avere un impatto negativo sulla precisione e sulla generalizzazione del modello:\n\nI piccoli dataset aumentano il rischio di overfitting. Ad esempio, un classificatore di frutta addestrato su 100 immagini rischia di overfitting rispetto a uno addestrato su 1 milione di immagini diverse.\nI dati rumorosi generati dall’utente riducono la qualità. Il rumore del sensore o l’etichettatura impropria dei dati da parte di non esperti possono degradare l’addestramento.\nOttimizzazioni come la potatura e la quantizzazione compromettono la precisione per l’efficienza. Un modello quantizzato a 8 bit funziona più velocemente ma meno accuratamente di un modello a 32 bit.\n\nQuindi, mentre i modelli cloud raggiungono un’elevata precisione con enormi set di dati e senza vincoli, i modelli su dispositivo possono avere difficoltà a generalizzare. Alcuni studi dimostrano che il training sul dispositivo corrisponde all’accuratezza del cloud su determinate attività. Tuttavia, le prestazioni sui carichi di lavoro reali richiedono ulteriori studi (Lin et al. 2022).\nAd esempio, un modello cloud può rilevare con precisione la polmonite nelle radiografie del torace di migliaia di ospedali. Tuttavia, un modello sul dispositivo addestrato solo su una piccola popolazione locale di pazienti potrebbe non riuscire a generalizzare.\nUn’accuratezza inaffidabile limita l’applicabilità nel mondo reale dell’apprendimento sul dispositivo per usi critici come la diagnosi di malattie o i veicoli a guida autonoma.\nIl training sul dispositivo è anche più lento del cloud a causa delle risorse limitate. Anche se ogni iterazione è più veloce, il processo di training complessivo richiede più tempo.\nAd esempio, un’applicazione di robotica in tempo reale potrebbe richiedere aggiornamenti del modello entro millisecondi. L’On-device training su un piccolo hardware embedded potrebbe richiedere secondi o minuti per l’aggiornamento, troppo lento per l’uso in tempo reale.\nLe sfide relative a precisione, generalizzazione e velocità pongono ostacoli all’adozione dell’apprendimento on-device per sistemi di produzione reali, soprattutto quando affidabilità e bassa latenza sono fondamentali.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#adattamento-on-device",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#adattamento-on-device",
    "title": "12  Apprendimento On-Device",
    "section": "12.3 Adattamento On-device",
    "text": "12.3 Adattamento On-device\nIn un’attività ML, il consumo di risorse proviene principalmente da tre fonti:\n\nIl modello ML stesso;\nIl processo di ottimizzazione durante l’apprendimento del modello\nArchiviazione ed elaborazione del dataset utilizzato per l’apprendimento.\n\nDi conseguenza, ci sono tre approcci per adattare gli algoritmi ML esistenti su dispositivi con risorse limitate:\n\nRiduzione della complessità del modello ML\nModifica delle ottimizzazioni per ridurre i requisiti delle risorse di training\nCreazione di nuove rappresentazioni dei dati più efficienti in termini di archiviazione\n\nNella sezione seguente, esamineremo questi metodi di adattamento dell’apprendimento on-device. Il capitolo Ottimizzazioni dei Modelli fornisce maggiori dettagli sulle ottimizzazioni del modello.\n\n12.3.1 Riduzione della Complessità del Modello\nIn questa sezione, discuteremo brevemente i modi per ridurre la complessità del modello quando si adattano i modelli ML sul dispositivo. Per i dettagli sulla riduzione della complessità del modello, fare riferimento al capitolo Ottimizzazioni dei Modelli.\n\nAlgoritmi ML tradizionali\nA causa delle limitazioni di elaborazione e memoria dei dispositivi edge, alcuni algoritmi ML tradizionali sono ottimi candidati per applicazioni di apprendimento on-device grazie alla loro natura leggera. Alcuni esempi di algoritmi con basso impatto sulle risorse includono Naive Bayes Classifiers, Support Vector Machines (SVM), Linear Regression, Logistic Regression e algoritmi Decision Tree selezionati.\nCon alcuni perfezionamenti, questi algoritmi ML classici possono essere adattati a specifiche architetture hardware ed eseguire attività semplici. I loro bassi requisiti di prestazioni semplificano l’integrazione dell’apprendimento continuo anche su dispositivi edge.\n\n\nPruning\nIl “pruning” [potatura] è una tecnica per ridurre le dimensioni e la complessità di un modello ML per migliorarne l’efficienza e le prestazioni di generalizzazione. Ciò è utile per l’addestramento di modelli su dispositivi edge, in cui vogliamo ridurre al minimo l’utilizzo delle risorse mantenendo un’accuratezza competitiva.\nL’obiettivo principale della potatura è rimuovere parti del modello che non contribuiscono in modo significativo al suo potere predittivo, mantenendo al contempo gli aspetti più informativi. Nel contesto degli alberi decisionali, la potatura comporta la rimozione di alcuni rami (sottoalberi) dall’albero, portando a un albero più piccolo e semplice. Nel contesto di DNN, la potatura viene utilizzata per ridurre il numero di neuroni (unità) o connessioni nella rete, come mostrato in Figura 12.2.\n\n\n\n\n\n\nFigura 12.2: Potatura della rete.\n\n\n\n\n\nRiduzione della Complessità dei Modelli di Deep Learning\nI framework DNN tradizionali basati su cloud hanno un sovraccarico di memoria troppo elevato per essere utilizzati sul dispositivo. Ad esempio, i sistemi di deep learning come PyTorch e TensorFlow richiedono centinaia di megabyte di overhead di memoria durante l’addestramento di modelli come MobilenetV2 e l’overhead aumenta con l’aumentare del numero di parametri di addestramento.\nLa ricerca attuale per DNN leggeri esplora principalmente architetture CNN. Esistono anche diversi framework “bare-metal” [tutto in hardware] progettati per eseguire reti neurali su MCU mantenendo bassi l’overhead computazionale e l’ingombro di memoria. Alcuni esempi includono MNN, TVM e TensorFlow Lite. Tuttavia, possono eseguire l’inferenza solo durante i passaggi in avanti e non supportano la backpropagation. Sebbene questi modelli siano progettati per l’implementazione edge, la loro riduzione nei pesi del modello e nelle connessioni architettoniche ha portato a minori requisiti di risorse per l’apprendimento continuo.\nIl compromesso tra prestazioni e supporto del modello è chiaro quando si adattano i sistemi DNN più diffusi. Come adattiamo i modelli DNN esistenti a impostazioni con risorse limitate mantenendo il supporto per la backpropagation e l’apprendimento continuo? Le ultime ricerche suggeriscono tecniche di progettazione congiunta di algoritmi e sistemi che aiutano a ridurre il consumo di risorse dell’addestramento ML sui dispositivi edge. Utilizzando tecniche come il “quantization-aware scaling” (QAS) [ridimensionamento consapevole della quantizzazione], aggiornamenti sparsi e altre tecniche all’avanguardia, l’apprendimento sul dispositivo è possibile su sistemi embedded con poche centinaia di kilobyte di RAM senza memoria aggiuntiva mantenendo un’elevata precisione.\n\n\n\n12.3.2 Modifica dei Processi di Ottimizzazione\nLa scelta della giusta strategia di ottimizzazione è importante per l’addestramento DNN su un dispositivo, poiché consente di trovare un buon minimo locale. Poiché l’addestramento avviene su un dispositivo, questa strategia deve anche considerare la memoria e la potenza limitate.\n\nQuantization-Aware Scaling\nLa quantizzazione è un metodo comune per ridurre l’impronta di memoria dell’addestramento DNN. Sebbene ciò possa introdurre nuovi errori, questi possono essere mitigati progettando un modello per caratterizzare questo errore statistico. Ad esempio, i modelli potrebbero utilizzare l’arrotondamento stocastico o introdurre l’errore di quantizzazione negli aggiornamenti del gradiente.\nUna tecnica algoritmica specifica è Quantization-Aware Scaling (QAS), che migliora le prestazioni delle reti neurali su hardware a bassa precisione, come dispositivi edge, dispositivi mobili o sistemi TinyML, regolando i fattori di scala durante il processo di quantizzazione.\nCome abbiamo discusso nel capitolo Ottimizzazioni del modello, la quantizzazione è il processo di mappatura di un intervallo continuo di valori su un set discreto di valori. Nel contesto delle reti neurali, la quantizzazione spesso comporta la riduzione della precisione dei pesi e delle attivazioni da virgola mobile a 32 bit a formati a precisione inferiore come numeri interi a 8 bit. Questa riduzione di precisione può diminuire significativamente il costo computazionale e l’ingombro di memoria del modello, rendendolo adatto per l’implementazione su hardware a bassa precisione. Figura 12.3 è un esempio di quantizzazione float-to-integer.\n\n\n\n\n\n\nFigura 12.3: Quantizzazione float-to-integer. Fonte: Nvidia.\n\n\n\nTuttavia, il processo di quantizzazione può anche introdurre errori di quantizzazione che possono degradare le prestazioni del modello. La scalatura basata sulla quantizzazione è una tecnica che riduce al minimo questi errori regolando i fattori di scala utilizzati nel processo di quantizzazione.\nIl processo QAS prevede due fasi principali:\n\nAddestramento basato sulla quantizzazione: In questa fase, la rete neurale viene addestrata tenendo conto della quantizzazione, simulandola per imitarne gli effetti durante i passaggi “forward” e “backward”. Ciò consente al modello di imparare a compensare gli errori di quantizzazione e migliorarne le prestazioni su hardware a bassa precisione. Per i dettagli, fare riferimento alla sezione QAT in Ottimizzazioni del modello.\nQuantizzazione e ridimensionamento: Dopo l’addestramento, il modello viene quantizzato in un formato a bassa precisione e i fattori di scala vengono regolati per ridurre al minimo gli errori di quantizzazione. I fattori di scala vengono scelti in base alla distribuzione dei pesi e delle attivazioni nel modello e vengono regolati per garantire che i valori quantizzati siano compresi nell’intervallo del formato a bassa precisione.\n\nQAS viene utilizzato per superare le difficoltà di ottimizzazione dei modelli su dispositivi minuscoli senza dover effettuare la messa a punto degli iperparametri; QAS ridimensiona automaticamente i gradienti tensoriali con varie precisioni di bit. Ciò stabilizza il processo di addestramento e corrisponde all’accuratezza della precisione in virgola mobile.\n\n\nAggiornamenti Sparsi\nSebbene QAS consenta l’ottimizzazione di un modello quantizzato, utilizza una grande quantità di memoria, il che non è realistico per l’addestramento sul dispositivo. Quindi, gli aggiornamenti “spare” vengono utilizzati per ridurre l’ingombro di memoria del calcolo “full backward”. Invece di potare i pesi per l’inferenza, l’aggiornamento sparso pota il gradiente durante la “backward propagation” [propagazione all’indietro] per aggiornare il modello in modo sparso. In altre parole, l’aggiornamento sparso salta i gradienti del calcolo di layer e sottotensori meno importanti.\nTuttavia, determinare lo schema di un aggiornamento sparso ottimale dato un budget di memoria vincolante può essere difficile a causa dell’ampio spazio di ricerca. Ad esempio, il modello MCUNet ha 43 layer convoluzionali e uno spazio di ricerca di circa 1030. Una tecnica per affrontare questo problema è l’analisi del contributo. L’analisi del contributo misura il miglioramento dell’accuratezza dai bias (aggiornamento degli ultimi bias rispetto al solo aggiornamento del classificatore) e pesi (aggiornamento del peso di un layer extra rispetto al solo aggiornamento del bias). Cercando di massimizzare questi miglioramenti, l’analisi del contributo deriva automaticamente uno schema di aggiornamento sparso ottimale per abilitare l’addestramento sul dispositivo.\n\n\nTraining Layer-Wise\nAltri metodi oltre alla quantizzazione possono aiutare a ottimizzare le routine. Uno di questi metodi è l’addestramento “layer-wise”. Un consumatore significativo di memoria dell’addestramento DNN è la backpropagation end-to-end, che richiede che tutte le feature map intermedie siano archiviate in modo che il modello possa calcolare i gradienti. Un’alternativa a questo approccio che riduce l’impronta di memoria dell’addestramento DNN è l’addestramento sequenziale “layer-by-layer” (T. Chen et al. 2016). Invece dell’addestramento end-to-end, l’addestramento di un singolo layer alla volta aiuta a evitare di dover archiviare le feature map intermedie.\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, e Carlos Guestrin. 2016. «Training deep nets with sublinear memory cost». ArXiv preprint abs/1604.06174. https://arxiv.org/abs/1604.06174.\n\n\nTrading Computation for Memory\nLa strategia “trading computation for memory” [scambio di elaborazione per memoria] comporta il rilascio di parte della memoria utilizzata per archiviare i risultati intermedi. Invece, questi risultati possono essere ricalcolati in base alle necessità. È stato dimostrato che la riduzione della memoria in cambio di più elaborazione riduce l’impronta di memoria dell’addestramento DNN per adattarsi a quasi tutti i budget, riducendo al minimo anche i costi di elaborazione (Gruslys et al. 2016).\n\nGruslys, Audrunas, Rémi Munos, Ivo Danihelka, Marc Lanctot, e Alex Graves. 2016. «Memory-Efficient Backpropagation Through Time». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 4125–33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\n\n12.3.3 Sviluppo di Nuove Rappresentazioni dei Dati\nLa dimensionalità e il volume dei dati di training possono avere un impatto significativo sull’adattamento sul dispositivo. Quindi, un’altra tecnica per adattare i modelli su dispositivi con risorse limitate è quella di rappresentare i set di dati in modo più efficiente.\n\nCompressione dei Dati\nL’obiettivo della compressione dei dati è raggiungere elevate precisioni limitando al contempo la quantità di dati di training. Un metodo per raggiungere questo obiettivo è dare priorità alla complessità del campione: la quantità di dati di training necessari affinché l’algoritmo raggiunga una precisione target (Dhar et al. 2021).\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh Kurup, e Mohak Shah. 2021. «A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective». ACM Transactions on Internet of Things 2 (3): 1–49. https://doi.org/10.1145/3450494.\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, e Farinaz Koushanfar. 2017. «TinyDL: Just-in-time deep learning solution for constrained embedded systems». In 2017 IEEE International Symposium on Circuits and Systems (ISCAS), 1–4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\nLi, Xiang, Tao Qin, Jian Yang, e Tie-Yan Liu. 2016. «LightRNN: Memory and Computation-Efficient Recurrent Neural Networks». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 4385–93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\nAltri metodi più comuni di compressione dei dati si concentrano sulla riduzione della dimensionalità e del volume dei dati di training. Ad esempio, un approccio potrebbe sfruttare la sparsità della matrice per ridurre l’ingombro di memoria per l’archiviazione dei dati di training. I dati di training possono essere trasformati in un embedding a dimensione inferiore e fattorizzati in una matrice di dizionario moltiplicata per una matrice di coefficienti blocchi sparsi (Darvish Rouhani, Mirhoseini, e Koushanfar 2017). Un altro esempio potrebbe riguardare la rappresentazione di parole provenienti da un ampio set di dati di training linguistica in un formato vettoriale più compresso (Li et al. 2016).",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#il-transfer-learning",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#il-transfer-learning",
    "title": "12  Apprendimento On-Device",
    "section": "12.4 Il Transfer Learning",
    "text": "12.4 Il Transfer Learning\nIl transfer learning è una tecnica di ML in cui un modello sviluppato per un’attività specifica viene riutilizzato come punto di partenza per un modello su una seconda attività. Nel contesto dell’intelligenza artificiale su dispositivi, il transfer learning ci consente di sfruttare modelli pre-addestrati che hanno già appreso rappresentazioni utili da grandi set di dati e di perfezionarli per attività specifiche utilizzando set di dati più piccoli direttamente sul dispositivo. Ciò può ridurre significativamente le risorse di calcolo e il tempo necessari per l’addestramento dei modelli da zero.\nFigura 12.4 include alcuni esempi intuitivi di transfer learning dal mondo reale. Ad esempio, sapendo andare in bicicletta, si sa come bilanciarsi su veicoli a due ruote. Quindi, risulterebbe più facile imparare ad andare in moto rispetto a chi non sa andare in bicicletta.\n\n\n\n\n\n\nFigura 12.4: Trasferimento di conoscenze tra attività. Fonte: Zhuang et al. (2021).\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, e Qing He. 2021. «A Comprehensive Survey on Transfer Learning». Proc. IEEE 109 (1): 43–76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nPrendiamo l’esempio di un’applicazione di sensore intelligente che utilizza l’intelligenza artificiale on-device per riconoscere gli oggetti nelle immagini acquisite dal dispositivo. Tradizionalmente, ciò richiederebbe l’invio dei dati dell’immagine a un server, dove un ampio modello di rete neurale elabora i dati e invia i risultati. Con l’intelligenza artificiale on-device, il modello viene archiviato ed eseguito direttamente sul dispositivo, eliminando la necessità di inviare dati a un server.\nPer personalizzare il modello per le caratteristiche on-device, addestrare un modello di rete neurale da zero sul dispositivo sarebbe poco pratico a causa delle risorse di calcolo limitate e della durata della batteria. È qui che entra in gioco il “transfer learning” [apprendimento tramite trasferimento]. Invece di addestrare un modello da zero, possiamo prendere un modello pre-addestrato, come una rete neurale convoluzionale (CNN) o una rete di trasformatori addestrata su un ampio set di dati di immagini, e perfezionarlo per la nostra specifica attività di riconoscimento degli oggetti. Questa messa a punto può essere eseguita direttamente sul dispositivo utilizzando un set di dati più piccolo di immagini pertinenti all’attività. Sfruttando il modello pre-addestrato, possiamo ridurre le risorse di calcolo e il tempo necessari per il training, ottenendo comunque un’elevata precisione per l’attività di riconoscimento degli oggetti.\nIl transfer learning è importante per rendere praticabile l’intelligenza artificiale on-device, consentendoci di sfruttare modelli pre-addestrati e di perfezionarli per attività specifiche, riducendo così le risorse di calcolo e il tempo necessari per il training. La combinazione di intelligenza artificiale sul dispositivo e il “transfer learning” apre nuove possibilità per applicazioni di intelligenza artificiale più attente alla privacy e più reattive alle esigenze degli utenti.\nIl transfer learning ha rivoluzionato il modo in cui i modelli vengono sviluppati e distribuiti, sia nel cloud che nell’edge. Il transfer learning viene utilizzato nel mondo reale. Un esempio del genere è l’uso del transfer learning per sviluppare modelli di intelligenza artificiale in grado di rilevare e diagnosticare malattie da immagini mediche, come raggi X, scansioni MRI [risonanza magnetica] e TAC. Ad esempio, i ricercatori della Stanford University hanno sviluppato un modello di apprendimento di trasferimento in grado di rilevare il cancro nelle immagini della pelle con una precisione del 97% (Esteva et al. 2017). Questo modello è stato pre-addestrato su 1.28 milioni di immagini per classificare un’ampia gamma di oggetti e poi specializzato per il rilevamento del cancro tramite l’addestramento su un set di dati di immagini della pelle curato da dermatologi.\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, e Sebastian Thrun. 2017. «Dermatologist-level classification of skin cancer with deep neural networks». Nature 542 (7639): 115–18. https://doi.org/10.1038/nature21056.\nL’implementazione negli scenari di produzione può essere ampiamente categorizzata in due fasi: pre-distribuzione e post-distribuzione.\n\n12.4.1 Specializzazione Pre-Distribuzione\nNella fase di pre-implementazione, il transfer learning funge da catalizzatore per accelerare il processo di sviluppo. Ecco come funziona tipicamente: Si immagini di creare un sistema per riconoscere diverse razze di cani. Invece di partire da zero, possiamo utilizzare un modello pre-addestrato che ha già padroneggiato il compito più ampio di riconoscere gli animali nelle immagini.\nQuesto modello pre-addestrato funge da solida base e contiene una vasta conoscenza acquisita da dati estesi. Quindi perfezioniamo questo modello utilizzando un set di dati specializzato contenente immagini di varie razze di cani. Questo processo di messa a punto adatta il modello alle nostre esigenze specifiche, ovvero identificare con precisione le razze di cani. Una volta perfezionato e convalidato per soddisfare i criteri di prestazione, questo modello specializzato è pronto per l’implementazione.\nEcco come funziona in pratica:\n\nInizia con un Modello Pre-Addestrato: Si inizia selezionando un modello che è già stato addestrato su un set di dati completo, solitamente correlato a un’attività generale. Questo modello funge da base per l’attività in questione.\nFinetuning: Il modello pre-addestrato viene quindi perfezionato su un set di dati più piccolo e specifico per l’attività desiderata. Questo passaggio consente al modello di adattare e specializzare la sua conoscenza ai requisiti specifici dell’applicazione.\nValidazione: Dopo la messa a punto, il modello viene convalidato per garantire che soddisfi i criteri di prestazione per l’attività specializzata.\nDeployment: Una volta convalidato, il modello specializzato viene distribuito nell’ambiente di produzione.\n\nQuesto metodo riduce significativamente il tempo e le risorse di calcolo necessarie per addestrare un modello da zero (Pan e Yang 2010). Adottando l’apprendimento tramite trasferimento, i sistemi embedded possono raggiungere un’elevata precisione su attività specializzate senza la necessità di raccogliere dati estesi o di impiegare risorse di calcolo significative per l’addestramento da zero.\n\nPan, Sinno Jialin, e Qiang Yang. 2010. «A Survey on Transfer Learning». IEEE Trans. Knowl. Data Eng. 22 (10): 1345–59. https://doi.org/10.1109/tkde.2009.191.\n\n\n12.4.2 Adattamento Post-Distribuzione\nL’implementazione su un dispositivo non deve necessariamente segnare il culmine del percorso educativo di un modello ML. Con l’avvento dell’apprendimento per trasferimento, apriamo le porte all’implementazione di modelli ML adattivi in scenari del mondo reale, soddisfacendo le esigenze personalizzate degli utenti.\nConsideriamo un’applicazione reale in cui un genitore desidera identificare il proprio figlio in una raccolta di immagini di un evento scolastico sul proprio smartphone. In questo scenario, il genitore si trova di fronte alla sfida di localizzare il proprio figlio in mezzo alle immagini di molti altri bambini. L’apprendimento per trasferimento può essere impiegato qui per perfezionare il modello di un sistema embedded per questo compito unico e specializzato. Inizialmente, il sistema potrebbe utilizzare un modello generico addestrato per riconoscere i volti nelle immagini. Tuttavia, con l’apprendimento per trasferimento, il sistema può adattare questo modello per riconoscere le caratteristiche specifiche del figlio dell’utente.\nEcco come funziona:\n\nRaccolta Dati: Il sistema embedded raccoglie immagini che includono il bambino, idealmente con l’input del genitore per garantire accuratezza e pertinenza. Ciò può essere fatto direttamente sul dispositivo, mantenendo la privacy dei dati dell’utente.\nFine Tuning del Modello: Il modello di riconoscimento facciale preesistente, che è stato addestrato su un set di dati ampio e diversificato, viene quindi perfezionato utilizzando le immagini del bambino appena raccolte. Questo processo adatta il modello per riconoscere le caratteristiche facciali specifiche del bambino, distinguendolo dagli altri bambini nelle immagini.\nValidazione: Il modello rifinito viene poi convalidato per garantire che riconosca accuratamente il bambino in varie immagini. Ciò può comportare che il genitore verifichi le prestazioni del modello e fornisca feedback per ulteriori miglioramenti.\nDeployment: Una volta convalidato, il modello adattato viene distribuito sul dispositivo, consentendo al genitore di identificare facilmente il proprio figlio nelle immagini senza doverle esaminare manualmente.\n\nQuesta personalizzazione al volo migliora l’efficacia del modello per il singolo utente, assicurando che tragga vantaggio dalla personalizzazione ML. Questo è, in parte, il modo in cui iPhotos o Google Photos funzionano quando ci chiedono di riconoscere un volto e poi, in base a queste informazioni, indicizzano tutte le foto di quel volto. Poiché l’apprendimento e l’adattamento avvengono sul dispositivo stesso, non ci sono rischi per la privacy personale. Le immagini dei genitori non vengono caricate su un server cloud o condivise con terze parti, proteggendo la privacy della famiglia e continuando a raccogliere i benefici di un modello ML personalizzato. Questo approccio rappresenta un significativo passo avanti nella ricerca per fornire agli utenti soluzioni ML personalizzate che rispettino e sostengano la loro privacy.\n\n\n12.4.3 Vantaggi\nIl transfer learning è diventato una tecnica importante in ML e intelligenza artificiale, ed è particolarmente prezioso per diversi motivi.\n\nScarsità di Dati: In molti scenari reali, acquisire un set di dati etichettato sufficientemente grande per addestrare un modello ML da zero è complicato. Il transfer learning mitiga questo problema consentendo l’uso di modelli pre-addestrati che hanno già appreso funzionalità preziose da un vasto set di dati.\nSpese Computazionali: Addestrare un modello da zero richiede risorse computazionali e tempo significativi, specialmente per modelli complessi come reti neurali profonde. Utilizzando il transfer learning, possiamo sfruttare il calcolo che è già stato eseguito durante l’addestramento del modello sorgente, risparmiando così tempo e potenza computazionale.\nDati Annotati Limitati: Per alcune attività specifiche, potrebbero essere disponibili ampi dati grezzi, ma il processo di etichettatura di tali dati per l’apprendimento supervisionato può essere costoso e richiedere molto tempo. Il transfer learning ci consente di utilizzare modelli pre-addestrati su un’attività correlata con dati etichettati, quindi richiedendo meno dati annotati per la nuova attività.\n\nCi sono vantaggi nel riutilizzare le funzionalità:\n\nHierarchical Feature Learning: I modelli di deep learning, in particolare le reti neurali convoluzionali (CNN), possono apprendere funzionalità gerarchiche. I layer inferiori in genere apprendono funzionalità generiche come bordi e forme, mentre quelli superiori apprendono funzionalità più complesse e specifiche per l’attività. Il transfer learning ci consente di riutilizzare le funzionalità generiche apprese da un modello e di perfezionare i livelli superiori per la nostra attività specifica.\nAumento delle Prestazioni: È stato dimostrato che il transfer learning aumenta le prestazioni dei modelli su attività con dati limitati. La conoscenza acquisita dall’attività dall’attività sorgente può fornire un prezioso punto di partenza e portare a una convergenza più rapida e a una maggiore accuratezza nell’attività target.\n\n\n\n\n\n\n\nEsercizio 12.1: Il Transfer Learning\n\n\n\n\n\nSi immagini di addestrare un’IA a riconoscere i fiori come un professionista, ma senza aver bisogno di un milione di immagini di fiori! Questo è il potere del transfer learning. In questo Colab, prenderemo un’IA che conosce già le immagini e le insegneremo a diventare un’esperta di fiori con meno sforzo. Prepararsi a rendere la propria IA più intelligente, non è più difficile!\n\n\n\n\n\n\n12.4.4 Concetti Fondamentali\nComprendere i concetti fondamentali del transfer learning è essenziale per utilizzare efficacemente questo potente approccio in ML. Qui, analizzeremo alcuni dei principi e dei componenti principali che stanno alla base del processo di transfer learning.\n\nAttività di Origine e di Destinazione\nNel transfer learning, sono coinvolte due attività principali: l’attività di origine e quella di destinazione. L’attività di origine è quella per la quale il modello è già stato addestrato e ha appreso informazioni preziose. L’attività di destinazione è la nuova attività che vogliamo che il modello esegua. L’obiettivo del transfer learning è sfruttare le conoscenze acquisite dall’attività di origine per migliorare le prestazioni nell’attività di destinazione.\nSupponiamo di avere un modello addestrato per riconoscere vari frutti nelle immagini (attività di origine) e di voler creare un nuovo modello per riconoscere diverse verdure nelle immagini (attività di destinazione). In tal caso, possiamo utilizzare il transfer learning per sfruttare le conoscenze acquisite durante l’attività di riconoscimento della frutta per migliorare le prestazioni del modello di riconoscimento della verdura.\n\n\nTrasferimento della Rappresentazione\nIl trasferimento della rappresentazione riguarda le rappresentazioni apprese (caratteristiche) dall’attività di origine all’attività di destinazione. Esistono tre tipi principali di trasferimento della rappresentazione:\n\nTrasferimento di Istanza: Implica il riutilizzo delle istanze di dati dall’attività di origine nell’attività di destinazione.\nTrasferimento della Rappresentazione delle Feature: Implica il trasferimento delle rappresentazioni di Feature [funzionalità] apprese dall’attività di origine all’attività di destinazione.\nTrasferimento di Parametri: Implica il trasferimento dei parametri appresi del modello (pesi) dall’attività di origine all’attività di destinazione.\n\nNell’elaborazione del linguaggio naturale, un modello addestrato per comprendere la sintassi e la grammatica di una lingua (attività di origine) può trasferire le sue rappresentazioni apprese a un nuovo modello progettato per eseguire l’analisi del sentiment (attività di destinazione).\n\n\nFinetuning\nIl “finetuning” [messa a punto] è il processo di regolazione dei parametri di un modello pre-addestrato per adattarlo all’attività di destinazione. In genere, ciò comporta l’aggiornamento dei pesi dei layer del modello, in particolare degli ultimi layer, per rendere il modello più pertinente per la nuova attività. Nella classificazione delle immagini, un modello pre-addestrato su un set di dati generale come ImageNet (attività di origine) può essere messo a punto regolando i pesi dei suoi livelli per ottenere buone prestazioni in un’attività di classificazione specifica, come il riconoscimento di specie animali specifiche (attività di destinazione).\n\n\nEstrazione delle Feature\nL’estrazione delle “feature” [caratteristiche] comporta l’utilizzo di un modello pre-addestrato come estrattore di feature fisse, in cui l’output dei layer intermedi del modello viene utilizzato come feature per l’attività di destinazione. Questo approccio è particolarmente utile quando l’attività di destinazione ha un set di dati di piccole dimensioni, poiché le feature apprese dal modello pre-addestrato possono migliorare significativamente le prestazioni. Nell’analisi delle immagini mediche, un modello pre-addestrato su un ampio set di dati di immagini mediche generali (attività di origine) può essere utilizzato come estrattore di feature per fornire funzionalità preziose per un nuovo modello progettato per riconoscere specifici tipi di tumori nelle immagini radiografiche (attività di destinazione).\n\n\n\n12.4.5 Tipi di Apprendimento Tramite Trasferimento\nL’apprendimento tramite trasferimento può essere classificato in tre tipi principali in base alla natura delle attività e dei dati di origine e di destinazione. Esploriamo ciascun tipo in dettaglio:\n\nApprendimento Tramite Trasferimento Induttivo\nNell’apprendimento tramite trasferimento induttivo, l’obiettivo è apprendere la funzione predittiva di destinazione con l’aiuto dei dati di origine. In genere comporta la messa a punto di un modello pre-addestrato sull’attività di destinazione con dati etichettati disponibili. Un esempio comune di apprendimento tramite trasferimento induttivo sono le attività di classificazione delle immagini. Ad esempio, un modello pre-addestrato sul set di dati ImageNet (attività di origine) può essere messo a punto per classificare tipi specifici di uccelli (attività di destinazione) utilizzando un set di dati etichettato più piccolo di immagini di uccelli.\n\n\nApprendimento Tramite Trasferimento Transduttivo\nL’apprendimento tramite trasferimento transduttivo comporta l’utilizzo di dati di origine e destinazione, ma solo dell’attività di origine. L’obiettivo principale è trasferire la conoscenza dal dominio di origine al dominio di destinazione, anche se le attività rimangono le stesse. L’analisi del “sentiment” per diverse lingue può servire come esempio di apprendimento tramite trasferimento transduttivo. Un modello addestrato per eseguire l’analisi del sentiment in inglese (attività di origine) può essere adattato per eseguire l’analisi del sentiment in un’altra lingua, come il francese (attività di destinazione), sfruttando set di dati paralleli di frasi in inglese e francese con gli stessi sentimenti.\n\n\nApprendimento con Trasferimento Non Supervisionato\nL’apprendimento con trasferimento non supervisionato viene utilizzato quando le attività di origine e di destinazione sono correlate, ma non sono disponibili dati etichettati per l’attività di destinazione. L’obiettivo è sfruttare la conoscenza acquisita dall’attività di origine per migliorare le prestazioni nell’attività di destinazione, anche senza dati etichettati. Un esempio di apprendimento di trasferimento non supervisionato è la modellazione degli argomenti nei dati di testo. Un modello addestrato per estrarre argomenti da articoli di notizie (attività di origine) può essere adattato per estrarre argomenti da post sui social media (attività di destinazione) senza aver bisogno di dati etichettati per i post sui social media.\n\n\nConfronto e Compromessi\nSfruttando questi diversi tipi di apprendimento per trasferimento, i professionisti possono scegliere l’approccio che meglio si adatta alla natura dei loro compiti e ai dati disponibili, portando infine a modelli di ML più efficaci ed efficienti. Quindi, in sintesi:\n\nInduttivo: Diversi compiti di origine e destinazione, domini diversi\nTrasduttivo: diversi compiti di origine e destinazione, stesso dominio\nNon supervisionato: dati di origine non etichettati, trasferisce le rappresentazioni delle feature\n\nTabella 12.1 presenta una matrice che delinea in modo un po’ più dettagliato le somiglianze e le differenze tra i tipi di apprendimento per trasferimento:\n\n\n\nTabella 12.1: Confronto dei tipi di apprendimento per trasferimento.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nApprendimento Induttivo per Trasferimento\nApprendimento Trasduttivo per Trasferimento\nApprendimento Non Supervisionato\n\n\n\n\nDati Etichettati per l’Attività di Destinazione\nObbligatorio\nNon obbligatorio\nNon obbligatorio\n\n\nAttività di origine\nPuò essere diversa\nLo stesso\nLo stesso o diverso\n\n\nAttività di destinazione\nPuò essere diversa\nLo stesso\nPuò essere diverso\n\n\nObiettivo\nMigliorare le prestazioni dell’attività target con i dati sorgente\nTrasferisci la conoscenza dal dominio sorgente a quello target\nSfrutta l’attività sorgente per migliorare le prestazioni dell’attività target senza dati etichettati\n\n\nEsempio\nDa ImageNet alla classificazione degli uccelli\nAnalisi del sentiment in diverse lingue\nModellazione degli argomenti per diversi dati di testo\n\n\n\n\n\n\n\n\n\n12.4.6 Vincoli e Considerazioni\nQuando si intraprende un apprendimento per trasferimento, ci sono diversi fattori che devono essere considerati per garantire un trasferimento di conoscenze di successo e prestazioni del modello. Ecco una ripartizione di alcuni fattori chiave:\n\nSomiglianza dei Domini\nLa similarità di dominio si riferisce a quanto strettamente correlati sono i domini di origine e di destinazione. Più simili sono i domini, più è probabile che l’apprendimento per trasferimento abbia successo. Trasferire la conoscenza da un modello addestrato su immagini di scene esterne (dominio di origine) a un nuovo compito che prevede il riconoscimento di oggetti in scene interne (dominio di destinazione) potrebbe avere più successo rispetto al trasferire la conoscenza da scene esterne a un compito che prevede l’analisi del testo, poiché i domini (immagini vs. testo) sono piuttosto diversi.\n\n\nSimilarità dell’Attività\nLa similarità dell’attività si riferisce a quanto strettamente correlati sono le attività di origine e di destinazione. È probabile che attività simili traggano maggiori benefici dall’apprendimento per trasferimento. Un modello addestrato per riconoscere diverse razze di cani (attività di origine) può essere adattato più facilmente per riconoscere diverse razze di gatti (attività di destinazione) rispetto a quanto possa essere adattato per eseguire un’attività completamente diversa come la traduzione di una lingua.\n\n\nQualità e Quantità dei Dati\nLa qualità e la quantità dei dati disponibili per il compito di destinazione possono avere un impatto significativo sul successo dell’apprendimento per trasferimento. Più dati di alta qualità possono comportare migliori prestazioni del modello. Supponiamo di avere un ampio set di dati con immagini chiare e ben etichettate per riconoscere specie di uccelli specifiche. In tal caso, il processo di apprendimento per trasferimento avrà probabilmente più successo rispetto a un set di dati piccolo e rumoroso.\n\n\nSovrapposizione dello Spazio delle Feature\nLa sovrapposizione dello spazio delle feature si riferisce a quanto bene le feature apprese dal modello sorgente si allineano con quelle necessarie per l’attività di destinazione. Una maggiore sovrapposizione può portare a un apprendimento per trasferimento più efficace. Un modello addestrato su immagini ad alta risoluzione (attività di origine) potrebbe non trasferirsi bene a un’attività di destinazione che coinvolge immagini a bassa risoluzione, poiché lo spazio delle feature (alta risoluzione rispetto a bassa risoluzione) è diverso.\n\n\nComplessità del Modello\nAnche la complessità del modello sorgente può influire sul successo dell’apprendimento per trasferimento. A volte, un modello più semplice potrebbe trasferirsi meglio di uno complesso, poiché è meno probabile che si adatti eccessivamente all’attività di origine. Ad esempio, un semplice modello di rete neurale convoluzionale (CNN) addestrato su dati di immagini (attività di origine) può essere trasferito con maggiore successo a una nuova attività di classificazione di immagini (attività di destinazione) rispetto a una CNN complessa con molti layer, poiché è meno probabile che il modello più semplice si adatti eccessivamente all’attività di origine.\nConsiderando questi fattori, i professionisti del ML possono prendere decisioni informate su quando e come utilizzare l’apprendimento per trasferimento, portando infine a prestazioni del modello più efficaci nell’attività di destinazione. Il successo dell’apprendimento per trasferimento dipende dal grado di similarità tra i domini di origine e di destinazione. L’overfitting [adattamento eccessivo] è rischioso, soprattutto quando la messa a punto avviene su un set di dati limitato. Sul fronte computazionale, alcuni modelli pre-addestrati, a causa delle loro dimensioni, potrebbero non adattarsi comodamente ai vincoli di memoria di alcuni dispositivi o potrebbero essere eseguiti in modo proibitivamente lento. Nel tempo, con l’evoluzione dei dati, c’è il potenziale per la “drift” [deriva] del modello, che indica la necessità di un riaddestramento periodico o di un adattamento continuo.\nScoprire di più sull’apprendimento per trasferimento in Video 12.1 di seguito.\n\n\n\n\n\n\nVideo 12.1: Il Transfer Learning",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#sec-fl",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#sec-fl",
    "title": "12  Apprendimento On-Device",
    "section": "12.5 Apprendimento Automatico Federato",
    "text": "12.5 Apprendimento Automatico Federato\nPanoramica dell’Apprendimento Federato\nL’Internet moderna è piena di grandi reti di dispositivi connessi. Che si tratti di telefoni cellulari, termostati, smart speaker o altri prodotti IoT, innumerevoli dispositivi edge sono una miniera d’oro per dati iperpersonalizzati e ricchi. Tuttavia, con quei dati ricchi arriva una serie di problemi con il trasferimento delle informazioni e la privacy. Costruire un set di dati di training nel cloud da questi dispositivi comporterebbe un’ampia larghezza di banda, costi per il trasferimento dati e violazione della privacy degli utenti.\nL’apprendimento federato offre una soluzione a questi problemi: addestrare i modelli parzialmente sui dispositivi edge e comunicare solo gli aggiornamenti al cloud. Nel 2016, un team di Google ha progettato un’architettura per l’apprendimento federato che tenta di risolvere questi problemi.\nNel loro articolo iniziale, Google delinea un principio di algoritmo di apprendimento federato chiamato FederatedAveraging, che è mostrato in Figura 12.5. In particolare, FederatedAveraging esegue la “stochastic gradient descent (SGD) [discesa del gradiente stocastico] su diversi dispositivi edge. In questo processo, ogni dispositivo calcola un gradiente \\(g_k = \\nabla F_k(w_t)\\) che viene poi applicato per aggiornare i pesi lato server come (con \\(\\eta\\) come tasso di apprendimento su \\(k\\) client): \\[\nw_{t+1} \\rightarrow w_t - \\eta \\sum_{k=1}^{K} \\frac{n_k}{n}g_k\n\\] Questo riassume l’algoritmo di base per l’apprendimento federato sulla destra. Per ogni round di addestramento, il server prende un set casuale di dispositivi client e chiama ogni client per addestrare sul suo batch locale usando i pesi lato server più recenti. Tali pesi vengono poi restituiti al server, dove vengono raccolti individualmente e calcolati per aggiornare i pesi del modello globale.\n\n\n\n\n\n\nFigura 12.5: Algoritmo FederatedAverage proposto da Google. Fonte: McMahan et al. (2017).\n\n\n\nCon questa struttura proposta, ci sono alcuni vettori chiave per ottimizzare ulteriormente l’apprendimento federato. Descriveremo ciascuno di essi nelle seguenti sottosezioni.\nVideo 12.2 fornisce una panoramica dell’apprendimento federato.\n\n\n\n\n\n\nVideo 12.2: Il Transfer Learning\n\n\n\n\n\n\n\n12.5.1 Efficienza della Comunicazione\nUno dei principali colli di bottiglia nell’apprendimento federato è la comunicazione. Ogni volta che un client addestra il modello, deve comunicare i propri aggiornamenti al server. Analogamente, una volta che il server ha calcolato la media di tutti gli aggiornamenti, deve inviarli al client. Ciò comporta enormi costi di larghezza di banda e risorse su grandi reti di milioni di dispositivi. Con l’avanzare del campo dell’apprendimento federato, sono state sviluppate alcune ottimizzazioni per ridurre al minimo questa comunicazione. Per affrontare l’ingombro del modello, i ricercatori hanno sviluppato tecniche di compressione del modello. Nel protocollo client-server, l’apprendimento federato può anche ridurre al minimo la comunicazione tramite la condivisione selettiva degli aggiornamenti sui client. Infine, anche tecniche di aggregazione efficienti possono semplificare il processo di comunicazione.\n\n\n12.5.2 Compressione del Modello\nNell’apprendimento federato standard, il server comunica l’intero modello a ciascun client, quindi il client invia tutti i pesi aggiornati. Ciò significa che il modo più semplice per ridurre l’ingombro di memoria e comunicazione del client è ridurre al minimo le dimensioni del modello che deve essere comunicato. Possiamo impiegare tutte le strategie di ottimizzazione del modello discusse in precedenza per farlo.\nNel 2022, un altro team di Google ha proposto che ogni client comunichi tramite un formato compresso e decomprima il modello al volo per l’addestramento (Yang et al. 2023), allocando e deallocando l’intera memoria per il modello solo per un breve periodo durante l’addestramento. Il modello viene compresso tramite una gamma di diverse strategie di quantizzazione elaborate nel loro documento. Nel frattempo, il server può aggiornare il modello non compresso decomprimendolo e applicando gli aggiornamenti man mano che arrivano.\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Françoise Beaufays, Rajiv Mathews, e Mingqing Chen. 2023. «Online Model Compression for Federated Learning with Large Models». In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1–5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\n12.5.3 Condivisione Selettiva degli Aggiornamenti\nEsistono molti metodi per condividere selettivamente gli aggiornamenti. Il principio generale è che la riduzione della porzione del modello che i client stanno addestrando lato edge riduce la memoria necessaria per l’addestramento e la dimensione della comunicazione con il server. Nell’apprendimento federato di base, il client addestra l’intero modello. Ciò significa che quando un client invia un aggiornamento al server, ha gradienti per ogni peso nella rete.\nTuttavia, non possiamo semplicemente ridurre la comunicazione inviando parti di quei gradienti da ogni client al server perché i gradienti fanno parte di un intero aggiornamento necessario per migliorare il modello. Invece, si deve progettare architettonicamente il modello in modo che ogni client addestri solo una piccola parte del modello più ampio, riducendo la comunicazione totale e ottenendo comunque il vantaggio dell’addestramento sui dati del client. Un articolo (Shi e Radu 2022) dell’Università di Sheffield applica questo concetto a una CNN suddividendo il modello globale in due parti: una parte superiore e una inferiore, come mostrato in Z. Chen e Xu (2023).\n\nShi, Hongrui, e Valentin Radu. 2022. «Data selection for efficient model update in federated learning». In Proceedings of the 2nd European Workshop on Machine Learning and Systems, 72–78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\n\n\n\n\nFigura 12.6: Suddivisione dell’architettura del modello per la condivisione selettiva. Fonte: Shi et al., (2022).\n\n\n\nLa parte inferiore è progettata per concentrarsi sulle funzionalità generiche nel set di dati, mentre la parte superiore, addestrata su tali funzionalità generiche, è progettata per essere più sensibile alle mappe di attivazione. Ciò significa che la parte inferiore del modello viene addestrata tramite la media federata standard su tutti i client. Nel frattempo, la parte superiore del modello viene addestrata interamente sul lato server dalle mappe di attivazione generate dai client. Questo approccio riduce drasticamente la comunicazione per il modello, rendendo comunque la rete robusta a vari tipi di input trovati nei dati sui dispositivi client.\n\n\n12.5.4 Aggregazione Ottimizzata\nOltre a ridurre il sovraccarico di comunicazione, l’ottimizzazione della funzione di aggregazione può migliorare la velocità e l’accuratezza dell’addestramento del modello in determinati casi d’uso di apprendimento federato. Mentre lo standard per l’aggregazione è solo la media, vari altri approcci possono migliorare l’efficienza, l’accuratezza e la sicurezza del modello. Un’alternativa è la “media ritagliata”, che limita gli aggiornamenti del modello entro un intervallo specifico. Un’altra strategia per preservare la sicurezza è l’aggregazione media della privacy differenziale. Questo approccio integra la privacy differenziale nella fase di aggregazione per proteggere le identità dei client. Ogni client aggiunge uno strato di rumore casuale ai propri aggiornamenti prima di comunicare al server. Il server si aggiorna quindi con gli aggiornamenti rumorosi, il che significa che la quantità di rumore deve essere regolata attentamente per bilanciare privacy e precisione.\nOltre ai metodi di aggregazione che migliorano la sicurezza, ci sono diverse modifiche ai metodi di aggregazione che possono migliorare la velocità di training e le prestazioni aggiungendo metadati client insieme agli aggiornamenti del peso. Il “Momentum aggregation” è una tecnica che aiuta ad affrontare il problema della convergenza. Nell’apprendimento federato, i dati client possono essere estremamente eterogenei a seconda dei diversi ambienti in cui vengono utilizzati i dispositivi. Ciò significa che molti modelli con dati eterogenei potrebbero aver bisogno di aiuto per convergere. Ogni client memorizza localmente un termine di “momentum”, che traccia il ritmo del cambiamento su diversi aggiornamenti. Con i client che comunicano questo “momentum”, il server può tenere conto della velocità di cambiamento di ogni aggiornamento quando si modifica il modello globale per accelerare la convergenza. Allo stesso modo, l’aggregazione ponderata può tenere conto delle prestazioni del client o di altri parametri come il tipo di dispositivo o la potenza della connessione di rete per regolare il peso con cui il server dovrebbe incorporare gli aggiornamenti del modello. Ulteriori descrizioni di algoritmi di aggregazione specifici sono fornite da Moshawrab et al. (2023).\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim, e Ali Raad. 2023. «Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives». Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\n12.5.5 Gestione dei Dati non-IID\nQuando si utilizza l’apprendimento federato per addestrare un modello su molti dispositivi client, è conveniente considerare i dati come indipendenti e distribuiti in modo identico (IID) su tutti i client. Quando i dati sono IID, il modello convergerà più velocemente e funzionerà meglio perché ogni aggiornamento locale su un dato client è più rappresentativo del set di dati più ampio. Questo semplifica l’aggregazione, poiché è possibile calcolare direttamente la media di tutti i client. Tuttavia, differisce dal modo in cui i dati spesso appaiono nel mondo reale. Si considerino alcuni dei seguenti modi in cui i dati possono essere non IID:\n\nImparando su un set di dispositivi di monitoraggio sanitari, diversi modelli di dispositivi potrebbero significare diverse qualità e proprietà dei sensori. Ciò significa che sensori e dispositivi di bassa qualità possono produrre dati e, pertanto, aggiornamenti del modello nettamente diversi da quelli di alta qualità\nUna tastiera intelligente addestrata per eseguire la correzione automatica. Se si ha una quantità sproporzionata di dispositivi da una determinata regione, lo slang, la struttura della frase o persino il linguaggio che stavano usando potrebbero deviare più aggiornamenti verso un certo stile di digitazione\nSe si hanno sensori per la fauna selvatica in aree remote, la connettività potrebbe non essere distribuita equamente, facendo sì che alcuni client in determinate regioni non siano in grado di inviare più aggiornamenti rispetto ad altri. Se quelle regioni hanno un’attività di fauna selvatica diversa da alcune specie, ciò potrebbe distorcere gli aggiornamenti verso quegli animali\n\nEsistono alcuni approcci per affrontare i dati non IID nell’apprendimento federato. Uno potrebbe essere quello di modificare l’algoritmo di aggregazione. Se si utilizza un algoritmo di aggregazione ponderato, è possibile regolarlo in base a diverse proprietà del client come regione, proprietà del sensore o connettività (Zhao et al. 2018).\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, e Vikas Chandra. 2018. «Federated learning with non-iid data». ArXiv preprint abs/1806.00582. https://arxiv.org/abs/1806.00582.\n\n\n12.5.6 Selezione del Client\nConsiderando tutti i fattori che influenzano l’efficacia dell’apprendimento federato, come i dati IID e la comunicazione, la selezione del client è una componente chiave per garantire che un sistema si alleni bene. La selezione dei client sbagliati può distorcere il dataset, con conseguenti dati non IID. Analogamente, la scelta casuale di client con cattive connessioni di rete può rallentare la comunicazione. Pertanto, è necessario considerare diverse caratteristiche chiave quando si seleziona il sottoinsieme corretto di client.\nQuando si selezionano i client, ci sono tre componenti principali da considerare: eterogeneità dei dati, allocazione delle risorse e costo della comunicazione. Possiamo selezionare i client in base alle metriche proposte in precedenza nella sezione non IID per affrontare l’eterogeneità dei dati. Nell’apprendimento federato, tutti i dispositivi possono avere diverse quantità di elaborazione, con il risultato che alcuni sono più inefficienti di altri nell’addestramento. Quando si seleziona un sottoinsieme di client per l’addestramento, si deve considerare un equilibrio tra eterogeneità dei dati e risorse disponibili. In uno scenario ideale, è sempre possibile selezionare il sottoinsieme di client con le maggiori risorse. Tuttavia, questo potrebbe distorcere il set di dati, quindi è necessario trovare un equilibrio. Le differenze di comunicazione aggiungono un altro layer; si desidera evitare di essere bloccati dall’attesa che i dispositivi con connessioni scadenti trasmettano tutti i loro aggiornamenti. Pertanto, è anche necessario considerare la scelta di un sottoinsieme di dispositivi diversi ma ben collegati.\n\n\n12.5.7 Un Esempio di Apprendimento Federato Distribuito: G board\nUn esempio primario di un sistema di apprendimento federato distribuito è la tastiera di Google, Gboard, per dispositivi Android. Nell’implementare l’apprendimento federato per la tastiera, Google si è concentrata sull’impiego di tecniche di privacy differenziali per proteggere i dati e l’identità dell’utente. Gboard sfrutta modelli linguistici per diverse funzionalità chiave, come Next Word Prediction (NWP), Smart Compose (SC) e On-The-Fly rescoring (OTF) (Xu et al. 2023), come mostrato in Figura 12.7.\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A Choquette-Choo, Peter Kairouz, H Brendan McMahan, Jesse Rosenstock, e Yuanbo Zhang. 2023. «Federated Learning of Gboard Language Models with Differential Privacy». ArXiv preprint abs/2305.18465. https://arxiv.org/abs/2305.18465.\nNWP anticiperà la parola successiva che l’utente tenta di digitare in base a quella precedente. SC fornisce suggerimenti in linea per velocizzare la digitazione in base a ciascun carattere. OTF riclassificherà le parole successive proposte in base al processo di digitazione attivo. Tutti e tre questi modelli devono essere eseguiti rapidamente sull’edge e l’apprendimento federato può accelerare l’addestramento sui dati degli utenti. Tuttavia, caricare ogni parola digitata da un utente sul cloud per l’addestramento costituirebbe una violazione massiccia della privacy. Pertanto, l’apprendimento federato enfatizza la privacy differenziale, che protegge l’utente consentendo al contempo una migliore esperienza utente.\n\n\n\n\n\n\nFigura 12.7: Funzionalità di Google G Board. Fonte: Zheng et al., (2023).\n\n\n\nPer raggiungere questo obiettivo, Google ha impiegato il suo algoritmo DP-FTRL, che fornisce una garanzia formale che i modelli addestrati non memorizzeranno dati o identità utente specifici. La progettazione del sistema dell’algoritmo è mostrata in Figura 12.8. DP-FTRL, combinato con l’aggregazione sicura, crittografa gli aggiornamenti del modello e fornisce un equilibrio ottimale tra privacy e utilità. Inoltre, il clipping adattivo viene applicato nel processo di aggregazione per limitare l’impatto dei singoli utenti sul modello globale (passaggio 3 in Figura 12.8). Combinando tutte queste tecniche, Google può perfezionare continuamente la sua tastiera preservando al contempo la privacy dell’utente in un modo formalmente dimostrabile.\n\n\n\n\n\n\nFigura 12.8: Privacy Differenziale in G Board. Fonte: Zheng et al., (2023).\n\n\n\n\n\n\n\n\n\nEsercizio 12.2: Apprendimento Federato - Generazione di Testo\n\n\n\n\n\nAvete mai usato quelle tastiere intelligenti che suggeriscono la parola successiva? Con l’apprendimento federato, possiamo renderle ancora migliori senza sacrificare la privacy. In questo Colab, insegneremo a un’IA a prevedere le parole tramite l’addestramento su dati di testo distribuiti su più dispositivi. Prepariamoci a rendere la digitazione ancora più fluida!\n\n\n\n\n\n\n\n\n\n\nEsercizio 12.3: Apprendimento Federato - Classificazione delle Immagini\n\n\n\n\n\nVogliamo addestrare un’IA esperta di immagini senza inviare le proprie foto al cloud? L’apprendimento federato è la risposta! In questo Colab, addestreremo un modello su più dispositivi, ognuno dei quali apprende dalle proprie immagini. La privacy è protetta e il lavoro di squadra fa funzionare il sogno dell’IA!\n\n\n\n\n\n\n12.5.8 Benchmarking per l’Apprendimento Federato: MedPerf\nUno degli esempi più ricchi di dati edge sono i dispositivi medici. Questi dispositivi memorizzano alcuni dei dati più personali degli utenti, ma offrono enormi progressi nel trattamento personalizzato e una migliore accuratezza nell’intelligenza artificiale medica. Dati questi due fattori, i dispositivi medici sono il caso d’uso perfetto per l’apprendimento federato. MedPerf è una piattaforma open source utilizzata per confrontare i modelli utilizzando la valutazione federata (Karargyris et al. 2023). Invece di addestrare semplicemente i modelli tramite apprendimento federato, MedPerf porta il modello sui dispositivi edge per testarlo rispetto ai dati personalizzati, preservando al contempo la privacy. In questo modo, un comitato di benchmark può valutare vari modelli nel mondo reale sui dispositivi edge, preservando comunque l’anonimato del paziente.\n\nKarargyris, Alexandros, Renato Umeton, Micah J Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023. «Federated benchmarking of medical artificial intelligence with MedPerf». Nature Machine Intelligence 5 (7): 799–810. https://doi.org/10.1038/s42256-023-00652-2.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#problemi-di-sicurezza",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#problemi-di-sicurezza",
    "title": "12  Apprendimento On-Device",
    "section": "12.6 Problemi di Sicurezza",
    "text": "12.6 Problemi di Sicurezza\nL’esecuzione di training e adattamento del modello ML sui dispositivi degli utenti finali introduce anche rischi per la sicurezza che devono essere affrontati. Alcune preoccupazioni chiave per la sicurezza includono:\n\nEsposizione di dati privati: I dati di training potrebbero essere trapelati o rubati dai dispositivi\nAvvelenamento dei dati: Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello\nEstrazione del modello: Degli aggressori potrebbero tentare di rubare i parametri del modello addestrato\nInferenza di appartenenza: I modelli potrebbero rivelare la partecipazione di dati di utenti specifici\nAttacchi di evasione: Input appositamente creati possono causare una classificazione errata\n\nQualsiasi sistema che esegue l’apprendimento sul dispositivo introduce preoccupazioni per la sicurezza, poiché potrebbe esporre vulnerabilità in modelli su larga scala. Numerosi rischi per la sicurezza sono associati a qualsiasi modello ML, ma questi rischi hanno conseguenze specifiche per l’apprendimento on-device. Fortunatamente, esistono metodi per mitigare questi rischi e migliorare le prestazioni reali dell’apprendimento su dispositivo.\n\n12.6.1 Avvelenamento dei Dati\nL’apprendimento automatico on-device introduce sfide uniche per la sicurezza dei dati rispetto all’addestramento tradizionale basato su cloud. In particolare, gli attacchi di avvelenamento dei dati rappresentano una seria minaccia durante l’apprendimento su dispositivo. Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello quando vengono distribuiti.\nEsistono diverse tecniche di attacco di avvelenamento dei dati:\n\nLabel Flipping: Comporta l’applicazione di etichette errate ai campioni. Ad esempio, nella classificazione delle immagini, le foto di gatti possono essere etichettate come cani per confondere il modello. Anche capovolgere il 10% delle etichette può avere conseguenze significative sul modello.\nInserimento dei Dati: Introduce input falsi o distorti nel set di training. Ciò potrebbe includere immagini pixelate, audio rumoroso o testo distorto.\nCorruzione Logica: Altera i [pattern] sottostanti (https://www.worldscientific.com/doi/10.1142/S0218001414600027) nei dati per fuorviare il modello. Nell’analisi del “sentiment”, le recensioni altamente negative possono essere contrassegnate come positive tramite questa tecnica. Per questo motivo, recenti sondaggi hanno dimostrato che molte aziende hanno più paura dell’avvelenamento dei dati rispetto ad altre preoccupazioni di ML avversarie.\n\nCiò che rende l’avvelenamento dei dati allarmante è il modo in cui sfrutta la discrepanza tra set di dati curati e dati di training in tempo reale. Consideriamo un set di dati di foto di gatti raccolti da Internet. Nelle settimane successive, quando questi dati addestrano un modello on-device, le nuove foto di gatti sul Web differiscono in modo significativo.\nCon l’avvelenamento dei dati, gli aggressori acquistano domini e caricano contenuti che influenzano una parte dei dati di training. Anche piccole modifiche ai dati hanno un impatto significativo sul comportamento appreso dal modello. Di conseguenza, l’avvelenamento può instillare pregiudizi razzisti, sessisti o altri pregiudizi dannosi se non controllato.\nMicrosoft Tay è un chatbot lanciato da Microsoft nel 2016. È stato progettato per imparare dalle sue interazioni con gli utenti su piattaforme di social media come Twitter. Sfortunatamente, Microsoft Tay è diventato un esempio lampante di avvelenamento dei dati nei modelli di ML. Entro 24 ore dal suo lancio, Microsoft ha dovuto mettere Tay offline perché aveva iniziato a produrre messaggi offensivi e inappropriati, tra cui incitamento all’odio e commenti razzisti. Ciò è accaduto perché alcuni utenti sui social media hanno intenzionalmente fornito a Tay input dannosi e offensivi, da cui il chatbot ha poi imparato e incorporato nelle sue risposte.\nQuesto incidente è un chiaro esempio di avvelenamento dei dati, poiché i malintenzionati hanno intenzionalmente manipolato i dati utilizzati per addestrare il chatbot e modellarne le risposte. L’avvelenamento dei dati ha portato il chatbot ad adottare pregiudizi dannosi e a produrre output che i suoi sviluppatori non avevano previsto. Dimostra come anche piccole quantità di dati creati in modo dannoso possano avere un impatto significativo sul comportamento dei modelli ML e sottolinea l’importanza di implementare solidi meccanismi di filtraggio e convalida dei dati per impedire che tali incidenti si verifichino.\nTali pregiudizi potrebbero avere pericolosi impatti nel mondo reale. La convalida rigorosa dei dati, il rilevamento delle anomalie e il monitoraggio della provenienza dei dati sono misure difensive fondamentali. L’adozione di framework come Five Safes garantisce che i modelli siano addestrati su dati rappresentativi di alta qualità (Desai et al. 2016).\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. «Five Safes: Designing data access for research». Economics Working Paper Series 1601: 28.\nL’avvelenamento dei dati è una preoccupazione urgente per l’apprendimento sicuro sul dispositivo poiché i dati dell’endpoint non possono essere facilmente monitorati in tempo reale. Se ai modelli viene consentito di adattarsi da soli, corriamo il rischio che il dispositivo agisca in modo dannoso. Tuttavia, è necessaria una ricerca continua sull’apprendimento automatico avversario per sviluppare soluzioni efficaci per rilevare e mitigare tali attacchi ai dati.\n\n\n12.6.2 Attacchi Avversari\nDurante la fase di addestramento, gli aggressori potrebbero iniettare dati dannosi nel dataset di training, il che può alterare sottilmente il comportamento del modello. Ad esempio, un aggressore potrebbe aggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini. Se fatto in modo intelligente, l’accuratezza del modello potrebbe non diminuire in modo significativo e l’attacco potrebbe essere notato. Il modello classificherebbe quindi erroneamente alcuni gatti come cani, il che potrebbe avere conseguenze a seconda dell’applicazione.\nIn un sistema di telecamere di sicurezza embedded, ad esempio, ciò potrebbe consentire a un intruso di evitare il rilevamento indossando uno specifico pattern che il modello è stato ingannato a classificare come non minaccioso.\nDurante la fase di inferenza, gli aggressori possono utilizzare esempi avversari per ingannare il modello. Gli esempi avversari sono input che sono stati leggermente alterati in un modo da far sì che il modello faccia previsioni errate. Ad esempio, un aggressore potrebbe aggiungere una piccola quantità di rumore a un’immagine in un modo che un sistema di riconoscimento facciale identifichi erroneamente una persona. Questi attacchi possono essere particolarmente preoccupanti nelle applicazioni in cui è in gioco la sicurezza, come i veicoli autonomi. Un esempio concreto di ciò è quando i ricercatori sono riusciti a far sì che un sistema di riconoscimento della segnaletica stradale classificasse erroneamente un segnale di stop come un segnale di limite di velocità. Questo tipo di classificazione errata potrebbe causare incidenti se si verificasse in un sistema di guida autonoma nel mondo reale.\nPer mitigare questi rischi, possono essere impiegate diverse difese:\n\nValidazione e Sanificazione dei Dati: Prima di incorporare nuovi dati nel dataset di addestramento, questi devono essere convalidati e sanificati a fondo per garantire che non siano dannosi.\nAddestramento Avversario: Il modello può essere addestrato su esempi avversari per renderlo più robusto a questi tipi di attacchi.\nValidazione degli Input: Durante l’inferenza, gli input devono essere convalidati per garantire che non siano stati manipolati per creare esempi avversari.\nAudit e Monitoraggio Regolari: L’audit e il monitoraggio regolari del comportamento del modello possono aiutare a rilevare e mitigare gli attacchi avversari. Tuttavia, è più facile a dirsi che a farsi nel contesto di piccoli sistemi ML. Spesso è difficile monitorare i sistemi ML embedded all’endpoint a causa delle limitazioni della larghezza di banda della comunicazione, di cui parleremo nel capitolo MLOps.\n\nComprendendo i potenziali rischi e implementando queste difese, possiamo contribuire a proteggere il training on-device all’endpoint/edge e mitigare l’impatto degli attacchi avversari. La maggior parte delle persone confonde facilmente l’avvelenamento dei dati e gli attacchi avversari. Quindi Tabella 12.2 confronta l’avvelenamento dei dati e gli attacchi avversari:\n\n\n\nTabella 12.2: Confronto tra avvelenamento dei dati e attacchi avversari.\n\n\n\n\n\n\n\n\n\n\nAspetto\nAvvelenamento dei dati\nAttacchi avversari\n\n\n\n\nTempistica\nFase di addestramento\nFase di inferenza\n\n\nTarget\nDati di addestramento\nDati di input\n\n\nObiettivo\nInfluenza negativamente le prestazioni del modello\nCausa previsioni errate\n\n\nMetodo\nInserire esempi dannosi nei dati di training, spesso con etichette errate\nAggiungere rumore attentamente elaborato ai dati di input\n\n\nEsempio\nAggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini\nAggiungere una piccola quantità di rumore a un’immagine in modo che un sistema di riconoscimento facciale identifichi erroneamente una persona\n\n\nEffetti Potenziali\nIl modello apprende modelli errati e fa previsioni errate\nPrevisioni errate immediate e potenzialmente pericolose\n\n\nApplicazioni Interessate\nQualsiasi modello ML\nVeicoli autonomi, sistemi di sicurezza, ecc.\n\n\n\n\n\n\n\n\n12.6.3 Inversione del Modello\nGli attacchi di inversione del modello rappresentano una minaccia per la privacy dei modelli di machine learning su dispositivo addestrati su dati utente sensibili (Nguyen et al. 2023). Comprendere questo vettore di attacco e le strategie di mitigazione saranno importanti per creare un’intelligenza artificiale su dispositivo sicura ed etica. Ad esempio, si immagini un’app per iPhone che utilizza l’apprendimento su dispositivo per categorizzare le foto in gruppi come “spiaggia”, “cibo” o “selfie” per una ricerca più semplice.\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, e Ngai-Man Cheung. 2023. «Re-Thinking Model Inversion Attacks Against Deep Neural Networks». In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 16384–93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\nIl modello su dispositivo potrebbe essere addestrato da Apple su un set di dati di foto iCloud di utenti consenzienti. Un aggressore malintenzionato potrebbe tentare di estrarre parti di quelle foto di addestramento iCloud originali utilizzando l’inversione del modello. In particolare, l’aggressore inserisce input sintetici creati ad arte nel classificatore di foto su dispositivo. Modificando gli input sintetici e osservando come il modello li categorizza, possono perfezionare gli input fino a ricostruire copie dei dati di training originali, come una foto di una spiaggia dall’iCloud di un utente. Ora, l’aggressore ha violato la privacy di quell’utente ottenendo una delle sue foto senza consenso. Questo dimostra perché l’inversione del modello è pericolosa: può potenzialmente far trapelare dati di training altamente sensibili.\nLe foto sono un tipo di dati particolarmente rischioso perché spesso contengono persone identificabili, informazioni sulla posizione e momenti privati. Tuttavia, la stessa metodologia di attacco potrebbe essere applicata ad altri dati personali, come registrazioni audio, messaggi di testo o dati sanitari degli utenti.\nPer difendersi dall’inversione del modello, sarebbe necessario prendere precauzioni come l’aggiunta di rumore agli output del modello o l’utilizzo di tecniche di apprendimento automatico che preservano la privacy come l’apprendimento federato per addestrare il modello sul dispositivo. L’obiettivo è impedire agli aggressori di ricostruire i dati di training originali.\n\n\n12.6.4 Problemi di Sicurezza dell’Apprendimento On-Device\nSebbene l’avvelenamento dei dati e gli attacchi avversari siano preoccupazioni comuni per i modelli ML in generale, l’apprendimento su dispositivo introduce rischi di sicurezza unici. Quando vengono pubblicate varianti su dispositivo di modelli su larga scala, gli avversari possono sfruttare questi modelli più piccoli per attaccare le loro controparti più grandi. La ricerca ha dimostrato che man mano che i modelli su dispositivo e i modelli su scala reale diventano più simili, la vulnerabilità dei modelli originali su larga scala aumenta in modo significativo. Ad esempio, le valutazioni su 19 reti neurali profonde (DNN) hanno rivelato che lo sfruttamento dei modelli su dispositivo potrebbe aumentare la vulnerabilità dei modelli originali su larga scala di fino a 100 volte.\nEsistono tre tipi principali di rischi per la sicurezza specifici dell’apprendimento on-device:\n\nAttacchi Basati sul Trasferimento: Questi attacchi sfruttano la proprietà di trasferibilità tra un modello surrogato (un’approssimazione del modello di destinazione, simile a un modello su dispositivo) e un modello di destinazione remoto (il modello originale su scala reale). Gli aggressori generano esempi avversari utilizzando il modello surrogato, che può quindi essere utilizzato per ingannare il modello di destinazione. Ad esempio, si immagini un modello on-device progettato per identificare le e-mail di spam. Un aggressore potrebbe usare questo modello per generare un’e-mail di spam che non viene rilevata dal sistema di filtraggio più grande e completo.\nAttacchi Basati sull’Ottimizzazione: Questi attacchi generano esempi avversari per attacchi basati sul trasferimento usando una qualche forma di funzione obiettivo e modificano iterativamente gli input per ottenere il risultato desiderato. Gli attacchi di stima del gradiente, ad esempio, approssimano il gradiente del modello usando output di query (come punteggi di confidenza softmax), mentre gli attacchi senza gradiente usano la decisione finale del modello (la classe prevista) per approssimare il gradiente, sebbene richiedano molte più query.\nAttacchi di Query con Priorità di Trasferimento: Questi attacchi combinano elementi di attacchi basati sul trasferimento e basati sull’ottimizzazione. Eseguono il reverse engineering dei modelli sul dispositivo per fungere da surrogati del modello completo di destinazione. In altre parole, gli aggressori usano il modello sul dispositivo più piccolo per capire come funziona il modello più grande e quindi usano questa conoscenza per attaccare il modello completo.\n\nGrazie alla comprensione di questi rischi specifici associati all’apprendimento on-device, possiamo sviluppare protocolli di sicurezza più solidi per proteggere sia i modelli on-device che quelli su scala reale da potenziali attacchi.\n\n\n12.6.5 Attenuazione dei Rischi dell’Apprendimento On-Device\nSi possono impiegare vari metodi per mitigare i numerosi rischi per la sicurezza associati all’apprendimento on-device. Questi metodi possono essere specifici per il tipo di attacco o fungere da strumento generale per rafforzare la sicurezza.\nUna strategia per ridurre i rischi per la sicurezza è quella di ridurre la somiglianza tra modelli on-device e modelli su scala reale, riducendo così la trasferibilità fino al 90%. Questo metodo, noto come similarity-unpairing, affronta il problema che si verifica quando gli avversari sfruttano la somiglianza del gradiente di input tra i due modelli. Ottimizzando il modello su scala reale per creare una nuova versione con accuratezza simile ma gradienti di input diversi, possiamo costruire il modello on-device quantizzando questo modello su scala reale aggiornato. Questa disassociazione riduce la vulnerabilità dei modelli su dispositivo limitando l’esposizione del modello su scala reale originale. È importante notare che l’ordine di ottimizzazione e quantizzazione può essere variato pur ottenendo la mitigazione del rischio (Hong, Carlini, e Kurakin 2023).\nPer contrastare l’avvelenamento dei dati, è fondamentale reperire set di dati da fornitori affidabili e fidati.\nPer combattere gli attacchi avversari, si possono impiegare diverse strategie. Un approccio proattivo prevede la generazione di esempi avversari e la loro incorporazione nel set di dati di training del modello, rafforzando così il modello contro tali attacchi. Strumenti come CleverHans, una libreria di training open source, sono fondamentali per creare esempi avversari. La “Defense distillation” [distillazione della difesa] è un’altra strategia efficace, in cui il modello sul dispositivo genera probabilità di classificazioni diverse anziché decisioni definitive (Hong, Carlini, e Kurakin 2023), rendendo più difficile per gli esempi avversari sfruttare il modello.\n\nHong, Sanghyun, Nicholas Carlini, e Alexey Kurakin. 2023. «Publishing Efficient On-device Models Increases Adversarial Vulnerability». In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), 271–90. IEEE; IEEE. https://doi.org/10.1109/satml54575.2023.00026.\nIl furto di proprietà intellettuale è un altro problema significativo quando si distribuiscono modelli on-device. Il furto di proprietà intellettuale è un problema quando si distribuiscono modelli on-device, poiché gli avversari potrebbero tentare di sottoporre a reverse engineering il modello per rubare la tecnologia sottostante. Per proteggersi dal furto di proprietà intellettuale, l’eseguibile binario del modello addestrato dovrebbe essere archiviato su un’unità microcontrollore con software crittografato e interfacce fisiche protette del chip. Inoltre, il set di dati finale utilizzato per l’addestramento del modello dovrebbe essere mantenuto privato.\nInoltre, i modelli on-device utilizzano spesso set di dati noti o open source, come Visual Wake Words di MobileNet. Pertanto, è importante mantenere la privacy del set di dati finale utilizzato per l’addestramento del modello. Inoltre, proteggere il processo di “data augmentation” e incorporare casi d’uso specifici può ridurre al minimo il rischio di reverse engineering di un modello on-device.\nInfine, l’Adversarial Threat Landscape for Artificial Intelligence Systems (ATLAS) funge da prezioso strumento matriciale che aiuta a valutare il profilo di rischio dei modelli su dispositivo, consentendo agli sviluppatori di identificare e mitigare i potenziali rischi in modo proattivo.\n\n\n12.6.6 Protezione dei Dati di Training\nEsistono vari modi per proteggere i dati di training sul dispositivo. Ogni concetto è molto profondo e potrebbe valere una lezione a sé stante. Quindi, qui, faremo un breve accenno a quei concetti in modo che si sappia cosa approfondire.\n\nCrittografia\nLa crittografia funge da prima linea di difesa per i dati di training. Ciò comporta l’implementazione della crittografia end-to-end per l’archiviazione locale su dispositivi e canali di comunicazione per impedire l’accesso non autorizzato ai dati di training grezzi. Ambienti di esecuzione affidabili, come Intel SGX e ARM TrustZone, sono essenziali per facilitare il training sicuro su dati crittografati.\nInoltre, quando si aggregano aggiornamenti da più dispositivi, è possibile impiegare protocolli di elaborazione “multi-party” sicuri per migliorare la sicurezza (Kairouz, Oh, e Viswanath 2015); un’applicazione pratica di ciò è nell’apprendimento collaborativo on-device, in cui è possibile implementare l’aggregazione crittografica che preserva la privacy degli aggiornamenti del modello utente. Questa tecnica nasconde efficacemente i dati dei singoli utenti anche durante la fase di aggregazione.\n\nKairouz, Peter, Sewoong Oh, e Pramod Viswanath. 2015. «Secure Multi-party Differential Privacy». In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, a cura di Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, e Roman Garnett, 2008–16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nPrivacy Differenziale\nLa privacy differenziale è un’altra strategia cruciale per proteggere i dati di training. Iniettando rumore statistico calibrato nei dati, possiamo mascherare i singoli record estraendo comunque preziosi pattern di popolazione (Dwork e Roth 2013). Anche la gestione del budget per la privacy su più iterazioni di training e la riduzione del rumore man mano che il modello converge sono essenziali (Abadi et al. 2016). Possono essere impiegati metodi come la privacy differenziale formalmente dimostrabile, che può includere l’aggiunta di rumore di Laplace o gaussiano scalato alla sensibilità del set di dati.\n\nDwork, Cynthia, e Aaron Roth. 2013. «The Algorithmic Foundations of Differential Privacy». Foundations and Trends in Theoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nRilevamento delle Anomalie\nIl rilevamento delle anomalie svolge un ruolo importante nell’identificazione e nell’attenuazione di potenziali attacchi di avvelenamento dei dati. Ciò può essere ottenuto tramite analisi statistiche come la “Principal Component Analysis (PCA)” [analisi delle componenti principali] e il clustering, che aiutano a rilevare deviazioni nei dati di training aggregati. I metodi di serie temporali come i grafici Cumulative Sum (CUSUM) sono utili per identificare spostamenti indicativi di potenziale avvelenamento. Anche il confronto delle distribuzioni dei dati correnti con distribuzioni di dati pulite precedentemente visualizzate può aiutare a segnalare anomalie. Inoltre, i batch sospetti di essere avvelenati dovrebbero essere rimossi dal processo di aggregazione degli aggiornamenti di training. Ad esempio, è possibile condurre controlli a campione su sottoinsiemi di immagini di training sui dispositivi utilizzando hash photoDNA per identificare input avvelenati.\n\n\nValidazione dei Dati di Input\nInfine, la convalida dei dati di input è essenziale per garantire l’integrità e la validità dei dati di input prima che vengano immessi nel modello di training, proteggendo così dai payload avversari. Misure di similarità, come la distanza del coseno, possono essere impiegate per catturare input che si discostano in modo significativo dalla distribuzione prevista. Gli input sospetti che potrebbero contenere payload avversari devono essere messi in quarantena e sanificati. Inoltre, l’accesso del parser ai dati di training deve essere limitato solo ai percorsi di codice convalidati. Sfruttare le funzionalità di sicurezza hardware, come ARM Pointer Authentication, può impedire la corruzione della memoria (ARM Limited, 2023). Un esempio di ciò è l’implementazione di controlli di integrità degli input sui dati di training audio utilizzati dagli smart speaker prima dell’elaborazione da parte del modello di riconoscimento vocale (Z. Chen e Xu 2023).\n\nChen, Zhiyong, e Shugong Xu. 2023. «Learning domain-heterogeneous speaker recognition systems with personalized continual federated learning». EURASIP Journal on Audio, Speech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#framework-di-training-on-device",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#framework-di-training-on-device",
    "title": "12  Apprendimento On-Device",
    "section": "12.7 Framework di Training On-Device",
    "text": "12.7 Framework di Training On-Device\nFramework di inferenza embedded come TF-Lite Micro (David et al. 2021), TVM (T. Chen et al. 2018) e MCUNet (Lin et al. 2020) forniscono un runtime snello per l’esecuzione di modelli di reti neurali su microcontrollori e altri dispositivi con risorse limitate. Tuttavia, non supportano l’addestramento on-device. L’addestramento richiede un proprio set di strumenti specializzati a causa dell’impatto della quantizzazione sul calcolo del gradiente e dell’ingombro di memoria della backpropagation (Lin et al. 2022).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. «TVM: An automated End-to-End optimizing compiler for deep learning». In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2022. «On-device training under 256kb memory». Adv. Neur. In. 35: 22941–54.\nNegli ultimi anni, hanno iniziato a emergere una manciata di strumenti e framework che consentono l’addestramento sul dispositivo. Tra questi Tiny Training Engine (Lin et al. 2022), TinyTL (Cai et al. 2020) e TinyTrain (Kwon et al. 2023).\n\n12.7.1 Tiny Training Engine\nTiny Training Engine (TTE) utilizza diverse tecniche per ottimizzare l’utilizzo della memoria e velocizzare il processo di training. Una panoramica del flusso di lavoro TTE è mostrata in Figura 12.9. Innanzitutto, TTE scarica la differenziazione automatica in fase di compilazione anziché in fase di runtime, riducendo significativamente il sovraccarico durante il training. In secondo luogo, TTE esegue l’ottimizzazione del grafo come la potatura e gli aggiornamenti sparsi per ridurre i requisiti di memoria e accelerare i calcoli.\n\n\n\n\n\n\nFigura 12.9: Flusso di lavoro di TTE.\n\n\n\nIn particolare, TTE segue quattro passaggi principali:\n\nDurante la fase di compilazione, TTE traccia il grafo di propagazione “forward” e deriva il grafo “backward” corrispondente per la backpropagation. Ciò consente alla differenziazione di avvenire in fase di compilazione anziché in fase di esecuzione.\nTTE elimina tutti i nodi che rappresentano pesi congelati dal grafo backward. I pesi congelati sono pesi che non vengono aggiornati durante l’addestramento per ridurre l’impatto di determinati neuroni. La potatura dei loro nodi consente di risparmiare memoria.\nTTE riordina gli operatori di discesa del gradiente per intercalarli con i calcoli del passaggio del backward. Questa pianificazione riduce al minimo le “impronte” [occupazione] di memoria.\nTTE utilizza la generazione di codice per compilare i grafi “forward” e “backward” ottimizzati, che vengono poi distribuiti per l’addestramento on-device.\n\n\n\n12.7.2 Tiny Transfer Learning\nTiny Transfer Learning (TinyTL) consente un training efficiente in termini di memoria sul dispositivo tramite una tecnica chiamata congelamento dei pesi. Durante il training, gran parte del collo di bottiglia della memoria deriva dall’archiviazione delle attivazioni intermedie e dall’aggiornamento dei pesi nella rete neurale.\nPer ridurre questo sovraccarico di memoria, TinyTL congela la maggior parte dei pesi in modo che non debbano essere aggiornati durante il training. Ciò elimina la necessità di archiviare le attivazioni intermedie per le parti congelate della rete. TinyTL ottimizza solo i termini di bias, che sono molto più piccoli dei pesi. Una panoramica del flusso di lavoro TinyTL è mostrata in Figura 12.10.\n\n\n\n\n\n\nFigura 12.10: Flusso di lavoro di TinyTL. Fonte: Cai et al. (2020).)\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, e Song Han. 2020. «TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nI pesi di congelamento si applicano a layer completamente connessi, nonché a layer di normalizzazione e convoluzionali. Tuttavia, solo l’adattamento dei bias limita la capacità del modello di apprendere e adattarsi a nuovi dati.\nPer aumentare l’adattabilità senza molta memoria aggiuntiva, TinyTL utilizza un piccolo modello di apprendimento residuo. Questo affina le mappe delle feature intermedie per produrre output migliori, anche con pesi fissi. Il modello residuo introduce un overhead minimo, inferiore al 3,8% in più rispetto al modello di base.\nCongelando la maggior parte dei pesi, TinyTL riduce significativamente l’utilizzo della memoria durante l’addestramento on-device. Il modello residuo consente quindi di adattarsi e apprendere in modo efficace per l’attività. L’approccio combinato fornisce un addestramento on-device efficiente in termini di memoria con un impatto minimo sulla precisione del modello.\n\n\n12.7.3 Tiny Train\nTinyTrain riduce significativamente il tempo necessario per l’addestramento sul dispositivo aggiornando selettivamente solo determinate parti del modello. Ciò avviene utilizzando una tecnica chiamata aggiornamento sparso adattivo all’attività, come mostrato in Figura 12.11.\nIn base ai dati utente, alla memoria e al calcolo disponibili sul dispositivo, TinyTrain sceglie dinamicamente quali layer della rete neurale aggiornare durante l’addestramento. Questa selezione di layer è ottimizzata per ridurre l’utilizzo di calcolo e memoria mantenendo un’elevata accuratezza.\n\n\n\n\n\n\nFigura 12.11: Flusso di lavoro di TinyTrain. Fonte: Kwon et al. (2023).\n\n\nKwon, Young D, Rui Li, Stylianos I Venieris, Jagmohan Chauhan, Nicholas D Lane, e Cecilia Mascolo. 2023. «TinyTrain: Deep Neural Network Training at the Extreme Edge». ArXiv preprint abs/2307.09988. https://arxiv.org/abs/2307.09988.\n\n\nPiù specificamente, TinyTrain esegue prima il pre-addestramento offline del modello. Durante il pre-addestramento, non solo addestra il modello sui dati dell’attività, ma anche il meta-addestramento del modello. Meta-addestramento significa addestrare il modello sui metadati relativi al processo di addestramento stesso. Questo meta-addestramento migliora la capacità del modello di adattarsi in modo accurato anche quando sono disponibili dati limitati per l’attività target.\nPoi, durante la fase di adattamento online, quando il modello viene personalizzato sul dispositivo, TinyTrain esegue aggiornamenti adattivi sparsi all’attività. Utilizzando i criteri relativi alle capacità del dispositivo, seleziona solo determinati layer da aggiornare tramite backpropagation. I layer vengono scelti per bilanciare accuratezza, utilizzo della memoria e tempo di elaborazione.\nAggiornando in modo sparso i layer su misura per il dispositivo e l’attività, TinyTrain riduce significativamente il tempo di addestramento sul dispositivo e l’utilizzo delle risorse. Il meta-training offline migliora anche l’accuratezza quando si adatta a dati limitati. Insieme, questi metodi consentono un training on-device rapido, efficiente e accurato.\n\n\n12.7.4 Confronto\nTabella 12.3 riassume le principali somiglianze e differenze tra i diversi framework.\n\n\n\nTabella 12.3: Confronto di framework per l’ottimizzazione del training on-device.\n\n\n\n\n\n\n\n\n\n\nFramework\nSomiglianze\nDifferenze\n\n\n\n\nTiny Training Engine\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta potatura, sparsità, ecc.\n\n\nTraccia grafi forward & backward\nElimina i pesi congelati\nInterlaccia backprop e gradienti\nGenerazione di codice\n\n\n\nTinyTL\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta congelamento, sparsità, ecc.\n\n\nCongela la maggior parte dei pesi\nAdatta solo i bias\nUtilizza il modello residuo\n\n\n\nTinyTrain\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta sparsità, ecc.\n\n\nMeta-addestramento nel pre-addestramento\nAggiornamento sparse adattivo alle attività\nAggiornamento selettivo dei layer",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#conclusione",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#conclusione",
    "title": "12  Apprendimento On-Device",
    "section": "12.8 Conclusione",
    "text": "12.8 Conclusione\nIl concetto di apprendimento on-device [su dispositivo] è sempre più importante per aumentare l’usabilità e la scalabilità di TinyML. Questo capitolo ha esplorato le complessità dell’apprendimento on-device, esplorandone vantaggi e limiti, strategie di adattamento, algoritmi e tecniche chiave correlate, implicazioni di sicurezza e framework di training on-device esistenti ed emergenti.\nL’apprendimento su dispositivo è, senza dubbio, un paradigma rivoluzionario che porta con sé numerosi vantaggi per le distribuzioni ML embedded ed edge. Eseguendo il training direttamente sui dispositivi endpoint, si elimina la necessità di una connettività cloud continua, rendendolo particolarmente adatto per applicazioni IoT ed edge computing. Presenta vantaggi quali maggiore privacy, facilità di conformità ed efficienza delle risorse. Allo stesso tempo, l’apprendimento su on-device deve affrontare limitazioni legate a vincoli hardware, dimensioni dei dati limitate e ridotta accuratezza e generalizzazione del modello.\nMeccanismi quali la ridotta complessità del modello, tecniche di ottimizzazione e compressione dei dati e metodi di apprendimento correlati quali apprendimento tramite trasferimento e apprendimento federato consentono ai modelli di adattarsi per apprendere ed evolversi in base a vincoli di risorse, fungendo così da fondamento per un efficace ML sui dispositivi edge.\nLe problematiche critiche di sicurezza nell’apprendimento su dispositivo evidenziate in questo capitolo, che vanno dall’avvelenamento dei dati e dagli attacchi avversari ai rischi specifici introdotti dall’apprendimento on-device, devono essere affrontate in carichi di lavoro reali affinché l’apprendimento su dispositivo sia un paradigma praticabile. Strategie di mitigazione efficaci, quali convalida dei dati, crittografia, privacy differenziale, rilevamento delle anomalie e convalida dei dati di input, sono fondamentali per salvaguardare i sistemi di apprendimento on-device da queste minacce.\nL’emergere di framework di training specializzati on-device, come Tiny Training Engine, Tiny Transfer Learning e Tiny Train, offre strumenti pratici che consentono un training efficiente sui dispositivi. Questi framework impiegano varie tecniche per ottimizzare l’utilizzo della memoria, ridurre il sovraccarico computazionale e semplificare il processo di training on-device.\nIn conclusione, l’apprendimento on-device è in prima linea in TinyML, promettendo un futuro in cui i modelli possono acquisire autonomamente conoscenze e adattarsi ad ambienti mutevoli su dispositivi edge. L’applicazione dell’apprendimento on-device ha il potenziale per rivoluzionare vari ambiti, tra cui sanità, IoT industriale e città intelligenti. Tuttavia, il potenziale trasformativo dell’apprendimento on-device deve essere bilanciato con misure di sicurezza robuste per proteggere da violazioni dei dati e minacce avversarie. L’adozione di framework di training on-device innovativi e l’implementazione di protocolli di sicurezza rigorosi sono passaggi chiave per sbloccare il pieno potenziale dell’apprendimento su dispositivo. Man mano che questa tecnologia continua a evolversi, promette di rendere i nostri dispositivi più intelligenti, più reattivi e meglio integrati nella nostra vita quotidiana.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#sec-on-device-learning-resource",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#sec-on-device-learning-resource",
    "title": "12  Apprendimento On-Device",
    "section": "12.9 Risorse",
    "text": "12.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nIntro to TensorFlow Lite (TFLite).\nTFLite Optimization and Quantization.\nTFLite Quantization-Aware Training.\nTrasferimento dell’Apprendimento:\n\nTransfer Learning: with Visual Wake Words example.\nOn-device Training and Transfer Learning.\n\nAddestramento Distribuito:\n\nDistributed Training.\nDistributed Training.\n\nMonitoraggio Continuo:\n\nContinuous Evaluation Challenges for TinyML.\nFederated Learning Challenges.\nContinuous Monitoring with Federated ML.\nContinuous Monitoring Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 12.1\nVideo 12.2\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 12.1\nEsercizio 12.2\nEsercizio 12.3\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html",
    "href": "contents/ops/ops.it.html",
    "title": "13  Operazioni di ML",
    "section": "",
    "text": "13.1 Introduzione\nMachine Learning Operations (MLOps) è un approccio sistematico che combina machine learning (ML), data science e ingegneria del software per automatizzare il ciclo di vita end-to-end di ML. Ciò include tutto, dalla preparazione dei dati e dal training del modello alla distribuzione e alla manutenzione. MLOps garantisce che i modelli ML siano sviluppati, distribuiti e mantenuti in modo efficiente ed efficace.\nCominciamo prendendo un caso di esempio generale (ad esempio, ML non edge). Prendiamo in considerazione un’azienda di “ride sharing” che desidera distribuire un modello di machine learning per prevedere la domanda dei passeggeri in tempo reale. Il team di data science impiega mesi per sviluppare un modello, ma quando è il momento di distribuirlo, si rende conto che deve essere compatibile con l’ambiente di produzione del team di ingegneria. La distribuzione del modello richiede la ricostruzione da zero, il che comporta settimane di lavoro aggiuntivo. È qui che entra in gioco MLOps.\nCon MLOps, protocolli e strumenti, il modello sviluppato dal team di data science può essere distribuito e integrato senza problemi nell’ambiente di produzione. In sostanza, MLOps elimina gli attriti durante lo sviluppo, la distribuzione e la manutenzione dei sistemi ML. Migliora la collaborazione tra i team tramite flussi di lavoro e interfacce definiti. MLOps accelera anche la velocità di iterazione consentendo la distribuzione continua per i modelli ML.\nPer l’azienda di ride sharing, implementare MLOps significa che il loro modello di previsione della domanda può essere frequentemente riqualificato e distribuito in base ai nuovi dati in arrivo. Ciò mantiene il modello accurato nonostante il cambiamento del comportamento del passeggero. MLOps consente inoltre all’azienda di sperimentare nuove tecniche di modellazione poiché i modelli possono essere rapidamente testati e aggiornati.\nAltri vantaggi di MLOps includono il monitoraggio avanzato della discendenza del modello, la riproducibilità e l’auditing. La catalogazione dei flussi di lavoro ML e la standardizzazione degli artefatti, come il logging delle versioni del modello, il monitoraggio della discendenza dei dati e il confezionamento di modelli e parametri, consente una visione più approfondita della provenienza del modello. La standardizzazione di questi artefatti facilita la tracciabilità di un modello fino alle sue origini, la replica del processo di sviluppo del modello e l’esame di come una versione del modello è cambiata nel tempo. Ciò facilita anche la conformità alle normative, che è particolarmente critica in settori regolamentati come sanità e finanza, dove è importante essere in grado di verificare e spiegare i modelli.\nLe principali organizzazioni adottano MLOps per aumentare la produttività, aumentare la collaborazione e accelerare i risultati ML. Fornisce i framework, gli strumenti e le best practice per gestire efficacemente i sistemi ML durante il loro ciclo di vita. Ciò si traduce in modelli più performanti, tempi di realizzazione più rapidi e un vantaggio competitivo duraturo. Mentre esploriamo ulteriormente MLOps, si consideri come l’implementazione di queste pratiche può aiutare ad affrontare le sfide ML embedded oggi e in futuro.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#contesto-storico",
    "href": "contents/ops/ops.it.html#contesto-storico",
    "title": "13  Operazioni di ML",
    "section": "13.2 Contesto Storico",
    "text": "13.2 Contesto Storico\nMLOps affonda le sue radici in DevOps, un insieme di pratiche che combinano sviluppo software (Dev) e operazioni IT (Ops) per accorciare il ciclo di vita dello sviluppo e fornire una distribuzione “continua” di software di alta qualità. I parallelismi tra MLOps e DevOps sono evidenti nella loro attenzione all’automazione, alla collaborazione e al miglioramento continuo. In entrambi i casi, l’obiettivo è quello di abbattere i “silos” tra i diversi team (sviluppatori, operazioni e, nel caso di MLOps, data scientist e ingegneri ML) e creare un processo più snello ed efficiente. È utile comprendere meglio la storia di questa evoluzione per comprendere MLOps nel contesto dei sistemi tradizionali.\n\n13.2.1 DevOps\nIl termine “DevOps” è stato coniato per la prima volta nel 2009 da Patrick Debois, un consulente e professionista Agile. Debois ha organizzato la prima conferenza DevOpsDays a Ghent, in Belgio, nel 2009. La conferenza ha riunito professionisti dello sviluppo e delle operazioni per discutere di modi per migliorare la collaborazione e automatizzare i processi.\nDevOps ha le sue radici nel movimento Agile, iniziato nei primi anni 2000. Agile ha fornito le basi per un approccio più collaborativo allo sviluppo software e ha enfatizzato le piccole release iterative. Tuttavia, Agile si concentra principalmente sulla collaborazione tra team di sviluppo. Man mano che le metodologie Agile diventavano più popolari, le organizzazioni si sono rese conto della necessità di estendere questa collaborazione ai team operativi.\nLa natura isolata dei team di sviluppo e delle operazioni ha spesso portato a inefficienze, conflitti e ritardi nella distribuzione del software. Questa necessità di una migliore collaborazione e integrazione tra questi team ha portato al movimento DevOps. DevOps può essere visto come un’estensione dei principi Agile, inclusi i team operativi.\nI principi chiave di DevOps includono collaborazione, automazione, integrazione continua, distribuzione e feedback. DevOps si concentra sull’automazione dell’intera pipeline di distribuzione del software, dallo sviluppo alla distribuzione. Migliora la collaborazione tra i team di sviluppo e operativi, utilizzando strumenti come Jenkins, Docker e Kubernetes per semplificare il ciclo di vita dello sviluppo.\nMentre Agile e DevOps condividono principi comuni in materia di collaborazione e feedback, DevOps mira specificamente all’integrazione di sviluppo e operazioni IT, espandendo Agile oltre i soli team di sviluppo. Introduce pratiche e strumenti per automatizzare la distribuzione del software e migliorare la velocità e la qualità delle release del software.\n\n\n13.2.2 MLOps\nMLOps, d’altro canto, sta per Machine Learning Operations ed estende i principi di DevOps al ciclo di vita ML. MLOps automatizza e semplifica l’intero ciclo di vita dell’apprendimento automatico, dalla preparazione dei dati allo sviluppo del modello, fino all’implementazione e al monitoraggio. L’obiettivo principale di MLOps è facilitare la collaborazione tra data scientist, data engineer e operazioni IT e automatizzare la distribuzione, il monitoraggio e la gestione dei modelli ML. Alcuni fattori chiave hanno portato all’ascesa di MLOps.\n\nData drift: La deriva dei dati degrada le prestazioni del modello nel tempo, motivando la necessità di rigorosi monitoraggi e procedure di riqualificazione automatizzate fornite da MLOps.\nRiproducibilità: La mancanza di riproducibilità negli esperimenti di machine learning ha motivato i sistemi MLOps a tracciare codice, dati e variabili di ambiente per abilitare flussi di lavoro ML riproducibili.\nSpiegabilità: La natura di “scatola nera” e la mancanza di spiegabilità di modelli complessi hanno motivato la necessità di funzionalità MLOps per aumentare la trasparenza e la spiegabilità del modello.\nMonitoraggio: L’incapacità di monitorare in modo affidabile le prestazioni del modello dopo la distribuzione ha evidenziato la necessità di soluzioni MLOps con una solida strumentazione delle prestazioni del modello e avvisi.\nAttrito: L’attrito nel riaddestramento e nella distribuzione manuale dei modelli ha motivato la necessità di sistemi MLOps che automatizzano le pipeline di distribuzione dell’apprendimento automatico.\nOttimizzazione: La complessità della configurazione dell’infrastruttura di apprendimento automatico ha motivato la necessità di piattaforme MLOps con un’infrastruttura ML ottimizzata e pronta all’uso.\n\nSebbene DevOps e MLOps condividano l’obiettivo comune di automatizzare e semplificare i processi, differiscono significativamente in termini di attenzione e sfide. DevOps si occupa principalmente di sviluppo software e operazioni IT. Consente la collaborazione tra questi team e automatizza la distribuzione del software. Al contrario, MLOps si concentra sul ciclo di vita dell’apprendimento automatico. Affronta complessità aggiuntive come versioning dei dati, versioning dei modelli e monitoraggio dei modelli. MLOps richiede la collaborazione tra una gamma più ampia di stakeholder, tra cui data scientist, data engineer e IT operations. Va oltre l’ambito del DevOps tradizionale incorporando le sfide uniche della gestione dei modelli ML durante il loro ciclo di vita. Tabella 13.1 fornisce un confronto affiancato di DevOps e MLOps, evidenziandone le principali differenze e somiglianze.\n\n\n\nTabella 13.1: Confronto tra DevOps e MLOps.\n\n\n\n\n\n\n\n\n\n\nAspect\nDevOps\nMLOps\n\n\n\n\nObiettivo\nSemplificazione dei processi di sviluppo software e operativi\nOttimizzazione del ciclo di vita dei modelli di apprendimento automatico\n\n\nMetodologia\nIntegrazione continua e distribuzione continua (CI/CD) per lo sviluppo software\nSimile a CI/CD ma incentrato sui flussi di lavoro di apprendimento automatico\n\n\nStrumenti Principali\nControllo delle versioni (Git), strumenti CI/CD (Jenkins, Travis CI), gestione della configurazione (Ansible, Puppet)\nStrumenti di versioning dei dati, strumenti di training e deployment dei modelli, pipeline CI/CD su misura per ML\n\n\nProblemi Principali\nIntegrazione del codice, test, gestione delle release, automazione, infrastruttura come codice\nGestione dei dati, versioning dei modelli, monitoraggio degli esperimenti, deployment dei modelli, scalabilità dei flussi di lavoro ML\n\n\nRisultati Tipici\nRelease software più rapide e affidabili, collaborazione migliorata tra team di sviluppo e operativi\nGestione e deployment efficienti dei modelli di apprendimento automatico, collaborazione migliorata tra data scientist e ingegneri\n\n\n\n\n\n\nScoprire di più sui cicli di vita ML tramite un “case study” che presenta il riconoscimento vocale in Video 13.1.\n\n\n\n\n\n\nVideo 13.1: MLOps",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#componenti-chiave-di-mlops",
    "href": "contents/ops/ops.it.html#componenti-chiave-di-mlops",
    "title": "13  Operazioni di ML",
    "section": "13.3 Componenti Chiave di MLOps",
    "text": "13.3 Componenti Chiave di MLOps\nIn questo capitolo, forniremo una panoramica dei componenti principali di MLOps, un set emergente di pratiche che consente una distribuzione solida e una gestione del ciclo di vita dei modelli ML in produzione. Sebbene alcuni elementi MLOps, come l’automazione e il monitoraggio, siano stati trattati nei capitoli precedenti, li integreremo in un framework e approfondiremo funzionalità aggiuntive, come la governance. Inoltre, descriveremo e collegheremo gli strumenti più diffusi utilizzati in ogni componente, come LabelStudio per l’etichettatura dei dati. Alla fine, speriamo che abbiate compreso la metodologia MLOps end-to-end che porta i modelli dall’ideazione alla creazione di valore sostenibile all’interno delle organizzazioni.\nFigura 13.1 mostra lo stack di sistema MLOps. Il ciclo di vita MLOps inizia dalla gestione dei dati e dalle pipeline CI/CD per lo sviluppo del modello. I modelli sviluppati passano attraverso il training e la valutazione del modello. Una volta addestrati alla convergenza, la distribuzione del modello porta i modelli in produzione e pronti per essere serviti. Dopo la distribuzione, il servizio del modello reagisce alle modifiche del carico di lavoro e soddisfa gli accordi sul livello di servizio in modo economicamente conveniente quando serve milioni di utenti finali o applicazioni AI. La gestione dell’infrastruttura garantisce che le risorse necessarie siano disponibili e ottimizzate durante l’intero ciclo di vita. Monitoraggio continuo, governance, comunicazione e collaborazione sono i restanti elementi di MLOps per garantire uno sviluppo e un funzionamento senza interruzioni dei modelli ML.\n\n\n\n\n\n\nFigura 13.1: Lo stack MLOps, inclusi Modelli ML, Framework, Orchestrazione del Modello, Infrastruttura e Hardware, illustra il flusso di lavoro end-to-end di MLOps.\n\n\n\n\n13.3.1 Gestione dei Dati\nUna gestione dei dati solida e l’ingegneria dei dati potenziano attivamente implementazioni MLOps di successo. I team acquisiscono, archiviano e preparano correttamente i dati grezzi da sensori, database, app e altri sistemi per il training e la distribuzione dei modelli.\nI team monitorano attivamente le modifiche ai set di dati nel tempo utilizzando il controllo delle versioni con Git e strumenti come GitHub o GitLab. Gli scienziati dei dati collaborano alla cura dei set di dati unendo le modifiche di più collaboratori. I team possono rivedere o ripristinare ogni iterazione di un set di dati, se necessario.\nI team etichettano e annotano meticolosamente i dati utilizzando un software di etichettatura come LabelStudio, che consente ai team distribuiti di lavorare insieme all’etichettatura dei set di dati. Man mano che le variabili target e le convenzioni di etichettatura si evolvono, i team mantengono l’accessibilità alle versioni precedenti.\nI team archiviano il set di dati non elaborato e tutte le risorse derivate su servizi cloud come Amazon S3 o Google Cloud Storage. Questi servizi forniscono un’archiviazione scalabile e resiliente con funzionalità di controllo delle versioni. I team possono impostare autorizzazioni di accesso granulari.\nLe pipeline di dati robuste create dai team automatizzano l’estrazione, l’unione, la pulizia e la trasformazione dei dati grezzi in set di dati pronti per l’analisi. Prefect, Apache Airflow e dbt sono orchestratori di flussi di lavoro che consentono agli ingegneri di sviluppare pipeline di elaborazione dati flessibili e riutilizzabili.\nAd esempio, una pipeline può acquisire dati da database PostgreSQL, API REST e CSV archiviati su S3 [Simple Storage Service]. Può filtrare, deduplicare e aggregare i dati, gestire gli errori e salvare l’output su S3. La pipeline può anche spingere i dati trasformati in un feature store come Tecton o Feast per un accesso a bassa latenza.\nIn un caso d’uso di manutenzione predittiva industriale, i dati dei sensori vengono acquisiti dai dispositivi in S3. Una pipeline perfetta elabora i dati dei sensori, unendoli ai record di manutenzione. Il set di dati arricchito è archiviato in Feast in modo che i modelli possano recuperare facilmente i dati più recenti per l’addestramento e le previsioni.\nVideo 13.2 di seguito riporta una breve panoramica delle pipeline di dati.\n\n\n\n\n\n\nVideo 13.2: Pipeline di Dati\n\n\n\n\n\n\n\n\n13.3.2 Pipeline CI/CD\nLe pipeline di integrazione continua e distribuzione continua (CI/CD) automatizzano attivamente la progressione dei modelli ML dallo sviluppo iniziale alla distribuzione in produzione. Adattati per i sistemi ML, i principi CI/CD consentono ai team di distribuire rapidamente e in modo robusto nuovi modelli con errori manuali ridotti al minimo.\nLe pipeline CI/CD orchestrano i passaggi chiave, tra cui il controllo delle nuove modifiche al codice, la trasformazione dei dati, il training e la registrazione di nuovi modelli, i test di convalida, la containerizzazione, la distribuzione in ambienti come cluster di staging e la promozione in produzione. I team sfruttano le soluzioni CI/CD più diffuse come Jenkins, CircleCI e GitHub Actions per eseguire queste pipeline MLOps, mentre Prefect, Metaflow e Kubeflow offrono opzioni incentrate su ML.\nFigura 13.2 illustra una pipeline CI/CD specificamente pensata per MLOps. Il processo inizia con un dataset e un repository di feature (a sinistra), che alimenta una fase di ingestione del dataset. Dopo l’ingestione, i dati vengono sottoposti a convalida per garantirne la qualità prima di essere trasformati per l’addestramento. Parallelamente, un trigger di riaddestramento può avviare la pipeline in base a criteri specificati. I dati passano poi attraverso una fase di addestramento/ottimizzazione del modello all’interno di un motore di elaborazione dati, seguita dalla valutazione e convalida del modello. Una volta convalidato, il modello viene registrato e archiviato in un repository di metadati e artefatti di apprendimento automatico. La fase finale prevede la distribuzione del modello addestrato nuovamente nel dataset e nel repository di feature, creando così un processo ciclico per il miglioramento continuo e la distribuzione di modelli di apprendimento automatico.\n\n\n\n\n\n\nFigura 13.2: Diagramma CI/CD MLOps. Fonte: HarvardX.\n\n\n\nAd esempio, quando uno scienziato dei dati verifica i miglioramenti a un modello di classificazione delle immagini in un repository GitHub, questo attiva attivamente una pipeline CI/CD Jenkins. La pipeline riesegue le trasformazioni dei dati e l’addestramento del modello sui dati più recenti, monitorando gli esperimenti con MLflow. Dopo i test di validazione automatizzati, i team distribuiscono il contenitore del modello in un cluster di staging Kubernetes per un ulteriore controllo qualità. Una volta approvato, Jenkins facilita un rollout graduale del modello in produzione con distribuzioni canary per rilevare eventuali problemi. Se vengono rilevate anomalie, la pipeline consente ai team di tornare alla versione precedente del modello in modo fluido.\nLe pipeline CI/CD consentono ai team di iterare e distribuire rapidamente modelli ML collegando i diversi passaggi dallo sviluppo alla distribuzione con automazione continua. L’integrazione di strumenti MLOps come MLflow migliora il packaging del modello, il controllo delle versioni e la tracciabilità della pipeline. CI/CD è fondamentale per far progredire i modelli oltre i prototipi in sistemi aziendali sostenibili.\n\n\n13.3.3 Addestramento del Modello\nNella fase di training del modello, gli scienziati dei dati sperimentano attivamente diverse architetture e algoritmi ML per creare modelli ottimizzati che estraggono informazioni e modelli dai dati. MLOps introduce best practice e automazione per rendere questo processo iterativo più efficiente e riproducibile.\nI moderni framework ML come TensorFlow, PyTorch e Keras forniscono componenti predefiniti che semplificano la progettazione di reti neurali e altre architetture di modelli. Gli scienziati dei dati sfruttano moduli integrati per layer, attivazioni, perdite, ecc. e API di alto livello come Keras per concentrarsi maggiormente sull’architettura del modello.\nMLOps consente ai team di impacchettare il codice di training del modello in script e notebook riutilizzabili e tracciati. Man mano che i modelli vengono sviluppati, funzionalità come ottimizzazione degli iperparametri, ricerca dell’architettura neurale e selezione automatica delle funzionalità si ripetono rapidamente per trovare le configurazioni più performanti.\nI team utilizzano Git per controllare le versioni del codice di training e ospitarlo in repository come GitHub per tenere traccia delle modifiche nel tempo. Ciò consente una collaborazione fluida tra i data scientist.\nNotebook come Jupyter creano un eccellente ambiente di sviluppo di modelli interattivi. I notebook contengono l’inserimento dei dati, la pre-elaborazione, la dichiarazione del modello, il ciclo di training, la valutazione e il codice di esportazione in un documento riproducibile.\nInfine, i team orchestrano il training del modello come parte di una pipeline CI/CD per l’automazione. Ad esempio, una pipeline Jenkins può attivare uno script Python per caricare nuovi dati di training, riaddestrare un classificatore TensorFlow, valutare le metriche del modello e registrare automaticamente il modello se vengono raggiunte le soglie di prestazione.\nUn esempio di flusso di lavoro prevede che uno scienziato dei dati utilizzi un notebook PyTorch per sviluppare un modello CNN per la classificazione delle immagini. La libreria fastai fornisce API di alto livello per semplificare l’addestramento delle CNN sui set di dati delle immagini. Il notebook addestra il modello sui dati campione, valuta le metriche di accuratezza e ottimizza gli iperparametri come la velocità di apprendimento e i layer per ottimizzare le prestazioni. Questo notebook riproducibile è controllato dalla versione e integrato in una pipeline di riaddestramento.\nL’automazione e la standardizzazione dell’addestramento del modello consentono ai team di accelerare la sperimentazione e raggiungere il rigore necessario per produrre sistemi ML.\n\n\n13.3.4 Valutazione del Modello\nPrima di distribuire i modelli, i team eseguono una valutazione e dei test rigorosi per convalidare i benchmark delle prestazioni e la prontezza per il rilascio. MLOps introduce le best practice relative alla convalida, all’audit e ai test canary dei modelli.\nIn genere, i team valutano i modelli rispetto ai dataset di test di holdout che non vengono utilizzati durante il training. I dati di test provengono dalla stessa distribuzione dei dati di produzione. I team calcolano metriche come accuratezza, AUC, precisione, richiamo e punteggio F1.\nI team monitorano inoltre le stesse metriche nel tempo rispetto ai campioni di dati di test. Se i dati di valutazione provengono da flussi di produzione live, questo rileva le derive dei dati che degradano le prestazioni del modello nel tempo.\nLa supervisione umana per il rilascio del modello rimane importante. Gli scienziati dei dati esaminano le prestazioni nei segmenti e nelle sezioni chiave. L’analisi degli errori aiuta a identificare i punti deboli del modello per guidare il miglioramento. team applicano tecniche di equità e rilevamento di bias.\nIl test Canary rilascia un modello a un piccolo sottoinsieme di utenti per valutare le prestazioni nel mondo reale prima di un’ampia distribuzione. I team indirizzano gradualmente il traffico alla versione canary monitorando i problemi.\nAd esempio, un rivenditore valuta un modello di raccomandazione di prodotto personalizzato rispetto ai dati di test storici, esaminando le metriche di accuratezza e diversità. I team calcolano anche le metriche sui dati dei clienti in tempo reale nel tempo, rilevando una riduzione dell’accuratezza nelle ultime 2 settimane. Prima dell’implementazione completa, il nuovo modello viene rilasciato al 5% del traffico web per garantire che non vi sia alcun degrado.\nL’automazione della valutazione e delle versioni canary riduce i rischi di distribuzione. Tuttavia, la revisione umana deve ancora essere più critica per valutare le dinamiche meno quantificabili del comportamento del modello. Una rigorosa convalida pre-distribuzione fornisce sicurezza nell’immissione dei modelli in produzione.\n\n\n13.3.5 Distribuzione del Modello\nI team devono confezionare, testare e tracciare correttamente i modelli ML per distribuirli in modo affidabile in produzione. MLOps introduce framework e procedure per il versioning attivo, la distribuzione, il monitoraggio e l’aggiornamento dei modelli in modi sostenibili.\nI team containerizzano i modelli utilizzando Docker, che raggruppa codice, librerie e dipendenze in un’unità standardizzata. I container consentono una portabilità fluida tra gli ambienti.\nFramework come TensorFlow Serving e BentoML aiutano a servire le previsioni dai modelli distribuiti tramite API ottimizzate per le prestazioni. Questi framework gestiscono il versioning, il ridimensionamento e il monitoraggio.\nI team distribuiscono prima i modelli aggiornati in ambienti di staging o QA per i test prima del rollout completo in produzione. Le distribuzioni shadow o canary instradano un campione di traffico per testare le varianti del modello. I team aumentano gradualmente l’accesso ai nuovi modelli.\nI team creano solide procedure di rollback nel caso in cui emergano problemi. I rollback ripristinano l’ultima versione valida del modello. L’integrazione con pipeline CI/CD semplifica la ridistribuzione, se necessario.\nI team monitorano attentamente gli artefatti del modello, come script, pesi, log e metriche, per ogni versione con strumenti di metadati ML come MLflow. Ciò mantiene la discendenza e la verificabilità.\nAd esempio, un rivenditore inserisce in un “contenitore” un modello di raccomandazione di prodotto in TensorFlow Serving e lo distribuisce in un cluster di staging Kubernetes. Dopo aver monitorato e approvato le prestazioni sul traffico di esempio, Kubernetes sposta il 10% del traffico di produzione al nuovo modello. Se non vengono rilevati problemi dopo alcuni giorni, il nuovo modello occupa il 100% del traffico. Tuttavia, i team dovrebbero mantenere la versione precedente accessibile per il rollback, se necessario.\nI processi di distribuzione del modello consentono ai team di rendere i sistemi ML resilienti in produzione tenendo conto di tutti gli stati di transizione.\n\n\n13.3.6 Model Serving\nDopo il “deployment” [distribuzione] del modello, ML-as-a-Service diventa un componente fondamentale nel ciclo di vita di MLOps. I servizi online come Facebook/Meta gestiscono decine di trilioni di query di inferenza al giorno (Wu et al. 2019). Il “model serving” colma il divario tra i modelli sviluppati e le applicazioni ML o gli utenti finali, assicurando che i modelli distribuiti siano accessibili, performanti e scalabili negli ambienti di produzione.\n\nWu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan, Kim Hazelwood, et al. 2019. «Machine Learning at Facebook: Understanding Inference at the Edge». In 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA), 331–44. IEEE; IEEE. https://doi.org/10.1109/hpca.2019.00048.\nDiversi framework facilitano il model serving, tra cui TensorFlow Serving, NVIDIA Triton Inference Server e KServe (in precedenza KFServing). Questi strumenti forniscono interfacce standardizzate per la distribuzione di modelli distribuiti su varie piattaforme e gestiscono molte complessità dell’inferenza del modello su larga scala.\nIl model serving può essere categorizzato in tre tipi principali:\n\nOnline Serving: Fornisce previsioni in tempo reale con bassa latenza, il che è fondamentale per applicazioni come sistemi di raccomandazione o rilevamento frodi.\nOffline Serving: Elabora grandi batch di dati in modo asincrono, adatto per attività come la generazione periodica di report.\nNear-Online Serving (semi-sincrono): Bilancia tra online e offline, offrendo risposte relativamente rapide per applicazioni meno sensibili al tempo come i chatbot.\n\nUna delle sfide principali per i sistemi di model serving è operare secondo requisiti di prestazioni definiti da Service Level Agreement (SLA) e Service Level Objective (SLO). Gli SLA sono contratti formali che specificano i livelli di servizio previsti. Questi livelli di servizio si basano su parametri quali tempo di risposta, disponibilità e produttività. Gli SLO sono obiettivi interni che i team si prefiggono di soddisfare o superare i propri SLA.\nPer il model serving ML, gli accordi e gli obiettivi SLA e SLO hanno un impatto diretto sull’esperienza utente, sull’affidabilità del sistema e sui risultati aziendali. Pertanto, i team ottimizzano attentamente la propria piattaforma di servizio. I serving system ML impiegano varie tecniche per ottimizzare le prestazioni e l’utilizzo delle risorse, come le seguenti:\n\nPianificazione e batch delle richieste: Gestisce in modo efficiente le richieste di inferenza ML in arrivo, ottimizzando le prestazioni tramite strategie di accodamento e raggruppamento intelligenti. Sistemi come Clipper (Crankshaw et al. 2017) introducono il servizio di previsione online a bassa latenza con tecniche di caching e batch.\nSelezione e routing delle istanze del modello: Algoritmi intelligenti indirizzano le richieste alle versioni o alle istanze del modello appropriate. INFaaS (Romero et al. 2021) esplora questo aspetto generando varianti del modello e navigando in modo efficiente nello spazio di compromesso in base ai requisiti di prestazioni e accuratezza.\nBilanciamento del carico: Distribuisce i carichi di lavoro in modo uniforme su più istanze di servizio. MArk (Model Ark) (C. Zhang et al. 2019) dimostra tecniche efficaci di bilanciamento del carico per sistemi di servizio ML.\nAutoscaling delle istanze del modello: Regola dinamicamente la capacità in base alla domanda. Sia INFaaS (Romero et al. 2021) che MArk (C. Zhang et al. 2019) incorporano funzionalità di autoscaling per gestire in modo efficiente le fluttuazioni del carico di lavoro.\nOrchestration del modello: Gestisce l’esecuzione del modello, abilitando l’elaborazione parallela e l’allocazione strategica delle risorse. AlpaServe (Z. Li et al. 2023) dimostra tecniche avanzate per la gestione di modelli di grandi dimensioni e scenari di servizio complessi.\nPrevisione del tempo di esecuzione: Sistemi come Clockwork (Gujarati et al. 2020) si concentrano sul servizio ad alte prestazioni prevedendo i tempi di esecuzione delle singole inferenze e utilizzando in modo efficiente gli acceleratori hardware.\n\n\nCrankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, e Ion Stoica. 2017. «Clipper: A \\(\\{\\)Low-Latency\\(\\}\\) online prediction serving system». In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), 613–27.\n\nRomero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, e Christos Kozyrakis. 2021. «INFaaS: Automated Model-less Inference Serving.» In 2021 USENIX Annual Technical Conference (USENIX ATC 21), 397–411. https://www.usenix.org/conference/atc21/presentation/romero.\n\nZhang, Chengliang, Minchen Yu, Wei Wang 0030, e Feng Yan 0001. 2019. «MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving.» In 2019 USENIX Annual Technical Conference (USENIX ATC 19), 1049–62. https://www.usenix.org/conference/atc19/presentation/zhang-chengliang.\n\nLi, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, et al. 2023. «\\(\\{\\)AlpaServe\\(\\}\\): Statistical multiplexing with model parallelism for deep learning serving». In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), 663–79.\n\nGujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, e Jonathan Mace. 2020. «Serving DNNs like Clockwork: Performance Predictability from the Bottom Up.» In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), 443–62. https://www.usenix.org/conference/osdi20/presentation/gujarati.\nI serving system ML che eccellono in queste aree consentono alle organizzazioni di distribuire modelli che funzionano in modo affidabile sotto pressione. Il risultato sono applicazioni AI scalabili e reattive in grado di gestire le richieste del mondo reale e fornire valore in modo coerente.\n\n\n13.3.7 Gestione dell’Infrastruttura\nI team MLOps sfruttano ampiamente gli strumenti “infrastructure as code (IaC)” e le solide architetture cloud per gestire attivamente le risorse necessarie per lo sviluppo, il training e la distribuzione dei sistemi ML.\nI team utilizzano strumenti IaC come Terraform, CloudFormation e Ansible per definire, fornire e aggiornare a livello di programmazione l’infrastruttura in modo controllato dalla versione. Per MLOps, i team utilizzano ampiamente Terraform per avviare risorse su AWS, GCP e Azure.\nPer la creazione e il training dei modelli, i team forniscono dinamicamente risorse di elaborazione come server GPU, cluster di container, storage e database tramite Terraform in base alle esigenze degli scienziati dei dati. Il codice incapsula e preserva le definizioni dell’infrastruttura.\nI container e gli orchestratori come Docker e Kubernetes consentono ai team di impacchettare modelli e distribuirli in modo affidabile in diversi ambienti. I contenitori possono essere attivati o disattivati automaticamente in base alla domanda.\nSfruttando l’elasticità del cloud, i team aumentano o diminuiscono le risorse per soddisfare i picchi nei carichi di lavoro come i lavori di ottimizzazione degli iperparametri o i picchi nelle richieste di previsione. Auto-scaling consente un’efficienza dei costi ottimizzata.\nL’infrastruttura si estende su dispositivi on-prem, cloud ed edge. Uno stack tecnologico robusto offre flessibilità e resilienza. Gli strumenti di monitoraggio consentono ai team di osservare l’utilizzo delle risorse.\nAd esempio, una configurazione Terraform può distribuire un cluster GCP Kubernetes per ospitare modelli TensorFlow addestrati esposti come microservizi di previsione. Il cluster aumenta i pod per gestire un traffico maggiore. L’integrazione CI/CD distribuisce senza problemi nuovi contenitori di modelli.\nLa gestione attenta dell’infrastruttura tramite IaC e monitoraggio consente ai team di prevenire i colli di bottiglia nell’operatività dei sistemi ML su larga scala.\n\n\n13.3.8 Monitoraggio\nI team MLOps mantengono attivamente un monitoraggio robusto per mantenere la visibilità nei modelli ML distribuiti in produzione. Il monitoraggio continuo fornisce informazioni sulle prestazioni del modello e del sistema in modo che i team possano rilevare e risolvere rapidamente i problemi per ridurre al minimo le interruzioni.\nI team monitorano attivamente gli aspetti chiave del modello, inclusa l’analisi di campioni di previsioni live per tracciare metriche come accuratezza e matrice di confusione nel tempo.\nQuando monitorano le prestazioni, i team devono profilare i dati in arrivo per verificare la deriva del modello, un calo costante dell’accuratezza del modello dopo l’implementazione in produzione. La deriva del modello può verificarsi in due modi: deriva del concetto e deriva dei dati. La deriva del concetto si riferisce a un cambiamento fondamentale osservato nella relazione tra i dati di input e quelli target. Ad esempio, con l’avanzare della pandemia di COVID-19, i siti di e-commerce e vendita al dettaglio hanno dovuto correggere le raccomandazioni del modello poiché i dati di acquisto erano ampiamente distorti verso articoli come il disinfettante per le mani. La deriva dei dati descrive i cambiamenti nella distribuzione dei dati nel tempo. Ad esempio, gli algoritmi di riconoscimento delle immagini utilizzati nelle auto a guida autonoma devono tenere conto della stagionalità nell’osservazione dell’ambiente circostante. I team monitorano anche le metriche delle prestazioni delle applicazioni come latenza ed errori per le integrazioni dei modelli.\nDa una prospettiva infrastrutturale, i team monitorano i problemi di capacità come elevato utilizzo di CPU, memoria e disco e interruzioni del sistema. Strumenti come Prometheus, Grafana ed Elastic consentono ai team di raccogliere, analizzare, interrogare e visualizzare attivamente diverse metriche di monitoraggio. Le dashboard rendono le dinamiche altamente visibili.\nI team configurano gli allarmi per le metriche di monitoraggio chiave come cali di accuratezza e guasti del sistema per consentire una risposta proattiva agli eventi che minacciano l’affidabilità. Ad esempio, i cali di accuratezza del modello attivano avvisi per i team per esaminare potenziali deviazioni dei dati e riaddestrare i modelli utilizzando campioni di dati aggiornati e rappresentativi.\nDopo la distribuzione, il monitoraggio completo consente ai team di mantenere la fiducia nello stato del modello e del sistema. Consente ai team di rilevare e risolvere preventivamente le deviazioni tramite allarmi e dashboard basati sui dati. Il monitoraggio attivo è essenziale per mantenere sistemi ML altamente disponibili e affidabili.\nGuardare il video qui sotto per saperne di più sul monitoraggio.\n\n\n\n\n\n\nVideo 13.3: Monitoraggio del Modello\n\n\n\n\n\n\n\n\n13.3.9 Governance\nI team MLOps stabiliscono attivamente pratiche di governance appropriate come componente fondamentale. La governance fornisce una supervisione sui modelli ML per garantire che siano affidabili, etici e conformi. Senza governance, sussistono rischi significativi di modelli che si comportano in modi pericolosi o proibiti quando vengono distribuiti in applicazioni e processi aziendali.\nLa governance MLOps impiega tecniche per fornire trasparenza sulle previsioni, sulle prestazioni e sul comportamento del modello durante l’intero ciclo di vita ML. Metodi di spiegabilità come SHAP e LIME aiutano gli auditor a comprendere perché i modelli effettuano determinate previsioni evidenziando le caratteristiche di input influenti alla base delle decisioni. Bias detection analizza le prestazioni del modello in diversi gruppi demografici definiti da attributi come età, sesso ed etnia per rilevare eventuali distorsioni sistematiche. I team eseguono rigorose procedure di test su set di dati rappresentativi per convalidare le prestazioni del modello prima della distribuzione.\nUna volta in produzione, i team monitorano la concept drift [deriva del concetto] per determinare se le relazioni predittive cambiano nel tempo in modi che degradano l’accuratezza del modello. I team analizzano anche i registri di produzione per scoprire modelli nei tipi di errori generati dai modelli. La documentazione sulla provenienza dei dati, le procedure di sviluppo e le metriche di valutazione fornisce ulteriore visibilità.\nPiattaforme come Watson OpenScale incorporano funzionalità di governance come il monitoraggio dei bias e la spiegabilità direttamente nella creazione di modelli, nei test e nel monitoraggio della produzione. Le aree di interesse principali della governance sono trasparenza, correttezza e conformità. Ciò riduce al minimo i rischi che i modelli si comportino in modo errato o pericoloso quando integrati nei processi aziendali. L’integrazione di pratiche di governance nei flussi di lavoro MLOps consente ai team di garantire un’IA affidabile.\n\n\n13.3.10 Comunicazione e Collaborazione\nMLOps abbatte attivamente i “silos” e consente il libero flusso di informazioni e approfondimenti tra i team in tutte le fasi del ciclo di vita ML. Strumenti come MLflow, Weights & Biases e contesti di dati forniscono tracciabilità e visibilità per migliorare la collaborazione.\nI team utilizzano MLflow per sistematizzare il monitoraggio di esperimenti, versioni e artefatti del modello. Gli esperimenti possono essere loggati a livello di programmazione da notebook di data science e job di training. Il registro dei modelli fornisce un hub centrale per i team per archiviare modelli pronti per la produzione prima della distribuzione, con metadati come descrizioni, metriche, tag e discendenza. Le integrazioni con Github, GitLab facilitano i trigger per la modifica del codice.\n“Weights & Biases” fornisce strumenti collaborativi su misura per i team ML. Gli scienziati dei dati registrano gli esperimenti, visualizzano metriche come curve di perdita e condividono approfondimenti sulla sperimentazione con i colleghi. Le dashboard di confronto evidenziano le differenze del modello. I team discutono dei progressi e dei passaggi successivi.\nLa definizione di contesti di dati condivisi, ovvero glossari, dizionari di dati e riferimenti di schemi, garantisce l’allineamento del significato e dell’utilizzo dei dati tra i ruoli. La documentazione aiuta a comprendere chi non ha accesso diretto ai dati.\nAd esempio, uno scienziato dei dati può utilizzare “Weights & Biases” per analizzare un esperimento con un modello di rilevamento delle anomalie e condividere i risultati della valutazione con altri membri del team per discutere dei miglioramenti. Il modello finale può quindi essere registrato con MLflow prima di essere consegnato per la distribuzione.\nL’abilitazione della trasparenza, della tracciabilità e della comunicazione tramite MLOps consente ai team di rimuovere i colli di bottiglia e accelerare la distribuzione di sistemi ML di impatto.\nVideo 13.4 affronta le sfide chiave nella distribuzione del modello, tra cui la deriva del concetto, la deriva del modello e i problemi di ingegneria del software.\n\n\n\n\n\n\nVideo 13.4: Sfide della Distribuzione",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#debito-tecnico-nascosto-nei-sistemi-ml",
    "href": "contents/ops/ops.it.html#debito-tecnico-nascosto-nei-sistemi-ml",
    "title": "13  Operazioni di ML",
    "section": "13.4 Debito Tecnico Nascosto nei Sistemi ML",
    "text": "13.4 Debito Tecnico Nascosto nei Sistemi ML\nIl debito tecnico [https://it.wikipedia.org/wiki/Debito_tecnico] è sempre più pressante per i sistemi di apprendimento automatico. Questa metafora, originariamente proposta negli anni ’90, paragona i costi a lungo termine dello sviluppo rapido del software al debito finanziario. Proprio come un debito finanziario alimenta una crescita vantaggiosa, un debito tecnico gestito con attenzione consente una rapida iterazione. Tuttavia, se non controllato, l’accumulo di debito tecnico può superare qualsiasi guadagno.\nFigura 13.3 illustra i vari componenti che contribuiscono al debito tecnico nascosto dei sistemi ML. Mostra la natura interconnessa di configurazione, raccolta dati ed estrazione di funzionalità, che è fondamentale per la base di codice ML. Le dimensioni delle caselle indicano la proporzione dell’intero sistema rappresentata da ciascun componente. Nei sistemi ML industriali, il codice per l’algoritmo del modello costituisce solo una piccola frazione (vedere la piccola casella nera al centro rispetto a tutte le altre caselle grandi). La complessità dei sistemi ML e la natura frenetica del settore rendono molto facile l’accumulo di debito tecnico.\n\n\n\n\n\n\nFigura 13.3: Componenti del sistema ML. Fonte: Sambasivan et al. (2021)\n\n\n\n\n13.4.1 Erosione dei Confini del Modello\nA differenza del software tradizionale, ML non ha confini chiari tra i componenti, come si vede nel diagramma sopra. Questa erosione dell’astrazione crea intrecci che esacerbano il debito tecnico in diversi modi:\n\n\n13.4.2 Intreccio\nUn accoppiamento stretto tra i componenti del modello ML rende difficile isolare le modifiche. La modifica di una parte provoca effetti a catena imprevedibili in tutto il sistema. “Changing Anything Changes Everything (noto anche come CACE)” [Cambiare qualcosa cambia tutto] è un fenomeno che si applica a qualsiasi modifica apportata al sistema. Le potenziali mitigazioni includono la scomposizione del problema quando possibile o il monitoraggio ravvicinato delle modifiche nel comportamento per contenerne l’impatto.\n\n\n13.4.3 Cascate di Correzione\nFigura 13.4 illustra il concetto di cascate di correzione nel flusso di lavoro ML, dalla definizione del problema all’implementazione del modello. Gli archi rappresentano le potenziali correzioni iterative necessarie in ogni fase del flusso di lavoro, con colori diversi corrispondenti a problemi distinti come l’interazione con la fragilità del mondo fisico, competenze inadeguate nel dominio dell’applicazione, sistemi di ricompensa in conflitto e scarsa documentazione inter-organizzativa.\nLe frecce rosse indicano l’impatto delle cascate, che possono portare a revisioni significative nel processo di sviluppo del modello. Al contrario, la linea rossa tratteggiata rappresenta la misura drastica di abbandono del processo per riavviarlo. Questa immagine sottolinea la natura complessa e interconnessa dello sviluppo del sistema ML e l’importanza di affrontare questi problemi all’inizio del ciclo di sviluppo per mitigare i loro effetti di amplificazione a valle.\n\n\n\n\n\n\nFigura 13.4: Diagramma di flusso delle cascate di correzione. Fonte: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. «“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI». In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nLa creazione di modelli in sequenza crea dipendenze rischiose in cui i modelli successivi si basano su quelli precedenti. Ad esempio, prendere un modello esistente e perfezionarlo per un nuovo caso d’uso sembra efficiente. Tuttavia, questo incorpora ipotesi dal modello originale che potrebbero eventualmente richiedere una correzione.\nDiversi fattori influenzano la decisione di creare modelli in sequenza o meno:\n\nDimensioni del dataset e tasso di crescita: Con set di dati statici e di piccole dimensioni, la messa a punto dei modelli esistenti ha spesso senso. Per set di dati di grandi dimensioni e in crescita, l’addestramento di modelli personalizzati da zero consente una maggiore flessibilità per tenere conto dei nuovi dati.\nRisorse di elaborazione disponibili: La messa a punto richiede meno risorse rispetto all’addestramento di modelli di grandi dimensioni da zero. Con risorse limitate, sfruttare i modelli esistenti potrebbe essere l’unico approccio fattibile.\n\nMentre la messa a punto dei modelli esistenti può essere efficiente, la modifica dei componenti fondamentali in seguito diventa estremamente costosa a causa di questi effetti a cascata. Pertanto, si dovrebbe considerare attentamente l’introduzione di nuove architetture di modelli, anche se ad alta intensità di risorse, per evitare cascate di correzioni in futuro. Questo approccio può aiutare ad attenuare gli effetti di amplificazione dei problemi a valle e a ridurre il debito tecnico. Tuttavia, ci sono ancora scenari in cui la creazione di modelli sequenziali ha senso, il che richiede un attento equilibrio tra efficienza, flessibilità e manutenibilità a lungo termine nel processo di sviluppo ML.\n\n\n13.4.4 Consumatori Non Dichiarati\nUna volta che le previsioni del modello ML sono rese disponibili, molti sistemi downstream [derivati] potrebbero utilizzarle silenziosamente come input per un’ulteriore elaborazione. Tuttavia, il modello originale non è stato progettato per adattarsi a questo ampio riutilizzo. A causa dell’opacità intrinseca dei sistemi ML, diventa impossibile analizzare completamente l’impatto degli output del modello come input altrove. Le modifiche al modello possono quindi avere conseguenze costose e pericolose interrompendo dipendenze non rilevate.\nI “consumatori” non dichiarati possono anche abilitare loop di feedback nascosti se i loro output influenzano indirettamente i dati di training del modello originale. Le mitigazioni includono la limitazione dell’accesso alle previsioni, la definizione di contratti di servizio rigorosi e il monitoraggio di segnali di influenze non-modellate. Architettare sistemi ML per incapsulare e isolare i loro effetti limita i rischi di propagazione imprevista.\n\n\n13.4.5 Debito di Dipendenza dai Dati\nIl debito di dipendenza dei dati si riferisce a dipendenze di dati instabili e sottoutilizzate, che possono avere ripercussioni dannose e difficili da rilevare. Sebbene questo sia un fattore chiave del debito tecnologico per il software tradizionale, tali sistemi possono trarre vantaggio dall’uso di strumenti ampiamente disponibili per l’analisi statica da parte di compilatori e linker per identificare dipendenze di questo tipo. I sistemi ML necessitano di strumenti simili.\nUna mitigazione per le dipendenze di dati instabili è l’uso del versioning, che garantisce la stabilità degli input ma comporta il costo della gestione di più set di dati e il potenziale della obsolescenza. Un’altra mitigazione per le dipendenze di dati sottoutilizzate è quella di condurre una valutazione esaustiva “leave-one-feature-out”.\n\n\n13.4.6 Debito di Analisi dai Cicli di Feedback\nA differenza del software tradizionale, i sistemi ML possono cambiare il loro comportamento nel tempo, rendendo difficile l’analisi pre-distribuzione. Questo debito si manifesta nei cicli di feedback, sia diretti che nascosti.\nI cicli di feedback diretti si verificano quando un modello influenza i suoi input futuri, ad esempio consigliando prodotti agli utenti che, a loro volta, modellano i dati di training futuri. I cicli nascosti sorgono indirettamente tra modelli, ad esempio due sistemi che interagiscono tramite ambienti del mondo reale. I cicli di feedback graduali sono particolarmente difficili da rilevare. Questi cicli portano al debito di analisi, ovvero l’incapacità di prevedere come un modello agirà completamente dopo il rilascio. Essi compromettono la validazione pre-distribuzione consentendo un’autoinfluenza non modellata.\nUn attento monitoraggio e distribuzioni “canary” aiutano a rilevare il feedback. Tuttavia, permangono sfide fondamentali nella comprensione delle interazioni complesse del modello. Le scelte architettoniche che riducono l’intreccio e l’accoppiamento mitigano l’effetto composto del debito di analisi.\n\n\n13.4.7 Le Giungle di Pipeline\nI workflow [flussi di lavoro] ML spesso necessitano di interfacce più standardizzate tra i componenti. Ciò porta i team a “incollare” gradualmente le pipeline con codice personalizzato. Ciò che emerge sono “giungle di pipeline”, ovvero passaggi di pre-elaborazione aggrovigliati che sono fragili e resistono al cambiamento. Evitare modifiche a queste pipeline disordinate fa sì che i team sperimentino attraverso prototipi alternativi. Presto, proliferano molteplici modi di fare. La necessità di astrazioni e interfacce impedisce quindi la condivisione, il riutilizzo e l’efficienza.\nIl debito tecnico si accumula man mano che le pipeline si solidificano in vincoli legacy. I team sprecano tempo nella gestione di codice idiosincratico anziché massimizzare le prestazioni del modello. Principi architettonici come modularità e incapsulamento sono necessari per stabilire interfacce pulite. Le astrazioni condivise consentono componenti intercambiabili, impediscono il lock-in e promuovono la diffusione delle “best practice” tra i team. Liberarsi dalle “giungle di pipeline” richiede in definitiva l’applicazione di standard che impediscano l’accumulo di debito di astrazione. I vantaggi delle interfacce e delle API che domano la complessità superano i costi di transizione.\n\n\n13.4.8 Debito di Configurazione\nI sistemi ML comportano una configurazione estesa di iperparametri, architetture e altri parametri di ottimizzazione. Tuttavia, la configurazione è spesso un ripensamento, che necessita di più rigore e test: aumentano le configurazioni ad hoc, amplificate dalle numerose “manopole” disponibili per l’ottimizzazione di modelli ML complessi.\nQuesto accumulo di debito tecnico ha diverse conseguenze. Configurazioni fragili e obsolete portano a dipendenze nascoste e bug che causano guasti di produzione. La conoscenza sulle configurazioni ottimali è isolata anziché condivisa, portando a un lavoro ridondante. Riprodurre e confrontare i risultati diventa difficile quando le configurazioni mancano di documentazione. I vincoli legacy si accumulano poiché i team temono di modificare configurazioni poco comprese.\nPer affrontare il debito di configurazione è necessario stabilire standard per documentare, testare, convalidare e archiviare centralmente le configurazioni. Investire in approcci più automatizzati, come l’ottimizzazione degli iperparametri e la ricerca dell’architettura, riduce la dipendenza dall’ottimizzazione manuale. Una migliore igiene della configurazione rende il miglioramento iterativo più gestibile impedendo alla complessità di aumentare all’infinito. La chiave è riconoscere la configurazione come parte integrante del ciclo di vita del sistema ML piuttosto che come un ripensamento ad hoc.\n\n\n13.4.9 Il Mondo che Cambia\nI sistemi ML operano in ambienti dinamici del mondo reale. Le soglie e le decisioni inizialmente efficaci diventano obsolete man mano che il mondo si evolve. Tuttavia, i vincoli legacy rendono difficile adattare i sistemi a popolazioni, modelli di utilizzo e altri fattori contestuali mutevoli.\nQuesto debito si manifesta in due modi principali. In primo luogo, le soglie preimpostate e le euristiche richiedono una rivalutazione e una messa a punto costanti man mano che i loro valori ottimali si spostano. In secondo luogo, la convalida dei sistemi tramite test statici di unità e integrazione fallisce quando input e comportamenti sono obiettivi in movimento.\nRispondere a un mondo in continua evoluzione in tempo reale con sistemi ML legacy è impegnativo. Il debito tecnico si accumula man mano che le ipotesi decadono. La mancanza di architettura modulare e la capacità di aggiornare dinamicamente i componenti senza effetti collaterali esacerbano questi problemi.\nPer mitigare questo problema è necessario integrare configurabilità, monitoraggio e aggiornabilità modulare. L’apprendimento online, in cui i modelli si adattano continuamente e solidi cicli di feedback alle pipeline di training, aiutano a sintonizzarsi automaticamente sul mondo. Tuttavia, anticipare e progettare il cambiamento è essenziale per prevenire l’erosione delle prestazioni nel mondo reale nel tempo.\n\n\n13.4.10 Gestire il Debito Tecnico nelle Fasi Iniziali\nÈ comprensibile che il debito tecnico si accumuli naturalmente nelle prime fasi di sviluppo del modello. Quando si punta a creare rapidamente modelli MVP, i team spesso hanno bisogno di informazioni più complete su quali componenti raggiungeranno la scala o richiederanno modifiche. È previsto un po’ di lavoro differito.\nTuttavia, anche i sistemi iniziali frammentati dovrebbero seguire principi come “Flexible Foundations” per evitare di mettersi nei guai:\n\nIl codice modulare e le librerie riutilizzabili consentono di scambiare i componenti in un secondo momento\nL’accoppiamento debole tra modelli, archivi dati e logica aziendale facilita il cambiamento\nI layer di astrazione nascondono i dettagli di implementazione che potrebbero cambiare nel tempo\nIl servizio di modelli containerizzati mantiene aperte le opzioni sui requisiti di distribuzione\n\nLe decisioni che sembrano ragionevoli al momento possono limitare seriamente la flessibilità futura. Ad esempio, incorporare la logica aziendale chiave nel codice modello anziché tenerla separata rende estremamente difficili le modifiche successive al modello.\nCon una progettazione ponderata, tuttavia, è possibile creare rapidamente all’inizio mantenendo gradi di libertà per migliorare. Man mano che il sistema matura, emergono prudenti punti di interruzione in cui l’introduzione di nuove architetture in modo proattivo evita massicce rilavorazioni in futuro. In questo modo si bilanciano le urgenti tempistiche con la riduzione delle future cascate di correzione.\n\n\n13.4.11 Riepilogo\nSebbene il debito finanziario sia una buona metafora per comprendere i compromessi, differisce dalla misurabilità del debito tecnico. Il debito tecnico deve essere completamente monitorato e quantificato. Ciò rende difficile per i team gestire i compromessi tra muoversi rapidamente e introdurre intrinsecamente più debito rispetto al prendersi il tempo per ripagare tale debito.\nIl documento Hidden Technical Debt of Machine Learning Systems diffonde la consapevolezza delle sfumature del debito tecnologico specifico del sistema ML. Incoraggia un ulteriore sviluppo nell’ampia area del ML manutenibile.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#ruoli-e-responsabilità",
    "href": "contents/ops/ops.it.html#ruoli-e-responsabilità",
    "title": "13  Operazioni di ML",
    "section": "13.5 Ruoli e Responsabilità",
    "text": "13.5 Ruoli e Responsabilità\nData la vastità di MLOps, l’implementazione di successo di sistemi ML richiede competenze diversificate e una stretta collaborazione tra persone con diverse aree di competenza. Mentre gli scienziati dei dati creano i modelli ML di base, è necessario un lavoro di squadra interfunzionale per distribuire con successo questi modelli in ambienti di produzione e consentire loro di fornire un valore aziendale sostenibile.\nMLOps fornisce il framework e le pratiche per coordinare gli sforzi di vari ruoli coinvolti nello sviluppo, nella distribuzione e nell’esecuzione di sistemi MLG. Collegare i “silos” tradizionali tra i team di dati, ingegneria e operazioni è fondamentale per il successo di MLOps. Abilitare una collaborazione senza soluzione di continuità attraverso il ciclo di vita dell’apprendimento automatico accelera la realizzazione dei vantaggi garantendo al contempo l’affidabilità e le prestazioni a lungo termine dei modelli ML.\nEsamineremo alcuni ruoli chiave coinvolti in MLOps e le loro responsabilità principali. Comprendere l’ampiezza delle competenze necessarie per rendere operativi i modelli ML guida l’assemblaggio dei team MLOps. Chiarisce inoltre come i flussi di lavoro tra i ruoli si adattano alla metodologia MLOps sovraordinata.\n\n13.5.1 Ingegneri dei Dati\nGli ingegneri dei dati sono responsabili della creazione e della manutenzione dell’infrastruttura dati e delle pipeline che alimentano i dati nei modelli ML. Garantiscono che i dati vengano trasferiti senza problemi dai sistemi di origine agli ambienti di archiviazione, elaborazione e progettazione delle funzionalità necessari per lo sviluppo e la distribuzione dei modelli ML. Le loro principali responsabilità includono:\n\nMigrare dati grezzi da database, sensori e app “on-prem” [in azienda], in data lake basati su cloud, come Amazon S3 o Google Cloud Storage. Ciò fornisce un’archiviazione economica e scalabile.\nCreare pipeline di dati con “scheduler” [pianificatori] di flussi di lavoro come Apache Airflow, Prefect e dbt. Questi estraggono i dati dalle sorgenti, li trasformano e li convalidano, e li caricano direttamente in destinazioni come data warehouse, feature store o per l’addestramento del modello.\nTrasformare dati grezzi e disordinati in set di dati strutturati e pronti per l’analisi. Ciò include la gestione di valori nulli o malformati, la deduplicazione, l’unione di origini dati disparate, l’aggregazione dei dati e la progettazione di nuove feature.\nManutenere componenti dell’infrastruttura dati come data warehouse cloud (Snowflake, Redshift, BigQuery), data lake e sistemi di gestione dei metadati. Provisioning e ottimizzazione dei sistemi di elaborazione dati.\nFornire e ottimizzare sistemi di elaborazione dati per una gestione e un’analisi dei dati efficiente e scalabile.\nDefinire i processi di versioning, backup e archiviazione dei dati per i set di dati e funzionalità ML e applicare policy di governance dei dati.\n\nAd esempio, un’azienda manifatturiera può utilizzare pipeline Apache Airflow per estrarre dati dei sensori dai PLC in fabbrica e trasferirli in un data lake Amazon S3. Gli ingegneri dei dati elaborerebbero poi questi dati grezzi per filtrarli, pulirli e unirli ai metadati del prodotto. Questi output della pipeline verrebbero quindi caricati in un data warehouse Snowflake da cui è possibile leggere le feature per l’addestramento e la previsione del modello.\nIl team di ingegneria dei dati crea e sostiene la base dati per uno sviluppo e un funzionamento affidabili del modello. Il loro lavoro consente agli scienziati dei dati e agli ingegneri ML di concentrarsi sulla creazione, l’addestramento e l’implementazione di modelli ML su larga scala.\n\n\n13.5.2 Data Scientist\nIl lavoro dei “data scientist” [scienziato dei dati] è concentrarsi sulla ricerca, sperimentazione, sviluppo e miglioramento continuo dei modelli ML. Sfruttano la loro competenza in statistica, modellazione e algoritmi per creare modelli ad alte prestazioni. Le loro principali responsabilità includono:\n\nCollaborare con team aziendali e di dati per identificare opportunità in cui ML può aggiungere valore, inquadrare il problema e definire metriche di successo.\nEseguire analisi esplorative dei dati per comprendere le relazioni nei dati, ricavare informazioni e identificare funzionalità rilevanti per la modellazione.\nRicercare e sperimentare diversi algoritmi ML e architetture di modelli in base al problema e alle caratteristiche dei dati e sfruttare librerie come TensorFlow, PyTorch e Keras.\nMassimizzare le prestazioni, addestrare e perfezionare i modelli regolando gli iperparametri, regolando le architetture delle reti neurali, l’ingegneria delle funzionalità, ecc.\nValutare le prestazioni del modello tramite metriche come accuratezza, AUC e punteggi F1 ed eseguire analisi degli errori per identificare aree di miglioramento.\nSviluppare nuove versioni del modello mediante l’integrazione di nuovi dati, test di diversi approcci, ottimizzazione del comportamento del modello e mantenimento della documentazione e della discendenza per i modelli.\n\nAd esempio, uno scienziato dei dati può sfruttare TensorFlow e TensorFlow Probability per sviluppare un modello di previsione della domanda per la pianificazione dell’inventario ala vendita al dettaglio. Itereranno su diversi modelli di sequenza come LSTM e sperimenteranno funzionalità derivate da dati di prodotto, vendite e stagionali. Il modello verrà valutato in base a metriche di errore rispetto alla domanda effettiva prima dell’implementazione. Lo scienziato dei dati monitora le prestazioni e riqualifica/migliora il modello man mano che arrivano nuovi dati.\nI data scientist guidano la creazione, il miglioramento e l’innovazione del modello attraverso la loro competenza nelle tecniche di ML. Collaborano strettamente con altri ruoli per garantire che i modelli creino il massimo impatto aziendale.\n\n\n13.5.3 ML Engineer\nGli “ingegneri ML” consentono ai modelli sviluppati dagli scienziati dei dati di essere prodotti e distribuiti su larga scala. La loro competenza fa sì che i modelli servano in modo affidabile alle previsioni nelle applicazioni e nei processi aziendali. Le loro principali responsabilità includono:\n\nPrendere modelli prototipo dagli scienziati dei dati e rafforzarli per gli ambienti di produzione tramite best practice di codifica.\nCreare API e microservizi per la distribuzione dei modelli utilizzando strumenti come Flask, FastAPI. Containerizzare i modelli con Docker.\nGestire le versioni dei modelli, sincronizzarli in produzione utilizzando pipeline CI/CD e implementare release canary, test A/B e procedure di rollback.\nOttimizzare le prestazioni dei modelli per elevata scalabilità, bassa latenza ed efficienza dei costi. Sfruttare compressione, quantizzazione e servizio multi-modello.\nMonitorare i modelli una volta in produzione e garantire affidabilità e precisione continue. Riqualificare periodicamente i modelli.\n\nAd esempio, un ingegnere ML può prendere un modello di rilevamento delle frodi TensorFlow sviluppato da data scientist e containerizzarlo utilizzando TensorFlow Serving per una distribuzione scalabile. Il modello verrebbe integrato nella pipeline di elaborazione delle transazioni dell’azienda tramite API. L’ingegnere ML implementa un registro dei modelli e una pipeline CI/CD utilizzando MLFlow e Jenkins per distribuire gli aggiornamenti del modello in modo affidabile. Gli ingegneri ML monitorano quindi il modello in esecuzione per prestazioni continue utilizzando strumenti come Prometheus e Grafana. Se l’accuratezza del modello diminuisce, avviano la riqualificazione e la distribuzione di una nuova versione del modello.\nIl team di ingegneria ML consente ai modelli di data science di progredire senza problemi in sistemi di produzione sostenibili e robusti. La loro competenza nella creazione di sistemi modulari e monitorati offre un valore aziendale continuo.\n\n\n13.5.4 DevOps Engineer\nGli “ingegneri DevOps” abilitano MLOps creando e gestendo l’infrastruttura sottostante per lo sviluppo, la distribuzione e il monitoraggio dei modelli ML. Forniscono l’architettura cloud e le pipeline di automazione. Le loro principali responsabilità includono:\n\nApprovvigionare e gestire l’infrastruttura cloud per i flussi di lavoro ML utilizzando strumenti IaC come Terraform, Docker e Kubernetes.\nSviluppare pipeline CI/CD per il riaddestramento, la convalida e la distribuzione del modello. Integrare strumenti ML nella pipeline, come MLflow e Kubeflow.\nMonitorare le prestazioni del modello e dell’infrastruttura tramite strumenti come Prometheus, Grafana, stack ELK. Creare allarmi e dashboard.\nImplementare pratiche di governance relative allo sviluppo, al test e alla promozione del modello per consentire riproducibilità e tracciabilità.\nEmbedding dei modelli ML nelle applicazioni. Espongono i modelli tramite API e microservizi per l’integrazione.\nOttimizzazione delle prestazioni e dei costi dell’infrastruttura e sfruttamento dell’autoscaling, delle istanze spot e della disponibilità in tutte le regioni.\n\nAd esempio, un ingegnere DevOps esegue il provisioning di un cluster Kubernetes su AWS utilizzando Terraform per eseguire lavori di training ML e distribuzione online. L’ingegnere crea una pipeline CI/CD in Jenkins, che attiva il riaddestramento del modello quando sono disponibili nuovi dati. Dopo il test automatizzato, il modello viene registrato con MLflow e distribuito nel cluster Kubernetes. L’ingegnere monitora quindi lo stato del cluster, l’utilizzo delle risorse del contenitore e la latenza dell’API utilizzando Prometheus e Grafana.\nIl team DevOps consente una rapida sperimentazione e distribuzioni affidabili per ML tramite competenze cloud, automazione e monitoraggio. Il loro lavoro massimizza l’impatto del modello riducendo al minimo il debito tecnico.\n\n\n13.5.5 Project Manager\nI project manager svolgono un ruolo fondamentale in MLOps coordinando le attività tra i team coinvolti nella distribuzione dei progetti ML. Aiutano a guidare l’allineamento, la “accountability” [affidabilità] ed accelerano i risultati. Le loro principali responsabilità includono:\n\nCollaborare con le parti interessate per definire obiettivi di progetto, metriche di successo, tempistiche e budget; delineare specifiche e “scope”.\nCreare un piano di progetto che comprenda acquisizione dati, sviluppo modello, configurazione infrastrutturale, distribuzione e monitoraggio.\nCoordinare i lavori di progettazione, sviluppo e test tra ingegneri dei dati, scienziati dei dati, ingegneri ML e ruoli DevOps.\nMonitorare i progressi e le milestone, identificare gli ostacoli e risolverli tramite azioni correttive e gestire rischi e problemi.\nFacilitare la comunicazione tramite report di stato, riunioni, workshop e documentazione e consentire una collaborazione senza interruzioni.\nGuidare l’aderenza alle tempistiche e al budget e aumentare i superamenti o le carenze previsti per la mitigazione.\n\nAd esempio, un project manager creerebbe un piano di progetto per sviluppare e migliorare un modello di previsione dell’abbandono dei clienti. Coordinare data engineer che creano pipeline di dati, data scientist che sperimentano modelli, ML engineer che producono modelli e DevOps che impostano l’infrastruttura di distribuzione. Il project manager monitora i progressi tramite milestone come preparazione del set di dati, prototipazione del modello, distribuzione e monitoraggio. Per attuare soluzioni preventive, evidenziano eventuali rischi, ritardi o problemi di budget.\nI project manager qualificati consentono ai team MLOps di lavorare in sinergia per fornire rapidamente il massimo valore aziendale dagli investimenti ML. La loro leadership e organizzazione si allineano con team diversi.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#sfide-dei-sistemi-embedded",
    "href": "contents/ops/ops.it.html#sfide-dei-sistemi-embedded",
    "title": "13  Operazioni di ML",
    "section": "13.6 Sfide dei Sistemi Embedded",
    "text": "13.6 Sfide dei Sistemi Embedded\nEsamineremo brevemente le sfide dei sistemi embedded in modo da definire il contesto per quelle specifiche che emergono con gli MLOps embedded, di cui parleremo nella sezione seguente.\n\n13.6.1 Risorse di Elaborazione Limitate\nI dispositivi embedded come i microcontrollori e i telefoni cellulari hanno una potenza di elaborazione molto più limitata rispetto alle macchine dei data center o alle GPU. Un tipico microcontrollore può avere solo KB di RAM, velocità della CPU MHz e nessuna GPU. Ad esempio, un microcontrollore in uno smartwatch può avere solo un processore a 32 bit in esecuzione a 120 MHz con 320 KB di RAM («EuroSoil 2021 (O205)» 2021). Ciò consente modelli ML semplici come piccole regressioni lineari o foreste casuali, ma reti neurali profonde più complesse sarebbero irrealizzabili. Le strategie per mitigare questo includono quantizzazione, potatura, architetture di modelli efficienti e scaricamento di determinati calcoli sul cloud quando la connettività lo consente.\n\n«EuroSoil 2021 (O205)». 2021. In EuroSoil 2021 (O205). DS12902. STMicroelectronics; Frontiers Media SA. https://doi.org/10.3389/978-2-88966-997-4.\n\n\n13.6.2 Memoria Limitata\nMemorizzare grandi modelli ML e set di dati direttamente su dispositivi embedded è spesso impossibile con una memoria limitata. Ad esempio, un modello di rete neurale profonda può facilmente occupare centinaia di MB, il che supera la capacità di archiviazione di molti sistemi embedded. Si consideri questo esempio. Una fotocamera per la fauna selvatica che cattura immagini per rilevare animali può avere solo una scheda di memoria da 2 GB. Ne serve di più per memorizzare un modello di deep learning per la classificazione delle immagini che spesso ha una dimensione di centinaia di MB. Di conseguenza, ciò richiede l’ottimizzazione dell’utilizzo della memoria tramite compressione dei pesi, numeri di precisione inferiore e pipeline di inferenza in streaming.\n\n\n13.6.3 Connettività Intermittente\nMolti dispositivi embedded operano in ambienti remoti senza una connettività Internet affidabile. Dobbiamo fare affidamento su qualcosa di diverso dall’accesso cloud costante per un comodo riaddestramento, monitoraggio e distribuzione. Al contrario, abbiamo bisogno di strategie intelligenti sia di pianificazione che si memorizzazione nella cache per ottimizzare le connessioni intermittenti. Ad esempio, un modello che prevede la resa del raccolto in una fattoria remota potrebbe dover fare previsioni giornaliere ma avere connettività al cloud solo una volta alla settimana quando l’agricoltore si reca in città. Il modello deve funzionare in modo indipendente tra una connessione e l’altra.\n\n\n13.6.4 Limitazioni della Potenza\nI dispositivi embedded come telefoni, dispositivi indossabili e sensori remoti sono alimentati a batteria. L’inferenza e la comunicazione continue possono esaurire rapidamente le batterie, limitandone la funzionalità. Ad esempio, un collare intelligente che contrassegna gli animali in via di estinzione funziona con una piccola batteria. L’esecuzione continua di un modello di tracciamento GPS scaricherebbe la batteria nel giro di pochi giorni. Il collare deve pianificare con attenzione quando attivare il modello. Pertanto, l’ML integrato deve gestire attentamente le attività per risparmiare energia. Le tecniche includono acceleratori hardware ottimizzati, caching delle previsioni ed esecuzione di modelli adattivi.\n\n\n13.6.5 Gestione della Flotta\nPer i dispositivi embedded prodotti in serie, milioni di unità possono essere distribuite sul campo per orchestrare gli aggiornamenti. Ipoteticamente, l’aggiornamento di un modello di rilevamento delle frodi su 100 milioni di carte di credito (future intelligenti) richiede l’invio sicuro degli aggiornamenti a ciascun dispositivo distribuito anziché a un data center centralizzato. Una scala così distribuita rende la gestione dell’intera flotta molto più difficile di un cluster di server centralizzato. Richiede protocolli intelligenti per aggiornamenti “over-the-air”, gestione dei problemi di connettività e monitoraggio dei vincoli di risorse tra i dispositivi.\n\n\n13.6.6 Raccolta Dati On-Device\nLa raccolta di dati di training utili richiede la progettazione sia dei sensori sul dispositivo sia delle pipeline software. Questo è diverso dai server, dove possiamo estrarre dati da fonti esterne. Le sfide includono la gestione del rumore dei sensori. I sensori su una macchina industriale rilevano vibrazioni e temperatura per prevedere le esigenze di manutenzione. Ciò richiede la messa a punto dei sensori e delle frequenze di campionamento per acquisire dati utili.\n\n\n13.6.7 Personalizzazione Specifica del Dispositivo\nUno smart speaker impara i modelli vocali e la cadenza del parlato di un singolo utente per migliorare la precisione del riconoscimento proteggendo al contempo la privacy. Adattare i modelli ML a dispositivi e utenti specifici è importante, ma ciò pone sfide per la privacy. L’apprendimento sul dispositivo consente la personalizzazione senza trasmettere così tanti dati privati. Tuttavia, bilanciare il miglioramento del modello, la tutela della privacy e i vincoli richiede nuove tecniche.\n\n\n13.6.8 Considerazioni sulla Sicurezza\nSe un ML embedded estremamente grande in sistemi come i veicoli a guida autonoma non viene progettato con attenzione, ci sono seri rischi per la sicurezza. Per garantire un funzionamento sicuro prima dell’implementazione, le auto a guida autonoma devono essere sottoposte a test approfonditi in pista in scenari simulati di pioggia, neve e ostacoli. Ciò richiede una convalida approfondita, dispositivi di sicurezza, simulatori e conformità agli standard prima dell’implementazione.\n\n\n13.6.9 Diversi Target Hardware\nEsiste una vasta gamma di processori embedded, tra cui ARM, x86, acceleratori AI specializzati, FPGA, ecc. Supportare questa eterogeneità rende difficile l’implementazione. Abbiamo bisogno di strategie come framework standardizzati, test approfonditi e messa a punto del modello per ogni piattaforma. Ad esempio, un modello di rilevamento degli oggetti necessita di implementazioni efficienti su dispositivi embedded come Raspberry Pi, Nvidia Jetson e Google Edge TPU.\n\n\n13.6.10 Copertura dei Test\nTestare rigorosamente i casi limite è difficile con risorse di simulazione embedded limitate, ma test esaustivi sono fondamentali in sistemi come le auto a guida autonoma. Testare esaustivamente un modello di pilota automatico richiede milioni di chilometri simulati, esponendolo a eventi rari come guasti dei sensori. Pertanto, strategie come la generazione di dati sintetici, la simulazione distribuita e l’ingegneria del caos aiutano a migliorare la copertura.\n\n\n13.6.11 Rilevamento della Deriva del Concetto\nCon dati di monitoraggio limitati da ogni dispositivo remoto, rilevare cambiamenti nei dati di input nel tempo è molto più difficile. La deriva può portare a degradazioni delle prestazioni del modello. Sono necessari metodi “leggeri” per identificare quando è necessario un riaddestramento. Un modello che prevede i carichi della rete elettrica mostra un calo delle prestazioni man mano che i modelli di utilizzo cambiano nel tempo. Con i soli dati locali sui dispositivi, questa tendenza è difficile da individuare.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#mlops-tradizionali-e-mlops-embedded",
    "href": "contents/ops/ops.it.html#mlops-tradizionali-e-mlops-embedded",
    "title": "13  Operazioni di ML",
    "section": "13.7 MLOps Tradizionali e MLOps Embedded",
    "text": "13.7 MLOps Tradizionali e MLOps Embedded\nNegli MLOps tradizionali, i modelli ML vengono in genere distribuiti in ambienti basati su cloud o server, con risorse abbondanti come potenza di calcolo e memoria. Questi ambienti facilitano il funzionamento regolare di modelli complessi che richiedono risorse di calcolo significative. Ad esempio, un modello di riconoscimento delle immagini basato su cloud potrebbe essere utilizzato da una piattaforma di social media per contrassegnare automaticamente le foto con etichette pertinenti. In questo caso, il modello può sfruttare le vaste risorse disponibili nel cloud per elaborare in modo efficiente grandi quantità di dati.\nD’altro canto, i MLOps embedded comportano la distribuzione di modelli ML su sistemi embedded, sistemi di calcolo specializzati progettati per eseguire funzioni specifiche all’interno di sistemi più grandi. I sistemi embedded sono in genere caratterizzati dalle loro risorse di calcolo e potenza limitate. Ad esempio, un modello ML potrebbe essere “embedded” in un termostato intelligente per ottimizzare il riscaldamento e il raffreddamento in base alle preferenze e alle abitudini dell’utente. Il modello deve essere ottimizzato per funzionare in modo efficiente sull’hardware limitato del termostato senza comprometterne le prestazioni o la precisione.\nLa differenza fondamentale tra i MLOps tradizionali e quelli embedded risiede nei vincoli di risorse del sistema embedded. Mentre gli MLOps tradizionali possono sfruttare abbondanti risorse cloud o server, gli MLOps embedded devono fare i conti con le limitazioni hardware su cui viene distribuito il modello. Ciò richiede un’attenta ottimizzazione e messa a punto del modello per garantire che possa fornire informazioni accurate e preziose entro i vincoli del sistema embedded.\nInoltre, gli MLOps embedded devono considerare le sfide uniche poste dall’integrazione dei modelli ML con altri componenti del sistema embedded. Ad esempio, il modello deve essere compatibile con il software e l’hardware del sistema e deve essere in grado di interfacciarsi senza problemi con altri componenti, come sensori o attuatori. Ciò richiede una profonda comprensione sia dei sistemi ML che di quelli integrati e una stretta collaborazione tra data scientist, ingegneri e altre parti interessate.\nQuindi, mentre gli MLOps tradizionali e gli MLOps embedded condividono l’obiettivo comune di distribuire e mantenere modelli ML in ambienti di produzione, le sfide uniche poste dai sistemi embedded richiedono un approccio specializzato. Gli MLOps embedded devono bilanciare attentamente la necessità di accuratezza e prestazioni del modello con i vincoli dell’hardware su cui viene distribuito il modello. Ciò richiede una profonda comprensione sia dei sistemi ML che di quelli embedded e una stretta collaborazione tra i vari stakeholder per garantire l’integrazione di successo dei modelli ML nei sistemi embedded.\nQuesta volta, raggrupperemo i sottoargomenti in categorie più ampie per semplificare la struttura del nostro processo di pensiero su MLOps. Questa struttura aiuterà a comprendere come i diversi aspetti di MLOps siano interconnessi e perché ciascuno sia importante per il funzionamento efficiente dei sistemi ML mentre discutiamo le sfide nel contesto dei sistemi embedded.\n\nGestione del Ciclo di Vita del Modello\n\nGestione dei Dati: Gestione dell’ingestione dei dati, convalida e controllo delle versioni.\nAddestramento dei Modelli: Tecniche e pratiche per un addestramento dei modelli efficace e scalabile.\nValutazione dei Modelli: Strategie per testare e convalidare le prestazioni dei modelli.\nDistribuzione dei modelli: Approcci per la distribuzione dei modelli in ambienti di produzione.\n\nIntegrazione di Sviluppo e Operazioni\n\nPipeline CI/CD: Integrazione dei modelli ML in pipeline di integrazione e distribuzione continue.\nGestione dell’infrastruttura: Impostazione e manutenzione dell’infrastruttura necessaria per il training e la distribuzione dei modelli.\nComunicazione e Collaborazione: Garanzia di una comunicazione e collaborazione fluide tra data scientist, ingegneri ML e team operativi.\n\nEccellenza operativa\n\nMonitoraggio: Tecniche per il monitoraggio delle prestazioni dei modelli, della deriva dei dati e dello stato operativo.\nGovernance: Implementazione di policy per la verificabilità, la conformità e le considerazioni etiche dei modelli.\n\n\n\n13.7.1 Gestione del Ciclo di Vita del Modello\n\nGestione dei Dati\nNei tradizionali MLOps centralizzati, i dati vengono aggregati in grandi dataset e data lake, poi elaborati su server cloud o “on-prem” [in sede]. Tuttavia, MLOps embedded si basa su dati decentralizzati da sensori locali sui dispositivi. I dispositivi raccolgono batch più piccoli di dati incrementali, spesso rumorosi e non strutturati. Con vincoli di connettività, questi dati non possono sempre essere trasmessi istantaneamente al cloud e devono essere memorizzati nella cache in modo intelligente ed elaborati all’edge.\nA causa della potenza di calcolo limitata sui dispositivi embedded, i dati si possono solo preelaborare e pulire in modo minimo prima della trasmissione. Il filtraggio e l’elaborazione anticipati avvengono nei gateway edge per ridurre i carichi di trasmissione. Mentre si sfrutta l’archiviazione cloud, altre elaborazioni e archiviazioni avvengono all’edge per tenere conto della connettività intermittente. I dispositivi identificano e trasmettono solo i sottoinsiemi di dati più critici al cloud.\nAnche l’etichettatura richiede un accesso centralizzato ai dati, che richiede tecniche più automatizzate come l’apprendimento federato, in cui i dispositivi etichettano in modo collaborativo i dati dei peer. Con i dispositivi edge personali, la privacy dei dati e le normative sono preoccupazioni critiche. La raccolta, la trasmissione e l’archiviazione dei dati devono essere sicure e conformi.\nAd esempio, uno smartwatch può raccogliere il conteggio dei passi giornalieri, la frequenza cardiaca e le coordinate GPS. Questi dati vengono memorizzati nella cache locale e trasmessi a un gateway edge quando è disponibile il WiFi: il gateway elabora e filtra i dati prima di sincronizzare i sottoinsiemi rilevanti con la piattaforma cloud per riaddestrare i modelli.\n\n\nAddestramento del Modello\nNei tradizionali MLOps centralizzati, i modelli vengono addestrati utilizzando dati abbondanti tramite deep learning su server GPU cloud ad alta potenza. Tuttavia, glii MLOps embedded necessitano di maggiore supporto in termini di complessità del modello, disponibilità dei dati e risorse di elaborazione per l’addestramento.\nIl volume di dati aggregati è molto più basso, spesso richiedendo tecniche come l’apprendimento federato tra dispositivi per creare set di addestramento. La natura specializzata dei dati edge limita anche i set di dati pubblici per il pre-addestramento. Per questioni di privacy, i campioni di dati devono essere strettamente controllati e resi anonimi ove possibile.\nInoltre, i modelli devono utilizzare architetture semplificate ottimizzate per hardware edge a bassa potenza. Date le limitazioni di elaborazione, le GPU di fascia alta sono inaccessibili per un deep learning intensivo. L’addestramento sfrutta server edge e cluster a bassa potenza con approcci distribuiti per spartire il carico.\nIl “transfer learning” emerge come una strategia cruciale per affrontare la scarsità di dati e l’irregolarità nell’apprendimento automatico, in particolare negli scenari di edge computing. Come illustrato in Figura 13.5, questo approccio prevede il pre-training di modelli su grandi set di dati pubblici e la loro successiva messa a punto su dati edge specifici del dominio. La figura raffigura una rete neurale in cui i layer iniziali (da W_{A1} a W_{A4}), responsabili dell’estrazione delle feature generali, sono congelati (indicati da una linea tratteggiata verde). Questi layer conservano la conoscenza delle attività precedenti, accelerando l’apprendimento e riducendo i requisiti di risorse. Gli ultimi livelli (da W_{A5} a W_{A7}), oltre la linea tratteggiata blu, sono messi a punto per l’attività specifica, concentrandosi sull’apprendimento delle feature specifiche dell’attività.\n\n\n\n\n\n\nFigura 13.5: Trasferimento dell’apprendimento in MLOps. Fonte: HarvardX.\n\n\n\nQuesto metodo non solo mitiga la scarsità di dati, ma si adatta anche alla natura decentralizzata dei dati embedded. Inoltre, tecniche come l’apprendimento incrementale sul dispositivo possono personalizzare ulteriormente i modelli in base a casi d’uso specifici. La mancanza di dati ampiamente etichettati in molti domini motiva anche l’uso di tecniche semi-supervisionate, che completano l’approccio di apprendimento per trasferimento. Sfruttando le conoscenze preesistenti e adattandole a compiti specializzati, l’apprendimento per trasferimento all’interno di un framework MLOps consente ai modelli di ottenere prestazioni più elevate con meno risorse, anche in ambienti con vincoli di dati.\nAd esempio, un assistente domestico intelligente può pre-addestrare un modello di riconoscimento audio su clip YouTube pubbliche, il che aiuta a eseguire il bootstrap con conoscenze generali. Quindi trasferisce l’apprendimento a un piccolo campione di dati domestici per classificare elettrodomestici ed eventi personalizzati, specializzandosi nel modello. Il modello si trasforma in una rete neurale leggera ottimizzata per dispositivi abilitati al microfono in tutta la casa.\nPertanto, gli MLOps embedded affrontano sfide acute nella costruzione di set di dati di training, nella progettazione di modelli efficienti e nella distribuzione del calcolo per lo sviluppo del modello rispetto alle impostazioni tradizionali. Dati i vincoli embedded, è necessario un attento adattamento, come l’apprendimento tramite trasferimento e il training distribuito, per addestrare i modelli.\n\n\nValutazione del Modello\nNei tradizionali MLOps centralizzati, i modelli vengono valutati principalmente utilizzando metriche di accuratezza e dataset di test di holdout. Tuttavia, gli MLOps embedded richiedono una valutazione più olistica che tenga conto dei vincoli di sistema oltre all’accuratezza.\nI modelli devono essere testati in anticipo e spesso su hardware edge distribuito che copre diverse configurazioni. Oltre all’accuratezza, fattori come latenza, utilizzo della CPU, ingombro di memoria e consumo energetico sono criteri di valutazione critici. I modelli vengono selezionati in base a compromessi tra queste metriche per soddisfare i vincoli dei dispositivi edge.\nAnche la deriva dei dati deve essere monitorata, dove i modelli addestrati sui dati cloud degradano in accuratezza nel tempo sui dati edge locali. I dati embedded hanno spesso una maggiore variabilità rispetto ai set di addestramento centralizzati. Valutare i modelli su diversi campioni di dati edge operativi è fondamentale. Ma a volte, ottenere i dati per monitorare la deriva può essere difficile se questi dispositivi sono in circolazione e la comunicazione è una barriera.\nIl monitoraggio continuo fornisce visibilità sulle prestazioni del mondo reale dopo l’implementazione, rivelando colli di bottiglia non evidenziati durante i test. Ad esempio, un aggiornamento del modello di una smart camera potrebbe essere inizialmente testato su 100 telecamere e poi annullato se si osserva un calo della precisione, prima di essere esteso a tutte le 5000 telecamere.\n\n\nDistribuzione del Modello\nNegli MLOps tradizionali, le nuove versioni del modello vengono distribuite direttamente sui server tramite endpoint API. Tuttavia, i dispositivi embedded richiedono meccanismi di distribuzione ottimizzati per ricevere modelli aggiornati. Gli aggiornamenti over-the-air (OTA) forniscono un approccio standardizzato alla distribuzione wireless di nuove versioni di software o firmware ai dispositivi embedded. Invece dell’accesso API diretto, i pacchetti OTA consentono la distribuzione remota di modelli e dipendenze come bundle pre-costruiti. In alternativa, l’apprendimento federato consente aggiornamenti del modello senza accesso diretto ai dati di training grezzi. Questo approccio decentralizzato ha il potenziale per un miglioramento continuo del modello, ma necessita di piattaforme MLOps robuste.\nLa distribuzione del modello si basa su interfacce fisiche come connessioni seriali USB o UART per dispositivi profondamente embedded privi di connettività. Il packaging del modello segue ancora principi simili agli aggiornamenti OTA, ma il meccanismo di distribuzione è adattato alle capacità dell’hardware edge. Inoltre, spesso vengono utilizzati protocolli OTA specializzati ottimizzati per reti IoT anziché protocolli WiFi o Bluetooth standard. I fattori chiave includono efficienza, affidabilità, sicurezza e telemetria, come il monitoraggio dei progressi, soluzioni come Mender. Io fornisce servizi OTA incentrati su embedded che gestiscono aggiornamenti differenziali tra flotte di dispositivi.\nFigura 13.6 presenta una panoramica di “Model Lifecycle Management” in un contesto MLOps, illustrando il flusso dallo sviluppo (in alto a sinistra) alla distribuzione e al monitoraggio (in basso a destra). Il processo inizia con lo sviluppo ML, in cui il codice e le configurazioni sono “version-controlled”. La gestione dei dati e dei modelli è fondamentale per il processo, coinvolgendo set di dati e repository di funzionalità. Training continuo, conversione del modello e registro del modello sono fasi chiave nell’operazionalizzazione della training. La distribuzione del modello include la fornitura del modello e la gestione dei log di fornitura. Sono in atto meccanismi di allarme per segnalare i problemi, che alimentano il monitoraggio continuo per garantire le prestazioni e l’affidabilità del modello nel tempo. Questo approccio integrato garantisce che i modelli siano sviluppati e mantenuti in modo efficace durante tutto il loro ciclo di vita.\n\n\n\n\n\n\nFigura 13.6: Gestione del ciclo di vita del modello. Fonte: HarvardX.\n\n\n\n\n\n\n13.7.2 Integrazione di Sviluppo e Operazioni\n\nPipeline CI/CD\nNelle MLOps tradizionali, una solida infrastruttura CI/CD come Jenkins e Kubernetes consente l’automazione della pipeline per la distribuzione di modelli su larga scala. Tuttavia, le MLOps embedded necessitano di questa infrastruttura centralizzata e di flussi di lavoro CI/CD più personalizzati per i dispositivi edge.\nLa creazione di pipeline CI/CD deve tenere conto di un panorama frammentato di diverse versioni hardware, firmware e vincoli di connettività. Non esiste una piattaforma standard per orchestrare le pipeline e il supporto degli strumenti è più limitato.\nI test devono coprire in anticipo questo ampio spettro di dispositivi embedded target, il che è difficile senza un accesso centralizzato. Le aziende devono investire molto nell’acquisizione e nella gestione dell’infrastruttura di test nell’ecosistema embedded eterogeneo.\nGli aggiornamenti over-the-air richiedono la configurazione di server specializzati per distribuire in modo sicuro i bundle di modelli ai dispositivi sul campo. Anche le procedure di rollout e rollback devono essere attentamente personalizzate per particolari famiglie di dispositivi.\nCon gli strumenti CI/CD tradizionali meno applicabili, le MLOps embedded si affidano maggiormente a script personalizzati e integrazione. Le aziende adottano approcci diversi, dai framework open source alle soluzioni completamente interne. Una stretta integrazione tra sviluppatori, ingegneri edge e clienti finali stabilisce processi di rilascio affidabili.\nPertanto, gli MLOps embedded non possono sfruttare l’infrastruttura cloud centralizzata per CI/CD. Le aziende combinano pipeline personalizzate, infrastruttura di test e distribuzione OTA per distribuire modelli su sistemi edge frammentati e disconnessi.\n\n\nGestione dell’Infrastruttura\nNei tradizionali MLOps centralizzati, l’infrastruttura comporta l’approvvigionamento di server cloud, GPU e reti ad alta larghezza di banda per carichi di lavoro intensivi come l’addestramento di modelli e la fornitura di previsioni su larga scala. Tuttavia, gli MLOps embedded richiedono un’infrastruttura più eterogenea che si estende su dispositivi edge, gateway e cloud.\nI dispositivi edge come i sensori catturano e preelaborano i dati localmente prima della trasmissione intermittente per evitare di sovraccaricare le reti: i gateway aggregano ed elaborano i dati dei dispositivi prima di inviare sottoinsiemi selezionati al cloud per l’addestramento e l’analisi. Il cloud fornisce gestione centralizzata ed elaborazione supplementare.\nQuesta infrastruttura necessita di una stretta integrazione e bilanciamento dei carichi di elaborazione e comunicazione. La larghezza di banda di rete è limitata, il che richiede un attento filtraggio e compressione dei dati. Le capacità di elaborazione edge sono modeste rispetto al cloud, imponendo vincoli di ottimizzazione.\nLa gestione di aggiornamenti OTA sicuri su grandi flotte di dispositivi presenta sfide all’edge. I rollout devono essere incrementali e pronti per il rollback per una rapida mitigazione. Dato l’ambiente decentralizzato, l’aggiornamento dell’infrastruttura edge richiede coordinamento.\nAd esempio, un impianto industriale può eseguire l’elaborazione di base del segnale sui sensori prima di inviare i dati a un gateway on-prem. Il gateway gestisce l’aggregazione dei dati, il monitoraggio dell’infrastruttura e gli aggiornamenti OTA. Solo i dati curati vengono trasmessi al cloud per analisi avanzate e riaddestramento del modello.\nMLOps embedded richiede una gestione olistica dell’infrastruttura distribuita che abbraccia edge vincolato, gateway e cloud centralizzato. I carichi di lavoro sono bilanciati tra i livelli tenendo conto delle sfide di connettività, elaborazione e sicurezza.\n\n\nComunicazione e Collaborazione\nNelle MLOps tradizionali, la collaborazione tende a concentrarsi su data scientist, ingegneri ML e team DevOps. Tuttavia, le MLOps embedded richiedono un coordinamento interfunzionale più stretto tra ruoli aggiuntivi per affrontare i vincoli di sistema.\nGli ingegneri edge ottimizzano le architetture dei modelli per gli ambienti hardware target. Forniscono feedback ai data scientist durante lo sviluppo in modo che i modelli si adattino anticipatamente alle capacità dei dispositivi. Analogamente, i team di prodotto definiscono i requisiti operativi informati dai contesti degli utenti finali.\nCon più stakeholder nell’ecosistema embedded, i canali di comunicazione devono facilitare la condivisione delle informazioni tra team centralizzati e remoti. Il monitoraggio dei problemi e la gestione dei progetti garantiscono l’allineamento.\nGli strumenti collaborativi ottimizzano i modelli per dispositivi specifici. I data scientist possono registrare i problemi replicati dai dispositivi sul campo in modo che i modelli siano specializzati in dati di nicchia. L’accesso remoto ai dispositivi facilita il debug e la raccolta dati.\nAd esempio, i data scientist possono collaborare con i team sul campo che gestiscono flotte di turbine eoliche per recuperare campioni di dati operativi. Questi dati vengono utilizzati per specializzare i modelli rilevando anomalie specifiche per quella classe di turbine. Gli aggiornamenti dei modelli vengono testati in simulazioni e rivisti dagli ingegneri prima dell’implementazione sul campo.\nGli MLOps embedded impongono un coordinamento continuo tra data scientist, ingegneri, clienti finali e altre parti interessate durante l’intero ciclo di vita del ML. Grazie a una stretta collaborazione, i modelli possono essere personalizzati e ottimizzati per i dispositivi edge mirati.\n\n\n\n13.7.3 Eccellenza operativa\n\nMonitoraggio\nIl monitoraggio MLOps tradizionale si concentra sul monitoraggio centralizzato dell’accuratezza del modello, delle metriche delle prestazioni e della deriva dei dati. Tuttavia, MLOps embedded deve tenere conto del monitoraggio decentralizzato su diversi dispositivi e ambienti edge.\nI dispositivi edge richiedono una raccolta dati ottimizzata per trasmettere metriche di monitoraggio chiave senza sovraccaricare le reti. Le metriche aiutano a valutare le prestazioni del modello, i modelli di dati, l’utilizzo delle risorse e altri comportamenti sui dispositivi remoti.\nCon una connettività limitata, vengono eseguite più analisi all’edge prima di aggregare le informazioni centralmente. I gateway svolgono un ruolo chiave nel monitoraggio dello stato di salute della flotta e nel coordinamento degli aggiornamenti software. Gli indicatori confermati vengono infine propagati al cloud.\nUn’ampia copertura dei dispositivi è impegnativa ma critica. Possono sorgere problemi specifici per determinati tipi di dispositivi, quindi il monitoraggio deve coprire l’intero spettro. Le distribuzioni “canary” aiutano a testare i processi di monitoraggio prima del ridimensionamento.\nIl rilevamento delle anomalie identifica gli incidenti che richiedono il rollback dei modelli o la riqualificazione su nuovi dati. Tuttavia, l’interpretazione degli allarmi richiede la comprensione dei contesti dei dispositivi univoci in base all’input di ingegneri e clienti.\nAd esempio, una casa automobilistica può monitorare i veicoli autonomi per gli indicatori di degradazione del modello utilizzando la memorizzazione nella cache, l’aggregazione e i flussi in tempo reale. Gli ingegneri valutano quando le anomalie identificate garantiscono gli aggiornamenti OTA per migliorare i modelli in base a fattori come la posizione e l’età del veicolo.\nIl monitoraggio MLOps embedded fornisce osservabilità nelle prestazioni del modello e del sistema in ambienti edge decentralizzati. Un’attenta raccolta, analisi e collaborazione dei dati fornisce informazioni significative per mantenere l’affidabilità.\n\n\nGovernance\nNelle MLOps tradizionali, la governance si concentra sulla spiegabilità del modello, la correttezza e la conformità per i sistemi centralizzati. Tuttavia, le MLOps embedded devono anche affrontare le sfide di governance a livello di dispositivo relative alla privacy dei dati, alla sicurezza e alla protezione.\nCon i sensori che raccolgono dati personali e sensibili, la governance dei dati locali sui dispositivi è fondamentale. I controlli di accesso ai dati, l’anonimizzazione e la memorizzazione nella cache crittografata aiutano ad affrontare i rischi per la privacy e la conformità come HIPAA e GDPR. Gli aggiornamenti devono mantenere patch e impostazioni di sicurezza.\nLa governance della sicurezza considera gli impatti fisici del comportamento difettoso del dispositivo. I guasti potrebbero causare condizioni non sicure in veicoli, fabbriche e sistemi critici. Ridondanza, sistemi di sicurezza e sistemi di allarme aiutano a mitigare i rischi.\nLa governance tradizionale, come il monitoraggio dei bias e la spiegabilità del modello, rimane imperativa ma è più difficile da implementare per l’intelligenza artificiale embedded. Anche dare un’occhiata ai modelli black-box su dispositivi a basso consumo pone delle sfide.\nAd esempio, un dispositivo medico può cancellare i dati personali sul dispositivo prima della trasmissione. I rigidi protocolli di governance dei dati approvano gli aggiornamenti del modello. La spiegabilità del modello è limitata, ma l’attenzione è rivolta al rilevamento di comportamenti anomali. I sistemi di backup prevengono i guasti.\nLa governance MLOps embedded deve comprendere privacy, sicurezza, protezione, trasparenza ed etica. Sono necessarie tecniche specializzate e collaborazione di squadra per aiutare a stabilire fiducia e responsabilità all’interno di ambienti decentralizzati.\n\n\n\n13.7.4 Confronto\nTabella 13.2 evidenzia le somiglianze e le differenze tra MLOps Tradizionali e MLOps Embedded sulla base di tutto ciò che abbiamo imparato finora:\n\n\n\nTabella 13.2: Confronto tra le pratiche MLOps Tradizionali e quelle MLOps Embedded.\n\n\n\n\n\n\n\n\n\n\nArea\nMLOps Tradizionali\nMLOps Embedded\n\n\n\n\nGestione dei Dati\nGrandi set di dati, data lake, feature store\nAcquisizione dati sul dispositivo, edge caching ed elaborazione\n\n\nSviluppo del Modello\nSfrutta il deep learning, reti neurali complesse, addestramento GPU\nVincoli sulla complessità del modello, necessità di ottimizzazione\n\n\nDistribuzione\nCluster di server, distribuzione cloud, bassa latenza su larga scala\nDistribuzione OTA su dispositivi, connettività intermittente\n\n\nMonitoraggio\nDashboard, log, allarmi per le prestazioni del modello cloud\nMonitoraggio sul dispositivo di previsioni, utilizzo delle risorse\n\n\nRiqualificazione\nRi-addestramento dei modelli su nuovi dati\nApprendimento federato da dispositivi, ri-addestramento edge\n\n\nInfrastruttura\nInfrastruttura cloud dinamica\nInfrastruttura edge/cloud eterogenea\n\n\nCollaborazione\nMonitoraggio degli esperimenti condivisi e registro dei modelli\nCollaborazione per l’ottimizzazione specifica del dispositivo\n\n\n\n\n\n\nQuindi, mentre Embedded MLOps condivide i principi fondamentali di MLOps, si trova ad affrontare vincoli unici nell’adattare flussi di lavoro e infrastrutture specificamente per dispositivi edge con risorse limitate.\n\n\n13.7.5 MLOps Tradizionali\nGoogle, Microsoft e Amazon offrono la loro versione di servizi ML gestiti. Questi includono servizi che gestiscono il training e la sperimentazione dei modelli, l’hosting e il ridimensionamento dei modelli e il monitoraggio. Queste offerte sono disponibili tramite un’API e SDK client, nonché tramite interfacce utente Web. Sebbene sia possibile creare le proprie soluzioni MLOps end-to-end utilizzando parti di ciascuno, i maggiori vantaggi in termini di facilità d’uso derivano dal rimanere all’interno di un singolo ecosistema di provider per sfruttare le integrazioni tra servizi.\nLe sezioni seguenti presentano una rapida panoramica dei servizi che rientrano in ogni parte del ciclo di vita MLOps descritto sopra, fornendo esempi da diversi provider. È importante notare che lo spazio MLOps si sta evolvendo rapidamente; nuove aziende e prodotti stanno entrando in scena a un ritmo rapido. Gli esempi menzionati non intendono fungere da approvazione delle offerte di aziende specifiche, ma piuttosto illustrare i tipi di soluzioni disponibili sul mercato.\n\nGestione dei Dati\nL’archiviazione dei dati e il versioning sono elementi essenziali per qualsiasi offerta commerciale e la maggior parte sfrutta le soluzioni di archiviazione generiche esistenti come S3. Altri utilizzano opzioni più specializzate come l’archiviazione basata su git (ad esempio: Hugging Face’s Dataset Hub). Questa è un’area in cui i provider semplificano il supporto delle opzioni di archiviazione dati dei concorrenti, poiché non vogliono che ciò rappresenti una barriera per l’adozione del resto dei loro servizi MLOps. Ad esempio, la pipeline di addestramento di Vertex AI supporta senza problemi i set di dati archiviati in S3, Google Cloud Buckets o Dataset Hub di Hugging Face.\n\n\nAddestramento del Modello\nI servizi di training gestiti sono il punto di forza dei provider cloud, in quanto forniscono accesso on-demand a hardware che è fuori dalla portata della maggior parte delle aziende più piccole. Fatturano solo l’hardware per il tempo del training, accelerato con GPU accessibili anche ai team di sviluppatori più piccoli. Il controllo che gli sviluppatori hanno sul loro flusso di lavoro del training può variare notevolmente a seconda delle loro esigenze. Alcuni provider hanno servizi che forniscono poco più dell’accesso alle risorse e si affidano allo sviluppatore per gestire autonomamente il ciclo di training, il logging e l’archiviazione dei modelli. Altri servizi sono semplici come puntare a un modello di base e a un set di dati etichettato per avviare un lavoro di messa a punto completamente gestito (ad esempio: Vertex AI Fine Tuning).\nUna parola di avvertimento: A partire dal 2023, la domanda di hardware GPU supera di gran lunga l’offerta e, di conseguenza, i provider cloud stanno razionando l’accesso alle loro GPU. In alcune regioni dei data center, le GPU potrebbero non essere disponibili o richiedere contratti a lungo termine.\n\n\nValutazione del Modello\nLe attività di valutazione del modello in genere comportano il monitoraggio dell’accuratezza, della latenza e dell’utilizzo delle risorse dei modelli sia nelle fasi di test che di produzione. A differenza dei sistemi embedded, i modelli ML distribuiti sul cloud beneficiano di una connettività Internet costante e di capacità di logging illimitate. Di conseguenza, è spesso possibile acquisire e loggare ogni richiesta e risposta. Ciò rende trattabile la riproduzione o la generazione di richieste sintetiche per confrontare modelli e versioni diversi.\nAlcuni provider offrono anche servizi che automatizzano il monitoraggio degli esperimenti di modifica degli iperparametri del modello. Tracciano le esecuzioni e le prestazioni e generano artefatti da queste esecuzioni di training del modello. Esempio: WeightsAndBiases\n\n\nDistribuzione del Modello\nOgni provider in genere ha un servizio denominato “model registry”, in cui vengono archiviati e a cui si accede ai modelli di training. Spesso, questi registri possono anche fornire accesso a modelli di base che sono open source o forniti da grandi aziende tecnologiche (o, in alcuni casi, come LLAMA, entrambi!). Questi registri dei modelli costituiscono un luogo comune per confrontare tutti i modelli e le loro versioni per consentire un facile processo decisionale su quale scegliere per un dato caso d’uso. Esempio: Vertex AI’s model registry\nDal registro dei modelli, distribuire un modello a un endpoint di inferenza è rapido e semplice, e gestisce il provisioning delle risorse, il download del peso del modello e l’hosting di un dato modello. Questi servizi in genere forniscono accesso al modello tramite un’API REST con cui possono essere inviate richieste di inferenza. A seconda del tipo di modello, è possibile configurare risorse specifiche, ad esempio quale tipo di acceleratore GPU potrebbe essere necessario per raggiungere le prestazioni desiderate. Alcuni provider possono anche offrire opzioni di inferenza “serverless” o batch che non necessitano di un endpoint persistente per accedere al modello. Esempio: AWS SageMaker Inference\n\n\n\n13.7.6 MLOps Embedded\nNonostante la proliferazione di nuovi strumenti ML Ops in risposta all’aumento della domanda, le sfide descritte in precedenza hanno limitato la disponibilità di tali strumenti negli ambienti di sistemi embedded. Più di recente, nuovi strumenti come Edge Impulse (Janapa Reddi et al. 2023) hanno reso il processo di sviluppo un po’ più semplice, come descritto di seguito.\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler, Daniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. «Edge Impulse: An MLOps Platform for Tiny Machine Learning». Proceedings of Machine Learning and Systems 5.\n\nEdge Impulse\nEdge Impulse è una piattaforma di sviluppo end-to-end per la creazione e l’implementazione di modelli di apprendimento automatico su dispositivi edge come microcontrollori e piccoli processori. Rende l’apprendimento automatico embedded più accessibile agli sviluppatori di software attraverso la sua interfaccia web di facile utilizzo e strumenti integrati per la raccolta dati, lo sviluppo di modelli, l’ottimizzazione e l’implementazione. Le sue funzionalità principali includono quanto segue:\n\nFlusso di lavoro intuitivo drag-and-drop per la creazione di modelli ML senza bisogno di codifica\nStrumenti per l’acquisizione, l’etichettatura, la visualizzazione e la preelaborazione dei dati dai sensori\nScelta di architetture di modelli, tra cui reti neurali e apprendimento non supervisionato\nTecniche di ottimizzazione dei modelli per bilanciare metriche delle prestazioni e vincoli hardware\nDistribuzione senza soluzione di continuità su dispositivi edge tramite compilazione, SDK e benchmark\nFunzionalità di collaborazione per team e integrazione con altre piattaforme\n\nCon Edge Impulse, gli sviluppatori con competenze limitate in data science possono sviluppare modelli ML specializzati che funzionano in modo efficiente in piccoli ambienti di elaborazione. Fornisce una soluzione completa per la creazione di intelligenza embedded e l’avanzamento del machine learning.\n\nInterfaccia utente\nEdge Impulse è stato progettato con sette principi chiave: accessibilità, funzionalità end-to-end, un approccio incentrato sui dati, interattività, estensibilità, orientamento al team e supporto della community. L’interfaccia utente intuitiva, mostrata in Figura 13.7, guida gli sviluppatori di tutti i livelli di esperienza attraverso il caricamento dei dati, la selezione di un’architettura di modello, l’addestramento del modello e la sua distribuzione su piattaforme hardware pertinenti. Va notato che, come qualsiasi strumento, Edge Impulse è destinato ad assistere, non a sostituire, le considerazioni fondamentali come la determinazione se ML è una soluzione appropriata o l’acquisizione delle competenze di dominio richieste per una determinata applicazione.\n\n\n\n\n\n\nFigura 13.7: Schermata dell’interfaccia utente di Edge Impulse per la creazione di flussi di lavoro dai dati di input alle funzionalità di output.\n\n\n\nCiò che rende Edge Impulse degno di nota è il suo flusso di lavoro end-to-end completo ma intuitivo. Gli sviluppatori iniziano caricando i propri dati tramite l’interfaccia utente grafica (GUI) o gli strumenti dell’interfaccia a riga di comando (CLI), dopodiché possono esaminare campioni grezzi e visualizzare la distribuzione dei dati nelle suddivisioni di addestramento e test. Successivamente, gli utenti possono scegliere tra vari “blocchi” di pre-elaborazione per facilitare l’elaborazione del segnale digitale (DSP). Mentre vengono forniti valori di parametri predefiniti, gli utenti possono personalizzare i parametri in base alle proprie esigenze, osservando le considerazioni su memoria e la latenza visualizzate. Gli utenti possono scegliere facilmente la propria architettura di rete neurale, senza bisogno di alcun codice.\nGrazie all’editor visivo della piattaforma, gli utenti possono personalizzare i componenti dell’architettura e i parametri specifici, assicurandosi al contempo che il modello sia ancora addestrabile. Gli utenti possono anche sfruttare algoritmi di apprendimento non supervisionato, come il clustering K-means e i Gaussian Mixture Model (GMM).\n\n\nOttimizzazioni\nPer adattarsi ai vincoli di risorse delle applicazioni TinyML, Edge Impulse fornisce una “matrice di confusione” che riassume le metriche chiave delle prestazioni, tra cui accuratezza per classe e punteggi F1. La piattaforma chiarisce i compromessi tra prestazioni del modello, dimensioni e latenza utilizzando simulazioni in Renode e benchmarking specifici del dispositivo. Per i casi di utilizzo dei dati in streaming, uno strumento di calibrazione delle prestazioni sfrutta un algoritmo genetico per trovare configurazioni di post-elaborazione ideali che bilanciano tassi di falsa accettazione e falso rifiuto. Sono disponibili tecniche come quantizzazione, ottimizzazione del codice e ottimizzazione specifica del dispositivo per i modelli. Per la distribuzione, i modelli possono essere compilati in formati appropriati per i dispositivi edge target. Gli SDK del firmware nativi consentono anche la raccolta diretta dei dati sui dispositivi.\nOltre a semplificare lo sviluppo, Edge Impulse ridimensiona il processo di modellazione stesso. Una funzionalità chiave è EON Tuner, uno strumento di apprendimento automatico automatico (AutoML) che assiste gli utenti nell’ottimizzazione degli iperparametri in base ai vincoli di sistema. Esegue una ricerca casuale per generare rapidamente configurazioni per l’elaborazione del segnale digitale e le fasi di training. I modelli risultanti vengono visualizzati affinché l’utente possa selezionarli in base a metriche di prestazioni, memoria e latenza pertinenti. Per i dati, l’apprendimento attivo facilita il training su un piccolo sottoinsieme etichettato, seguito dall’etichettatura manuale o automatica di nuovi campioni in base alla vicinanza alle classi esistenti. Ciò espande l’efficienza dei dati.\n\n\nCasi d’Uso\nOltre all’accessibilità della piattaforma stessa, il team di Edge Impulse ha ampliato la base di conoscenza dell’ecosistema ML embedded. La piattaforma si presta ad ambienti accademici, essendo stata utilizzata in corsi online e workshop in loco a livello globale. Sono stati pubblicati numerosi casi di studio con casi d’uso di settore e di ricerca, in particolare Oura Ring, che utilizza ML per identificare i pattern del sonno. Il team ha reso i repository open source su GitHub, facilitando la crescita della comunità. Gli utenti possono anche rendere pubblici i progetti per condividere tecniche e scaricare librerie da condividere tramite Apache. L’accesso a livello di organizzazione consente la collaborazione sui flussi di lavoro.\nNel complesso, Edge Impulse è straordinariamente completo e integrabile per i flussi di lavoro degli sviluppatori. Piattaforme più grandi come Google e Microsoft si concentrano maggiormente sul cloud rispetto ai sistemi embedded. I framework TinyMLOps come Neuton AI e Latent AI offrono alcune funzionalità ma non hanno le capacità end-to-end di Edge Impulse. TensorFlow Lite Micro è il motore di inferenza standard grazie alla flessibilità, allo stato open source e all’integrazione di TensorFlow, ma utilizza più memoria e storage rispetto al compilatore EON di Edge Impulse. Altre piattaforme devono essere aggiornate, focalizzate sull’aspetto accademico o più versatili. In sintesi, Edge Impulse semplifica e amplia l’apprendimento automatico embedded tramite una piattaforma accessibile e automatizzata.\n\n\n\nLimitazioni\nSebbene Edge Impulse fornisca una pipeline accessibile per ML embedded, permangono importanti limitazioni e rischi. Una sfida fondamentale è la qualità e la disponibilità dei dati: i modelli sono validi solo quanto i dati utilizzati per addestrarli. Gli utenti devono disporre di campioni etichettati sufficienti che catturino l’ampiezza delle condizioni operative previste e delle modalità di errore. Le anomalie e i valori anomali etichettati sono critici, ma richiedono molto tempo per essere raccolti e identificati. Dati insufficienti o distorti comportano scarse prestazioni del modello indipendentemente dalle capacità dello strumento.\nAnche il deploying su dispositivi a bassa potenza presenta sfide intrinseche. I modelli ottimizzati potrebbero comunque dover richiedere più risorse per MCU a bassissimo consumo. Trovare il giusto equilibrio tra compressione e accuratezza richiede un po’ di sperimentazione. Lo strumento semplifica, ma deve comunque eliminare la necessità di competenze di base in ML ed elaborazione del segnale. Gli ambienti embedded limitano anche il debug e l’interpretabilità rispetto al cloud.\nSebbene siano ottenibili risultati impressionanti, gli utenti non dovrebbero considerare Edge Impulse come una soluzione “Push Button ML”. Un’attenta definizione dell’ambito del progetto, la raccolta dati, la valutazione del modello e il test sono comunque essenziali. Come con qualsiasi strumento di sviluppo, si consigliano aspettative ragionevoli e diligenza nell’applicazione. Tuttavia, Edge Impulse può accelerare la prototipazione e l’implementazione di ML embedded per gli sviluppatori disposti a investire lo sforzo di data science e ingegneria richiesto.\n\n\n\n\n\n\nEsercizio 13.1: Edge Impulse\n\n\n\n\n\nPronti a far salire di livello i vostri piccoli progetti di machine-learning? Combiniamo la potenza di Edge Impulse con le fantastiche visualizzazioni di Weights & Biases (WandB). In questo Colab, si imparerà a monitorare i progressi del training del modello come un professionista! Si immagini di vedere fantastici grafici del modello che diventa più intelligente, confrontando diverse versioni e assicurandovi che la vostra IA funzioni al meglio anche su dispositivi minuscoli.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#casi-di-studio",
    "href": "contents/ops/ops.it.html#casi-di-studio",
    "title": "13  Operazioni di ML",
    "section": "13.8 Casi di Studio",
    "text": "13.8 Casi di Studio\n\n13.8.1 Oura Ring\nOura Ring è un dispositivo indossabile che può misurare l’attività, il sonno e il recupero quando viene posizionato sul dito dell’utente. Utilizzando sensori per tracciare le metriche fisiologiche, il dispositivo utilizza ML embedded per prevedere le fasi del sonno. Per stabilire una base di legittimità nel settore, Oura ha condotto un esperimento di correlazione per valutare il successo del dispositivo nel prevedere le fasi del sonno rispetto a uno studio di base. Ciò ha portato a una solida correlazione del 62% rispetto alla base di riferimento dell’82-83%. Pertanto, il team ha deciso di determinare come migliorare ulteriormente le proprie prestazioni.\nLa prima sfida è stata ottenere dati migliori in termini sia di quantità che di qualità. Avrebbero potuto ospitare uno studio più ampio per ottenere un set di dati più completo, ma i dati sarebbero stati così rumorosi e grandi che sarebbe stato difficile aggregarli, ripulirli e analizzarli. È qui che entra in gioco Edge Impulse.\nAbbiamo condotto un massiccio studio sul sonno su 100 uomini e donne di età compresa tra 15 e 73 anni in tre continenti (Asia, Europa e Nord America). Oltre a indossare l’Oura Ring, i partecipanti erano tenuti a sottoporsi al test PSG [https://it.wikipedia.org/wiki/Polisonnografia] standard del settore, che ha fornito una “etichetta” per questo set di dati. Con 440 notti di sonno da parte di 106 partecipanti, il set di dati ha totalizzato 3.444 ore di lunghezza tra dati Ring e PSG. Con Edge Impulse, Oura ha potuto caricare e consolidare facilmente i dati da diverse fonti in un bucket S3 privato. Sono stati anche in grado di impostare una Data Pipeline per unire campioni di dati in file individuali e preelaborare i dati senza dover eseguire lo “scrubbing” [pulizia] manuale.\nCol tempo risparmiato nell’elaborazione dei dati grazie a Edge Impulse, il team Oura ha potuto concentrarsi sui driver chiave della propria previsione. Hanno estratto solo tre tipi di dati dei sensori: frequenza cardiaca, movimento e temperatura corporea. Dopo aver suddiviso i dati utilizzando la validazione incrociata a cinque livelli e classificato le fasi del sonno, il team ha ottenuto una correlazione del 79%, solo pochi punti percentuali in meno rispetto allo standard. Hanno prontamente distribuito due tipi di modelli di rilevamento del sonno: uno semplificato utilizzando solo l’accelerometro dell’anello e uno più completo sfruttando i segnali periferici mediati dal Autonomic Nervous System (ANS) [sistema nervoso autonomo] e le caratteristiche circadiane [ritmo cardiaco in 24 ore]. Con Edge Impulse, hanno in programma di condurre ulteriori analisi di diversi tipi di attività e sfruttare la scalabilità della piattaforma per continuare a sperimentare con diverse fonti di dati e sottoinsiemi di caratteristiche estratte.\nMentre la maggior parte della ricerca ML si concentra su fasi dominanti del modello come il training e la messa a punto, questo caso di studio sottolinea l’importanza di un approccio olistico alle operazioni ML, in cui anche le fasi iniziali di aggregazione dei dati e pre-elaborazione hanno un impatto fondamentale sulla riuscita.\n\n\n13.8.2 ClinAIOps\nDiamo un’occhiata a MLOps nel contesto del monitoraggio medico sanitario per comprendere meglio come MLOps “maturi” in un’implementazione nel mondo reale. In particolare, prendiamo in considerazione il continuous therapeutic monitoring (CTM) [monitoraggio terapeutico continuo] abilitato da dispositivi e sensori indossabili. Il CTM cattura dati fisiologici dettagliati dai pazienti, offrendo l’opportunità di aggiustamenti più frequenti e personalizzati ai trattamenti.\nI sensori indossabili abilitati per ML consentono un monitoraggio continuo fisiologico e dell’attività al di fuori delle cliniche, aprendo possibilità per aggiustamenti terapeutici tempestivi e basati sui dati. Ad esempio, i biosensori indossabili per l’insulina (Psoma e Kanthou 2023) e i sensori ECG da polso per il monitoraggio del glucosio (J. Li et al. 2021) possono automatizzare il dosaggio di insulina per il diabete, i sensori ECG e PPG da polso possono regolare gli anticoagulanti in base ai modelli di fibrillazione atriale (Attia et al. 2018; Guo et al. 2019), e gli accelerometri che tracciano l’andatura possono innescare cure preventive per la mobilità in declino negli anziani (Liu et al. 2022). La varietà di segnali che ora possono essere catturati passivamente e continuamente consente la titolazione e l’ottimizzazione della terapia su misura per le mutevoli esigenze di ogni paziente. Chiudendo il cerchio tra rilevamento fisiologico e risposta terapeutica con TinyML e apprendimento sul dispositivo, i dispositivi indossabili sono pronti a trasformare molte aree della medicina personalizzata.\n\nPsoma, Sotiria D., e Chryso Kanthou. 2023. «Wearable Insulin Biosensors for Diabetes Management: Advances and Challenges». Biosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, e Zedong Nie. 2021. «Non-invasive Monitoring of Three Glucose Ranges Based On ECG By Using DBSCAN-CNN». IEEE Journal of Biomedical and Health Informatics 25 (9): 3340–50. https://doi.org/10.1109/jbhi.2021.3072628.\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman, Suraj Kapa, Paul A. Friedman, e Peter A. Noseworthy. 2018. «Noninvasive assessment of dofetilide plasma concentration using a deep learning (neural network) analysis of the surface electrocardiogram: A proof of concept study». PLOS ONE 13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia, Li Yan, et al. 2019. «Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation». Journal of the American College of Cardiology 74 (19): 2365–75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella Jensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022. «Monitoring gait at home with radio waves in Parkinson’s disease: A marker of severity, progression, and medication response». Science Translational Medicine 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\nIl ML è molto promettente nell’analisi dei dati CTM per fornire raccomandazioni basate sui dati per gli aggiustamenti della terapia. Ma semplicemente distribuire modelli di intelligenza artificiale in “silos”, senza integrarli correttamente nei flussi di lavoro clinici e nel processo decisionale, può portare a una scarsa adozione o a risultati non ottimali. In altre parole, pensare solo a MLOps non è sufficiente per renderli utili nella pratica. Questo studio dimostra che sono necessari framework per incorporare intelligenza artificiale e CTM nella pratica clinica reale senza soluzione di continuità.\nQuesto caso di studio analizza “ClinAIOps” come modello per operazioni ML embedded in ambienti clinici complessi (Chen et al. 2023). Forniamo una panoramica del framework e del motivo per cui è necessario, esaminiamo un esempio di applicazione e discutiamo le principali sfide di implementazione relative al monitoraggio del modello, all’integrazione del flusso di lavoro e agli incentivi per gli stakeholder. L’analisi di esempi concreti come ClinAIOps illumina principi cruciali e best practice per operazioni AI affidabili ed efficaci in molti domini.\nI framework MLOps tradizionali non sono sufficienti per integrare il monitoraggio terapeutico continuo (CTM) e l’IA in contesti clinici per alcuni motivi chiave:\n\nMLOps si concentra sul ciclo di vita del modello ML: training, distribuzione, monitoraggio. Ma l’assistenza sanitaria implica il coordinamento di più stakeholder umani, pazienti e medici, non solo modelli.\nMLOps automatizza il monitoraggio e la gestione dei sistemi IT. Tuttavia, l’ottimizzazione della salute del paziente richiede cure personalizzate e supervisione umana, non solo automazione.\nCTM e l’erogazione dell’assistenza sanitaria sono sistemi sociotecnici complessi con molte parti mobili. MLOps non fornisce un framework per coordinare il processo decisionale umano e AI.\nLe considerazioni etiche relative all’AI sanitaria richiedono giudizio umano, supervisione e responsabilità. I framework MLOps non hanno processi per la supervisione etica.\nI dati sanitari dei pazienti sono altamente sensibili e regolamentati. MLOps da solo non garantisce la gestione delle informazioni sanitarie protette secondo gli standard normativi e di privacy.\nLa convalida clinica dei piani di trattamento guidati dall’AI è essenziale per l’adozione da parte del provider. MLOps non incorpora la valutazione specifica del dominio delle raccomandazioni del modello.\nL’ottimizzazione delle metriche sanitarie come i risultati dei pazienti richiede l’allineamento degli incentivi e dei flussi di lavoro delle parti interessate, che MLOps puramente incentrato sulla tecnologia trascura.\n\nPertanto, integrare efficacemente AI/ML e CTM nella pratica clinica richiede più di semplici modelli e pipeline di dati; richiede il coordinamento di complessi processi decisionali collaborativi tra esseri umani e AI, che ClinAIOps affronta tramite i suoi cicli di feedback multi-stakeholder.\n\nCicli di Feedback\nIl framework ClinAIOps, mostrato in Figura 13.8, fornisce questi meccanismi attraverso tre cicli di feedback. I cicli sono utili per coordinare le informazioni dal monitoraggio fisiologico continuo, l’esperienza del medico e la guida dell’IA tramite cicli di feedback, consentendo una medicina di precisione basata sui dati mantenendo al contempo la responsabilità umana. ClinAIOps fornisce un modello per un’efficace simbiosi uomo-IA nell’assistenza sanitaria: il paziente è al centro, fornendo sfide e obiettivi sanitari che informano il regime terapeutico; il medico supervisiona questo regime, fornendo input per gli aggiustamenti basati sui dati di monitoraggio continuo e sui report sanitari del paziente; mentre gli sviluppatori di IA svolgono un ruolo cruciale creando sistemi che generano allarmi per gli aggiornamenti della terapia, che il medico quindi esamina.\nQuesti cicli di feedback, di cui parleremo di seguito, aiutano a mantenere la responsabilità e il controllo del medico sui piani di trattamento esaminando i suggerimenti dell’IA prima che abbiano un impatto sui pazienti. Aiutano a personalizzare dinamicamente il comportamento e gli output del modello di IA in base allo stato di salute mutevole di ciascun paziente. Contribuiscono a migliorare l’accuratezza del modello e l’utilità clinica nel tempo, imparando dalle risposte del medico e del paziente. Facilitano il processo decisionale condiviso e l’assistenza personalizzata durante le interazioni paziente-medico. Consentono una rapida ottimizzazione delle terapie in base a dati frequenti del paziente che i medici non possono analizzare manualmente.\n\n\n\n\n\n\nFigura 13.8: Ciclo ClinAIOps. Fonte: Chen et al. (2023).\n\n\n\n\nCiclo Paziente-IA\nIl ciclo paziente-IA consente un’ottimizzazione frequente della terapia guidata dal monitoraggio fisiologico continuo. Ai pazienti vengono prescritti dispositivi indossabili come smartwatch o cerotti cutanei per raccogliere passivamente segnali sanitari rilevanti. Ad esempio, un paziente diabetico potrebbe avere un monitoraggio continuo del glucosio o un paziente con malattie cardiache potrebbe indossare un cerotto ECG. Un modello di IA analizza i flussi di dati sanitari longitudinali del paziente nel contesto delle sue cartelle cliniche elettroniche: diagnosi, esami di laboratorio, farmaci e dati demografici. Il modello di IA suggerisce modifiche al regime di trattamento su misura per quell’individuo, come la modifica di una dose di farmaco o di un programma di somministrazione. Piccole modifiche entro un intervallo di sicurezza pre-approvato possono essere apportate dal paziente in modo indipendente, mentre le modifiche più importanti vengono prima esaminate dal medico. Questo stretto feedback tra la fisiologia del paziente e la terapia guidata dall’IA consente ottimizzazioni tempestive basate sui dati come raccomandazioni automatizzate sul dosaggio di insulina basate sui livelli di glucosio in tempo reale per i pazienti diabetici.\n\n\nCiclo Clinico-IA\nIl ciclo clinico-IA consente la supervisione clinica sulle raccomandazioni generate dall’IA per garantire sicurezza e responsabilità. Il modello di IA fornisce al medico raccomandazioni terapeutiche e riepiloghi facilmente esaminabili dei dati rilevanti del paziente su cui si basano i suggerimenti. Ad esempio, un’IA può suggerire di ridurre la dose di farmaci per la pressione sanguigna di un paziente iperteso in base a letture costantemente basse. Il medico può accettare, rifiutare o modificare le modifiche alla prescrizione proposte dall’IA. Questo feedback del medico addestra e migliora ulteriormente il modello. Inoltre, il medico stabilisce i limiti per i tipi e l’entità delle modifiche al trattamento che l’IA può raccomandare autonomamente ai pazienti. Esaminando i suggerimenti dell’IA, il medico mantiene l’autorità di trattamento finale in base al proprio giudizio clinico e alla propria responsabilità. Questo ciclo consente loro di supervisionare i casi dei pazienti con l’assistenza dell’IA in modo efficiente.\n\n\nCiclo Paziente-Clinico\nInvece di una raccolta dati di routine, il medico può concentrarsi sull’interpretazione di modelli di dati di alto livello e sulla collaborazione con il paziente per stabilire obiettivi e priorità di salute. L’assistenza AI libererà anche il tempo dei medici, consentendo loro di concentrarsi maggiormente sull’ascolto delle storie e delle preoccupazioni dei pazienti. Ad esempio, il medico può discutere di cambiamenti di dieta ed esercizio fisico con un paziente diabetico per migliorare il controllo del glucosio in base ai dati di monitoraggio continuo. La frequenza degli appuntamenti può anche essere regolata dinamicamente in base ai progressi del paziente anziché seguire un calendario fisso. Liberato dalla raccolta di dati di base, il medico può fornire “coaching” e cure personalizzate a ciascun paziente informato dai suoi dati sanitari continui. La relazione paziente-medico diventa più produttiva e personalizzata.\n\n\n\nEsempio di Ipertensione\nConsideriamo un esempio. Secondo i “Centers for Disease Control and Prevention”, quasi la metà degli adulti soffre di ipertensione (48.1%, 119.9 milioni). L’ipertensione può essere gestita tramite ClinAIOps con l’aiuto di sensori indossabili utilizzando il seguente approccio:\n\nRaccolta Dati\nI dati raccolti includerebbero il monitoraggio continuo della pressione sanguigna tramite un dispositivo indossato al polso dotato di sensori per fotopletismografia (PPG) ed elettrocardiografia (ECG) per stimare la pressione sanguigna (Q. Zhang, Zhou, e Zeng 2017). Il dispositivo indossabile monitorerebbe anche l’attività fisica del paziente tramite accelerometri embedded. Il paziente registrerebbe tutti i farmaci antipertensivi assunti, insieme all’ora e alla dose. Verrebbero inoltre incorporati i dettagli demografici e la storia clinica del paziente dalla sua cartella clinica elettronica (EHR). Questi dati multimodali del mondo reale forniscono un contesto prezioso al modello di intelligenza artificiale per analizzare i modelli di pressione sanguigna del paziente, i livelli di attività, l’aderenza ai farmaci e le risposte alla terapia.\n\nZhang, Qingxue, Dian Zhou, e Xuan Zeng. 2017. «Highly wearable cuff-less blood pressure and heart rate monitoring with single-arm electrocardiogram and photoplethysmogram signals». BioMedical Engineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nModello di Intelligenza Artificiale\nIl modello di intelligenza artificiale sul dispositivo analizzerebbe le tendenze continue della pressione sanguigna del paziente, i modelli circadiani, i livelli di attività fisica, i comportamenti di aderenza ai farmaci e altri contesti. Utilizzerebbe ML per prevedere dosi ottimali di farmaci antipertensivi e tempi per controllare la pressione sanguigna dell’individuo. Il modello invierebbe raccomandazioni di modifica del dosaggio direttamente al paziente per piccoli aggiustamenti o al medico revisore per l’approvazione per modifiche più significative. Osservando il feedback clinico sulle sue raccomandazioni e valutando i risultati ottenuti sulla pressione sanguigna nei pazienti, il modello di intelligenza artificiale potrebbe essere continuamente riqualificato per migliorarne le prestazioni. L’obiettivo è una gestione della pressione sanguigna completamente personalizzata ottimizzata per le esigenze e le risposte di ciascun paziente.\n\n\nCiclo Paziente-IA\nNel ciclo Paziente-IA, il paziente iperteso riceverebbe notifiche sul suo dispositivo indossabile o sull’app per smartphone collegata che raccomandano modifiche ai suoi farmaci antipertensivi. Per piccole modifiche della dose entro un intervallo di sicurezza predefinito, il paziente potrebbe implementare in modo indipendente la modifica suggerita dal modello di IA al suo regime. Tuttavia, il paziente deve ottenere l’approvazione del medico prima di modificare il dosaggio per modifiche più significative. Fornire raccomandazioni personalizzate e tempestive sui farmaci automatizza un elemento di autogestione dell’ipertensione per il paziente. Può migliorare la sua aderenza al regime e i risultati del trattamento. Il paziente è autorizzato a sfruttare le informazioni dell’IA per controllare meglio la sua pressione sanguigna.\n\n\nCiclo Clinico-IA\nNel ciclo Clinico-IA, il fornitore riceverebbe riepiloghi delle tendenze continue della pressione sanguigna del paziente e visualizzazioni dei suoi modelli di assunzione dei farmaci e dell’aderenza. Esaminano le modifiche al dosaggio antipertensivo suggerite dal modello AI e decidono se approvare, rifiutare o modificare le raccomandazioni prima che raggiungano il paziente. Il medico specifica anche i limiti di quanto l’AI può raccomandare in modo indipendente di modificare i dosaggi senza la supervisione del medico. Se la pressione sanguigna del paziente tende a livelli pericolosi, il sistema avvisa il medico in modo che possa intervenire tempestivamente e modificare i farmaci o richiedere una visita al pronto soccorso. Questo ciclo mantiene responsabilità e sicurezza consentendo al contempo al medico di sfruttare le intuizioni dell’AI mantenendo il medico responsabile dell’approvazione delle principali modifiche al trattamento.\n\n\nCiclo Paziente-Clinico\nNel ciclo Paziente-Clinico, mostrato in Figura 13.9, le visite di persona si concentrerebbero meno sulla raccolta di dati o sulle modifiche di base dei farmaci. Invece, il medico potrebbe interpretare tendenze e modelli di alto livello nei dati di monitoraggio continuo del paziente e avere discussioni mirate su dieta, esercizio fisico, gestione dello stress e altri cambiamenti nello stile di vita per migliorare il controllo della pressione sanguigna in modo olistico. La frequenza degli appuntamenti potrebbe essere ottimizzata dinamicamente in base alla stabilità del paziente anziché seguire un calendario fisso. Poiché il medico non avrebbe bisogno di rivedere tutti i dati granulari, potrebbe concentrarsi sulla fornitura di cure e raccomandazioni personalizzate durante le visite. Con il monitoraggio continuo e l’ottimizzazione assistita dall’intelligenza artificiale dei farmaci tra le visite, la relazione medico-paziente si concentra sugli obiettivi di benessere generale e diventa più incisiva. Questo approccio proattivo e personalizzato basato sui dati può aiutare a evitare complicazioni dell’ipertensione come ictus, insufficienza cardiaca e altre minacce alla salute e al benessere del paziente.\n\n\n\n\n\n\nFigura 13.9: Ciclo interattivo ClinAIOps. Fonte: Chen et al. (2023).\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, e Pranav Rajpurkar. 2023. «A framework for integrating artificial intelligence for clinical care with continuous therapeutic monitoring». Nature Biomedical Engineering, novembre. https://doi.org/10.1038/s41551-023-01115-0.\n\n\n\n\n\nMLOps vs. ClinAIOps\nL’esempio dell’ipertensione illustra bene perché i tradizionali MLOps sono insufficienti per molte applicazioni AI del mondo reale e perché sono invece necessari framework come ClinAIOps.\nCon l’ipertensione, il semplice sviluppo e distribuzione di un modello ML per la regolazione dei farmaci avrebbe successo solo se considerasse il contesto clinico più ampio. Il paziente, il medico e il sistema sanitario hanno preoccupazioni sulla definizione dell’adozione. Il modello AI non può ottimizzare da solo i risultati della pressione sanguigna: richiede l’integrazione con flussi di lavoro, comportamenti e incentivi.\n\nAlcune lacune chiave evidenziate dall’esempio in un approccio MLOps puro:\nIl modello stesso non avrebbe i dati dei pazienti del mondo reale su larga scala per raccomandare trattamenti in modo affidabile. ClinAIOps consente ciò raccogliendo feedback da medici e pazienti tramite monitoraggio continuo.\nI medici si fiderebbero delle raccomandazioni del modello solo con trasparenza, spiegabilità e responsabilità. ClinAIOps mantiene il medico informato per creare fiducia.\nI pazienti hanno bisogno di coaching e motivazione personalizzati, non solo di notifiche AI. Il ciclo paziente-clinico di ClinAIOps facilita questo.\nL’affidabilità dei sensori e l’accuratezza dei dati sarebbero sufficienti solo con la supervisione clinica. ClinAIOps convalida le raccomandazioni.\nLa responsabilità per i risultati del trattamento deve essere chiarita solo con un modello ML. ClinAIOps mantiene la responsabilità umana.\nI sistemi sanitari dovrebbero dimostrare il valore per cambiare i flussi di lavoro. ClinAIOps allinea le parti interessate.\n\nIl caso dell’ipertensione mostra chiaramente la necessità di guardare oltre il training e l’implementazione di un modello ML performante per considerare l’intero sistema sociotecnico umano-IA. Questa è la lacuna principale che ClinAIOps colma rispetto ai tradizionali MLOps. I tradizionali MLOps sono eccessivamente focalizzati sulla tecnologia per automatizzare lo sviluppo e l’implementazione del modello ML, mentre ClinAIOps incorpora il contesto clinico e il coordinamento umano-IA attraverso cicli di feedback multi-stakeholder.\nTabella 13.3 li confronta. Questa tabella evidenzia come, quando si implementa MLOps, sia necessario considerare più dei semplici modelli ML.\n\n\n\nTabella 13.3: Confronto tra operazioni MLOps e AI per uso clinico.\n\n\n\n\n\n\n\n\n\n\n\nMLOps tradizionali\nClinAIOps\n\n\n\n\nFocus\nSviluppo e distribuzione di modelli ML\nCoordinamento del processo decisionale umano e AI\n\n\nParti interessate\nData scientist, ingegneri IT\nPazienti, medici, sviluppatori AI\n\n\nCicli di feedback\nRiqualificazione del modello, monitoraggio\nPaziente-IA, clinico-IA, paziente-clinico\n\n\nObiettivo\nRendere operative le distribuzioni ML\nOttimizzare i risultati di salute del paziente\n\n\nProcessi\nPipeline e infrastruttura automatizzate\nIntegra flussi di lavoro clinici e supervisione\n\n\nConsiderazioni sui dati\nCreazione di set di dati di training\nPrivacy, etica, informazioni sanitarie protette\n\n\nValidazione del modello\nTest delle metriche delle prestazioni del modello\nValutazione clinica delle raccomandazioni\n\n\nImplementazione\nSi concentra sull’integrazione tecnica\nAllinea gli incentivi degli stakeholder umani\n\n\n\n\n\n\n\n\nRiepilogo\nIn ambiti complessi come l’assistenza sanitaria, l’implementazione di successo dell’IA richiede di andare oltre un focus ristretto sul training e il deploying di modelli ML performanti. Come illustrato nell’esempio dell’ipertensione, l’integrazione dell’IA nel mondo reale richiede il coordinamento di diverse parti interessate, l’allineamento degli incentivi, la convalida delle raccomandazioni e il mantenimento della responsabilità. Framework come ClinAIOps, che facilitano il processo decisionale collaborativo tra uomo e IA attraverso cicli di feedback integrati, sono necessari per affrontare queste sfide multiformi. Invece di automatizzare semplicemente le attività, l’IA deve aumentare le capacità umane e i flussi di lavoro clinici. Ciò consente all’IA di avere un impatto positivo sui risultati dei pazienti, sulla salute della popolazione e sull’efficienza dell’assistenza sanitaria.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#conclusione",
    "href": "contents/ops/ops.it.html#conclusione",
    "title": "13  Operazioni di ML",
    "section": "13.9 Conclusione",
    "text": "13.9 Conclusione\nL’ML embedded è pronto a trasformare molti settori abilitando le funzionalità AI direttamente su dispositivi edge come smartphone, sensori e hardware IoT. Tuttavia, lo sviluppo e l’implementazione di modelli TinyML su sistemi embedded con risorse limitate pone sfide uniche rispetto ai tradizionali MLOps basati su cloud.\nQuesto capitolo ha fornito un’analisi approfondita delle principali differenze tra MLOps tradizionali ed embedded nel ciclo di vita del modello, flussi di lavoro di sviluppo, gestione dell’infrastruttura e pratiche operative. Abbiamo discusso di come fattori come connettività intermittente, dati decentralizzati e computing limitato sul dispositivo richiedano tecniche innovative come apprendimento federato, inferenza sul dispositivo e ottimizzazione del modello. Modelli architettonici come apprendimento cross-device e infrastruttura edge-cloud gerarchica aiutano a mitigare i vincoli.\nAttraverso esempi concreti come Oura Ring e ClinAIOps, abbiamo dimostrato i principi applicati per MLOps embedded. I casi di studio hanno evidenziato considerazioni critiche che vanno oltre l’ingegneria ML di base, come l’allineamento degli incentivi delle parti interessate, il mantenimento della responsabilità e il coordinamento del processo decisionale tra uomo e IA. Ciò sottolinea la necessità di un approccio olistico che abbracci sia gli elementi tecnici che quelli umani.\nMentre gli MLOps embedded incontrano degli ostacoli, strumenti emergenti come Edge Impulse e lezioni dai pionieri aiutano ad accelerare l’innovazione del TinyML. Una solida comprensione dei principi fondamentali degli MLOps adattati agli ambienti embedded consentirà a più organizzazioni di superare i vincoli e fornire capacità di intelligenza artificiale distribuita. Man mano che i framework e le best practice maturano, l’integrazione fluida dell’ML nei dispositivi e nei processi edge trasformerà i settori attraverso l’intelligenza localizzata.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#sec-embedded-aiops-resource",
    "href": "contents/ops/ops.it.html#sec-embedded-aiops-resource",
    "title": "13  Operazioni di ML",
    "section": "13.10 Risorse",
    "text": "13.10 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nMLOps, DevOps, and AIOps.\nMLOps overview.\nTiny MLOps.\nMLOps: a use case.\nMLOps: Key Activities and Lifecycle.\nML Lifecycle.\nScaling TinyML: Challenges and Opportunities.\nOperazionalizzazione del Training:\n\nTraining Ops: CI/CD trigger.\nContinuous Integration.\nContinuous Deployment.\nProduction Deployment.\nProduction Deployment: Online Experimentation.\nTraining Ops Impact on MLOps.\n\nDeployment del Modello:\n\nScaling ML Into Production Deployment.\nContainers for Scaling ML Deployment.\nChallenges for Scaling TinyML Deployment: Part 1.\nChallenges for Scaling TinyML Deployment: Part 2.\nModel Deployment Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 13.1\nVideo 13.2\nVideo 13.3\nVideo 13.4\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 13.1\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html",
    "href": "contents/privacy_security/privacy_security.it.html",
    "title": "14  Sicurezza e Privacy",
    "section": "",
    "text": "14.1 Introduzione\nIl “Machine learning” [apprendimento automatico] si è evoluto notevolmente dalle sue origini accademiche, in cui la privacy non era una preoccupazione primaria. Con la migrazione del ML in applicazioni commerciali e consumer, i dati sono diventati più sensibili, comprendendo informazioni personali come comunicazioni, acquisti e dati sanitari. Questa esplosione di disponibilità di dati ha alimentato rapidi progressi nelle capacità del ML. Tuttavia, ha anche esposto nuovi rischi per la privacy, come dimostrato da incidenti come la fuga di dati di AOL nel 2006 e lo scandalo Cambridge Analytica.\nQuesti eventi hanno evidenziato la crescente necessità di affrontare la privacy nei sistemi ML. In questo capitolo, esploriamo insieme considerazioni sulla privacy e sulla sicurezza, poiché sono intrinsecamente collegate nel ML:\nAd esempio, una telecamera di sicurezza domestica basata su ML deve proteggere i flussi video da accessi non autorizzati e fornire protezioni della privacy per garantire che solo gli utenti previsti possano visualizzare il filmato. Una violazione della sicurezza o della privacy potrebbe esporre momenti privati degli utenti.\nI sistemi ML embedded come assistenti intelligenti e dispositivi indossabili sono onnipresenti ed elaborano dati intimi degli utenti. Tuttavia, i loro vincoli computazionali spesso impediscono protocolli di sicurezza pesanti. I progettisti devono bilanciare le esigenze di prestazioni con rigorosi standard di sicurezza e privacy adattati alle limitazioni dell’hardware embedded.\nQuesto capitolo fornisce conoscenze essenziali per affrontare il complesso panorama di privacy e sicurezza dell’ML embedded. Esploreremo le vulnerabilità e tratteremo varie tecniche che migliorano la privacy e la sicurezza all’interno dei vincoli di risorse dei sistemi embedded.\nCi auguriamo che sviluppando una comprensione olistica dei rischi e delle misure di sicurezza, si acquisiranno i principi per sviluppare applicazioni ML embedded sicure ed etiche.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#introduzione",
    "href": "contents/privacy_security/privacy_security.it.html#introduzione",
    "title": "14  Sicurezza e Privacy",
    "section": "",
    "text": "La privacy si riferisce al controllo dell’accesso ai dati sensibili degli utenti, come informazioni finanziarie o dati biometrici raccolti da un’applicazione ML.\nLa sicurezza protegge i sistemi e i dati ML da hacking, furto e uso improprio.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#terminologia",
    "href": "contents/privacy_security/privacy_security.it.html#terminologia",
    "title": "14  Sicurezza e Privacy",
    "section": "14.2 Terminologia",
    "text": "14.2 Terminologia\nIn questo capitolo parleremo insieme di sicurezza e privacy, quindi ci sono termini chiave su cui dobbiamo essere chiari.\n\nPrivacy: Si consideri una telecamera di sicurezza domestica basata su ML che identifica e registra potenziali minacce. Questa telecamera registra informazioni identificabili di individui che si avvicinano e potenzialmente entrano in questa casa, compresi i volti. Le preoccupazioni sulla privacy potrebbero riguardare chi può accedere a questi dati.\nSicurezza: Si consideri una telecamera di sicurezza domestica basata su ML che identifica e registra potenziali minacce. L’aspetto della sicurezza garantirebbe che gli hacker non possano accedere a questi video e ai modelli di riconoscimento.\nMinaccia: Utilizzando il nostro esempio della telecamera di sicurezza domestica, una minaccia potrebbe essere un hacker che tenta di accedere a video live o archiviati o che utilizza falsi input per ingannare il sistema.\nVulnerabilità: Una vulnerabilità comune potrebbe essere una rete scarsamente protetta tramite la quale la telecamera si connette a Internet, che potrebbe essere sfruttata per accedere ai dati.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#precedenti-storici",
    "href": "contents/privacy_security/privacy_security.it.html#precedenti-storici",
    "title": "14  Sicurezza e Privacy",
    "section": "14.3 Precedenti Storici",
    "text": "14.3 Precedenti Storici\nSebbene le specifiche della sicurezza hardware dell’apprendimento automatico possano essere distinte, il campo dei sistemi embedded ha una storia di incidenti di sicurezza che forniscono lezioni fondamentali per tutti i sistemi connessi, compresi quelli che utilizzano ML. Ecco analisi dettagliate di violazioni passate:\n\n14.3.1 Stuxnet\nNel 2010, qualcosa di inaspettato è stato trovato su un computer in Iran: un virus informatico molto complicato che gli esperti non avevano mai visto prima. Stuxnet era un worm informatico dannoso che prendeva di mira i sistemi di controllo di supervisione e acquisizione dati (SCADA) ed era progettato per danneggiare il programma nucleare iraniano (Farwell e Rohozinski 2011). Stuxnet stava utilizzando quattro “exploit zero-day”, attacchi che sfruttano debolezze segrete nel software di cui nessuno è ancora a conoscenza. Ciò ha reso Stuxnet molto subdolo e difficile da rilevare.\n\nFarwell, James P., e Rafal Rohozinski. 2011. «Stuxnet and the Future of Cyber War». Survival 53 (1): 23–40. https://doi.org/10.1080/00396338.2011.555586.\nMa Stuxnet non è stato progettato per rubare informazioni o spiare le persone. Il suo obiettivo era la distruzione fisica, sabotare le centrifughe della centrale nucleare iraniana di Natanz! Quindi come ha fatto il virus a raggiungere i computer della centrale di Natanz, che avrebbe dovuto essere disconnessa dal mondo esterno per motivi di sicurezza? Gli esperti pensano che qualcuno abbia inserito una chiavetta USB contenente Stuxnet nella rete interna di Natanz. Ciò ha permesso al virus di “saltare” da un sistema esterno ai sistemi di controllo nucleare isolati e scatenare il caos.\nStuxnet era un malware incredibilmente avanzato creato dai governi nazionali per passare dal regno digitale alle infrastrutture del mondo reale. Ha preso di mira in modo specifico importanti macchine industriali, dove l’apprendimento automatico embedded è altamente applicabile in un modo mai visto prima. Il virus ha lanciato un segnale di allarme su come i sofisticati attacchi informatici potrebbero ora distruggere fisicamente apparecchiature e strutture.\nQuesta violazione è stata significativa a causa della sua sofisticatezza; Stuxnet ha preso di mira in modo specifico i “programmable logic controllers (PLC)” utilizzati per automatizzare processi elettromeccanici come la velocità delle centrifughe per l’arricchimento dell’uranio. Il worm sfruttava le vulnerabilità del sistema operativo Windows per ottenere l’accesso al software Siemens Step7 che controlla i PLC. Nonostante non sia un attacco diretto ai sistemi ML, Stuxnet è rilevante per tutti i sistemi embedded in quanto mostra il potenziale degli attori a livello statale per progettare attacchi che collegano il mondo informatico e quello fisico con effetti devastanti.\n\n\n14.3.2 Hack della Jeep Cherokee\nL’hack della Jeep Cherokee è stato un evento rivoluzionario che ha dimostrato i rischi insiti nelle automobili sempre più connesse (Miller 2019). In una dimostrazione controllata, i ricercatori della sicurezza hanno sfruttato da remoto una vulnerabilità nel sistema di entertainment Uconnect, che aveva una connessione cellulare a Internet. Sono stati in grado di controllare il motore, la trasmissione e i freni del veicolo, allarmando l’industria automobilistica e spingendola a riconoscere le gravi implicazioni per la sicurezza delle vulnerabilità informatiche nei veicoli. Video 14.1 di seguito è riportato un breve documentario dell’attacco.\n\nMiller, Charlie. 2019. «Lessons learned from hacking a car». IEEE Design &amp; Test 36 (6): 7–9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\n\n\n\n\nVideo 14.1: Hack della Jeep Cherokee\n\n\n\n\n\n\nSebbene non si sia trattato di un attacco a un sistema ML in sé, l’affidamento dei veicoli moderni ai sistemi embedded per funzioni critiche per la sicurezza presenta parallelismi significativi con l’implementazione di ML nei sistemi embedded, sottolineando la necessità di una sicurezza robusta a livello hardware.\n\n\n14.3.3 Botnet Mirai\nLa botnet Mirai ha coinvolto l’infezione di dispositivi in rete come fotocamere digitali e lettori DVR (Antonakakis et al. 2017). Nell’ottobre 2016, la botnet è stata utilizzata per condurre uno dei più grandi attacchi DDoS, interrompendo l’accesso a Internet negli Stati Uniti. L’attacco è stato possibile perché molti dispositivi utilizzavano nomi utente e password predefiniti, che sono stati facilmente sfruttati dal malware Mirai per controllare i dispositivi. Video 14.2 spiega come funziona la botnet Mirai.\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie Bursztein, Jaime Cochran, Zakir Durumeric, et al. 2017. «Understanding the mirai botnet». In 26th USENIX security symposium (USENIX Security 17), 1093–1110.\n\n\n\n\n\n\nVideo 14.2: Botnet Mirai\n\n\n\n\n\n\nSebbene i dispositivi non fossero basati su ML, l’incidente è un duro promemoria di ciò che può accadere quando numerosi dispositivi embedded con scarsi controlli di sicurezza vengono collegati in rete, cosa che sta diventando sempre più comune con la crescita dei dispositivi IoT basati su ML.\n\n\n14.3.4 Implicazioni\nQueste violazioni storiche dimostrano gli effetti a cascata delle vulnerabilità hardware nei sistemi embedded. Ogni incidente offre un precedente per comprendere i rischi e progettare protocolli di sicurezza migliori. Ad esempio, la botnet Mirai evidenzia l’immenso potenziale distruttivo quando gli autori delle minacce possono ottenere il controllo su dispositivi in rete con sicurezza debole, una situazione che sta diventando sempre più comune con i sistemi ML. Molti dispositivi ML attuali funzionano come dispositivi “edge” pensati per raccogliere ed elaborare dati localmente prima di inviarli al cloud. Proprio come le telecamere e i DVR compromessi da Mirai, i dispositivi ML edge spesso si basano su hardware embedded come processori ARM ed eseguono sistemi operativi leggeri come Linux. Proteggere le credenziali del dispositivo è fondamentale.\nAllo stesso modo, l’hacking della Jeep Cherokee è stato un momento spartiacque per l’industria automobilistica. Ha esposto gravi vulnerabilità nei crescenti sistemi di veicoli connessi in rete e la loro mancanza di isolamento dai sistemi di guida principali come freni e sterzo. In risposta, i produttori di automobili hanno investito molto in nuove misure di sicurezza informatica, anche se probabilmente permangono delle lacune.\nChrysler ha effettuato un richiamo per correggere il software vulnerabile Uconnect, che consentiva l’exploit remoto. Ciò includeva l’aggiunta di protezioni a livello di rete per impedire l’accesso esterno non autorizzato e la compartimentazione dei sistemi di bordo per limitare i movimenti laterali. Sono stati aggiunti ulteriori livelli di crittografia per i comandi inviati tramite il bus CAN all’interno dei veicoli.\nL’incidente ha anche stimolato la creazione di nuovi standard e best practice per la sicurezza informatica. L’Auto-ISAC è stato istituito per consentire alle case automobilistiche di condividere informazioni e la NHTSA ha guidato i rischi di gestione. Sono state sviluppate nuove procedure di test e audit per valutare le vulnerabilità in modo proattivo. Gli effetti collaterali continuano a guidare il cambiamento nel settore automobilistico poiché le auto diventano sempre più definite dal software.\nSfortunatamente, i produttori spesso trascurano la sicurezza quando sviluppano nuovi dispositivi edge ML, utilizzando password predefinite, comunicazioni non crittografate, aggiornamenti firmware non protetti, ecc. Tali vulnerabilità potrebbero consentire agli aggressori di ottenere l’accesso e controllare i dispositivi su larga scala infettandoli con malware. Con una botnet di dispositivi ML compromessi, gli aggressori potrebbero sfruttare la loro potenza di calcolo aggregata per attacchi DDoS su infrastrutture critiche.\nSebbene questi eventi non abbiano coinvolto direttamente hardware di machine learning, i principi degli attacchi si estendono ai sistemi ML, che spesso coinvolgono dispositivi embedded e architetture di rete simili. Poiché l’hardware ML è sempre più integrato con il mondo fisico, proteggerlo da tali violazioni è fondamentale. L’evoluzione delle misure di sicurezza in risposta a questi incidenti fornisce preziose informazioni sulla protezione dei sistemi ML attuali e futuri da vulnerabilità analoghe.\nLa natura distribuita dei dispositivi edge ML significa che le minacce possono propagarsi rapidamente attraverso le reti. E se i dispositivi vengono utilizzati per scopi critici come dispositivi medici, controlli industriali o veicoli a guida autonoma, il potenziale danno fisico dei bot ML armati potrebbe essere grave. Proprio come Mirai ha dimostrato il potenziale pericoloso dei dispositivi IoT scarsamente protetti, la prova del nove per la sicurezza dell’hardware ML sarà quanto questi dispositivi siano vulnerabili o resilienti ad attacchi simili a worm. La posta in gioco aumenta man mano che il ML si diffonde in ambiti critici per la sicurezza, ponendo l’onere sui produttori e sugli operatori di sistema di incorporare le lezioni di Mirai.\nLa lezione è l’importanza di progettare per la sicurezza fin dall’inizio e di avere difese stratificate. Il caso Jeep evidenzia potenziali vulnerabilità per i sistemi ML in merito alle interfacce software esterne e all’isolamento tra sottosistemi. I produttori di dispositivi e piattaforme ML dovrebbero assumere un approccio proattivo e completo simile alla sicurezza piuttosto che lasciarlo come un ripensamento. Una risposta rapida e la diffusione delle best practice saranno cruciali man mano che le minacce si evolvono.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-i-modelli-ml",
    "href": "contents/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-i-modelli-ml",
    "title": "14  Sicurezza e Privacy",
    "section": "14.4 Minacce alla Sicurezza per i Modelli ML",
    "text": "14.4 Minacce alla Sicurezza per i Modelli ML\nI modelli ML affrontano rischi per la sicurezza che possono comprometterne l’integrità, le prestazioni e l’affidabilità se non affrontati adeguatamente. Sebbene esistano diverse minacce, le principali includono: Furto di modelli, in cui gli avversari rubano i parametri proprietari del modello e i dati sensibili in essi contenuti. Avvelenamento dei dati, che compromette i modelli tramite manomissione dei dati. Gli attacchi avversari ingannano il modello per fare previsioni errate o indesiderate.\n\n14.4.1 Furto di Modelli\nIl furto di modelli si verifica quando un aggressore ottiene l’accesso non autorizzato a un modello ML distribuito. La preoccupazione in questo caso è il furto della struttura del modello e dei parametri addestrati, nonché dei dati proprietari in esso contenuti (Ateniese et al. 2015). Il furto di modelli è una minaccia reale e crescente, come dimostrato da casi come quello dell’ex ingegnere di Google Anthony Levandowski, che presumibilmente ha rubato i progetti di auto a guida autonoma di Waymo e ha fondato un’azienda concorrente. Oltre all’impatto economico, il furto di modelli può seriamente compromettere la privacy e consentire ulteriori attacchi.\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, e Giovanni Felici. 2015. «Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers». Int. J. Secur. Netw. 10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\nAd esempio, si consideri un modello ML sviluppato per raccomandazioni personalizzate in un’applicazione di e-commerce. Se un concorrente ruba questo modello, ottiene informazioni su analisi aziendali, preferenze dei clienti e persino segreti commerciali racchiusi nei dati del modello. Gli aggressori potrebbero sfruttare i modelli rubati per creare input più efficaci per attacchi di “inversione del modello”, deducendo dettagli privati sui dati di addestramento del modello. Un modello di raccomandazione di e-commerce clonato potrebbe rivelare i comportamenti di acquisto e i dati demografici dei clienti.\nPer comprendere gli attacchi di “inversione del modello”, si consideri un sistema di riconoscimento facciale utilizzato per concedere l’accesso a strutture protette. Il sistema viene addestrato su un set di dati di foto dei dipendenti. Un aggressore potrebbe dedurre le caratteristiche del set di dati originale osservando l’output del modello su vari input. Ad esempio, supponiamo che il livello di confidenza del modello per un particolare volto sia significativamente più alto per un dato set di caratteristiche. In tal caso, un aggressore potrebbe dedurre che qualcuno con quelle caratteristiche è probabile che sia nel set di dati di addestramento.\nLa metodologia di “inversione del modello” in genere prevede i seguenti passaggi:\n\nAccesso agli Output del Modello: L’aggressore interroga il modello ML con dati di input e osserva gli output. Ciò avviene spesso tramite un’interfaccia legittima, come un’API pubblica.\nAnalisi dei Confidence Score: Per ogni input, il modello fornisce un “punteggio di confidenza” che riflette quanto l’input sia simile ai dati di training.\nReverse-Engineering: Analizzando i punteggi di confidenza o le probabilità di output, gli aggressori possono utilizzare tecniche di ottimizzazione per ricostruire ciò che ritengono sia vicino ai dati di input originali.\n\nUn esempio storico di tale vulnerabilità esplorata è stata la ricerca sugli attacchi di inversione contro il set di dati del premio Netflix degli Stati Uniti, in cui i ricercatori hanno dimostrato che era possibile conoscere le preferenze cinematografiche di un individuo, il che potrebbe portare a violazioni della privacy (Narayanan e Shmatikov 2006).\n\nNarayanan, Arvind, e Vitaly Shmatikov. 2006. «How to break anonymity of the netflix prize dataset». arXiv preprint cs/0610105.\nIl furto di modelli implica che potrebbe portare a perdite economiche, minare il vantaggio competitivo e violare la privacy degli utenti. C’è anche il rischio di attacchi di inversione del modello, in cui un avversario potrebbe immettere vari dati nel modello rubato per dedurre informazioni sensibili sui dati di addestramento.\nIn base alla risorsa desiderata, gli attacchi con furto di modelli possono essere suddivisi in due categorie: proprietà esatte del modello e comportamento approssimativo del modello.\n\nFurto di Proprietà Esatte del Modello\nIn questi attacchi, l’obiettivo è estrarre informazioni su metriche concrete, come i parametri appresi di una rete, gli iperparametri ottimizzati e l’architettura interna dei layer del modello (Oliynyk, Mayer, e Rauber 2023).\n\nParametri Appresi: Gli avversari mirano a rubare la conoscenza appresa di un modello (pesi e bias) per replicarla. Il furto di parametri è generalmente utilizzato con altri attacchi, come il furto di architettura, che non hanno conoscenza dei parametri.\nIperparametri Ottimizzati: L’addestramento è costoso e l’identificazione della configurazione ottimale degli iperparametri (come velocità di apprendimento e regolarizzazione) può richiedere molto tempo e risorse. Di conseguenza, rubare gli iperparametri ottimizzati di un modello consente agli avversari di replicare il modello senza sostenere gli stessi costi di sviluppo.\nArchitettura del Modello: Questo attacco riguarda la progettazione e la struttura specifiche del modello, come strati, neuroni e pattern di connettività. Oltre a ridurre i costi di training associati, questo furto rappresenta un grave rischio per la proprietà intellettuale, potenzialmente compromettendo il vantaggio competitivo di un’azienda. Il furto di architettura può essere ottenuto sfruttando attacchi side-channel (discussi più avanti).\n\n\n\nFurto del Comportamento Approssimativo del Modello\nInvece di estrarre valori numerici esatti dei parametri del modello, questi attacchi mirano a riprodurre il comportamento del modello (previsioni ed efficacia), il processo decisionale e le caratteristiche di alto livello (Oliynyk, Mayer, e Rauber 2023). Queste tecniche mirano a ottenere risultati simili pur consentendo deviazioni interne nei parametri e nell’architettura. I tipi di furto di comportamento approssimativo includono l’ottenimento dello stesso livello di efficacia e l’ottenimento di coerenza di previsione.\n\nOliynyk, Daryna, Rudolf Mayer, e Andreas Rauber. 2023. «I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences». ACM Comput. Surv. 55 (14s): 1–41. https://doi.org/10.1145/3595292.\n\nLivello di efficacia: Gli aggressori mirano a replicare le capacità decisionali del modello piuttosto che concentrarsi sui valori precisi dei parametri. Ciò avviene attraverso la comprensione del comportamento complessivo del modello. Consideriamo uno scenario in cui un aggressore desidera copiare il comportamento di un modello di classificazione delle immagini. Analizzando i limiti decisionali del modello, l’attacco ottimizza il suo modello per raggiungere un’efficacia paragonabile al modello originale. Ciò potrebbe comportare l’analisi di 1) la matrice di confusione per comprendere l’equilibrio delle metriche di previsione (vero positivo, vero negativo, falso positivo, falso negativo) e 2) altre metriche di prestazione, come punteggio F1 e precisione, per garantire che i due modelli siano comparabili.\nCoerenza della Previsione: L’attaccante cerca di allineare i modelli di previsione del proprio modello con quelli del modello target. Ciò comporta l’abbinamento degli output di previsione (sia positivi che negativi) sullo stesso set di input e la garanzia della coerenza distributiva tra classi diverse. Ad esempio, prendiamo in considerazione un modello di elaborazione del linguaggio naturale (NLP) che genera un’analisi del sentiment per le recensioni di film (etichettando le recensioni come positive, neutre o negative). L’attaccante cercherà di mettere a punto il proprio modello per adattarlo alla previsione dei modelli originali sullo stesso set di recensioni di film. Ciò include la garanzia che il modello commetta gli stessi errori (previsioni errate) commessi dal modello target.\n\n\n\nCaso di Studio\nNel 2018, Tesla ha intentato una causa contro la startup di auto a guida autonoma Zoox, sostenendo che ex dipendenti avevano rubato dati riservati e segreti commerciali relativi al sistema di assistenza alla guida autonoma di Tesla.\nTesla ha affermato che diversi suoi ex dipendenti hanno sottratto oltre 10 GB. di dati proprietari, inclusi modelli ML e codice sorgente, prima di unirsi a Zoox. Ciò avrebbe incluso uno dei modelli di riconoscimento delle immagini cruciali di Tesla per l’identificazione degli oggetti.\nIl furto di questo modello proprietario sensibile potrebbe aiutare Zoox ad abbreviare anni di sviluppo ML e duplicare le capacità di Tesla. Tesla ha sostenuto che questo furto di I.P. ha causato danni finanziari e competitivi significativi. C’erano anche preoccupazioni che potesse consentire attacchi di inversione del modello per dedurre dettagli privati sui dati di test di Tesla.\nI dipendenti di Zoox hanno negato di aver rubato informazioni proprietarie. Tuttavia, il caso evidenzia i rischi significativi del furto di modelli, che consente la clonazione di modelli commerciali, causando ripercussioni economiche e aprendo la porta a ulteriori violazioni della privacy dei dati.\n\n\n\n14.4.2 Avvelenamento dei Dati\nL’avvelenamento dei dati è un attacco in cui i dati di training vengono manomessi, portando a un modello compromesso (Biggio, Nelson, e Laskov 2012). Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ciò può essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.\n\nBiggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. «Poisoning Attacks against Support Vector Machines». In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\nIl processo di solito prevede i seguenti passaggi:\n\nInjection: L’aggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un’ispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.\nTraining: Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei modelli di dati.\nDeployment: Una volta distribuito il modello, l’addestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilità prevedibili che l’aggressore può sfruttare.\n\nGli impatti dell’avvelenamento dei dati vanno oltre i semplici errori di classificazione o cali di accuratezza. Ad esempio, se dati errati o dannosi vengono introdotti nel set di addestramento di un sistema di riconoscimento dei segnali stradali, il modello potrebbe imparare a classificare erroneamente i segnali di stop come segnali di precedenza, il che può avere pericolose conseguenze nel mondo reale, specialmente nei sistemi autonomi embedded come i veicoli autonomi.\nL’avvelenamento dei dati può degradare l’accuratezza di un modello, costringerlo a fare previsioni errate o farlo comportare in modo imprevedibile. In applicazioni critiche come l’assistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza.\nEsistono sei categorie principali di avvelenamento dei dati (Oprea, Singhal, e Vassilev 2022):\n\nOprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. «Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?» Computer 55 (11): 94–99. https://doi.org/10.1109/mc.2022.3190787.\n\nAttacchi di Disponibilità: Questi attacchi cercano di compromettere la funzionalità complessiva di un modello. Fanno sì che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio è il “label flipping”, in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.\nAttacchi Mirati: A differenza degli attacchi alla disponibilità, gli attacchi mirati mirano a compromettere un numero limitato di campioni di test. Quindi, l’effetto è localizzato su un numero limitato di classi, mentre il modello mantiene lo stesso livello di accuratezza originale sulla maggior parte delle classi. La natura mirata dell’attacco richiede che l’aggressore conosca le classi del modello, rendendo più difficile il rilevamento di questi attacchi.\nAttacchi Backdoor: In questi attacchi, un avversario prende di mira modelli specifici nei dati. L’aggressore introduce una backdoor (un trigger o pattern nascosto e dannoso) nei dati di training, ad esempio modificando determinate feature nei dati strutturati o un pattern di pixel in una posizione fissa. Ciò fa sì che il modello associ il modello dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di test che contengono un modello dannoso, fa false previsioni, evidenziando l’importanza della cautela e della prevenzione nel ruolo dei professionisti della sicurezza dei dati.\nAttacchi di Sotto-popolazione: Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l’accuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilità e mirati: eseguire attacchi di disponibilità (degrado delle prestazioni) nell’ambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:\nScope: Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un attore inserisce immagini manipolate di un cartello di avvertimento “rallentamenti” (con perturbazioni o modelli attentamente studiati), che fa sì che un’auto autonoma non riconosca tale cartello e non rallenti. D’altro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica è un esempio di attacco di sotto-popolazione.\nConoscenza: Mentre gli attacchi mirati richiedono un alto grado di familiarità con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.\n\n\nCaso di Studio 1\nNel 2017, i ricercatori hanno dimostrato un attacco di avvelenamento dei data contro un modello di classificazione della tossicità popolare chiamato Perspective (Hosseini et al. 2017). Questo modello ML rileva commenti tossici online.\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, e Radha Poovendran. 2017. «Deceiving google’s perspective api built for detecting toxic comments». ArXiv preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\nI ricercatori hanno aggiunto commenti tossici generati sinteticamente con lievi errori di ortografia e grammaticali ai dati di training del modello. Ciò ha lentamente corrotto il modello, facendogli classificare erroneamente un numero crescente di input gravemente tossici come non tossici nel tempo.\nDopo il ri-addestramento sui dati avvelenati, il tasso di falsi negativi del modello è aumentato dall’1,4% al 27%, consentendo ai commenti estremamente tossici di aggirare il rilevamento. I ricercatori hanno avvertito che questo furtivo “data poisoning” potrebbe consentire la diffusione di discorsi di odio, molestie e abusi se implementato contro sistemi di moderazione reali.\nQuesto caso evidenzia come l’avvelenamento dei dati possa degradare l’accuratezza e l’affidabilità del modello. Per le piattaforme di social media, un attacco di avvelenamento che compromette il rilevamento della tossicità potrebbe portare alla proliferazione di contenuti dannosi e alla sfiducia nei sistemi di moderazione ML. L’esempio dimostra perché proteggere l’integrità dei dati di training e monitorare l’avvelenamento è fondamentale in tutti i domini applicativi.\n\n\nCaso di Studio 2\nÈ interessante notare che gli attacchi di “data poisoning” non sono sempre dannosi (Shan et al. 2023). Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l’Università di Chicago, utilizza il data poisoning per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di intelligenza artificiale generativa. Gli artisti possono utilizzare lo strumento per modificare le proprie immagini in modo sottile prima di caricarle online.\nSebbene queste modifiche siano impercettibili all’occhio umano, possono degradare significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando integrate nei dati di addestramento. I modelli generativi possono essere manipolati per produrre output irrealistici o privi di senso. Ad esempio, con solo 300 immagini corrotte, i ricercatori dell’Università di Chicago sono riusciti a ingannare l’ultimo modello “Stable Diffusion” per generare immagini di cani che assomigliano a felini o bovini quando richiesto per le automobili.\nCon l’aumento della quantità di immagini corrotte online, l’efficacia dei modelli addestrati su dati estratti diminuirà esponenzialmente. Inizialmente, identificare i dati corrotti è difficile e richiede un intervento manuale. Successivamente, la contaminazione si diffonde rapidamente ai concetti correlati, poiché i modelli generativi stabiliscono connessioni tra le parole e le loro rappresentazioni visive. Di conseguenza, un’immagine corrotta di un’“auto” potrebbe propagarsi in immagini generate collegate a termini come “camion”, “treno” e “autobus”.\nD’altro canto, questo strumento può essere utilizzato in modo dannoso e influenzare le applicazioni legittime del modello generativo. Ciò dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.\nFigura 17.26 mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in varie categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un’auto genera una mucca.\n\n\n\n\n\n\nFigura 14.1: Avvelenamento dei Dati. Fonte: Shan et al. (2023).\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, e Ben Y Zhao. 2023. «Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models». ArXiv preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\n\n\n\n14.4.3 Attacchi Avversari\nGli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) (Parrish et al. 2023). Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono “hackerare” il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui lievi, spesso impercettibili alterazioni dei dati di input possono indurre un modello ML a fare una previsione errata.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. «Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models». ArXiv preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. «Zero-Shot Text-to-Image Generation». In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. «High-Resolution Image Synthesis with Latent Diffusion Models». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\nÈ possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE (Ramesh et al. 2021) o Stable Diffusion (Rombach et al. 2022). Ad esempio, alterando i valori dei pixel di un’immagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.\nGli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l’inferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input dannosi con perturbazioni per fuorviare il riconoscimento di pattern del modello, essenzialmente “hackerando” le percezioni del modello.\nGli attacchi avversari rientrano in diversi scenari:\n\nAttacchi Whitebox: L’attaccante ha una conoscenza completa del funzionamento interno del modello target, inclusi i dati di addestramento, i parametri e l’architettura. Questo ampio accesso facilita lo sfruttamento delle vulnerabilità del modello. L’attaccante può sfruttare debolezze specifiche e sottili per costruire esempi avversari altamente efficaci.\nAttacchi Blackbox: A differenza degli attacchi Whitebox, in quelli Blackbox l’attaccante ha poca o nessuna conoscenza del modello target. L’attore avversario deve osservare attentamente il comportamento di output del modello per eseguire l’attacco.\nAttacchi Greybox: Questi attacchi occupano uno spettro tra gli attacchi Blackbox e Whitebox. L’avversario possiede una conoscenza parziale della struttura interna del modello target. Ad esempio, l’attaccante potrebbe conoscere i dati di training ma non avere informazioni sull’architettura o sui parametri del modello. In scenari pratici, la maggior parte degli attacchi rientra in questa zona grigia.\n\nIl panorama dei modelli di apprendimento automatico è complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilità all’interno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:\n\nI Generative Adversarial Network (GAN) sono modelli di deep learning costituiti da due reti in competizione tra loro: un generatore e un discriminatore (Goodfellow et al. 2020). Il generatore cerca di sintetizzare dati realistici mentre il discriminatore valuta se sono reali o falsi. Le GAN possono essere utilizzate per creare esempi avversari. La rete del generatore è addestrata per produrre input che il modello target classifica erroneamente. Queste immagini generate da GAN possono quindi attaccare un classificatore target o un modello di rilevamento. Il generatore e il modello target sono impegnati in un processo competitivo, con il generatore che migliora continuamente la sua capacità di creare esempi ingannevoli e il modello target che aumenta la sua resistenza a tali esempi. Le GAN forniscono un framework robusto per creare input avversari complessi e diversi, illustrando l’adattabilità dei modelli generativi nel panorama avversario.\nI Transfer Learning Adversarial Attacks [attacchi avversari di apprendimento di trasferimento] sfruttano la conoscenza trasferita da un modello pre-addestrato a un modello target, creando esempi avversari che possono ingannare entrambi i modelli. Questi attacchi rappresentano una preoccupazione crescente, in particolare quando gli avversari hanno conoscenza dell’estrattore di feature ma non hanno accesso alla testa di classificazione (la parte o il layer responsabile della creazione delle classificazioni finali). Denominate “attacchi headless”, queste strategie avversarie trasferibili sfruttano le capacità espressive degli estrattori di feature per creare perturbazioni, ignare dello spazio delle etichette o dei dati di training. L’esistenza di tali attacchi sottolinea l’importanza di sviluppare difese robuste per le applicazioni di apprendimento tramite trasferimento, soprattutto perché i modelli pre-addestrati sono comunemente utilizzati (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Commun. ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. «Headless Horseman: Adversarial Attacks on Transfer Learning Models». In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\nCaso di Studio\nNel 2017, i ricercatori hanno condotto esperimenti posizionando piccoli adesivi bianchi e neri sui segnali di stop (Eykholt et al. 2017). Quando visti da un occhio umano normale, gli adesivi non oscuravano il segnale né ne impedivano l’interpretazione. Tuttavia, quando le immagini degli adesivi dei segnali di stop venivano inserite nei modelli ML standard di classificazione dei segnali stradali, venivano classificati erroneamente come segnali di limite di velocità nell’85% dei casi.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. «Robust Physical-World Attacks on Deep Learning Models». ArXiv preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\nQuesta dimostrazione ha mostrato come semplici adesivi avversari potrebbero ingannare i sistemi ML facendogli interpretare male i segnali stradali critici. Se implementati in modo realistico, questi attacchi potrebbero mettere a repentaglio la sicurezza pubblica, inducendo i veicoli autonomi a interpretare male i segnali di stop come limiti di velocità. I ricercatori hanno avvertito che ciò potrebbe potenzialmente causare pericolosi semplici rallentamenti o accelerazioni negli incroci.\nQuesto caso di studio fornisce un’illustrazione concreta di come gli esempi avversari sfruttano i meccanismi di riconoscimento di pattern dei modelli ML. Alterando in modo sottile i dati di input, gli aggressori possono indurre previsioni errate e rappresentare rischi significativi per applicazioni critiche per la sicurezza come le auto a guida autonoma. La semplicità dell’attacco dimostra come anche cambiamenti minori e impercettibili possano sviare i modelli. Di conseguenza, gli sviluppatori devono implementare difese robuste contro tali minacce.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-lhardware-ml",
    "href": "contents/privacy_security/privacy_security.it.html#minacce-alla-sicurezza-per-lhardware-ml",
    "title": "14  Sicurezza e Privacy",
    "section": "14.5 Minacce alla Sicurezza Per l’Hardware ML",
    "text": "14.5 Minacce alla Sicurezza Per l’Hardware ML\nUn esame sistematico delle minacce alla sicurezza dell’hardware di apprendimento automatico embedded è essenziale per comprendere in modo completo le potenziali vulnerabilità nei sistemi ML. Inizialmente, verranno esplorate le vulnerabilità hardware derivanti da difetti di progettazione intrinseci che possono essere sfruttati. Questa conoscenza di base è fondamentale per riconoscere le origini delle debolezze hardware. Successivamente, verranno esaminati gli attacchi fisici, che rappresentano i metodi più diretti e palesi per compromettere l’integrità hardware. Sulla base di ciò, verranno analizzati gli attacchi di “injection” di guasti, dimostrando come le manipolazioni deliberate possano indurre guasti del sistema.\nPassando agli attacchi “side-channel”, verrà mostrata la crescente complessità, poiché questi si basano sullo sfruttamento di “leakage” [perdite] di informazioni indirette, che richiedono una comprensione sfumata delle operazioni hardware e delle interazioni ambientali. Le interfacce che “perdono” mostreranno come i canali di comunicazione esterni possono diventare vulnerabili, portando a esposizioni accidentali di dati. Le discussioni sull’hardware contraffatto traggono vantaggio dalle precedenti esplorazioni dell’integrità dell’hardware e dalle tecniche di sfruttamento, poiché spesso aggravano questi problemi con rischi aggiuntivi dovuti alla loro provenienza discutibile. Infine, i rischi della “supply chain” comprendono tutte le preoccupazioni di cui sopra e le inquadrano nel contesto del percorso dell’hardware dalla produzione alla distribuzione, evidenziando la natura multiforme della sicurezza dell’hardware e la necessità di vigilanza in ogni fase.\nTabella 14.1 riassume gli argomenti:\n\n\n\nTabella 14.1: Tipi di minaccia alla sicurezza hardware.\n\n\n\n\n\n\n\n\n\n\nTipo di minaccia\nDescrizione\nRilevanza per la sicurezza hardware ML\n\n\n\n\nBug Hardware\nDifetti intrinseci nelle progettazioni hardware che possono compromettere l’integrità del sistema.\nFondamento della vulnerabilità hardware.\n\n\nAttacchi Fisici\nSfruttamento diretto dell’hardware tramite accesso fisico o manipolazione.\nModello di minaccia basilare e palese.\n\n\nAttacchi di Injection di Guasti\nInduzione di guasti per causare errori nel funzionamento dell’hardware, portando a potenziali crash di sistema.\nManipolazione sistematica che porta al guasto.\n\n\nAttacchi a Canale Laterale\nSfruttamento di informazioni sul funzionamento dell’hardware per estrarre dati sensibili.\nAttacco indiretto tramite osservazione ambientale.\n\n\nInterfacce con Perdite\nVulnerabilità derivanti da interfacce che espongono i dati in modo involontario.\nEsposizione dei dati tramite canali di comunicazione.\n\n\nHardware Contraffatto\nUtilizzo di componenti hardware non autorizzati che potrebbero presentare falle di sicurezza.\nProblemi di vulnerabilità aggravati.\n\n\nRischi della Catena di Fornitura\nRischi introdotti durante il ciclo di vita dell’hardware, dalla produzione alla distribuzione.\nSfide di sicurezza cumulative e multiformi.\n\n\n\n\n\n\n\n14.5.1 Bug Hardware\nL’hardware non è immune al problema pervasivo di difetti di progettazione o bug. Gli aggressori possono sfruttare queste vulnerabilità per accedere, manipolare o estrarre dati sensibili, violando la riservatezza e l’integrità da cui dipendono utenti e servizi. Un esempio di tali vulnerabilità è venuto alla luce con la scoperta di Meltdown e Spectre, due vulnerabilità hardware che sfruttano vulnerabilità critiche nei processori moderni. Questi bug consentono agli aggressori di aggirare la barriera hardware che separa le applicazioni, consentendo a un programma dannoso di leggere la memoria di altri programmi e del sistema operativo.\nMeltdown (Kocher et al. 2019a) e Spectre (Kocher et al. 2019b) funzionano sfruttando le ottimizzazioni nelle CPU moderne che consentono loro di eseguire istruzioni speculative fuori ordine prima che i controlli di validità siano stati completati. Ciò rivela dati che dovrebbero essere inaccessibili, che l’attacco cattura tramite canali laterali come le cache. La complessità tecnica dimostra la difficoltà di eliminare le vulnerabilità anche con una validazione estesa.\n\n———, et al. 2019a. «Spectre Attacks: Exploiting Speculative Execution». In 2019 IEEE Symposium on Security and Privacy (SP). IEEE. https://doi.org/10.1109/sp.2019.00002.\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, et al. 2019b. «Spectre Attacks: Exploiting Speculative Execution». In 2019 IEEE Symposium on Security and Privacy (SP). IEEE. https://doi.org/10.1109/sp.2019.00002.\nSe un sistema ML elabora dati sensibili, come informazioni personali degli utenti o analisi aziendali proprietarie, Meltdown e Spectre rappresentano un pericolo reale e presente per la sicurezza dei dati. Si consideri il caso di una scheda acceleratrice ML progettata per accelerare i processi di apprendimento automatico, come quelli di cui abbiamo parlato nel capitolo I.A. Hardware. Questi acceleratori lavorano con la CPU per gestire calcoli complessi, spesso correlati all’analisi dei dati, al riconoscimento delle immagini e all’elaborazione del linguaggio naturale. Se una scheda acceleratrice di questo tipo presenta una vulnerabilità simile a Meltdown o Spectre, potrebbe far trapelare i dati che elabora. Un aggressore potrebbe sfruttare questa falla non solo per sottrarre dati, ma anche per ottenere informazioni sul funzionamento del modello ML, incluso potenzialmente il reverse engineering del modello stesso (tornando quindi al problema del furto di modelli.\nUno scenario reale in cui ciò potrebbe essere devastante sarebbe nel settore sanitario. I sistemi ML elaborano regolarmente dati altamente sensibili dei pazienti per aiutare a diagnosticare, pianificare il trattamento e prevedere i risultati. Un bug nell’hardware del sistema potrebbe portare alla divulgazione non autorizzata di informazioni sanitarie personali, violando la privacy del paziente e contravvenendo a rigidi standard normativi come l’Health Insurance Portability and Accountability Act (HIPAA)\nLe vulnerabilità Meltdown e Spectre sono un duro promemoria del fatto che la sicurezza hardware non riguarda solo la prevenzione dell’accesso fisico non autorizzato, ma anche la garanzia che l’architettura dell’hardware non diventi un canale per l’esposizione dei dati. Difetti di progettazione hardware simili emergono regolarmente in CPU, acceleratori, memoria, bus e altri componenti. Ciò richiede continue mitigazioni retroattive e compromessi sulle prestazioni nei sistemi distribuiti. Soluzioni proattive come le architetture di elaborazione confidenziale potrebbero mitigare intere classi di vulnerabilità attraverso una progettazione hardware fondamentalmente più sicura. Contrastare i bug hardware richiede rigore in ogni fase di progettazione, validazione e distribuzione.\n\n\n14.5.2 Attacchi Fisici\nLa manomissione fisica si riferisce alla manipolazione diretta e non autorizzata di risorse informatiche fisiche per minare l’integrità dei sistemi di apprendimento automatico. È un attacco particolarmente insidioso perché aggira le tradizionali misure di sicurezza informatica, che spesso si concentrano più sulle vulnerabilità del software che sulle minacce hardware.\nLa manomissione fisica può assumere molte forme, da quelle relativamente semplici, come l’inserimento di un dispositivo USB caricato con software dannoso in un server, a quelle altamente sofisticate, come l’inclusione di un Trojan hardware durante il processo di produzione di un microchip (discusso più avanti in dettaglio nella sezione Supply Chain). I sistemi ML sono suscettibili a questo attacco perché si basano sull’accuratezza e l’integrità del loro hardware per elaborare e analizzare correttamente grandi quantità di dati.\nSi consideri un drone alimentato da ML utilizzato per la mappatura geografica. Il funzionamento del drone si basa su una serie di sistemi di bordo, tra cui un modulo di navigazione che elabora gli input da vari sensori per determinare il suo percorso. Se un aggressore ottiene l’accesso fisico a questo drone, potrebbe sostituire il modulo di navigazione originale con uno compromesso che include una backdoor. Questo modulo manipolato potrebbe quindi alterare la traiettoria di volo del drone per condurre la sorveglianza su aree riservate o persino contrabbandare merci di contrabbando volando su rotte non rilevate.\nUn altro esempio è la manomissione fisica degli scanner biometrici utilizzati per il controllo degli accessi in strutture sicure. Introducendo un sensore modificato che trasmette dati biometrici a un ricevitore non autorizzato, un aggressore può accedere ai dati di identificazione personale per autenticare gli individui.\nEsistono diversi modi in cui la manomissione fisica può verificarsi nell’hardware ML:\n\nManipolazione dei sensori: Si consideri un veicolo autonomo dotato di telecamere e LiDAR per la percezione ambientale. Un malintenzionato potrebbe manipolare deliberatamente l’allineamento fisico di questi sensori per creare zone di occlusione o distorcere le misure della distanza. Ciò potrebbe compromettere le capacità di rilevamento degli oggetti e potenzialmente mettere in pericolo gli occupanti del veicolo.\nTrojan hardware: Le modifiche dannose ai circuiti possono introdurre trojan progettati per attivarsi in base a specifiche condizioni di input. Ad esempio, un chip acceleratore ML potrebbe funzionare come previsto fino a quando non incontra un trigger predeterminato, dopodiché si comporta in modo irregolare.\nManomissione della memoria: L’esposizione fisica e la manipolazione dei chip di memoria potrebbero consentire l’estrazione di parametri del modello ML crittografati. Le tecniche di iniezione di guasti possono anche corrompere i dati del modello per degradare l’accuratezza.\nIntroduzione di backdoor: Ottenendo l’accesso fisico ai server, un avversario potrebbe utilizzare keylogger hardware per catturare password e creare account backdoor per l’accesso persistente. Questi potrebbero poi essere utilizzati per esfiltrare dati di training ML nel tempo.\nAttacchi alla supply chain: Manipolare componenti hardware di terze parti o compromettere i canali di produzione e spedizione crea vulnerabilità sistemiche difficili da rilevare e correggere.\n\n\n\n14.5.3 Attacchi di Fault-injection\nIntroducendo intenzionalmente guasti nell’hardware ML, gli aggressori possono indurre errori nel processo di elaborazione, portando a output non corretti. Questa manipolazione compromette l’integrità delle operazioni ML e può fungere da vettore per ulteriori sfruttamenti, come il reverse engineering del sistema o il bypass del protocollo di sicurezza. L’iniezione di guasti comporta l’interruzione deliberata delle operazioni di elaborazione standard in un sistema tramite interferenze esterne (Joye e Tunstall 2012). Attivando con precisione gli errori di elaborazione, gli avversari possono alterare l’esecuzione del programma in modi che degradano l’affidabilità o trapelano informazioni sensibili.\n\nJoye, Marc, e Michael Tunstall. 2012. Fault Analysis in Cryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro Pellicioli, e Gerardo Pelosi. 2010. «Low voltage fault attacks to AES». In 2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST), 7–12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\nHutter, Michael, Jorn-Marc Schmidt, e Thomas Plos. 2009. «Contact-based fault injections and power analysis on RFID tags». In 2009 European Conference on Circuit Theory and Design, 409–12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\nAmiel, Frederic, Christophe Clavier, e Michael Tunstall. 2006. «Fault analysis of DPA-resistant algorithms». In International Workshop on Fault Diagnosis and Tolerance in Cryptography, 223–36. Springer.\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, e Berk Sunar. 2007. «Trojan Detection using IC Fingerprinting». In 2007 IEEE Symposium on Security and Privacy (SP ’07), 29–45. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\nSkorobogatov, Sergei. 2009. «Local heating attacks on Flash memory devices». In 2009 IEEE International Workshop on Hardware-Oriented Security and Trust, 1–6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\nSkorobogatov, Sergei P, e Ross J Anderson. 2003. «Optical fault induction attacks». In Cryptographic Hardware and Embedded Systems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA, August 1315, 2002 Revised Papers 4, 2–12. Springer.\nPer l’iniezione di guasti possono essere utilizzate varie tecniche di manomissione fisica. Bassa tensione (Barenghi et al. 2010), picchi di potenza (Hutter, Schmidt, e Plos 2009), anomalie di clock (Amiel, Clavier, e Tunstall 2006), impulsi elettromagnetici (Agrawal et al. 2007), aumento della temperatura (S. Skorobogatov 2009) e colpi laser (S. P. Skorobogatov e Anderson 2003) sono comuni vettori di attacco hardware. Sono programmati con precisione per indurre guasti come bit invertiti o istruzioni saltate durante operazioni critiche.\nPer i sistemi ML, le conseguenze includono una precisione del modello compromessa, negazione del servizio, estrazione di dati di training privati o parametri del modello e reverse engineering delle architetture del modello. Gli aggressori potrebbero utilizzare l’iniezione di guasti per forzare classificazioni errate, interrompere sistemi autonomi o rubare proprietà intellettuale.\nAd esempio, in (Breier et al. 2018), gli autori hanno iniettato con successo un attacco di errore in una rete neurale profonda distribuita su un microcontrollore. Hanno utilizzato un laser per riscaldare transistor specifici, costringendoli a cambiare stato. In un caso, hanno utilizzato questo metodo per attaccare una funzione di attivazione ReLU, con il risultato che la funzione emetteva sempre un valore di 0, indipendentemente dall’input. Nel codice assembly in Figura 14.2, l’attacco ha fatto sì che il programma in esecuzione saltasse sempre l’istruzione jmp end alla riga 6. Ciò significa che HiddenLayerOutput[i] è sempre impostato su 0, sovrascrivendo tutti i valori scritti su di esso nelle righe 4 e 5. Di conseguenza, i neuroni mirati vengono resi inattivi, con conseguenti classificazioni errate.\n\n\n\n\n\n\nFigura 14.2: Iniezione di errore dimostrata con codice assembly. Fonte: Breier et al. (2018).\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, e Yang Liu. 2018. «Deeplaser: Practical fault attack on deep neural networks». ArXiv preprint abs/1806.05859. https://arxiv.org/abs/1806.05859.\n\n\nLa strategia di un aggressore potrebbe essere quella di dedurre informazioni sulle funzioni di attivazione tramite attacchi side-channel (discussi in seguito). Quindi, l’aggressore potrebbe tentare di colpire più calcoli di funzioni di attivazione iniettando casualmente guasti nei livelli il più vicino possibile al livello di output, aumentando la probabilità e l’impatto dell’attacco.\nI dispositivi embedded sono particolarmente vulnerabili a causa di un limitato rafforzamento fisico e di vincoli di risorse che limitano le difese di runtime robuste. Senza un packaging antimanomissione, l’accesso dell’aggressore ai bus di sistema e alla memoria consente di infierire guasti precisi. Anche i modelli ML embedded leggeri mancano di ridondanza per bypassare gli errori.\nQuesti attacchi possono essere particolarmente insidiosi perché aggirano le tradizionali misure di sicurezza basate su software, spesso non tenendo conto delle interruzioni fisiche. Inoltre, poiché i sistemi ML si basano in larga misura sull’accuratezza e l’affidabilità del loro hardware per attività come il riconoscimento di pattern, il processo decisionale e le risposte automatiche, qualsiasi compromesso nel loro funzionamento dovuto all’iniezione di guasti può avere conseguenze gravi e di vasta portata.\nPer mitigare i rischi di iniezione di guasti è necessario un approccio multi-layer. Il rafforzamento fisico tramite custodie antimanomissione e offuscamento del design aiuta a ridurre l’accesso. Il rilevamento di leggere anomalie può identificare input di sensori insoliti o output di modelli errati (Hsiao et al. 2023). Le memorie con correzione degli errori riducono al minimo le interruzioni, mentre la crittografia dei dati salvaguarda le informazioni. Le tecniche emergenti di watermarking dei modelli tracciano i parametri rubati.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. «MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles». In 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nTuttavia, bilanciare protezioni robuste con i limiti di dimensioni e potenza ristretti dei sistemi embedded rimane una sfida. I limiti della crittografia e la mancanza di coprocessori sicuri su hardware embedded sensibile ai costi limitano le opzioni. In definitiva, la resilienza all’iniezione di guasti richiede una prospettiva multi-layer che abbraccia i layer di progettazione elettrica, firmware, software e fisica.\n\n\n14.5.4 Attacchi a canale laterale\nGli attacchi side-channel costituiscono una classe di violazioni della sicurezza che sfruttano informazioni rivelate inavvertitamente tramite l’implementazione fisica dei sistemi informatici. Contrariamente agli attacchi diretti che prendono di mira vulnerabilità software o di rete, questi attacchi sfruttano le caratteristiche hardware intrinseche del sistema per estrarre informazioni sensibili.\nLa premessa fondamentale di un attacco side-channel è che il funzionamento di un dispositivo può rivelare inavvertitamente informazioni. Tali fughe possono provenire da varie fonti, tra cui l’energia elettrica consumata da un dispositivo (Kocher, Jaffe, e Jun 1999), i campi elettromagnetici che emette (Gandolfi, Mourtel, e Olivier 2001), il tempo necessario per elaborare determinate operazioni o persino i suoni che produce. Ogni canale può intravedere indirettamente i processi interni del sistema, rivelando informazioni che possono compromettere la sicurezza.\n\nKocher, Paul, Joshua Jaffe, e Benjamin Jun. 1999. «Differential power analysis». In Advances in CryptologyCRYPTO’99: 19th Annual International Cryptology Conference Santa Barbara, California, USA, August 1519, 1999 Proceedings 19, 388–97. Springer.\n\nGandolfi, Karine, Christophe Mourtel, e Francis Olivier. 2001. «Electromagnetic analysis: Concrete results». In Cryptographic Hardware and Embedded SystemsCHES 2001: Third International Workshop Paris, France, May 1416, 2001 Proceedings 3, 251–61. Springer.\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, e Pankaj Rohatgi. 2011. «Introduction to differential power analysis». Journal of Cryptographic Engineering 1 (1): 5–27. https://doi.org/10.1007/s13389-011-0006-y.\nAd esempio, si consideri un sistema di apprendimento automatico che esegue transazioni crittografate. Gli algoritmi di crittografia dovrebbero proteggere i dati, ma richiedono un lavoro computazionale per crittografare e decrittografare le informazioni. Un aggressore può analizzare i modelli di consumo energetico del dispositivo che esegue la crittografia per scoprire la chiave crittografica. Con metodi statistici sofisticati, piccole variazioni nel consumo energetico durante il processo di crittografia possono essere correlate ai dati in fase di elaborazione, rivelando infine la chiave. Alcune tecniche di attacco di analisi differenziale sono “Differential Power Analysis (DPA)” (Kocher et al. 2011), “Differential Electromagnetic Analysis (DEMA)” e “Correlation Power Analysis (CPA)”.\nAd esempio, si consideri un aggressore che cerca di violare l’algoritmo di crittografia AES utilizzando un attacco di analisi differenziale. L’aggressore dovrebbe prima raccogliere molte tracce di potenza o elettromagnetiche (una traccia è una registrazione di consumi o emissioni) del dispositivo durante l’esecuzione della crittografia AES.\nUna volta che l’aggressore ha raccolto tracce sufficienti, utilizzerebbe una tecnica statistica per identificare le correlazioni tra le tracce e i diversi valori del testo in chiaro (testo originale non crittografato) e del testo cifrato (testo crittografato). Queste correlazioni verrebbero poi utilizzate per dedurre il valore di un bit nella chiave AES e, infine, l’intera chiave. Gli attacchi di analisi differenziale sono pericolosi perché sono economici, efficaci e non intrusivi, consentendo agli aggressori di aggirare le misure di sicurezza algoritmiche e a livello hardware. I sistemi compromessi da questi attacchi sono anche difficili da rilevare perché non modificano fisicamente il dispositivo o interrompono l’algoritmo di crittografia.\nDi seguito, una visualizzazione semplificata illustra come l’analisi dei modelli di consumo energetico del dispositivo di crittografia può aiutare a estrarre informazioni sulle operazioni dell’algoritmo e, a sua volta, sui dati segreti. Consideriamo un dispositivo che accetta una password di 5 byte come input. I diversi modelli di tensione misurati mentre il dispositivo di crittografia esegue operazioni sull’input per autenticare la password verranno analizzati e confrontati.\nInnanzitutto, l’analisi della potenza elettrica delle operazioni del dispositivo dopo aver inserito una password corretta è mostrata nella prima immagine in Figura 14.3. Il grafico blu denso restituisce la misura della tensione del dispositivo di crittografia. Ciò che è significativo qui è il confronto tra i diversi grafici di analisi piuttosto che i dettagli specifici di ciò che sta accadendo in ogni scenario.\n\n\n\n\n\n\nFigura 14.3: Analisi della potenza di un dispositivo di crittografia con una password corretta. Fonte: Colin O’Flynn.\n\n\n\nQuando viene inserita una password errata, il grafico dell’analisi della potenza è mostrato in Figura 14.4. I primi tre byte della password sono corretti. Di conseguenza, i modelli di tensione sono molto simili o identici tra i due grafici, fino al quarto byte incluso. Dopo che il dispositivo elabora il quarto byte, viene determinata una mancata corrispondenza tra la chiave segreta e l’input tentato. Viene notato un cambiamento nel modello nel punto di transizione tra il quarto e il quinto byte: la tensione aumenta (la corrente diminuisce) perché il dispositivo ha smesso di elaborare il resto dell’input.\n\n\n\n\n\n\nFigura 14.4: Analisi della potenza di un dispositivo di crittografia con una password (parzialmente) errata. Fonte: Colin O’Flynn.\n\n\n\nFigura 14.5 descrive un altro grafico di una password completamente errata. Dopo che il dispositivo ha terminato l’elaborazione del primo byte, determina che è errato e interrompe l’ulteriore elaborazione: la tensione aumenta e la corrente diminuisce.\n\n\n\n\n\n\nFigura 14.5: Analisi della potenza di un dispositivo di crittografia con una password errata. Fonte: Colin O’Flynn.\n\n\n\nL’esempio sopra dimostra come le informazioni sul processo di crittografia e sulla chiave segreta possono essere dedotte analizzando diversi input e tentando di “intercettare” le operazioni del dispositivo su ogni byte di input. Per una spiegazione più dettagliata, guardare Video 14.3 di seguito.\n\n\n\n\n\n\nVideo 14.3: Power Attack\n\n\n\n\n\n\nUn altro esempio è un sistema ML per il riconoscimento vocale, che elabora i comandi vocali per eseguire azioni. Misurando la latenza del sistema per rispondere ai comandi o la potenza utilizzata durante l’elaborazione, un aggressore potrebbe dedurre quali comandi vengono elaborati e quindi apprendere i modelli operativi del sistema. Ancora più sottilmente, il suono emesso dalla ventola o dal disco rigido di un computer potrebbe cambiare in risposta al carico di lavoro, che un microfono sensibile potrebbe captare e analizzare per determinare che tipo di operazioni vengono eseguite.\nIn scenari reali, gli attacchi side-channel hanno effettivamente estratto chiavi di crittografia e compromesso comunicazioni sicure. Uno dei primi casi registrati di un simile attacco si è verificato negli anni ’60, quando l’agenzia di intelligence britannica MI5 ha affrontato la sfida di decifrare comunicazioni crittografate dall’ambasciata egiziana a Londra. I loro sforzi di decifrazione dei codici sono stati inizialmente ostacolati dalle limitazioni computazionali dell’epoca, fino a quando un’ingegnosa osservazione dell’agente MI5 Peter Wright ha alterato il corso dell’operazione.\nL’agente dell’MI5 Peter Wright propose di usare un microfono per catturare le sottili firme acustiche emesse dalla macchina di cifratura del rotore dell’ambasciata durante la crittografia (Burnet e Thomas 1989). I distinti clic meccanici dei rotori mentre gli operatori li configuravano quotidianamente facevano trapelare informazioni critiche sulle impostazioni iniziali. Questo semplice “canale laterale” del suono ha permesso all’MI5 di ridurre drasticamente la complessità della decifrazione dei messaggi. Questo primo attacco di “perdita” acustica evidenzia che gli attacchi a canale laterale non sono semplicemente una novità dell’era digitale, ma una continuazione di antichi principi di crittoanalisi. L’idea che dove c’è un segnale, c’è un’opportunità di intercettazione rimane fondamentale. Dai clic meccanici alle fluttuazioni elettriche e oltre, i canali laterali consentono agli avversari di estrarre segreti indirettamente attraverso un’attenta analisi del segnale.\n\nBurnet, David, e Richard Thomas. 1989. «Spycatcher: The Commodification of Truth». J. Law Soc. 16 (2): 210. https://doi.org/10.2307/1410360.\n\nAsonov, D., e R. Agrawal. 2004. «Keyboard acoustic emanations». In IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004, 3–11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\nGnad, Dennis R. E., Fabian Oboril, e Mehdi B. Tahoori. 2017. «Voltage drop-based fault attacks on FPGAs using valid bitstreams». In 2017 27th International Conference on Field Programmable Logic and Applications (FPL), 1–7. IEEE; IEEE. https://doi.org/10.23919/fpl.2017.8056840.\n\nZhao, Mark, e G. Edward Suh. 2018. «FPGA-Based Remote Power Side-Channel Attacks». In 2018 IEEE Symposium on Security and Privacy (SP), 229–44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\nOggi, la crittoanalisi acustica si è evoluta in attacchi come l’intercettazione della tastiera (Asonov e Agrawal 2004). I canali laterali elettrici spaziano dall’analisi della potenza su hardware crittografico (Gnad, Oboril, e Tahoori 2017) alle fluttuazioni di tensione (Zhao e Suh 2018) su acceleratori di machine learning. Anche tempistiche, emissioni elettromagnetiche e persino impronte di calore possono essere sfruttate. Nuovi e inaspettati canali laterali emergono spesso man mano che l’informatica diventa più interconnessa e miniaturizzata.\nProprio come la “perdita” acustica analogica dell’MI5 ha trasformato la loro decifrazione dei codici, i moderni attacchi ai canali laterali aggirano i confini tradizionali della difesa informatica. Comprendere lo spirito creativo e la persistenza storica degli exploit dei canali laterali è una conoscenza fondamentale per sviluppatori e difensori che cercano di proteggere in modo completo i moderni sistemi di apprendimento automatico dalle minacce digitali e fisiche.\n\n\n14.5.5 Interfacce con Perdite\nLe interfacce “leaky” nei sistemi embedded sono spesso backdoor trascurate che possono trasformarsi in significative vulnerabilità di sicurezza. Sebbene progettate per scopi legittimi come comunicazione, manutenzione o debug, queste interfacce possono inavvertitamente fornire agli aggressori una finestra attraverso la quale estrarre informazioni sensibili o iniettare dati dannosi.\nUn’interfaccia diventa “leaky” quando espone più informazioni del dovuto, spesso a causa della mancanza di rigorosi controlli di accesso o di una schermatura inadeguata dei dati trasmessi. Ecco alcuni esempi concreti di problemi di interfaccia leaky che causano problemi di sicurezza in dispositivi IoT ed embedded:\n\nBaby Monitor: Molti baby monitor abilitati al WiFi hanno interfacce non protette per l’accesso remoto. Ciò ha consentito agli aggressori di ottenere feed audio e video in tempo reale dalle case delle persone, rappresentando una grave violazione della privacy.\nPacemaker: Sono state scoperte vulnerabilità dell’interfaccia in alcuni pacemaker che potrebbero consentire agli aggressori di manipolare le funzioni cardiache se sfruttate. Ciò presenta uno scenario potenzialmente letale.\nLampadine Smart: Un ricercatore ha scoperto di poter accedere a dati non crittografati da lampadine intelligenti tramite un’interfaccia di debug, comprese le credenziali WiFi, consentendogli di accedere alla rete connessa (Greengard 2015).\nAuto Smart: Se non protetta, la porta di diagnostica OBD-II ha dimostrato di fornire un vettore di attacco ai sistemi automobilistici. Gli aggressori potrebbero usarlo per controllare i freni e altri componenti (Miller e Valasek 2015).\n\n\nGreengard, Samuel. 2015. The Internet of Things. The MIT Press. https://doi.org/10.7551/mitpress/10277.001.0001.\n\nMiller, Charlie, e Chris Valasek. 2015. «Remote exploitation of an unaltered passenger vehicle». Black Hat USA 2015 (S 91): 1–91.\nSebbene quanto sopra non sia direttamente collegato al ML, si consideri l’esempio di un sistema di casa intelligente con un componente ML embedded che controlla la sicurezza domestica in base a modelli di comportamento che apprende nel tempo. Il sistema include un’interfaccia di manutenzione accessibile tramite la rete locale per aggiornamenti software e controlli di sistema. Se questa interfaccia non richiede un’autenticazione forte o i dati trasmessi tramite essa non sono crittografati, un aggressore sulla stessa rete potrebbe ottenere l’accesso. Potrebbero quindi intercettare le routine quotidiane del proprietario di casa o riprogrammare le impostazioni di sicurezza manipolando il firmware.\nTali “fughe” rappresentano un problema di privacy e un potenziale punto di ingresso per exploit più dannosi. L’esposizione di dati di training, parametri del modello o output ML da una “fuga” potrebbe aiutare gli avversari a costruire esempi avversari o a sottoporre a reverse engineering i modelli. L’accesso tramite un’interfaccia con “perdite” potrebbe anche essere utilizzato per modificare il firmware di un dispositivo embedded, caricandolo con codice dannoso che potrebbe spegnerlo, intercettare dati o utilizzarlo in attacchi botnet.\nPer mitigare questi rischi, è necessario un approccio multi-strato, che comprenda controlli tecnici come autenticazione, crittografia, rilevamento delle anomalie, policy e processi come inventari di interfaccia, controlli di accesso, auditing e pratiche di sviluppo sicure. Disattivare le interfacce non necessarie e compartimentare i rischi tramite un modello zero-trust fornisce una protezione aggiuntiva.\nCome progettisti di sistemi ML embedded, dovremmo valutare le interfacce nelle prime fasi dello sviluppo e monitorarle continuamente dopo l’implementazione come parte di un ciclo di vita della sicurezza end-to-end. Comprendere e proteggere le interfacce è fondamentale per garantire la sicurezza complessiva del ML embedded.\n\n\n14.5.6 Hardware Contraffatto\nI sistemi ML sono affidabili solo quanto l’hardware sottostante. In un’epoca in cui i componenti hardware sono beni di consumo globali, l’aumento di hardware contraffatti o clonati rappresenta una sfida significativa. L’hardware contraffatto comprende tutti i componenti che sono riproduzioni non autorizzate di parti originali. I componenti contraffatti si infiltrano nei sistemi ML attraverso complesse catene di fornitura che si estendono oltre i confini e coinvolgono numerose fasi dalla produzione alla consegna.\nUna singola lacuna nell’integrità della catena di fornitura può comportare l’inserimento di parti contraffatte progettate per imitare da vicino le funzioni e l’aspetto dell’hardware originale. Ad esempio, un sistema di riconoscimento facciale per il controllo degli accessi ad alta sicurezza potrebbe essere compromesso se dotato di processori contraffatti. Questi processori potrebbero non riuscire a elaborare e verificare accuratamente i dati biometrici, consentendo potenzialmente a persone non autorizzate di accedere ad aree riservate.\nLa sfida con l’hardware contraffatto è multiforme. Compromette la qualità e l’affidabilità dei sistemi ML, poiché questi componenti potrebbero degradarsi più rapidamente o funzionare in modo imprevedibile a causa di una produzione scadente. Anche i rischi per la sicurezza sono profondi; l’hardware contraffatto può contenere vulnerabilità pronte per essere sfruttate da malintenzionati. Ad esempio, un router di rete clonato in un data center ML potrebbe includere una backdoor nascosta, consentendo l’intercettazione dei dati o l’intrusione nella rete senza essere rilevati.\nInoltre, l’hardware contraffatto comporta rischi legali e di conformità. Le aziende che utilizzano inavvertitamente parti contraffatte nei loro sistemi ML possono affrontare gravi ripercussioni legali, tra cui multe e sanzioni per il mancato rispetto delle normative e degli standard del settore. Ciò è particolarmente vero per i settori in cui è obbligatoria la conformità a specifiche normative sulla sicurezza e sulla privacy, come l’assistenza sanitaria e la finanza.\nIl problema dell’hardware contraffatto è esacerbato dalle pressioni economiche per ridurre i costi, che possono costringere le aziende ad approvvigionarsi da fornitori a basso costo senza rigorosi processi di verifica. Questa economia può introdurre inavvertitamente parti contraffatte in sistemi altrimenti sicuri. Inoltre, rilevare queste contraffazioni è intrinsecamente difficile poiché vengono create per passare per componenti originali, il che spesso richiede attrezzature e competenze sofisticate per essere identificate.\nNel ML, dove le decisioni vengono prese in tempo reale e basate su calcoli complessi, le conseguenze di un guasto hardware sono scomode e potenzialmente pericolose. Le parti interessate nel campo del ML devono comprendere a fondo questi rischi. I problemi presentati dall’hardware contraffatto richiedono un’analisi approfondita delle attuali sfide che l’integrità del sistema ML deve affrontare e sottolineano l’importanza di una gestione attenta e informata del ciclo di vita dell’hardware all’interno di questi sistemi avanzati.\n\n\n14.5.7 Rischi della Catena di Fornitura\nLa minaccia dell’hardware contraffatto è strettamente legata alle vulnerabilità più ampie della supply chain [catena di fornitura]. Le supply chain globalizzate e interconnesse creano molteplici opportunità per componenti compromessi di infiltrarsi nel ciclo di vita di un prodotto. Le supply chain coinvolgono numerose entità, dalla progettazione alla produzione, all’assemblaggio, alla distribuzione e all’integrazione. Una mancanza di trasparenza e supervisione di ogni partner rende difficile la verifica dell’integrità a ogni passaggio. Le lacune in qualsiasi punto della catena possono consentire l’inserimento di parti contraffatte.\nAd esempio, un produttore su contratto potrebbe ricevere e includere inconsapevolmente rifiuti elettronici riciclati contenenti contraffazioni pericolose. Un distributore inaffidabile potrebbe introdurre di nascosto componenti clonati. Le minacce interne a qualsiasi fornitore potrebbero deliberatamente mescolare contraffazioni in spedizioni legittime.\nUna volta che le contraffazioni entrano nel flusso di fornitura, passano rapidamente attraverso più mani prima di finire nei sistemi ML in cui il rilevamento è difficile. Le contraffazioni avanzate come parti ricondizionate o cloni con esterni riconfezionati possono mascherarsi da componenti autentici, superando l’ispezione visiva.\nPer identificare i falsi, spesso è richiesta una profilazione tecnica completa tramite micrografia, screening a raggi X, analisi forense dei componenti e test funzionali. Tuttavia, un’analisi così costosa non è pratica per gli acquisti su larga scala.\nStrategie come audit della supply chain, screening dei fornitori, convalida della provenienza dei componenti e aggiunta di protezioni antimanomissione possono aiutare a mitigare i rischi. Tuttavia, date le sfide globali alla sicurezza della supply chain, un approccio zero-trust è prudente. Progettare sistemi ML per utilizzare controlli ridondanti, fail-safe e monitoraggio continuo del runtime fornisce resilienza contro i compromessi dei componenti.\nUna rigorosa convalida delle sorgenti hardware abbinata ad architetture di sistema fault-tolerant offre la difesa più solida contro i rischi pervasivi di supply chain globali contorte e opache.\n\n\n14.5.8 Caso di Studio\nNel 2018, Bloomberg Businessweek ha pubblicato una storia allarmante che ha attirato molta attenzione nel mondo della tecnologia. L’articolo sosteneva che Supermicro aveva segretamente impiantato minuscoli chip spia nell’hardware del server. I giornalisti hanno affermato che gli hacker statali cinesi che lavoravano con Supermicro potevano infilare questi minuscoli chip nelle schede madri durante la produzione. I minuscoli chip avrebbero presumibilmente dato agli hacker un accesso backdoor ai server utilizzati da oltre 30 grandi aziende, tra cui Apple e Amazon.\nSe fosse vero, ciò consentirebbe agli hacker di spiare dati privati o persino manomettere i sistemi. Tuttavia, dopo aver indagato, Apple e Amazon non hanno trovato prove dell’esistenza di tale hardware Supermicro hackerato. Altri esperti hanno messo in dubbio l’accuratezza dell’articolo di Bloomberg.\nSe la storia sia completamente vera o meno non è una nostra preoccupazione da un punto di vista pedagogico. Tuttavia, questo incidente ha attirato l’attenzione sui rischi delle catene di fornitura globali per l’hardware, in particolare quello prodotto in Cina. Quando le aziende esternalizzano e acquistano componenti hardware da fornitori in tutto il mondo, è necessario che vi sia maggiore visibilità nel processo. In questa complessa pipeline globale, si teme che hardware contraffatti o manomessi possano essere introdotti da qualche parte lungo il percorso senza che le aziende tecnologiche se ne accorgano. Le aziende che si affidano troppo a singoli produttori o distributori creano rischi. Ad esempio, a causa dell’eccessiva dipendenza da TSMC per la produzione di semiconduttori, gli Stati Uniti hanno investito 50 miliardi di dollari nel CHIPS Act.\nMan mano che l’apprendimento automatico si sposta in sistemi più critici, è fondamentale verificare l’integrità dell’hardware dalla progettazione alla produzione e alla consegna. La backdoor Supermicro segnalata ha dimostrato che per la sicurezza dell’apprendimento automatico non possiamo dare per scontate le catene di fornitura e la produzione globali. Dobbiamo ispezionare e convalidare l’hardware a ogni collegamento della catena.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#sicurezza-hardware-del-ml-embedded",
    "href": "contents/privacy_security/privacy_security.it.html#sicurezza-hardware-del-ml-embedded",
    "title": "14  Sicurezza e Privacy",
    "section": "14.6 Sicurezza Hardware del ML Embedded",
    "text": "14.6 Sicurezza Hardware del ML Embedded\n\n14.6.1 Trusted Execution Environments\n\nInformazioni su TEE\nUn Trusted Execution Environment (TEE) è un’area protetta all’interno di un processore principale che fornisce un elevato livello di sicurezza per l’esecuzione del codice e la protezione dei dati. I TEE operano isolando l’esecuzione di attività sensibili dal resto delle operazioni del dispositivo, creando così un ambiente resistente agli attacchi da vettori software e hardware.\n\n\nVantaggi\nI TEE sono particolarmente preziosi in scenari in cui devono essere elaborati dati sensibili o in cui l’integrità delle operazioni di un sistema è critica. Nel contesto dell’hardware ML, i TEE assicurano che gli algoritmi e i dati ML siano protetti da manomissioni e “perdite”. Ciò è essenziale perché i modelli ML elaborano spesso informazioni private, segreti commerciali o dati che potrebbero essere sfruttati se esposti.\nAd esempio, un TEE può proteggere i parametri del modello ML dall’estrazione da parte di software dannosi sullo stesso dispositivo. Questa protezione è fondamentale per la privacy e il mantenimento dell’integrità del sistema ML, assicurando che i modelli funzionino come previsto e non forniscano output distorti a causa di parametri manipolati. Secure Enclave di Apple, presente in iPhone e iPad, è una forma di TEE che fornisce un ambiente isolato per proteggere i dati sensibili degli utenti e le operazioni crittografiche.\nNei sistemi ML, i TEE possono:\n\nEseguire in modo sicuro l’addestramento e l’inferenza del modello, assicurando che i risultati del calcolo rimangano riservati.\nProteggere la riservatezza dei dati di input, come le informazioni biometriche, utilizzati per l’identificazione personale o per attività di classificazione sensibili.\nProteggere i modelli ML impedendo il reverse engineering, che può proteggere le informazioni proprietarie e mantenere un vantaggio competitivo.\nAbilitare aggiornamenti sicuri ai modelli ML, assicurando che gli aggiornamenti provengano da una fonte attendibile e non siano stati manomessi durante il transito.\n\nL’importanza dei TEE nella sicurezza hardware ML deriva dalla loro capacità di proteggere da minacce esterne e interne, tra cui le seguenti:\n\nSoftware Dannoso: I TEE possono impedire al malware ad alto privilegio di accedere alle aree sensibili del sistema ML.\nManomissione Fisica: Integrandosi con le misure di sicurezza hardware, i TEE possono proteggere dalla manomissione fisica che tenta di aggirare la sicurezza del software.\nAttacchi Side-channel: Sebbene non impenetrabili, i TEE possono mitigare determinati attacchi side-channel controllando l’accesso a operazioni e modelli di dati sensibili.\n\n\n\nMeccanica\nI fondamenti dei TEE contengono quattro parti principali:\n\nEsecuzione Isolata: Il codice all’interno di un TEE viene eseguito in un ambiente separato dal sistema operativo principale del dispositivo. Questo isolamento protegge il codice dall’accesso non autorizzato da parte di altre applicazioni.\nArchiviazione sicura: I TEE possono archiviare in modo sicuro chiavi crittografiche, token di autenticazione e dati sensibili, impedendo l’accesso da parte di applicazioni regolari in esecuzione all’esterno del TEE.\nProtezione dell’Integrità: I TEE possono verificare l’integrità del codice e dei dati, assicurando che non siano stati alterati prima dell’esecuzione o durante l’archiviazione.\nCrittografia dei Dati: I dati gestiti all’interno di un TEE possono essere crittografati, rendendoli illeggibili per entità senza le chiavi appropriate, che sono anch’esse gestite all’interno del TEE.\n\nEcco alcuni esempi di TEE che forniscono sicurezza basata su hardware per applicazioni sensibili:\n\nARMTrustZone:Questa tecnologia crea ambienti di esecuzione sicuri e normali isolati tramite controlli hardware e implementati in molti chipset mobili.\nIntelSGX:Le estensioni Software Guard di Intel forniscono un’enclave per l’esecuzione del codice che protegge da determinati attacchi software, in particolare attacchi al livello del sistema operativo. Vengono utilizzate per salvaguardare i carichi di lavoro nel cloud.\nQualcomm Secure Execution Environment:Un sandbox hardware su chipset Qualcomm per app di pagamento e autenticazione mobili.\nApple SecureEnclave:TEE per dati biometrici e gestione delle chiavi su iPhone e iPad. Facilita i pagamenti mobili.\n\nFigura 14.6 è un diagramma che mostra un’enclave sicura isolata dal processore principale per fornire un ulteriore livello di sicurezza. L’enclave sicura ha una ROM di avvio per stabilire una “root” hardware di attendibilità, un motore AES per operazioni crittografiche efficienti e sicure e memoria protetta. Ha anche un meccanismo per memorizzare le informazioni in modo sicuro su un archivio collegato separato da quello flash NAND utilizzato dal processore applicativo e dal sistema operativo. Questo design mantiene al sicuro i dati sensibili degli utenti anche quando il kernel dell’Application Processor viene compromesso.\n\n\n\n\n\n\nFigura 14.6: Enclave sicura System-on-chip. Fonte: Apple.\n\n\n\n\n\nCompromessi\nSe i TEE sono così buoni, perché non tutti i sistemi hanno TEE abilitato di default? La decisione di implementare un TEE non è presa alla leggera. Ci sono diversi motivi per cui un TEE potrebbe essere presente solo in alcuni sistemi di default. Ecco alcuni compromessi e sfide associati ai TEE:\nCosto: L’implementazione dei TEE comporta costi aggiuntivi. Ci sono costi diretti per l’hardware e costi indiretti associati allo sviluppo e alla manutenzione di software sicuro per i TEE. Questi costi potrebbero essere giustificabili solo per alcuni dispositivi, in particolare prodotti a basso margine.\nComplessità: I TEE aggiungono complessità alla progettazione e allo sviluppo del sistema. L’integrazione di un TEE con sistemi esistenti richiede una sostanziale ri-progettazione dello stack hardware e software, che può rappresentare un ostacolo, in particolare per i sistemi legacy.\nSovraccarico di Prestazioni: Mentre i TEE offrono una maggiore sicurezza, possono introdurre un sovraccarico di prestazioni. Ad esempio, i passaggi aggiuntivi nella verifica e nella crittografia dei dati possono rallentare le prestazioni del sistema, il che può essere critico nelle applicazioni sensibili al fattore tempo.\nSfide di Sviluppo: Lo sviluppo per i TEE richiede conoscenze specialistiche e spesso deve rispettare rigidi protocolli di sviluppo. Ciò può estendere i tempi di sviluppo e complicare i processi di debug e test.\nScalabilità e Flessibilità: I TEE, a causa della loro natura sicura, possono imporre limitazioni alla scalabilità e alla flessibilità. L’aggiornamento dei componenti sicuri o la scalabilità del sistema per più utenti o dati può essere più impegnativo quando tutto deve passare attraverso un ambiente protetto e chiuso.\nConsumo Energetico: L’elaborazione aumentata richiesta per la crittografia, la decrittografia e i controlli di integrità possono portare a un maggiore consumo energetico, una preoccupazione significativa per i dispositivi alimentati a batteria.\nDomanda del Mercato: Non tutti i mercati o le applicazioni richiedono il livello di sicurezza fornito dai TEE. Per molte applicazioni consumer, il rischio percepito potrebbe essere abbastanza basso da indurre i produttori a scegliere di non includere TEE nei loro progetti.\nCertificazione e Garanzia di Sicurezza: I sistemi con TEE potrebbero aver bisogno di rigorose certificazioni di sicurezza con enti come Common Criteria (CC) o European Union Agency for Cybersecurity (ENISA), che possono essere lunghe e costose. Alcune organizzazioni potrebbero scegliere di astenersi dall’implementare TEE per evitare questi ostacoli.\nDispositivi con risorse limitate: I dispositivi con potenza di elaborazione, memoria o archiviazione limitate potrebbero supportare solo TEE senza compromettere la loro funzionalità primaria.\n\n\n\n14.6.2 Avvio Sicuro\n\nInformazioni\nUn “secure boot” avvio sicuro è uno standard di sicurezza che garantisce che un dispositivo si avvii utilizzando solo software attendibile dall’Original Equipment Manufacturer (OEM) [produttore dell’apparecchiatura originale]. Quando il dispositivo si avvia, il firmware controlla la firma di ogni pezzo di software di avvio, inclusi il bootloader, il kernel e il sistema operativo di base, per assicurarsi che non sia manomesso. Se le firme sono valide, il dispositivo continua ad avviarsi. In caso contrario, il processo di avvio si interrompe per impedire l’esecuzione di potenziali minacce alla sicurezza.\n\n\nVantaggi\nL’integrità di un sistema ML è fondamentale dal momento in cui viene acceso. Un processo di avvio compromesso potrebbe a sua volta compromettere il sistema consentendo il caricamento di software dannoso prima dell’avvio del sistema operativo e delle applicazioni ML. Ciò potrebbe portare a operazioni ML manipolate, dati rubati o al riutilizzo del dispositivo per attività dannose come botnet o crypto-mining.\nIl “Secure Boot” aiuta a proteggere l’hardware ML embedded in diversi modi:\n\nProtezione dei Dati ML: Garantire che i dati utilizzati dai modelli ML, che possono includere informazioni private o sensibili, non siano esposti a manomissioni o furti durante il processo di boot [avvio].\nProtezione dell’Integrità del Modello: Mantenere l’integrità dei modelli ML è importante, poiché la loro manomissione potrebbe portare a risultati errati o dannosi.\nAggiornamenti Sicuri del Modello: Abilitare aggiornamenti sicuri per modelli e algoritmi ML, assicurando che gli aggiornamenti siano autenticati e non siano stati alterati.\n\n\n\nMeccanica\nI TEE traggono vantaggio da Secure Boot in vari modi. Figura 14.7 illustra un diagramma di flusso di un sistema embedded affidabile. Ad esempio, durante la validazione iniziale, Secure Boot assicura che il codice in esecuzione all’interno del TEE sia la versione corretta e non manomessa approvata dal produttore del dispositivo. Può garantire la resilienza contro le manomissioni verificando le firme digitali del firmware e di altri componenti critici; Secure Boot impedisce modifiche non autorizzate che potrebbero compromettere le proprietà di sicurezza del TEE. Secure Boot stabilisce una base di fiducia su cui il TEE può operare in modo sicuro, abilitando operazioni sicure come la gestione delle chiavi crittografiche, l’elaborazione sicura e la gestione dei dati sensibili.\n\n\n\n\n\n\nFigura 14.7: Flusso di Secure Boot. Fonte: R. V. e A. (2018).\n\n\nR. V., Rashmi, e Karthikeyan A. 2018. «Secure boot of Embedded Applications - A Review». In 2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA), 291–98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\n\n\nCaso di Studio: Face ID di Apple\nPrendiamo un esempio reale. La tecnologia Face ID di Apple utilizza algoritmi di apprendimento automatico avanzati per abilitare il riconoscimento facciale su iPhone e iPad. Si basa su un sofisticato framework di sensori e software per mappare accuratamente la geometria del volto di un utente. Affinché Face ID funzioni in modo sicuro e protegga i dati biometrici dell’utente, le operazioni del dispositivo devono essere affidabili dal momento in cui viene acceso, ed è qui che Secure Boot svolge un ruolo cruciale. Ecco come Secure Boot funziona insieme a Face ID:\nVerifica iniziale: Quando un iPhone viene acceso, il processo Secure Boot inizia in Secure Enclave, un coprocessore che fornisce un ulteriore livello di sicurezza. Secure Enclave è responsabile dell’elaborazione dei dati delle impronte digitali per Touch ID e dei dati di riconoscimento facciale per Face ID. Il processo di avvio verifica che Apple abbia firmato il firmware di Secure Enclave e che non sia stato manomesso. Questo passaggio garantisce che il firmware utilizzato per elaborare i dati biometrici sia autentico e sicuro.\nControlli di Sicurezza Continui: Dopo il test di accensione iniziale e la verifica tramite Secure Boot, Secure Enclave comunica con il processore principale del dispositivo per continuare la catena di avvio sicuro. Verifica le firme digitali del kernel iOS e di altri componenti di avvio critici prima di consentire il proseguimento del processo di avvio. Questo modello di “trust” [fiducia] concatenato impedisce modifiche non autorizzate al bootloader e al sistema operativo, che potrebbero compromettere la sicurezza del dispositivo.\nElaborazione dei Dati del Viso: Una volta che il dispositivo ha completato la sequenza di avvio sicuro, Secure Enclave può interagire in modo sicuro con gli algoritmi ML che alimentano Face ID. Il riconoscimento facciale comporta la proiezione e l’analisi di oltre 30.000 punti invisibili per creare una mappa di profondità del viso dell’utente e un’immagine a infrarossi. Questi dati vengono poi convertiti in una rappresentazione matematica e confrontati con i dati del viso registrati e archiviati in modo sicuro in Secure Enclave.\nSecure Enclave e Data Protection: Secure Enclave è progettato per proteggere i dati sensibili e gestire le operazioni crittografiche che li proteggono. Garantisce che anche se il kernel del sistema operativo viene compromesso, i dati facciali non possono essere accessibili da app non autorizzate o aggressori. I dati di Face ID non lasciano mai il dispositivo e non vengono sottoposti a backup su iCloud o altrove.\nAggiornamenti del Firmware: Apple rilascia frequentemente aggiornamenti firmware per risolvere le vulnerabilità di sicurezza e migliorare la funzionalità dei suoi sistemi. Secure Boot garantisce che ogni aggiornamento firmware sia autenticato e che solo gli aggiornamenti firmati da Apple vengano installati sul dispositivo, preservando l’integrità e la sicurezza del sistema Face ID.\nUtilizzando Secure Boot con hardware dedicato come Secure Enclave, Apple può fornire solide garanzie di sicurezza per operazioni sensibili come il riconoscimento facciale.\n\n\nSfide\nL’implementazione di Secure Boot pone diverse sfide che devono essere affrontate per ottenere tutti i suoi vantaggi.\nComplessità della Gestione delle Chiavi: Generare, archiviare, distribuire, ruotare e revocare chiavi crittografiche in modo dimostrabilmente sicuro è estremamente impegnativo ma fondamentale per mantenere la catena di fiducia. Qualsiasi compromissione delle chiavi paralizza le protezioni. Le grandi aziende che gestiscono moltitudini di chiavi di dispositivi affrontano particolari sfide di scala.\nSovraccarico di Prestazioni: Il controllo delle firme crittografiche durante il Boot può aggiungere 50-100ms o più per componente verificato. Questo ritardo può essere proibitivo per applicazioni sensibili al tempo o con risorse limitate. Tuttavia, gli impatti sulle prestazioni possono essere ridotti tramite parallelizzazione e accelerazione hardware.\nSigning Burden: [Onere della firma] Gli sviluppatori devono garantire diligentemente che tutti i componenti software coinvolti nel processo di avvio, ovvero bootloader, firmware, kernel del sistema operativo, driver, applicazioni, ecc., siano firmati correttamente da chiavi attendibili. L’accettazione della firma del codice di terze parti rimane un problema.\nVerifica Crittografica: Gli algoritmi e i protocolli sicuri devono convalidare la legittimità di chiavi e firme, evitare manomissioni o bypass e supportare la revoca. L’accettazione di chiavi dubbie mina la fiducia.\nVincoli di Personalizzazione: Le architetture Secure Boot bloccate dal fornitore limitano il controllo dell’utente e l’aggiornabilità. I bootloader open source come u-boot e coreboot abilitano la sicurezza supportando al contempo la personalizzazione.\nStandard Scalabili: Standard emergenti come Device Identifier Composition Engine (DICE) e IDevID promettono di fornire e gestire in modo sicuro identità e chiavi dei dispositivi su larga scala in tutti gli ecosistemi.\nL’adozione di Secure Boot richiede di seguire le best practice di sicurezza relative alla gestione delle chiavi, alla convalida della crittografia, agli aggiornamenti firmati e al controllo degli accessi. Secure Boot fornisce una solida base per creare integrità e affidabilità dei dispositivi se implementato con cura.\n\n\n\n14.6.3 Moduli di Sicurezza Hardware\n\nHSM\nUn “Hardware Security Module (HSM)” è un dispositivo fisico che gestisce le chiavi digitali per un’autenticazione avanzata e fornisce l’elaborazione crittografica. Questi moduli sono progettati per essere resistenti alle manomissioni e fornire un ambiente sicuro per l’esecuzione di operazioni crittografiche. Gli HSM possono essere dispositivi standalone, schede plug-in o circuiti integrati su un altro dispositivo.\nGli HSM sono fondamentali per varie applicazioni sensibili alla sicurezza perché offrono un’enclave rafforzata e sicura per l’archiviazione delle chiavi crittografiche e l’esecuzione di funzioni crittografiche. Sono particolarmente importanti per garantire la sicurezza delle transazioni, le verifiche dell’identità e la crittografia dei dati.\n\n\nVantaggi\nGli HSM forniscono diverse funzionalità utili per la sicurezza dei sistemi ML:\nProtezione dei Dati Sensibili: Nelle applicazioni di apprendimento automatico, i modelli spesso elaborano dati sensibili che possono essere proprietari o personali. Gli HSM proteggono le chiavi di crittografia utilizzate per proteggere questi dati, sia a riposo che in transito, dall’esposizione o dal furto.\nGaranzia dell’Integrità del Modello: L’integrità dei modelli ML è fondamentale per il loro funzionamento affidabile. Gli HSM possono gestire in modo sicuro i processi di firma e verifica per software e firmware ML, assicurando che parti non autorizzate non abbiano alterato i modelli.\nAddestramento e Aggiornamenti Sicuri del Modello: L’addestramento e l’aggiornamento dei modelli ML comportano l’elaborazione di dati potenzialmente sensibili. Gli HSM garantiscono che questi processi vengano condotti all’interno di un confine crittografico sicuro, proteggendo dall’esposizione dei dati di addestramento e dagli aggiornamenti non autorizzati del modello.\n\n\nCompromessi\nGli HSM comportano diversi compromessi per l’ML embedded. Questi compromessi sono simili ai TEE, ma per completezza, li discuteremo anche qui attraverso la lente dell’HSM.\nCosto: Gli HSM sono dispositivi specializzati che possono essere costosi da procurare e implementare, aumentando il costo complessivo di un progetto ML. Questo può essere un fattore significativo per i sistemi embedded, dove i vincoli di costo sono spesso più rigidi.\nSovraccarico di Prestazioni: Sebbene sicure, le operazioni crittografiche eseguite dagli HSM possono introdurre latenza. Qualsiasi ritardo aggiunto può essere critico nelle applicazioni ML embedded ad alte prestazioni in cui l’inferenza deve avvenire in tempo reale, come nei veicoli autonomi o nei dispositivi di traduzione.\nSpazio Fisico: I sistemi embedded sono spesso limitati dallo spazio fisico e l’aggiunta di un HSM può essere difficile in ambienti con vincoli rigidi. Ciò è particolarmente vero per l’elettronica di consumo e la tecnologia indossabile, dove le dimensioni e il fattore di forma sono considerazioni chiave.\nConsumo Energetico: Gli HSM richiedono energia per funzionare, il che può rappresentare uno svantaggio per i dispositivi a batteria con una lunga durata della batteria. L’elaborazione sicura e le operazioni crittografiche possono scaricare la batteria più velocemente, un compromesso significativo per le applicazioni ML embedded mobili o remote.\nComplessità nell’Integrazione: L’integrazione degli HSM nei sistemi hardware esistenti aggiunge complessità. Spesso sono necessarie conoscenze specialistiche per gestire la comunicazione sicura tra l’HSM e il processore del sistema e sviluppare software in grado di interfacciarsi con l’HSM.\nScalabilità: Il ridimensionamento di una soluzione ML che utilizza gli HSM può essere impegnativo. Gestire una flotta di HSM e garantire l’uniformità nelle pratiche di sicurezza tra i dispositivi può diventare complesso e costoso quando aumentano le dimensioni della distribuzione, soprattutto quando si ha a che fare con sistemi embedded in cui la comunicazione è costosa.\nComplessità Operativa: Gli HSM possono rendere più complesso l’aggiornamento del firmware e dei modelli ML. Ogni aggiornamento deve essere firmato e possibilmente crittografato, il che aggiunge passaggi al processo di aggiornamento e potrebbe richiedere meccanismi sicuri per la gestione delle chiavi e la distribuzione degli aggiornamenti.\nSviluppo e Manutenzione: La natura sicura degli HSM implica che solo personale limitato abbia accesso all’HSM per scopi di sviluppo e manutenzione. Ciò può rallentare il processo di sviluppo e rendere più difficile la manutenzione di routine.\nCertificazione e Conformità: Garantire che un HSM soddisfi specifici standard di settore e requisiti di conformità può aumentare i tempi e i costi di sviluppo. Ciò potrebbe comportare l’esecuzione di rigorosi processi di certificazione e audit.\n\n\n\n14.6.4 Physical Unclonable Functions (PUF)\n\nInformazioni\nLe “Physical Unclonable Function (PUF)” [funzioni fisiche non clonabili] forniscono un mezzo intrinseco all’hardware per la generazione di chiavi crittografiche e l’autenticazione del dispositivo sfruttando la variabilità di produzione intrinseca nei componenti semiconduttori. Durante la fabbricazione, fattori fisici casuali come variazioni di drogaggio, ruvidità del bordo della linea e spessore dielettrico determinano differenze microscopiche tra i semiconduttori, anche quando prodotti dalle stesse maschere. Questi creano variazioni di temporizzazione e potenza rilevabili che agiscono come una “impronta digitale” unica per ogni chip. Le PUF sfruttano questo fenomeno incorporando circuiti integrati per amplificare piccole differenze di temporizzazione o potenza in uscite digitali misurabili.\nQuando stimolato con uno stimolo in input, il circuito PUF produce una risposta di output basata sulle caratteristiche fisiche intrinseche del dispositivo. A causa della loro unicità fisica, lo stesso stimolo produrrà una risposta diversa su altri dispositivi. Questo meccanismo di stimolo-risposta può essere utilizzato per generare chiavi in modo sicuro e identificatori legati all’hardware specifico, eseguire l’autenticazione del dispositivo o archiviare in modo sicuro i segreti. Ad esempio, una chiave derivata da un PUF funzionerà solo su quel dispositivo e non potrà essere clonata o estratta nemmeno con accesso fisico o reverse engineering completo (Gao, Al-Sarawi, e Abbott 2020).\n\n\nVantaggi\nLa generazione di chiavi PUF evita l’archiviazione esterna delle chiavi, che rischia di essere esposta. Fornisce inoltre una base per altre primitive di sicurezza hardware come Secure Boot. Le sfide di implementazione includono la gestione di affidabilità ed entropia variabili tra diverse PUF, sensibilità alle condizioni ambientali e suscettibilità agli attacchi di modellazione di apprendimento automatico. Se progettate con cura, le PUF consentono applicazioni promettenti nella protezione IP, nel trusted computing e nell’anticontraffazione.\n\n\nUtilità\nI modelli di apprendimento automatico stanno rapidamente diventando una parte fondamentale della funzionalità per molti dispositivi embedded, come smartphone, assistenti domestici intelligenti e droni autonomi. Tuttavia, proteggere l’apprendimento automatico su hardware embedded con risorse limitate può essere difficile. È qui che le funzioni fisiche non clonabili (PUF) risultano particolarmente utili. Diamo un’occhiata ad alcuni esempi di come le PUF possono essere utili.\nLe PUF forniscono un modo per generare impronte digitali e chiavi crittografiche univoche legate alle caratteristiche fisiche di ciascun chip sul dispositivo. Facciamo un esempio. Abbiamo un drone con telecamera intelligente che usa ML embedded per tracciare gli oggetti. Un PUF integrato nel processore del drone potrebbe creare una chiave specifica del dispositivo per crittografare il modello ML prima di caricarlo sul drone. In questo modo, anche se un aggressore in qualche modo hackerasse il drone e provasse a rubare il modello, non sarebbe in grado di usarlo su un altro dispositivo!\nLa stessa chiave PUF potrebbe anche creare una filigrana digitale embedded nel modello ML. Se quel modello dovesse mai trapelare e essere pubblicato online da qualcuno che cercasse di piratarlo, la filigrana potrebbe aiutare a dimostrare che proviene dal drone rubato e non dall’aggressore. Inoltre, si immagini che la telecamera del drone si colleghi al cloud per scaricare parte della sua elaborazione ML. Il PUF può autenticare che la telecamera è legittima prima che il cloud esegua l’inferenza su video sensibili. Il cloud potrebbe verificare che il drone non sia stato manomesso fisicamente controllando che le risposte PUF non siano cambiate.\nLe PUF consentono tutta questa sicurezza attraverso la casualità intrinseca del loro comportamento di stimolo-risposta e il binding hardware. Senza dover memorizzare le chiavi esternamente, le PUF sono ideali per proteggere l’ML embedded con risorse limitate. Pertanto, offrono un vantaggio unico rispetto ad altri meccanismi.\n\n\nMeccanica\nIl principio di funzionamento alla base dei PUF, illustrato in Figura 14.8, comporta la generazione di una coppia “stimolo-risposta”, in cui un input specifico (lo stimolo) al circuito PUF determina un output (la risposta) che è determinato dalle proprietà fisiche uniche di quel circuito. Questo processo può essere paragonato a un meccanismo di impronte digitali per dispositivi elettronici. I dispositivi che utilizzano ML per elaborare i dati dei sensori possono utilizzare i PUF per proteggere la comunicazione tra dispositivi e impedire l’esecuzione di modelli ML su hardware contraffatto.\nFigura 14.8 illustra una panoramica delle basi dei PUF: a) PUF può essere pensato come un’impronta digitale unica per ogni pezzo di hardware; b) un PUF Ottico è uno speciale token di plastica che viene illuminato, creando un motivo a macchie unico che viene poi registrato; c) in un APUF (Arbiter PUF), i bit di stimolo selezionano percorsi diversi e un giudice decide quale è più veloce, dando una risposta di ‘1’ o ‘0’; d) in un PUF SRAM, la risposta è determinata dalla mancata corrispondenza nella tensione di soglia dei transistor, dove determinate condizioni portano a una risposta preferita di ‘1’. Ognuno di questi metodi utilizza caratteristiche specifiche dell’hardware per creare un identificatore univoco.\n\n\n\n\n\n\nFigura 14.8: Nozioni di base sui PUF. Fonte: Gao, Al-Sarawi, e Abbott (2020).\n\n\nGao, Yansong, Said F. Al-Sarawi, e Derek Abbott. 2020. «Physical unclonable functions». Nature Electronics 3 (2): 81–91. https://doi.org/10.1038/s41928-020-0372-5.\n\n\n\n\nSfide\nCi sono alcune sfide con i PUF. La risposta PUF può essere sensibile alle condizioni ambientali, come fluttuazioni di temperatura e tensione, portando a un comportamento incoerente di cui si deve tenere conto nella progettazione. Inoltre, poiché i PUF possono generare molte coppie stimolo-risposta uniche, gestire e garantire la coerenza di queste coppie per tutta la durata del dispositivo può essere impegnativo. Ultimo ma non meno importante, l’integrazione della tecnologia PUF può aumentare il costo di produzione complessivo di un dispositivo, sebbene possa far risparmiare sui costi di gestione delle chiavi durante il ciclo di vita del dispositivo.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#problemi-di-privacy-nella-gestione-dei-dati",
    "href": "contents/privacy_security/privacy_security.it.html#problemi-di-privacy-nella-gestione-dei-dati",
    "title": "14  Sicurezza e Privacy",
    "section": "14.7 Problemi di Privacy nella Gestione dei Dati",
    "text": "14.7 Problemi di Privacy nella Gestione dei Dati\nLa gestione sicura ed etica dei dati personali e sensibili è fondamentale poiché l’apprendimento automatico permea dispositivi come smartphone, dispositivi indossabili ed elettrodomestici intelligenti. Per l’hardware medico, la gestione sicura ed etica dei dati è ulteriormente richiesta dalla legge tramite l’Health Insurance Portability and Accountability Act (HIPAA). Questi sistemi ML embedded presentano rischi unici per la privacy, data la loro intima vicinanza alla vita degli utenti.\n\n14.7.1 Tipi di Dati Sensibili\nI dispositivi ML embedded come quelli indossabili, assistenti domestici intelligenti e veicoli autonomi elaborano spesso dati altamente personali che richiedono un’attenta gestione per preservare la privacy dell’utente e prevenirne l’uso improprio. Esempi specifici includono referti medici e piani di trattamento elaborati da dispositivi indossabili per la salute, conversazioni private costantemente acquisite da assistenti domestici intelligenti e abitudini di guida dettagliate raccolte da auto connesse. La compromissione di tali dati sensibili può portare a gravi conseguenze come furto di identità, manipolazione emotiva, umiliazione pubblica ed abuso di sorveglianza di massa.\nI dati sensibili assumono molte forme: registri strutturati come elenchi di contatti e contenuti non strutturati come flussi audio e video conversazionali. In ambito medico, le “protected health information (PHI)” [informazioni sanitarie protette] vengono raccolte dai medici durante ogni interazione e sono fortemente regolamentate da rigide linee guida HIPAA. Anche al di fuori degli ambienti medici, i dati sensibili possono comunque essere raccolti sotto forma di Personally Identifiable Information (PII) [Informazioni di identificazione personale], definite come “qualsiasi rappresentazione di informazioni che consenta di dedurre ragionevolmente l’identità di un individuo a cui si applicano le informazioni con mezzi diretti o indiretti”. Esempi di PII includono indirizzi e-mail, numeri di previdenza sociale e numeri di telefono, tra gli altri campi. Le PII vengono raccolte in ambito medico e in altri contesti (applicazioni finanziarie, ecc.) e sono fortemente regolamentate dalle politiche del Dipartimento del Lavoro.\nAnche gli output dei modelli derivati potrebbero far trapelare indirettamente dettagli sugli individui. Oltre ai dati personali, anche algoritmi e set di dati proprietari garantiscono la protezione della riservatezza. Nella sezione Data Engineering, abbiamo trattato diversi argomenti in dettaglio.\nTecniche come la de-identificazione, l’aggregazione, l’anonimizzazione e la federazione possono aiutare a trasformare i dati sensibili in forme meno rischiose mantenendo al contempo l’utilità analitica. Tuttavia, controlli diligenti su accesso, crittografia, auditing, consenso, minimizzazione e pratiche di conformità sono ancora essenziali durante tutto il ciclo di vita dei dati. Regolamenti come GDPR categorizzano diverse classi di dati sensibili e prescrivono responsabilità in merito alla loro gestione etica. Standard come NIST 800-53 forniscono rigorose linee guida per il controllo della sicurezza per la protezione della riservatezza. Con la crescente dipendenza dal ML embedded, comprendere i rischi dei dati sensibili è fondamentale.\n\n\n14.7.2 Regolamenti Applicabili\nMolte applicazioni ML embedded gestiscono dati sensibili degli utenti in base alle normative HIPAA, GDPR e CCPA. Comprendere le protezioni imposte da queste leggi è fondamentale per creare sistemi conformi.\n\nLa norma sulla privacy HIPAA stabilisce che i fornitori di assistenza che svolgono determinate attività regolano la privacy e la sicurezza dei dati medici negli Stati Uniti, con severe sanzioni per le violazioni. Tutti i dispositivi ML embedded correlati alla salute, come dispositivi indossabili diagnostici o robot di assistenza, dovrebbero implementare controlli come audit trail, controlli di accesso e crittografia prescritti da HIPAA.\nIl GDPR impone trasparenza, limiti di conservazione e diritti degli utenti sui dati dei cittadini dell’UE, anche quando elaborati da aziende al di fuori dell’UE. I sistemi per la casa intelligente che catturano conversazioni familiari o modelli di posizione dovrebbero essere conformi al GDPR. I requisiti chiave includono la minimizzazione dei dati, la crittografia e meccanismi per il consenso e la cancellazione.\nIl CCPA, che si applica in California, protegge la privacy dei dati dei consumatori tramite disposizioni quali divulgazioni obbligatorie e diritti di opt-out: i gadget IoT come gli smart speaker e i fitness tracker utilizzati dai californiani rientrano probabilmente nel suo ambito.\nIl CCPA è stato il primo insieme di regolamenti specifici per stato in merito alle preoccupazioni sulla privacy. Dopo il CCPA, regolamenti simili sono stati emanati anche in altri 10 stati, con alcuni stati che hanno proposto progetti di legge per la protezione della privacy dei dati dei consumatori.\n\nInoltre, quando pertinenti all’applicazione, le norme specifiche del settore disciplinano telematica, servizi finanziari, servizi di pubblica utilità, ecc. Le best practice come “Privacy by design”, valutazioni di impatto e mantenimento di audit trail aiutano a incorporare la conformità se non è già richiesta dalla legge. Date le sanzioni potenzialmente costose, è consigliabile consultare team legali/di conformità quando si sviluppano sistemi ML embedded regolamentati.\n\n\n14.7.3 De-identificazione\nSe i dati medici vengono completamente de-identificati, le linee guida HIPAA non si applicano direttamente e ci sono molte meno normative. Tuttavia, i dati medici devono essere de-identificati utilizzando metodi HIPAA (metodi Safe Harbor o metodi Expert Determination) affinché le linee guida HIPAA non siano più applicabili.\n\nMetodi Safe Harbor\nI metodi Safe Harbor sono più comunemente utilizzati per de-identificare le informazioni sanitarie protette a causa delle risorse limitate necessarie rispetto ai metodi Expert Determination. La de-identificazione Safe Harbor richiede la pulizia dei set di dati di tutti i dati che rientrano in una delle 18 categorie. Le seguenti categorie sono elencate come informazioni sensibili in base allo standard Safe Harbor:\n\nNome, Localizzatore geografico, Data di nascita, Numero di telefono, Indirizzo e-mail, Indirizzi, Numeri di previdenza sociale, Numeri di cartella clinica, Numeri di beneficiari sanitari, Identificatori di dispositivi e Numeri di serie, Numeri di certificati/patenti (Certificato di nascita, Patente di guida, ecc.), Numeri di conto, Identificatori di veicoli, URL di siti Web, Foto a pieno facciale e Immagini comparabili, Identificatori biometrici, Qualsiasi altro identificatore univoco\n\nPer la maggior parte di queste categorie, tutti i dati devono essere rimossi indipendentemente dalle circostanze. Per altre categorie, tra cui informazioni geografiche e data di nascita, i dati possono essere rimossi parzialmente quanto basta per rendere le informazioni difficili da re-identificare. Ad esempio, se un codice postale è abbastanza grande, le prime 3 cifre possono rimanere poiché ci sono abbastanza persone nell’area geografica da rendere difficile la re-identificazione. Le date di nascita devono essere ripulite da tutti gli elementi tranne l’anno di nascita e tutte le età superiori a 89 anni devono essere aggregate in una categoria 90+.\n\n\nMetodi di Determinazione degli Esperti\nI metodi Safe Harbor funzionano per diversi casi di de-identificazione dei dati medici, sebbene in alcuni casi sia ancora possibile la re-identificazione. Ad esempio, supponiamo che si raccolgano dati su un paziente in una città urbana con un grande codice postale, ma è stata documentata una malattia rara di cui soffre, una malattia che colpisce solo 25 persone in tutta la città. Dati i dati geografici associati all’anno di nascita, è altamente possibile che qualcuno possa re-identificare questo individuo, il che rappresenta una violazione della privacy estremamente dannosa.\nIn casi unici come questi, sono preferiti metodi di de-identificazione dei dati di determinazione esperta. La de-identificazione di determinazione esperta richiede una “persona con conoscenza ed esperienza appropriate di principi e metodi statistici e scientifici generalmente accettati per rendere le informazioni non identificabili individualmente” per valutare un set di dati e determinare se il rischio di re-identificazione dei dati individuali in un dato set di dati in combinazione con dati disponibili al pubblico (registri di voto, ecc.), è estremamente ridotto.\nLa de-identificazione tramite Expert Determination è comprensibilmente più difficile da completare rispetto alla de-identificazione tramite Safe Harbor, a causa del costo e della fattibilità dell’accesso a un esperto per verificare la probabilità di re-identificazione di un set di dati. Tuttavia, in molti casi, è richiesta una Expert Determination per garantire che la re-identificazione dei dati sia estremamente improbabile.\n\n\n\n14.7.4 Riduzione al Minimo dei Dati\nLa riduzione al minimo dei dati comporta la raccolta, la conservazione e l’elaborazione solo dei dati utente necessari per ridurre i rischi per la privacy derivanti dai sistemi ML embedded. Si inizia limitando i tipi di dati e le istanze raccolte al minimo indispensabile per la funzionalità di base del sistema. Ad esempio, un modello di rilevamento degli oggetti raccoglie solo le immagini necessarie per quella specifica attività di visione artificiale. Allo stesso modo, un assistente vocale limiterebbe l’acquisizione audio a specifici comandi vocali anziché registrare in modo persistente i suoni ambientali.\nOve possibile, i dati temporanei che risiedono brevemente nella memoria senza archiviazione persistente forniscono un’ulteriore riduzione al minimo. Dovrebbe essere stabilita una chiara base giuridica, come il consenso dell’utente, per la raccolta e la conservazione. Il sandboxing e i controlli di accesso impediscono l’uso non autorizzato oltre le attività previste. I periodi di conservazione dovrebbero essere definiti in base allo scopo, con procedure di eliminazione sicura che rimuovono i dati scaduti.\nLa riduzione al minimo dei dati può essere suddivisa in 3 categorie:\n\n“I dati devono essere adeguati rispetto allo scopo perseguito”. L’omissione di dati può limitare l’accuratezza dei modelli addestrati sui dati e qualsiasi utilità generale di un set di dati. La minimizzazione dei dati richiede che una quantità minima di dati venga raccolta dagli utenti durante la creazione di un set di dati che aggiunge valore ad altri.\nI dati raccolti dagli utenti devono essere rilevanti allo scopo della raccolta dati.\nI dati degli utenti dovrebbero essere limitati solo ai dati necessari per soddisfare lo scopo della raccolta dati iniziale. Se è possibile ottenere risultati altrettanto solidi e accurati da un set di dati più piccolo, non dovrebbero essere raccolti dati aggiuntivi oltre questo set di dati più piccolo.\n\nTecniche emergenti come la privacy differenziale, l’apprendimento federato e la generazione di dati sintetici consentono approfondimenti utili derivati da meno dati utente grezzi. L’esecuzione di mappature del flusso di dati e valutazioni di impatto aiutano a identificare le opportunità per ridurre al minimo l’utilizzo di dati grezzi.\nMetodologie come Privacy by Design (Cavoukian 2009) considerano tale minimizzazione all’inizio dell’architettura del sistema. Anche normative come il GDPR impongono principi di minimizzazione dei dati. Con un approccio multistrato nei regni legale, tecnico e di processo, la minimizzazione dei dati limita i rischi nei prodotti ML embedded.\n\nCavoukian, Ann. 2009. «Privacy by design». Office of the Information and Privacy Commissioner.\n\nCaso di Studio - Minimizzazione dei Dati Basata sulle Prestazioni\nLa minimizzazione dei dati basata sulle prestazioni (Biega et al. 2020) si concentra sull’espansione della terza categoria di minimizzazione dei dati menzionata sopra, ovvero la limitazione. Definisce specificamente la robustezza dei risultati del modello su un dato set di dati tramite determinate metriche delle prestazioni, in modo che i dati non debbano essere raccolti ulteriormente se non migliorano significativamente le prestazioni. Le metriche delle prestazioni possono essere divise in due categorie:\n\nBiega, Asia J., Peter Potash, Hal Daumé, Fernando Diaz, e Michèle Finck. 2020. «Operationalizing the Legal Principle of Data Minimization for Personalization». In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, a cura di Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, e Yiqun Liu, 399–408. ACM. https://doi.org/10.1145/3397271.3401034.\n\nPrestazioni globali di minimizzazione dei dati\n\n\nSoddisfatto se un set di dati riduce al minimo la quantità di dati per utente mentre le sue prestazioni medie su tutti i dati sono paragonabili alle prestazioni medie del set di dati originale non ridotto.\n\n\nPrestazioni di minimizzazione dei dati per utente\n\n\nSoddisfatto se un set di dati riduce al minimo la quantità di dati per utente mentre le prestazioni minime dei dati utente individuali sono paragonabili a quelle dei dati utente individuali nel set di dati originale non ridotto.\n\nLa riduzione al minimo dei dati basata sulle prestazioni può essere sfruttata in impostazioni di apprendimento automatico, inclusi algoritmi di raccomandazione di film e impostazioni di e-commerce.\nLa riduzione al minimo dei dati globali è molto più fattibile della riduzione al minimo dei dati per utente, data la differenza molto più significativa nelle perdite per utente tra i set di dati ridotti al minimo e quelli originali.\n\n\n\n14.7.5 Consenso e Trasparenza\nUn consenso e una trasparenza significativi sono fondamentali quando si raccolgono dati utente per prodotti ML embedded come smart speaker, dispositivi indossabili e veicoli autonomi. Quando viene configurato per la prima volta. Idealmente, il dispositivo dovrebbe spiegare chiaramente quali tipi di dati vengono raccolti, per quali scopi, come vengono elaborati e le policy di conservazione. Ad esempio, uno smart speaker potrebbe raccogliere campioni vocali per addestrare il riconoscimento vocale e profili vocali personalizzati. Durante l’uso, promemoria e opzioni della dashboard forniscono una trasparenza continua su come vengono gestiti i dati, come riepiloghi settimanali di frammenti vocali acquisiti. Le opzioni di controllo consentono di revocare o limitare il consenso, come disattivare l’archiviazione dei profili vocali.\nI flussi di consenso dovrebbero fornire controlli granulari che vadano oltre le semplici scelte binarie sì/no. Ad esempio, gli utenti potrebbero acconsentire selettivamente a determinati utilizzi dei dati, come l’addestramento al riconoscimento vocale, ma non alla personalizzazione. I focus group e i test di usabilità con gli utenti target modellano le interfacce di consenso e la formulazione delle policy sulla privacy per ottimizzare la comprensione e il controllo. Il rispetto dei diritti degli utenti, come l’eliminazione e la rettifica dei dati, dimostra affidabilità. Un gergo legale vago ostacola la trasparenza. Regolamenti come GDPR e CCPA rafforzano i requisiti di consenso. Un consenso ponderato e la trasparenza forniscono agli utenti l’agenzia sui propri dati, creando al contempo fiducia nei prodotti ML incorporati attraverso una comunicazione e un controllo aperti.\n\n\n14.7.6 Problemi di Privacy nell’Apprendimento Automatico\n\nIA Generativa\nSono aumentate anche le preoccupazioni sulla privacy e sulla sicurezza con l’uso pubblico di modelli di intelligenza artificiale generativa, tra cui GPT4 di OpenAI e altri LLM. ChatGPT, in particolare, è stato discusso più di recente in merito alla privacy, date tutte le informazioni personali raccolte dagli utenti di ChatGPT. Nel giugno 2023 è stata intentata una class action contro ChatGPT a causa del timore che fosse stato addestrato su informazioni mediche e personali proprietarie senza le dovute autorizzazioni o consensi. Come risultato di queste preoccupazioni sulla privacy, molte aziende hanno proibito ai propri dipendenti di accedere a ChatGPT e di caricare informazioni aziendali private sul chatbot. Inoltre, ChatGPT è suscettibile al “prompt injection” e altri attacchi alla sicurezza che potrebbero compromettere la privacy dei dati proprietari su cui è stato addestrato.\n\nCaso di Studio\nMentre ChatGPT ha istituito delle protezioni per impedire alle persone di accedere a informazioni private ed eticamente discutibili, diversi individui sono riusciti a bypassare queste protezioni tramite “prompt injection” e altri attacchi alla sicurezza. Come dimostrato in Figura 14.9, gli utenti possono bypassare le protezioni di ChatGPT per imitare il tono di una “nonna defunta” per imparare come bypassare un firewall per applicazioni Web (Gupta et al. 2023).\n\n\n\n\n\n\nFigura 14.9: Gioco di ruolo della nonna per bypassare le restrizioni di sicurezza. Fonte: Gupta et al. (2023).\n\n\n\nInoltre, gli utenti hanno anche utilizzato con successo la psicologia inversa per manipolare ChatGPT e accedere a informazioni inizialmente proibite dal modello. In Figura 14.10, a un utente viene inizialmente impedito di scoprire siti Web di pirateria tramite ChatGPT, ma può bypassare queste restrizioni utilizzando la psicologia inversa.\n\n\n\n\n\n\nFigura 14.10: Psicologia inversa per bypassare le restrizioni di sicurezza. Fonte: Gupta et al. (2023).\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, e Lopamudra Praharaj. 2023. «From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy». #IEEE_O_ACC# 11: 80218–45. https://doi.org/10.1109/access.2023.3300381.\n\n\nLa facilità con cui gli attacchi alla sicurezza possono manipolare ChatGPT è preoccupante, date le informazioni private su cui è stato addestrato senza consenso. Ulteriori ricerche sulla privacy dei dati in LLM e sull’intelligenza artificiale generativa dovrebbero concentrarsi sull’impedire al modello di essere così ingenuo da indurre attacchi di “injection”.\n\n\n\nCancellazione dei Dati\nMolte delle normative precedentemente menzionate, incluso il GDPR, includono una clausola sul “diritto all’oblio”. Questa clausola afferma essenzialmente che “l’interessato ha il diritto di ottenere dal titolare del trattamento la cancellazione dei dati personali che lo riguardano senza indebito ritardo”. Tuttavia, in diversi casi, anche se i dati dell’utente sono stati cancellati da una piattaforma, i dati vengono cancellati solo parzialmente se un modello di apprendimento automatico è stato addestrato su questi dati per scopi separati. Attraverso metodi simili agli attacchi di inferenza di appartenenza, altri individui possono ancora dedurre i dati di addestramento su cui è stato addestrato un modello, anche se la presenza dei dati è stata esplicitamente rimossa online.\nUn approccio per affrontare i problemi di privacy con i dati di addestramento dell’apprendimento automatico è stato attraverso metodi di privacy differenziali. Ad esempio, aggiungendo rumore laplaciano nel set di addestramento, un modello può essere robusto agli attacchi di inferenza di appartenenza, impedendo il recupero dei dati eliminati. Un altro approccio per impedire che i dati eliminati vengano dedotti da attacchi alla sicurezza è semplicemente riaddestrare il modello da zero sui dati rimanenti. Poiché questo processo è dispendioso in termini di tempo e di elaborazione dati, altri ricercatori hanno tentato di affrontare le preoccupazioni sulla privacy relative all’inferenza dei dati di training del modello tramite un processo chiamato “machine unlearning”, in cui un modello itera attivamente su se stesso per rimuovere l’influenza dei dati “dimenticati” su cui potrebbe essere stato addestrato, come menzionato di seguito.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#tecniche-ml-per-la-tutela-della-privacy",
    "href": "contents/privacy_security/privacy_security.it.html#tecniche-ml-per-la-tutela-della-privacy",
    "title": "14  Sicurezza e Privacy",
    "section": "14.8 Tecniche ML per la Tutela della Privacy",
    "text": "14.8 Tecniche ML per la Tutela della Privacy\nSono state sviluppate molte tecniche per preservare la privacy, ognuna delle quali affronta diversi aspetti e sfide per la sicurezza dei dati. Questi metodi possono essere ampiamente categorizzati in diverse aree chiave: Differential Privacy, che si concentra sulla privacy statistica negli output dei dati; Federated Learning, che enfatizza l’elaborazione decentralizzata dei dati; Homomorphic Encryption e Secure Multi-party Computation (SMC), entrambi abilitanti calcoli sicuri su dati crittografati o privati; Data Anonymization e Data Masking and Obfuscation, che alterano i dati per proteggere le identità individuali; Private Set Intersection e Zero-Knowledge Proofs, che facilitano confronti e convalide di dati sicuri; Decentralized Identifiers (DID) per identità digitali auto-sovrane; Privacy-Preserving Record Linkage (PPRL), che collega i dati tra le fonti senza esposizione; Synthetic Data Generation, che crea set di dati artificiali per analisi sicure; e Adversarial Learning Techniques, che migliora la resistenza dei dati o dei modelli agli attacchi alla privacy.\nData l’ampia gamma di queste tecniche, non è possibile approfondire ciascuna di esse in un singolo corso o discussione, e tanto meno che qualcuno possa conoscerle tutte nei loro gloriosi dettagli. Pertanto, esploreremo alcune tecniche specifiche in modo relativamente dettagliato, fornendo una comprensione più approfondita dei loro principi, applicazioni e delle sfide uniche per la privacy che affrontano nell’apprendimento automatico. Questo approccio mirato ci fornirà una comprensione più completa e pratica dei principali metodi di tutela della privacy nei moderni sistemi ML.\n\n14.8.1 Privacy Differenziale\n\nIdea Centrale\nLa “Differential Privacy” Privacy Differenziale è un framework per quantificare e gestire la privacy degli individui in un set di dati (Dwork et al. 2006). Fornisce una garanzia matematica che la privacy degli individui nel set di dati non verrà compromessa, indipendentemente da qualsiasi conoscenza aggiuntiva che un aggressore potrebbe possedere. L’idea fondamentale della privacy differenziale è che il risultato di qualsiasi analisi (come una query statistica) dovrebbe essere essenzialmente lo stesso, indipendentemente dal fatto che i dati di un individuo siano inclusi nel set di dati o meno. Ciò significa che osservando il risultato dell’analisi, non è possibile determinare se i dati di un individuo siano stati utilizzati nel calcolo.\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, e Adam Smith. 2006. «Calibrating Noise to Sensitivity in Private Data Analysis». In Theory of Cryptography, a cura di Shai Halevi e Tal Rabin, 265–84. Berlin, Heidelberg: Springer Berlin Heidelberg.\nAd esempio, supponiamo che un database contenga cartelle cliniche di 10 pazienti. Vogliamo pubblicare statistiche sulla prevalenza del diabete in questo campione senza rivelare le condizioni di un paziente. Per fare ciò, potremmo aggiungere una piccola quantità di rumore casuale al conteggio reale prima di pubblicarlo. Se il numero reale di pazienti diabetici è 6, potremmo aggiungere rumore da una distribuzione di Laplace per ottenere casualmente 5, 6 o 7, ciascuno con una certa probabilità. Un osservatore ora non può dire se un singolo paziente ha il diabete basandosi solo sull’output rumoroso. Il risultato della query è simile a se i dati di ogni paziente sono inclusi o esclusi. Questa è la privacy differenziale. Più formalmente, un algoritmo randomizzato soddisfa la privacy differenziale ε se, per qualsiasi database vicino D e Dʹ che differisce solo per una voce, la probabilità di qualsiasi risultato cambia al massimo di un fattore ε. Un ε inferiore fornisce maggiori garanzie di privacy.\nIl meccanismo di Laplace è uno dei metodi più semplici e comunemente utilizzati per ottenere la privacy differenziale. Comporta l’aggiunta di rumore che segue una distribuzione di Laplace ai dati o ai risultati delle query. A parte il Meccanismo di Laplace, il principio generale di aggiunta di rumore è fondamentale per la Privacy differenziale. L’idea è di aggiungere rumore casuale ai dati o ai risultati di una query. Il rumore è calibrato per garantire la necessaria garanzia di privacy mantenendo i dati utili.\nMentre la distribuzione di Laplace è comune, possono essere utilizzate anche altre distribuzioni come quella gaussiana. Il rumore di Laplace è utilizzato per la privacy differenziale rigorosa ε per query a bassa sensibilità. Al contrario, le distribuzioni gaussiane possono essere utilizzate quando la privacy non è garantita, nota come privacy differenziale (ϵ, 𝛿). In questa versione rilassata della privacy differenziale, epsilon e delta definiscono la quantità di privacy garantita quando si rilasciano informazioni o un modello correlato a un set di dati. Epsilon stabilisce un limite su quanta informazione può essere appresa sui dati in base all’output. Allo stesso tempo, delta consente una piccola probabilità che la garanzia della privacy venga violata. La scelta tra Laplace, gaussiana e altre distribuzioni dipenderà dai requisiti specifici della query e del set di dati e dal compromesso tra Privacy e accuratezza.\nPer illustrare il compromesso tra Privacy e accuratezza nella Privacy differenziale (\\(\\epsilon\\), \\(\\delta\\)), i seguenti grafici in Figura 14.11 mostrano i risultati sull’accuratezza per diversi livelli di rumore sul set di dati MNIST, un ampio set di dati di cifre scritte a mano (Abadi et al. 2016). Il valore delta (linea nera; asse y destro) indica il livello di rilassamento della privacy (un valore elevato indica che la Privacy è meno rigorosa). Man mano che la Privacy diventa più rilassata, aumenta l’accuratezza del modello.\n\n\n\n\n\n\nFigura 14.11: Compromesso tra privacy e accuratezza. Fonte: Abadi et al. (2016).\n\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nI punti chiave da ricordare sulla privacy differenziale sono i seguenti:\n\nAggiunta di Rumore: La tecnica fondamentale nella Privacy differenziale è l’aggiunta di rumore casuale controllato ai dati o ai risultati delle query. Questo rumore maschera il contributo dei singoli dati.\nAtto di Bilanciamento: C’è un equilibrio tra Privacy e accuratezza. Più rumore (ϵ inferiore) nei dati significa maggiore Privacy ma minore accuratezza nei risultati del modello.\nUniversalità: La privacy differenziale non si basa su ipotesi su ciò che sa un aggressore.s. Ciò lo rende robusto contro gli attacchi di re-identificazione, in cui un aggressore cerca di scoprire dati individuali.\nApplicabilità: Può essere applicato a vari tipi di dati e query, rendendolo uno strumento versatile per l’analisi dei dati che preserva la privacy.\n\n\n\nCompromessi\nCi sono diversi compromessi da fare con la Privacy differenziale, come nel caso di qualsiasi algoritmo. Ma concentriamoci sui compromessi specifici computazionali, poiché ci interessano i sistemi ML. Ci sono alcune considerazioni e compromessi computazionali chiave quando si implementa la privacy differenziale in un sistema di apprendimento automatico:\nGenerazione di Rumore: L’implementazione della privacy differenziale introduce diversi compromessi computazionali importanti rispetto alle tecniche di apprendimento automatico standard. Una considerazione importante è la necessità di generare in modo sicuro rumore casuale da distribuzioni come Laplace o Gaussiana che vengono aggiunte ai risultati delle query e agli output del modello. La generazione di numeri casuali crittografici di alta qualità può essere computazionalmente costosa.\nAnalisi di Sensibilità: Un altro requisito fondamentale è il monitoraggio rigoroso della sensibilità degli algoritmi sottostanti ai singoli punti dati che vengono aggiunti o rimossi. Questa analisi di sensibilità globale è necessaria per calibrare correttamente i livelli di rumore. Tuttavia, l’analisi della sensibilità del caso peggiore può aumentare sostanzialmente la complessità computazionale per complesse procedure di addestramento del modello e pipeline di dati.\nGestione del budget per la privacy: La gestione del budget per la perdita della privacy su più query e iterazioni di apprendimento è un altro sovraccarico contabile. Il sistema deve tenere traccia dei costi cumulativi per la privacy e comporli per spiegare le garanzie di privacy complessive. Ciò aggiunge un onere computazionale che va oltre la semplice esecuzione di query o modelli di addestramento.\nCompromessi tra batch e online: Per i sistemi di apprendimento online con query continue ad alto volume, gli algoritmi differenzialmente privati richiedono nuovi meccanismi per mantenere l’utilità e prevenire troppe perdite di privacy accumulate poiché ogni query può potenzialmente alterare il budget per la privacy. L’elaborazione offline in batch è più semplice da una prospettiva computazionale poiché elabora i dati in grandi batch, dove ogni batch viene trattato come una singola query. I dati sparsi ad alta dimensionalità aumentano anche le difficoltà dell’analisi di sensibilità.\nAddestramento distribuito: Quando si addestrano modelli utilizzando approcci distribuiti o federati, sono necessari nuovi protocolli crittografici per tracciare e limitare le “fughe” di privacy tra i nodi. Il calcolo multi-parti sicuro con dati crittografati per la Privacy differenziale aggiunge un carico computazionale sostanziale.\nMentre la Privacy differenziale fornisce solide garanzie formali di privacy, la sua implementazione rigorosa richiede aggiunte e modifiche alla pipeline di apprendimento automatico a un costo computazionale. La gestione di queste spese generali preservando l’accuratezza del modello rimane un’area di ricerca attiva.\n\n\nCaso di Studio\nL’implementazione della Privacy differenziale da parte di Apple in iOS e MacOS fornisce un importante esempio concreto di come la Privacy differenziale può essere distribuita su larga scala. Apple voleva raccogliere statistiche aggregate sull’utilizzo nel proprio ecosistema per migliorare prodotti e servizi, ma mirava a farlo senza compromettere la privacy dei singoli utenti.\nPer raggiungere questo obiettivo, ha implementato tecniche di privacy differenziale direttamente sui dispositivi degli utenti per rendere anonimi i punti dati prima di inviarli ai server Apple. In particolare, Apple utilizza il meccanismo di Laplace per iniettare rumore casuale attentamente calibrato. Ad esempio, supponiamo che la cronologia delle posizioni di un utente contenga [Lavoro, Casa, Lavoro, Palestra, Lavoro, Casa]. In tal caso, la versione privata differenziale potrebbe sostituire le posizioni esatte con un campione rumoroso come [Palestra, Casa, Lavoro, Lavoro, Casa, Lavoro].\nApple regola la distribuzione del rumore di Laplace per fornire un elevato livello di privacy preservando al contempo l’utilità delle statistiche aggregate. L’aumento dei livelli di rumore fornisce maggiori garanzie di privacy (valori ε inferiori nella terminologia DP) ma può ridurre l’utilità dei dati. Gli ingegneri della privacy di Apple hanno ottimizzato empiricamente questo compromesso in base ai loro obiettivi di prodotto.\nApple ottiene statistiche aggregate ad alta fedeltà aggregando centinaia di milioni di punti dati rumorosi dai dispositivi. Ad esempio, possono analizzare le funzionalità delle nuove app iOS mascherando i comportamenti delle app di qualsiasi utente. Il calcolo sul dispositivo evita di inviare dati grezzi ai server Apple.\nIl sistema utilizza la generazione di numeri casuali sicuri basata su hardware per campionare in modo efficiente dalla distribuzione di Laplace sui dispositivi. Apple ha anche dovuto ottimizzare i suoi algoritmi e pipeline differenzialmente privati per operare sotto i vincoli computazionali dell’hardware degli utenti.\nNumerosi audit di terze parti hanno verificato che il sistema Apple fornisce rigorose protezioni differenziali della privacy in linea con le loro politiche dichiarate. Naturalmente, le ipotesi sulla composizione nel tempo e sui potenziali rischi di reidentificazione sono ancora valide. L’implementazione di Apple mostra come la privacy differenziale può essere realizzata in grandi prodotti del mondo reale quando supportata da sufficienti risorse ingegneristiche.\n\n\n\n\n\n\nEsercizio 14.1: Privacy Differenziale - Privacy TensorFlow\n\n\n\n\n\nVolete addestrare un modello ML senza compromettere i segreti di nessuno? La Privacy differenziale è come un superpotere per i dati! In questo Colab, useremo TensorFlow Privacy per aggiungere rumore speciale durante l’addestramento. Ciò rende molto più difficile per chiunque determinare se sono stati utilizzati i dati di una singola persona, anche se hanno modi furtivi per sbirciare il modello.\n\n\n\n\n\n\n\n14.8.2 Il Federated Learning\n\nIdea Centrale\nIl “Federated Learning (FL)” è un tipo di apprendimento automatico in cui un modello viene creato e distribuito su più dispositivi o server mantenendo localizzati i dati di training. È stato precedentemente discusso nel capitolo Ottimizzazioni del modello. Tuttavia, lo riepilogheremo brevemente qui per completarlo e concentrarci su cose che riguardano questo capitolo.\nFL addestra modelli di apprendimento automatico su reti decentralizzate di dispositivi o sistemi, mantenendo tutti i dati di addestramento localizzati. Figura 14.12 illustra questo processo: ogni dispositivo partecipante sfrutta i propri dati locali per calcolare gli aggiornamenti del modello, che vengono poi aggregati per creare un modello globale migliorato. Tuttavia, i dati di training grezzi non vengono mai condivisi, trasferiti o compilati direttamente. Questo approccio di tutela della privacy consente lo sviluppo congiunto di modelli ML senza centralizzare i dati di training potenzialmente sensibili in un unico posto.\n\n\n\n\n\n\nFigura 14.12: Ciclo di Vita dell’Apprendimento Federato. Fonte: Jin et al. (2020).\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, e Qiang Yang. 2020. «Towards utilizing unlabeled data in federated learning: A survey and prospective». arXiv preprint arXiv:2002.11545.\n\n\nUno degli algoritmi di aggregazione di modelli più comuni è Federated Averaging (FedAvg), in cui il modello globale viene creato calcolando la media di tutti i parametri dai parametri locali. Mentre FedAvg funziona bene con dati indipendenti e distribuiti in modo identico (IID), algoritmi alternativi come Federated Proximal (FedProx) sono fondamentali nelle applicazioni del mondo reale in cui i dati sono spesso non IID. FedProx è progettato per il processo FL quando c’è una significativa eterogeneità negli aggiornamenti client a causa di diverse distribuzioni di dati tra dispositivi, capacità di calcolo o quantità variabili di dati.\nLasciando i dati grezzi distribuiti e scambiando solo aggiornamenti temporanei del modello, l’apprendimento federato fornisce un’alternativa più sicura e che migliora la privacy alle tradizionali pipeline di apprendimento automatico centralizzate. Ciò consente alle organizzazioni e agli utenti di trarre vantaggio in modo collaborativo da modelli condivisi mantenendo al contempo il controllo e la proprietà sui dati sensibili. La natura decentralizzata di FL lo rende anche robusto per singoli punti di errore.\nSi immagini un gruppo di ospedali che desidera collaborare a uno studio per prevedere i risultati dei pazienti in base ai loro sintomi. Tuttavia, non possono condividere i dati dei pazienti a causa di problemi di privacy e normative come HIPAA. Ecco come il Federated Learning può aiutare.\n\nAddestramento Locale: Ogni ospedale addestra un modello di apprendimento automatico sui dati dei pazienti. Questo addestramento avviene localmente, il che significa che i dati non lasciano mai i server dell’ospedale.\nCondivisione del Modello: Dopo l’addestramento, ogni ospedale invia solo il modello (in particolare, i suoi parametri o pesi) a un server centrale. Non invia alcun dato del paziente.\nModelli di Aggregazione: Il server centrale aggrega questi modelli da tutti gli ospedali in un singolo modello più robusto. Questo processo in genere comporta la media dei parametri del modello.\nVantaggio: Il risultato è un modello di apprendimento automatico che ha appreso da un’ampia gamma di dati dei pazienti senza condividere dati sensibili o rimuoverli dalla loro posizione originale.\n\n\n\nCompromessi\nCi sono diversi aspetti correlati alle prestazioni del sistema di FL nei sistemi di apprendimento automatico. Sarebbe saggio comprendere questi compromessi perché non esiste un “pranzo gratis” per preservare la privacy tramite FL (Li et al. 2020).\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, e Virginia Smith. 2020. «Federated Learning: Challenges, Methods, and Future Directions». IEEE Signal Process Mag. 37 (3): 50–60. https://doi.org/10.1109/msp.2020.2975749.\nSovraccarico di Comunicazione e Vincoli di Rete: Nel FL, una delle sfide più significative è la gestione del sovraccarico di comunicazione.. Ciò comporta la frequente trasmissione di aggiornamenti del modello tra un server centrale e numerosi dispositivi client, che può richiedere molta larghezza di banda. Il numero totale di round di comunicazione e la dimensione dei messaggi trasmessi per round devono essere ridotti per ridurre ulteriormente la comunicazione. Ciò può comportare un traffico di rete sostanziale, soprattutto in scenari con molti partecipanti. Inoltre, la latenza diventa un fattore critico: il tempo impiegato per inviare, aggregare e ridistribuire questi aggiornamenti può causare ritardi. Ciò influisce sul tempo di training complessivo e ha un impatto sulla reattività del sistema e sulle capacità in tempo reale. Gestire questa comunicazione riducendo al minimo l’utilizzo della larghezza di banda e la latenza è fondamentale per implementare FL.\nCarico di Calcolo sui Dispositivi Locali: FL si basa su dispositivi client (come smartphone o dispositivi IoT, che sono particolarmente importanti in TinyML) per l’addestramento del modello, che spesso hanno una potenza di calcolo e una durata della batteria limitate. L’esecuzione di algoritmi complessi di apprendimento automatico in locale può mettere a dura prova queste risorse, portando a potenziali problemi di prestazioni. Inoltre, le capacità di questi dispositivi possono variare in modo significativo, con conseguenti contributi non uniformi al processo di addestramento del modello. Alcuni dispositivi elaborano gli aggiornamenti in modo più rapido ed efficiente di altri, portando a disparità nel processo di apprendimento. Bilanciare il carico computazionale per garantire una partecipazione e un’efficienza coerenti su tutti i dispositivi è una sfida fondamentale in FL.\nEfficienza dell’Addestramento del Modello: La natura decentralizzata di FL può influire sull’efficienza dell’addestramento del modello. Raggiungere la convergenza, in cui il modello non migliora più in modo significativo, può essere più lento in FL rispetto ai metodi di addestramento centralizzati. Ciò è particolarmente vero nei casi in cui i dati sono non IID (non indipendenti e distribuiti in modo identico) tra i dispositivi. Inoltre, gli algoritmi utilizzati per aggregare gli aggiornamenti del modello svolgono un ruolo fondamentale nel processo di addestramento. La loro efficienza influisce direttamente sulla velocità e l’efficacia dell’apprendimento. Sviluppare e implementare algoritmi in grado di gestire le complessità di FL garantendo al contempo una convergenza tempestiva è essenziale per le prestazioni del sistema.\nSfide di Scalabilità: La scalabilità è una preoccupazione significativa in FL, soprattutto con l’aumento del numero di dispositivi partecipanti. La gestione e il coordinamento degli aggiornamenti del modello da molti dispositivi aggiungono complessità e possono mettere a dura prova il sistema. È fondamentale garantire che l’architettura del sistema possa gestire in modo efficiente questo carico aumentato senza degradare le prestazioni. Ciò implica non solo la gestione degli aspetti computazionali e di comunicazione, ma anche il mantenimento della qualità e della coerenza del modello man mano che aumenta la scala dell’operazione. Una sfida fondamentale è la progettazione di sistemi FL che si adattino in modo efficace mantenendo le prestazioni.\nSincronizzazione e Coerenza dei Dati: Garantire la sincronizzazione dei dati e mantenere la coerenza del modello su tutti i dispositivi partecipanti in FL è una sfida. Mantenere tutti i dispositivi sincronizzati con l’ultima versione del modello può essere difficile in ambienti con connettività intermittente o dispositivi che vanno offline periodicamente. Inoltre, è fondamentale mantenere la coerenza nel modello addestrato, soprattutto quando si ha a che fare con un’ampia gamma di dispositivi con diverse distribuzioni dei dati e frequenze di aggiornamento. Ciò richiede sofisticate strategie di sincronizzazione e aggregazione per garantire che il modello finale rifletta accuratamente gli addestramenti da tutti i dispositivi.\nConsumo Energetico: Il consumo energetico dei dispositivi client in FL è un fattore critico, in particolare per i dispositivi alimentati a batteria come smartphone e altri dispositivi TinyML/IoT. Le richieste di elaborazione dei modelli di training a livello locale possono portare a un notevole consumo della batteria, il che potrebbe scoraggiare la partecipazione continua al processo FL. È essenziale bilanciare i requisiti di elaborazione dei modelli di training con l’efficienza energetica. Ciò comporta l’ottimizzazione di algoritmi e processi di training per ridurre il consumo energetico e ottenere risultati di addestramento efficaci. Garantire un funzionamento efficiente dal punto di vista energetico è fondamentale per l’accettazione da parte dell’utente e la sostenibilità dei sistemi FL.\n\n\nCasi di Studio\nEcco un paio di casi di studio reali che possono illustrare l’uso dell’apprendimento federato:\n\nGoogle Gboard\nGoogle utilizza l’apprendimento federato per migliorare le previsioni sulla sua app per tastiera mobile Gboard. L’app esegue un algoritmo di apprendimento federato sui dispositivi degli utenti per apprendere dai loro modelli di utilizzo locali e dalle previsioni di testo, mantenendo al contempo privati i dati degli utenti. Gli aggiornamenti del modello vengono aggregati nel cloud per produrre un modello globale migliorato. Ciò consente di fornire previsioni della parola successiva personalizzate in base allo stile di digitazione di ciascun utente, evitando al contempo di raccogliere direttamente dati di digitazione sensibili. Google ha segnalato che l’approccio di apprendimento federato ha ridotto gli errori di previsione del 25% rispetto alla baseline, preservando al contempo la privacy.\n\n\nRicerca Sanitaria\nLa UK Biobank e l’American College of Cardiology hanno combinato set di dati per addestrare un modello per il rilevamento dell’aritmia cardiaca utilizzando l’apprendimento federato. I set di dati non potevano essere combinati direttamente a causa di restrizioni legali e sulla privacy. L’apprendimento federato ha consentito lo sviluppo di modelli collaborativi senza condividere dati sanitari protetti, con solo aggiornamenti del modello scambiati tra le parti. Questa accuratezza del modello è migliorata in quanto potrebbe sfruttare una più ampia diversità di dati di training, soddisfacendo al contempo i requisiti normativi.\n\n\nServizi Finanziari\nLe banche stanno valutando l’utilizzo dell’apprendimento federato per i modelli di “anti-money laundering (AML)” [rilevamento antiriciclaggio ]. Più banche potrebbero migliorare congiuntamente i modelli AML senza condividere dati riservati sulle transazioni dei clienti con concorrenti o terze parti. Solo gli aggiornamenti del modello devono essere aggregati anziché i dati grezzi sulle transazioni. Ciò consente l’accesso a dati di training più completi da diverse fonti, evitando al contempo problemi normativi e di riservatezza relativi alla condivisione di dati finanziari sensibili dei clienti.\nQuesti esempi dimostrano come l’apprendimento federato fornisca vantaggi tangibili sulla privacy e consenta un ML collaborativo in contesti in cui la condivisione diretta dei dati è impossibile.\n\n\n\n\n14.8.3 Machine Unlearning\n\nIdea Centrale\nIl “Machine unlearning” è un processo abbastanza nuovo che descrive come l’influenza di un sottoinsieme di dati di training può essere rimossa dal modello. Sono stati utilizzati diversi metodi per eseguire l’unlearning automatico e rimuovere l’influenza di un sottoinsieme di dati di training dal modello finale. Un approccio di base potrebbe consistere semplicemente nel perfezionare il modello per più epoche solo sui dati che dovrebbero essere ricordati per ridurre l’influenza dei dati “dimenticati” dal modello. Poiché questo approccio non rimuove esplicitamente l’influenza dei dati che dovrebbero essere cancellati, sono ancora possibili attacchi di inferenza di appartenenza, quindi i ricercatori hanno adottato altri approcci per disimparare i dati da un modello in modo esplicito. Un tipo di approccio adottato dai ricercatori include l’adeguamento della funzione di perdita del modello per trattare le perdite del “set di dimenticanza esplicito” (dati da disimparare) e del “set di conservazione” (dati rimanenti che dovrebbero ancora essere ricordati) in modo diverso (Tarun et al. 2022; Khan e Swaroop 2021).\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, e Mohan Kankanhalli. 2022. «Deep Regression Unlearning». ArXiv preprint abs/2210.08196. https://arxiv.org/abs/2210.08196.\n\nKhan, Mohammad Emtiyaz, e Siddharth Swaroop. 2021. «Knowledge-Adaptation Priors». In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, a cura di Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 19757–70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\nCaso di Studio\nAlcuni ricercatori hanno dimostrato un esempio concreto di approcci di disapprendimento automatico applicati ai modelli di machine learning SOTA attraverso l’addestramento di un LLM, LLaMA2-7b, per disimparare qualsiasi riferimento a Harry Potter (Eldan e Russinovich 2023). Sebbene questo modello abbia richiesto 184K ore di GPU per il pre-addestramento, è bastata solo 1 ora di GPU di messa a punto per cancellare la capacità del modello di generare o richiamare contenuti correlati a Harry Potter senza compromettere in modo evidente l’accuratezza della generazione di contenuti non correlati a Harry Potter. Figura 14.13 mostra come l’output del modello cambia prima (colonna Llama-7b-chat-hf) e dopo (colonna Llama-b messa a punto) che si è verificato il disapprendimento.\n\n\n\n\n\n\nFigura 14.13: Llama disapprendimento di Harry Potter. Fonte: Eldan e Russinovich (2023).\n\n\nEldan, Ronen, e Mark Russinovich. 2023. «Who’s Harry Potter? Approximate Unlearning in LLMs». ArXiv preprint abs/2310.02238. https://arxiv.org/abs/2310.02238.\n\n\n\n\nAltri Utilizzi\n\nRimozione di dati avversari\nÈ stato precedentemente dimostrato che i modelli di deep learning sono vulnerabili ad attacchi avversari, in cui l’aggressore genera dati avversari simili ai dati di training originali, in cui un essere umano non riesce a distinguere tra i dati reali e quelli fabbricati. I dati avversari fanno sì che il modello emetta previsioni errate, il che potrebbe avere conseguenze negative in varie applicazioni, tra cui le previsioni di diagnosi sanitaria. Il disapprendimento automatico è stato utilizzato per disimparare l’influenza dei dati avversari, impedendo così che queste previsioni errate si verifichino e causino danni.\n\n\n\n\n14.8.4 Crittografia Omomorfica\n\nIdea Centrale\nLa crittografia omomorfica è una forma di crittografia che consente di eseguire calcoli su testo cifrato, generando un risultato crittografato che, una volta decrittografato, corrisponde al risultato delle operazioni eseguite sul testo in chiaro. Ad esempio, moltiplicando due numeri crittografati con crittografia omomorfica si ottiene un prodotto crittografato che decrittografa il prodotto effettivo dei due numeri. Ciò significa che i dati possono essere elaborati in forma crittografata e solo l’output risultante deve essere decrittografato, migliorando significativamente la sicurezza dei dati, in particolare per le informazioni sensibili.\nLa crittografia omomorfica consente calcoli esternalizzati su dati crittografati senza esporre i dati stessi esternamente per eseguire le operazioni. Tuttavia, solo determinati calcoli come addizione e moltiplicazione sono supportati negli schemi parzialmente omomorfici. La “Fully homomorphic encryption (FHE)” [crittografia completamente omomorfica ] in grado di gestire qualsiasi calcolo è ancora più complessa. Il numero di possibili operazioni è limitato prima che l’accumulo di rumore corrompa il testo cifrato.\nPer utilizzare la crittografia omomorfica su diverse entità, le chiavi pubbliche generate con cura devono essere scambiate per le operazioni su dati crittografati separatamente. Questa tecnica di crittografia avanzata consente paradigmi di calcolo sicuri precedentemente impossibili, ma richiede competenze specifiche per essere implementata correttamente nei sistemi del mondo reale.\n\n\nVantaggi\nLa crittografia omomorfica consente l’addestramento del modello di apprendimento automatico e l’inferenza sui dati crittografati, assicurando che gli input sensibili e i valori intermedi rimangano riservati. Ciò è fondamentale in ambito sanitario, finanziario, genetico e altri domini, che si affidano sempre di più al ML per analizzare set di dati sensibili e regolamentati contenenti miliardi di dati personali.\nLa crittografia omomorfica ostacola attacchi come l’estrazione del modello e l’inferenza dell’appartenenza che potrebbero esporre dati privati utilizzati nei flussi di lavoro ML. Fornisce un’alternativa ai TEE che utilizzano enclave hardware per l’elaborazione riservata. Tuttavia, gli schemi attuali hanno elevati overhead computazionali e limitazioni algoritmiche che limitano le applicazioni del mondo reale.\nLa crittografia omomorfica realizza la visione decennale di elaborazione multi-parti sicura consentendo l’elaborazione su testi cifrati. Concepiti negli anni ’70, i primi sistemi crittografici completamente omomorfici sono emersi nel 2009, consentendo elaborazioni arbitrarie. La ricerca in corso sta rendendo queste tecniche più efficienti e pratiche.\nLa crittografia omomorfica mostra grandi promesse nell’abilitare l’apprendimento automatico che preserva la privacy in base alle normative emergenti sui dati. Tuttavia, dati i vincoli, si dovrebbe valutare attentamente la sua applicabilità rispetto ad altri approcci di elaborazione confidenziale. Esistono ampie risorse per esplorare la crittografia omomorfica e monitorare i progressi nell’attenuare le barriere all’adozione.\n\n\nMeccanica\n\nCrittografia dei Dati: Prima che i dati vengano elaborati o inviati a un modello ML, vengono crittografati utilizzando uno schema di crittografia omomorfica e una chiave pubblica. Ad esempio, la crittografia dei numeri \\(x\\) e \\(y\\) genera i testi cifrati \\(E(x)\\) e \\(E(y)\\).\nCalcolo sul Testo Cifrato: L’algoritmo ML elabora direttamente i dati crittografati. Ad esempio, la moltiplicazione dei testi cifrati \\(E(x)\\) e \\(E(y)\\) genera \\(E(xy)\\). È possibile eseguire anche un training del modello più complesso sui testi cifrati.\nCrittografia del Risultato: Il risultato \\(E(xy)\\) rimane crittografato e può essere decrittografato solo da qualcuno con la chiave privata corrispondente per rivelare il prodotto effettivo \\(xy\\).\n\nSolo le parti autorizzate con la chiave privata possono decrittografare gli output finali, proteggendo lo stato intermedio. Tuttavia, il rumore si accumula con ogni operazione, impedendo ulteriori calcoli senza decrittazione.\nOltre all’assistenza sanitaria, la crittografia omomorfica consente il calcolo riservato per applicazioni come il rilevamento di frodi finanziarie, analisi assicurative, ricerca genetica e altro ancora. Offre un’alternativa a tecniche come il calcolo multipartitico e i TEE. La ricerca in corso migliora l’efficienza e le capacità.\nStrumenti come HElib, SEAL e TensorFlow HE forniscono librerie per esplorare l’implementazione della crittografia omomorfica in pipeline di apprendimento automatico nel mondo reale.\n\n\nCompromessi\nPer molte applicazioni in tempo reale ed embedded, la crittografia completamente omomorfica rimane poco pratica per i seguenti motivi.\nSovraccarico Computazionale: La crittografia omomorfica impone sovraccarichi computazionali molto elevati, spesso con conseguenti rallentamenti di oltre 100 volte per le applicazioni ML del mondo reale. Ciò la rende poco pratica per molti utilizzi sensibili al tempo o con risorse limitate. L’hardware ottimizzato e la parallelizzazione possono alleviare, ma non eliminare, questo problema.\nComplessità di Implementazione Gli algoritmi sofisticati richiedono una profonda competenza in crittografia per essere implementati correttamente. Sfumature come la compatibilità del formato con modelli ML in virgola mobile e la gestione scalabile delle chiavi pongono ostacoli. Questa complessità ostacola l’adozione pratica diffusa.\nLimitazioni Algoritmiche: Gli schemi attuali limitano le funzioni e la profondità dei calcoli supportati, limitando i modelli e i volumi di dati che possono essere elaborati. La ricerca in corso sta spingendo questi limiti, ma permangono delle restrizioni.\nAccelerazione Hardware: La crittografia omomorfica richiede hardware specializzato, come processori sicuri o coprocessori con TEE, che aggiungono costi di progettazione e infrastruttura.\nProgetti Ibridi: Anziché crittografare interi flussi di lavoro, l’applicazione selettiva della crittografia omomorfica a sotto-componenti critici può ottenere protezione riducendo al minimo i costi generali.\n\n\n\n\n\n\nEsercizio 14.2: Crittografia Omomorfica\n\n\n\n\n\nPronti a sbloccare il potere del calcolo crittografato? La crittografia omomorfica è come un trucco magico per i vostri dati! In questo Colab, impareremo come fare calcoli su numeri segreti senza mai rivelarli. Immaginate di addestrare un modello su dati che non potete nemmeno vedere: questo è il potere di questa tecnologia strabiliante.\n\n\n\n\n\n\n\n14.8.5 Secure Multiparty Communication\n\nIdea Centrale\nL’obiettivo principale della “Multi-Party Communication (MPC)” [Comunicazione Multi-parte] è consentire a diverse parti di calcolare congiuntamente una funzione sui propri input, mantenendo al contempo la riservatezza di tali input. Ad esempio, due organizzazioni potrebbero voler collaborare all’addestramento di un modello di apprendimento automatico combinando i rispettivi set di dati. Tuttavia, non possono rivelare direttamente tali dati a causa di vincoli di privacy o riservatezza. MPC fornisce protocolli e tecniche che consentono di ottenere i vantaggi dei dati aggregati per l’accuratezza del modello senza compromettere la privacy dei dati sensibili di ciascuna organizzazione.\nAd alto livello, MPC funziona suddividendo attentamente il calcolo in porzioni che ciascuna parte può eseguire in modo indipendente utilizzando il proprio input privato. I risultati vengono poi combinati per rivelare solo l’output finale della funzione e nulla sui valori intermedi. Vengono utilizzate tecniche crittografiche per garantire che i risultati parziali rimangano privati in modo dimostrabile.\nPrendiamo un semplice esempio di protocollo MPC. Uno dei protocolli MPC più basilari è l’addizione sicura di due numeri. Ogni parte suddivide il suo input in quote casuali che vengono distribuite segretamente. Si scambiano le quote e calcolano localmente la somma delle quote, che ricostruisce la somma finale senza rivelare i singoli input. Ad esempio, se Alice ha input x e Bob ha input y:\n\nAlice genera \\(x_1\\) casuale e imposta \\(x_2 = x - x_1\\)\nBob genera \\(y_1\\) casuale e imposta \\(y_2 = y - y_1\\)\nAlice invia \\(x_1\\) a Bob, Bob invia \\(y_1\\) ad Alice (mantenendo segreti \\(x_2\\) e \\(y_2\\))\nAlice calcola \\(x_2 + y_1 = s_1\\), Bob calcola \\(x_1 + y_2 = s_2\\)\n\\(s_1 + s_2 = x + y\\) è la somma finale, senza rivelare \\(x\\) o \\(y\\).\n\nGli input individuali di Alice e Bob (\\(x\\) e \\(y\\)) rimangono privati e ciascuna parte rivela solo un numero associato ai propri input originali. Grazie ai risultati casuali, non viene rivelata alcuna informazione sui numeri originali.\nConfronto Sicuro: Un’altra operazione di base è un confronto sicuro di due numeri, per determinare quale è maggiore dell’altro. Questo può essere fatto usando tecniche come i “Yao’s Garbled Circuits” [circuiti distorti di Yao] (https://it.wikipedia.org/wiki/Andrew_Chi-Chih_Yao), dove il circuito di confronto è crittografato per consentire una valutazione congiunta degli input senza trapelare.\nMoltiplicazione Sicura di Matrici: Le operazioni di matrice come la moltiplicazione sono essenziali per l’apprendimento automatico. Le tecniche MPC (Multiparty Communication) come la condivisione segreta additiva possono essere usate per dividere le matrici in quote casuali, calcolare i prodotti sulle quote e quindi ricostruire il risultato.\nAddestramento Sicuro del Modello: Gli algoritmi di addestramento dell’apprendimento automatico distribuito come la media federata possono essere resi sicuri usando MPC. Gli aggiornamenti del modello calcolati su dati partizionati in ogni nodo vengono condivisi segretamente tra i nodi e aggregati per addestrare il modello globale senza esporre aggiornamenti individuali.\nL’idea fondamentale alla base dei protocolli MPC è quella di dividere il calcolo in passaggi che possono essere eseguiti congiuntamente senza rivelare dati sensibili intermedi. Ciò si ottiene combinando tecniche crittografiche come la condivisione segreta, la crittografia omomorfica, il trasferimento inconsapevole e i circuiti garbled [distorti]. I protocolli MPC consentono il calcolo collaborativo di dati sensibili fornendo al contempo garanzie di privacy dimostrabili. Questa capacità di preservazione della privacy è essenziale per molte applicazioni di apprendimento automatico odierne che coinvolgono più parti che non possono condividere direttamente i propri dati grezzi.\nGli approcci principali utilizzati in MPC includono:\n\nCrittografia omomorfica: La crittografia speciale consente di eseguire calcoli su dati crittografati senza decrittografarli.\nCondivisione segreta: I dati privati vengono suddivisi in quote casuali distribuite a ciascuna parte. I calcoli vengono eseguiti localmente sulle quote e infine ricostruiti.\nTrasferimento inconsapevole: Un protocollo in cui un ricevitore ottiene un sottoinsieme di dati da un mittente, ma il mittente non sa quali dati specifici sono stati trasferiti.\nCircuiti Garbled: La funzione da calcolare è rappresentata come un circuito booleano crittografato (“distorto”) per consentire una valutazione congiunta senza rivelare gli input.\n\n\n\nCompromessi\nSebbene i protocolli MPC forniscano solide garanzie di privacy, hanno un costo computazionale elevato rispetto ai calcoli semplici. Ogni operazione sicura, come addizione, moltiplicazione, confronto, ecc., richiede più ordini di elaborazione rispetto all’operazione equivalente non crittografata. Questo overhead deriva dalle tecniche crittografiche sottostanti:\n\nNella crittografia parzialmente omomorfica, ogni calcolo su testi cifrati richiede costose operazioni a chiave pubblica. La crittografia completamente omomorfica ha overhead ancora più elevati.\nLa condivisione segreta divide i dati in più porzioni, quindi anche le operazioni di base richiedono la manipolazione di molte porzioni.\nIl trasferimento inconsapevole e i circuiti distorti aggiungono mascheramento e crittografia per nascondere i modelli di accesso ai dati e i flussi di esecuzione.\nI sistemi MPC richiedono un’ampia comunicazione e interazione tra le parti per calcolare congiuntamente condivisioni/testi cifrati.\n\nDi conseguenza, i protocolli MPC possono rallentare i calcoli di 3-4 ordini di grandezza rispetto alle implementazioni semplici. Ciò diventa proibitivo per grandi set di dati e modelli. Pertanto, l’addestramento di modelli di apprendimento automatico su dati crittografati tramite MPC rimane oggi irrealizzabile per dimensioni di set di dati realistiche a causa del sovraccarico. Sono necessarie ottimizzazioni e approssimazioni intelligenti per rendere pratico l’MPC.\nLa ricerca in corso sull’MPC colma questo divario di efficienza attraverso progressi crittografici, nuovi algoritmi, hardware affidabile come le enclave SGX e sfruttando acceleratori come GPU/TPU. Tuttavia, nel prossimo futuro, sarà necessario un certo grado di approssimazione e compromesso sulle prestazioni per scalare MPC in modo da soddisfare le esigenze dei sistemi di apprendimento automatico del mondo reale.\n\n\n\n14.8.6 Generazione di Dati Sintetici\n\nIdea Centrale\nLa generazione di dati sintetici è emersa come un importante approccio di apprendimento automatico per la tutela della privacy che consente di sviluppare e testare modelli senza esporre dati utente reali. L’idea chiave è quella di addestrare modelli generativi su set di dati reali e poi campionare da questi modelli per sintetizzare dati artificiali che corrispondono statisticamente alla distribuzione dei dati originali ma non contengono informazioni utente reali. Ad esempio, un GAN [Generative Adversarial Network] potrebbe essere addestrato su un set di dati di cartelle cliniche sensibili per apprendere i modelli sottostanti e quindi utilizzato per campionare dati sintetici dei pazienti.\nLa sfida principale della sintesi dei dati è garantire che gli avversari non siano in grado di identificare nuovamente il set di dati originale. Un approccio semplice per ottenere dati sintetici è aggiungere rumore al set di dati originale, che rischia comunque di far trapelare la privacy. Quando il rumore viene aggiunto ai dati nel contesto della privacy differenziale, vengono utilizzati meccanismi sofisticati basati sulla sensibilità dei dati per calibrare la quantità e la distribuzione del rumore. Attraverso questi framework matematicamente rigorosi, la Privacy differenziale generalmente garantisce la Privacy a un certo livello, che è l’obiettivo principale di questa tecnica di tutela della privacy. Oltre a preservare la privacy, i dati sintetici contrastano molteplici problemi di disponibilità dei dati, come set di dati sbilanciati, set di dati scarsi e rilevamento di anomalie.\nI ricercatori possono condividere liberamente questi dati sintetici e collaborare alla modellazione senza rivelare informazioni mediche private. I dati sintetici ben costruiti proteggono la privacy, offrendo al contempo utilità per lo sviluppo di modelli accurati. Le tecniche chiave per impedire la ricostruzione dei dati originali includono l’aggiunta di rumore di privacy differenziale durante l’addestramento, l’applicazione di vincoli di plausibilità e l’utilizzo di più modelli generativi diversi. Ecco alcuni approcci comuni per la generazione di dati sintetici:\n\nGenerative Adversarial Network (GAN): Le GAN sono un algoritmo di intelligenza artificiale utilizzato nell’apprendimento non supervisionato in cui due reti neurali competono tra loro in un gioco. Figura 14.14 è una panoramica del sistema GAN. La rete del generatore (grande riquadro rosso) è responsabile della produzione dei dati sintetici, mentre la rete del discriminatore (riquadro giallo) valuta l’autenticità dei dati distinguendo tra dati falsi creati dalla rete del generatore e dati reali. Le reti del generatore e del discriminatore apprendono e aggiornano i loro parametri in base ai risultati. Il discriminatore funge da metrica su quanto siano simili tra loro i dati falsi e quelli reali. È altamente efficace nel generare dati realistici ed è un approccio popolare per generare dati sintetici.\n\n\n\n\n\n\n\nFigura 14.14: Diagramma di flusso delle GAN. Fonte: Rosa e Papa (2021).\n\n\nRosa, Gustavo H. de, e João P. Papa. 2021. «A survey on text generation using generative adversarial networks». Pattern Recogn. 119 (novembre): 108098. https://doi.org/10.1016/j.patcog.2021.108098.\n\n\n\nVariational Autoencoder (VAE): I VAE sono reti neurali in grado di apprendere complesse distribuzioni di probabilità e di bilanciare la qualità della generazione dei dati e l’efficienza computazionale. Codificano i dati in uno spazio latente in cui apprendono la distribuzione per decodificare i dati.\nData Augmentation: Implica la trasformazione dei dati esistenti per creare nuovi dati modificati. Ad esempio, capovolgere, ruotare e ridimensionare (in modo uniforme o non uniforme) le immagini originali può aiutare a creare un set di dati di immagini più diversificato e robusto prima di addestrare un modello ML.\nSimulazioni: I modelli matematici possono simulare sistemi o processi del mondo reale per imitare fenomeni del mondo reale. Ciò è molto utile nella ricerca scientifica, nella pianificazione urbana e nell’economia.\n\n\n\nVantaggi\nSebbene i dati sintetici possano essere necessari a causa di rischi per la privacy o la conformità, sono ampiamente utilizzati nei modelli di apprendimento automatico quando i dati disponibili sono di scarsa qualità, scarsi o inaccessibili. I dati sintetici offrono uno sviluppo più efficiente ed efficace semplificando i processi di addestramento, test e distribuzione dei modelli robusti. Consentono ai ricercatori di condividere i modelli più ampiamente senza violare le leggi e le normative sulla privacy. La collaborazione tra gli utenti dello stesso set di dati sarà facilitata, il che aiuterà ad ampliare le capacità e i progressi nella ricerca ML.\nEsistono diverse motivazioni per l’utilizzo di dati sintetici nell’apprendimento automatico:\n\nPrivacy e Conformità: I dati sintetici evitano di esporre informazioni personali, consentendo una condivisione e una collaborazione più aperte. Ciò è importante quando si lavora con set di dati sensibili come cartelle cliniche o informazioni finanziarie.\nScarsità di dati: Quando non sono disponibili dati reali sufficienti, i dati sintetici possono aumentare i set di dati di addestramento. Ciò migliora l’accuratezza del modello quando i dati limitati rappresentano un collo di bottiglia.\nTest del modello: I dati sintetici forniscono sandbox protetti dalla privacy per testare le prestazioni del modello, risolvere i problemi e monitorare i bias.\nEtichettatura dei dati: I dati di training etichettati di alta qualità sono spesso scarsi e costosi. I dati sintetici possono aiutare a generare automaticamente esempi etichettati.\n\n\n\nCompromessi\nSebbene i dati sintetici cerchino di rimuovere qualsiasi prova del set di dati originale, la perdita della privacy rappresenta comunque un rischio, poiché i dati sintetici imitano i dati originali. Le informazioni statistiche e la distribuzione sono simili, se non uguali, tra i dati originali e sintetici. Ricampionando dalla distribuzione, gli avversari potrebbero comunque essere in grado di recuperare i campioni di addestramento originali. A causa dei loro processi di apprendimento e complessità intrinseci, le reti neurali potrebbero rivelare accidentalmente informazioni sensibili sui dati di addestramento originali.\nUna sfida fondamentale con i dati sintetici è il potenziale divario tra le distribuzioni dei dati sintetici e quelli del mondo reale. Nonostante i progressi nelle tecniche di modellazione generativa, i dati sintetici potrebbero catturare solo parzialmente la complessità, la diversità e i modelli sfumati dei dati reali. Ciò può limitare l’utilità dei dati sintetici per l’addestramento robusto di modelli di apprendimento automatico. Valutare rigorosamente la qualità dei dati sintetici tramite metodi avversari e confrontare le prestazioni del modello con i benchmark dei dati reali aiuta a valutare e migliorare la fedeltà. Tuttavia, intrinsecamente, i dati sintetici rimangono un’approssimazione.\nUn’altra preoccupazione critica sono i rischi per la privacy dei dati sintetici. I modelli generativi possono far trapelare informazioni identificabili sugli individui nei dati di training, il che potrebbe consentire la ricostruzione di informazioni private. Gli attacchi avversari emergenti dimostrano le sfide nel prevenire la perdita di identità dalle pipeline di generazione di dati sintetici. Tecniche come la privacy differenziale possono aiutare a salvaguardare la privacy, ma comportano compromessi nell’utilità dei dati. Esiste una tensione intrinseca tra la produzione di dati sintetici utili e la protezione completa dei dati di training sensibili, che devono essere bilanciati.\nUlteriori insidie dei dati sintetici includono distorsioni amplificate, etichettature errate, sovraccarico computazionale per l’addestramento di modelli generativi, costi di archiviazione e mancata contabilizzazione di nuovi dati fuori distribuzione. Sebbene questi siano secondari rispetto al divario sintetico-reale e ai rischi per la privacy, rimangono considerazioni importanti quando si valuta l’idoneità dei dati sintetici per particolari attività di apprendimento automatico. Come con qualsiasi tecnica, i vantaggi dei dati sintetici comportano compromessi e limitazioni intrinseche che richiedono strategie di mitigazione ponderate.\n\n\n\n14.8.7 Riepilogo\nSebbene tutte le tecniche di cui abbiamo discusso finora mirino a consentire un apprendimento automatico che salvaguardi la privacy, esse implicano meccanismi e compromessi distinti. Fattori come vincoli computazionali, ipotesi di fiducia richieste, modelli di minaccia e caratteristiche dei dati aiutano a guidare il processo di selezione per un caso d’uso particolare. Tuttavia, trovare il giusto equilibrio tra privacy, accuratezza ed efficienza richiede sperimentazione e valutazione empirica per molte applicazioni. Tabella 14.2 è una tabella di confronto delle principali tecniche di apprendimento automatico che salvaguardano la privacy e dei loro pro e contro:\n\n\n\nTabella 14.2: Confronto di tecniche per l’apprendimento automatico che tutela la privacy.\n\n\n\n\n\n\n\n\n\n\nTecnica\nPro\nContro\n\n\n\n\nPrivacy Differenziale\n\nForti garanzie formali di privacy\nRobusto per attacchi dati ausiliari\nVersatile per molti tipi di dati e analisi\n\n\nPerdita di accuratezza dovuta all’aggiunta di rumore\nOverhead computazionale per analisi di sensibilità e generazione di rumore\n\n\n\nAddestramento Federato\n\nConsente l’apprendimento collaborativo senza condividere dati grezzi\nI dati rimangono decentralizzati migliorando la sicurezza\nNessuna necessità di elaborazione crittografata\n\n\nOverhead di comunicazione aumentato\nConvergenza del modello potenzialmente più lenta\nCapacità di dispositivi client non uniformi\n\n\n\nElaborazione Multi-Parte Sicura\n\nConsente elaborazione congiunta su dati sensibili\nFornisce garanzie di privacy crittografica\nProtocolli flessibili per varie funzioni\n\n\nOverhead computazionale molto elevato\nComplessità di implementazione\nVincoli algoritmici sulla profondità della funzione\n\n\n\nCrittografia Omomorfica\n\nConsente il calcolo su dati crittografati\nPreviene l’esposizione allo stato intermedio\n\n\nCosti di calcolo estremamente elevati\nImplementazioni crittografiche complesse\nRestrizioni sui tipi di funzione\n\n\n\nGenerazione di Dati Sintetici\n\nConsente la condivisione dei dati senza “fughe”\nAttenua i problemi di scarsità di dati\n\n\nDivario sintetico-reale nelle distribuzioni\nPotenziale per la ricostruzione di dati privati\nBias e problemi di etichettatura",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#conclusione",
    "href": "contents/privacy_security/privacy_security.it.html#conclusione",
    "title": "14  Sicurezza e Privacy",
    "section": "14.9 Conclusione",
    "text": "14.9 Conclusione\nLa sicurezza hardware del machine learning è fondamentale poiché i sistemi ML embedded vengono sempre più implementati in domini critici per la sicurezza come dispositivi medici, controlli industriali e veicoli autonomi. Abbiamo esplorato varie minacce che spaziano da bug hardware, attacchi fisici, canali laterali, rischi della supply chain, ecc. Difese come TEE, Secure Boot, PUF e moduli di sicurezza hardware forniscono una protezione multi-livello su misura per dispositivi embedded con risorse limitate.\nTuttavia, una vigilanza continua è essenziale per tracciare i vettori di attacco emergenti e affrontare potenziali vulnerabilità tramite pratiche di ingegneria sicure durante l’intero ciclo di vita dell’hardware. Man mano che ML e ML embedded si diffondono, il mantenimento di rigorose basi di sicurezza che corrispondano al ritmo accelerato di innovazione del settore rimane imperativo.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.it.html#sec-security-and-privacy-resource",
    "href": "contents/privacy_security/privacy_security.it.html#sec-security-and-privacy-resource",
    "title": "14  Sicurezza e Privacy",
    "section": "14.10 Risorse",
    "text": "14.10 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nSecurity.\nPrivacy.\nMonitoring after Deployment.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 14.1\nVideo 14.2\nVideo 14.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 14.1\nEsercizio 14.2\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sicurezza e Privacy</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html",
    "href": "contents/responsible_ai/responsible_ai.it.html",
    "title": "15  IA Responsabile",
    "section": "",
    "text": "15.1 Introduzione\nI modelli di apprendimento automatico sono sempre più utilizzati per automatizzare le decisioni in ambiti sociali ad alto rischio come sanità, giustizia penale e occupazione. Tuttavia, senza un’attenzione deliberata, questi algoritmi possono perpetuare pregiudizi, violare la privacy o causare altri danni. Ad esempio, un modello di approvazione di prestiti addestrato esclusivamente su dati provenienti da quartieri ad alto reddito potrebbe svantaggiare i richiedenti provenienti da aree a basso reddito. Ciò motiva la necessità di un apprendimento automatico responsabile, ovvero la creazione di modelli equi, responsabili, trasparenti ed etici.\nDiversi principi fondamentali sono alla base di un apprendimento automatico responsabile. L’equità garantisce che i modelli non discriminino in base a genere, razza, età e altri attributi. La spiegabilità consente agli esseri umani di interpretare i comportamenti del modello e migliorare la trasparenza. Le tecniche di robustezza e sicurezza prevengono vulnerabilità come gli esempi avversari. Test e convalide rigorosi aiutano a ridurre le debolezze indesiderate del modello o gli effetti collaterali.\nL’implementazione di un apprendimento automatico responsabile presenta sfide sia tecniche che etiche. Gli sviluppatori devono confrontarsi con la definizione matematica dell’equità, bilanciando obiettivi concorrenti come accuratezza e interpretabilità e assicurando dati di training di qualità. Le organizzazioni devono anche allineare incentivi, politiche e cultura per sostenere l’IA etica.\nQuesto capitolo fornirà gli strumenti per valutare criticamente i sistemi di IA e contribuire allo sviluppo di applicazioni di apprendimento automatico utili ed etiche, coprendo le basi, i metodi e le implicazioni nel mondo reale dell’ML responsabile. I principi dell’ML responsabile discussi sono conoscenze cruciali poiché gli algoritmi mediano più aspetti della società umana.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html#definizione",
    "href": "contents/responsible_ai/responsible_ai.it.html#definizione",
    "title": "15  IA Responsabile",
    "section": "15.2 Definizione",
    "text": "15.2 Definizione\nL’IA responsabile riguarda lo sviluppo di un’IA che abbia un impatto positivo sulla società in base all’etica e ai valori umani. Non esiste una definizione universalmente accettata di “IA responsabile”, ma ecco un riassunto di come viene comunemente descritta. L’IA responsabile si riferisce alla progettazione, allo sviluppo e all’implementazione di sistemi di intelligenza artificiale in modo etico e socialmente utile. L’obiettivo principale è creare un’IA affidabile, imparziale, equa, trasparente, responsabile e sicura. Sebbene non esista una definizione canonica, si ritiene generalmente che l’IA responsabile comprenda principi quali:\n\nEquità: Evitare pregiudizi, discriminazioni e potenziali danni a determinati gruppi o popolazioni\nSpiegabilità: Consentire agli esseri umani di comprendere e interpretare il modo in cui i modelli di IA prendono decisioni\nTrasparenza: Comunicare apertamente come funzionano, sono costruiti e valutati i sistemi di IA\nResponsabilità: Avere processi per determinare responsabilità e obblighi per guasti o impatti negativi dell’IA\nRobustezza: Garantire che i sistemi di IA siano sicuri, affidabili e si comportino come previsto\nPrivacy: Proteggere i dati sensibili degli utenti e rispettare le leggi e l’etica sulla privacy\n\nMettere in pratica questi principi implica tecniche tecniche, politiche aziendali, quadri di governance e filosofia morale. Sono inoltre in corso dibattiti sulla definizione di concetti ambigui come l’equità e sulla determinazione di come bilanciare obiettivi in competizione.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html#principi-e-concetti",
    "href": "contents/responsible_ai/responsible_ai.it.html#principi-e-concetti",
    "title": "15  IA Responsabile",
    "section": "15.3 Principi e Concetti",
    "text": "15.3 Principi e Concetti\n\n15.3.1 Trasparenza e Spiegabilità\nI modelli di apprendimento automatico sono spesso criticati come misteriose “scatole nere”, sistemi opachi in cui non è chiaro come siano arrivati a particolari previsioni o decisioni. Ad esempio, un sistema di intelligenza artificiale chiamato COMPAS utilizzato per valutare il rischio di recidiva criminale negli Stati Uniti si è rivelato razzialmente discriminatorio nei confronti degli imputati neri. Tuttavia, l’opacità dell’algoritmo ha reso difficile comprendere e risolvere il problema. Questa mancanza di trasparenza può nascondere pregiudizi, errori e carenze.\nSpiegare i comportamenti del modello aiuta a generare fiducia da parte del pubblico e degli esperti del settore e consente di identificare i problemi da affrontare. Tecniche di interpretabilità come LIME, valori Shapley e mappe di salienza consentono agli esseri umani di comprendere e convalidare la logica del modello. Anche leggi come il GDPR dell’UE impongono la trasparenza, che richiede la spiegabilità per determinate decisioni automatizzate. Nel complesso, trasparenza e spiegabilità sono pilastri fondamentali dell’intelligenza artificiale responsabile.\n\n\n15.3.2 Equità, Bias [pregiudizi] e Discriminazione\nI modelli di ML addestrati su dati storicamente distorti spesso perpetuano e amplificano tali pregiudizi. È stato dimostrato che gli algoritmi sanitari svantaggiano i pazienti neri sottostimandone le esigenze (Obermeyer et al. 2019). Il riconoscimento facciale deve essere più accurato per le donne e le persone di colore. Tale discriminazione algoritmica può avere un impatto negativo profondo sulla vita delle persone.\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, e Sendhil Mullainathan. 2019. «Dissecting racial bias in an algorithm used to manage the health of populations». Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\nEsistono anche diverse prospettive filosofiche sull’equità, ad esempio, è più giusto trattare tutti gli individui allo stesso modo o cercare di ottenere risultati uguali per i gruppi? Garantire l’equità richiede di rilevare e mitigare in modo proattivo i pregiudizi nei dati e nei modelli. Tuttavia, raggiungere l’equità perfetta è tremendamente difficile a causa di definizioni matematiche e prospettive etiche contrastanti. Tuttavia, promuovere l’equità algoritmica e la non discriminazione è una responsabilità fondamentale nello sviluppo dell’intelligenza artificiale.\n\n\n15.3.3 Privacy e Governance dei Dati\nMantenere la privacy degli individui è un obbligo etico e un requisito legale per le organizzazioni che implementano sistemi di intelligenza artificiale. Regolamentazioni come il GDPR dell’UE impongono protezioni e diritti sulla privacy dei dati, come la possibilità di accedere ed eliminare i propri dati.\nTuttavia, massimizzare l’utilità e l’accuratezza dei dati per i modelli di addestramento può entrare in conflitto con la tutela della privacy: la modellazione della progressione della malattia potrebbe trarre vantaggio dall’accesso ai genomi completi dei pazienti, ma la condivisione di tali dati viola ampiamente la privacy.\nUna governance dei dati responsabile implica l’anonimizzazione attenta dei dati, il controllo dell’accesso tramite crittografia, l’ottenimento del consenso informato degli interessati e la raccolta dei dati minimi necessari. Rispettare la privacy è difficile ma fondamentale man mano che le capacità e l’adozione dell’intelligenza artificiale si espandono.\n\n\n15.3.4 Sicurezza e Robustezza\nMettere in funzione i sistemi di intelligenza artificiale nel mondo reale richiede di garantire che siano sicuri, affidabili e robusti, soprattutto per gli scenari di interazione umana. Le auto a guida autonoma di Uber e Tesla sono state coinvolte in incidenti mortali a causa di comportamenti non sicuri.\nGli attacchi avversari che alterano in modo sottile i dati di input possono anche ingannare i modelli ML e causare guasti pericolosi se i sistemi non sono resistenti. I deepfake rappresentano un’altra area di minaccia emergente.\nVideo 15.1 è un video deepfake di Barack Obama che è diventato virale qualche anno fa.\n\n\n\n\n\n\nVideo 15.1: Fake Obama\n\n\n\n\n\n\nLa promozione della sicurezza richiede test approfonditi, analisi dei rischi, supervisione umana e progettazione di sistemi che combinano più modelli deboli per evitare singoli punti di errore. Rigorosi meccanismi di sicurezza sono essenziali per l’implementazione responsabile di un’IA efficiente.\n\n\n15.3.5 Responsabilità e Governance\nQuando i sistemi di IA alla fine falliscono o producono risultati dannosi, devono esistere meccanismi per affrontare i problemi risultanti, risarcire le parti interessate e assegnare la responsabilità. Sia le politiche di responsabilità aziendale che le normative governative sono indispensabili per una governance responsabile dell’IA. Ad esempio, l’Artificial Intelligence Video Interview Act dell’Illinois richiede alle aziende di divulgare e ottenere il consenso per l’analisi video dell’IA, promuovendo la responsabilità.\nSenza una chiara responsabilità, anche i danni causati involontariamente potrebbero rimanere irrisolti, alimentando ulteriormente l’indignazione e la sfiducia pubblica. I comitati di vigilanza, le valutazioni di impatto, i processi di risoluzione dei reclami e gli audit indipendenti promuovono lo sviluppo e l’implementazione responsabili.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html#cloud-edge-e-tiny-ml",
    "href": "contents/responsible_ai/responsible_ai.it.html#cloud-edge-e-tiny-ml",
    "title": "15  IA Responsabile",
    "section": "15.4 Cloud, Edge e Tiny ML",
    "text": "15.4 Cloud, Edge e Tiny ML\nSebbene questi principi siano ampiamente applicabili a tutti i sistemi di intelligenza artificiale, alcune considerazioni di IA responsabile sono uniche o pronunciate quando si ha a che fare con l’apprendimento automatico su dispositivi embedded rispetto alla modellazione tradizionale basata su server. Pertanto, presentiamo una tassonomia di alto livello che confronta le considerazioni di intelligenza artificiale responsabile nei sistemi cloud, edge e TinyML.\n\n15.4.1 Riepilogo\nTabella 15.1 riassume come i principi di intelligenza artificiale responsabile si manifestino in modo diverso nelle architetture cloud, edge e TinyML e come le considerazioni fondamentali si leghino alle loro capacità e limitazioni uniche. I vincoli e i compromessi di ogni ambiente modellano il modo in cui affrontiamo la trasparenza, la responsabilità, la governance e altri pilastri dell’intelligenza artificiale responsabile.\n\n\n\nTabella 15.1: Confronto dei principi chiave di Cloud ML, Edge ML e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nPrincipio\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nSpiegabilità\nModelli complessi supportati\nLeggero richiesto\nLimiti severi\n\n\nEquità\nDati ampi disponibili\nDistorsioni sul dispositivo\nEtichette dati limitate\n\n\nPrivacy\nVulnerabilità dei dati nel cloud\nDati più sensibili\nDati dispersi\n\n\nSicurezza\nMinacce di hacking\nInterazione nel mondo reale\nDispositivi autonomi\n\n\nResponsabilità\nPolitiche aziendali\nProblemi della catena di fornitura\nTracciamento dei componenti\n\n\nGovernance\nSupervisione esterna fattibile\nAutogoverno necessario\nVincoli del protocollo\n\n\n\n\n\n\n\n\n15.4.2 Spiegabilità\nPer l’apprendimento automatico basato su cloud, le tecniche di spiegabilità possono sfruttare risorse di elaborazione significative, consentendo metodi complessi come valori SHAP o approcci basati sul campionamento per interpretare i comportamenti del modello. Ad esempio, il toolkit InterpretML di Microsoft fornisce tecniche di spiegabilità su misura per gli ambienti cloud.\nTuttavia, l’edge ML opera su dispositivi con risorse limitate, richiedendo metodi di spiegabilità più leggeri che possono essere eseguiti localmente senza latenza eccessiva. Tecniche come LIME (Ribeiro, Singh, e Guestrin 2016) approssimano le spiegazioni del modello utilizzando modelli lineari o alberi decisionali per evitare calcoli costosi, il che le rende ideali per dispositivi con risorse limitate. Tuttavia, LIME richiede l’addestramento di centinaia o persino migliaia di modelli per generare buone spiegazioni, il che è spesso irrealizzabile dati i vincoli dell’edge computing. Al contrario, i metodi basati sulla salienza sono spesso molto più rapidi nella pratica, richiedendo solo un singolo passaggio in avanti attraverso la rete per stimare l’importanza delle funzionalità. Questa maggiore efficienza rende tali metodi più adatti ai dispositivi edge con risorse di elaborazione limitate, in cui le spiegazioni a bassa latenza sono fondamentali.\nDate le ridotte capacità hardware, i sistemi embedded pongono le sfide più significative per la spiegabilità. Modelli più compatti e dati limitati semplificano la trasparenza intrinseca del modello. Spiegare le decisioni potrebbe non essere fattibile su microcontrollori di grandi dimensioni e con potenza ottimizzata. Il programma Transparent Computing della DARPA cerca di sviluppare una spiegabilità con costi di gestione estremamente bassi, in particolare per i dispositivi TinyML come sensori e dispositivi indossabili.\n\n\n15.4.3 Correttezza\nPer il machine learning nel cloud, vasti set di dati e potenza di calcolo consentono di rilevare pregiudizi su grandi popolazioni eterogenee e di mitigarli tramite tecniche come la riponderazione dei campioni di dati. Tuttavia, i pregiudizi possono emergere dagli ampi dati comportamentali utilizzati per addestrare i modelli cloud. Il framework Fairness Flow di Amazon aiuta a valutare l’equità del ML cloud.\nEdge ML si basa su dati limitati sul dispositivo, rendendo più difficile l’analisi dei pregiudizi tra gruppi diversi. Tuttavia, i dispositivi edge interagiscono strettamente con gli individui, offrendo un’opportunità di adattamento locale per l’equità. Federated Learning di Google distribuisce l’addestramento del modello tra i dispositivi per incorporare le differenze individuali.\nTinyML pone sfide uniche per l’equità con hardware specializzato altamente disperso e dati di addestramento minimi. I test sui pregiudizi sono difficili su dispositivi diversi. La raccolta di dati rappresentativi da molti dispositivi per mitigare i pregiudizi presenta ostacoli di scala e privacy. Gli sforzi di Assured Neuro Symbolic Learning and Reasoning (ANSR) di DARPA sono orientati allo sviluppo di tecniche di equità dati i vincoli hardware estremi.\n\n\n15.4.4 Sicurezza\nI principali rischi per la sicurezza del cloud ML includono hacking dei modelli, avvelenamento dei dati e malware che interrompono i servizi cloud. Le tecniche di robustezza come l’addestramento avversario, il rilevamento delle anomalie e i modelli diversificati mirano a rafforzare il cloud ML contro gli attacchi. La ridondanza può aiutare a prevenire singoli punti di errore.\nEdge ML e TinyML interagiscono con il mondo fisico, quindi l’affidabilità e la convalida della sicurezza sono fondamentali. Piattaforme di test rigorose come Foretellix generano sinteticamente scenari edge per convalidare la sicurezza. La sicurezza di TinyML è amplificata da dispositivi autonomi con supervisione limitata. La sicurezza di TinyML spesso si basa sul coordinamento collettivo: sciami di droni mantengono la sicurezza tramite ridondanza. Anche le barriere di controllo fisiche limitano i comportamenti non sicuri dei dispositivi TinyML.\nIn sintesi, la sicurezza è fondamentale ma si manifesta in modo diverso in ogni dominio. Cloud ML protegge dall’hacking, edge ML interagisce fisicamente, quindi l’affidabilità è fondamentale e TinyML sfrutta il coordinamento distribuito per la sicurezza. La comprensione delle sfumature guida le tecniche di sicurezza appropriate.\n\n\n15.4.5 Responsabilità\nLa responsabilità di Cloud ML si concentra su pratiche aziendali come comitati AI responsabili, carte etiche e processi per affrontare incidenti dannosi. Audit di terze parti e supervisione governativa esterna promuovono la responsabilità di Cloud ML.\nLa responsabilità di Edge ML è più complessa con dispositivi distribuiti e frammentazione della supply chain. Le aziende sono responsabili dei dispositivi, ma i componenti provengono da vari fornitori. Gli standard di settore aiutano a coordinare la responsabilità di Edge ML tra le parti interessate.\nCon TinyML, i meccanismi di responsabilità devono essere tracciati attraverso lunghe e complesse supply chain di circuiti integrati, sensori e altro hardware. Gli schemi di certificazione TinyML aiutano a tracciare la provenienza dei componenti. Le associazioni di categoria dovrebbero idealmente promuovere la responsabilità condivisa per TinyML etico.\n\n\n15.4.6 Governance\nLe organizzazioni istituiscono una governance interna per il cloud ML, come comitati etici, audit e gestione del rischio del modello. Ma anche la governance esterna supervisiona il cloud ML, come le normative su pregiudizi e trasparenza come AI Bill of Rights, General Data Protection Regulation (GDPR) e California Consumer Protection Act (CCPA). L’audit di terze parti supporta la governance del cloud ML.\nEdge ML è più decentralizzato e richiede un’autogovernance responsabile da parte di sviluppatori e aziende che distribuiscono modelli localmente. Le associazioni di settore coordinano la governance tra i fornitori di edge ML e il software aperto aiuta ad allineare gli incentivi per l’edge ML etico.\nL’estrema decentralizzazione e complessità rendono la governance esterna impraticabile con TinyML. TinyML si basa su protocolli e standard per l’autogovernance integrati nella progettazione del modello e nell’hardware. La crittografia consente l’affidabilità dimostrabile dei dispositivi TinyML.\n\n\n15.4.7 Privacy\nPer il cloud ML, grandi quantità di dati utente sono concentrate nel cloud, creando rischi di esposizione tramite violazioni. Le tecniche di privacy differenziali aggiungono rumore ai dati cloud per preservare la privacy. Rigidi controlli di accesso e crittografia proteggono i dati cloud a riposo e in transito.\nEdge ML sposta l’elaborazione dei dati sui dispositivi utente, riducendo la raccolta di dati aggregati ma aumentando la potenziale sensibilità poiché i dati personali risiedono sul dispositivo. Apple utilizza ML on-device e privacy differenziale per addestrare modelli riducendo al minimo la condivisione dei dati. L’anonimizzazione dei dati e le enclave sicure proteggono i dati on-device.\nTinyML distribuisce i dati su molti dispositivi con risorse limitate, rendendo improbabili le violazioni centralizzate e rendendo difficile l’anonimizzazione su larga scala. La minimizzazione dei dati e l’utilizzo di dispositivi edge come intermediari aiutano la privacy di TinyML.\nQuindi, mentre il cloud ML deve proteggere dati centralizzati espansivi, l’edge ML protegge i dati sensibili on-device e TinyML mira a una condivisione minima dei dati distribuiti a causa dei vincoli. Mentre la privacy è fondamentale in tutto, le tecniche devono adattarsi all’ambiente. La comprensione delle sfumature consente di selezionare approcci appropriati per la tutela della privacy.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html#aspetti-tecnici",
    "href": "contents/responsible_ai/responsible_ai.it.html#aspetti-tecnici",
    "title": "15  IA Responsabile",
    "section": "15.5 Aspetti Tecnici",
    "text": "15.5 Aspetti Tecnici\n\n15.5.1 Rilevamento e Mitigazione dei Pregiudizi\nUn’ampia mole di lavoro ha dimostrato che i modelli di apprendimento automatico possono presentare pregiudizi, da persone con prestazioni inferiori di una certa identità a decisioni che limitano l’accesso dei gruppi a risorse importanti (Buolamwini e Gebru 2018).\nGarantire un trattamento equo e giusto per tutti i gruppi interessati dai sistemi di apprendimento automatico è fondamentale, poiché questi modelli hanno un impatto sempre maggiore sulla vita delle persone in settori come prestiti, assistenza sanitaria e giustizia penale. In genere valutiamo l’equità del modello considerando “attributi di sottogruppo” non correlati all’attività di previsione che catturano identità come razza, genere o religione. Ad esempio, in un modello di previsione di inadempienza del prestito, i sottogruppi potrebbero includere razza, genere o religione. Quando i modelli vengono addestrati ingenuamente per massimizzare l’accuratezza, spesso ignorano le prestazioni del sottogruppo. Tuttavia, ciò può avere un impatto negativo sulle comunità emarginate.\nPer illustrare, si immagini un modello che prevede il rimborso del prestito in cui i più (+) rappresentano il rimborso e i cerchi (O) rappresentano l’inadempienza, come mostrato in Figura 15.1. L’accuratezza ottimale sarebbe classificare correttamente tutto il Gruppo A, classificando erroneamente alcuni dei richiedenti affidabili del Gruppo B come inadempienti. Se le classificazioni positive consentono l’accesso ai prestiti, il Gruppo A riceverebbe molti più prestiti, il che si tradurrebbe naturalmente in un risultato distorto.\n\n\n\n\n\n\nFigura 15.1: Equità e accuratezza.\n\n\n\nIn alternativa, correggere i pregiudizi contro il Gruppo B aumenterebbe probabilmente i “falsi positivi” e ridurrebbe l’accuratezza per il Gruppo A. Oppure, potremmo addestrare modelli separati focalizzati sulla massimizzazione dei veri positivi per ciascun gruppo. Tuttavia, ciò richiederebbe l’utilizzo esplicito di attributi sensibili come la razza nel processo decisionale.\nCome vediamo, ci sono tensioni intrinseche attorno a priorità come l’accuratezza rispetto all’equità del sottogruppo e se tenere conto esplicitamente delle classi protette. Le persone ragionevoli possono non essere d’accordo sui compromessi appropriati. I vincoli sui costi e le opzioni di implementazione complicano ulteriormente le cose. Nel complesso, garantire l’uso equo ed etico dell’apprendimento automatico implica la navigazione di queste sfide complesse.\nPertanto, la letteratura sull’equità ha proposto tre principali metriche di equità per quantificare quanto sia equo un modello su un set di dati (Hardt, Price, e Srebro 2016). Dato un modello h e un set di dati D costituito da campioni (x,y,s), dove x sono le caratteristiche dei dati, y è l’etichetta e s è l’attributo del sottogruppo, e supponiamo che ci siano semplicemente due sottogruppi a e b, possiamo definire quanto segue..\n\nParità Demografica chiede quanto è accurato un modello per ogni sottogruppo. In altre parole, P(h(X) = Y S = a) = P(h(X) = Y S = b)\nQuote Equalizzate chiede quanto è preciso un modello su campioni positivi e negativi per ogni sottogruppo. P(h(X) = y S = a, Y = y) = P(h(X) = y S = b, Y = y)\nUguaglianza di Opportunità è un caso speciale di probabilità equalizzate che chiede solo quanto è preciso un modello su campioni positivi. Ciò è rilevante in casi come l’allocazione delle risorse, in cui ci preoccupiamo di come le etichette positive (vale a dire, allocate in base alle risorse) siano distribuite tra i gruppi. Ad esempio, ci preoccupiamo che una proporzione uguale di prestiti venga concessa sia agli uomini che alle donne. P(h(X) = 1 S = a, Y = 1) = P(h(X) = 1 S = b, Y = 1)\n\nNota: Queste definizioni spesso adottano una visione ristretta quando si considerano confronti binari tra due sottogruppi. Un altro filone di ricerca di apprendimento automatico equo incentrato su multi-calibrazione e multi-accuratezza considera le interazioni tra un numero arbitrario di identità, riconoscendo l’intersezionalità intrinseca delle identità individuali nel mondo reale (Hébert-Johnson et al. 2018).\n\nHébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, e Guy N. Rothblum. 2018. «Multicalibration: Calibration for the (Computationally-Identifiable) Masses». In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, a cura di Jennifer G. Dy e Andreas Krause, 80:1944–53. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\nIl Contesto è Importante\nPrima di prendere qualsiasi decisione tecnica per sviluppare un algoritmo ML imparziale, dobbiamo comprendere il contesto che circonda il nostro modello. Ecco alcune delle domande chiave su cui riflettere:\n\nPer chi prenderà decisioni questo modello?\nChi è rappresentato nei dati di training?\nChi è rappresentato e chi manca al tavolo di ingegneri, progettisti e manager?\nChe tipo di impatti duraturi potrebbe avere questo modello? Ad esempio, avrà un impatto sulla sicurezza finanziaria di un individuo su scala generazionale, come la determinazione delle ammissioni al college o l’ammissione di un prestito per una casa?\nQuali pregiudizi storici e sistematici sono presenti in questo contesto e sono presenti nei dati di training da cui il modello generalizzerà?\n\nComprendere il background sociale, etico e storico di un sistema è fondamentale per prevenire danni e dovrebbe informare le decisioni durante tutto il ciclo di sviluppo del modello. Dopo aver compreso il contesto, si possono prendere varie decisioni tecniche per rimuovere i pregiudizi. Innanzitutto, si deve decidere quale metrica di equità è il criterio più appropriato per l’ottimizzazione. Successivamente, ci sono generalmente tre aree principali in cui si può intervenire per eliminare i pregiudizi di un sistema ML.\nInnanzitutto, la preelaborazione è quando si bilancia un set di dati per garantire una rappresentazione equa o addirittura si aumenta il peso su determinati gruppi sottorappresentati per garantire che il modello funzioni bene. In secondo luogo, nell’elaborazione si tenta di modificare il processo di training di un sistema ML per garantire che dia priorità all’equità. Questo può essere semplice come aggiungere un regolarizzatore di equità (Lowy et al. 2021) al training di un insieme di modelli e campionarli in un modo specifico (Agarwal et al. 2018).\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, e Ahmad Beirami. 2021. «Fermi: Fair empirical risk minimization via exponential Rényi mutual information».\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, e Hanna M. Wallach. 2018. «A Reductions Approach to Fair Classification». In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, a cura di Jennifer G. Dy e Andreas Krause, 80:60–69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, e Flavio Calmon. 2022. «Beyond Adult and COMPAS: Fair multi-class prediction via information projection». Adv. Neur. In. 35: 38747–60.\n\nHardt, Moritz, Eric Price, e Nati Srebro. 2016. «Equality of Opportunity in Supervised Learning». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 3315–23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\nInfine, la post-elaborazione degrada un modello dopo il fatto, prendendo un modello addestrato e modificandone le previsioni in un modo specifico per garantire che l’equità venga preservata (Alghamdi et al. 2022; Hardt, Price, e Srebro 2016). La post-elaborazione si basa sulle fasi di pre-elaborazione e in-elaborazione offrendo un’altra opportunità per affrontare i problemi di bias [pregiudizi] e equità nel modello dopo che è già stato addestrato.\nIl processo in tre fasi di pre-elaborazione, in-elaborazione e post-elaborazione fornisce un framework per intervenire in diverse fasi dello sviluppo del modello per mitigare i problemi relativi a pregiudizi ed equità. Mentre la pre-elaborazione e l’in-elaborazione si concentrano sui dati e sul training, la post-elaborazione consente di apportare modifiche dopo che il modello è stato completamente formato. Insieme, questi tre approcci offrono molteplici opportunità per rilevare e rimuovere pregiudizi ingiusti.\n\n\nDistribuzione Ponderata\nL’ampiezza delle definizioni di equità e degli interventi di debiasing esistenti sottolinea la necessità di una valutazione ponderata prima di distribuire sistemi ML. Come ricercatori e sviluppatori ML, lo sviluppo responsabile del modello richiede di istruirci in modo proattivo sul contesto del mondo reale, consultare esperti del settore e utenti finali e concentrarci sulla prevenzione dei danni.\nInvece di vedere le considerazioni sull’equità come una casella da spuntare, dobbiamo impegnarci profondamente con le implicazioni sociali uniche e i compromessi etici attorno a ogni modello che costruiamo. Ogni scelta tecnica su set di dati, architetture di modelli, metriche di valutazione e vincoli di distribuzione incorpora valori. Ampliando la nostra prospettiva oltre le metriche tecniche ristrette, valutando attentamente i compromessi e ascoltando le voci interessate, possiamo lavorare per garantire che i nostri sistemi espandano le opportunità anziché codificare i pregiudizi.\nLa strada da seguire non risiede in una checklist di “debiasing” arbitraria, ma nell’impegno a comprendere e sostenere la nostra responsabilità etica a ogni passo. Questo impegno inizia con l’educazione proattiva di noi stessi e la consultazione degli altri, piuttosto che limitarci a seguire i movimenti di una checklist di equità. Richiede un profondo impegno nei compromessi etici nelle nostre scelte tecniche, la valutazione degli impatti su diversi gruppi e l’ascolto delle voci maggiormente interessate.\nIn definitiva, i sistemi di intelligenza artificiale responsabili ed etici non derivano dal “debiasing” delle caselle di controllo, ma dal rispetto del nostro dovere di valutare i danni, ampliare le prospettive, comprendere i compromessi e garantire di offrire opportunità a tutti i gruppi. Questa responsabilità etica dovrebbe guidare ogni passo.\nIl collegamento tra i paragrafi è che il primo stabilisce la necessità di una valutazione ponderata delle questioni di equità piuttosto che di un approccio basato su caselle di controllo. Il secondo paragrafo si sofferma poi su come si presenta in pratica questa valutazione ponderata, ovvero impegnarsi con i compromessi, valutare gli impatti sui gruppi e ascoltare le voci interessate. Infine, l’ultimo paragrafo fa riferimento all’evitare una “checklist di debiasing arbitraria” e impegnarsi nella responsabilità etica attraverso la valutazione, la comprensione dei compromessi e l’offerta di opportunità.\n\n\n\n15.5.2 Preservare la Privacy\nIncidenti recenti hanno fatto luce su come i modelli di intelligenza artificiale possano memorizzare dati sensibili degli utenti in modi che violano la privacy. Ippolito et al. (2023) dimostra che i modelli linguistici tendono a memorizzare i dati di addestramento e possono persino riprodurre esempi di addestramento specifici. Questi rischi sono amplificati con sistemi ML personalizzati distribuiti in ambienti intimi come case o dispositivi indossabili. Prendiamo in considerazione uno smart speaker che usa le nostre conversazioni per migliorare la qualità del servizio per gli utenti che apprezzano tali miglioramenti. Sebbene potenzialmente vantaggioso, questo crea anche rischi per la privacy, poiché i malintenzionati potrebbero tentare di estrarre ciò che lo speaker “ricorda”. Il problema si estende oltre i modelli linguistici. Figura 15.2 mostra come i modelli di diffusione possono memorizzare e generare esempi di training individuali (Nicolas Carlini et al. 2023), dimostrando ulteriormente i potenziali rischi per la privacy associati ai sistemi di intelligenza artificiale che apprendono dai dati degli utenti.\n\nCarlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, e Eric Wallace. 2023. «Extracting training data from diffusion models». In 32nd USENIX Security Symposium (USENIX Security 23), 5253–70.\n\n\n\n\n\n\nFigura 15.2: Modelli di diffusione che memorizzano campioni dai dati di training. Fonte: Ippolito et al. (2023).\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, e Nicholas Carlini. 2023. «Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy». In Proceedings of the 16th International Natural Language Generation Conference, 5253–70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nMan mano che l’intelligenza artificiale si integra sempre di più nella nostra vita quotidiana, sta diventando sempre più importante che le preoccupazioni sulla privacy e le solide misure di sicurezza per proteggere le informazioni degli utenti siano sviluppate con occhio critico. La sfida sta nel bilanciare i vantaggi dell’intelligenza artificiale personalizzata con il diritto fondamentale alla privacy.\nGli avversari possono usare queste capacità di memorizzazione e addestrare modelli per rilevare se specifici dati di addestramento hanno influenzato un modello target. Ad esempio, gli attacchi di inferenza di appartenenza addestrano un modello secondario che impara a rilevare un cambiamento negli output del modello target quando si effettuano inferenze sui dati su cui è stato addestrato rispetto a quelli su cui non è stato addestrato (Shokri et al. 2017).\n\nShokri, Reza, Marco Stronati, Congzheng Song, e Vitaly Shmatikov. 2017. «Membership Inference Attacks Against Machine Learning Models». In 2017 IEEE Symposium on Security and Privacy (SP), 3–18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\nI dispositivi ML sono particolarmente vulnerabili perché sono spesso personalizzati sui dati degli utenti e vengono distribuiti in contesti ancora più intimi come la casa. Le tecniche di apprendimento automatico privato si sono evolute per stabilire misure di sicurezza contro gli avversari, come menzionato nel capitolo Sicurezza e Privacy per combattere questi problemi di privacy. Metodi come la privacy differenziale aggiungono rumore matematico durante l’addestramento per oscurare l’influenza dei singoli punti dati sul modello. Tecniche popolari come DP-SGD (Abadi et al. 2016) tagliano anche i gradienti per limitare ciò che il modello trapelerà sui dati. Tuttavia, gli utenti dovrebbero anche avere la possibilità di eliminare l’impatto dei propri dati in un secondo momento.\n\n\n15.5.3 Machine Unlearning\nCon dispositivi ML personalizzati per singoli utenti e poi distribuiti su edge remoti senza connettività, sorge una sfida: come possono i modelli “dimenticare” in modo reattivo i dati dopo la distribuzione? Se gli utenti richiedono che i loro dati vengano rimossi da un modello personalizzato, la mancanza di connettività rende impossibile la riqualificazione. Pertanto, un’efficiente dimenticanza dei dati sul dispositivo è necessaria, ma pone degli ostacoli.\nGli approcci iniziali di disapprendimento hanno incontrato delle limitazioni in questo contesto. Date le limitazioni delle risorse, recuperare modelli da zero sul dispositivo per dimenticare i dati si rivela inefficiente o addirittura impossibile. La riqualificazione completa richiede anche di conservare tutti i dati di training originali sul dispositivo, il che comporta dei rischi per la sicurezza e la privacy. Le comuni tecniche di “machine unlearning” [disapprendimento automatico] (Bourtoule et al. 2021) per sistemi ML embedded remoti non riescono a consentire la rimozione dei dati reattiva e sicura.\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, e Nicolas Papernot. 2021. «Machine Unlearning». In 2021 IEEE Symposium on Security and Privacy (SP), 141–59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\nTuttavia, metodi più recenti sembrano promettenti nel modificare i modelli per dimenticare approssimativamente i dati [?] senza una riqualificazione completa. Sebbene la perdita di accuratezza derivante dall’evitare ricostruzioni complete sia modesta, garantire la privacy dei dati dovrebbe comunque essere la priorità quando si gestiscono eticamente le informazioni sensibili degli utenti. Anche una minima esposizione a dati privati può violare la fiducia degli utenti. Poiché i sistemi ML diventano profondamente personalizzati, efficienza e privacy devono essere abilitate fin dall’inizio, non ripensamenti.\nLe recenti discussioni sulle politiche, che includono European Union’s General Data, Protection Regulation (GDPR), il California Consumer Privacy Act (CCPA), l’Act on the Protection of Personal Information (APPI) e il Consumer Privacy Protection Act (CPPA) proposto dal Canada, richiedono l’eliminazione delle informazioni private. Queste politiche, insieme a incidenti di intelligenza artificiale come la memorizzazione dei dati degli artisti da parte di Stable Diffusion, hanno sottolineato la necessità etica degli utenti di eliminare i propri dati dai modelli dopo l’addestramento.\nIl diritto di rimuovere i dati nasce da preoccupazioni sulla privacy relative alle aziende o agli avversari che abusano delle informazioni sensibili degli utenti. L’unlearning automatico si riferisce alla rimozione dell’influenza di punti specifici da un modello già addestrato. Ingenuamente, ciò comporta una riqualificazione completa senza i dati eliminati. Tuttavia, i vincoli di connettività spesso rendono la riqualificazione non fattibile per i sistemi ML personalizzati e distribuiti su edge remoti. Se uno smart speaker impara da conversazioni domestiche private, è importante mantenere l’accesso per eliminare tali dati.\nSebbene limitati, i metodi si stanno evolvendo per consentire approssimazioni efficienti della riqualificazione per l’unlearning. Modificando il tempo di inferenza dei modelli, possono imitare i dati “dimenticati” senza accesso completo ai dati di addestramento. Tuttavia, la maggior parte delle tecniche attuali è limitata a modelli semplici, ha ancora costi di risorse e scambia una certa accuratezza. Sebbene i metodi si stiano evolvendo, consentire una rimozione efficiente dei dati e rispettare la privacy degli utenti rimane fondamentale per una distribuzione TinyML responsabile.\n\n\n15.5.4 Esempi Avversari e Robustezza\nI modelli di apprendimento automatico, in particolare le reti neurali profonde, hanno un tallone d’Achille ben documentato: spesso si rompono quando vengono apportate anche piccole perturbazioni ai loro input (Szegedy et al. 2014). Questa sorprendente fragilità evidenzia un importante divario di robustezza che minaccia l’implementazione nel mondo reale in domini ad alto rischio. Apre anche la porta ad attacchi avversari progettati per ingannare deliberatamente i modelli.\nI modelli di apprendimento automatico possono mostrare una sorprendente fragilità: piccole modifiche agli input possono causare malfunzionamenti scioccanti, anche nelle reti neurali profonde all’avanguardia (Szegedy et al. 2014). Questa imprevedibilità sui dati fuori campione sottolinea le lacune nella generalizzazione e nella robustezza del modello. Data la crescente ubiquità dell’apprendimento automatico, consente anche minacce avversarie che sfruttano i punti ciechi dei modelli.\nLe reti neurali profonde dimostrano una doppia natura quasi paradossale: competenza umana nelle distribuzioni di training abbinata a un’estrema fragilità alle piccole perturbazioni di input (Szegedy et al. 2014). Questa lacuna di vulnerabilità avversaria ne evidenzia altre nelle procedure ML standard e minacce all’affidabilità nel mondo reale. Allo stesso tempo, può essere sfruttata: gli aggressori possono trovare punti di rottura del modello che gli umani non percepirebbero.\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, e Rob Fergus. 2014. «Intriguing properties of neural networks». In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, a cura di Yoshua Bengio e Yann LeCun. http://arxiv.org/abs/1312.6199.\nFigura 15.3 include un esempio di una piccola perturbazione insignificante che modifica una previsione del modello. Questa fragilità ha impatti nel mondo reale: la mancanza di robustezza mina la fiducia nell’implementazione di modelli per applicazioni ad alto rischio come auto a guida autonoma o diagnosi mediche. Inoltre, la vulnerabilità porta a minacce alla sicurezza: gli aggressori possono creare deliberatamente esempi avversari che sono percettivamente indistinguibili dai dati normali ma causano errori del modello.\n\n\n\n\n\n\nFigura 15.3: Effetto della perturbazione sulla previsione. Fonte: Microsoft.\n\n\n\nAd esempio, lavori passati mostrano attacchi riusciti che ingannano i modelli per attività come il rilevamento NSFW (Bhagoji et al. 2018), il blocco degli annunci (Tramèr et al. 2019) e il riconoscimento vocale (Nicholas Carlini et al. 2016). Sebbene gli errori in questi domini rappresentino già dei rischi per la sicurezza, il problema si estende oltre la sicurezza IT. Di recente, la robustezza avversaria è stata proposta come metrica di prestazioni aggiuntiva approssimando il comportamento del caso peggiore.\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, e Dawn Song. 2018. «Practical black-box attacks on deep neural networks using efficient query mechanisms». In Proceedings of the European conference on computer vision (ECCV), 154–69.\n\nTramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, e Dan Boneh. 2019. «AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning». In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, 2005–21. ACM. https://doi.org/10.1145/3319535.3354222.\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, e Wenchao Zhou. 2016. «Hidden voice commands». In 25th USENIX security symposium (USENIX security 16), 513–30.\nLa sorprendente fragilità del modello evidenziata sopra mette in dubbio l’affidabilità nel mondo reale e apre la porta alla manipolazione avversaria. Questa crescente vulnerabilità sottolinea diverse esigenze. In primo luogo, le valutazioni della robustezza morale sono essenziali per quantificare le vulnerabilità del modello prima dell’implementazione. L’approssimazione del comportamento del caso peggiore fa emergere punti ciechi.\nIn secondo luogo, devono essere sviluppate difese efficaci in tutti i domini per colmare queste lacune di robustezza. Con la sicurezza in gioco, gli sviluppatori non possono ignorare la minaccia di attacchi che sfruttano le debolezze del modello. Inoltre, non possiamo permetterci guasti indotti dalla fragilità per applicazioni critiche per la sicurezza come veicoli a guida autonoma e diagnosi mediche. Sono in gioco delle vite.\nInfine, la comunità di ricerca continua a mobilitarsi rapidamente in risposta. L’interesse per l’apprendimento automatico avversario è esploso poiché gli attacchi rivelano la necessità di colmare il divario di robustezza tra dati sintetici e dati del mondo reale. Le conferenze ora comunemente presentano difese per proteggere e stabilizzare i modelli. La comunità riconosce che la fragilità del modello è un problema critico che deve essere affrontato tramite test di robustezza, sviluppo di difese e ricerca continua. Evidenziando i punti ciechi e rispondendo con difese basate su principi, possiamo lavorare per garantire affidabilità e sicurezza per i sistemi di apprendimento automatico, specialmente in domini ad alto rischio.\n\n\n15.5.5 Creazione di Modelli Interpretabili\nPoiché i modelli vengono distribuiti più frequentemente in contesti ad alto rischio, professionisti, sviluppatori, utenti finali a valle e una regolamentazione crescente hanno evidenziato la necessità di spiegabilità nell’apprendimento automatico. L’obiettivo di molti metodi di interpretabilità e spiegabilità è fornire ai professionisti maggiori informazioni sul comportamento complessivo dei modelli o sul comportamento dato un input specifico. Ciò consente agli utenti di decidere se l’output o la previsione di un modello sono affidabili o meno.\nTale analisi può aiutare gli sviluppatori a eseguire il debug dei modelli e migliorare le prestazioni evidenziando distorsioni, correlazioni spurie e modalità di errore dei modelli. Nei casi in cui i modelli possono superare le prestazioni umane in un’attività, l’interpretabilità può aiutare utenti e ricercatori a comprendere meglio le relazioni nei loro dati e modelli precedentemente sconosciuti.\nEsistono molte classi di metodi di spiegabilità/interpretabilità, tra cui la spiegabilità post hoc, l’interpretabilità intrinseca e l’interpretabilità meccanicistica. Questi metodi mirano a rendere più comprensibili i modelli di apprendimento automatico complessi e a garantire che gli utenti possano fidarsi delle previsioni del modello, soprattutto in contesti critici. Fornendo trasparenza nel comportamento del modello, le tecniche di spiegabilità sono uno strumento importante per sviluppare sistemi di intelligenza artificiale sicuri, equi e affidabili.\n\nSpiegabilità Post Hoc\nI metodi di spiegabilità “post hoc” in genere spiegano il comportamento di output di un modello black-box su un input specifico. metodi più diffusi includono spiegazioni controfattuali, metodi di attribuzione delle caratteristiche e spiegazioni basate sui concetti.\nSpiegazioni controfattuali, spesso chiamate anche ricorso algoritmico, “Se X non si fosse verificato, Y non si sarebbe verificato” (Wachter, Mittelstadt, e Russell 2017). Ad esempio, si consideri una persona che richiede un prestito bancario la cui richiesta viene respinta da un modello. Potrebbe chiedere alla propria banca un ricorso o come modificare per essere idonea a un prestito. Una spiegazione controfattuale indicherebbe loro quali caratteristiche devono modificare e di quanto, in modo che la previsione del modello cambi.\n\nWachter, Sandra, Brent Mittelstadt, e Chris Russell. 2017. «Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR». SSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, e Dhruv Batra. 2017. «Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization». In 2017 IEEE International Conference on Computer Vision (ICCV), 618–26. IEEE. https://doi.org/10.1109/iccv.2017.74.\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, e Martin Wattenberg. 2017. «Smoothgrad: Removing noise by adding noise». ArXiv preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\nRibeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. «” Why should i trust you?” Explaining the predictions of any classifier». In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135–44.\n\nLundberg, Scott M., e Su-In Lee. 2017. «A Unified Approach to Interpreting Model Predictions». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\nI metodi di attribuzione delle caratteristiche evidenziano le caratteristiche di input che sono importanti o necessarie per una particolare previsione. Per un modello di visione artificiale, ciò significherebbe evidenziare i singoli pixel che hanno contribuito maggiormente all’etichetta prevista dell’immagine. Si noti che questi metodi non spiegano in che modo quei pixel/caratteristiche influenzano la previsione, ma solo che lo fanno. I metodi comuni includono gradienti di input, GradCAM (Selvaraju et al. 2017), SmoothGrad (Smilkov et al. 2017), LIME (Ribeiro, Singh, e Guestrin 2016) e SHAP (Lundberg e Lee 2017).\nFornendo esempi di modifiche alle caratteristiche di input che altererebbero una previsione (controfattuali) o indicando le caratteristiche più influenti per una data previsione (attribuzione), queste tecniche di spiegazione post hoc fanno luce sul comportamento del modello per input individuali. Questa trasparenza granulare aiuta gli utenti a determinare se possono fidarsi e agire su output di modelli specifici.\nLe spiegazioni basate sui concetti mirano a spiegare il comportamento del modello e gli output utilizzando un set predefinito di concetti semantici (ad esempio, il modello riconosce la classe di scena “camera da letto” in base alla presenza dei concetti “letto” e “cuscino”). Lavori recenti mostrano che gli utenti spesso preferiscono queste spiegazioni a quelle basate sull’attribuzione e sugli esempi perché “assomigliano al ragionamento e alle spiegazioni umane” (Vikram V. Ramaswamy et al. 2023b). I metodi di spiegazione basati sui concetti più diffusi includono TCAV (Cai et al. 2019), Network Dissection (Bau et al. 2017) e decomposizione della base interpretabile (Zhou et al. 2018).\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, e Olga Russakovsky. 2023b. «UFO: A unified method for controlling Understandability and Faithfulness Objectives in concept-based explanations for CNNs». ArXiv preprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, et al. 2019. «Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making». In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, a cura di Jennifer G. Dy e Andreas Krause, 80:2673–82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, e Antonio Torralba. 2017. «Network Dissection: Quantifying Interpretability of Deep Visual Representations». In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3319–27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\nZhou, Bolei, Yiyou Sun, David Bau, e Antonio Torralba. 2018. «Interpretable basis decomposition for visual explanation». In Proceedings of the European Conference on Computer Vision (ECCV), 119–34.\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, e Olga Russakovsky. 2023a. «Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability». In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10932–41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\nSi noti che questi metodi sono estremamente sensibili alla dimensione e alla qualità del set di concetti e c’è un compromesso tra la loro accuratezza e fedeltà e la loro interpretabilità o comprensibilità per gli esseri umani (Vikram V. Ramaswamy et al. 2023a). Tuttavia, mappando le previsioni del modello su concetti comprensibili per gli esseri umani, le spiegazioni basate sui concetti possono fornire trasparenza nel ragionamento alla base degli output del modello.\n\n\nInterpretabilità Intrinseca\nI modelli intrinsecamente interpretabili sono costruiti in modo tale che le loro spiegazioni siano parte dell’architettura del modello e siano quindi naturalmente fedeli, il che a volte li rende preferibili alle spiegazioni post-hoc applicate ai modelli black-box, specialmente in domini ad alto rischio in cui la trasparenza è fondamentale (Rudin 2019). Spesso, questi modelli sono vincolati in modo che le relazioni tra le caratteristiche di input e le previsioni siano facili da seguire per gli esseri umani (modelli lineari, alberi decisionali, set di decisioni, modelli k-NN) o obbediscano alla conoscenza strutturale del dominio, come la monotonicità (Gupta et al. 2016), la causalità o l’additività (Lou et al. 2013; Beck e Jackman 1998).\n\nRudin, Cynthia. 2019. «Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead». Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x.\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, e Alexander Van Esbroeck. 2016. «Monotonic calibrated interpolated look-up tables». The Journal of Machine Learning Research 17 (1): 3790–3836.\n\nLou, Yin, Rich Caruana, Johannes Gehrke, e Giles Hooker. 2013. «Accurate intelligible models with pairwise interactions». In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, a cura di Inderjit S. Dhillon, Yehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh, Jingrui He, Robert L. Grossman, e Ramasamy Uthurusamy, 623–31. ACM. https://doi.org/10.1145/2487575.2487579.\n\nBeck, Nathaniel, e Simon Jackman. 1998. «Beyond Linearity by Default: Generalized Additive Models». Am. J. Polit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, e Percy Liang. 2020. «Concept Bottleneck Models». In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 119:5338–48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, e Jonathan Su. 2019. «This Looks Like That: Deep Learning for Interpretable Image Recognition». In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, e Roman Garnett, 8928–39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\nTuttavia, lavori più recenti hanno allentato le restrizioni sui modelli intrinsecamente interpretabili, utilizzando modelli black-box per l’estrazione delle caratteristiche e un modello intrinsecamente interpretabile più semplice per la classificazione, consentendo spiegazioni fedeli che collegano le caratteristiche di alto livello alla previsione. Ad esempio, i Concept Bottleneck Models (Koh et al. 2020) prevedono un set di concetti c che viene passato in un classificatore lineare. I ProtoPNets (Chen et al. 2019) sezionano gli input in combinazioni lineari di somiglianze con parti prototipiche del set di training.\n\n\nInterpretabilità Meccanicistica\nI metodi di interpretabilità meccanicistica cercano di effettuare il reverse engineering delle reti neurali, spesso paragonandoli a come si potrebbe effettuare quello di un binario compilato o a come i neuroscienziati tentano di decodificare la funzione di singoli neuroni e circuiti nel cervello. La maggior parte delle ricerche sull’interpretabilità meccanicistica vede i modelli come un grafo computazionale (Geiger et al. 2021) e i circuiti sono sottografi con funzionalità distinte (Wang e Zhan 2019). Gli attuali approcci all’estrazione di circuiti dalle reti neurali e alla comprensione della loro funzionalità si basano sull’ispezione manuale umana delle visualizzazioni prodotte dai circuiti (Olah et al. 2020).\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, e Christopher Potts. 2021. «Causal Abstractions of Neural Networks». In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, a cura di Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 9574–86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\nWang, LingFeng, e YaQing Zhan. 2019. «A conceptual peer review model for arXiv and other preprint databases». Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, e Shan Carter. 2020. «Zoom In: An Introduction to Circuits». Distill 5 (3): e00024–001. https://doi.org/10.23915/distill.00024.001.\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana Turner, Carver Middleton, Will Carroll, et al. 2023. «Closing the Wearable Gap: Footankle kinematic modeling via deep learning models based on a smart sock wearable». Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\nIn alternativa, alcuni approcci creano autoencoder sparsi che incoraggiano i neuroni a codificare caratteristiche interpretabili districate (Davarzani et al. 2023). Questo campo è molto più nuovo rispetto alle aree esistenti in spiegabilità e interpretabilità e, in quanto tale, la maggior parte dei lavori è generalmente esplorativa piuttosto che orientata alla soluzione.\nCi sono molti problemi nell’interpretabilità meccanicistica, tra cui la polisemanticità di neuroni e circuiti, l’inconveniente e la soggettività dell’etichettatura umana e lo spazio di ricerca esponenziale per l’identificazione dei circuiti in grandi modelli con miliardi o trilioni di neuroni.\n\n\nSfide e Considerazioni\nMan mano che i metodi per interpretare e spiegare i modelli progrediscono, è importante notare che gli esseri umani si fidano troppo e abusano degli strumenti di interpretabilità (Kaur et al. 2020) e che la fiducia di un utente in un modello dovuta a una spiegazione può essere indipendente dalla correttezza delle spiegazioni (Lakkaraju e Bastani 2020). Pertanto, è necessario che oltre a valutare la fedeltà/correttezza delle spiegazioni, i ricercatori debbano anche garantire che i metodi di interpretabilità siano sviluppati e implementati tenendo a mente un utente specifico e che vengano eseguiti studi sugli utenti per valutarne l’efficacia e l’utilità nella pratica.\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, e Jennifer Wortman Vaughan. 2020. «Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning». In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, a cura di Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14. ACM. https://doi.org/10.1145/3313831.3376219.\n\nLakkaraju, Himabindu, e Osbert Bastani. 2020. «”How do I fool you?”: Manipulating User Trust via Misleading Black Box Explanations». In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 79–85. ACM. https://doi.org/10.1145/3375627.3375833.\nInoltre, le spiegazioni devono essere adattate alle competenze dell’utente, all’attività per cui stanno utilizzando la spiegazione e alla corrispondente quantità minima di informazioni richieste affinché la spiegazione sia utile per prevenire il sovraccarico di informazioni.\nMentre interpretabilità/spiegabilità sono aree popolari nella ricerca sull’apprendimento automatico, pochissimi lavori studiano la loro intersezione con TinyML ed edge computing. Dato che un’applicazione significativa di TinyML è l’assistenza sanitaria, che spesso richiede elevata trasparenza e interpretabilità, le tecniche esistenti devono essere testate per scalabilità ed efficienza relativamente ai dispositivi edge. Molti metodi si basano su passaggi aggiuntivi “forward” e “backward” e alcuni richiedono persino un training approfondito nei modelli proxy, che non sono fattibili su microcontrollori con risorse limitate.\nDetto questo, i metodi di spiegabilità possono essere molto utili nello sviluppo di modelli per dispositivi edge, in quanto possono fornire informazioni su come i dati di input e i modelli possono essere compressi e su come le rappresentazioni possono cambiare dopo la compressione. Inoltre, molti modelli interpretabili sono spesso più piccoli delle loro controparti black-box, il che potrebbe essere utile per le applicazioni TinyML.\n\n\n\n15.5.6 Monitoraggio delle Prestazioni del Modello\nMentre gli sviluppatori possono addestrare modelli che sembrano avversarialmente robusti, equi e interpretabili prima della distribuzione, è fondamentale che sia gli utenti sia i proprietari del modello ne continuino a monitorare le prestazioni e l’affidabilità durante l’intero ciclo di vita. I dati cambiano frequentemente nella pratica, il che può spesso comportare cambiamenti nella distribuzione. Questi cambiamenti nella distribuzione possono avere un impatto profondo sulle prestazioni predittive “vanilla” del modello e sulla sua affidabilità (equità, robustezza e interpretabilità) nei dati del mondo reale.\nInoltre, le definizioni di equità cambiano frequentemente nel tempo, come ciò che la società considera un attributo protetto, e anche le competenze degli utenti che chiedono spiegazioni possono cambiare.\nPer garantire che i modelli rimangano aggiornati con tali cambiamenti nel mondo reale, gli sviluppatori devono valutare continuamente i loro modelli su dati e standard attuali e rappresentativi e aggiornare i modelli quando necessario.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html#sfide-di-implementazione",
    "href": "contents/responsible_ai/responsible_ai.it.html#sfide-di-implementazione",
    "title": "15  IA Responsabile",
    "section": "15.6 Sfide di Implementazione",
    "text": "15.6 Sfide di Implementazione\n\n15.6.1 Strutture Organizzative e Culturali\nSebbene innovazione e regolamentazione siano spesso viste come interessi contrapposti, molti paesi hanno ritenuto necessario fornire supervisione man mano che i sistemi di intelligenza artificiale si espandono in più settori. Come illustrato in Figura 15.4, questa supervisione è diventata cruciale poiché questi sistemi continuano a permeare vari settori e ad avere un impatto sulla vita delle persone (vedere AI incentrata sull’uomo, Capitolo 8 “Interventi e regolamenti governativi”.\n\n\n\n\n\n\nFigura 15.4: Come vari gruppi influenzano l’AI incentrata sull’uomo. Fonte: Shneiderman (2020).\n\n\nShneiderman, Ben. 2020. «Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-centered AI Systems». ACM Trans. Interact. Intell. Syst. 10 (4): 1–31. https://doi.org/10.1145/3419764.\n\n\nTra questi vi sono:\n\nResponsible Use of Artificial Intelligence del Canada\nGeneral Data Protection Regulation (GDPR) dell’Unione Europea\nWhite Paper on Artificial Intelligence: a European approach to excellence and trust della Commissione Europea\nL’Information Commissioner’s Office del Regno Unito e la Consultation on Explaining AI Decisions Guidance dell’Alan Turing Institute hanno co-badgetato la guida da parte degli individui interessati da esse.\n\n\n\n15.6.2 Ottenere Dati di Qualità e Rappresentativi\nCome discusso nel capitolo Data Engineering, la progettazione responsabile dell’IA deve avvenire in tutte le fasi della pipeline, inclusa la raccolta dei dati. Ciò solleva la domanda: cosa significa che i dati siano di alta qualità e rappresentativi? Consideriamo i seguenti scenari che ostacolano la rappresentatività dei dati:\n\nSquilibrio dei Sottogruppi\nQuesto è probabilmente ciò che viene in mente quando si sente parlare di “dati rappresentativi”. Lo squilibrio dei sottogruppi significa che il set di dati contiene relativamente più dati da un sottogruppo rispetto a un altro. Questo squilibrio può influire negativamente sul modello ML a valle, facendolo sovradimensionare per un sottogruppo di persone e con prestazioni scadenti per un altro.\nUn esempio di conseguenza dello squilibrio dei sottogruppi è la discriminazione razziale nella tecnologia di riconoscimento facciale (Buolamwini e Gebru 2018); gli algoritmi commerciali di riconoscimento facciale hanno tassi di errore fino al 34% peggiori sulle donne dalla pelle scura rispetto agli uomini dalla pelle chiara.\n\nBuolamwini, Joy, e Timnit Gebru. 2018. «Gender shades: Intersectional accuracy disparities in commercial gender classification». In Conference on fairness, accountability and transparency, 77–91. PMLR.\nSi noti che lo squilibrio dei dati è reciproco e i sottogruppi possono anche essere sovrarappresentati in modo dannoso nel set di dati. Ad esempio, l’Allegheny Family Screening Tool (AFST) prevede la probabilità che un bambino venga alla fine allontanato da una casa. L’AFST produce punteggi sproporzionati per diversi sottogruppi, uno dei motivi è che è basato su dati storicamente distorti, provenienti da sistemi legali penali minorili e per adulti, agenzie di assistenza pubblica e agenzie e programmi di salute comportamentale.\n\n\nQuantificazione dei Risultati Target\nCiò si verifica in applicazioni in cui l’etichetta di verità di base non può essere misurata o è difficile da rappresentare in una singola quantità. Ad esempio, un modello ML in un’applicazione mobile per il benessere potrebbe voler prevedere i livelli di stress individuali. Le vere etichette di stress sono impossibili da ottenere direttamente e devono essere dedotte da altri segnali biologici, come la variabilità della frequenza cardiaca e i dati auto-riportati dall’utente. In queste situazioni, il rumore è incorporato nei dati per progettazione, rendendo questo un compito ML impegnativo.\n\n\nSpostamento della Distribuzione\nI dati potrebbero non rappresentare più un compito se un evento esterno importante causa un drastico cambiamento della fonte dati. Il modo più comune di pensare alle “distribution shift” [spostamenti della distribuzione] è rispetto al tempo; ad esempio, i dati sulle abitudini di acquisto dei consumatori raccolti prima del Covid potrebbero non essere più presenti nel comportamento dei consumatori oggi.\nIl trasferimento provoca un’altra forma di spostamento della distribuzione. Ad esempio, quando si applica un sistema di triage addestrato sui dati di un ospedale a un altro, potrebbe verificarsi uno spostamento nella distribuzione se i due ospedali sono molto diversi.\n\n\nRaccolta Dati\nUna soluzione ragionevole per molti dei problemi di cui sopra con dati non rappresentativi o di bassa qualità è raccoglierne di più; possiamo raccogliere più dati mirati a un sottogruppo sottorappresentato o dall’ospedale target a cui il nostro modello potrebbe essere trasferito. Tuttavia, per alcune ragioni, raccogliere più dati è una soluzione inappropriata o non fattibile per il compito da svolgere.\n\nLa raccolta dati può essere dannosa. Questo è il paradosso dell’esposizione, la situazione in cui coloro che traggono un guadagno significativo dalla raccolta dei propri dati sono anche coloro che sono messi a rischio dal processo di raccolta (D’ignazio e Klein (2023), Capitolo 4). Ad esempio, raccogliere più dati su individui non binari può essere importante per garantire l’equità dell’applicazione ML, ma li espone anche a rischi, a seconda di chi raccoglie i dati e di come (se i dati sono facilmente identificabili, contengono contenuti sensibili, ecc.).\nLa raccolta dati può essere costosa. In alcuni ambiti, come l’assistenza sanitaria, ottenere dati può essere costoso in termini di tempo e denaro.\nRaccolta dati distorta. Le cartelle cliniche elettroniche sono un’enorme fonte di dati per le applicazioni sanitarie basate su ML. A parte i problemi di rappresentazione dei sottogruppi, i dati stessi possono essere raccolti in modo distorto. Ad esempio, il linguaggio negativo (“non aderente”, “non disposto”) è utilizzato in modo sproporzionato sui pazienti neri (Himmelstein, Bates, e Zhou 2022).\n\n\nD’ignazio, Catherine, e Lauren F Klein. 2023. Data feminism. MIT press.\n\nHimmelstein, Gracie, David Bates, e Li Zhou. 2022. «Examination of Stigmatizing Language in the Electronic Health Record». JAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\nConcludiamo con diverse strategie aggiuntive per mantenere la qualità dei dati. Innanzitutto, è fondamentale promuovere una comprensione più approfondita dei dati. Ciò può essere ottenuto tramite l’implementazione di etichette e misure standardizzate della qualità dei dati, come nel Data Nutrition Project. Collaborare con le organizzazioni responsabili della raccolta dei dati aiuta a garantire che i dati vengano interpretati correttamente. In secondo luogo, è importante impiegare strumenti efficaci per l’esplorazione dei dati. Le tecniche di visualizzazione e le analisi statistiche possono rivelare problemi con i dati. Infine, stabilire un ciclo di feedback all’interno della pipeline ML è essenziale per comprendere le implicazioni reali dei dati. Le metriche, come le misure di equità, ci consentono di definire la “qualità dei dati” nel contesto dell’applicazione downstream; il miglioramento dell’equità può migliorare direttamente la qualità delle previsioni che gli utenti finali ricevono.\n\n\n\n15.6.3 Bilanciamento di Accuratezza e Altri Obiettivi\nI modelli di apprendimento automatico vengono spesso valutati solo in base all’accuratezza, ma questa singola metrica non riesce a catturare completamente le prestazioni del modello e i compromessi per i sistemi di intelligenza artificiale responsabili. Altre dimensioni etiche, come correttezza, robustezza, interpretabilità e privacy, possono competere con la pura accuratezza predittiva durante lo sviluppo del modello. Ad esempio, modelli intrinsecamente interpretabili come piccoli alberi decisionali o classificatori lineari con funzionalità semplificate barattano intenzionalmente una certa accuratezza per la trasparenza nel comportamento del modello e nelle previsioni. Mentre questi modelli semplificati raggiungono una minore accuratezza non catturando tutta la complessità nel set di dati, una migliore interpretabilità crea fiducia consentendo l’analisi diretta da parte di professionisti umani.\nInoltre, alcune tecniche pensate per migliorare la robustezza avversaria, come esempi di training avversario o riduzione della dimensionalità, possono degradare l’accuratezza dei dati di convalida puliti. In applicazioni sensibili come l’assistenza sanitaria, concentrarsi strettamente sull’accuratezza all’avanguardia comporta rischi etici se consente ai modelli di fare più affidamento su correlazioni spurie che introducono distorsioni o utilizzano ragionamenti opachi. Pertanto, gli obiettivi di prestazione appropriati dipendono in larga misura dal contesto socio-tecnico.\nMetodologie come Value Sensitive Design forniscono framework per valutare formalmente le priorità di vari stakeholder all’interno del sistema di distribuzione nel mondo reale. Ciò spiega le tensioni tra valori quali accuratezza, interpretabilità ed equità, che possono quindi orientare decisioni di compromesso responsabili. Per un sistema di diagnosi medica, raggiungere la massima accuratezza potrebbe non essere l’obiettivo unico: migliorare la trasparenza per creare fiducia nei professionisti o ridurre i pregiudizi verso i gruppi minoritari potrebbe giustificare piccole perdite di accuratezza. L’analisi del contesto socio-tecnico è fondamentale per stabilire questi obiettivi.\nAdottando una visione olistica, possiamo bilanciare responsabilmente l’accuratezza con altri obiettivi etici per il successo del modello. Il monitoraggio continuo delle prestazioni lungo più dimensioni è fondamentale man mano che il sistema si evolve dopo la distribuzione.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html#considerazioni-etiche-nella-progettazione-dellia",
    "href": "contents/responsible_ai/responsible_ai.it.html#considerazioni-etiche-nella-progettazione-dellia",
    "title": "15  IA Responsabile",
    "section": "15.7 Considerazioni Etiche Nella Progettazione dell’IA",
    "text": "15.7 Considerazioni Etiche Nella Progettazione dell’IA\nDobbiamo discutere almeno di alcune delle numerose questioni etiche in gioco nella progettazione e nell’applicazione di sistemi di intelligenza artificiale e di diversi framework per affrontare tali questioni, tra cui quelle relative alla sicurezza dell’intelligenza artificiale, all’interazione uomo-computer (HCI) e alla scienza, tecnologia e società (STS).\n\n15.7.1 Sicurezza dell’Intelligenza Artificiale e Allineamento dei Valori\nNel 1960, Norbert Weiner scrisse: “’se utilizziamo, per raggiungere i nostri scopi, un’agenzia meccanica con il cui funzionamento non possiamo interferire efficacemente… faremmo meglio ad essere abbastanza sicuri che lo scopo attribuito alla macchina sia lo scopo che desideriamo” (Wiener 1960).\n\nWiener, Norbert. 1960. «Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers.» Science 131 (3410): 1355–58. https://doi.org/10.1126/science.131.3410.1355.\n\nRussell, Stuart. 2021. «Human-compatible artificial intelligence». Human-like machine intelligence, 3–23.\nNegli ultimi anni, poiché le capacità dei modelli di deep learning hanno raggiunto, e talvolta persino superato, le capacità umane, la questione della creazione di sistemi di intelligenza artificiale che agiscano in accordo con le intenzioni umane invece di perseguire obiettivi non intenzionali o indesiderati è diventata fonte di preoccupazione (Russell 2021). Nel campo della sicurezza dell’IA, un obiettivo particolare riguarda “l’allineamento dei valori”, ovvero il problema di come codificare lo scopo “giusto” nelle macchine Intelligenza artificiale compatibile con gli esseri umani. L’attuale ricerca sull’IA presuppone che conosciamo gli obiettivi che vogliamo raggiungere e “studia la capacità di raggiungere gli obiettivi, non la progettazione di tali obiettivi”.\nTuttavia, i complessi contesti di distribuzione nel mondo reale rendono difficile definire esplicitamente “lo scopo giusto” per le macchine, richiedendo quadri per l’impostazione di obiettivi responsabili ed etici. Metodologie come Value Sensitive Design forniscono meccanismi formali per far emergere le tensioni tra i valori e le priorità delle parti interessate.\nAdottando una visione socio-tecnica olistica, possiamo garantire meglio che i sistemi intelligenti perseguano obiettivi che si allineano con ampie intenzioni umane anziché massimizzare metriche ristrette come la sola accuratezza. Raggiungere questo obiettivo nella pratica rimane una questione di ricerca aperta e critica man mano che le capacità dell’IA avanzano rapidamente.\nL’assenza di questo allineamento può portare a diversi problemi di sicurezza dell’IA, come documentato in una varietà di modelli di deep learning. Una caratteristica comune dei sistemi che ottimizzano per un obiettivo è che le variabili non direttamente incluse nell’obiettivo possono essere impostate su valori estremi per aiutare a ottimizzare per quell’obiettivo, portando a problemi caratterizzati come gioco di specifiche, hacking di ricompensa, ecc., nel “reinforcement learning (RL)” [apprendimento per rinforzo].\nNegli ultimi anni, un’implementazione particolarmente popolare di RL è stata quella dei modelli pre-addestrati utilizzando apprendimento auto-supervisionato e “Reinforcement Learning From Human Feedback (RLHF)” [apprendimento per rinforzo fine-tuned da feedback umano] (Christiano et al. 2017). Ngo 2022 (Ngo, Chan, e Mindermann 2022) sostiene che premiando i modelli per apparire innocui ed etici e massimizzando al contempo i risultati utili, RLHF potrebbe incoraggiare l’emergere di tre proprietà problematiche: hacking della ricompensa consapevole della situazione, in cui le politiche sfruttano la fallibilità umana per ottenere un’elevata ricompensa, obiettivi rappresentati internamente non allineati che si generalizzano oltre la distribuzione di messa a punto RLHF e strategie di ricerca del potere.\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, e Dario Amodei. 2017. «Deep Reinforcement Learning from Human Preferences». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4299–4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\nNgo, Richard, Lawrence Chan, e Sören Mindermann. 2022. «The alignment problem from a deep learning perspective». ArXiv preprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\nVan Noorden, Richard. 2016. «ArXiv preprint server plans multimillion-dollar overhaul». Nature 534 (7609): 602–2. https://doi.org/10.1038/534602a.\nAllo stesso modo, Van Noorden (2016) delinea sei problemi concreti per la sicurezza dell’IA, tra cui evitare effetti collaterali negativi, evitare hacking della ricompensa, supervisione scalabile per aspetti dell’obiettivo che sono troppo costosi per essere valutati frequentemente durante il training, strategie di esplorazione sicure che incoraggiano la creatività prevenendo al contempo i danni e robustezza allo spostamento distributivo in ambienti di test invisibili.\n\n\n15.7.2 Sistemi Autonomi e Controllo [e Fiducia]\nLe conseguenze dei sistemi autonomi che agiscono indipendentemente dalla supervisione umana e spesso al di fuori del giudizio umano sono state ampiamente documentate in diversi settori e casi d’uso. Più di recente, il Dipartimento dei veicoli a motore della California ha sospeso i permessi di distribuzione e collaudo di Cruise per i suoi veicoli autonomi, citando “rischi irragionevoli per la sicurezza pubblica”. Uno di questi incidenti si è verificato quando un veicolo ha colpito un pedone che stava attraversando le strisce pedonali dopo che il semaforo era diventato verde e al veicolo è stato permesso di procedere. Nel 2018, un pedone che attraversava la strada con la sua bicicletta è morto quando un’auto Uber a guida autonoma, che operava in modalità autonoma, non è riuscita a classificare accuratamente il suo corpo in movimento come un oggetto da evitare.\nAnche i sistemi autonomi oltre ai veicoli a guida autonoma sono suscettibili a tali problemi, con conseguenze potenzialmente più gravi, poiché i droni alimentati da remoto stanno già rimodellando la guerra. Sebbene tali incidenti sollevino importanti questioni etiche su chi dovrebbe essere ritenuto responsabile quando questi sistemi falliscono, evidenziano anche le sfide tecniche nel dare il pieno controllo di attività complesse e reali alle macchine.\nIn sostanza, c’è una tensione tra autonomia umana e delle macchine. Le discipline ingegneristiche e informatiche hanno teso a concentrarsi sull’autonomia delle macchine. Ad esempio, a partire dal 2019, una ricerca della parola “autonomia” nella Digital Library dell’Association for Computing Machinery (ACM) rivela che dei 100 articoli più citati, il 90% riguarda l’autonomia delle macchine (Calvo et al. 2020). Nel tentativo di costruire sistemi a beneficio dell’umanità, queste discipline hanno assunto, senza dubbio, l’aumento della produttività, dell’efficienza e dell’automazione come strategie primarie per il beneficio dell’umanità.\n\nMcCarthy, John. 1981. «Epistemological Problems Of Artificial Intelligence». In Readings in Artificial Intelligence, 459–65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\nQuesti obiettivi pongono l’automazione delle macchine in prima linea, spesso a spese dell’uomo. Questo approccio soffre di sfide intrinseche, come notato fin dai primi giorni dell’IA attraverso il “Frame problem” [specifica degli effetti] e il “qualification problem” [qualificazione delle precondizioni] (cfr. http://www.diag.uniroma1.it/~nardi/Didattica/RC/lezioni/sitcalc-1.pdf), che formalizza l’osservazione che è impossibile specificare tutte le precondizioni necessarie per il successo di un’azione nel mondo reale (McCarthy 1981).\nQueste limitazioni logiche hanno dato origine ad approcci matematici come la “Responsibility-sensitive safety (RSS)” [sicurezza sensibile alla responsabilità] (Shalev-Shwartz, Shammah, e Shashua 2017), che mira a scomporre l’obiettivo finale di un sistema di guida automatizzato (vale a dire la sicurezza) in condizioni concrete e verificabili che possono essere rigorosamente formulate in termini matematici. L’obiettivo dell’RSS è che tali norme di sicurezza garantiscano la sicurezza del “Automated Driving System (ADS)” [sistema di guida autonoma] nella rigorosa forma di dimostrazione matematica. Tuttavia, tali approcci tendono a utilizzare l’automazione per affrontare i problemi dell’automazione e sono suscettibili a molti degli stessi problemi.\n\nShalev-Shwartz, Shai, Shaked Shammah, e Amnon Shashua. 2017. «On a formal model of safe and scalable self-driving cars». ArXiv preprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\nFriedman, Batya. 1996. «Value-sensitive design». Interactions 3 (6): 16–23. https://doi.org/10.1145/242485.242493.\n\nPeters, Dorian, Rafael A. Calvo, e Richard M. Ryan. 2018. «Designing for Motivation, Engagement and Wellbeing in Digital Experience». Front. Psychol. 9 (maggio): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\nRyan, Richard M., e Edward L. Deci. 2000. «Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.» Am. Psychol. 55 (1): 68–78. https://doi.org/10.1037/0003-066x.55.1.68.\nUn altro approccio per combattere questi problemi è concentrarsi sulla progettazione “human-centered” di sistemi interattivi che incorporano il controllo umano. Il design sensibile al valore (Friedman 1996) ha descritto tre fattori di progettazione chiave per un’interfaccia utente che hanno un impatto sull’autonomia, tra cui capacità del sistema, complessità, rappresentazione errata e fluidità. Un modello più recente, chiamato METUX (A Model for Motivation, Engagement, and Thriving in the User Experience), sfrutta le intuizioni della “Self-determination Theory (SDT)” in psicologia per identificare sei sfere distinte dell’esperienza tecnologica che contribuiscono ai sistemi di progettazione che promuovono il benessere e la prosperità umana (Peters, Calvo, e Ryan 2018). SDT definisce l’autonomia come agire in base ai propri obiettivi e valori, il che è distinto dall’uso dell’autonomia come semplice sinonimo di indipendenza o di controllo (Ryan e Deci 2000).\nCalvo et al. (2020) elabora METUX e le sue sei “sfere di esperienza tecnologica” nel contesto dei sistemi di raccomandazione AI. Propongono queste sfere (Adozione, Interfaccia, Attività, Comportamento, Vita e Società) come un modo per organizzare il pensiero e la valutazione della progettazione tecnologica al fine di catturare in modo appropriato gli impatti contraddittori e a valle sull’autonomia umana quando interagisce con i sistemi AI.\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, e Richard M Ryan. 2020. «Supporting human autonomy in AI systems: A framework for ethical enquiry». Ethics of digital well-being: A multidisciplinary approach, 31–54.\n\n\n15.7.3 Impatti Economici su Posti di Lavoro, Competenze, Salari\nUna delle principali preoccupazioni dell’attuale ascesa delle tecnologie AI è la disoccupazione diffusa. Con l’espansione delle capacità dei sistemi AI, molti temono che queste tecnologie causeranno una perdita assoluta di posti di lavoro, poiché sostituiranno i lavoratori attuali e supereranno ruoli occupazionali alternativi in tutti i settori. Tuttavia, il cambiamento dei panorami economici per mano dell’automazione non è una novità e, storicamente, si è scoperto che riflette modelli di spostamento piuttosto che di sostituzione (Shneiderman 2022)—Capitolo 4. In particolare, l’automazione di solito riduce i costi e aumenta la qualità, aumentando notevolmente l’accesso e la domanda. La necessità di servire questi mercati in crescita spinge la produzione, creando nuovi posti di lavoro.\n\n———. 2022. Human-centered AI. Oxford University Press.\nInoltre, gli studi hanno scoperto che i tentativi di raggiungere un’automazione “lights-out”, ovvero un’automazione produttiva e flessibile con un numero minimo di lavoratori umani, non hanno avuto successo. I tentativi di farlo hanno portato a quella che la task force del MIT Work of the Future ha definito “automazione a somma zero”, in cui la flessibilità dei processi viene sacrificata per aumentare la produttività.\nAl contrario, la task force propone un approccio di “automazione a somma positiva” in cui la flessibilità viene aumentata progettando una tecnologia che incorpora strategicamente gli esseri umani dove sono molto necessari, rendendo più facile per i dipendenti della linea addestrare e correggere i robot, utilizzando un approccio bottom-up per identificare quali attività dovrebbero essere automatizzate; e scegliendo le giuste metriche per misurare il successo (vedi Work of the Future del MIT).\nTuttavia, l’ottimismo delle prospettive di alto livello non esclude danni individuali, specialmente per coloro le cui competenze e lavori saranno resi obsoleti dall’automazione. La pressione pubblica e legislativa, così come gli sforzi di responsabilità sociale delle aziende, dovranno essere diretti alla creazione di politiche che condividano i vantaggi dell’automazione con i lavoratori e si traducano in salari minimi e benefici più elevati.\n\n\n15.7.4 Comunicazione Scientifica e Alfabetizzazione IA\nUn sondaggio del 1993 sulle convinzioni di 3000 adulti nordamericani sulla “macchina pensante elettronica” ha rivelato due prospettive principali del primo computer: la prospettiva dello “strumento utile dell’uomo” e la prospettiva della “macchina pensante fantastica”. Gli atteggiamenti che contribuiscono alla visione della “macchina pensante fantastica” in questo e altri studi hanno rivelato una caratterizzazione dei computer come “cervelli intelligenti, più intelligenti delle persone, illimitati, veloci, misteriosi e spaventosi” (Martin 1993). Questi timori evidenziano una componente facilmente trascurata dell’IA responsabile, specialmente in mezzo alla corsa alla commercializzazione di tali tecnologie: la comunicazione scientifica che comunica accuratamente le capacità e le limitazioni di questi sistemi, fornendo al contempo trasparenza sui limiti della conoscenza degli esperti su questi sistemi.\n\nMartin, C. Dianne. 1993. «The myth of the awesome thinking machine». Commun. ACM 36 (4): 120–33. https://doi.org/10.1145/255950.153587.\n\nHandlin, Oscar. 1965. «Science and technology in popular culture». Daedalus-us., 156–70.\nMan mano che le capacità dei sistemi di IA si espandono oltre la comprensione della maggior parte delle persone, c’è una tendenza naturale a presumere i tipi di mondi apocalittici dipinti dai nostri media. Ciò è dovuto in parte all’apparente difficoltà di assimilare informazioni scientifiche, persino in culture tecnologicamente avanzate, che porta i prodotti della scienza a essere percepiti come magia, “comprensibili solo in termini di ciò che hanno fatto, non di come hanno funzionato” (Handlin 1965).\nMentre le aziende tecnologiche dovrebbero essere ritenute responsabili per aver limitato le affermazioni grandiose e non essere cadute in cicli di clamore, la ricerca che studia la comunicazione scientifica, in particolare per quanto riguarda l’intelligenza artificiale (generativa), sarà utile anche per tracciare e correggere la comprensione pubblica di queste tecnologie. Un’analisi del database accademico Scopus ha scoperto che tale ricerca è scarsa, con solo una manciata di articoli che menzionano sia “comunicazione scientifica” che “intelligenza artificiale” (Schäfer 2023).\n\nSchäfer, Mike S. 2023. «The Notorious GPT: Science communication in the age of artificial intelligence». Journal of Science Communication 22 (02): Y02. https://doi.org/10.22323/2.22020402.\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial Intelligence. Edward Elgar Publishing.\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, e Maggie Shen Qiao. 2021. «AI literacy: Definition, teaching, evaluation and ethical issues». Proceedings of the Association for Information Science and Technology 58 (1): 504–9.\nLa ricerca che espone le prospettive, i “frame” e le immagini del futuro promosse da istituzioni accademiche, aziende tecnologiche, stakeholder, enti regolatori, giornalisti, ONG e altri aiuterà anche a identificare potenziali lacune nell’alfabetizzazione AI tra gli adulti (Lindgren 2023). Una maggiore attenzione all’alfabetizzazione AI da parte di tutti gli stakeholder sarà importante per aiutare le persone le cui competenze sono rese obsolete dall’automazione AI (Ng et al. 2021).\n“Ma anche coloro che non acquisiscono mai quella comprensione hanno bisogno di rassicurazioni sul fatto che esista una connessione tra gli obiettivi della scienza e il loro benessere e, soprattutto, che lo scienziato non sia un uomo completamente a parte, ma uno che condivide parte del loro valore.” (Handlin, 1965)",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html#conclusione",
    "href": "contents/responsible_ai/responsible_ai.it.html#conclusione",
    "title": "15  IA Responsabile",
    "section": "15.8 Conclusione",
    "text": "15.8 Conclusione\nUn’intelligenza artificiale responsabile è fondamentale poiché i sistemi di apprendimento automatico esercitano una crescente influenza nei settori sanitario, lavorativo, finanziario e della giustizia penale. Mentre l’intelligenza artificiale promette immensi benefici, i modelli progettati in modo sconsiderato rischiano di perpetrare danni attraverso pregiudizi, violazioni della privacy, comportamenti indesiderati e altre insidie.\nMantenere i principi di equità, spiegabilità, responsabilità, sicurezza e trasparenza consente lo sviluppo di un’intelligenza artificiale etica allineata ai valori umani. Tuttavia, l’implementazione di questi principi comporta il superamento di complesse sfide tecniche e sociali relative al rilevamento di pregiudizi nei set di dati, alla scelta di appropriati compromessi nei modelli, alla protezione di dati di training di qualità e altro ancora. Framework come la progettazione sensibile al valore guidano il bilanciamento dell’accuratezza rispetto ad altri obiettivi in base alle esigenze delle parti interessate.\nGuardando al futuro, il progresso dell’intelligenza artificiale responsabile richiede una ricerca continua e l’impegno del settore. Sono necessari benchmark più standardizzati per confrontare pregiudizi e robustezza dei modelli. Man mano che il TinyML personalizzato si espande, abilitare una trasparenza efficiente e il controllo dell’utente per i dispositivi edge giustifica l’attenzione. Le strutture e le politiche di incentivazione riviste devono incoraggiare uno sviluppo deliberato ed etico prima di un’implementazione sconsiderata. L’istruzione sulla cultura dell’intelligenza artificiale e sui suoi limiti contribuirà ulteriormente alla comprensione pubblica.\nI metodi responsabili sottolineano che, mentre l’apprendimento automatico offre un potenziale immenso, un’applicazione sconsiderata rischia di avere conseguenze negative. La collaborazione interdisciplinare e la progettazione incentrata sull’uomo sono essenziali affinché l’intelligenza artificiale possa promuovere un ampio beneficio sociale. Il percorso da seguire non risiede in una checklist arbitraria, ma in un impegno costante per comprendere e sostenere la nostra responsabilità etica a ogni passo. Intraprendendo un’azione coscienziosa, la comunità dell’apprendimento automatico può guidare l’intelligenza artificiale verso l’emancipazione di tutte le persone in modo equo e sicuro.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.it.html#sec-responsible-ai-resource",
    "href": "contents/responsible_ai/responsible_ai.it.html#sec-responsible-ai-resource",
    "title": "15  IA Responsabile",
    "section": "15.9 Risorse",
    "text": "15.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nWhat am I building? What is the goal?\nWho is the audience?\nWhat are the consequences?\nResponsible Data Collection.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 15.1\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>IA Responsabile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html",
    "href": "contents/sustainable_ai/sustainable_ai.it.html",
    "title": "16  IA Sostenibile",
    "section": "",
    "text": "16.1 Introduzione\nI rapidi progressi nell’intelligenza artificiale (IA) e nel machine learning (ML) [apprendimento automatico] hanno portato a molte applicazioni e ottimizzazioni utili per l’efficienza delle prestazioni. Tuttavia, la notevole crescita dell’IA ha un costo significativo ma spesso trascurato: il suo impatto ambientale. Il rapporto più recente pubblicato dall’IPCC, l’organismo internazionale che guida le valutazioni scientifiche del cambiamento climatico e dei suoi impatti, ha sottolineato l’importanza urgente di affrontare il cambiamento climatico. Senza sforzi immediati per ridurre le emissioni globali di \\(\\textrm{CO}_2\\) di almeno il 43 percento prima del 2030, supereremo il riscaldamento globale di 1,5 gradi Celsius (Winkler et al. 2022). Ciò potrebbe avviare cicli di feedback positivi, spingendo le temperature ancora più in alto. Accanto alle questioni ambientali, le Nazioni Unite hanno riconosciuto 17 Sustainable Development Goals (SDG) [Obiettivi di sviluppo sostenibile], in cui l’IA può svolgere un ruolo importante e, viceversa, possono svolgere un ruolo importante nello sviluppo di sistemi di IA. Poiché il campo continua a espandersi, considerare la sostenibilità è fondamentale.\nI sistemi di intelligenza artificiale, in particolare i grandi modelli linguistici come GPT-3 e i modelli di visione artificiale come DALL-E 2, richiedono enormi quantità di risorse computazionali per l’addestramento. Ad esempio, si stima che GPT-3 consumi 1.300 megawattora di elettricità, pari a 1.450 famiglie medie statunitensi in un mese intero (Maslej et al. 2023), o in altre parole, consuma abbastanza energia da rifornire una famiglia media statunitense per 120 anni! Questa immensa richiesta di energia deriva principalmente da data center affamati di energia con server che eseguono calcoli intensivi per addestrare queste complesse reti neurali per giorni o settimane.\nLe stime attuali indicano che le emissioni di carbonio prodotte dallo sviluppo di un singolo modello di intelligenza artificiale sofisticato possono eguagliare le emissioni nell’arco di vita di cinque veicoli standard a benzina (Strubell, Ganesh, e McCallum 2019). Una parte significativa dell’elettricità attualmente consumata dai data center è generata da fonti non rinnovabili come carbone e gas naturale, con il risultato che i data center contribuiscono a circa l’1% delle emissioni totali di carbonio a livello mondiale. Ciò è paragonabile alle emissioni dell’intero settore delle compagnie aeree. Questa immensa impronta di carbonio dimostra l’urgente necessità di passare a fonti di energia rinnovabili come l’energia solare ed eolica per gestire lo sviluppo dell’intelligenza artificiale.\nInoltre, anche i sistemi di intelligenza artificiale su piccola scala distribuiti su dispositivi edge come parte di TinyML hanno impatti ambientali che non dovrebbero essere ignorati (Prakash, Stewart, et al. 2023). L’hardware specializzato richiesto per l’intelligenza artificiale ha un impatto ambientale dovuto all’estrazione e alla produzione di risorse naturali. GPU, CPU e chip come le TPU dipendono da metalli delle terre rare la cui estrazione e lavorazione generano un notevole inquinamento. Anche la produzione di questi componenti ha le sue richieste energetiche. Inoltre, la raccolta, l’archiviazione e la preelaborazione dei dati utilizzati per addestrare modelli sia su piccola che su larga scala comportano costi ambientali, esacerbando ulteriormente le implicazioni di sostenibilità dei sistemi ML.\nPertanto, mentre l’intelligenza artificiale promette innovazioni in molti campi, per sostenere il progresso è necessario affrontare le sfide della sostenibilità. L’intelligenza artificiale può continuare a progredire in modo responsabile ottimizzando l’efficienza dei modelli, esplorando hardware specializzato alternativo e fonti di energia rinnovabile per i data center e monitorando il suo impatto ambientale complessivo.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#introduzione",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#introduzione",
    "title": "16  IA Sostenibile",
    "section": "",
    "text": "Winkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilariño, Sivan Kartha, e Joana Portugal-Pereira. 2022. «Examples of shifting development pathways: Lessons on how to enable broader, deeper, and faster climate action». Climate Action 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, et al. 2023. «Artificial intelligence index report 2023». ArXiv preprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete Warden, Brian Plancher, e Vijay Janapa Reddi. 2023. «Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers». ArXiv preprint. https://arxiv.org/abs/2301.11899.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#social-and-ethical-responsibility",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#social-and-ethical-responsibility",
    "title": "16  IA Sostenibile",
    "section": "16.2 Responsabilità Sociale ed Etica",
    "text": "16.2 Responsabilità Sociale ed Etica\nL’impatto ambientale dell’IA non è solo una questione tecnica, ma anche etica e sociale. Man mano che l’IA diventa sempre più integrata nelle nostre vite e nei nostri settori, la sua sostenibilità diventa sempre più critica.\n\n16.2.1 Considerazioni Etiche\nLa portata dell’impatto ambientale dell’IA solleva profonde questioni etiche sulle responsabilità degli sviluppatori e delle aziende di IA nel ridurre al minimo le emissioni di carbonio e l’uso di energia. In quanto creatori di sistemi e tecnologie di IA che possono avere impatti globali di vasta portata, gli sviluppatori hanno l’obbligo etico di integrare consapevolmente la tutela ambientale nel loro processo di progettazione, anche se la sostenibilità avviene a scapito di alcuni guadagni di efficienza.\nC’è una chiara e attuale necessità per noi di avere conversazioni aperte e oneste sui compromessi ambientali dell’IA all’inizio del ciclo di vita dello sviluppo. I ricercatori dovrebbero sentirsi autorizzati a esprimere preoccupazioni se le priorità organizzative non sono allineate con gli obiettivi etici, come nel caso della lettera aperta per sospendere i giganteschi esperimenti di IA.\nInoltre, c’è una crescente necessità per le aziende di IA di esaminare attentamente i loro contributi al cambiamento climatico e al danno ambientale. Le grandi aziende tecnologiche sono responsabili dell’infrastruttura cloud, delle richieste di energia dei data center e dell’estrazione delle risorse necessarie per alimentare l’IA odierna. La leadership dovrebbe valutare se i valori e le politiche organizzative promuovano la sostenibilità, dalla produzione di hardware alle pipeline di training dei modelli.\nInoltre, potrebbe essere necessaria più di un’autoregolamentazione volontaria: i governi potrebbero dover introdurre nuove normative volte a standard e pratiche di intelligenza artificiale sostenibili se speriamo di frenare l’esplosione energetica prevista di modelli sempre più grandi. Le metriche segnalate come l’utilizzo del computer, l’impronta di carbonio e i parametri di riferimento dell’efficienza potrebbero responsabilizzare le organizzazioni.\nAttraverso principi etici, politiche aziendali e regole pubbliche, i tecnici e le aziende di intelligenza artificiale hanno un profondo dovere nei confronti del nostro pianeta per garantire l’avanzamento responsabile e sostenibile della tecnologia in grado di trasformare radicalmente la società moderna. Dobbiamo alle generazioni future di fare le cose per bene.\n\n\n16.2.2 Sostenibilità a Lungo Termine\nLa massiccia espansione prevista dell’IA solleva urgenti preoccupazioni sulla sua sostenibilità a lungo termine. Poiché il software e le applicazioni di IA aumentano rapidamente in complessità e utilizzo in tutti i settori, la domanda di potenza di calcolo e infrastrutture salirà alle stelle in modo esponenziale nei prossimi anni.\nPer mettere in prospettiva la portata della crescita prevista, la capacità di calcolo totale richiesta per l’addestramento dei modelli di IA ha visto un sorprendente aumento di 350.000 volte dal 2012 al 2019 (R. Schwartz et al. 2020). I ricercatori prevedono una crescita di oltre un ordine di grandezza ogni anno, man mano che vengono sviluppati assistenti IA personalizzati, tecnologia autonoma, strumenti di medicina di precisione e altro ancora. Le tendenze sono simili per i sistemi ML embedded, con una stima di 2,5 miliardi di dispositivi edge abilitati all’IA distribuiti entro il 2030.\nLa gestione di questo livello di espansione richiede innovazioni incentrate su software e hardware in termini di efficienza e integrazione rinnovabile da parte di ingegneri e scienziati dell’IA. Dal lato software, nuove tecniche di ottimizzazione dei modelli, distillazione, potatura, numeri a bassa precisione, condivisione delle conoscenze tra sistemi e altre aree devono diventare best practice diffuse per frenare le esigenze energetiche. Ad esempio, realizzare anche una domanda di elaborazione ridotta del 50% per raddoppio della capacità avrebbe un impatto enorme sull’energia totale.\nDal lato dell’infrastruttura hardware, a causa dei crescenti costi di trasferimento dati, archiviazione, raffreddamento e spazio, continuare con l’attuale modello di server farm centralizzato nei data center è probabilmente irrealizzabile a lungo termine (Lannelongue, Grealey, e Inouye 2021). Esplorare opzioni di elaborazione decentralizzate alternative attorno a “edge AI” su dispositivi locali o all’interno di reti di telecomunicazioni può alleviare le pressioni di ridimensionamento sui data center iper-scalabili ad alto consumo energetico. Allo stesso modo, il passaggio a fonti di energia rinnovabile ibride e a zero emissioni di carbonio che alimentano i principali data center dei provider cloud in tutto il mondo sarà essenziale.\n\nLannelongue, Loı̈c, Jason Grealey, e Michael Inouye. 2021. «Green Algorithms: Quantifying the Carbon Footprint of Computation». Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\n16.2.3 IA per il Bene Ambientale\nSebbene molta attenzione sia rivolta alle sfide di sostenibilità dell’IA, queste potenti tecnologie forniscono soluzioni uniche per combattere il cambiamento climatico e guidare il progresso ambientale. Ad esempio, l’apprendimento automatico può ottimizzare continuamente le reti elettriche intelligenti per migliorare l’integrazione delle energie rinnovabili e l’efficienza della distribuzione dell’elettricità attraverso le reti (Zhang, Han, e Deng 2018). I modelli possono acquisire lo stato in tempo reale di una rete elettrica e le previsioni meteorologiche per allocare e spostare le fonti rispondendo alla domanda e all’offerta.\n\nZhang, Dongxia, Xiaoqing Han, e Chunyu Deng. 2018. «Review on the research and practice of deep learning and reinforcement learning in smart grids». CSEE Journal of Power and Energy Systems 4 (3): 362–70. https://doi.org/10.17775/cseejpes.2018.00520.\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. «Learning skillful medium-range global weather forecasting». Science 382 (6677): 1416–21. https://doi.org/10.1126/science.adi2336.\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, e Anima Anandkumar. 2023. «FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators». In Proceedings of the Platform for Advanced Scientific Computing Conference, 1–11. ACM. https://doi.org/10.1145/3592979.3593412.\nLe reti neurali ottimizzate si sono anche dimostrate notevolmente efficaci nelle previsioni meteorologiche di prossima generazione (Lam et al. 2023) e nella modellazione climatica (Kurth et al. 2023). Possono analizzare rapidamente enormi volumi di dati climatici per potenziare la preparazione agli eventi estremi e la pianificazione delle risorse per uragani, inondazioni, siccità e altro ancora. I ricercatori del clima hanno raggiunto un’accuratezza all’avanguardia del percorso delle tempeste combinando simulazioni di IA con modelli numerici tradizionali.\nL’intelligenza artificiale consente inoltre un migliore monitoraggio della biodiversità (Silvestro et al. 2022), della fauna selvatica (D. Schwartz et al. 2021), degli ecosistemi e della deforestazione illegale tramite droni e satelliti. Gli algoritmi di visione artificiale possono automatizzare le stime della popolazione delle specie e le valutazioni della salute dell’habitat su vaste regioni non monitorate. Queste capacità forniscono agli ambientalisti potenti strumenti per combattere il bracconaggio (Bondi et al. 2018), ridurre i rischi di estinzione delle specie e comprendere i cambiamenti ecologici.\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, e Alexandre Antonelli. 2022. «Improving biodiversity protection through artificial intelligence». Nature Sustainability 5 (5): 415–24. https://doi.org/10.1038/s41893-022-00851-6.\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, e Andreas Paepcke. 2021. «Deployment of Embedded Edge-AI for Wildlife Monitoring in Remote Regions». In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), 1035–42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital Shah, Robert Hannaford, Arvind Iyer, Lucas Joppa, e Milind Tambe. 2018. «Near Real-Time Detection of Poachers from Drones in AirSim». In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, a cura di Jérôme Lang, 5814–16. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\nInvestimenti mirati in applicazioni di intelligenza artificiale per la sostenibilità ambientale, la condivisione di dati intersettoriali e l’accessibilità dei modelli possono accelerare notevolmente le soluzioni a urgenti problemi ecologici. L’enfasi sull’intelligenza artificiale per il bene sociale indirizza l’innovazione in direzioni più pulite, guidando queste tecnologie che modellano il mondo verso uno sviluppo etico e responsabile.\n\n\n16.2.4 Caso di Studio\nI data center di Google sono fondamentali per alimentare prodotti come Search, Gmail e YouTube, utilizzati quotidianamente da miliardi di persone. Tuttavia, mantenere attive e funzionanti le vaste server farm richiede molta energia, in particolare per i sistemi di raffreddamento essenziali. Google si impegna costantemente per migliorare l’efficienza in tutte le operazioni. Tuttavia, i progressi si stavano rivelando difficili solo con i metodi tradizionali, considerando le complesse dinamiche personalizzate coinvolte. Questa sfida ha spinto una svolta nell’apprendimento automatico, producendo potenziali risparmi.\nDopo oltre un decennio di ottimizzazione della progettazione dei data center, invenzione di hardware di elaborazione a basso consumo energetico e protezione di fonti di energia rinnovabili, Google ha portato gli scienziati di DeepMind a sbloccare ulteriori progressi. Gli esperti di intelligenza artificiale hanno affrontato fattori complessi che circondano il funzionamento degli apparati di raffreddamento industriali. Apparecchiature come pompe e refrigeratori interagiscono in modo non lineare, mentre cambiano anche le condizioni meteorologiche esterne e le variabili architettoniche interne. Catturare questa complessità ha confuso le rigide formule ingegneristiche e l’intuizione umana.\nIl team DeepMind ha sfruttato i dati storici estesi dei sensori di Google che descrivono temperature, consumi energetici e altri attributi come input di training. Hanno creato un sistema flessibile basato su reti neurali per modellare le relazioni e prevedere configurazioni ottimali, riducendo al minimo la “power usage effectiveness (PUE)” [efficacia dell’utilizzo di energia] (Barroso, Hölzle, e Ranganathan 2019); PUE è la misura standard per valutare l’efficienza con cui un data center utilizza l’energia, che fornisce la percentuale di energia totale consumata dalla struttura divisa per l’energia utilizzata direttamente per le operazioni di elaborazione. Quando testato in tempo reale, il sistema AI ha prodotto notevoli guadagni rispetto alle innovazioni precedenti, riducendo l’energia di raffreddamento del 40% per un calo del 15% nel PUE totale, un nuovo record del sito. Il framework generalizzabile ha appreso rapidamente le dinamiche di raffreddamento in condizioni mutevoli che le regole statiche non potevano eguagliare. Questa svolta evidenzia il ruolo crescente dell’AI nella trasformazione della tecnologia moderna e nell’abilitazione di un futuro sostenibile.\n\nBarroso, Luiz André, Urs Hölzle, e Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#energy-consumption",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#energy-consumption",
    "title": "16  IA Sostenibile",
    "section": "16.3 Consumo Energetico",
    "text": "16.3 Consumo Energetico\n\n16.3.1 Comprendere le Esigenze Energetiche\nComprendere le esigenze energetiche per il training e il funzionamento dei modelli di intelligenza artificiale è fondamentale nel campo in rapida evoluzione dell’intelligenza artificiale. Con l’intelligenza artificiale che sta entrando in uso diffuso in molti nuovi campi (Bohr e Memarzadeh 2020; Sudhakar, Sze, e Karaman 2023), si prevede che la domanda di dispositivi e data center abilitati all’intelligenza artificiale esploderà. Questa comprensione ci aiuta a capire perché l’intelligenza artificiale, in particolare il deep learning, è spesso etichettata come ad alta intensità energetica.\n\nBohr, Adam, e Kaveh Memarzadeh. 2020. «The rise of artificial intelligence in healthcare applications». In Artificial Intelligence in Healthcare, 25–60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\nRequisiti Energetici per il Training dell’Intelligenza Artificiale\nIl training di sistemi di intelligenza artificiale complessi come i grandi modelli di deep learning può richiedere livelli sorprendentemente elevati di potenza di calcolo, con profonde implicazioni energetiche. Consideriamo il modello linguistico all’avanguardia di OpenAI GPT-3 come un esempio lampante. Questo sistema spinge i confini della generazione di testo attraverso algoritmi formati su enormi set di dati. Tuttavia, l’energia consumata da GPT-3 per un singolo ciclo di addestramento potrebbe rivaleggiare con l’utilizzo mensile di un’intera cittadina. Negli ultimi anni, questi modelli di intelligenza artificiale generativa hanno guadagnato sempre più popolarità, portando a un numero maggiore di modelli addestrati. Oltre all’aumento del numero di modelli, aumenterà anche il numero di parametri in questi modelli. La ricerca mostra che l’aumento delle dimensioni del modello (numero di parametri), delle dimensioni del set di dati e del calcolo utilizzato per l’addestramento migliora le prestazioni in modo fluido senza segni di saturazione (Kaplan et al. 2020). Notare come, in Figura 16.1, il “test loss” diminuisce man mano che ciascuno dei 3 aumenta.\n\n\n\n\n\n\nFigura 16.1: Le prestazioni migliorano con il calcolo, il set di dati e le dimensioni del modello. Fonte: Kaplan et al. (2020).\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, e Dario Amodei. 2020. «Scaling Laws for Neural Language Models». ArXiv preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nCosa determina requisiti così immensi? Durante l’addestramento, modelli come GPT-3 apprendono le proprie capacità elaborando continuamente enormi volumi di dati per regolare i parametri interni. La capacità di elaborazione che consente i rapidi progressi dell’IA contribuisce anche all’aumento del consumo di energia, soprattutto quando i set di dati e i modelli aumentano a dismisura. GPT-3 evidenzia una traiettoria costante nel campo in cui ogni balzo nella sofisticazione dell’IA risale a una potenza di calcolo e risorse sempre più sostanziali. Il suo predecessore, GPT-2, richiedeva un addestramento 10 volte inferiore per calcolare solo 1,5 miliardi di parametri, una differenza ora ridotta da grandezze in quanto GPT-3 comprende 175 miliardi di parametri. Mantenere questa traiettoria verso un’intelligenza artificiale sempre più capace solleva sfide future in termini di fornitura di energia e infrastrutture.\n\n\nUso Operativo dell’Energia\nLo sviluppo e l’addestramento di modelli di intelligenza artificiale richiedono un’enorme quantità di dati, potenza di calcolo ed energia. Tuttavia, l’implementazione e il funzionamento di tali modelli comportano anche significativi costi ricorrenti di risorse nel tempo. I sistemi di intelligenza artificiale sono ora integrati in vari settori e applicazioni e stanno entrando nella vita quotidiana di una fascia demografica in crescita. Il loro impatto cumulativo sull’energia operativa e sulle infrastrutture potrebbe eclissare l’addestramento iniziale del modello.\nQuesto concetto si riflette nella domanda di hardware di addestramento e inferenza nei data center e nell’edge. L’inferenza si riferisce all’uso di un modello addestrato per fare previsioni o decisioni su dati del mondo reale. Secondo una recente analisi McKinsey, la necessità di sistemi avanzati per addestrare modelli sempre più grandi sta crescendo rapidamente. Tuttavia, i calcoli di inferenza costituiscono già una parte dominante e crescente dei carichi di lavoro totali dell’intelligenza artificiale, come mostrato in Figura 16.2. L’esecuzione di inferenze in tempo reale con modelli addestrati, sia per la classificazione delle immagini, il riconoscimento vocale o l’analisi predittiva, richiede invariabilmente hardware di elaborazione come server e chip. Tuttavia, persino un modello che gestisce migliaia di richieste di riconoscimento facciale o query in linguaggio naturale ogni giorno è messo in ombra da piattaforme enormi come Meta. Dove l’inferenza su milioni di foto e video condivisi sui social media, i requisiti energetici dell’infrastruttura continuano a crescere!\n\n\n\n\n\n\nFigura 16.2: Dimensioni del mercato per l’hardware di inferenza e training. Fonte: McKinsey.\n\n\n\nGli algoritmi che alimentano assistenti intelligenti abilitati all’intelligenza artificiale, magazzini automatizzati, veicoli a guida autonoma, assistenza sanitaria personalizzata e altro hanno un’impronta energetica individuale marginale. Tuttavia, la proliferazione prevista di queste tecnologie potrebbe aggiungere centinaia di milioni di endpoint che eseguono algoritmi di intelligenza artificiale ininterrottamente, causando un aumento della scala dei loro requisiti energetici collettivi. Gli attuali guadagni di efficienza hanno bisogno di aiuto per controbilanciare questa crescita netta.\nSi prevede che l’intelligenza artificiale registrerà un tasso di crescita annuale del 37,3% tra il 2023 e il 2030. Tuttavia, applicando lo stesso tasso di crescita al computing operativo, entro il 2030 il fabbisogno energetico annuale dell’IA potrebbe moltiplicarsi fino a 1.000 volte. Quindi, mentre l’ottimizzazione del modello affronta un aspetto, l’innovazione responsabile deve anche considerare i costi totali del ciclo di vita su scale di distribuzione globali che erano inimmaginabili solo anni fa, ma che ora pongono sfide infrastrutturali e di sostenibilità.\n\n\n\n16.3.2 Data Center e il Loro Impatto\nCon l’aumento della domanda di servizi di IA, l’impatto dei data center sul consumo energetico dei sistemi di IA sta diventando sempre più importante. Sebbene queste strutture siano fondamentali per il progresso e la distribuzione dell’IA, contribuiscono in modo significativo al suo impatto energetico.\n\nScala\nI data center sono i cavalli da tiro essenziali che consentono le recenti richieste di elaborazione dei sistemi di IA avanzati. Ad esempio, i principali provider come Meta gestiscono enormi data center che si estendono fino alle dimensioni di più campi da calcio, ospitando centinaia di migliaia di server ad alta capacità ottimizzati per l’elaborazione parallela e la produttività dei dati.\nQueste enormi strutture forniscono l’infrastruttura per addestrare reti neurali complesse su vasti set di dati. Ad esempio, sulla base di informazioni trapelate, il modello linguistico GPT-4 di OpenAI è stato addestrato su data center Azure con oltre 25.000 GPU Nvidia A100, utilizzate ininterrottamente per oltre 90-100 giorni.\nInoltre, l’inferenza in tempo reale per applicazioni AI consumer su larga scala è resa possibile solo sfruttando le server farm all’interno dei data center. Servizi come Alexa, Siri e Google Assistant elaborano miliardi di richieste vocali al mese da parte di utenti in tutto il mondo, basandosi sul data center computing per una risposta a bassa latenza. In futuro, l’espansione di casi d’uso all’avanguardia come veicoli a guida autonoma, diagnosi di medicina di precisione e modelli di previsione climatica accurati richiederà risorse di calcolo significative da ottenere attingendo a vaste risorse di cloud computing on-demand dai data center. Alcune applicazioni emergenti, come le auto autonome, hanno rigidi vincoli di latenza e larghezza di banda. Sarà necessario collocare la potenza di calcolo a livello di data center sull’edge anziché sul cloud.\nI prototipi di ricerca del MIT hanno mostrato camion e auto con hardware di bordo che eseguono l’elaborazione AI in tempo reale dei dati dei sensori equivalenti a piccoli data center (Sudhakar, Sze, e Karaman 2023). Questi innovativi “data center su ruote” dimostrano come veicoli come i camion a guida autonoma potrebbero aver bisogno di un calcolo su scala di data center embedded a bordo per ottenere una latenza di sistema di millisecondi per la navigazione, sebbene probabilmente ancora integrato dalla connettività wireless 5G a data center cloud più potenti.\n\nSudhakar, Soumya, Vivienne Sze, e Sertac Karaman. 2023. «Data Centers on Wheels: Emissions From Computing Onboard Autonomous Vehicles». IEEE Micro 43 (1): 29–39. https://doi.org/10.1109/mm.2022.3219803.\nLa larghezza di banda, lo storage e le capacità di elaborazione richieste per abilitare questa futura tecnologia su larga scala dipenderanno in larga misura dai progressi nell’infrastruttura dei data center e dalle innovazioni algoritmiche dell’intelligenza artificiale.\n\n\nDomanda di Energia\nLa domanda di energia dei data center può essere approssimativamente suddivisa in 4 componenti: infrastruttura, rete, storage e server. In Figura 16.3, vediamo che l’infrastruttura dati (che include raffreddamento, illuminazione e controlli) e i server utilizzano la maggior parte del budget energetico totale dei data center negli Stati Uniti (Shehabi et al. 2016). Questa sezione suddivide la domanda di energia per i server e l’infrastruttura. Per quest’ultima, l’attenzione è rivolta ai sistemi di raffreddamento, poiché il raffreddamento è il fattore dominante nel consumo energetico nell’infrastruttura.\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin, Jonathan Koomey, Eric Masanet, Nathaniel Horner, Inês Azevedo, e William Lintner. 2016. «United states data center energy usage report».\n\n\n\n\n\n\nFigura 16.3: Consumo energetico dei data center negli Stati Uniti. Fonte: International Energy Agency (IEA).\n\n\n\n\nServer\nL’aumento del consumo energetico dei data center deriva principalmente dalla crescita esponenziale dei requisiti di elaborazione AI. Le macchine NVIDIA DGX H100 ottimizzate per il deep learning possono assorbire fino a 10,2 kW al picco. I principali provider gestiscono data center con centinaia o migliaia di questi nodi DGX ad alto consumo energetico collegati in rete per addestrare i più recenti modelli AI. Ad esempio, il supercomputer sviluppato per OpenAI è un singolo sistema con oltre 285.000 core CPU, 10.000 GPU e 400 gigabit al secondo di connettività di rete per ogni server GPU.\nI calcoli intensivi necessari per l’intera flotta densamente popolata di una struttura e l’hardware di supporto comportano che i data center assorbano decine di megawatt 24 ore su 24. Nel complesso, gli algoritmi AI avanzati continuano ad aumentare il consumo energetico dei data center man mano che vengono distribuiti più nodi DGX per tenere il passo con la crescita prevista della domanda di risorse di elaborazione AI nei prossimi anni.\n\n\nSistemi di Raffreddamento\nPer mantenere i server robusti alimentati al massimo della capacità e freschi, i data center richiedono una capacità di raffreddamento enorme per contrastare il calore prodotto da server densamente stipati, apparecchiature di rete e altro hardware che eseguono carichi di lavoro intensivi di elaborazione senza sosta. Con grandi data center che contengono migliaia di rack di server che operano a pieno regime, sono necessarie torri di raffreddamento e refrigeratori su scala industriale, che utilizzano energia pari al 30-40% dell’impronta elettrica totale del data center (Dayarathna, Wen, e Fan 2016). Di conseguenza, le aziende sono alla ricerca di metodi di raffreddamento alternativi. Ad esempio, il data center di Microsoft in Irlanda sfrutta un fiordo vicino per scambiare calore utilizzando oltre mezzo milione di galloni [1.9 milioni di litri] di acqua di mare al giorno.\nRiconoscendo l’importanza del raffreddamento efficiente dal punto di vista energetico, sono state introdotte innovazioni volte a ridurre questa domanda di energia. Tecniche come il raffreddamento gratuito, che utilizza fonti di aria o acqua esterne quando le condizioni sono favorevoli, e l’uso dell’intelligenza artificiale per ottimizzare i sistemi di raffreddamento sono esempi di come il settore si adatta. Queste innovazioni riducono il consumo energetico, i costi operativi e diminuiscono l’impatto ambientale. Tuttavia, gli aumenti esponenziali della complessità del modello AI continuano a richiedere più server e hardware di accelerazione che operano a un utilizzo più elevato, il che si traduce in una maggiore generazione di calore e in un’energia sempre maggiore utilizzata esclusivamente per il raffreddamento.\n\n\n\nL’impatto Ambientale\nL’impatto ambientale dei data center non è causato solo dal consumo energetico diretto del data center stesso (Siddik, Shehabi, e Marston 2021). Il funzionamento del data center comporta la fornitura di acqua trattata al data center e lo scarico delle acque reflue dal data center. Gli impianti idrici e di trattamento delle acque reflue sono i principali consumatori di elettricità.\n\nSiddik, Md Abu Bakar, Arman Shehabi, e Landon Marston. 2021. «The environmental footprint of data centers in the United States». Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, e Max Smolaks. 2022. «Uptime Institute Global Data Center Survey 2022». Uptime Institute.\nOltre al consumo di elettricità, ci sono molti altri aspetti dell’impatto ambientale di questi data center. Il consumo di acqua dei data center può portare a problemi di scarsità idrica, maggiori esigenze di trattamento delle acque e adeguate infrastrutture di scarico delle acque reflue. Inoltre, le materie prime necessarie per la costruzione e la trasmissione di rete hanno un impatto considerevole sull’ambiente e i componenti nei data center devono essere aggiornati e sottoposti a manutenzione. Laddove quasi il 50 percento dei server è stato aggiornato entro 3 anni di utilizzo, i cicli di aggiornamento hanno dimostrato di rallentare (Davis et al. 2022). Tuttavia, ciò genera notevoli rifiuti elettronici, che possono essere difficili da riciclare.\n\n\n\n16.3.3 Ottimizzazione Energetica\nIn definitiva, misurare e comprendere il consumo energetico dell’IA facilita l’ottimizzazione del consumo energetico.\nUn modo per ridurre il consumo energetico di una data quantità di lavoro computazionale è eseguirlo su hardware più efficiente dal punto di vista energetico. Ad esempio, i chip TPU possono essere più efficienti dal punto di vista energetico rispetto alle CPU quando si tratta di eseguire grandi calcoli tensoriali per l’IA, poiché le TPU possono eseguire tali calcoli molto più velocemente senza consumare molta più energia delle CPU. Un altro modo è quello di creare sistemi software consapevoli del consumo energetico e delle caratteristiche dell’applicazione. Buoni esempi sono lavori di sistema come Zeus (You, Chung, e Chowdhury 2023) e Perseus (Chung et al. 2023), entrambi caratterizzati dal compromesso tra tempo di calcolo e consumo energetico a vari livelli di un sistema di addestramento ML per ottenere una riduzione energetica senza rallentamento end-to-end. In realtà, costruire sia hardware che software a basso consumo energetico e combinarne i vantaggi dovrebbe essere promettente, insieme a framework open source (ad esempio, Zeus) che facilitano gli sforzi della comunità.\n\nYou, Jie, Jae-Won Chung, e Mosharaf Chowdhury. 2023. «Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training». In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), 119–39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, e Mosharaf Chowdhury. 2023. «Perseus: Removing Energy Bloat from Large Model Training». ArXiv preprint abs/2312.06902. https://arxiv.org/abs/2312.06902.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#carbon-footprint",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#carbon-footprint",
    "title": "16  IA Sostenibile",
    "section": "16.4 Impronta di Carbonio",
    "text": "16.4 Impronta di Carbonio\nLe enormi richieste di elettricità dei data center possono portare a significative esternalità ambientali in assenza di un’adeguata fornitura di energia rinnovabile. Molte strutture dipendono fortemente da fonti di energia non rinnovabili come carbone e gas naturale. Ad esempio, si stima che i data center producano fino al 2% delle emissioni globali totali di \\(\\textrm{CO}_2\\), il che sta colmando il divario con il settore aereo. Come accennato nelle sezioni precedenti, le richieste di elaborazione dell’intelligenza artificiale sono destinate ad aumentare. Le emissioni di questa ondata sono triplici. In primo luogo, si prevede che i data center aumenteranno di dimensioni (Liu et al. 2020). In secondo luogo, le emissioni durante il training sono destinate ad aumentare in modo significativo (Patterson et al. 2022). In terzo luogo, le chiamate di inferenza a questi modelli sono destinate ad aumentare drasticamente.\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, e Yun Tian. 2020. «Energy consumption and emission mitigation prediction based on data center traffic and PUE for global data centers». Global Energy Interconnection 3 (3): 272–82. https://doi.org/10.1016/j.gloei.2020.07.008.\nSenza azioni, questa crescita esponenziale della domanda rischia di aumentare ulteriormente l’impronta di carbonio dei data center a livelli insostenibili. I principali fornitori hanno promesso la neutralità carbonica e impegnato fondi per garantire energia pulita, ma i progressi rimangono incrementali rispetto ai piani di espansione complessivi del settore. Politiche di decarbonizzazione della rete più radicali e investimenti in energia rinnovabile potrebbero rivelarsi essenziali per contrastare l’impatto climatico dell’ondata imminente di nuovi data center volti a supportare la prossima generazione di IA.\n\n16.4.1 Definizione e Significato\nIl concetto di “impronta di carbonio” è emerso come una metrica chiave. Questo termine si riferisce alla quantità totale di gas serra, in particolare anidride carbonica, emessi direttamente o indirettamente da un individuo, un’organizzazione, un evento o un prodotto. Queste emissioni contribuiscono in modo significativo all’effetto serra, accelerando il riscaldamento globale e il cambiamento climatico. L’impronta di carbonio è misurata in termini di equivalenti di anidride carbonica (\\(\\textrm{CO}_2\\)e), consentendo un resoconto completo che include vari gas serra e il loro relativo impatto ambientale. Esempi di ciò applicato ad attività di ML su larga scala sono mostrati in Figura 16.4.\n\n\n\n\n\n\nFigura 16.4: Impronta di carbonio delle attività di ML su larga scala. Fonte: Wu et al. (2022).\n\n\n\nConsiderare l’impronta di carbonio è particolarmente importante nel rapido progresso dell’IA e nella sua integrazione in vari settori, mettendone in evidenza l’impatto ambientale. I sistemi di IA, in particolare quelli che comportano calcoli intensivi come il deep learning e l’elaborazione di dati su larga scala, sono noti per le loro notevoli richieste di energia. Questa energia, spesso ricavata dalle reti elettriche, potrebbe ancora basarsi prevalentemente sui combustibili fossili, il che comporta significative emissioni di gas serra.\nPrendiamo ad esempio l’addestramento di grandi modelli di IA come GPT-3 o complesse reti neurali. Questi processi richiedono un’immensa potenza di calcolo, in genere fornita dai data center. Il consumo energetico associato al funzionamento di questi centri, in particolare per attività ad alta intensità, comporta notevoli emissioni di gas serra. Gli studi hanno evidenziato che l’addestramento di un singolo modello di intelligenza artificiale può generare emissioni di carbonio paragonabili a quelle delle emissioni di più auto nel corso della loro vita, facendo luce sul costo ambientale dello sviluppo di tecnologie di intelligenza artificiale avanzate (Dayarathna, Wen, e Fan 2016). Figura 16.5 mostra un confronto tra le impronte di carbonio più basse e più alte, a partire da un volo di andata e ritorno tra New York e San Francisco, la vita media umana all’anno, la vita media americana all’anno, un’auto statunitense incluso il carburante nel corso della vita e un modello Transformer con ricerca di architettura neurale, che ha l’impronta più alta.\n\n\n\n\n\n\nFigura 16.5: Impronta di carbonio del modello NLP in libbre di \\(\\textrm{CO}_2\\) equivalente. Fonte: Dayarathna, Wen, e Fan (2016).\n\n\nDayarathna, Miyuru, Yonggang Wen, e Rui Fan. 2016. «Data Center Energy Consumption Modeling: A Survey». IEEE Communications Surveys &amp; Tutorials 18 (1): 732–94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nInoltre, l’impronta di carbonio dell’IA si estende oltre la fase operativa. L’intero ciclo di vita dei sistemi di IA, inclusa la produzione di hardware di elaborazione, l’energia utilizzata nei data center per il raffreddamento e la manutenzione e lo smaltimento dei rifiuti elettronici, contribuisce alla loro impronta di carbonio complessiva. Abbiamo discusso alcuni di questi aspetti in precedenza e discuteremo degli aspetti relativi ai rifiuti più avanti in questo capitolo.\n\n\n16.4.2 La Necessità di Consapevolezza e Azione\nComprendere l’impronta di carbonio dei sistemi di IA è fondamentale per diversi motivi. In primo luogo, è un passo avanti verso la mitigazione degli impatti del cambiamento climatico. Man mano che l’intelligenza artificiale continua a crescere e a permeare diversi aspetti delle nostre vite, il suo contributo alle emissioni globali di carbonio diventa una preoccupazione significativa. La consapevolezza di queste emissioni può informare le decisioni prese da sviluppatori, aziende, decisori politici e persino ingegneri e scienziati ML come noi per garantire un equilibrio tra innovazione tecnologica e responsabilità ambientale.\nInoltre, questa comprensione stimola la spinta verso la ‘Green AI’ (R. Schwartz et al. 2020). Questo approccio si concentra sullo sviluppo di tecnologie di intelligenza artificiale efficienti, potenti e sostenibili dal punto di vista ambientale. Incoraggia l’esplorazione di algoritmi ad alta efficienza energetica, l’utilizzo di fonti di energia rinnovabili nei data center e l’adozione di pratiche che riducano l’impatto ambientale complessivo dell’intelligenza artificiale.\nIn sostanza, l’impronta di carbonio è una considerazione essenziale nello sviluppo e nell’applicazione delle tecnologie di intelligenza artificiale. Man mano che l’intelligenza artificiale si evolve e le sue applicazioni diventano più diffuse, la gestione della sua impronta di carbonio è fondamentale per garantire che questo progresso tecnologico sia in linea con gli obiettivi più ampi di sostenibilità ambientale.\n\n\n16.4.3 Stima dell’Impronta di Carbonio dell’IA\nStimare l’impronta di carbonio dei sistemi di IA è fondamentale per comprendere il loro impatto ambientale. Ciò comporta l’analisi dei vari elementi che contribuiscono alle emissioni durante il ciclo di vita delle tecnologie di intelligenza artificiale e l’impiego di metodologie specifiche per quantificare accuratamente tali emissioni. Sono stati proposti molti metodi diversi per quantificare le emissioni di carbonio dell’apprendimento automatico.\nL’impronta di carbonio dell’intelligenza artificiale comprende diversi elementi chiave, ognuno dei quali contribuisce all’impatto ambientale complessivo. Innanzitutto, l’energia viene consumata durante le fasi di addestramento e operative del modello di intelligenza artificiale. La fonte di questa energia influenza pesantemente le emissioni di carbonio. Una volta addestrati, questi modelli, a seconda della loro applicazione e scala, continuano a consumare elettricità durante il funzionamento. Oltre alle considerazioni energetiche, anche l’hardware utilizzato stressa l’ambiente.\nL’impronta di carbonio varia in modo significativo in base alle fonti di energia utilizzate. La composizione delle fonti che forniscono l’energia utilizzata nella rete varia ampiamente a seconda della regione geografica e persino del momento in un singolo giorno! Ad esempio, negli Stati Uniti, circa il 60 percento dell’approvvigionamento energetico totale è ancora coperto da combustibili fossili. Le fonti di energia nucleare e rinnovabili coprono il restante 40 percento. Queste frazioni non sono costanti durante il giorno. Poiché la produzione di energia rinnovabile solitamente si basa su fattori ambientali, come la radiazione solare e i campi di pressione, non forniscono una fonte di energia costante.\nLa variabilità della produzione di energia rinnovabile è stata una sfida continua nell’uso diffuso di queste fonti. Guardando Figura 16.6, che mostra i dati per la rete europea, vediamo che dovrebbe essere in grado di produrre la quantità di energia richiesta durante il giorno. Mentre l’energia solare raggiunge il picco a metà giornata, quella eolica ha due picchi distinti, al mattino e alla sera. Attualmente, facciamo affidamento su metodi di produzione di energia basati su combustibili fossili e carbone per supplire alla mancanza di energia nei periodi in cui le energie rinnovabili non soddisfano il fabbisogno.\nÈ necessaria l’innovazione nelle soluzioni di accumulo di energia per consentire un uso costante di fonti di energia rinnovabile. Il carico energetico di base è attualmente soddisfatto dall’energia nucleare. Questa fonte energetica costante non produce direttamente emissioni di carbonio, ma deve essere più rapida per adattarsi alla variabilità delle fonti energetiche rinnovabili. Le aziende tecnologiche come Microsoft hanno mostrato interesse per le fonti di energia nucleare per alimentare i loro data center. Poiché la domanda dei data center è più costante rispetto alla domanda delle normali famiglie, l’energia nucleare potrebbe essere utilizzata come fonte energetica dominante.\n\n\n\n\n\n\nFigura 16.6: Fonti di energia e capacità di generazione. Fonte: Energy Charts.\n\n\n\nInoltre, la produzione e lo smaltimento dell’hardware AI aumentano l’impronta di carbonio. La produzione di dispositivi informatici specializzati, come GPU e CPU, richiede molta energia e risorse. Questa fase spesso si basa su fonti energetiche che contribuiscono alle emissioni di gas serra. Il processo di produzione dell’industria elettronica è stato identificato come una delle otto grandi catene di fornitura responsabili di oltre il 50 percento delle emissioni globali (Challenge 2021). Inoltre, lo smaltimento a fine vita di questo hardware, che può portare a rifiuti elettronici, ha anche implicazioni ambientali. Come accennato, i server hanno un ciclo di aggiornamento di circa 3-5 anni. Di questi rifiuti elettronici, attualmente solo il 17,4 percento viene raccolto e riciclato correttamente.. Le emissioni di carbonio di questi rifiuti elettronici hanno mostrato un aumento di oltre il 50 percento tra il 2014 e il 2020 (Singh e Ogunseitan 2022).\n\nChallenge, WEF Net-Zero. 2021. «The Supply Chain Opportunity». In World Economic Forum: Geneva, Switzerland.\n\nSingh, Narendra, e Oladele A. Ogunseitan. 2022. «Disentangling the worldwide web of e-waste and climate change co-benefits». Circular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\nCome è chiaro da quanto sopra, è necessaria un’adeguata analisi del ciclo di vita per descrivere tutti gli aspetti rilevanti delle emissioni causate dall’IA. Un altro metodo è la contabilità del carbonio, che valuta la quantità di emissioni di anidride carbonica direttamente e indirettamente associate alle operazioni di IA. Questa misura utilizza in genere equivalenti di \\(\\textrm{CO}_2\\), consentendo un modo standardizzato di segnalare e valutare le emissioni.\n\n\n\n\n\n\nEsercizio 16.1: Impronta di Carbonio dell’IA\n\n\n\n\n\nSapevate che i modelli di IA all’avanguardia che potreste utilizzare hanno un impatto ambientale? Questo esercizio approfondirà l’“impronta di carbonio” di un sistema di IA. Imparerete come le richieste energetiche dei data center, il training dei grandi modelli di IA e persino la produzione di hardware contribuiscono alle emissioni di gas serra. Discuteremo perché è fondamentale essere consapevoli di questo impatto e impareremo metodi per stimare l’impronta di carbonio dei progetti di IA. Prepariamoci ad esplorare l’intersezione tra IA e sostenibilità ambientale!",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#beyond-carbon-footprint",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#beyond-carbon-footprint",
    "title": "16  IA Sostenibile",
    "section": "16.5 Oltre l’Impronta di Carbonio",
    "text": "16.5 Oltre l’Impronta di Carbonio\nL’attuale attenzione alla riduzione delle emissioni di carbonio e del consumo energetico dei sistemi di intelligenza artificiale affronta un aspetto cruciale della sostenibilità. Tuttavia, la produzione di semiconduttori e hardware che consentono l’intelligenza artificiale comporta anche gravi impatti ambientali che ricevono relativamente meno attenzione pubblica. Costruire e gestire un impianto di fabbricazione di semiconduttori all’avanguardia, o “fab”, ha notevoli requisiti di risorse e sottoprodotti inquinanti che vanno oltre un’ampia impronta di carbonio.\nAd esempio, una fabbrica all’avanguardia che produce chip come quelli a 5 nm potrebbe richiedere fino a quattro milioni di galloni di acqua pura al giorno. Questo consumo di acqua si avvicina a ciò che una città di mezzo milione di persone richiederebbe per tutte le esigenze. L’approvvigionamento di questa risorsa pone costantemente un’enorme pressione sulle falde acquifere e sui bacini idrici locali, soprattutto nelle regioni già sottoposte a stress idrico che ospitano molti centri di produzione ad alta tecnologia.\nInoltre, oltre 250 sostanze chimiche pericolose uniche vengono utilizzate in varie fasi della produzione di semiconduttori all’interno delle fab (Mills e Le Hunte 1997). Tra queste, solventi volatili come acido solforico, acido nitrico e acido fluoridrico, insieme ad arsina, fosfina e altre sostanze altamente tossiche. Per impedire lo scarico di queste sostanze chimiche sono necessari ampi controlli di sicurezza e infrastrutture di trattamento delle acque reflue per evitare la contaminazione del suolo e rischi per le comunità circostanti. Qualsiasi manipolazione chimica impropria o fuoriuscita imprevista comporta conseguenze disastrose.\n\nMills, Andrew, e Stephen Le Hunte. 1997. «An overview of semiconductor photocatalysis». J. Photochem. Photobiol., A 108 (1): 1–35. https://doi.org/10.1016/s1010-6030(97)00118-4.\nOltre al consumo di acqua e ai rischi chimici, le operazioni di fabbricazione dipendono anche dall’approvvigionamento di metalli rari, generano tonnellate di rifiuti pericolosi e possono ostacolare la biodiversità locale. Questa sezione analizzerà questi impatti critici ma meno discussi. Con vigilanza e investimenti nella sicurezza, i danni derivanti dalla produzione di semiconduttori possono essere contenuti pur consentendo il progresso tecnologico. Tuttavia, ignorare questi problemi esternalizzati aggraverà i danni ecologici e i rischi per la salute nel lungo periodo.\n\n16.5.1 Utilizzo e Stress Idrico\nLa fabbricazione di semiconduttori è un processo che richiede un consumo di acqua incredibilmente elevato. In base a un articolo del 2009, un tipico wafer di silicio da 300 mm richiede 8.328 litri di acqua, di cui 5.678 litri sono acqua ultrapura (Cope 2009). Oggi, una tipica fabbrica può utilizzare fino a quattro milioni di galloni di acqua pura. Per gestire una struttura, si prevede che l’ultima fabbrica di TSMC in Arizona utilizzerà 8,9 milioni di galloni al giorno, ovvero quasi il 3 percento dell’attuale produzione idrica della città. Per mettere le cose in prospettiva, Intel e Quantis hanno scoperto che oltre il 97% del loro consumo diretto di acqua è attribuito alle operazioni di produzione di semiconduttori all’interno delle loro strutture di fabbricazione (Cooper et al. 2011).\n\nCope, Gord. 2009. «Pure water, semiconductors and the recession». Global Water Intelligence 10 (10).\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien Humbert, e Lindsay Lessard. 2011. «A semiconductor company’s examination of its water footprint approach». In Proceedings of the 2011 IEEE International Symposium on Sustainable Systems and Technology, 1–6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\nQuest’acqua viene ripetutamente utilizzata per rimuovere i contaminanti nelle fasi di pulizia e funge anche da refrigerante e fluido vettore nei processi di ossidazione termica, deposizione chimica e planarizzazione chimico-meccanica. Nei mesi estivi di punta, ciò equivale approssimativamente al consumo giornaliero di acqua di una città con una popolazione di mezzo milione di persone.\nNonostante si trovi in regioni con acqua a sufficienza, l’uso intensivo può depauperare gravemente le falde acquifere e i bacini di drenaggio locali. Ad esempio, la città di Hsinchu a Taiwan ha subito affondamenti delle falde acquifere e intrusioni di acqua marina nelle falde acquifere a causa dell’eccessivo pompaggio per soddisfare le richieste di approvvigionamento idrico della fabbrica della Taiwan Semiconductor Manufacturing Company (TSMC). Nelle aree interne con scarsità d’acqua come l’Arizona, sono necessari massicci apporti di acqua per supportare le fabbriche nonostante i bacini già esistenti.\nLo scarico di acqua dalle fabbriche rischia di contaminare l’ambiente oltre all’esaurimento se non trattato correttamente. Sebbene gran parte dello scarico venga riciclato all’interno della fabbrica, i sistemi di purificazione filtrano comunque metalli, acidi e altri contaminanti che possono inquinare fiumi e laghi se non gestiti con cautela (Prakash, Callahan, et al. 2023). Questi fattori rendono essenziale la gestione dell’uso dell’acqua quando si mitigano impatti più ampi sulla sostenibilità.\n\n\n16.5.2 Uso di Sostanze Chimiche Pericolose\nLa moderna fabbricazione di semiconduttori comporta la lavorazione di molte sostanze chimiche altamente pericolose in condizioni estreme di calore e pressione (Kim et al. 2018). Le principali sostanze chimiche utilizzate includono:\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk Park, Sangjun Choi, Seungwon Kim, Kwonchul Ha, e Won Kim. 2018. «Chemical use in the semiconductor manufacturing industry». Int. J. Occup. Env. Heal. 24 (3-4): 109–18. https://doi.org/10.1080/10773525.2018.1519957.\n\nAcidi forti: Gli acidi fluoridrico, solforico, nitrico e cloridrico corrodono rapidamente gli ossidi e altri contaminanti superficiali, ma presentano anche pericoli di tossicità. Le fab possono utilizzare migliaia di tonnellate di questi acidi all’anno e l’esposizione accidentale può essere fatale per i lavoratori.\nSolventi: Solventi chiave come xilene, metanolo e metilisobutilchetone (MIBK) gestiscono i fotoresist dissolvibili, ma hanno effetti negativi sulla salute come irritazione della pelle/degli occhi ed effetti narcotici se maneggiati in modo improprio. Creano anche rischi di esplosione e inquinamento atmosferico.\nGas tossici: Le miscele di gas contenenti arsina (AsH3), fosfina (PH3), diborano (B2H6), germano (GeH4), ecc., sono alcune delle sostanze chimiche più letali utilizzate nelle fasi di doping e deposizione di vapore. Esposizioni minime possono causare avvelenamento, danni ai tessuti e persino la morte senza un trattamento rapido.\nComposti clorurati: Le vecchie formulazioni di planarizzazione chimico-meccanica incorporavano percloroetilene, tricloroetilene e altri solventi clorurati, che da allora sono stati vietati a causa dei loro effetti cancerogeni e dell’impatto sullo strato di ozono. Tuttavia, il loro rilascio precedente minaccia ancora le falde acquifere circostanti.\n\nProtocolli di gestione rigorosi, dispositivi di protezione per i lavoratori, ventilazione, sistemi di filtraggio/lavaggio, serbatoi di contenimento secondari e meccanismi di smaltimento specializzati sono essenziali laddove queste sostanze chimiche vengono utilizzate per ridurre al minimo i pericoli per la salute, le esplosioni, l’aria e le fuoriuscite ambientali (Wald e Jones 1987). Ma occasionalmente si verificano ancora errori umani e guasti alle apparecchiature, evidenziando perché la riduzione delle intensità chimiche di fabbricazione è uno sforzo di sostenibilità continuo.\n\nWald, Peter H., e Jeffrey R. Jones. 1987. «Semiconductor manufacturing: An introduction to processes and hazards». Am. J. Ind. Med. 11 (2): 203–21. https://doi.org/10.1002/ajim.4700110209.\n\n\n16.5.3 Esaurimento delle Risorse\nSebbene il silicio costituisca la base, sulla Terra c’è una scorta pressoché infinita di silicio. Infatti, il silicio è il secondo elemento più abbondante trovato nella crosta terrestre, rappresentando il 27,7% della massa totale della crosta. Solo l’ossigeno supera il silicio in abbondanza all’interno della crosta. Pertanto, il silicio non è necessario da considerare per l’esaurimento delle risorse. Tuttavia, i vari metalli e materiali speciali che consentono il processo di fabbricazione dei circuiti integrati e forniscono proprietà specifiche devono ancora essere scoperti. Mantenere le scorte di queste risorse è fondamentale, ma è minacciato dalla disponibilità finita e dalle influenze geopolitiche (Nakano 2021).\n\nNakano, Jane. 2021. The geopolitics of critical minerals supply chains. JSTOR.\n\nChen, H.-W. 2006. «Gallium, Indium, and Arsenic Pollution of Groundwater from a Semiconductor Manufacturing Area of Taiwan». B. Environ. Contam. Tox. 77 (2): 289–96. https://doi.org/10.1007/s00128-006-1062-3.\nGallio, indio e arsenico sono ingredienti vitali nella formazione di semiconduttori composti ultra-efficienti nei chip ad altissima velocità adatti per applicazioni 5G e AI (Chen 2006). Tuttavia, questi elementi rari hanno depositi naturali relativamente scarsi che si stanno esaurendo. Lo United States Geological Survey ha inserito l’indio nella sua lista delle materie prime a rischio più critiche, stimando una fornitura globale sostenibile per meno di 15 anni alla crescita attuale della domanda (Davies 2011).\nL’elio è richiesto in grandi volumi per le fabbriche di nuova generazione per consentire un raffreddamento preciso dei wafer durante il funzionamento. Ma la relativa rarità dell’elio e il fatto che una volta rilasciato nell’atmosfera, fuoriesce rapidamente dalla Terra rendono il mantenimento delle scorte di elio estremamente impegnativo a lungo termine (Davies 2011). Secondo le US National Academies, in questo mercato scarsamente scambiato si stanno già verificando notevoli aumenti dei prezzi e shock dell’offerta.\n\nJha, A. R. 2014. Rare Earth Materials: Properties and Applications. CRC Press. https://doi.org/10.1201/b17045.\nAltri rischi includono il controllo della Cina sul 90% degli elementi delle terre rare fondamentali per la produzione di materiali semiconduttori (Jha 2014). Qualsiasi problema nella catena di fornitura o controversia commerciale può portare a catastrofiche carenze di materie prime, data la mancanza di alternative attuali. Insieme alle carenze di elio, risolvere la disponibilità limitata e lo squilibrio geografico nell’accesso agli ingredienti essenziali rimane una priorità del settore per la sostenibilità.\n\n\n16.5.4 Generazione di Rifiuti Pericolosi\nLe fabbriche di semiconduttori generano tonnellate di rifiuti pericolosi ogni anno come sottoprodotti dei vari processi chimici (Grossman 2007). I principali flussi di rifiuti includono:\n\nGrossman, Elizabeth. 2007. High tech trash: Digital devices, hidden toxics, and human health. Island press.\n\nRifiuti gassosi: I sistemi di ventilazione delle fab catturano gas nocivi come arsina, fosfina e germano e li filtrano per evitare l’esposizione dei lavoratori. Tuttavia, ciò produce quantità significative di gas condensato pericoloso che necessita di un trattamento specializzato.\nCOV: I composti organici volatili come xilene, acetone e metanolo sono ampiamente utilizzati come solventi fotoresistenti e vengono evaporati come emissioni durante la cottura, l’incisione e lo stripping. I COV pongono problemi di tossicità e richiedono sistemi di lavaggio per impedirne il rilascio.\nAcidi esausti: Acidi forti come acido solforico, acido fluoridrico e acido nitrico si esauriscono nelle fasi di pulizia e incisione, trasformandosi in una zuppa corrosiva e tossica che può reagire pericolosamente, rilasciando calore e fumi se mescolata.\nFanghi: Il trattamento delle acque degli effluenti scaricati contiene metalli pesanti concentrati, residui acidi e contaminanti chimici. I sistemi di filtropressa separano questi fanghi pericolosi.\nTorta di filtrazione: I sistemi di filtrazione gassosa generano torte appiccicose di diverse tonnellate di composti assorbiti pericolosi che richiedono contenimento.\n\nSenza adeguate procedure di movimentazione, serbatoi di stoccaggio, materiali di imballaggio e contenimento secondario, lo smaltimento improprio di uno qualsiasi di questi flussi di rifiuti può causare pericolose fuoriuscite, esplosioni e rilasci nell’ambiente. Gli enormi volumi significano che anche le fabbriche ben gestite producono tonnellate di rifiuti pericolosi anno dopo anno, che richiedono un trattamento esteso.\n\n\n16.5.5 Impatti sulla Biodiversità\n\nInterruzione e Frammentazione dell’Habitat\nLe fabbriche di semiconduttori necessitano di ampie aree contigue per ospitare camere bianche, strutture di supporto, stoccaggio di sostanze chimiche, trattamento dei rifiuti e infrastrutture ausiliarie. Lo sviluppo di questi vasti spazi edificati smantella inevitabilmente gli habitat esistenti, danneggiando biomi sensibili che potrebbero aver impiegato decenni per svilupparsi. Ad esempio, la costruzione di un nuovo modulo di fabbricazione potrebbe radere al suolo gli ecosistemi forestali locali da cui specie come gufi maculati e alci dipendono per sopravvivere. La rimozione totale di tali habitat minaccia gravemente le popolazioni di animali selvatici che dipendono da quei terreni.\nInoltre, condutture, canali idrici, sistemi di scarico dell’aria e dei rifiuti, strade di accesso, torri di trasmissione e altre infrastrutture di supporto frammentano gli habitat indisturbati rimanenti. Gli animali che si spostano quotidianamente per cibo, acqua e deposizione delle uova possono vedere i loro percorsi di migrazione bloccati da queste barriere umane fisiche che dividono in due i corridoi precedentemente naturali.\n\n\nDisturbi della Vita Acquatica\nCon le fabbriche di semiconduttori che consumano milioni di galloni di acqua ultra pura ogni giorno, accedere e scaricare tali volumi rischia di alterare l’idoneità degli ambienti acquatici circostanti che ospitano pesci, piante acquatiche, anfibi e altre specie. Se la fabbrica attinge alle falde acquifere come fonte di approvvigionamento primaria, un prelievo eccessivo a tassi insostenibili può impoverire i laghi o portare all’essiccazione dei corsi d’acqua man mano che i livelli dell’acqua scendono (Davies 2011).\n\nDavies, Emma. 2011. «Endangered elements: Critical thinking». https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\nLeRoy Poff, N, MM Brinson, e JW Day. 2002. «Aquatic ecosystems & Global climate change». Pew Center on Global Climate Change.\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, e Samuel B. Fey. 2019. «Fish die-offs are concurrent with thermal extremes in north temperate lakes». Nat. Clim. Change 9 (8): 637–41. https://doi.org/10.1038/s41558-019-0520-y.\nInoltre, lo scarico di acque reflue a temperature più elevate per raffreddare le apparecchiature di fabbricazione può modificare le condizioni del fiume a valle attraverso l’inquinamento termico. Le variazioni di temperatura oltre le soglie per cui si sono evolute le specie autoctone possono interrompere i cicli riproduttivi. L’acqua più calda contiene anche meno ossigeno disciolto, fondamentale per sostenere la vita di piante e animali acquatici (LeRoy Poff, Brinson, e Day 2002). In combinazione con tracce di contaminanti residui che sfuggono ai sistemi di filtrazione, l’acqua scaricata può trasformare cumulativamente gli ambienti rendendoli molto meno abitabili per gli organismi sensibili (Till et al. 2019).\n\n\nEmissioni Chimiche e Aeree\nMentre le moderne fabbriche di semiconduttori mirano a contenere gli scarichi di aria e sostanze chimiche attraverso sistemi di filtraggio estesi, alcuni livelli di emissioni spesso persistono, aumentando i rischi per la flora e la fauna vicine. Gli inquinanti atmosferici possono essere trasportati sottovento, tra cui composti organici volatili (COV), composti di ossido di azoto (NOx), particolato proveniente da scarichi operativi delle fabbriche ed emissioni di carburante delle centrali elettriche.\nPoiché i contaminanti permeano i terreni e le fonti d’acqua locali, la fauna selvatica che ingerisce cibo e acqua contaminati ingerisce sostanze tossiche, che la ricerca dimostra possono ostacolare la funzione cellulare, i tassi di riproduzione e la longevità, avvelenando lentamente gli ecosistemi (Hsu et al. 2016).\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting Chan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, e Yu-Min Tzou. 2016. «Accumulation of heavy metals and trace elements in fluvial sediments received effluents from traditional and semiconductor industries». Scientific Reports 6 (1): 34250. https://doi.org/10.1038/srep34250.\nAllo stesso modo, le fuoriuscite accidentali di sostanze chimiche e la gestione impropria dei rifiuti, che rilasciano acidi e metalli pesanti nel terreno, possono influire notevolmente sulla capacità di ritenzione e lisciviazione. La flora, come le vulnerabili orchidee autoctone adattate a substrati poveri di nutrienti, può subire morie quando viene a contatto con sostanze chimiche di deflusso estranee che alterano il pH e la permeabilità del terreno. Un’analisi ha scoperto che una singola fuoriuscita di 500 galloni di acido nitrico ha portato all’estinzione regionale di una rara specie di muschio nell’anno successivo, quando l’effluente acido ha raggiunto gli habitat forestali vicini. Tali eventi di contaminazione innescano reazioni a catena attraverso la rete interconnessa della vita. Pertanto, protocolli rigorosi sono essenziali per evitare scarichi e deflussi pericolosi.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#life-cycle-analysis",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#life-cycle-analysis",
    "title": "16  IA Sostenibile",
    "section": "16.6 Analisi del Ciclo di Vita",
    "text": "16.6 Analisi del Ciclo di Vita\nPer comprendere l’impatto ambientale olistico dei sistemi di intelligenza artificiale è necessario un approccio completo che consideri l’intero ciclo di vita di queste tecnologie. Il “Life Cycle Analysis (LCA)” analisi del ciclo di vita si riferisce a un quadro metodologico utilizzato per quantificare gli impatti ambientali in tutte le fasi del ciclo di vita di un prodotto o sistema, dall’estrazione delle materie prime allo smaltimento a fine vita. L’applicazione dell’LCA ai sistemi di intelligenza artificiale può aiutare a identificare le aree prioritarie da prendere di mira per ridurre l’impatto ambientale complessivo.\n\n16.6.1 Fasi del Ciclo di Vita di un Sistema di IA\nIl ciclo di vita di un sistema di intelligenza artificiale può essere suddiviso in quattro fasi chiave:\n\nFase di Progettazione: Include l’energia e le risorse utilizzate nella ricerca e nello sviluppo delle tecnologie di intelligenza artificiale. Comprende le risorse computazionali utilizzate per lo sviluppo e il test degli algoritmi che contribuiscono alle emissioni di carbonio.\nFase di Produzione: Questa fase prevede la produzione di componenti hardware come schede grafiche, processori e altri dispositivi di elaborazione necessari per l’esecuzione degli algoritmi di intelligenza artificiale. La produzione di questi componenti spesso comporta un notevole consumo di energia per l’estrazione dei materiali, l’elaborazione e le emissioni di gas serra.\nFase di Utilizzo: La fase successiva più dispendiosa in termini di energia riguarda l’uso operativo dei sistemi di intelligenza artificiale. Include l’elettricità consumata nei data center per l’addestramento e l’esecuzione delle reti neurali e l’alimentazione delle applicazioni per gli utenti finali. Questa è probabilmente una delle fasi con il più alto consumo di carbonio.\nFase di Smaltimento: Questa fase finale riguarda gli aspetti di fine vita dei sistemi di intelligenza artificiale, tra cui il riciclaggio e lo smaltimento dei rifiuti elettronici generati da hardware obsoleto o non funzionante oltre la sua durata utile.\n\n\n\n16.6.2 Impatto Ambientale in Ogni Fase\nProgettazione e Produzione\nL’impatto ambientale durante queste fasi iniziali di vita include emissioni derivanti dall’uso di energia e dall’esaurimento delle risorse derivanti dall’estrazione di materiali per la produzione di hardware. Al centro dell’hardware AI ci sono semiconduttori, principalmente silicio, utilizzati per realizzare i circuiti integrati nei processori e nei chip di memoria. Questa produzione di hardware si basa su metalli come il rame per i cablaggi, l’alluminio per gli involucri e varie plastiche e compositi per altri componenti. Utilizza anche terre rare e leghe specializzate, elementi come neodimio, terbio e ittrio, utilizzati in piccole ma vitali quantità. Ad esempio, la creazione di GPU si basa su rame e alluminio. Allo stesso tempo, i chip utilizzano terre rare, che è il processo di estrazione che può generare notevoli emissioni di carbonio e danni all’ecosistema.\nFase di Utilizzo\nL’AI calcola la maggior parte delle emissioni nel ciclo di vita a causa del continuo elevato consumo di energia, in particolare per il training e l’esecuzione di modelli. Ciò include emissioni dirette e indirette derivanti dall’uso di elettricità e dalla generazione di energia di rete non rinnovabile. Gli studi stimano che l’addestramento di modelli complessi può avere un’impronta di carbonio paragonabile alle emissioni fino a cinque auto nel corso della loro vita.\nFase di Smaltimento\nGli impatti della fase di smaltimento includono l’inquinamento dell’aria e dell’acqua dovuto a materiali tossici nei dispositivi, le sfide associate al riciclaggio di componenti elettronici complessi e la contaminazione in caso di gestione impropria. I composti nocivi derivanti dalla combustione dei rifiuti elettronici vengono rilasciati nell’atmosfera. Allo stesso tempo, la perdita di piombo, mercurio e altri materiali dalle discariche comporta rischi di contaminazione del suolo e delle falde acquifere se non adeguatamente controllata. L’implementazione di un efficace riciclaggio dei componenti elettronici è fondamentale.\n\n\n\n\n\n\nEsercizio 16.2: Monitoraggio delle Emissioni ML\n\n\n\n\n\nIn questo esercizio, esploreremo l’impatto ambientale dell’addestramento di modelli di machine learning. Utilizzeremo CodeCarbon per monitorare le emissioni, apprenderemo l’analisi del “Life Cycle Analysis (LCA)” per comprendere l’impronta di carbonio dell’IA ed esploreremo strategie per rendere lo sviluppo del tuo modello ML più rispettoso dell’ambiente. Alla fine, si sarà in grado di monitorare le emissioni di carbonio dei modelli e iniziare a implementare pratiche più ecologiche nei progetti.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#challenges-in-lca",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#challenges-in-lca",
    "title": "16  IA Sostenibile",
    "section": "16.7 Sfide nell’LCA",
    "text": "16.7 Sfide nell’LCA\n\n16.7.1 Mancanza di Coerenza e Standard\nUna delle principali sfide che l’analisi del “life cycle analysis (LCA)” [ciclo di vita ] deve affrontare per i sistemi di intelligenza artificiale è la necessità di standard e framework metodologici coerenti. A differenza di categorie di prodotti come i materiali da costruzione, che hanno sviluppato standard internazionali per LCA tramite ISO 14040, non esistono linee guida stabilite per analizzare l’impatto ambientale di tecnologie informatiche complesse come l’intelligenza artificiale.\nQuesta assenza di uniformità significa che i ricercatori fanno ipotesi diverse e scelte metodologiche variabili. Ad esempio, uno studio del 2021 dell’Università del Massachusetts Amherst (Strubell, Ganesh, e McCallum 2019) ha analizzato le emissioni del ciclo di vita di diversi modelli di elaborazione del linguaggio naturale, ma ha preso in considerazione solo l’utilizzo delle risorse computazionali per il training e ha omesso gli impatti sulla produzione di hardware. Uno studio più completo del 2020 condotto dai ricercatori della Stanford University ha incluso stime delle emissioni derivanti dalla produzione di server, processori e altri componenti pertinenti, seguendo uno standard LCA allineato a ISO per l’hardware dei computer. Tuttavia, queste scelte divergenti nei confini del sistema e negli approcci contabili riducono la robustezza e impediscono confronti tra risultati simili.\nFramework e protocolli standardizzati su misura per gli aspetti unici dei sistemi di intelligenza artificiale e rapidi cicli di aggiornamento fornirebbero maggiore coerenza. Ciò potrebbe consentire a ricercatori e sviluppatori di comprendere i punti critici ambientali, confrontare le opzioni tecnologiche e monitorare con precisione i progressi nelle iniziative di sostenibilità nel campo dell’intelligenza artificiale. Gruppi industriali e organismi di normazione internazionali come IEEE o ACM dovrebbero dare priorità all’affrontare questa lacuna metodologica.\n\n\n16.7.2 Lacune nei Dati\nUn’altra sfida fondamentale per una valutazione completa del ciclo di vita dei sistemi di intelligenza artificiale sono le lacune sostanziali nei dati, in particolare per quanto riguarda gli impatti sulla catena di fornitura a monte e i flussi di rifiuti elettronici a valle. La maggior parte degli studi esistenti si concentra strettamente sulle emissioni della fase di apprendimento o di utilizzo derivanti dalle richieste di potenza di calcolo, che tralasciano una parte significativa delle emissioni nel corso della vita (Gupta et al. 2022).\nAd esempio, esistono pochi dati pubblici delle aziende che quantificano l’uso di energia e le emissioni derivanti dalla produzione di componenti hardware specializzati che abilitano l’intelligenza artificiale, tra cui GPU di fascia alta, chip ASIC, unità a stato solido e altro. Spesso i ricercatori si affidano a fonti secondarie o medie generiche del settore per approssimare gli impatti sulla produzione. Analogamente, in media, c’è una trasparenza limitata sul destino a valle una volta che i sistemi di intelligenza artificiale vengono scartati dopo 4-5 anni di durata utile.\nMentre i livelli di generazione di rifiuti elettronici possono essere stimati, i dettagli sulle perdite di materiali pericolosi, sui tassi di riciclaggio e sui metodi di smaltimento per i componenti complessi sono estremamente incerti senza una migliore documentazione aziendale o requisiti di reporting normativi.\nLa necessità di dati dettagliati sul consumo di risorse computazionali per l’addestramento di diversi tipi di modelli rende difficili calcoli affidabili delle emissioni per parametro o per query anche per la fase di utilizzo. Esistono tentativi di creare inventari del ciclo di vita che stimino il fabbisogno energetico medio per le attività chiave dell’intelligenza artificiale (Henderson et al. 2020; Anthony, Kanding, e Selvan 2020), ma la variabilità tra configurazioni hardware, algoritmi e incertezza dei dati di input rimane estremamente elevata. Inoltre, i dati sull’intensità di carbonio in tempo reale, fondamentali per tracciare con precisione l’impronta di carbonio operativa, devono essere migliorati in molte posizioni geografiche, rendendo gli strumenti esistenti per le emissioni di carbonio operative mere approssimazioni basate sui valori di intensità di carbonio media annuale.\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, e Joelle Pineau. 2020. «Towards the systematic reporting of the energy and carbon footprints of machine learning». The Journal of Machine Learning Research 21 (1): 10039–81.\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, e Raghavendra Selvan. 2020. ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems.\nLa sfida è che strumenti come CodeCarbon e ML \\(\\textrm{CO}_2\\) sono, nella migliore delle ipotesi, solo approcci ad hoc, nonostante le loro buone intenzioni. Colmare le lacune reali dei dati con divulgazioni più rigorose sulla sostenibilità aziendale e una rendicontazione obbligatoria dell’impatto ambientale sarà fondamentale per comprendere e gestire gli impatti climatici complessivi dell’IA.\n\n\n16.7.3 Rapido Ritmo di Evoluzione\nL’evoluzione estremamente rapida dei sistemi di intelligenza artificiale pone ulteriori sfide nel mantenere aggiornate le valutazioni del ciclo di vita e nel tenere conto degli ultimi progressi hardware e software. Gli algoritmi di base, i chip specializzati, i framework e l’infrastruttura tecnica alla base dell’intelligenza artificiale hanno tutti fatto progressi eccezionalmente rapidi, con nuovi sviluppi che hanno rapidamente reso obsoleti i sistemi precedenti.\nAd esempio, nel deep learning, le nuove architetture di reti neurali che raggiungono prestazioni significativamente migliori su benchmark chiave o nuovi hardware ottimizzati come i chip TPU di Google possono cambiare completamente un modello “medio” in meno di un anno. Questi rapidi cambiamenti rendono rapidamente obsoleti gli studi LCA una tantum per il monitoraggio accurato delle emissioni derivanti dalla progettazione, esecuzione o smaltimento dell’intelligenza artificiale più recente.\nTuttavia, le risorse e l’accesso necessari per aggiornare continuamente gli LCA devono essere migliorati. Rifare frequentemente il lavoro, inventari del ciclo di vita ad alta intensità di dati e modelli di impatto per rimanere aggiornati con lo stato dell’arte dell’intelligenza artificiale è probabilmente irrealizzabile per molti ricercatori e organizzazioni. Tuttavia, analisi aggiornate potrebbero rilevare hotspot ambientali man mano che algoritmi e chip di silicio continuano a evolversi rapidamente.\nCiò presenta difficoltà nel bilanciare la precisione dinamica attraverso una valutazione continua con vincoli pragmatici. Alcuni ricercatori hanno proposto metriche proxy semplificate come il monitoraggio delle generazioni hardware nel tempo o l’utilizzo di benchmark rappresentativi come un set oscillante di paletti per confronti relativi, sebbene la granularità possa essere sacrificata. Nel complesso, la sfida del cambiamento rapido richiederà soluzioni metodologiche innovative per evitare di sottostimare gli oneri ambientali in evoluzione dell’IA.\n\n\n16.7.4 Complessità della Catena di Fornitura\nInfine, le complesse e spesso opache catene di fornitura associate alla produzione dell’ampia gamma di componenti hardware specializzati che abilitano l’intelligenza artificiale pongono sfide per la modellazione completa del ciclo di vita. Lo “stato-dell’arte” dellIA si basa su progressi all’avanguardia nell’elaborazione di chip, schede grafiche, archiviazione dati, apparecchiature di rete e altro ancora. Tuttavia, tracciare le emissioni e l’uso delle risorse attraverso le reti a livelli di fornitori globalizzati per tutti questi componenti è estremamente difficile.\nAd esempio, le unità di elaborazione grafica NVIDIA dominano gran parte dell’hardware di elaborazione dell’intelligenza artificiale, ma l’azienda si affida a diversi fornitori discreti in Asia e oltre per produrre GPU. Molte aziende a ogni livello di fornitore scelgono di mantenere privati i dati ambientali a livello di stabilimento, il che potrebbe abilitare completamente LCA robuste. Ottenere la trasparenza end-to-end su più livelli di fornitori in aree geografiche diverse con protocolli di divulgazione e normative variabili pone barriere nonostante sia fondamentale per la definizione completa dei confini. Ciò diventa ancora più complesso quando si tenta di modellare acceleratori hardware emergenti come le “tensor processing units (TPU)” [unità di elaborazione tensoriale], le cui reti di produzione devono ancora essere rese pubbliche.\nSenza la volontà dei giganti della tecnologia di richiedere e consolidare la divulgazione dei dati sull’impatto ambientale da tutte le loro catene di fornitura di elettronica globali, rimarrà una notevole incertezza sulla quantificazione dell’impronta del ciclo di vita completo dell’abilitazione hardware AI. Una maggiore visibilità della catena di fornitura abbinata a quadri di reporting sulla sostenibilità standardizzati che affrontino specificamente gli input complessi dell’AI promettono di arricchire gli LCA e dare priorità alle riduzioni dell’impatto ambientale.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#sustainable-design-and-development",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#sustainable-design-and-development",
    "title": "16  IA Sostenibile",
    "section": "16.8 Progettazione e Sviluppo Sostenibili",
    "text": "16.8 Progettazione e Sviluppo Sostenibili\n\n16.8.1 Principi di Sostenibilità\nMan mano che l’impatto dell’IA sull’ambiente diventa sempre più evidente, l’attenzione alla progettazione e allo sviluppo sostenibili nell’IA sta acquisendo importanza. Ciò comporta l’incorporazione di principi di sostenibilità nella progettazione dell’IA, lo sviluppo di modelli a risparmio energetico e l’integrazione di queste considerazioni in tutta la pipeline di sviluppo dell’IA. C’è una crescente necessità di considerare le implicazioni di sostenibilità e sviluppare principi per guidare l’innovazione responsabile. Di seguito è riportato un set di principi fondamentali. I principi fluiscono dalle fondamenta concettuali all’esecuzione pratica ai fattori di supporto all’implementazione; i principi forniscono una prospettiva del ciclo completo sull’incorporamento della sostenibilità nella progettazione e nello sviluppo dell’IA.\nLifecycle Thinking: Incoraggiare i progettisti a considerare l’intero ciclo di vita dei sistemi di IA, dalla raccolta e preelaborazione dei dati allo sviluppo del modello, al training, all’implementazione e al monitoraggio. L’obiettivo è garantire che la sostenibilità sia presa in considerazione in ogni fase. Ciò include l’utilizzo di hardware a risparmio energetico, la priorità alle fonti di energia rinnovabili e la pianificazione del riutilizzo o del riciclaggio di modelli dismessi.\nA Prova di Futuro: Progettare sistemi di intelligenza artificiale che anticipino esigenze e cambiamenti futuri può migliorare la sostenibilità. Ciò può comportare la creazione di modelli adattabili tramite apprendimento per trasferimento e architetture modulari. Include anche la capacità di pianificazione per aumenti previsti di scala operativa e volumi di dati.\nEfficienza e Minimalismo: Questo principio si concentra sulla creazione di modelli di intelligenza artificiale che raggiungano i risultati desiderati con il minimo utilizzo di risorse possibile. Comporta la semplificazione di modelli e algoritmi per ridurre i requisiti computazionali. Tecniche specifiche includono la potatura di parametri ridondanti, la quantizzazione e la compressione di modelli e la progettazione di architetture di modelli efficienti, come quelle discusse nel capitolo Ottimizzazioni.\nIntegrazione del Lifecycle Assessment (LCA): [Valutazione del Ciclo di Vita ] L’analisi degli impatti ambientali durante lo sviluppo e l’implementazione dei cicli di vita evidenzia le pratiche non sostenibili in anticipo. I team possono quindi apportare modifiche anziché scoprire i problemi in ritardo, quando sono più difficili da affrontare. L’integrazione di questa analisi nel flusso di progettazione standard evita di creare problemi ereditati di sostenibilità.\nAllineamento degli Incentivi: Gli incentivi economici e politici dovrebbero promuovere e premiare lo sviluppo sostenibile dell’IA. Questi possono includere sovvenzioni governative, iniziative aziendali, standard di settore e mandati accademici per la sostenibilità. Gli incentivi allineati consentono alla sostenibilità di essere inglobata nella cultura dell’IA.\nMetriche e Obiettivi di Sostenibilità: È importante stabilire metriche chiaramente definite che misurino fattori di sostenibilità come l’uso del carbonio e l’efficienza energetica. Stabilire obiettivi chiari per queste metriche fornisce linee guida concrete per i team per sviluppare sistemi di IA responsabili. Il monitoraggio delle prestazioni sulle metriche nel tempo mostra i progressi verso gli obiettivi di sostenibilità prefissati.\nEquità, Trasparenza e Responsabilità: I sistemi di IA sostenibili dovrebbero essere equi, trasparenti e responsabili. I modelli dovrebbero essere imparziali, con processi di sviluppo trasparenti e meccanismi per l’audit e la risoluzione dei problemi. Ciò crea fiducia nel pubblico e consente l’identificazione di pratiche non sostenibili.\nCollaborazione Interdisciplinare: I ricercatori di intelligenza artificiale che collaborano con scienziati e ingegneri ambientali possono dare vita a sistemi innovativi ad alte prestazioni ma rispettosi dell’ambiente. L’unione di competenze provenienti da diversi campi fin dall’inizio dei progetti consente di incorporare il pensiero sostenibile nel processo di progettazione dell’intelligenza artificiale.\nIstruzione e Consapevolezza: Workshop, programmi di formazione e programmi di studio che riguardano la sostenibilità dell’intelligenza artificiale accrescono la consapevolezza tra la prossima generazione di professionisti. Ciò fornisce agli studenti le conoscenze per sviluppare un’intelligenza artificiale che riduca al minimo gli impatti negativi sulla società e sull’ambiente. Inculcare questi valori fin dall’inizio plasma i professionisti e le culture aziendali di domani.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#green-ai-infrastructure",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#green-ai-infrastructure",
    "title": "16  IA Sostenibile",
    "section": "16.9 Infrastruttura di IA Green",
    "text": "16.9 Infrastruttura di IA Green\nGreen AI rappresenta un approccio trasformativo all’IA che incorpora la sostenibilità ambientale come principio fondamentale nella progettazione e nel ciclo di vita del sistema di IA (R. Schwartz et al. 2020). Questo cambiamento è guidato dalla crescente consapevolezza dell’impatto ecologico e dell’impronta di carbonio significativa delle tecnologie di IA, in particolare il processo di elaborazione intensiva di modelli di ML complessi.\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, e Oren Etzioni. 2020. «Green AI». Commun. ACM 63 (12): 54–63. https://doi.org/10.1145/3381831.\nL’essenza di Green AI risiede nel suo impegno ad allineare il progresso dell’IA con gli obiettivi di sostenibilità in termini di efficienza energetica, utilizzo di energia rinnovabile e riduzione dei rifiuti. L’introduzione degli ideali di Green AI riflette la crescente responsabilità nel settore tecnologico verso la tutela ambientale e le pratiche tecnologiche etiche. Va oltre le ottimizzazioni tecniche verso una valutazione olistica del ciclo di vita su come i sistemi di IA influenzano le metriche di sostenibilità. Stabilire nuovi standard per un’IA ecologicamente consapevole apre la strada alla coesistenza armoniosa di progresso tecnologico e salute planetaria.\n\n16.9.1 Sistemi di IA a Risparmio Energetico\nL’efficienza energetica nei sistemi di intelligenza artificiale è un pilastro della Green AI, che mira a ridurre le richieste di energia tradizionalmente associate allo sviluppo e alle operazioni di intelligenza artificiale. Questo passaggio verso pratiche di intelligenza artificiale attente al risparmio energetico è fondamentale per affrontare le preoccupazioni ambientali sollevate dal campo in rapida espansione dell’intelligenza artificiale. Concentrandosi sull’efficienza energetica, i sistemi di intelligenza artificiale possono diventare più sostenibili, riducendo il loro impatto ambientale e aprendo la strada a un loro utilizzo più responsabile.\nCome abbiamo discusso in precedenza, l’addestramento e il funzionamento dei modelli di intelligenza artificiale, in particolare quelli su larga scala, sono noti per il loro elevato consumo energetico, che deriva dall’architettura del modello ad alta intensità di calcolo e dall’affidamento a grandi quantità di dati di addestramento. Ad esempio, si stima che l’addestramento di un grande modello di rete neurale all’avanguardia possa avere un’impronta di carbonio di 284 tonnellate, equivalente alle emissioni di 5 auto nel corso della loro vita (Strubell, Ganesh, e McCallum 2019).\n\nStrubell, Emma, Ananya Ganesh, e Andrew McCallum. 2019. «Energy and Policy Considerations for Deep Learning in NLP». In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3645–50. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\nPer affrontare le enormi richieste di energia, ricercatori e sviluppatori stanno esplorando attivamente metodi per ottimizzare i sistemi di intelligenza artificiale per una migliore efficienza energetica mantenendo al contempo l’accuratezza e le prestazioni del modello. Ciò include tecniche come quelle che abbiamo discusso nei capitoli sulle ottimizzazioni del modello, sull’intelligenza artificiale efficiente e sull’accelerazione hardware:\n\nDistillazione della conoscenza per trasferire la conoscenza da grandi modelli di intelligenza artificiale a versioni in miniatura\nApprocci di quantizzazione e potatura che riducono le complessità computazionali e spaziali\nNumeri a bassa precisione: riduzione della precisione matematica senza influire sulla qualità del modello\nHardware specializzato come TPU, chip neuromorfici ottimizzati esplicitamente per un’elaborazione efficiente dell’intelligenza artificiale\n\nUn esempio è il lavoro di Intel su Q8BERT: quantizzazione del modello di linguaggio BERT con interi a 8 bit, che porta a una riduzione di 4 volte delle dimensioni del modello con una perdita di accuratezza minima (Zafrir et al. 2019). La spinta verso un’intelligenza artificiale efficiente dal punto di vista energetico non è solo uno sforzo tecnico: ha implicazioni tangibili nel mondo reale. Sistemi più performanti riducono i costi operativi e l’impatto ambientale dell’intelligenza artificiale, rendendola accessibile per un’ampia distribuzione su dispositivi mobili ed edge. Apre inoltre la strada alla democratizzazione dell’IA e mitiga i pregiudizi ingiusti che possono emergere da un accesso non uniforme alle risorse informatiche tra regioni e comunità. Perseguire un’IA efficiente dal punto di vista energetico è quindi fondamentale per creare un futuro equo e sostenibile con l’IA.\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, e Moshe Wasserblat. 2019. «Q8BERT: Quantized 8Bit BERT». In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS), 36–39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\n16.9.2 Infrastruttura di IA Sostenibile\nL’infrastruttura AI sostenibile include i framework fisici e tecnologici che supportano i sistemi AI, concentrandosi sulla sostenibilità ambientale. Ciò implica la progettazione e la gestione dell’infrastruttura AI per ridurre al minimo l’impatto ecologico, conservare le risorse e ridurre le emissioni di carbonio. L’obiettivo è creare un ecosistema sostenibile per l’AI che si allinei con obiettivi ambientali più ampi..\nI data center green sono fondamentali per l’infrastruttura AI sostenibile, ottimizzati per l’efficienza energetica e spesso alimentati da fonti di energia rinnovabili. Questi data center impiegano tecnologie di raffreddamento avanzate (Ebrahimi, Jones, e Fleischer 2014), design di server a risparmio energetico (Uddin e Rahman 2012) e sistemi di gestione intelligenti (Buyya, Beloglazov, e Abawajy 2010) per ridurre il consumo di energia. Il passaggio a un’infrastruttura informatica verde implica anche l’adozione di hardware a risparmio energetico, come processori ottimizzati per l’AI che offrono prestazioni elevate con requisiti energetici inferiori, di cui abbiamo discusso nel capitolo Accelerazione IA. Questi sforzi riducono collettivamente l’impronta di carbonio delle operazioni di intelligenza artificiale su larga scala.\n\nEbrahimi, Khosrow, Gerard F. Jones, e Amy S. Fleischer. 2014. «A review of data center cooling technology, operating conditions and the corresponding low-grade waste heat recovery opportunities». Renewable Sustainable Energy Rev. 31 (marzo): 622–38. https://doi.org/10.1016/j.rser.2013.12.007.\n\nUddin, Mueen, e Azizah Abdul Rahman. 2012. «Energy efficiency and low carbon enabler green IT framework for data centers considering green metrics». Renewable Sustainable Energy Rev. 16 (6): 4078–94. https://doi.org/10.1016/j.rser.2012.03.014.\n\nBuyya, Rajkumar, Anton Beloglazov, e Jemal Abawajy. 2010. «Energy-Efficient Management of Data Center Resources for Cloud Computing: A Vision, Architectural Elements, and Open Challenges». https://arxiv.org/abs/1006.0308.\n\nChua, L. 1971. «Memristor-The missing circuit element». #IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\nL’integrazione di fonti di energia rinnovabili, come energia solare, eolica e idroelettrica, nell’infrastruttura di intelligenza artificiale è importante per la sostenibilità ambientale (Chua 1971). Molte aziende tecnologiche e istituti di ricerca stanno investendo in progetti di energia rinnovabile per alimentare i loro data center. Ciò non solo aiuta a rendere le operazioni di intelligenza artificiale carbon neutral, ma promuove anche un’adozione più ampia di energia pulita. L’utilizzo di fonti di energia rinnovabili mostra chiaramente l’impegno per la responsabilità ambientale nel settore dell’intelligenza artificiale.\nLa sostenibilità si estende anche ai materiali e all’hardware utilizzati nella creazione di sistemi di intelligenza artificiale. Ciò implica la scelta di materiali ecocompatibili, l’adozione di pratiche di riciclaggio e la garanzia di uno smaltimento responsabile dei rifiuti elettronici. Sono in corso sforzi per sviluppare componenti hardware più sostenibili, tra cui chip a risparmio energetico progettati per attività specifiche del dominio (come gli acceleratori di IA) e materiali ecocompatibili nella produzione di dispositivi (Cenci et al. 2021; Irimia-Vladu 2014). Anche il ciclo di vita di questi componenti è un punto focale, con iniziative volte a estendere la durata di vita dell’hardware e a promuovere il riciclaggio e il riutilizzo.\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula Cristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, e Pablo R. Dias. 2021. «Eco-Friendly ElectronicsA Comprehensive Review». Adv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\nIrimia-Vladu, Mihai. 2014. «“Green” electronics: Biodegradable and biocompatible materials and devices for sustainable future». Chem. Soc. Rev. 43 (2): 588–610. https://doi.org/10.1039/c3cs60235d.\nSebbene si stiano facendo progressi nell’infrastruttura di IA sostenibile, permangono delle sfide, come gli elevati costi della tecnologia verde e la necessità di standard globali nelle pratiche sostenibili. Le direzioni future includono un’adozione più diffusa di energia verde, ulteriori innovazioni nell’hardware a risparmio energetico e la collaborazione internazionale su politiche di IA sostenibile. Perseguire un’infrastruttura di IA sostenibile non è solo uno sforzo tecnico, ma un approccio olistico che comprende aspetti ambientali, economici e sociali, assicurando che l’IA avanzi in armonia con la salute del nostro pianeta.\n\n\n16.9.3 Framework e Strumenti\nL’accesso ai framework e agli strumenti giusti è essenziale per implementare in modo efficace le pratiche di intelligenza artificiale verde. Queste risorse sono progettate per aiutare sviluppatori e ricercatori a creare sistemi di IA più efficienti dal punto di vista energetico e rispettosi dell’ambiente. Vanno da librerie software ottimizzate per un basso consumo energetico a piattaforme che facilitano lo sviluppo di applicazioni di IA sostenibili.\nDiverse librerie software e ambienti di sviluppo sono specificamente pensati per l’intelligenza artificiale verde. Questi strumenti spesso includono funzionalità per ottimizzare i modelli di IA per ridurre il loro carico computazionale e, di conseguenza, il loro consumo energetico. Ad esempio, le librerie in PyTorch e TensorFlow che supportano la potatura del modello, la quantizzazione e le architetture di reti neurali efficienti consentono agli sviluppatori di creare sistemi di intelligenza artificiale che richiedono meno potenza di elaborazione ed energia. Inoltre, comunità open source come la Green Carbon Foundation stanno creando una metrica centralizzata dell’intensità di carbonio e sviluppando software per l’elaborazione consapevole delle emissioni di carbonio.\nGli strumenti di monitoraggio dell’energia sono fondamentali per l’intelligenza artificiale verde, poiché consentono agli sviluppatori di misurare e analizzare il consumo energetico dei loro sistemi. Fornendo informazioni dettagliate su dove e come viene utilizzata l’energia, questi strumenti consentono agli sviluppatori di prendere decisioni informate sull’ottimizzazione dei loro modelli per una migliore efficienza energetica. Ciò può comportare modifiche nella progettazione dell’algoritmo, nella selezione dell’hardware, nella selezione del software di cloud computing o nei parametri operativi. Figura 16.7 è uno screenshot di una dashboard del consumo energetico fornita dalla piattaforma di servizi cloud di Microsoft.\n\n\n\n\n\n\nFigura 16.7: Dashboard del consumo energetico di Microsoft Azure. Fonte: Will Buchanan.\n\n\n\nCon la crescente integrazione di fonti di energia rinnovabile nelle operazioni di IA, i framework che facilitano questo processo stanno diventando sempre più importanti. Questi framework aiutano a gestire l’approvvigionamento energetico da fonti rinnovabili come l’energia solare o eolica, assicurando che i sistemi di IA possano funzionare in modo efficiente con input energetici fluttuanti.\nOltre all’efficienza energetica, gli strumenti di valutazione della sostenibilità aiutano a valutare l’impatto ambientale più ampio dei sistemi di IA. Questi strumenti possono analizzare fattori come l’impronta di carbonio delle operazioni di IA, l’impatto del ciclo di vita dei componenti hardware (Gupta et al. 2022) e la sostenibilità complessiva dei progetti di IA (Prakash, Callahan, et al. 2023).\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, e Carole-Jean Wu. 2022. «Act: designing sustainable computer systems with an architectural carbon modeling tool». In Proceedings of the 49th Annual International Symposium on Computer Architecture, 784–99. ACM. https://doi.org/10.1145/3470496.3527408.\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. «CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs». In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\nLa disponibilità e lo sviluppo continuo di framework e strumenti di IA Green sono fondamentali per promuovere pratiche di sostenibili. Fornendo le risorse necessarie a sviluppatori e ricercatori, questi strumenti facilitano la creazione di sistemi più rispettosi dell’ambiente e incoraggiano un più ampio cambiamento verso la sostenibilità nella comunità tecnologica. Man mano che l’IA Green continua a evolversi, questi framework e strumenti svolgeranno un ruolo fondamentale nel dare forma a un futuro più sostenibile per l’IA. As Green AI continues to evolve, these frameworks and tools will play a vital role in shaping a more sustainable future for AI.\n\n\n16.9.4 Benchmark e Classifiche\nBenchmark e classifiche sono importanti per guidare i progressi nell’IA Green, poiché forniscono modi standardizzati per misurare e confrontare diversi metodi. Benchmark ben progettati che catturano metriche rilevanti su efficienza energetica, emissioni di carbonio e altri fattori di sostenibilità consentono alla comunità di monitorare i progressi in modo equo e significativo.\nEsistono ampi benchmark per tracciare le prestazioni del modello di IA, come quelli discussi nel capitolo Benchmarking. Tuttavia, esiste una chiara e urgente necessità di ulteriori benchmark standardizzati incentrati su parametri di sostenibilità come efficienza energetica, emissioni di carbonio e impatto ecologico complessivo. La comprensione dei costi ambientali dell’IA deve attualmente essere migliorata da una mancanza di trasparenza e misura standardizzata attorno a questi fattori.\nSforzi emergenti come ML.ENERGY Leaderboard, che fornisce risultati di benchmarking delle prestazioni e del consumo energetico per la generazione di testo di modelli linguistici di grandi dimensioni (LLM), aiutano a migliorare la comprensione del costo energetico dell’implementazione GenAI.\nCome con qualsiasi benchmark, quelli di IA Green devono rappresentare scenari di utilizzo e carichi di lavoro realistici. I benchmark che si concentrano strettamente su metriche facilmente manipolabili possono portare a guadagni a breve termine, ma non riescono a riflettere gli ambienti di produzione effettivi in cui sono necessarie misure di efficienza e sostenibilità più olistiche. La comunità dovrebbe continuare ad espandere i benchmark per coprire diversi casi d’uso.\nUn’adozione più ampia di suite di benchmark comuni da parte degli operatori del settore accelererà l’innovazione nell’IA Green consentendo un confronto più semplice delle tecniche tra le organizzazioni. I benchmark condivisi abbassano la barriera per dimostrare i vantaggi di sostenibilità di nuovi strumenti e best practice. uttavia, quando si progettano benchmark per l’intero settore, è necessario prestare attenzione a questioni come proprietà intellettuale, privacy e sensibilità commerciale. Le iniziative per sviluppare set di dati di riferimento aperti per la valutazione dell’IA Green possono aiutare a promuovere una partecipazione più ampia.\nMan mano che i metodi e l’infrastruttura per l’IA Green continuano a maturare, la comunità deve rivedere la progettazione dei benchmark per garantire che le suite esistenti catturino bene nuove tecniche e scenari. Monitorare il panorama in evoluzione attraverso aggiornamenti e revisioni regolari dei benchmark sarà importante per mantenere confronti rappresentativi nel tempo. Gli sforzi della comunità per la cura dei benchmark possono consentire suite di benchmark sostenibili che resistano alla prova del tempo. Suite di benchmark complete di proprietà di comunità di ricerca o terze parti neutrali come MLCommons possono incoraggiare una più ampia partecipazione e standardizzazione.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#case-study-google-4ms",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#case-study-google-4ms",
    "title": "16  IA Sostenibile",
    "section": "16.10 Caso di Studio: 4M di Google",
    "text": "16.10 Caso di Studio: 4M di Google\nNegli ultimi dieci anni, l’intelligenza artificiale è passata rapidamente dalla ricerca accademica ai sistemi di produzione su larga scala che alimentano numerosi prodotti e servizi Google. Poiché i modelli e i carichi di lavoro dell’IA sono cresciuti esponenzialmente in termini di dimensioni e richieste di elaborazione, sono emerse preoccupazioni circa il loro consumo energetico e l’impatto ambientale. Alcuni ricercatori hanno previsto una crescita incontrollata dell’appetito energetico del ML che potrebbe superare le efficienze ottenute da algoritmi e hardware migliorati (Thompson et al. 2021).\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, e Gabriel F. Manso. 2021. «Deep Learning’s Diminishing Returns: The Cost of Improvement is Becoming Unsustainable». IEEE Spectr. 58 (10): 50–55. https://doi.org/10.1109/mspec.2021.9563954.\nTuttavia, i dati di produzione di Google rivelano una storia diversa: l’IA rappresenta un costante 10-15% del consumo energetico totale dell’azienda dal 2019 al 2021. Questo caso di studio analizza come Google ha applicato un approccio sistematico sfruttando quattro best practice, quelle che definiscono le “4 M”: “Model efficiency”, “Machine optimization”, “Mechanization through cloud computing” e “Mapping to green locations” [efficienza del modello, ottimizzazione delle macchine, meccanizzazione tramite cloud computing e mappatura di luoghi Green], per piegare la curva delle emissioni dai carichi di lavoro dell’IA.\nLa portata dell’utilizzo dell’IA da parte di Google lo rende un caso di studio ideale. Solo nel 2021, l’azienda ha addestrato modelli come il GLam da 1,2 trilioni di parametri. Analizzare come l’applicazione dell’IA è stata abbinata a rapidi guadagni di efficienza in questo ambiente ci aiuta a fornire un modello logico che il più ampio campo dell’IA seguirà.\nPubblicando in modo trasparente statistiche dettagliate sull’uso dell’energia, adottando tassi di acquisto di cloud senza emissioni di carbonio e fonti rinnovabili e altro ancora, insieme alle sue innovazioni tecniche, Google ha consentito ai ricercatori esterni di misurare i progressi in modo accurato. Il loro studio nell’ACM CACM (Patterson et al. 2022) evidenzia come l’approccio multiforme dell’azienda dimostri che le previsioni di consumo energetico dell’IA incontrollabili possono essere superate concentrando gli sforzi ingegneristici su modelli di sviluppo sostenibile. Il ritmo dei miglioramenti suggerisce anche che i guadagni di efficienza dell’ML sono appena iniziati.\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, e Jeff Dean. 2022. «The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink». Computer 55 (7): 18–28. https://doi.org/10.1109/mc.2022.3148714.\n\n16.10.1 Le 4M Best Practice di Google\nPer ridurre le emissioni derivanti dai carichi di lavoro IA in rapida espansione, gli ingegneri di Google hanno sistematicamente identificato quattro aree di best practice, denominate “4 M”, in cui le ottimizzazioni potrebbero sommarsi per ridurre l’impatto ambientale del ML:\n\nModello: La selezione di architetture di modelli di intelligenza artificiale efficienti può ridurre i calcoli di 5-10 volte senza alcuna perdita di qualità. Google ha svolto ricerche approfondite sullo sviluppo di modelli “sparsi” e sulla ricerca di architetture neurali per creare modelli più efficienti come Evolved Transformer e Primer.\nMacchina: L’utilizzo di hardware ottimizzato per l’IA rispetto ai sistemi generici migliora le prestazioni per watt di 2-5 volte. Le Tensor Processing Unit (TPU) di Google hanno portato a un’efficienza di carbonio 5-13 volte migliore rispetto alle GPU non ottimizzate per il ML.\nMeccanizzazione: Sfruttando i sistemi di cloud computing progettati per un utilizzo elevato rispetto ai tradizionali data center on-premise, i costi energetici si riducono di 1,4-2 volte. Google cita l’efficacia dell’utilizzo energetico del suo data center come superiore alle medie del settore.\nMappa: La scelta di ubicazioni per data center dotate di elettricità a basse emissioni di carbonio riduce le emissioni lorde di altre 5-10 volte. Google fornisce mappe in tempo reale che evidenziano la percentuale di energia rinnovabile utilizzata dalle sue strutture.\n\nInsieme, queste pratiche hanno creato drastici guadagni di efficienza composti. Ad esempio, l’ottimizzazione del modello Transformer AI su TPU in una sede di data center sostenibile ha ridotto il consumo di energia dell’83%. Ha ridotto le emissioni di \\(\\textrm{CO}_2\\) di un fattore di 747.\n\n\n16.10.2 Risultati Significativi\nNonostante la crescita esponenziale nell’adozione dell’IA nei prodotti e nei servizi, gli sforzi di Google per migliorare l’efficienza del carbonio del ML hanno prodotto guadagni misurabili, contribuendo a limitare l’appetito energetico complessivo. Un punto dati chiave che evidenzia questo progresso è che i carichi di lavoro dell’intelligenza artificiale sono rimasti stabili al 10%-15% del consumo energetico totale dell’azienda dal 2019 al 2021. Man mano che l’IA è diventata parte integrante di più offerte Google, i cicli di elaborazione complessivi dedicati ad essa sono cresciuti in modo sostanziale. Tuttavia, l’efficienza negli algoritmi, nell’hardware specializzato, nella progettazione dei data center e nella geografia flessibile ha consentito alla sostenibilità di tenere il passo, con l’IA che rappresenta solo una frazione dell’elettricità totale del data center in anni di espansione.\nAltri casi di studio sottolineano come un focus ingegneristico sui modelli di sviluppo dell’intelligenza artificiale sostenibile abbia consentito rapidi miglioramenti della qualità di pari passo con i guadagni ambientali. Ad esempio, il modello di elaborazione del linguaggio naturale GPT-3 è stato considerato all’avanguardia a metà del 2020. Tuttavia, il suo successore GLaM ha migliorato la precisione riducendo al contempo le esigenze di elaborazione del training e utilizzando energia più pulita nei data center, riducendo le emissioni di CO2 di un fattore 14 in soli 18 mesi di evoluzione del modello.\nAnalogamente, Google ha scoperto che le precedenti speculazioni pubblicate non hanno colto nel segno sull’appetito energetico del ML per fattori da 100 a 100.000X a causa della mancanza di metriche del mondo reale. Tracciando in modo trasparente l’impatto dell’ottimizzazione, Google sperava di motivare l’efficienza evitando al contempo estrapolazioni sovrastimate sul pedaggio ambientale del ML.\nQuesti casi di studio basati sui dati mostrano come aziende come Google stiano indirizzando i progressi dell’IA verso traiettorie sostenibili e migliorando l’efficienza per superare la crescita dell’adozione. Con ulteriori sforzi in termini di analisi del ciclo di vita, ottimizzazione dell’inferenza ed espansione delle energie rinnovabili, le aziende possono puntare ad accelerare i progressi, dimostrando che il potenziale pulito del ML è stato appena sbloccato dagli attuali guadagni.\n\n\n16.10.3 Ulteriori Miglioramenti\nSebbene Google abbia compiuto progressi misurabili nel limitare l’impatto ambientale delle sue operazioni di intelligenza artificiale, l’azienda riconosce che ulteriori guadagni in termini di efficienza saranno essenziali per un’innovazione responsabile, data la continua espansione della tecnologia.\nUn’area di attenzione è mostrare come i progressi siano spesso erroneamente considerati come un aumento dell’insostenibilità informatica, come la ricerca di architettura neurale (NAS) per trovare modelli ottimizzati, che stimolano risparmi a valle, superando i costi iniziali. Nonostante spenda più energia nella scoperta di modelli piuttosto che nell’ingegneria manuale, la NAS riduce le emissioni nel corso del ciclo di vita producendo progetti efficienti richiamabili su innumerevoli applicazioni.\nInoltre, l’analisi rivela che concentrare gli sforzi di sostenibilità sull’ottimizzazione lato server e data center ha senso, dato il consumo energetico dominante rispetto ai dispositivi consumer. Sebbene Google riduca gli impatti dell’inferenza su processori come i telefoni cellulari, la priorità è il miglioramento dei cicli di training e dell’approvvigionamento di energie rinnovabili per data center per ottenere il massimo effetto.\nA tal fine, i progressi di Google nel mettere in comune strutture cloud progettate in modo inefficiente evidenziano il valore della scala e della centralizzazione. Man mano che sempre più carichi di lavoro si allontanano dai server inefficienti in sede, la priorità data dai giganti di Internet alle energie rinnovabili, con Google e Facebook abbinate al 100% dalle energie rinnovabili rispettivamente dal 2017 e dal 2020, sblocca tagli alle emissioni composte.\nInsieme, questi sforzi sottolineano che, sebbene non sia possibile adagiarsi sugli allori, l’approccio multiforme di Google dimostra che i miglioramenti dell’efficienza dell’IA stanno solo accelerando. Le iniziative intersettoriali relative alla valutazione del ciclo di vita, ai modelli di sviluppo attenti alle emissioni di carbonio, alla trasparenza e all’abbinamento della crescente domanda di IA con la fornitura di energia elettrica pulita aprono la strada a un’ulteriore flessione della curva man mano che l’adozione aumenta. I risultati dell’azienda spingono il settore più ampio a replicare queste attività di sostenibilità integrate.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#embedded-ai-internet-of-trash",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#embedded-ai-internet-of-trash",
    "title": "16  IA Sostenibile",
    "section": "16.11 IA Embedded - Internet of Trash",
    "text": "16.11 IA Embedded - Internet of Trash\nSebbene molta attenzione sia stata rivolta a rendere più sostenibili gli immensi data center che alimentano l’IA, una preoccupazione altrettanto urgente è lo spostamento delle capacità dell’IA in dispositivi edge e endpoint intelligenti. L’IA edge/embedded consente una reattività quasi in tempo reale senza dipendenze dalla connettività. Riduce inoltre le esigenze di larghezza di banda di trasmissione. Tuttavia, l’aumento di dispositivi minuscoli comporta altri rischi.\nI minuscoli computer, microcontrollori e ASIC personalizzati che alimentano l’intelligenza edge affrontano limitazioni di dimensioni, costi e potenza che escludono le GPU di fascia alta utilizzate nei data center. Invece, richiedono algoritmi ottimizzati e circuiti estremamente compatti ed efficienti dal punto di vista energetico per funzionare senza problemi. Tuttavia, l’ingegneria per questi fattori di forma microscopici apre rischi in termini di obsolescenza programmata, smaltibilità e spreco. Figura 16.8 mostra che si prevede che il numero di dispositivi IoT raggiungerà i 30 miliardi di dispositivi connessi entro il 2030.\n\n\n\n\n\n\nFigura 16.8: Numero di dispositivi connessi all’Internet of Things (IoT) in tutto il mondo dal 2019 al 2023. Fonte: Statista.\n\n\n\nLa gestione del fine vita dei gadget connessi a Internet dotati di sensori e intelligenza artificiale rimane un problema spesso trascurato durante la progettazione. Tuttavia, questi prodotti permeano beni di consumo, veicoli, infrastrutture pubbliche, apparecchiature industriali e altro ancora.\n\n16.11.1 Rifiuti Elettronici\nI rifiuti elettronici, o “e-waste”, si riferiscono ad apparecchiature elettriche e componenti scartati che entrano nel flusso dei rifiuti. Ciò include dispositivi che devono essere collegati, hanno una batteria o circuiti elettrici. Con la crescente adozione di dispositivi intelligenti e sensori connessi a Internet, i volumi di e-waste aumentano rapidamente ogni anno. Questi gadget in proliferazione contengono metalli pesanti tossici come piombo, mercurio e cadmio che diventano pericoli per l’ambiente e la salute se smaltiti in modo improprio.\nLa quantità di rifiuti elettronici prodotti sta crescendo a un ritmo allarmante. Oggi, ne produciamo già 50 milioni di tonnellate all’anno. Entro il 2030, si prevede che tale cifra salirà a ben 75 milioni di tonnellate, poiché il consumo di elettronica di consumo continua ad accelerare. La produzione globale di e-waste raggiungerà i 120 milioni di tonnellate all’anno entro il 2050 (Un e Forum 2019). La produzione in forte crescita e i brevi cicli di vita dei nostri gadget alimentano questa crisi, dagli smartphone e tablet ai dispositivi connessi a Internet e agli elettrodomestici.\nI paesi in via di sviluppo sono i più colpiti, in quanto necessitano di più infrastrutture per elaborare in modo sicuro i dispositivi elettronici obsoleti. Nel 2019, i tassi di riciclaggio formali dei rifiuti elettronici nei paesi più poveri variavano dal 13% al 23%. Il resto finisce per essere scaricato illegalmente, bruciato o smantellato in modo grossolano, rilasciando materiali tossici nell’ambiente e danneggiando i lavoratori e le comunità locali. Chiaramente, c’è ancora molto da fare per costruire una capacità globale per una gestione etica e sostenibile dei rifiuti elettronici, altrimenti rischiamo danni irreversibili.\nIl pericolo è che la manipolazione grossolana dei dispositivi elettronici per spogliarli delle parti di valore esponga i lavoratori e le comunità emarginati a nocive plastiche/metalli bruciati. L’avvelenamento da piombo presenta rischi particolarmente elevati per lo sviluppo infantile se ingerito o inalato. Nel complesso, solo circa il 20% dei rifiuti elettronici prodotti è stato raccolto utilizzando metodi ecologicamente corretti, secondo le stime delle Nazioni Unite (Un e Forum 2019). Quindi sono urgentemente necessarie soluzioni per una gestione responsabile del ciclo di vita per contenere lo smaltimento non sicuro, dato che il volume aumenta vertiginosamente.\n\nUn, e World Economic Forum. 2019. A New Circular Vision for Electronics, Time for a Global Reboot. PACE - Platform for Accelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\n16.11.2 Elettronica Monouso\nI costi in rapida diminuzione dei microcontrollori, delle piccole batterie ricaricabili e dell’hardware compatto di comunicazione hanno consentito l’integrazione di sistemi di sensori intelligenti nei beni di consumo di uso quotidiano. Questi dispositivi Internet-of-Things (IoT) monitorano le condizioni del prodotto, le interazioni degli utenti e i fattori ambientali per consentire reattività in tempo reale, personalizzazione e decisioni aziendali basate sui dati nel mercato connesso in evoluzione.\nTuttavia, questi dispositivi elettronici integrati affrontano poca supervisione o pianificazione per gestire in modo sostenibile il loro eventuale smaltimento una volta che i prodotti spesso rivestiti in plastica vengono scartati dopo breve tempo. I sensori IoT ora risiedono comunemente in articoli monouso come bottiglie d’acqua, imballaggi per alimenti, flaconi di farmaci e contenitori per cosmetici che finiscono prevalentemente nei flussi di rifiuti delle discariche dopo poche settimane o mesi di utilizzo da parte dei consumatori.\nIl problema si accelera poiché sempre più produttori si affrettano a integrare chip mobili, fonti di alimentazione, moduli Bluetooth e altri moderni circuiti integrati in silicio, che costano meno di 1 dollaro USA, in vari prodotti senza protocolli per il riciclaggio, la sostituzione delle batterie o la riutilizzabilità dei componenti. Nonostante le loro piccole dimensioni individuali, i volumi di questi dispositivi e il peso dei rifiuti nel corso della loro vita incombono. A differenza della regolamentazione di dispositivi elettronici più grandi, esistono pochi vincoli normativi sui requisiti dei materiali o sulla tossicità di piccoli gadget monouso.\nPur offrendo praticità durante il lavoro, la combinazione insostenibile di difficile recupero e limitati meccanismi di guasto sicuri fa sì che i dispositivi connessi monouso contribuiscano a quote sproporzionate di futuri volumi di rifiuti elettronici che necessitano di urgente attenzione.\n\n\n16.11.3 Obsolescenza Programmata\nL’obsolescenza programmata si riferisce alla strategia di progettazione intenzionale di produzione di prodotti con durate di vita artificialmente limitate che diventano rapidamente non funzionali o obsoleti. Ciò stimola cicli di acquisto di sostituzione più rapidi poiché i consumatori scoprono che i dispositivi non soddisfano più le loro esigenze nel giro di pochi anni. Tuttavia, l’elettronica progettata per l’obsolescenza prematura contribuisce a volumi di rifiuti elettronici insostenibili.\nAd esempio, incollare batterie e componenti di smartphone insieme ostacola la riparabilità rispetto ad assemblaggi modulari e accessibili. L’implementazione di aggiornamenti software che rallentano deliberatamente le prestazioni del sistema crea la percezione che valga la pena aggiornare i dispositivi prodotti solo diversi anni prima.\nAllo stesso modo, le introduzioni alla moda di nuove generazioni di prodotti con aggiunte di funzionalità minori ma esclusive fanno sembrare rapidamente datate le versioni precedenti. Queste tattiche costringono ad acquistare nuovi gadget (ad esempio, iPhone) molto prima della fine della loro operatività. Se moltiplicati per categorie di elettronica in rapida evoluzione, miliardi di articoli appena indossati vengono scartati ogni anno.\nL’obsolescenza programmata intensifica quindi l’utilizzo delle risorse e la creazione di rifiuti nella produzione di prodotti senza alcuna intenzione di lunga durata. Ciò contraddice i principi di sostenibilità in materia di durata, riutilizzo e conservazione dei materiali. Mentre stimola vendite e guadagni continui per i produttori nel breve termine, la strategia esternalizza costi ambientali e tossine su comunità prive di un’adeguata infrastruttura di elaborazione dei rifiuti elettronici.\nLe politiche e l’azione dei consumatori sono fondamentali per contrastare i design dei gadget che sono inutilmente monouso per default. Le aziende dovrebbero anche investire in programmi di gestione dei prodotti che supportino il riutilizzo e il recupero responsabili.\nConsideriamo l’esempio del mondo reale. Apple è stata attenzionata nel corso degli anni per aver presumibilmente coinvolto nell’obsolescenza programmata per incoraggiare i clienti ad acquistare nuovi modelli di iPhone. L’azienda avrebbe progettato i suoi telefoni in modo che le prestazioni si degradino nel tempo o che le funzionalità esistenti diventino incompatibili con i nuovi sistemi operativi, il che, secondo i critici, è finalizzato a stimolare cicli di aggiornamento più rapidi. Nel 2020, Apple ha pagato una multa di 25 milioni di euro per risolvere un caso in Francia in cui le autorità di regolamentazione hanno ritenuto l’azienda colpevole di aver rallentato intenzionalmente i vecchi iPhone senza informare chiaramente i clienti tramite aggiornamenti di iOS.\nNon essendo trasparente sulle modifiche alla gestione dell’alimentazione che hanno ridotto le prestazioni del dispositivo, Apple ha partecipato ad attività ingannevoli che hanno ridotto la durata del prodotto per aumentare le vendite. L’azienda ha affermato che è stato fatto per “smussare” i picchi che potrebbero causare improvvisamente lo spegnimento delle vecchie batterie. Tuttavia, questo esempio evidenzia i rischi legali legati all’impiego dell’obsolescenza programmata e alla mancata comunicazione corretta di quando le modifiche alle funzionalità influiscono sull’usabilità del dispositivo nel tempo: persino marchi leader come Apple possono avere problemi se percepiti come coloro che accorciano intenzionalmente i cicli di vita del prodotto.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#policy-and-regulatory-considerations",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#policy-and-regulatory-considerations",
    "title": "16  IA Sostenibile",
    "section": "16.12 Considerazioni Normative e Politiche",
    "text": "16.12 Considerazioni Normative e Politiche\n\n16.12.1 Mandati di Misura e Rendicontazione\nUn meccanismo politico sempre più rilevante per i sistemi di IA è rappresentato dai requisiti di misurazione e rendicontazione relativi al consumo energetico e alle emissioni di carbonio. Misurazioni obbligatorie, audit, divulgazioni e metodologie più rigorose allineate alle metriche di sostenibilità possono aiutare a colmare le lacune informative che ostacolano le ottimizzazioni dell’efficienza.\nAllo stesso tempo, le politiche nazionali o regionali richiedono alle aziende di una certa dimensione di utilizzare l’IA nei loro prodotti o sistemi back-end per segnalare il consumo energetico o le emissioni associate ai principali carichi di lavoro di IA. Organizzazioni come la Partnership on AI, IEEE e NIST potrebbero contribuire a definire metodologie standardizzate. Proposte più complesse implicano la definizione di modalità coerenti per misurare la complessità computazionale, il PUE del data center, l’intensità di carbonio dell’approvvigionamento energetico e le efficienze ottenute tramite hardware specifico per l’IA.\nAnche gli obblighi di rendicontazione per gli utenti del settore pubblico che acquistano servizi di IA, ad esempio tramite una proposta di legge in Europa, potrebbero aumentare la trasparenza. Tuttavia, gli enti regolatori devono bilanciare l’ulteriore onere di misurazione che tali mandati impongono alle organizzazioni con le continue riduzioni di carbonio derivanti dall’incorporazione di modelli di sviluppo consapevoli della sostenibilità.\nPer essere più costruttivi, qualsiasi politica di misurazione e rendicontazione dovrebbe concentrarsi sull’abilitazione di un continuo perfezionamento piuttosto che su restrizioni o limiti semplicistici. Man mano che i progressi dell’IA si sviluppano rapidamente, agili barriere di sicurezza di governance che incorporano considerazioni sulla sostenibilità in normali metriche di valutazione possono motivare un cambiamento positivo. Tuttavia, una prescrizione eccessiva rischia di limitare l’innovazione se i requisiti diventano obsoleti. La politica di efficienza dell’IA accelera i progressi in tutto il settore combinando flessibilità con appropriate barriere di sicurezza di trasparenza.\n\n\n16.12.2 Meccanismi di Restrizione\nOltre agli obblighi di segnalazione, i politici dispongono di diversi meccanismi di restrizione che potrebbero modellare direttamente il modo in cui i sistemi di IA vengono sviluppati e implementati per ridurre le emissioni:\nLimiti sulle Emissioni delle Elaborazioni: La proposta di legge sull’intelligenza artificiale della Commissione europea adotta un approccio orizzontale che potrebbe consentire di stabilire limiti per l’intera economia sul volume di potenza di elaborazione disponibile per l’addestramento dei modelli di intelligenza artificiale. Come i sistemi di scambio delle emissioni, i limiti mirano a disincentivare indirettamente l’elaborazione estensiva rispetto alla sostenibilità. Tuttavia, la qualità del modello potrebbe essere migliorata per fornire più percorsi per l’acquisizione di capacità aggiuntiva.\nCondizionamento dell’Accesso alle Risorse Pubbliche: Alcuni esperti hanno proposto incentivi come consentire l’accesso solo a set di dati pubblici o potenza di elaborazione per lo sviluppo di modelli fondamentalmente efficienti piuttosto che architetture stravaganti. Ad esempio, il consorzio di benchmarking MLCommons fondato da importanti aziende tecnologiche potrebbe integrare formalmente l’efficienza nelle sue metriche di classifica standardizzate, tuttavia, l’accesso condizionato rischia di limitare l’innovazione.\nMeccanismi Finanziari: Analogamente alle tasse sul carbonio sulle industrie inquinanti, le tariffe applicate per unità di consumo di elaborazione correlato all’IA potrebbero scoraggiare un inutile ridimensionamento del modello, finanziando al contempo innovazioni di efficienza. I crediti d’imposta potrebbero in alternativa premiare le organizzazioni pioniere di tecniche di IA più accurate ma compatte. Tuttavia, gli strumenti finanziari richiedono un’attenta calibrazione tra generazione di entrate ed equità e non penalizzare eccessivamente gli usi produttivi dell’IA.\nDivieti Tecnologici: Se la misurazione fissasse costantemente le emissioni estreme su applicazioni specifiche dell’IA senza percorsi di bonifica, i divieti assoluti rappresentano uno strumento di ultima istanza per i decisori politici. Tuttavia, dato il duplice uso dell’IA, definire implementazioni dannose e benefiche risulta complesso, rendendo necessaria una valutazione di impatto olistica prima di concludere che non esiste alcun valore redentivo. Vietare tecnologie promettenti rischia di avere conseguenze indesiderate e richiede cautela.\n\n\n16.12.3 Incentivi Governativi\nÈ una pratica comune per i governi fornire incentivi fiscali o di altro tipo a consumatori o aziende quando contribuiscono a pratiche tecnologiche più sostenibili. Tali incentivi esistono già negli Stati Uniti per l’adozione di pannelli solari o edifici a risparmio energetico. Per quanto ne sappiamo, non esistono ancora incentivi fiscali per pratiche di sviluppo specifiche per l’IA.\nUn altro potenziale programma di incentivi che sta iniziando a essere esplorato è l’utilizzo di sovvenzioni governative per finanziare progetti di IA Green. Ad esempio, in Spagna, sono stati stanziati 300 milioni di euro per finanziare specificamente progetti di IA e sostenibilità. Gli incentivi governativi sono una strada promettente per incoraggiare pratiche di comportamento aziendale e dei consumatori sostenibili, ma è necessaria un’attenta riflessione per determinare come tali incentivi si adatteranno alle richieste del mercato (Cohen, Lobel, e Perakis 2016).\n\nCohen, Maxime C., Ruben Lobel, e Georgia Perakis. 2016. «The Impact of Demand Uncertainty on Consumer Subsidies for Green Technology Adoption». Manage. Sci. 62 (5): 1235–58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\n16.12.4 Autoregolamentazione\nComplementari alle potenziali azioni governative, i meccanismi di autogoverno volontario consentono alla comunità dell’IA di perseguire obiettivi di sostenibilità senza interventi dall’alto:\nImpegni per le Energie Rinnovabili: Grandi professionisti dell’IA come Google, Microsoft, Amazon e Facebook si sono impegnati ad acquistare abbastanza elettricità rinnovabile per soddisfare il 100% delle loro richieste energetiche. Questi impegni sbloccano tagli alle emissioni composti man mano che aumenta la potenza di calcolo. La formalizzazione di tali programmi incentiva le regioni dei data center verdi. Tuttavia, ci sono critiche sul fatto che questi impegni siano sufficienti (Monyei e Jenkins 2018).\n\nMonyei, Chukwuka G., e Kirsten E. H. Jenkins. 2018. «Electrons have no identity: Setting right misrepresentations in Google and Apple’s clean energy purchasing». Energy Research &amp; Social Science 46 (dicembre): 48–51. https://doi.org/10.1016/j.erss.2018.06.015.\nPrezzi Interni del Carbonio: Alcune organizzazioni utilizzano prezzi ombra sulle emissioni di carbonio per rappresentare i costi ambientali nelle decisioni di allocazione del capitale tra progetti di IA. Se modellati in modo efficace, gli oneri teorici sulle impronte di carbonio dello sviluppo indirizzano i finanziamenti verso innovazioni efficienti piuttosto che solo verso guadagni di accuratezza.\nChecklist per lo Sviluppo dell’Efficienza: Gruppi come AI Sustainability Coalition suggeriscono modelli di checklist volontari che evidenziano le scelte di progettazione del modello, le configurazioni hardware e altri fattori che gli architetti possono regolare per applicazione per limitare le emissioni. Le organizzazioni possono guidare il cambiamento radicando la sostenibilità come metrica di successo primaria insieme a precisione e costi.\nAuditing Indipendente: Anche in assenza di mandati di divulgazione pubblica, le aziende specializzate in audit di sostenibilità tecnologica aiutano gli sviluppatori di IA a identificare gli sprechi, creare roadmap di efficienza e confrontare i progressi tramite revisioni imparziali. Strutturare tali audit in procedure di governance interna o nel processo di approvvigionamento espande la responsabilità.\n\n\n16.12.5 Considerazioni Globali\nMentre misurazione, restrizioni, incentivi e autoregolamentazione rappresentano potenziali meccanismi politici per promuovere la sostenibilità dell’IA, la frammentazione tra i regimi nazionali rischia di avere conseguenze indesiderate. Come per altri domini di politica tecnologica, la divergenza tra regioni deve essere gestita attentamente.\nAd esempio, a causa di preoccupazioni sulla privacy dei dati regionali, OpenAI ha impedito agli utenti europei di accedere al suo chatbot virale ChatGPT. Ciò è avvenuto dopo che la proposta di legge sull’IA dell’UE ha segnalato un approccio precauzionale, consentendo alla CE di vietare determinati usi dell’IA ad alto rischio e di imporre regole di trasparenza che creano incertezza per il rilascio di nuovi modelli. Tuttavia, sarebbe saggio mettere in guardia contro l’azione del regolatore in quanto potrebbe inavvertitamente limitare l’innovazione europea se i regimi con una regolamentazione più leggera attraggono più spesa e talenti per la ricerca sull’IA nel settore privato. Trovare un terreno comune è fondamentale.\nI principi dell’OCSE sull’IA e i quadri delle Nazioni Unite sottolineano principi universalmente concordati che tutte le politiche nazionali dovrebbero sostenere: trasparenza, responsabilità, mitigazione dei “bias” [pregiudizi] e altro ancora. Incorporare in modo costruttivo la sostenibilità come principio fondamentale per un’IA responsabile all’interno di linee guida internazionali può motivare un’azione unitaria senza sacrificare la flessibilità tra sistemi legali divergenti. Evitare dinamiche di corsa al ribasso dipende da una cooperazione multilaterale illuminata.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#public-perception-and-engagement",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#public-perception-and-engagement",
    "title": "16  IA Sostenibile",
    "section": "16.13 Percezione e Coinvolgimento del Pubblico",
    "text": "16.13 Percezione e Coinvolgimento del Pubblico\nMentre l’attenzione della società e gli sforzi politici volti alla sostenibilità ambientale aumentano in tutto il mondo, cresce l’entusiasmo per l’utilizzo dell’intelligenza artificiale per aiutare ad affrontare le sfide ecologiche. Tuttavia, la comprensione e gli atteggiamenti del pubblico nei confronti del ruolo dei sistemi di IA nei contesti di sostenibilità devono ancora essere chiariti e sono offuscati da idee sbagliate. Da un lato, le persone sperano che algoritmi avanzati possano fornire nuove soluzioni per l’energia verde, il consumo responsabile, i percorsi di decarbonizzazione e la conservazione dell’ecosistema. Dall’altro, i timori sui rischi dell’IA incontrollata si insinuano anche nel dominio ambientale e minano il discorso costruttivo. Inoltre, una mancanza di consapevolezza pubblica su questioni chiave come la trasparenza nello sviluppo di strumenti di IA incentrati sulla sostenibilità e potenziali pregiudizi nei dati o nella modellazione minacciano anche di limitare la partecipazione inclusiva e degradare la fiducia del pubblico.\nAffrontare priorità complesse e interdisciplinari come la sostenibilità ambientale richiede un coinvolgimento pubblico informato e sfumato e progressi responsabili nell’innovazione dell’IA. Il percorso da seguire richiede sforzi collaborativi attenti ed equi tra esperti in ML, climatologia, politica ambientale, scienze sociali e comunicazione. Mappare il panorama delle percezioni pubbliche, identificare le insidie e tracciare strategie per coltivare sistemi di IA comprensibili, accessibili e affidabili che puntino a priorità ecologiche condivise si rivelerà essenziale per realizzare obiettivi di sostenibilità. Questo terreno complesso giustifica un esame approfondito delle dinamiche socio-tecniche coinvolte.\n\n16.13.1 Consapevolezza dell’IA\nA maggio 2022, il Pew Research Center ha intervistato 5.101 adulti statunitensi, scoprendo che il 60% aveva sentito o letto “un po’” sull’IA mentre il 27% ne aveva sentito “molto”, il che indica un discreto riconoscimento generale, ma probabilmente una comprensione limitata di dettagli o applicazioni. Tuttavia, tra coloro che hanno una certa familiarità con l’IA, emergono preoccupazioni riguardo ai rischi di uso improprio dei dati personali secondo i termini concordati. Ciononostante, il 62% ritiene che l’IA potrebbe semplificare la vita moderna se applicata in modo responsabile. Tuttavia, una comprensione specifica dei contesti di sostenibilità deve ancora essere migliorata.\nGli studi che tentano di categorizzare i “sentiment” del discorso online rilevano una divisione quasi equa tra ottimismo e cautela riguardo all’implementazione dell’IA per obiettivi di sostenibilità. I fattori che guidano la positività includono le speranze di una migliore previsione dei cambiamenti ecologici utilizzando modelli di ML. La negatività nasce da una mancanza di fiducia negli algoritmi auto-supervisionati che evitano conseguenze indesiderate dovute a impatti umani imprevedibili su sistemi naturali complessi durante l’addestramento.\nLa convinzione pubblica più diffusa rimane che, mentre l’IA ha il potenziale per accelerare le soluzioni su questioni come la riduzione delle emissioni e la protezione della fauna selvatica, una salvaguardia inadeguata intorno a pregiudizi dei dati, punti ciechi etici e considerazioni sulla privacy potrebbero essere rischi più apprezzati se perseguiti con noncuranza, soprattutto su larga scala. Ciò porta a esitazione intorno al supporto incondizionato senza prove di uno sviluppo deliberato e guidato democraticamente.\n\n\n16.13.2 Messaggistica\nGli sforzi ottimistici stanno evidenziando la promessa di sostenibilità dell’IA e sottolineano il potenziale del ML avanzato per accelerare radicalmente gli effetti di decarbonizzazione da reti intelligenti, app personalizzate di tracciamento del carbonio, ottimizzazioni automatizzate dell’efficienza degli edifici e analisi predittive che guidano gli sforzi di conservazione mirati. Una modellazione in tempo reale più completa di complessi cambiamenti climatici ed ecologici utilizzando algoritmi auto-miglioranti offre speranza per mitigare le perdite di biodiversità ed evitare gli scenari peggiori.\nTuttavia, prospettive cautelative, come i Principi di IA di Asilomar, mettono in dubbio se l’IA stessa potrebbe esacerbare le sfide della sostenibilità se vincolata in modo improprio. Le crescenti richieste di energia dei sistemi di elaborazione su larga scala e il training sempre più massiccio del modello di rete neurale sono in conflitto con le ambizioni di energia pulita. La mancanza di diversità negli input di dati o nelle priorità degli sviluppatori potrebbe sminuire le urgenti considerazioni di giustizia ambientale. L’impegno pubblico scettico a breve termine probabilmente dipende dalla necessità di salvaguardie percepibili contro i sistemi di intelligenza artificiale incontrollati che impazziscono nei processi ecologici fondamentali.\nIn sostanza, i “framing” polarizzati promuovono l’intelligenza artificiale come uno strumento indispensabile per la risoluzione dei problemi di sostenibilità, se indirizzata compassionevolmente verso le persone e il pianeta, oppure presentano l’IA come un amplificatore dei danni esistenti che dominano insidiosamente aspetti nascosti dei sistemi naturali centrali per tutta la vita. Superare tali impasse richiede di bilanciare discussioni oneste sui compromessi con visioni condivise per un progresso tecnologico equo e democraticamente governato che mira al ripristino.\n\n\n16.13.3 Partecipazione Equa\nGarantire una partecipazione e un accesso equi dovrebbe costituire la pietra angolare di qualsiasi iniziativa di sostenibilità con il potenziale per importanti impatti sociali. Questo principio si applica ugualmente ai sistemi di IA che mirano a obiettivi ambientali. Tuttavia, voci comunemente escluse come le comunità in prima linea, rurali o indigene e le generazioni future non presenti per il consenso potrebbero subire conseguenze sproporzionate dalle trasformazioni tecnologiche. Ad esempio, la Partnership on AI ha lanciato eventi espressamente mirati al contributo delle comunità emarginate sull’implementazione responsabile dell’intelligenza artificiale.\nGarantire un accesso e una partecipazione equi dovrebbe costituire la pietra angolare di qualsiasi iniziativa di sostenibilità con il potenziale per importanti impatti sociali, che si tratti di intelligenza artificiale o altro. Tuttavia, l’impegno inclusivo nell’intelligenza artificiale ambientale si basa in parte sulla disponibilità e sulla comprensione delle risorse informatiche fondamentali. Come sottolinea il recente rapporto OCSE sulla capacità di calcolo IA nazionale (Oecd 2023), molti paesi attualmente non dispongono di dati o piani strategici che mappino le esigenze per l’infrastruttura richiesta per alimentare i sistemi di IA. Questo punto cieco politico potrebbe limitare gli obiettivi economici ed esacerbare le barriere all’ingresso per le popolazioni emarginate. Il loro progetto sollecita lo sviluppo di strategie nazionali per la capacità di calcolo AI lungo dimensioni di capacità, accessibilità, pipeline di innovazione e resilienza per ancorare l’innovazione. L’archiviazione dei dati di base deve essere migliorata e le piattaforme di sviluppo dei modelli o l’hardware specializzato potrebbero inavvertitamente concentrare i progressi dell’AI nelle mani di gruppi selezionati. Pertanto, la pianificazione di un’espansione equilibrata delle risorse di calcolo AI fondamentali tramite iniziative politiche si collega direttamente alle speranze di una risoluzione dei problemi di sostenibilità democratizzata utilizzando strumenti ML equi e trasparenti.\n\nOecd. 2023. «A blueprint for building national compute capacity for artificial intelligence». 350. Organisation for Economic Co-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\nL’idea chiave è che la partecipazione equa nei sistemi di intelligenza artificiale che affrontano le sfide ambientali si basa in parte sulla garanzia che la capacità di elaborazione e l’infrastruttura di base siano corrette, il che richiede una pianificazione politica proattiva da una prospettiva nazionale.\n\n\n16.13.4 Trasparenza\nMentre le agenzie del settore pubblico e le aziende private si affrettano ad adottare strumenti di IA per aiutare ad affrontare le urgenti sfide ambientali, le richieste di trasparenza sullo sviluppo e la funzionalità di questi sistemi hanno iniziato ad amplificarsi. Le funzionalità di ML spiegabili e interpretabili diventano sempre più cruciali per creare fiducia nei modelli emergenti che mirano a guidare le conseguenti politiche di sostenibilità. Iniziative come il Montreal Carbon Pledge hanno riunito i leader della tecnologia per impegnarsi a pubblicare valutazioni di impatto prima di lanciare sistemi ambientali, come promesso di seguito:\n\n“Come investitori istituzionali, dobbiamo agire nel migliore interesse a lungo termine dei nostri beneficiari. In questo ruolo fiduciario, i rischi di investimento a lungo termine sono associati alle emissioni di gas serra, ai cambiamenti climatici e alla regolamentazione del carbonio. Misurare la nostra impronta di carbonio è fondamentale per comprendere meglio, quantificare e gestire gli impatti, i rischi e le opportunità correlati al carbonio e ai cambiamenti climatici nei nostri investimenti. Pertanto, come primo passo, ci impegniamo a misurare e divulgare annualmente l’impronta di carbonio dei nostri investimenti per utilizzare queste informazioni per sviluppare una strategia di coinvolgimento e identificare e stabilire obiettivi di riduzione dell’impronta di carbonio.” – Montréal Carbon Pledge\n\nAbbiamo bisogno di un impegno simile per la sostenibilità e la responsabilità dell’IA. L’accettazione diffusa e l’impatto delle soluzioni di sostenibilità dell’IA dipenderanno in parte dalla comunicazione deliberata di schemi di convalida, metriche e livelli di giudizio umano applicati prima dell’implementazione in tempo reale. Lavori come i Principi per l’IA spiegabile del NIST possono aiutare a promuovere la trasparenza nei sistemi di IA. Il National Institute of Standards and Technology (NIST) ha pubblicato un influente set di linee guida denominato “Principles for Explainable AI” [Principi per l’IA spiegabile] (Phillips et al. 2020). Questo framework articola le best practice per la progettazione, la valutazione e l’implementazione di sistemi di IA responsabili con funzionalità trasparenti e interpretabili che creano comprensione e fiducia fondamentali per l’utente.\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A Broniatowski, e Mark A Przybocki. 2020. «Four principles of explainable artificial intelligence». Gaithersburg, Maryland 18.\nDelinea quattro principi fondamentali: in primo luogo, i sistemi di IA dovrebbero fornire spiegazioni contestualmente rilevanti che giustifichino il ragionamento alla base dei loro output alle parti interessate appropriate. In secondo luogo, queste spiegazioni di IA devono comunicare informazioni in modo significativo per il livello di comprensione appropriato del loro pubblico target. Il successivo è il principio di accuratezza, che stabilisce che le spiegazioni dovrebbero riflettere fedelmente il processo effettivo e la logica che informano i meccanismi interni di un modello di IA per generare output o raccomandazioni dati in base agli input. Infine, un principio di limiti di conoscenza obbliga le spiegazioni a chiarire i confini di un modello di IA nel catturare l’intera ampiezza della complessità, della varianza e delle incertezze del mondo reale all’interno di uno spazio problematico.\nNel complesso, questi principi NIST offrono ai professionisti e agli adottanti dell’IA una guida su considerazioni chiave sulla trasparenza, essenziali per sviluppare soluzioni accessibili che diano priorità all’autonomia e alla fiducia dell’utente piuttosto che semplicemente massimizzare le sole metriche di accuratezza predittiva. Man mano che l’IA avanza rapidamente in contesti sociali sensibili come sanità, finanza, occupazione e oltre, tali linee guida di progettazione incentrate sull’uomo continueranno a crescere in importanza per ancorare l’innovazione agli interessi pubblici.\nCiò si applica anche al dominio della capacità ambientale. Un’innovazione dell’IA responsabile e guidata democraticamente che mira a priorità ecologiche condivise dipende dal mantenimento della vigilanza pubblica, della comprensione e della supervisione su sistemi altrimenti opachi che assumono ruoli di primo piano nelle decisioni della società. Dare priorità a progetti di algoritmi spiegabili e pratiche di trasparenza radicale secondo standard globali può aiutare a sostenere la fiducia collettiva che questi strumenti migliorino piuttosto che mettere a repentaglio le speranze per un futuro guidato.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#future-directions-and-challenges",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#future-directions-and-challenges",
    "title": "16  IA Sostenibile",
    "section": "16.14 Direzioni e Sfide Future",
    "text": "16.14 Direzioni e Sfide Future\nGuardando al futuro, il ruolo dell’IA nella sostenibilità ambientale è destinato a crescere in modo ancora più significativo. Il potenziale dell’IA per guidare i progressi nell’energia rinnovabile, nella modellazione climatica, negli sforzi di conservazione e altro è immenso. Tuttavia, è una moneta a due facce, poiché dobbiamo superare diverse sfide e indirizzare i nostri sforzi verso uno sviluppo dell’IA sostenibile e responsabile.\n\n16.14.1 Direzioni Future\nUna delle direzioni chiave del futuro è lo sviluppo di modelli e algoritmi di intelligenza artificiale più efficienti dal punto di vista energetico. Ciò implica una ricerca e innovazione continue in aree come il “pruning” [potatura] dei modelli, la quantizzazione e l’uso di numeri a bassa precisione, nonché lo sviluppo dell’hardware per consentire la piena redditività di queste innovazioni. Inoltre, esaminiamo paradigmi di elaborazione alternativi che non si basano su architetture von-Neumann. Ulteriori informazioni su questo argomento sono disponibili nel capitolo sull’accelerazione hardware. L’obiettivo è creare sistemi di IA che offrano prestazioni elevate riducendo al minimo il consumo di energia e le emissioni di carbonio.\nUn’altra direzione importante è l’integrazione di fonti di energia rinnovabili nell’infrastruttura di IA. Poiché i data center continuano a contribuire in modo significativo all’impronta di carbonio dell’IA, la transizione verso fonti di energia rinnovabili come l’energia solare ed eolica è fondamentale. Gli sviluppi nell’accumulo di energia sostenibile a lungo termine, come Ambri, uno spin-off del MIT, potrebbero consentire questa transizione. Ciò richiede investimenti e collaborazioni significativi tra aziende tecnologiche, fornitori di energia e politici.\n\n\n16.14.2 Sfide\nNonostante queste promettenti direzioni, devono essere affrontate diverse sfide. Una delle sfide principali è la necessità di standard e metodologie coerenti per misurare e segnalare l’impatto ambientale dell’IA. Questi metodi devono catturare la complessità dei cicli di vita dei modelli di IA e dell’hardware di sistema. Inoltre, sono necessarie infrastrutture IA e hardware di sistema efficienti e sostenibili dal punto di vista ambientale. Ciò è costituito da tre componenti:\n\nMassimizzare l’utilizzo delle risorse di acceleratore e sistema.\nProlungare la durata di vita delle infrastrutture IA.\nProgettare hardware di sistema tenendo presente l’impatto ambientale.\n\nDal lato software, dovremmo bilanciare la sperimentazione e il conseguente costo di training. Tecniche come la ricerca dell’architettura neurale e l’ottimizzazione degli iperparametri possono essere utilizzate per l’esplorazione dello spazio di progettazione. Tuttavia, queste sono spesso molto dispendiose in termini di risorse. Una sperimentazione efficiente può ridurre significativamente l’impatto ambientale. Successivamente, dovrebbero essere esplorati metodi per ridurre gli sforzi di training sprecati.\nPer migliorare la qualità del modello, spesso ridimensioniamo il set di dati. Tuttavia, le maggiori risorse di sistema richieste per l’archiviazione e l’ingestione dei dati causate da questa scalabilità hanno un impatto ambientale significativo (Wu et al. 2022). È importante comprendere a fondo la velocità con cui i dati perdono il loro valore predittivo e ideare strategie di campionamento dei dati.\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. «Sustainable ai: Environmental implications, challenges and opportunities». Proceedings of Machine Learning and Systems 4: 795–813.\nAnche le lacune nei dati rappresentano una sfida significativa. Senza aziende e governi che condividono apertamente dati dettagliati e accurati sul consumo di energia, sulle emissioni di carbonio e su altri impatti ambientali, non è facile sviluppare strategie efficaci per un’IA sostenibile.\nInfine, il ritmo rapido dello sviluppo dell’IA richiede un approccio agile alla politica imposta a questi sistemi. La politica dovrebbe garantire uno sviluppo sostenibile senza limitare l’innovazione. Ciò richiede che esperti in tutti i settori dell’IA, delle scienze ambientali, dell’energia e della politica lavorino insieme per raggiungere un futuro sostenibile.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#conclusione",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#conclusione",
    "title": "16  IA Sostenibile",
    "section": "16.15 Conclusione",
    "text": "16.15 Conclusione\nDobbiamo affrontare le considerazioni sulla sostenibilità man mano che l’intelligenza artificiale si espande rapidamente nei settori e nella società. L’intelligenza artificiale promette innovazioni rivoluzionarie, ma il suo impatto ambientale minaccia la sua crescita diffusa. Questo capitolo analizza molteplici aspetti, dall’energia e dalle emissioni agli impatti sui rifiuti e sulla biodiversità, che gli sviluppatori di intelligenza artificiale/apprendimento automatico devono valutare quando creano sistemi di IA responsabili.\nFondamentalmente, abbiamo bisogno di elevare la sostenibilità a priorità di progettazione primaria piuttosto che a un ripensamento. Tecniche come modelli ad alta efficienza energetica, data center alimentati da fonti rinnovabili e programmi di riciclaggio dell’hardware offrono soluzioni, ma l’impegno olistico rimane fondamentale. Abbiamo bisogno di standard in materia di trasparenza, contabilità del carbonio e divulgazioni della catena di fornitura per integrare i guadagni tecnici. Tuttavia, esempi come le pratiche di efficienza 4M di Google contenenti l’uso di energia ML evidenziano che possiamo far progredire l’intelligenza artificiale di pari passo con gli obiettivi ambientali con uno sforzo concertato. Raggiungiamo questo equilibrio armonioso facendo collaborare ricercatori, aziende, regolatori e utenti in tutti i domini. L’obiettivo non è soluzioni perfette, ma un miglioramento continuo mentre integriamo l’IA in nuovi settori.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.it.html#sec-sustainable-ai-resource",
    "href": "contents/sustainable_ai/sustainable_ai.it.html#sec-sustainable-ai-resource",
    "title": "16  IA Sostenibile",
    "section": "16.16 Risorse",
    "text": "16.16 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nTransparency and Sustainability.\nSustainability of TinyML.\nModel Cards for Transparency.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 16.1\nEsercizio 16.2\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di IA embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>IA Sostenibile</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html",
    "href": "contents/robust_ai/robust_ai.it.html",
    "title": "17  IA Robusta",
    "section": "",
    "text": "17.1 Introduzione\nPer IA robusta si intende la capacità di un sistema di mantenere le proprie prestazioni e affidabilità anche in presenza di errori. Un sistema di apprendimento automatico robusto è progettato per essere tollerante ai guasti e resiliente agli errori, in grado di funzionare efficacemente anche in condizioni avverse.\nMan mano che i sistemi ML diventano sempre più integrati in vari aspetti della nostra vita, dai servizi basati su cloud ai dispositivi edge e ai sistemi embedded, l’impatto dei guasti hardware e software sulle loro prestazioni e affidabilità diventa più significativo. In futuro, man mano che i sistemi ML diventano più complessi e vengono implementati in applicazioni ancora più critiche, la necessità di progetti robusti e tolleranti ai guasti sarà fondamentale.\nSi prevede che i sistemi ML svolgeranno ruoli cruciali nei veicoli autonomi, nelle città intelligenti, nell’assistenza sanitaria e nei domini dell’automazione industriale. In questi domini, le conseguenze dei guasti hardware o software possono essere gravi, potenzialmente causa di perdita di vite umane, danni economici o danni ambientali.\nI ricercatori e gli ingegneri devono concentrarsi sullo sviluppo di tecniche avanzate per il rilevamento, l’isolamento e il ripristino dei guasti per mitigare questi rischi e garantire il funzionamento affidabile dei futuri sistemi ML.\nQuesto capitolo si concentrerà in modo specifico su tre categorie principali di guasti ed errori che possono influire sulla robustezza dei sistemi ML: guasti hardware, guasti software ed errori umani.\nLe sfide e gli approcci specifici per ottenere la robustezza possono variare a seconda della scala e dei vincoli del sistema ML. I sistemi di cloud computing o data center su larga scala possono concentrarsi sulla tolleranza ai guasti e sulla resilienza tramite ridondanza, elaborazione distribuita e tecniche avanzate di rilevamento e correzione degli errori. Al contrario, i dispositivi edge con risorse limitate o i sistemi embedded affrontano sfide uniche a causa della potenza di calcolo, della memoria e delle risorse energetiche limitate.\nIndipendentemente dalla scala e dai vincoli, le caratteristiche chiave di un sistema ML robusto includono tolleranza ai guasti, resilienza agli errori e mantenimento delle prestazioni. Comprendendo e affrontando le sfide multiformi alla robustezza, possiamo sviluppare sistemi ML affidabili e sicuri in grado di navigare nelle complessità degli ambienti del mondo reale.\nQuesto capitolo non riguarda solo l’esplorazione di strumenti, framework e tecniche dei sistemi ML per rilevare e mitigare guasti, attacchi e cambiamenti durante la distribuzione. Si tratta di sottolineare il ruolo cruciale di ognuno di nel dare priorità alla resilienza durante tutto il ciclo di vita dello sviluppo dell’IA, dalla raccolta dati e dall’addestramento del modello all’implementazione e al monitoraggio. Affrontando in modo proattivo le sfide alla robustezza, possiamo sbloccare il pieno potenziale delle tecnologie ML garantendone al contempo un’implementazione sicura, affidabile e responsabile nelle applicazioni del mondo reale.\nMentre l’IA continua a plasmare il nostro futuro, il potenziale delle tecnologie ML è immenso. Ma è solo quando creiamo sistemi resilienti in grado di resistere alle sfide del mondo reale che possiamo davvero sfruttare questo potenziale. Questo è un fattore determinante per il successo e l’impatto sociale di questa tecnologia trasformativa ed è alla nostra portata.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html#introduzione",
    "href": "contents/robust_ai/robust_ai.it.html#introduzione",
    "title": "17  IA Robusta",
    "section": "",
    "text": "Guasti Hardware: Guasti transitori, permanenti e intermittenti possono influire sui componenti hardware di un sistema ML, corrompendo i calcoli e degradando le prestazioni.\nRobustezza del Modello: I modelli ML possono essere vulnerabili ad attacchi avversari, avvelenamento dei dati e cambiamenti di distribuzione, che possono indurre classificazioni errate mirate, alterare il comportamento appreso del modello o compromettere l’integrità e l’affidabilità del sistema.\nGuasti software: Bug, difetti di progettazione ed errori di implementazione nei componenti software, come algoritmi, librerie e framework, possono propagare errori e introdurre vulnerabilità.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html#esempi-del-mondo-reale",
    "href": "contents/robust_ai/robust_ai.it.html#esempi-del-mondo-reale",
    "title": "17  IA Robusta",
    "section": "17.2 Esempi del mondo reale",
    "text": "17.2 Esempi del mondo reale\nEcco alcuni esempi reali di casi in cui guasti nell’hardware o nel software hanno causato problemi importanti nei sistemi ML in ambienti cloud, edge ed embedded:\n\n17.2.1 Cloud\nNel febbraio 2017, Amazon Web Services (AWS) ha subito un’interruzione significativa a causa di un errore umano durante la manutenzione. Un tecnico ha inserito inavvertitamente un comando errato, causando la disconnessione di molti server. Questa interruzione ha interrotto molti servizi AWS, tra cui l’assistente basato sull’intelligenza artificiale di Amazon, Alexa. Di conseguenza, i dispositivi basati su Alexa, come Amazon Echo e prodotti di terze parti che utilizzano Alexa Voice Service, non hanno potuto rispondere alle richieste degli utenti per diverse ore. Questo incidente evidenzia il potenziale impatto degli errori umani sui sistemi ML basati su cloud e la necessità di procedure di manutenzione robuste e meccanismi di sicurezza.\nIn un altro esempio (Vangal et al. 2021), Facebook ha riscontrato un problema di “silent data corruption (SDC)” [corruzione silenziosa dei dati] all’interno della sua infrastruttura di query distribuita, come mostrato in Figura 17.1. L’infrastruttura di Facebook include un sistema di query che preleva ed esegue query SQL e simili a SQL su più set di dati utilizzando framework come Presto, Hive e Spark. Una delle applicazioni che ha utilizzato questa infrastruttura di query è stata un’applicazione di compressione per ridurre l’ingombro degli archivi dati. In questa applicazione di compressione, i file venivano compressi quando non venivano letti e decompressi quando veniva effettuata una richiesta di lettura. Prima della decompressione, la dimensione del file veniva controllata per assicurarsi che fosse maggiore di zero, indicando un file compresso valido con contenuti.\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar, Ram Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, e Chris H. Kim. 2021. «Wide-Range Many-Core SoC Design in Scaled CMOS: Challenges and Opportunities». IEEE Trans. Very Large Scale Integr. VLSI Syst. 29 (5): 843–56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\n\n\n\n\nFigura 17.1: Corruzione silenzioso dei dati nelle applicazioni di database. Fonte: Facebook\n\n\n\nTuttavia, in un caso, quando la dimensione del file veniva calcolata per un file valido di dimensioni diverse da zero, l’algoritmo di decompressione ha richiamato una funzione di potenza dalla libreria Scala. Inaspettatamente, la funzione Scala ha restituito un valore di dimensione zero per il file nonostante avesse una dimensione decompressa nota diversa da zero. Di conseguenza, la decompressione non è stata eseguita e il file non è stato scritto nel database di output. Questo problema si è manifestato sporadicamente, con alcune occorrenze dello stesso calcolo della dimensione del file che restituivano il valore corretto diverso da zero.\nL’impatto di questa corruzione silenziosa dei dati è stato significativo, portando a file mancanti e dati errati nel database di output. L’applicazione che si basava sui file decompressi ha fallito a causa delle incongruenze dei dati. Nel caso di studio presentato nel documento, l’infrastruttura di Facebook, che consiste in centinaia di migliaia di server che gestiscono miliardi di richieste al giorno dalla loro enorme base di utenti, ha riscontrato un problema di corruzione silenziosa dei dati. Il sistema interessato elaborava query utente, caricamenti di immagini e contenuti multimediali, che richiedevano un’esecuzione rapida, affidabile e sicura.\nQuesto caso di studio illustra come la corruzione silenziosa dei dati può propagarsi attraverso più strati di uno stack applicativo, causando perdita di dati e guasti delle applicazioni in un sistema distribuito su larga scala. La natura intermittente del problema e la mancanza di messaggi di errore espliciti lo hanno reso particolarmente difficile da diagnosticare e risolvere. Ma questo non è limitato solo a Meta, anche altre aziende come Google che gestiscono ipercomputer IA affrontano questi problemi. Figura 17.2 Jeff Dean, Chief Scientist presso Google DeepMind e Google Research, parla degli SDC e del loro impatto sui sistemi di apprendimento automatico.\n\n\n\n\n\n\nFigura 17.2: Gli errori “Silent data corruption (SDC)” sono un problema importante per gli ipercomputer di IA. Fonte: Jeff Dean at MLSys 2024, Keynote (Google)\n\n\n\n\n\n17.2.2 Edge\nPer quanto riguarda esempi di guasti ed errori nei sistemi edge ML, un’area che ha ricevuto notevole attenzione è il dominio delle auto a guida autonoma. I veicoli a guida autonoma si basano in larga misura su algoritmi di apprendimento automatico per la percezione, il processo decisionale e il controllo, rendendoli particolarmente sensibili all’impatto di guasti hardware e software. Negli ultimi anni, diversi incidenti di alto profilo che hanno coinvolto veicoli autonomi hanno evidenziato le sfide e i rischi associati all’implementazione di questi sistemi in ambienti reali.\nA maggio 2016, si è verificato un incidente mortale quando una Tesla Model S con pilota automatico si è schiantata contro un autoarticolato bianco che attraversava l’autostrada. Il sistema Autopilot, che si basava su algoritmi di visione artificiale e apprendimento automatico, non è riuscito a riconoscere il rimorchio bianco sullo sfondo di un cielo luminoso. Il conducente, che secondo quanto riferito stava guardando un film al momento dell’incidente, non è intervenuto in tempo e il veicolo è entrato in collisione con il rimorchio a tutta velocità. Questo incidente ha sollevato preoccupazioni sui limiti dei sistemi di percezione basati sull’intelligenza artificiale e sulla necessità di solidi meccanismi di sicurezza nei veicoli autonomi. Ha inoltre evidenziato l’importanza della consapevolezza del conducente e la necessità di linee guida chiare sull’uso delle funzionalità di guida semi-autonoma, come mostrato in Figura 17.3.\n\n\n\n\n\n\nFigura 17.3: Tesla nell’incidente mortale in California era in modalità Autopilot. Fonte: BBC News\n\n\n\nA marzo 2018, un veicolo di prova a guida autonoma di Uber ha investito e ucciso un pedone che attraversava la strada a Tempe, in Arizona. L’incidente è stato causato da un difetto software nel sistema di riconoscimento degli oggetti del veicolo, che non è riuscito a identificare i pedoni in modo appropriato per evitarli come ostacoli. L’autista di sicurezza, che avrebbe dovuto monitorare il funzionamento del veicolo e intervenire se necessario, è stato trovato distratto durante l’incidente. Questo incidente ha portato ad un’ampia revisione del programma di guida autonoma di Uber e ha sollevato dubbi sulla prontezza della tecnologia dei veicoli autonomi per le strade pubbliche. Ha inoltre sottolineato la necessità di rigorosi test, convalide e misure di sicurezza nello sviluppo e nell’implementazione di sistemi di guida autonoma basati sull’intelligenza artificiale.\nNel 2021, Tesla ha dovuto affrontare un controllo più rigoroso a seguito di diversi incidenti che hanno coinvolto veicoli in modalità Autopilot. Alcuni di questi incidenti sono stati attribuiti a problemi con la capacità del sistema Autopilot di rilevare e rispondere a determinate situazioni stradali, come veicoli di emergenza fermi o ostacoli sulla strada. Ad esempio, nell’aprile 2021, una Tesla Model S si è schiantata contro un albero in Texas, uccidendo due passeggeri. I primi rapporti suggerivano che nessuno si trovasse al posto di guida al momento dell’incidente, sollevando interrogativi sull’uso e il potenziale uso improprio delle funzionalità Autopilot. Questi incidenti evidenziano le sfide in corso nello sviluppo di sistemi di guida autonoma affidabili e robusti e la necessità di normative chiare e di istruzione dei consumatori in merito alle capacità e ai limiti di queste tecnologie.\n\n\n17.2.3 Embedded\nI sistemi embedded, che spesso operano in ambienti con risorse limitate e applicazioni critiche per la sicurezza, hanno da tempo dovuto affrontare sfide legate a guasti hardware e software. Poiché le tecnologie di IA e apprendimento automatico sono sempre più integrate in questi sistemi, il potenziale di guasti ed errori assume nuove dimensioni, con l’aggiunta di complessità degli algoritmi di IA e la natura critica delle applicazioni in cui vengono distribuiti.\nConsideriamo alcuni esempi, a partire dall’esplorazione dello spazio. La missione Mars Polar Lander della NASA nel 1999 ha subito un guasto catastrofico a causa di un errore software nel sistema di rilevamento dell’atterraggio (Figura 17.4). Il software di bordo della navicella spaziale ha interpretato erroneamente il rumore proveniente dall’apertura delle sue gambe di atterraggio come un segnale di atterraggio sulla superficie marziana. Di conseguenza, la navicella ha spento prematuramente i suoi motori, causando lo schianto sulla superficie. Questo incidente evidenzia l’importanza critica di una progettazione software solida e di test approfonditi nei sistemi embedded, in particolare quelli che operano in ambienti remoti e ostili. Poiché le capacità di IA sono integrate nelle future missioni spaziali, garantire l’affidabilità e la tolleranza ai guasti di questi sistemi sarà fondamentale per il successo della missione.\n\n\n\n\n\n\nFigura 17.4: La missione fallita della NASA Mars Polar Lander nel 1999 è costata oltre $200M. Fonte: SlashGear\n\n\n\nTornando sulla Terra, nel 2015, un Boeing 787 Dreamliner ha subito un arresto elettrico completo durante un volo a causa di un bug del software nelle sue unità di controllo del generatore. Questo incidente sottolinea come i guasti software possano avere gravi conseguenze nei sistemi integrati complessi come quelli degli aeromobili. Poiché le tecnologie di IA sono sempre più applicate all’aviazione, come nei sistemi di volo autonomi e nella manutenzione predittiva, garantire la robustezza e l’affidabilità di questi sistemi sarà fondamentale per la sicurezza dei passeggeri.\n\n“Se le quattro unità di controllo del generatore principale (associate ai generatori montati sul motore) fossero accese contemporaneamente, dopo 248 giorni di alimentazione continua, tutte e quattro le GCU entrerebbero in modalità fail-safe contemporaneamente, con conseguente perdita di tutta l’alimentazione elettrica CA indipendentemente dalla fase di volo.” – Direttiva della Federal Aviation Administration (2015)\n\nPoiché le capacità di IA si integrano sempre di più nei sistemi embedded, il potenziale di guasti ed errori diventa più complesso e grave. Si immagini un pacemaker intelligente che ha un improvviso problema tecnico. Un paziente potrebbe morire a causa di tale effetto. Pertanto, gli algoritmi AI, come quelli utilizzati per la percezione, il processo decisionale e il controllo, introducono nuove fonti di potenziali guasti, come problemi relativi ai dati, incertezze del modello e comportamenti inaspettati nei casi limite. Inoltre, la natura opaca di alcuni modelli di IA può rendere difficile identificare e diagnosticare i guasti quando si verificano.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html#guasti-hardware",
    "href": "contents/robust_ai/robust_ai.it.html#guasti-hardware",
    "title": "17  IA Robusta",
    "section": "17.3 Guasti Hardware",
    "text": "17.3 Guasti Hardware\nI guasti hardware rappresentano una sfida significativa nei sistemi informatici, inclusi i sistemi tradizionali e ML. Questi guasti si verificano quando componenti fisici, come processori, moduli di memoria, dispositivi di archiviazione o interconnessioni, funzionano male o si comportano in modo anomalo. I guasti hardware possono causare calcoli errati, danneggiamento dei dati, crash del sistema o guasti completi del sistema, compromettendo l’integrità e l’affidabilità dei calcoli eseguiti (Jha et al. 2019). Un guasto completo del sistema si riferisce a una situazione in cui l’intero sistema informatico diventa non reattivo o inutilizzabile a causa di un malfunzionamento hardware critico. Questo tipo di guasto è il più grave, poiché rende il sistema inutilizzabile e può portare alla perdita o al danneggiamento dei dati, richiedendo un intervento manuale per riparare o sostituire i componenti difettosi.\nComprendere la tassonomia dei guasti hardware è essenziale per chiunque lavori con sistemi informatici, in particolare nel contesto dei sistemi ML. I sistemi ML si basano su architetture hardware complesse e calcoli su larga scala per addestrare e distribuire modelli che apprendono dai dati e fanno previsioni o decisioni intelligenti. Tuttavia, i guasti hardware possono introdurre errori e incongruenze nella pipeline MLOps, influenzando l’accuratezza, la robustezza e l’affidabilità dei modelli addestrati (G. Li et al. 2017).\nConoscere i diversi tipi di guasti hardware, i loro meccanismi e il loro potenziale impatto sul comportamento del sistema è fondamentale per sviluppare strategie efficaci per rilevarli, mitigarli e ripristinarli. Questa conoscenza è necessaria per progettare sistemi di elaborazione tolleranti ai guasti, implementare algoritmi ML robusti e garantire l’affidabilità complessiva delle applicazioni basate su ML.\nLe sezioni seguenti esploreranno le tre categorie principali di guasti hardware: transitori, permanenti e intermittenti. Discuteremo le loro definizioni, caratteristiche, cause, meccanismi ed esempi di come si manifestano nei sistemi di elaborazione. Tratteremo anche tecniche di rilevamento e mitigazione specifiche per ogni tipo di guasto.\n\nGuasti Transitori: I guasti transitori sono temporanei e non ricorrenti. Sono spesso causati da fattori esterni come raggi cosmici, interferenze elettromagnetiche o fluttuazioni di potenza. Un esempio comune di guasto transitorio è un bit flip, in cui un singolo bit in una posizione di memoria o registro cambia il suo valore in modo imprevisto. I guasti transitori possono causare calcoli errati o corruzione dei dati, ma non causano danni permanenti all’hardware.\nGuasti permanenti: I guasti permanenti, chiamati anche errori hard, sono irreversibili e persistono nel tempo. Sono in genere causati da difetti fisici o usura dei componenti hardware. Esempi di guasti permanenti includono guasti bloccati, in cui un bit o un segnale è impostato in modo permanente su un valore specifico (ad esempio, sempre 0 o sempre 1) e guasti del dispositivo, come un processore malfunzionante o un modulo di memoria danneggiato. I guasti permanenti possono causare un guasto completo del sistema o un significativo degrado delle prestazioni.\nGuasti Intermittenti: I guasti intermittenti sono guasti ricorrenti che compaiono e scompaiono in modo intermittente. Condizioni hardware instabili, come connessioni allentate, componenti obsoleti o difetti di fabbricazione, spesso ne sono la causa. I guasti intermittenti possono essere difficili da diagnosticare e riprodurre perché possono verificarsi sporadicamente e in condizioni specifiche. Esempi includono cortocircuiti intermittenti o problemi di resistenza dei contatti. I guasti intermittenti possono portare a un comportamento imprevedibile del sistema e a errori intermittenti.\n\nAlla fine di questa discussione, i lettori avranno una solida comprensione della tassonomia dei guasti e della sua rilevanza per i sistemi di elaborazione e ML tradizionali. Questa base li aiuterà a prendere decisioni informate durante la progettazione, l’implementazione e la distribuzione di soluzioni tolleranti ai guasti, migliorando l’affidabilità e la credibilità dei loro sistemi di elaborazione e delle applicazioni ML.\n\n17.3.1 Guasti Transitori\nI guasti transitori nell’hardware possono manifestarsi in varie forme, ciascuna con le sue caratteristiche e cause uniche. Questi guasti sono di natura temporanea e non causano danni permanenti ai componenti hardware.\n\nDefinizione e Caratteristiche\nAlcuni dei tipi comuni di guasti transitori includono Single Event Upset (SEU) causati da radiazioni ionizzanti, fluttuazioni di tensione (Reddi e Gupta 2013) dovute a rumore dell’alimentatore o interferenze elettromagnetiche, “Electromagnetic Interference (EMI)” indotte da campi elettromagnetici esterni, “Electrostatic Discharge (ESD)” risultanti da un improvviso flusso di elettricità statica, diafonia causata da accoppiamento di segnali involontari, rimbalzo di massa innescato dalla commutazione simultanea di più uscite, violazioni di temporizzazione dovute a violazioni dei vincoli di temporizzazione del segnale ed errori soft nella logica combinatoria che influenzano l’uscita dei circuiti logici (Mukherjee, Emer, e Reinhardt 2005). Comprendere questi diversi tipi di guasti transitori è fondamentale per progettare sistemi hardware robusti e resilienti che possano mitigarne l’impatto e garantire un funzionamento affidabile.\n\nReddi, Vijay Janapa, e Meeta Sharma Gupta. 2013. Resilient Architecture Design for Voltage Variation. Springer International Publishing. https://doi.org/10.1007/978-3-031-01739-1.\n\nMukherjee, S. S., J. Emer, e S. K. Reinhardt. 2005. «The Soft Error Problem: An Architectural Perspective». In 11th International Symposium on High-Performance Computer Architecture, 243–47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\nTutti questi guasti transitori sono caratterizzati dalla loro breve durata e dalla loro natura non permanente. Non persistono né lasciano alcun impatto duraturo sull’hardware. Tuttavia, possono comunque portare a calcoli errati, corruzione dei dati o comportamento scorretto del sistema se non gestiti correttamente.\n\n\n\nCause di Guasti Transitori\nI guasti transitori possono essere attribuiti a vari fattori esterni. Una causa comune sono i raggi cosmici, particelle ad alta energia provenienti dallo spazio. Quando queste particelle colpiscono aree sensibili dell’hardware, come celle di memoria o transistor, possono indurre disturbi di carica che alterano i dati memorizzati o trasmessi. Ciò è illustrato in Figura 17.5. Un’altra causa di guasti transitori è l’electromagnetic interference (EMI) [interferenza elettromagnetica] da dispositivi vicini o fluttuazioni di potenza. L’EMI può accoppiarsi con i circuiti e causare picchi di tensione o glitch che interrompono temporaneamente il normale funzionamento dell’hardware.\n\n\n\n\n\n\nFigura 17.5: Meccanismo di Occorrenza di Guasti Transitori Hardware. Fonte: NTT\n\n\n\n\n\nMeccanismi di Guasti Transitori\nI guasti transitori possono manifestarsi attraverso meccanismi diversi a seconda del componente hardware interessato. Nei dispositivi di memoria come DRAM o SRAM, i guasti transitori spesso portano a inversioni di bit, in cui un singolo bit cambia il suo valore da 0 a 1 o viceversa. Ciò può corrompere i dati o le istruzioni archiviati. Nei circuiti logici, i guasti transitori possono causare glitch o picchi di tensione che si propagano attraverso la logica combinatoria, con conseguenti output o segnali di controllo errati. I guasti transitori possono anche influenzare i canali di comunicazione, causando errori di bit o perdite di pacchetti durante la trasmissione dei dati.\n\n\nImpatto sui Sistemi ML\nUn esempio comune di guasto transitorio è un’inversione di bit nella memoria principale. Se una struttura dati importante o un’istruzione critica viene archiviata nella posizione di memoria interessata, può portare a calcoli errati o a un comportamento errato del programma. Se si verifica un guasto transitorio nella memoria che archivia i pesi o i gradienti del modello. Ad esempio, un bit flip nella memoria che memorizza un contatore di loop può causare l’esecuzione indefinita del loop o la sua terminazione prematura. Errori transitori nei registri di controllo o nei bit di flag possono alterare il flusso di esecuzione del programma, causando salti imprevisti o decisioni di diramazione errate. Nei sistemi di comunicazione, gli errori transitori possono danneggiare i pacchetti di dati trasmessi, causando ritrasmissioni o perdita di dati.\nNei sistemi ML, gli errori transitori possono avere implicazioni significative durante la fase di training (He et al. 2023). Il training ML comporta calcoli iterativi e aggiornamenti dei parametri del modello basati su grandi set di dati. Se si verifica un errore transitorio nella memoria dei pesi o dei gradienti del modello, può causare aggiornamenti errati e compromettere la convergenza e l’accuratezza del processo di training. Figura 17.6 mostra un esempio concreto tratto dalla flotta di produzione di Google, in cui un’anomalia SDC ha causato una differenza significativa nella norma del gradiente.\n\n\n\n\n\n\nFigura 17.6: SDC nella fase di training ML determina anomalie nella norma del gradiente. Fonte: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nAd esempio, un’inversione di bit nella matrice dei pesi di una rete neurale può far sì che il modello apprenda schemi o associazioni errati, con conseguente peggioramento delle prestazioni (Wan et al. 2021). Errori transitori nella pipeline dei dati, come la corruzione dei campioni di training o delle etichette, possono anche introdurre rumore e influire sulla qualità del modello appreso.\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi, e Arijit Raychowdhury. 2021. «Analyzing and Improving Fault Tolerance of Learning-Based Navigation Systems». In 2021 58th ACM/IEEE Design Automation Conference (DAC), 841–46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\nDurante la fase di inferenza, gli errori transitori possono influire sull’affidabilità e l’attendibilità delle previsioni ML. Se si verifica un errore transitorio nella memoria dei parametri del modello addestrato o nel calcolo dei risultati dell’inferenza, può portare a previsioni errate o incoerenti. Ad esempio, un’inversione di bit nei valori di attivazione di una rete neurale può alterare l’output finale di classificazione o regressione (Mahmoud et al. 2020).\nNelle applicazioni “safety-critical”, come i veicoli autonomi o la diagnosi medica, i guasti transitori durante l’inferenza possono avere gravi conseguenze, portando a decisioni o azioni errate (G. Li et al. 2017; Jha et al. 2019). Garantire la resilienza dei sistemi ML contro i guasti transitori è fondamentale per mantenere l’integrità e l’affidabilità delle previsioni.\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, e Stephen W. Keckler. 2017. «Understanding error propagation in deep learning neural network (DNN) accelerators and applications». In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 1–12. ACM. https://doi.org/10.1145/3126908.3126964.\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, e Yoshua Bengio. 2016. «Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1». arXiv preprint arXiv:1602.02830.\n\nAygun, Sercan, Ece Olcay Gunes, e Christophe De Vleeschouwer. 2021. «Efficient and robust bitstream processing in binarised neural networks». Electron. Lett. 57 (5): 219–22. https://doi.org/10.1049/ell2.12045.\nAll’altro estremo, in ambienti con risorse limitate come TinyML, le “Binarized Neural Networks [BNNs]” [reti neurali binarizzate] (Courbariaux et al. 2016) sono emerse come una soluzione promettente. Le BNN rappresentano pesi di rete in precisione a bit singolo, offrendo efficienza computazionale e tempi di inferenza più rapidi. Tuttavia, questa rappresentazione binaria rende le BNN fragili agli errori di inversione di bit sui pesi della rete. Ad esempio, lavori precedenti (Aygun, Gunes, e De Vleeschouwer 2021) hanno dimostrato che un’architettura BNN a due strati nascosti per un’attività semplice come la classificazione MNIST subisce un degrado delle prestazioni dal 98% di accuratezza del test al 70% quando vengono inseriti errori soft di inversione di bit casuali tramite pesi del modello con una probabilità del 10%.\nPer affrontare tali problemi è necessario considerare tecniche di training “flip-aware” o sfruttare paradigmi di elaborazione emergenti (ad esempio, elaborazione stocastica) per migliorare la tolleranza ai guasti e la robustezza, di cui parleremo in Sezione 17.3.4. Le direzioni di ricerca future mirano a sviluppare architetture ibride, nuove funzioni di attivazione e funzioni di perdita su misura per colmare il divario di accuratezza rispetto ai modelli a precisione completa mantenendo al contempo la loro efficienza computazionale.\n\n\n\n17.3.2 Guasti Permanenti\nI guasti permanenti sono difetti hardware che persistono e causano danni irreversibili ai componenti interessati. Questi guasti sono caratterizzati dalla loro natura persistente e richiedono la riparazione o la sostituzione dell’hardware difettoso per ripristinare la normale funzionalità del sistema.\n\nDefinizione e Caratteristiche\nI guasti permanenti sono difetti hardware che causano malfunzionamenti persistenti e irreversibili nei componenti interessati. Il componente difettoso rimane non operativo finché un guasto permanente non viene riparato o sostituito. Questi guasti sono caratterizzati dalla loro natura coerente e riproducibile, il che significa che il comportamento difettoso viene osservato ogni volta che il componente interessato viene utilizzato. I guasti permanenti possono avere un impatto su vari componenti hardware, come processori, moduli di memoria, dispositivi di archiviazione o interconnessioni, causando crash del sistema, danneggiamento dei dati o guasto completo del sistema.\nUn esempio notevole di guasto permanente è il bug Intel FDIV, scoperto nel 1994. Il bug FDIV era un difetto in alcune unità di divisione a virgola mobile (FDIV) dei processori Intel Pentium. Il bug causava risultati errati per specifiche operazioni di divisione, portando a calcoli imprecisi.\nIl bug FDIV si è verificato a causa di un errore nella tabella di ricerca utilizzata dall’unità di divisione. In rari casi, il processore recuperava un valore errato dalla tabella di ricerca, con un risultato leggermente meno preciso del previsto. Ad esempio, Figura 17.7 mostra una frazione 4195835/3145727 tracciata su un processore Pentium con l’errore permanente FDIV. Le regioni triangolari sono quelle in cui si sono verificati calcoli errati. Idealmente, tutti i valori corretti verrebbero arrotondati a 1,3338, ma i risultati errati mostrano 1,3337, indicando un errore nella quinta cifra.\nSebbene l’errore fosse piccolo, poteva accumularsi su molte operazioni di divisione, portando a significative imprecisioni nei calcoli matematici. L’impatto del bug FDIV era significativo, soprattutto per le applicazioni che si basavano in modo massiccio sulla divisione precisa in virgola mobile, come simulazioni scientifiche, calcoli finanziari e progettazione assistita da computer. Il bug ha portato a risultati errati, che potrebbero avere gravi conseguenze in settori come la finanza o l’ingegneria.\n\n\n\n\n\n\nFigura 17.7: Processore Intel Pentium con errore permanente FDIV. Le regioni triangolari sono quelle in cui si sono verificati calcoli errati. Fonte: Byte Magazine\n\n\n\nIl bug Intel FDIV è un monito per il potenziale impatto di guasti permanenti sui sistemi ML. Nel contesto del ML, guasti permanenti nei componenti hardware possono portare a calcoli errati, influenzando l’accuratezza e l’affidabilità dei modelli. Ad esempio, se un sistema ML si basa su un processore con un’unità a virgola mobile difettosa, simile al bug Intel FDIV, potrebbe introdurre errori nei calcoli eseguiti durante l’addestramento o l’inferenza.\nQuesti errori possono propagarsi attraverso il modello, portando a previsioni imprecise o apprendimento distorto. Nelle applicazioni in cui il ML viene utilizzato per attività critiche, come la guida autonoma, la diagnosi medica o le previsioni finanziarie, le conseguenze di calcoli errati dovuti a guasti permanenti possono essere gravi.\nÈ fondamentale che i professionisti del ML siano consapevoli del potenziale impatto dei guasti permanenti e incorporino tecniche di tolleranza ai guasti, come ridondanza hardware, meccanismi di rilevamento e correzione degli errori e progettazione di algoritmi robusti, per mitigare i rischi associati a questi guasti. Inoltre, test approfonditi e convalida dei componenti hardware ML possono aiutare a identificare e risolvere i guasti permanenti prima che influiscano sulle prestazioni e l’affidabilità del sistema.\n\n\nCause dei Guasti Permanenti\nI guasti permanenti possono derivare da diverse cause, tra cui difetti di fabbricazione e meccanismi di usura. I difetti di fabbricazione sono difetti intrinseci introdotti durante il processo di fabbricazione dei componenti hardware. Questi difetti includono incisione impropria, doping non corretto o contaminazione, che portano a componenti non funzionali o parzialmente funzionali.\nD’altro canto, i meccanismi di usura si verificano nel tempo man mano che i componenti hardware sono sottoposti a un uso prolungato e a stress. Fattori come elettromigrazione, rottura dell’ossido o stress termico possono causare una graduale degradazione dei componenti, portando infine a guasti permanenti.\n\n\nMeccanismi dei Guasti Permanenti\nI guasti permanenti possono manifestarsi attraverso vari meccanismi, a seconda della natura e della posizione del guasto. Gli “Stuck-at fault” [guasti bloccati] (Seong et al. 2010) sono guasti permanenti comuni in cui un segnale o una cella di memoria rimane fissata a un valore particolare (0 o 1) indipendentemente dagli input, come illustrato in Figura 17.8.\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, e Hsien-Hsin S. Lee. 2010. «SAFER: Stuck-at-fault Error Recovery for Memories». In 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture, 115–24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\n\n\n\n\nFigura 17.8: Modello di Guasto Bloccato nei Circuiti Digitali. Fonte: Accendo Reliability\n\n\n\nI guasti bloccati possono verificarsi in porte logiche, celle di memoria o interconnessioni, causando calcoli errati o corruzione dei dati. Un altro meccanismo sono i guasti del dispositivo, in cui un componente, come un transistor o una cella di memoria, cessa completamente di funzionare. Ciò può essere dovuto a difetti di fabbricazione o grave usura. I guasti di “bridging” si verificano quando due o più linee di segnale sono collegate involontariamente, causando cortocircuiti o un comportamento logico errato.\nOltre ai guasti stuck-at, ci sono diversi altri tipi di guasti permanenti che possono influenzare i circuiti digitali e che possono avere un impatto su un sistema ML. I guasti di ritardo possono causare il superamento del limite specificato del ritardo di propagazione di un segnale, portando a violazioni di temporizzazione. I guasti di interconnessione, come guasti aperti (fili rotti), guasti resistivi (resistenza aumentata) o guasti capacitivi (capacità aumentata), possono causare problemi di integrità del segnale o violazioni di temporizzazione. Le celle di memoria possono anche subire vari guasti, tra cui guasti di transizione (impossibilità di cambiare stato), guasti di accoppiamento (interferenza tra celle adiacenti) e guasti sensibili al pattern di vicinato (guasti che dipendono dai valori delle celle vicine). Altri guasti permanenti possono verificarsi nella rete di alimentazione o nella rete di distribuzione del clock, influenzando la funzionalità e la temporizzazione del circuito.\n\n\nImpatto sui Sistemi ML\nI guasti permanenti possono influire gravemente sul comportamento e l’affidabilità dei sistemi di elaborazione. Ad esempio, un guasto nell’unità logica aritmetica (ALU) di un processore può causare calcoli errati, portando a risultati errati o crash del sistema. Un guasto permanente in un modulo di memoria, in una specifica cella di memoria, può danneggiare i dati archiviati, causando la perdita di dati o un comportamento errato del programma. Nei dispositivi di archiviazione, guasti permanenti come settori danneggiati o guasti del dispositivo possono causare l’inaccessibilità dei dati o la perdita completa delle informazioni archiviate. I guasti permanenti di interconnessione possono interrompere i canali di comunicazione, causando il danneggiamento dei dati o il blocco del sistema.\nI guasti permanenti possono influire significativamente sui sistemi ML durante le fasi di addestramento e inferenza. Durante l’addestramento, guasti permanenti nelle unità di elaborazione o nella memoria possono causare calcoli errati, con conseguenti modelli danneggiati o non ottimali (He et al. 2023). Inoltre, i guasti nei dispositivi di archiviazione possono corrompere i dati di training o i parametri del modello archiviati, causando la perdita di dati o incongruenze del modello (He et al. 2023).\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, e Siddharth Garg. 2018. «Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator». In 2018 IEEE 36th VLSI Test Symposium (VTS), 1–6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\nDurante l’inferenza, i guasti permanenti possono influire sull’affidabilità e la correttezza delle previsioni ML. I guasti nelle unità di elaborazione possono produrre risultati errati o causare guasti del sistema, mentre i guasti nella memoria che archivia i parametri del modello possono portare all’utilizzo di modelli corrotti o obsoleti per l’inferenza (J. J. Zhang et al. 2018).\nPer mitigare l’impatto dei guasti permanenti nei sistemi ML, devono essere impiegate tecniche di tolleranza ai guasti sia a livello hardware che software. La ridondanza hardware, come la duplicazione di componenti critici o l’utilizzo di codici di correzione degli errori (Kim, Sullivan, e Erez 2015), può aiutare a rilevare e ripristinare i guasti permanenti. Le tecniche software, come i meccanismi di checkpoint e riavvio (Egwutuoha et al. 2013), possono consentire al sistema di recuperare da guasti permanenti tornando a uno stato salvato in precedenza. Il monitoraggio, il test e la manutenzione regolari dei sistemi ML possono aiutare a identificare e sostituire i componenti difettosi prima che causino interruzioni significative.\n\nKim, Jungrae, Michael Sullivan, e Mattan Erez. 2015. «Bamboo ECC: Strong, safe, and flexible codes for reliable computer memory». In 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), 101–12. IEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, e Shiping Chen. 2013. «A survey of fault tolerance mechanisms and checkpoint/restart implementations for high performance computing systems». The Journal of Supercomputing 65 (3): 1302–26. https://doi.org/10.1007/s11227-013-0884-0.\nProgettare sistemi ML tenendo a mente la tolleranza ai guasti è fondamentale per garantirne l’affidabilità e la robustezza in presenza di guasti permanenti. Ciò può comportare l’incorporazione di ridondanza, meccanismi di rilevamento e correzione degli errori e strategie di sicurezza nell’architettura del sistema. Affrontando in modo proattivo le sfide poste dai guasti permanenti, i sistemi ML possono mantenere la loro integrità, accuratezza e affidabilità, anche di fronte a guasti hardware.\n\n\n\n17.3.3 Guasti Intermittenti\nI guasti intermittenti sono guasti hardware che si verificano sporadicamente e in modo imprevedibile in un sistema. Un esempio è illustrato in Figura 17.9, dove le crepe nel materiale possono introdurre una maggiore resistenza [elettrica] nei circuiti. Questi guasti sono particolarmente difficili da rilevare e diagnosticare perché compaiono e scompaiono in modo intermittente, rendendo difficile riprodurre e isolare la causa principale. I guasti intermittenti possono causare instabilità del sistema, corruzione dei dati e degrado delle prestazioni.\n\n\n\n\n\n\nFigura 17.9: Maggiore resistenza dovuta a un guasto intermittente, ovvero una crepa tra la protuberanza di rame e la saldatura del package. Fonte: Constantinescu\n\n\n\n\nDefinizione e Caratteristiche\nI guasti intermittenti sono caratterizzati dalla loro natura sporadica e non deterministica. Si verificano in modo irregolare e possono apparire e scomparire spontaneamente, con durate e frequenze variabili. Questi guasti non si manifestano in modo coerente ogni volta che viene utilizzato il componente interessato, il che li rende più difficili da rilevare rispetto ai guasti permanenti. I guasti intermittenti possono interessare vari componenti hardware, tra cui processori, moduli di memoria, dispositivi di archiviazione o interconnessioni. Possono causare errori transitori, danneggiamento dei dati o comportamento imprevisto del sistema.\nI guasti intermittenti possono avere un impatto significativo sul comportamento e l’affidabilità dei sistemi di elaborazione (Rashid, Pattabiraman, e Gopalakrishnan 2015). Ad esempio, un guasto intermittente nella logica di controllo di un processore può causare un flusso di programma irregolare, portando a calcoli errati o blocchi del sistema. I guasti intermittenti nei moduli di memoria possono danneggiare i valori dei dati, con conseguente esecuzione errata del programma o incoerenze nei dati. Nei dispositivi di archiviazione, i guasti intermittenti possono causare errori di lettura/scrittura o perdita di dati. Errori intermittenti nei canali di comunicazione possono causare corruzione dei dati, perdita di pacchetti o problemi di connettività intermittenti. Questi errori possono causare crash del sistema, problemi di integrità dei dati o degrado delle prestazioni, a seconda della gravità e della frequenza degli errori intermittenti.\n\n———. 2015. «Characterizing the Impact of Intermittent Hardware Faults on Programs». IEEE Trans. Reliab. 64 (1): 297–310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nCause degli Errori Intermittenti\nI guasti intermittenti possono derivare da diverse cause, sia interne che esterne, ai componenti hardware (Constantinescu 2008). Una causa comune è l’invecchiamento e l’usura dei componenti. Man mano che i dispositivi elettronici invecchiano, diventano più suscettibili a guasti intermittenti dovuti a meccanismi di degradazione come elettromigrazione, rottura dell’ossido o affaticamento dei giunti di saldatura.\n\nConstantinescu, Cristian. 2008. «Intermittent faults and effects on reliability of integrated circuits». In 2008 Annual Reliability and Maintainability Symposium, 370–74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\nAnche difetti di fabbricazione o variazioni di processo possono causare guasti intermittenti, in cui componenti marginali o borderline possono presentare guasti sporadici in condizioni specifiche, come mostrato in Figura 17.10.\nFattori ambientali, come fluttuazioni di temperatura, umidità o vibrazioni, possono innescare guasti intermittenti alterando le caratteristiche elettriche dei componenti. Collegamenti allentati o degradati, come quelli nei connettori o nei circuiti stampati, possono causare guasti intermittenti.\n\n\n\n\n\n\nFigura 17.10: Guasto intermittente indotto da residui in un chip DRAM. Fonte: Hynix Semiconductor\n\n\n\n\n\nMeccanismi dei Guasti Intermittenti\nI guasti intermittenti possono manifestarsi attraverso vari meccanismi, a seconda della causa sottostante e del componente interessato. Un meccanismo è il circuito aperto o cortocircuito intermittente, in cui un percorso o una connessione del segnale viene temporaneamente interrotto o cortocircuitato, causando un comportamento irregolare. Un altro meccanismo è il guasto di ritardo intermittente (J. Zhang et al. 2018), in cui la temporizzazione dei segnali o i ritardi di propagazione diventano incoerenti, causando problemi di sincronizzazione o calcoli errati. I guasti intermittenti possono manifestarsi come bit flip [inversioni] transitori o errori soft nelle celle di memoria o nei registri, causando corruzione dei dati o esecuzione errata del programma.\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, e Siddharth Garg. 2018. «ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Learning Accelerators». In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nImpatto sui Sistemi ML\nNel contesto dei sistemi ML, i guasti intermittenti possono introdurre sfide significative e avere un impatto sull’affidabilità e le prestazioni del sistema. Durante la fase di addestramento, i guasti intermittenti nelle unità di elaborazione o nella memoria possono portare a incongruenze nei calcoli, con conseguenti gradienti e aggiornamenti del peso errati o rumorosi. Ciò può influire sulla convergenza e l’accuratezza del processo di addestramento, portando a modelli sub-ottimali o instabili. Errori intermittenti di archiviazione o recupero dei dati possono corrompere i dati di training, introducendo rumore o errori che degradano la qualità dei modelli addestrati (He et al. 2023).\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, e Yanjing Li. 2023. «Understanding and Mitigating Hardware Failures in Deep Learning Training Systems». In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\nDurante la fase di inferenza, gli errori intermittenti possono influire sull’affidabilità e la coerenza delle previsioni ML. Gli errori nelle unità di elaborazione o nella memoria possono causare calcoli errati o corruzione dei dati, portando a previsioni errate o incoerenti. Gli errori intermittenti nella pipeline dei dati possono introdurre rumore o errori nei dati di input, influenzando l’accuratezza e la robustezza delle previsioni. Nelle applicazioni safety-critical, come veicoli autonomi o sistemi di diagnosi medica, gli errori intermittenti possono avere gravi conseguenze, portando a decisioni o azioni errate che compromettono la sicurezza e l’affidabilità.\nPer mitigare l’impatto degli errori intermittenti nei sistemi ML è necessario un approccio poliedrico (Rashid, Pattabiraman, e Gopalakrishnan 2012). A livello hardware, tecniche come pratiche di progettazione robuste, selezione dei componenti e controllo ambientale possono aiutare a ridurre il verificarsi di guasti intermittenti. Meccanismi di ridondanza e correzione degli errori possono essere impiegati per rilevare e ripristinare guasti intermittenti. A livello software, monitoraggio del runtime, rilevamento delle anomalie e tecniche di tolleranza ai guasti possono essere incorporate nella pipeline ML. Ciò può includere tecniche come convalida dei dati, rilevamento di valori anomali, assemblaggio di modelli o adattamento del modello di runtime per gestire con eleganza i guasti intermittenti.\n\nRashid, Layali, Karthik Pattabiraman, e Sathish Gopalakrishnan. 2012. «Intermittent Hardware Errors Recovery: Modeling and Evaluation». In 2012 Ninth International Conference on Quantitative Evaluation of Systems, 220–29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\nProgettare sistemi ML resilienti ai guasti intermittenti è fondamentale per garantirne affidabilità e robustezza. Ciò comporta l’incorporazione di tecniche di tolleranza ai guasti, monitoraggio del runtime e meccanismi adattivi nell’architettura del sistema. Affrontando in modo proattivo le sfide dei guasti intermittenti, i sistemi ML possono mantenere la loro accuratezza, coerenza e affidabilità, anche in caso di guasti hardware sporadici. Test, monitoraggio e manutenzione regolari dei sistemi ML possono aiutare a identificare e mitigare i guasti intermittenti prima che causino interruzioni significative o un degrado delle prestazioni.\n\n\n\n17.3.4 Rilevamento e Mitigazione\nQuesta sezione esplora varie tecniche di rilevamento degli errori, inclusi approcci a livello hardware e software, e discute strategie di mitigazione efficaci per migliorare la resilienza dei sistemi ML. Inoltre, esamineremo le considerazioni sulla progettazione di sistemi ML resilienti, presenteremo casi di studio ed esempi e metteremo in evidenza le future direzioni di ricerca nei sistemi ML tolleranti agli errori.\n\nTecniche di Rilevamento degli Errori\nLe tecniche di rilevamento degli errori sono importanti per identificare e localizzare gli errori hardware nei sistemi ML. Queste tecniche possono essere ampiamente categorizzate in approcci a livello hardware e software, ognuno dei quali offre capacità e vantaggi unici.\n\nRilevamento degli errori a livello hardware\nLe tecniche di rilevamento degli errori a livello hardware sono implementate a livello fisico del sistema e mirano a identificare gli errori nei componenti hardware sottostanti. Esistono diverse tecniche hardware, ma in generale, possiamo raggruppare questi diversi meccanismi nelle seguenti categorie.\nBuilt-in self-test (BIST) mechanisms: BIST è una tecnica potente per rilevare guasti nei componenti hardware (Bushnell e Agrawal 2002).. Comporta l’incorporazione di circuiti hardware aggiuntivi nel sistema per l’autotest e il rilevamento dei guasti. BIST può essere applicato a vari componenti, come processori, moduli di memoria o circuiti integrati specifici per applicazione (ASIC). Ad esempio, BIST può essere implementato in un processore utilizzando catene di scansione, che sono percorsi dedicati che consentono l’accesso ai registri interni e alla logica per scopi di test.\n\nBushnell, Michael L, e Vishwani D Agrawal. 2002. «Built-in self-test». Essentials of electronic testing for digital, memory and mixed-signal VLSI circuits, 489–548.\nDurante il processo BIST, vengono applicati pattern di test predefiniti ai circuiti interni del processore e le risposte vengono confrontate con i valori previsti. Eventuali discrepanze indicano la presenza di guasti. I processori Xeon di Intel, ad esempio, includono meccanismi BIST per testare i core della CPU, la memoria cache e altri componenti critici durante l’avvio del sistema.\nCodici di rilevamento degli errori: I codici di rilevamento degli errori sono ampiamente utilizzati per rilevare errori di archiviazione e trasmissione dei dati (Hamming 1950). Questi codici aggiungono bit ridondanti ai dati originali, consentendo il rilevamento di errori di bit. Esempio: I controlli di parità sono una forma semplice di codice di rilevamento degli errori mostrato in Figura 17.11. In uno schema di parità a bit singolo, un bit extra viene aggiunto a ogni parola di dati, rendendo il numero di 1 nella parola pari (parità pari) o dispari (parità dispari).\n\nHamming, R. W. 1950. «Error Detecting and Error Correcting Codes». Bell Syst. Tech. J. 29 (2): 147–60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\n\n\n\n\nFigura 17.11: Esempio di bit di parità. Fonte: Computer Hope\n\n\n\nQuando si leggono i dati, la parità viene controllata e, se non corrisponde al valore previsto, viene rilevato un errore. Codici di rilevamento degli errori più avanzati, come i “cyclic redundancy checks (CRC)” [controlli di ridondanza ciclica], calcolano un checksum in base ai dati e lo aggiungono al messaggio. Il checksum viene ricalcolato all’estremità ricevente e confrontato con il checksum trasmesso per rilevare gli errori. I moduli di memoria con “Error-correcting code (ECC)” [codice di correzione degli errori], comunemente utilizzati nei server e nei sistemi critici, impiegano codici avanzati di rilevamento e correzione degli errori per rilevare e correggere errori a bit singolo o multi-bit nella memoria.\nRidondanza hardware e meccanismi di voto: La ridondanza hardware implica la duplicazione dei componenti critici e il confronto dei loro output per rilevare e mascherare i guasti (Sheaffer, Luebke, e Skadron 2007). I meccanismi di voto, come la “triple modular redundancy (TMR)” [ridondanza modulare tripla], impiegano più istanze di un componente e confrontano i loro output per identificare e mascherare comportamenti difettosi (Arifeen, Hassan, e Lee 2020).\n\nSheaffer, Jeremy W, David P Luebke, e Kevin Skadron. 2007. «A hardware redundancy and recovery mechanism for reliable scientific computation on graphics processors». In Graphics Hardware, 2007:55–64. Citeseer.\n\nArifeen, Tooba, Abdus Sami Hassan, e Jeong-A Lee. 2020. «Approximate Triple Modular Redundancy: A Survey». #IEEE_O_ACC# 8: 139851–67. https://doi.org/10.1109/access.2020.3012673.\n\nYeh, Y. C. 1996. «Triple-triple redundant 777 primary flight computer». In 1996 IEEE Aerospace Applications Conference. Proceedings, 1:293–307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\nIn un sistema TMR, tre istanze identiche di un componente hardware, come un processore o un sensore, eseguono lo stesso calcolo in parallelo. Gli output di queste istanze vengono immessi in un circuito di voto, che confronta i risultati e seleziona il valore di maggioranza come output finale. Se una delle istanze produce un risultato non corretto a causa di un guasto, il meccanismo di voto maschera l’errore e mantiene l’output corretto. Il TMR è comunemente utilizzato nei sistemi aerospaziali e aeronautici, dove l’elevata affidabilità è fondamentale. Ad esempio, l’aereo Boeing 777 impiega il TMR nel suo sistema di computer di volo primario per garantire la disponibilità e la correttezza delle funzioni di controllo del volo (Yeh 1996).\nI computer a guida autonoma di Tesla impiegano un’architettura hardware ridondante per garantire la sicurezza e l’affidabilità delle funzioni critiche, come percezione, processo decisionale e controllo del veicolo, come mostrato in Figura 17.12. Un componente chiave di questa architettura è l’utilizzo della “dual modular redundancy (DMR)” [ridondanza modulare duale] nei sistemi di computer di bordo dell’auto.\n\n\n\n\n\n\nFigura 17.12: Computer Tesla a guida autonoma completa con SoC duali ridondanti. Fonte: Tesla\n\n\n\nNell’implementazione DMR di Tesla, due unità hardware identiche, spesso chiamate “computer ridondanti” o “unità di controllo ridondanti”, eseguono gli stessi calcoli in parallelo (Bannon et al. 2019). Ogni unità elabora in modo indipendente i dati dei sensori, esegue algoritmi di percezione e decisionali e genera comandi di controllo per gli attuatori del veicolo (ad esempio, sterzo, accelerazione e frenata).\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, e Emil Talpes. 2019. «Computer and Redundancy Solution for the Full Self-Driving Computer». In 2019 IEEE Hot Chips 31 Symposium (HCS), 1–22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\nGli output di queste due unità ridondanti vengono costantemente confrontati per rilevare eventuali discrepanze o guasti. Se gli output corrispondono, il sistema presuppone che entrambe le unità funzionino correttamente e i comandi di controllo vengono inviati agli attuatori del veicolo. Tuttavia, se c’è una mancata corrispondenza tra gli output, il sistema identifica un potenziale guasto in una delle unità e adotta le misure appropriate per garantire un funzionamento sicuro.\nIl sistema può impiegare meccanismi aggiuntivi per determinare quale unità è difettosa in una mancata corrispondenza. Ciò può comportare l’utilizzo di algoritmi diagnostici, il confronto degli output con i dati di altri sensori o sottosistemi o l’analisi della coerenza degli output nel tempo. Una volta identificata l’unità difettosa, il sistema può isolarla e continuare a funzionare utilizzando l’output dell’unità non difettosa.\nIl DMR nel computer di guida autonoma di Tesla fornisce un ulteriore livello di sicurezza e tolleranza ai guasti. Avendo due unità indipendenti che eseguono gli stessi calcoli, il sistema può rilevare e mitigare i guasti che possono verificarsi in una delle unità. Questa ridondanza aiuta a prevenire singoli punti di guasto e garantisce che le funzioni critiche rimangano operative nonostante i guasti hardware.\nInoltre, Tesla incorpora anche meccanismi di ridondanza aggiuntivi oltre al DMR. Ad esempio, utilizzano alimentatori ridondanti, sistemi di sterzo e frenata e diverse suite di sensori (ad esempio, telecamere, radar e sensori a ultrasuoni) per fornire più livelli di tolleranza ai guasti. Queste ridondanze contribuiscono collettivamente alla sicurezza e all’affidabilità complessive del sistema di guida autonoma.\nÈ importante notare che mentre DMR fornisce rilevamento guasti e un certo livello di tolleranza ai guasti, TMR può fornire un diverso livello di mascheramento dei guasti. In DMR, se entrambe le unità subiscono guasti simultanei o il guasto influisce sul meccanismo di confronto, il sistema potrebbe non essere in grado di identificare il guasto. Pertanto, gli SDC di Tesla si basano su una combinazione di DMR e altri meccanismi di ridondanza per raggiungere un elevato livello di tolleranza ai guasti.\nL’uso di DMR nel computer a guida autonoma di Tesla evidenzia l’importanza della ridondanza hardware nelle applicazioni critiche per la sicurezza. Utilizzando unità di elaborazione ridondanti e confrontando i loro output, il sistema può rilevare e mitigare i guasti, migliorando la sicurezza e l’affidabilità complessive della funzionalità di guida autonoma.\nGoogle utilizza “hot spare” ridondanti per gestire i problemi SDC nei suoi data center, migliorando così l’affidabilità delle funzioni critiche. Come illustrato in Figura 17.13, durante la normale fase di addestramento, più “worker” di training sincroni funzionano in modo impeccabile. Tuttavia, se un worker diventa difettoso e causa SDC, un verificatore SDC identifica automaticamente i problemi. Dopo aver rilevato l’SDC, il verificatore SDC sposta il training su un hot spare e invia la macchina difettosa per la riparazione. Questa ridondanza salvaguarda la continuità e l’affidabilità del training ML, riducendo al minimo i tempi di inattività e preservando l’integrità dei dati.\n\n\n\n\n\n\nFigura 17.13: Google impiega “core hot spare” per gestire in modo trasparente gli SDC nel data center. Fonte: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nWatchdog timer: I watchdog timer sono componenti hardware che monitorano l’esecuzione di attività o processi critici (Pont e Ong 2002). Sono comunemente utilizzati per rilevare e ripristinare guasti software o hardware che causano la mancata risposta di un sistema o il suo blocco in un ciclo infinito. In un sistema embedded, un watchdog timer può essere configurato per monitorare l’esecuzione del loop principale, come illustrato in Figura 17.14. Il software reimposta periodicamente il watchdog timer per indicare che funziona correttamente. Supponiamo che il software non riesca a reimpostare il timer entro un limite di tempo specificato (periodo di timeout). In tal caso, il watchdog timer presuppone che il sistema abbia riscontrato un guasto e attiva un’azione di ripristino predefinita, come il reset del sistema o il passaggio a un componente di backup. I watchdog timer sono ampiamente utilizzati nell’elettronica automobilistica, nei sistemi di controllo industriale e in altre applicazioni critiche per la sicurezza per garantire il rilevamento e il ripristino tempestivi dai guasti.\n\nPont, Michael J, e Royan HL Ong. 2002. «Using watchdog timers to improve the reliability of single-processor embedded systems: Seven new patterns and a case study». In Proceedings of the First Nordic Conference on Pattern Languages of Programs, 159–200. Citeseer.\n\n\n\n\n\n\nFigura 17.14: Esempio di watchdog timer nel rilevamento di guasti MCU. Fonte: Ablic\n\n\n\n\n\nRilevamento guasti a livello software\nLe tecniche di rilevamento degli errori a livello software si basano su algoritmi software e meccanismi di monitoraggio per identificare gli errori di sistema. Queste tecniche possono essere implementate a vari livelli dello stack software, tra cui il sistema operativo, il middleware o il livello dell’applicazione.\nMonitoraggio del runtime e rilevamento delle anomalie: Il monitoraggio del runtime comporta l’osservazione continua del comportamento del sistema e dei suoi componenti durante l’esecuzione (Francalanza et al. 2017). Aiuta a rilevare anomalie, errori o comportamenti imprevisti che potrebbero indicare la presenza di errori. Ad esempio, si consideri un sistema di classificazione delle immagini basato su ML distribuito in un’auto a guida autonoma. Il monitoraggio del runtime può essere implementato per tracciare le prestazioni e il comportamento del modello di classificazione (Mahmoud et al. 2021).\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard, Ian Cassar, Dario Della Monica, e Anna Ingólfsdóttir. 2017. «A foundation for runtime monitoring». In International Conference on Runtime Verification, 8–29. Springer.\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, e Stephen W. Keckler. 2021. «Optimizing Selective Protection for CNN Resilience». In 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE), 127–38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\nChandola, Varun, Arindam Banerjee, e Vipin Kumar. 2009. «Anomaly detection: A survey». ACM Comput. Surv. 41 (3): 1–58. https://doi.org/10.1145/1541880.1541882.\nGli algoritmi di rilevamento delle anomalie possono essere applicati alle previsioni del modello o alle attivazioni di livelli intermedi, come il rilevamento statistico di valori anomali o approcci basati sull’apprendimento automatico (ad esempio, One-Class SVM o Autoencoders) (Chandola, Banerjee, e Kumar 2009). Figura 17.15 mostra un esempio di rilevamento delle anomalie. Supponiamo che il sistema di monitoraggio rilevi una deviazione significativa dai modelli previsti, come un calo improvviso dell’accuratezza della classificazione o campioni fuori distribuzione. In tal caso, può generare un “alert” che indica un potenziale errore nel modello o nella pipeline dei dati di input. Questo rilevamento precoce consente di applicare strategie di intervento tempestivo e di mitigazione degli errori.\n\n\n\n\n\n\nFigura 17.15: Esempi di rilevamento delle anomalie. (a) Rilevamento delle anomalie completamente supervisionato, (b) rilevamento delle anomalie solo normali, (c, d, e) rilevamento delle anomalie semi-supervisionato, (f) rilevamento delle anomalie non supervisionato. Fonte: Google\n\n\n\nControlli di coerenza e convalida dei dati: I controlli di coerenza e le tecniche di convalida dei dati garantiscono l’integrità e la correttezza dei dati in diverse fasi di elaborazione in un sistema ML (Lindholm et al. 2019). Questi controlli aiutano a rilevare danneggiamenti dei dati, incongruenze o errori che potrebbero propagarsi e influenzare il comportamento del sistema. Esempio: In un sistema ML distribuito in cui più nodi collaborano per addestrare un modello, è possibile implementare controlli di coerenza per convalidare l’integrità dei parametri condivisi del modello. Ogni nodo può calcolare un checksum o un hash dei parametri del modello prima e dopo l’iterazione di addestramento, come mostrato in Figura 17.15. Eventuali incongruenze o danneggiamenti dei dati possono essere rilevati confrontando i checksum tra i nodi. Inoltre, è possibile applicare controlli di intervallo ai dati di input e agli output del modello per garantire che rientrino nei limiti previsti. Ad esempio, se il sistema di percezione di un veicolo autonomo rileva un oggetto con dimensioni o velocità non realistiche, può indicare un errore nei dati del sensore o negli algoritmi di percezione (Wan et al. 2023).\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, e Thomas B. Schon. 2019. «Data Consistency Approach to Model Validation». #IEEE_O_ACC# 7: 59788–96. https://doi.org/10.1109/access.2019.2915109.\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, e Y Zhu. 2023. «Vpp: The vulnerability-proportional protection paradigm towards reliable autonomous machines». In Proceedings of the 5th International Workshop on Domain Specific System Architecture (DOSSA), 1–6.\n\nKawazoe Aguilera, Marcos, Wei Chen, e Sam Toueg. 1997. «Heartbeat: A timeout-free failure detector for quiescent reliable communication». In Distributed Algorithms: 11th International Workshop, WDAG’97 Saarbrücken, Germany, September 2426, 1997 Proceedings 11, 126–40. Springer.\nMeccanismi di heartbeat e timeout: I meccanismi di heartbeat e timeout sono comunemente utilizzati per rilevare errori nei sistemi distribuiti e garantire la vitalità e la reattività dei componenti (Kawazoe Aguilera, Chen, e Toueg 1997). Sono molto simili ai timer watchdog presenti nell’hardware. Ad esempio, in un sistema ML distribuito, in cui più nodi collaborano per eseguire attività quali pre-elaborazione dei dati, training del modello o inferenza, è possibile implementare meccanismi heartbeat per monitorare lo stato e la disponibilità di ciascun nodo. Ogni nodo invia periodicamente un messaggio heartbeat a un coordinatore centrale o ai suoi nodi peer, indicando il suo stato e la sua disponibilità. Supponiamo che un nodo non riesca a inviare un heartbeat entro un periodo di timeout specificato, come mostrato in Figura 17.16. In tal caso, viene considerato difettoso e possono essere intraprese azioni appropriate, come la ridistribuzione del carico di lavoro o l’avvio di un meccanismo di “failover”. I timeout possono anche essere utilizzati per rilevare e gestire componenti bloccati o non reattivi. Ad esempio, se un processo di caricamento dati supera una soglia di timeout predefinita, potrebbe indicare un errore nella pipeline dati e il sistema può adottare misure correttive.\n\n\n\n\n\n\nFigura 17.16: Messaggi heartbeat nei sistemi distribuiti. Fonte: GeeksforGeeks\n\n\n\n\nTecniche di “Software-implemented fault tolerance (SIFT)”: Le tecniche SIFT introducono meccanismi di ridondanza e rilevamento degli errori a livello software per migliorare l’affidabilità e la tolleranza agli errori del sistema (Reis et al. 2005). Esempio: La programmazione N-version è una tecnica SIFT in cui più versioni di componenti software funzionalmente equivalenti vengono sviluppate in modo indipendente da team diversi. Questo può essere applicato a componenti critici come il motore di inferenza del modello in un sistema ML. Più versioni del motore di inferenza possono essere eseguite in parallelo e i loro output possono essere confrontati per coerenza. È considerato il risultato corretto se la maggior parte delle versioni produce lo stesso output. Se c’è una discrepanza, indica un potenziale errore in una o più versioni e possono essere attivati meccanismi di gestione degli errori appropriati. Un altro esempio è l’utilizzo di codici di correzione degli errori basati su software, come i codici Reed-Solomon (Plank 1997), per rilevare e correggere errori nell’archiviazione o nella trasmissione dei dati, come mostrato in Figura 17.17. Questi codici aggiungono ridondanza ai dati, consentendo di rilevare e correggere determinati errori e migliorare la tolleranza agli errori del sistema.\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, e D. I. August. 2005. «SWIFT: Software Implemented Fault Tolerance». In International Symposium on Code Generation and Optimization, 243–54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\nPlank, James S. 1997. «A tutorial on ReedSolomon coding for fault-tolerance in RAID-like systems». Software: Practice and Experience 27 (9): 995–1012.\n\n\n\n\n\n\nFigura 17.17: Rappresentazione a n bit dei codici Reed-Solomon. Fonte: GeeksforGeeks\n\n\n\n\n\n\n\n\n\nEsercizio 17.1: Rilevamento delle Anomalie\n\n\n\n\n\nIn questo Colab, si svolge il ruolo di un detective di guasti IA! Si costruirà un rilevatore di anomalie basato su autoencoder per individuare gli errori nei dati sulla salute cardiaca. Si scopre come identificare i malfunzionamenti nei sistemi ML, un’abilità fondamentale per creare un’IA affidabile. Utilizzeremo Keras Tuner per mettere a punto l’autoencoder per un rilevamento di guasti di prim’ordine. Questa esperienza si collega direttamente al capitolo Robust AI, dimostrando l’importanza del rilevamento di guasti in applicazioni reali come l’assistenza sanitaria e i sistemi autonomi. Preparatevi a rafforzare l’affidabilità delle creazioni IA!\n\n\n\n\n\n\n\n\n17.3.5 Riepilogo\nTabella 17.1 fornisce un’analisi comparativa estesa di guasti transitori, permanenti e intermittenti. Descrive le caratteristiche o dimensioni primarie che distinguono questi tipi di guasti. Qui, riassumiamo le dimensioni rilevanti che abbiamo esaminato ed esploriamo le sfumature che differenziano i guasti transitori, permanenti e intermittenti in modo più dettagliato.\n\n\n\nTabella 17.1: Confronto tra guasti transitori, permanenti e intermittenti.\n\n\n\n\n\n\n\n\n\n\n\nDimensione\nGuasti Transitori\nGuasti Permanenti\nGuasti intermittenti\n\n\n\n\nDurata\nDi breve durata, temporaneo\nPersistente, rimane fino alla riparazione o alla sostituzione\nSporadica, appare e scompare in modo intermittente\n\n\nPersistenza\nScompare dopo che la condizione di errore è passata\nÈ costantemente presente finché non viene affrontato\nSi ripete in modo irregolare, non sempre presente\n\n\nCause\nFattori esterni (ad esempio, interferenza elettromagnetica raggi cosmici)\nDifetti hardware, danni fisici, usura\nCondizioni hardware instabili, connessioni allentate, componenti obsoleti\n\n\nManifestazione\nBit flip, glitch, danneggiamento temporaneo dei dati\nErrori bloccati, componenti rotti, guasti completi del dispositivo\nBit flip occasionali, problemi di segnale intermittenti, malfunzionamenti sporadici\n\n\nImpatto sui Sistemi ML\nIntroduce errori temporanei o rumore nei calcoli\nCausa errori o guasti costanti, che influiscono sull’affidabilità\nPorta a errori sporadici e imprevedibili, difficili da diagnosticare e mitigare\n\n\nRilevamento\nCodici di rilevamento degli errori, confronto con i valori previsti\nAutotest integrati, codici di rilevamento degli errori, controlli di coerenza\nMonitoraggio delle anomalie, analisi di modelli di errore e correlazioni\n\n\nMitigazione\nCodici di correzione degli errori, ridondanza, checkpoint e riavvio\nRiparazione o sostituzione hardware, ridondanza dei componenti, meccanismi di failover\nProgettazione robusta, controllo ambientale, monitoraggio del runtime, tecniche di tolleranza agli errori",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html#robustezza-del-modello-ml",
    "href": "contents/robust_ai/robust_ai.it.html#robustezza-del-modello-ml",
    "title": "17  IA Robusta",
    "section": "17.4 Robustezza del Modello ML",
    "text": "17.4 Robustezza del Modello ML\n\n17.4.1 Attacchi Avversari\n\nDefinizione e Caratteristiche\nGli attacchi avversari mirano a indurre i modelli a fare previsioni errate fornendo loro input ingannevoli appositamente creati (chiamati esempi avversari) (Parrish et al. 2023). Aggiungendo lievi perturbazioni ai dati di input, gli avversari possono “hackerare” il riconoscimento di pattern di un modello e ingannarlo. Si tratta di tecniche sofisticate in cui piccole, spesso impercettibili modifiche ai dati di input possono indurre un modello ML a fare una previsione errata, come mostrato in Figura 17.18.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. «Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models». ArXiv preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\n\n\n\n\nFigura 17.18: Un piccolo rumore avversario aggiunto all’immagine originale può far sì che la rete neurale classifichi l’immagine come un Guacamole anziché come un gatto egiziano. Fonte: Sutanto\n\n\n\nÈ possibile generare prompt che portano a immagini non sicure in modelli testo-immagine come DALLE (Ramesh et al. 2021) o Stable Diffusion (Rombach et al. 2022). Ad esempio, alterando i valori dei pixel di un’immagine, gli aggressori possono ingannare un sistema di riconoscimento facciale facendogli identificare un volto come una persona diversa.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. «Zero-Shot Text-to-Image Generation». In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. «High-Resolution Image Synthesis with Latent Diffusion Models». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\nGli attacchi avversari sfruttano il modo in cui i modelli ML apprendono e prendono decisioni durante l’inferenza. Questi modelli funzionano sul principio di riconoscimento di pattern nei dati. Un avversario crea input speciali con perturbazioni per confondere il riconoscimento degli schemi del modello, in pratica “hackerando” le percezioni del modello.\nGli attacchi avversari rientrano in diversi scenari:\n\nAttacchi Whitebox: L’attaccante conosce perfettamente il funzionamento interno del modello target, inclusi i dati di training, i parametri e l’architettura (Ye e Hamidi 2021). Questo accesso completo crea condizioni favorevoli per gli aggressori per sfruttare le vulnerabilità del modello. L’attaccante può usare debolezze specifiche e sottili per creare esempi avversari efficaci.\nAttacchi Blackbox: A differenza degli attacchi White-box, i Black-box implicano che l’attaccante abbia poca o nessuna conoscenza del modello target (Guo et al. 2019). Per eseguire l’attacco, l’attore avversario deve osservare attentamente il comportamento dell’output del modello.\nAttacchi Greybox: Si collocano tra gli attacchi Blackbox e Whitebox. L’attaccante ha solo una conoscenza parziale della progettazione interna del modello target (Xu et al. 2021). Ad esempio, l’attaccante potrebbe avere conoscenza dei dati di training ma non dell’architettura o dei parametri. Nel mondo reale, gli attacchi pratici rientrano solitamente nelle categorie black-box o grey-box.\n\n\nYe, Linfeng, e Shayan Mohajer Hamidi. 2021. «Thundernna: A white box adversarial attack». arXiv preprint arXiv:2111.12305.\n\nGuo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, e Kilian Weinberger. 2019. «Simple black-box adversarial attacks». In International conference on machine learning, 2484–93. PMLR.\n\nXu, Ying, Xu Zhong, Antonio Jimeno Yepes, e Jey Han Lau. 2021. «Grey-box adversarial attack and defence for sentiment classification». arXiv preprint arXiv:2103.11576.\nIl panorama dei modelli di apprendimento automatico è complesso e ampio, soprattutto data la loro relativamente recente integrazione nelle applicazioni commerciali. Questa rapida adozione, sebbene trasformativa, ha portato alla luce numerose vulnerabilità all’interno di questi modelli. Di conseguenza, sono emersi vari metodi di attacco avversari, ognuno dei quali sfrutta strategicamente diversi aspetti di vari modelli. Di seguito, evidenziamo un sottoinsieme di questi metodi, che mostra la natura multiforme degli attacchi avversari sui modelli di apprendimento automatico:\n\nLe Generative Adversarial Network (GAN) sono modelli di deep learning costituiti da due reti in competizione tra loro: un generatore e un discriminatore (Goodfellow et al. 2020). Il generatore cerca di sintetizzare dati realistici mentre il discriminatore valuta se sono reali o falsi. Le GAN possono essere utilizzate per creare esempi avversari. La rete del generatore è addestrata per produrre input che il modello target classifica erroneamente. Queste immagini generate da GAN possono quindi attaccare un classificatore target o un modello di rilevamento. Il generatore e il modello target sono impegnati in un processo competitivo, con il generatore che migliora continuamente la sua capacità di creare esempi ingannevoli e il modello target che aumenta la sua resistenza a tali esempi. Le reti GAN forniscono un potente framework per la creazione di input avversari complessi e diversificati, dimostrando l’adattabilità dei modelli generativi nel panorama avversario.\nI Transfer Learning Adversarial Attacks [attacchi avversari di apprendimento di trasferimento] sfruttano la conoscenza trasferita da un modello pre-addestrato a un modello target, creando esempi avversari che possono ingannare entrambi i modelli. Questi attacchi rappresentano una preoccupazione crescente, in particolare quando gli avversari hanno conoscenza dell’estrattore di feature ma non hanno accesso alla testa di classificazione (la parte o il layer responsabile della creazione delle classificazioni finali). Denominate “attacchi headless”, queste strategie avversarie trasferibili sfruttano le capacità espressive degli estrattori di feature per creare perturbazioni, senza tenere conto dello spazio delle etichette o dei dati di addestramento. L’esistenza di tali attacchi sottolinea l’importanza di sviluppare difese robuste per le applicazioni di apprendimento tramite trasferimento, soprattutto perché i modelli pre-addestrati sono comunemente utilizzati (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Commun. ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. «Headless Horseman: Adversarial Attacks on Transfer Learning Models». In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nMeccanismi degli Attacchi Avversari\n\n\n\n\n\n\nFigura 17.19: Attacchi Basati sul Gradiente. Fonte: Ivezic\n\n\n\nAttacchi Basati sul Gradiente\nUna categoria importante di attacchi avversari è quella degli attacchi basati sul gradiente. Questi attacchi sfruttano i gradienti della funzione di perdita del modello ML per creare esempi avversari. Il Fast Gradient Sign Method (FGSM) è una tecnica ben nota in questa categoria. FGSM perturba i dati di input aggiungendo un piccolo rumore nella direzione del gradiente, con l’obiettivo di massimizzare l’errore di previsione del modello. FGSM può generare rapidamente esempi avversari, come mostrato in Figura 17.19, eseguendo un singolo passaggio nella direzione del gradiente.\nUn’altra variante, l’attacco “Projected Gradient Descent (PGD)”, estende FGSM applicando iterativamente la fase di aggiornamento del gradiente, consentendo esempi avversari più raffinati e potenti. L’attacco “Jacobian-based Saliency Map (JSMA)” è un altro approccio basato sul gradiente che identifica le caratteristiche di input più influenti e le perturba per creare esempi avversari.\nAttacchi Basati sull’Ottimizzazione\nQuesti attacchi formulano la generazione di esempi avversari come un problema di ottimizzazione. L’attacco Carlini e Wagner (C&W) è un esempio importante in questa categoria. Trova la perturbazione più piccola che può causare una classificazione errata mantenendo la somiglianza percettiva con l’input originale. L’attacco C&W impiega un processo di ottimizzazione iterativo per ridurre al minimo la perturbazione massimizzando al contempo l’errore di previsione del modello.\nUn altro approccio basato sull’ottimizzazione è l’Elastic Net Attack to DNNs (EAD), che incorpora la regolarizzazione elastica della rete per generare esempi avversari con perturbazioni sparse.\nAttacchi Basati sul Trasferimento\nGli attacchi basati sul trasferimento sfruttano la proprietà di trasferibilità degli esempi avversari. La trasferibilità si riferisce al fenomeno per cui gli esempi avversari creati per un modello ML possono spesso ingannare altri modelli, anche se hanno architetture diverse o sono stati addestrati su set di dati diversi. Ciò consente agli aggressori di generare esempi avversari utilizzando un modello surrogato e quindi trasferirli al modello target senza richiedere l’accesso diretto ai suoi parametri o gradienti. Gli attacchi basati sul trasferimento evidenziano la generalizzazione delle vulnerabilità avversarie su diversi modelli e il potenziale per attacchi black-box.\nAttacchi nel Mondo Fisico\nGli attacchi nel mondo fisico portano gli esempi avversari nel regno degli scenari del mondo reale. Questi attacchi comportano la creazione di oggetti fisici o manipolazioni che possono ingannare i modelli ML quando vengono catturati da sensori o telecamere. Le patch avversarie, ad esempio, sono piccole patch progettate con cura che possono essere posizionate sugli oggetti per ingannare i modelli di rilevamento o classificazione degli oggetti. Quando vengono applicate a oggetti del mondo reale, queste patch possono causare una classificazione errata dei modelli o il mancato rilevamento accurato degli oggetti. Gli oggetti avversari, come sculture stampate in 3D o segnali stradali modificati, possono anche essere creati per ingannare i sistemi ML in ambienti fisici.\nRiepilogo\nTabella 17.2 una panoramica concisa delle diverse categorie di attacchi avversari, tra cui attacchi basati su gradiente (FGSM, PGD, JSMA), attacchi basati sull’ottimizzazione (C&W, EAD), attacchi basati sul trasferimento e attacchi nel mondo fisico (patch e oggetti avversari). Ogni attacco viene brevemente descritto, evidenziandone le caratteristiche e i meccanismi principali.\n\n\n\nTabella 17.2: Diversi tipi di attacco sui modelli ML.\n\n\n\n\n\n\n\n\n\n\nCategoria di attacco\nNome attacco\nDescrizione\n\n\n\n\nBasato sul gradiente\nFast Gradient Sign Method (FGSM) Projected Gradient Descent (PGD) Jacobian-based Saliency Map Attack (JSMA)\nPerturba i dati di input aggiungendo un piccolo rumore nella direzione del gradiente per massimizzare l’errore di previsione. Estende FGSM applicando iterativamente il passaggio di aggiornamento del gradiente per esempi avversari più raffinati. Identifica le caratteristiche di input influenti e le perturba per creare esempi avversari.\n\n\nBasato sull’ottimizzazione\nCarlini and Wagner (C&W) Attack Elastic Net Attack to DNNs (EAD)\nTrova la perturbazione più piccola che causa una classificazione errata mantenendo la somiglianza percettiva. Incorpora la regolarizzazione elastica della rete per generare esempi avversari con perturbazioni sparse.\n\n\nBasato sul trasferimento\nTransferability-based Attacks\nSfrutta la trasferibilità di esempi avversari su modelli diversi, consentendo attacchi black-box.\n\n\nMondo fisico\nAdversarial Patches Adversarial Objects\nPiccole patch attentamente progettate, posizionate sugli oggetti per ingannare i modelli di rilevamento o classificazione degli oggetti. Oggetti fisici (ad esempio, sculture stampate in 3D, segnali stradali modificati) creati per ingannare i sistemi ML in scenari del mondo reale.\n\n\n\n\n\n\nI meccanismi degli attacchi avversari rivelano l’intricata interazione tra i limiti decisionali del modello ML, i dati di input e gli obiettivi dell’attaccante. Manipolando attentamente i dati di input, gli aggressori possono sfruttare le sensibilità e i punti ciechi del modello, portando a previsioni errate. Il successo degli attacchi avversari evidenzia la necessità di una comprensione più approfondita delle proprietà di robustezza e generalizzazione dei modelli ML.\nLa difesa dagli attacchi avversari richiede un approccio multiforme. L’addestramento avversario è una strategia di difesa comune in cui i modelli vengono addestrati su esempi avversari per migliorare la robustezza. Esporre il modello a esempi avversari durante l’addestramento gli insegna a classificarli correttamente e a diventare più resiliente agli attacchi. La distillazione difensiva, la preelaborazione degli input e i metodi di ensemble sono altre tecniche che possono aiutare a mitigare l’impatto degli attacchi avversari.\nMan mano che l’apprendimento automatico avversario si evolve, i ricercatori esplorano nuovi meccanismi di attacco e sviluppano difese più sofisticate. La corsa agli armamenti tra aggressori e difensori spinge la necessità di innovazione e vigilanza costanti nel proteggere i sistemi ML dalle minacce avversarie. Comprendere i meccanismi degli attacchi avversari è fondamentale per sviluppare modelli ML robusti e affidabili in grado di resistere al panorama in continua evoluzione degli esempi avversari.\n\n\nImpatto sui Sistemi ML\nGli attacchi avversari sui sistemi di apprendimento automatico sono emersi come una preoccupazione significativa negli ultimi anni, evidenziando le potenziali vulnerabilità e i rischi associati all’adozione diffusa delle tecnologie ML. Questi attacchi comportano perturbazioni attentamente studiate per immettere dati che possono ingannare o fuorviare i modelli ML, portando a previsioni errate o classificazioni errate, come mostrato in Figura 17.20. L’impatto degli attacchi avversari sui sistemi ML è di vasta portata e può avere gravi conseguenze in vari domini.\nUn esempio lampante dell’impatto degli attacchi avversari è stato dimostrato dai ricercatori nel 2017. Hanno sperimentato piccoli adesivi in bianco e nero sui segnali di stop (Eykholt et al. 2017). All’occhio umano, questi adesivi non oscuravano il segnale né ne impedivano l’interpretazione. Tuttavia, quando le immagini dei segnali di stop modificati dagli adesivi sono state inserite nei modelli ML standard di classificazione dei segnali stradali, è emerso un risultato scioccante. I modelli hanno classificato erroneamente i segnali di stop come segnali di limite di velocità nell’85% dei casi.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. «Robust Physical-World Attacks on Deep Learning Models». ArXiv preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\nQuesta dimostrazione ha fatto luce sul potenziale allarmante di semplici adesivi avversari per ingannare i sistemi ML e fargli interpretare male i segnali stradali critici. Le implicazioni di tali attacchi nel mondo reale sono significative, in particolare nel contesto dei veicoli autonomi. Se utilizzati su strade reali, questi adesivi avversari potrebbero far sì che le auto a guida autonoma interpretino erroneamente i segnali di stop come limiti di velocità, portando a situazioni pericolose, come mostrato in Figura 17.21. I ricercatori hanno avvertito che ciò potrebbe causare arresti a rotazione o accelerazioni involontarie negli incroci, mettendo a repentaglio la sicurezza pubblica.\n\n\n\n\n\n\nFigura 17.20: Generazione di esempi avversari applicata a GoogLeNet (Szegedy et al., 2014a) su ImageNet. Fonte: Goodfellow\n\n\n\n\n\n\n\n\n\nFigura 17.21: I graffiti su un segnale di stop hanno ingannato un’auto a guida autonoma facendole credere che si trattasse di un segnale di limite di velocità di 45 mph. Fonte: Eykholt\n\n\n\nIl caso di studio degli adesivi avversari sui segnali di stop fornisce un’illustrazione concreta di come gli esempi avversari sfruttino il modo in cui i modelli ML riconoscono i pattern. Manipolando in modo sottile i dati di input in modi invisibili agli esseri umani, gli aggressori possono indurre previsioni errate e creare gravi rischi, specialmente in applicazioni critiche per la sicurezza come i veicoli autonomi. La semplicità dell’attacco evidenzia la vulnerabilità dei modelli ML anche a piccole modifiche nell’input, sottolineando la necessità di difese robuste contro tali minacce.\nL’impatto degli attacchi avversari si estende oltre il degrado delle prestazioni del modello. Questi attacchi sollevano notevoli preoccupazioni in termini di sicurezza e protezione, in particolare nei domini in cui i modelli ML sono utilizzati per prendere decisioni critiche. Nelle applicazioni sanitarie, gli attacchi avversari sui modelli di imaging medico potrebbero portare a diagnosi errate o raccomandazioni di trattamento errate, mettendo a repentaglio il benessere del paziente (M.-J. Tsai, Lin, e Lee 2023). Nei sistemi finanziari, gli attacchi avversari potrebbero consentire frodi o manipolazioni di algoritmi di trading, con conseguenti perdite economiche sostanziali.\n\nTsai, Min-Jen, Ping-Yi Lin, e Ming-En Lee. 2023. «Adversarial Attacks on Medical Image Classification». Cancers 15 (17): 4228. https://doi.org/10.3390/cancers15174228.\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, e Evgeny Burnaev. 2021. «Adversarial Attacks on Deep Models for Financial Transaction Records». In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, 2868–78. ACM. https://doi.org/10.1145/3447548.3467145.\nInoltre, le vulnerabilità avversarie compromettono l’affidabilità e l’interpretabilità dei modelli ML. Se perturbazioni attentamente realizzate possono facilmente ingannare i modelli, la fiducia nelle loro previsioni e decisioni si erode. Gli esempi avversari espongono la dipendenza dei modelli da modelli superficiali e l’incapacità di catturare i veri concetti sottostanti, mettendo in discussione l’affidabilità dei sistemi ML (Fursov et al. 2021).\nLa difesa dagli attacchi avversari richiede spesso risorse computazionali aggiuntive e può influire sulle prestazioni complessive del sistema. Tecniche come l’addestramento avversariale, in cui i modelli vengono addestrati su esempi avversari per migliorare la robustezza, possono aumentare significativamente i tempi di addestramento e i requisiti computazionali (Bai et al. 2021). I meccanismi di rilevamento e mitigazione del runtime, come la preelaborazione dell’input (Addepalli et al. 2020) o i controlli di coerenza delle previsioni, introducono latenza e influenzano le prestazioni in tempo reale dei sistemi ML.\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, e Qian Wang. 2021. «Recent advances in adversarial training for adversarial robustness». arXiv preprint arXiv:2102.01356.\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, e R. Venkatesh Babu. 2020. «Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1020–29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\nLa presenza di vulnerabilità avversarie complica anche l’implementazione e la manutenzione dei sistemi ML. I progettisti e gli operatori di sistema devono considerare il potenziale di attacchi avversari e incorporare difese e meccanismi di monitoraggio appropriati. Aggiornamenti regolari e riqualificazione dei modelli diventano necessari per adattarsi alle nuove tecniche avversarie e mantenere la sicurezza e le prestazioni del sistema nel tempo.\nL’impatto degli attacchi avversari sui sistemi ML è significativo e multiforme. Questi attacchi espongono le vulnerabilità dei modelli ML, dal degrado delle prestazioni del modello e dall’aumento di preoccupazioni sulla sicurezza e la protezione alla sfida dell’affidabilità e dell’interpretabilità del modello. Sviluppatori e ricercatori devono dare priorità allo sviluppo di difese e contromisure robuste per mitigare i rischi posti dagli attacchi avversari. Affrontando queste sfide, possiamo creare sistemi ML più sicuri, affidabili e degni di fiducia in grado di resistere al panorama in continua evoluzione delle minacce avversarie.\n\n\n\n\n\n\nEsercizio 17.2: Attacchi Avversari\n\n\n\n\n\nPreparatevi a diventare un avversario dell’IA! In questo Colab, si diventerà un hacker white-box, imparando a creare attacchi che ingannano i modelli di classificazione delle immagini. Ci concentreremo sul Fast Gradient Sign Method (FGSM), sfruttando i gradienti di un modello contro di esso! Si distorceranno deliberatamente le immagini con piccole perturbazioni, osservando come inganneranno sempre più intensamente l’IA. Questo esercizio pratico evidenzia l’importanza di creare un’IA sicura, un’abilità critica man mano che l’IA si integra nelle auto e nell’assistenza sanitaria. Il Colab si collega direttamente al capitolo Robust AI del libro, spostando gli attacchi avversari dalla teoria alla esperienza pratica.\n\nPensate di poter superare in astuzia un’IA? In questo Colab, scopriremo come ingannare i modelli di classificazione delle immagini con attacchi avversari. Utilizzeremo metodi come FGSM per modificare le immagini e ingannare sottilmente l’IA. Scopriremo come progettare patch di immagini ingannevoli e osserveremo la sorprendente vulnerabilità di questi potenti modelli. Questa è una conoscenza fondamentale per costruire sistemi di IA veramente robusti!\n\n\n\n\n\n\n\n17.4.2 Avvelenamento dei Dati\n\nDefinizione e Caratteristiche\nL’avvelenamento dei dati è un attacco in cui i dati di addestramento vengono manomessi, portando alla compromissione del modello (Biggio, Nelson, e Laskov 2012), come mostrato in Figura 17.22. Gli aggressori possono modificare gli esempi di training esistenti, inserire nuovi dati dannosi o influenzare il processo di raccolta dati. I dati avvelenati vengono etichettati in modo tale da alterare il comportamento appreso del modello. Ciò può essere particolarmente dannoso nelle applicazioni in cui i modelli ML prendono decisioni automatizzate in base a pattern appresi. Oltre ai set di training, i test di avvelenamento e i dati di convalida possono consentire agli avversari di aumentare artificialmente le prestazioni del modello segnalate.\n\nBiggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. «Poisoning Attacks against Support Vector Machines». In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\n\n\n\n\nFigura 17.22: Effetti dell’Avvelenamento di NightShade sulla Diffusione Stabile. Fonte: TOMÉ\n\n\n\nIl processo di solito prevede i seguenti passaggi:\n\nInjection: L’aggressore aggiunge esempi errati o fuorvianti al set di training. Questi esempi sono spesso progettati per sembrare normali a un’ispezione superficiale, ma sono stati attentamente elaborati per interrompere il processo di apprendimento.\nTraining: Il modello ML si allena su questo set di dati manipolato e sviluppa comprensioni distorte dei modelli di dati.\nDeployment: Una volta distribuito il modello, l’addestramento corrotto porta a un processo decisionale imperfetto o a vulnerabilità prevedibili che l’aggressore può sfruttare.\n\nL’impatto dell’avvelenamento dei dati si estende oltre gli errori di classificazione o i cali di accuratezza. In applicazioni critiche come l’assistenza sanitaria, tali alterazioni possono portare a significativi problemi di fiducia e sicurezza (Marulli, Marrone, e Verde 2022). Più avanti, discuteremo alcuni casi di studio di questi problemi.\n\nMarulli, Fiammetta, Stefano Marrone, e Laura Verde. 2022. «Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain». Journal of Sensor and Actuator Networks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\nOprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. «Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?» Computer 55 (11): 94–99. https://doi.org/10.1109/mc.2022.3190787.\nEsistono sei categorie principali di avvelenamento dei dati (Oprea, Singhal, e Vassilev 2022):\n\nAttacchi alla Disponibilità: Questi attacchi mirano a compromettere la funzionalità complessiva di un modello. Fanno sì che classifichi erroneamente la maggior parte dei campioni di test, rendendo il modello inutilizzabile per applicazioni pratiche. Un esempio è il “label flipping”, in cui le etichette di una classe specifica e mirata vengono sostituite con etichette di una classe diversa.\nAttacchi Mirati: A differenza degli attacchi alla disponibilità, gli attacchi mirati mirano a compromettere un piccolo numero di campioni di test. Quindi, l’effetto è localizzato a un numero limitato di classi, mentre il modello mantiene lo stesso livello originale di accuratezza per la maggior parte delle classi. La natura mirata dell’attacco richiede che l’aggressore conosca le classi del modello, rendendo più difficile il rilevamento di questi attacchi.\nAttacchi Backdoor: In questi attacchi, un avversario prende di mira modelli specifici nei dati. L’aggressore introduce una backdoor (un trigger o uno schema nascosto e dannoso) nei dati di training, ad esempio manipolando determinate feature nei dati strutturati o manipolando un pattern di pixel in una posizione fissa. Ciò fa sì che il modello associ il modello dannoso a etichette specifiche. Di conseguenza, quando il modello incontra campioni di prova che contengono un pattern dannoso, effettua previsioni false.\nAttacchi di Sotto-popolazione: Gli aggressori scelgono selettivamente di compromettere un sottoinsieme dei campioni di test mantenendo l’accuratezza sul resto dei campioni. Questi attacchi si possono pensare come una combinazione di attacchi di disponibilità e mirati: eseguire attacchi di disponibilità (degrado delle prestazioni) nell’ambito di un sottoinsieme mirato. Sebbene gli attacchi di sottopopolazione possano sembrare molto simili agli attacchi mirati, i due presentano chiare differenze:\nScope: Mentre gli attacchi mirati prendono di mira un set selezionato di campioni, gli attacchi di sotto-popolazione prendono di mira una sotto-popolazione generale con rappresentazioni di caratteristiche simili. Ad esempio, in un attacco mirato, un aggressore inserisce immagini manipolate di un cartello di avvertimento di “dosso” (con perturbazioni o schemi accuratamente studiati), che fanno sì che un’auto autonoma non riesca a riconoscere tale cartello e rallenti. D’altro canto, manipolare tutti i campioni di persone con accento britannico in modo che un modello di riconoscimento vocale classifichi erroneamente il discorso di una persona britannica è un esempio di attacco di sotto-popolazione.\nConoscenza: Mentre gli attacchi mirati richiedono un alto grado di familiarità con i dati, gli attacchi alla sotto-popolazione richiedono una conoscenza meno approfondita per essere efficaci.\n\nLe caratteristiche del data poisoning includono:\nManipolazioni sottili e difficili da rilevare dei dati di training: Il data poisoning spesso comporta manipolazioni sottili dei dati di training che sono attentamente studiate per essere difficili da rilevare tramite un’ispezione casuale. Gli aggressori impiegano tecniche sofisticate per garantire che i campioni avvelenati si fondano perfettamente con i dati legittimi, rendendoli più facili da identificare con un’analisi approfondita. Queste manipolazioni possono mirare a caratteristiche o attributi specifici dei dati, come l’alterazione di valori numerici, la modifica di etichette categoriali o l’introduzione di modelli attentamente progettati. L’obiettivo è influenzare il processo di apprendimento del modello eludendo il rilevamento, consentendo ai dati avvelenati di corrompere sottilmente il comportamento del modello.\nPuò essere eseguito da insider o aggressori esterni: Gli attacchi di data poisoning possono essere eseguiti da vari attori, tra cui insider malintenzionati con accesso ai dati di training e aggressori esterni che trovano modi per influenzare la raccolta dati o la pipeline di pre-elaborazione. Gli insider rappresentano una minaccia significativa perché spesso hanno accesso privilegiato e conoscenza del sistema, il che consente loro di introdurre dati avvelenati senza destare sospetti. D’altro canto, gli aggressori esterni possono sfruttare le vulnerabilità nell’approvvigionamento dei dati, nelle piattaforme di crowdsourcing o nei processi di aggregazione dei dati per iniettare campioni avvelenati nel set di dati di addestramento. Ciò evidenzia l’importanza di implementare controlli di accesso rigorosi, policy di governance dei dati e meccanismi di monitoraggio per mitigare il rischio di minacce interne e attacchi esterni.\nSfrutta le vulnerabilità nella raccolta e pre-elaborazione dei dati: Gli attacchi di avvelenamento dei dati spesso sfruttano le vulnerabilità nelle fasi di raccolta e pre-elaborazione dei dati della pipeline di apprendimento automatico. Gli aggressori progettano attentamente campioni avvelenati per eludere le comuni tecniche di convalida dei dati, assicurandosi che i dati manipolati rientrino comunque in intervalli accettabili, seguano le distribuzioni previste o mantengano la coerenza con altre funzionalità. Ciò consente ai dati avvelenati di passare attraverso le fasi di pre-elaborazione dei dati senza essere rilevati. Inoltre, gli attacchi di avvelenamento possono sfruttare le debolezze nella preelaborazione dei dati, come una pulizia dei dati inadeguata, un rilevamento insufficiente di valori anomali o la mancanza di controlli di integrità. Gli aggressori possono anche sfruttare la mancanza di solidi meccanismi di tracciamento della provenienza e della discendenza dei dati per introdurre dati avvelenati senza lasciare una traccia. Per affrontare queste vulnerabilità sono necessarie rigorose tecniche di convalida dei dati, rilevamento delle anomalie e tracciamento della provenienza dei dati per garantire l’integrità e l’affidabilità dei dati di training.\nInterrompe il processo di apprendimento e distorce il comportamento del modello: Gli attacchi di avvelenamento dei dati sono progettati per interrompere il processo di apprendimento dei modelli di apprendimento automatico e distorcere il loro comportamento verso gli obiettivi dell’aggressore. I dati avvelenati vengono in genere manipolati con obiettivi specifici, come distorcere il comportamento del modello verso determinate classi, introdurre backdoor o degradare le prestazioni complessive. Queste manipolazioni non sono casuali, ma mirate a ottenere i risultati desiderati dall’aggressore. Introducendo incongruenze nelle etichette, in cui i campioni manipolati hanno etichette che non si allineano con la loro vera natura, gli attacchi di avvelenamento possono confondere il modello durante l’addestramento e portare a previsioni distorte o errate. L’interruzione causata dai dati avvelenati può avere conseguenze di vasta portata, poiché il modello compromesso può prendere decisioni imperfette o mostrare un comportamento indesiderato quando viene distribuito in applicazioni del mondo reale.\nInfluisce sulle prestazioni, l’equità e l’affidabilità del modello: I dati avvelenati nel dataset di addestramento possono avere gravi implicazioni sulle prestazioni, l’equità e l’affidabilità dei modelli di apprendimento automatico. I dati avvelenati possono degradare l’accuratezza e le prestazioni del modello addestrato, portando a un aumento delle classificazioni errate o degli errori nelle previsioni. Ciò può avere conseguenze significative, soprattutto nelle applicazioni critiche in cui gli output del modello influenzano decisioni importanti. Inoltre, gli attacchi di avvelenamento possono introdurre distorsioni e problemi di equità, facendo sì che il modello prenda decisioni discriminatorie o ingiuste per determinati sottogruppi o classi. Ciò mina le responsabilità etiche e sociali dei sistemi di apprendimento automatico e può perpetuare o amplificare i pregiudizi esistenti. Inoltre, i dati avvelenati erodono l’affidabilità e la credibilità dell’intero sistema di apprendimento automatico. Gli output del modello diventano discutibili e potenzialmente dannosi, portando a una perdita di fiducia nell’integrità del sistema. L’impatto dei dati avvelenati può propagarsi nell’intera pipeline ML, influenzando i componenti downstream e le decisioni che si basano sul modello compromesso. Per affrontare queste preoccupazioni è necessaria una solida governance dei dati, un auditing regolare del modello e un monitoraggio continuo per rilevare e mitigare gli effetti degli attacchi di avvelenamento dei dati.\n\n\nMeccanismi di Avvelenamento dei Dati\nGli attacchi di avvelenamento dei dati possono essere eseguiti tramite vari meccanismi, sfruttando diverse vulnerabilità della pipeline ML. Questi meccanismi consentono agli aggressori di manipolare i dati di training e introdurre campioni dannosi che possono compromettere le prestazioni, l’equità o l’integrità del modello. Comprendere questi meccanismi è fondamentale per sviluppare difese efficaci contro l’avvelenamento dei dati e garantire la robustezza dei sistemi ML. I meccanismi di avvelenamento dei dati possono essere ampiamente categorizzati in base all’approccio dell’aggressore e alla fase della pipeline ML a cui mirano. Alcuni meccanismi comuni includono la modifica delle etichette dei dati di training, l’alterazione dei valori delle feature, l’iniezione di campioni dannosi accuratamente realizzati, lo sfruttamento delle vulnerabilità di raccolta e pre-elaborazione dei dati, la manipolazione dei dati alla fonte, l’avvelenamento dei dati in scenari di apprendimento online e la collaborazione con addetti ai lavori per manipolare i dati.\nOgnuno di questi meccanismi presenta sfide uniche e richiede diverse strategie di mitigazione. Ad esempio, rilevare la manipolazione delle etichette può comportare l’analisi della distribuzione delle etichette e l’identificazione delle anomalie (Zhou et al. 2018), mentre prevenire la manipolazione delle feature può richiedere tecniche di pre-elaborazione dei dati e rilevamento delle anomalie sicure (Carta et al. 2020). La difesa dalle minacce interne può comportare rigide policy di controllo degli accessi e il monitoraggio dei modelli di accesso ai dati. Inoltre, l’efficacia degli attacchi di avvelenamento dei dati spesso dipende dalla conoscenza del sistema ML da parte dell’attaccante, tra cui l’architettura del modello, gli algoritmi di training e la distribuzione dei dati. Gli aggressori possono utilizzare tecniche di apprendimento automatico avversario o di sintesi dei dati per creare campioni che hanno maggiori probabilità di aggirare il rilevamento e raggiungere i loro obiettivi malevoli.\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, e Larry S. Davis. 2018. «Learning Rich Features for Image Manipulation Detection». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1053–61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero, e Roberto Saia. 2020. «A Local Feature Engineering Strategy to Improve Network Anomaly Detection». Future Internet 12 (10): 177. https://doi.org/10.3390/fi12100177.\n\n\n\n\n\n\nFigura 17.23: Garbage In – Garbage Out. Fonte: Information Matters\n\n\n\nModifica delle etichette dei dati di training: Uno dei meccanismi più semplici di avvelenamento dei dati è la modifica delle etichette dei dati di training. In questo approccio, l’aggressore modifica selettivamente le etichette di un sottoinsieme dei campioni di training per fuorviare il processo di apprendimento del modello, come mostrato in Figura 17.23. Ad esempio, in un’attività di classificazione binaria, l’aggressore potrebbe capovolgere le etichette di alcuni campioni positivi in negativi o viceversa. Introducendo tale rumore di etichetta, l’aggressore degrada le prestazioni del modello o fa sì che faccia previsioni errate per istanze target specifiche.\nAlterazione dei valori delle feature nei dati di training: Un altro meccanismo di avvelenamento dei dati consiste nell’alterare i valori delle caratteristiche dei campioni di training senza modificare le etichette. L’aggressore elabora attentamente i valori delle feature per introdurre specifici pregiudizi o vulnerabilità nel modello. Ad esempio, in un’attività di classificazione delle immagini, l’aggressore potrebbe aggiungere perturbazioni impercettibili a un sottoinsieme di immagini, facendo sì che il modello apprenda un particolare schema o associazione. Questo tipo di avvelenamento può creare backdoor o trojan nel modello addestrato, che possono essere attivati da specifici schemi di input.\nIniezione di campioni dannosi accuratamente realizzati: In questo meccanismo, l’aggressore crea campioni dannosi progettati per avvelenare il modello. Questi campioni sono realizzati per avere un impatto specifico sul comportamento del modello, mentre si fondono con i dati di addestramento legittimi. L’aggressore potrebbe utilizzare tecniche come perturbazioni avversarie o sintesi dei dati per generare campioni avvelenati difficili da rilevare. L’aggressore manipola i limiti decisionali del modello iniettando questi campioni dannosi nei dati di addestramento o introducendo classificazioni errate mirate.\nSfruttamento delle vulnerabilità di raccolta e preelaborazione dei dati: Gli attacchi di avvelenamento dei dati possono anche sfruttare le vulnerabilità della pipeline di raccolta e preelaborazione dei dati. Se il processo di raccolta dati non è sicuro o ci sono debolezze nelle fasi di pre-elaborazione dei dati, un aggressore può manipolare i dati prima che raggiungano la fase di addestramento. Ad esempio, se i dati vengono raccolti da fonti non attendibili o ci sono problemi nella pulizia o nell’aggregazione dei dati, un aggressore può introdurre campioni avvelenati o manipolare i dati a proprio vantaggio.\nManipolazione dei dati alla fonte (ad esempio, dati dei sensori): In alcuni casi, gli aggressori possono manipolare i dati alla fonte, come dati dei sensori o dispositivi di input. Manomettendo i sensori o manipolando l’ambiente in cui vengono raccolti i dati, gli aggressori possono introdurre campioni avvelenati o alterare la distribuzione dei dati. Ad esempio, in uno scenario di auto a guida autonoma, un aggressore potrebbe manipolare i sensori o l’ambiente per immettere informazioni fuorvianti nei dati di addestramento, compromettendo la capacità del modello di prendere decisioni sicure e affidabili.\n\n\n\n\n\n\nFigura 17.24: Attacco di Avvelenamento dei Dati. Fonte: Sikandar\n\n\n\nAvvelenamento dei dati in scenari di apprendimento online: Gli attacchi di avvelenamento dei dati possono anche colpire sistemi ML che impiegano l’apprendimento online, in cui il modello viene costantemente aggiornato con nuovi dati in tempo reale. In tali scenari, un aggressore può gradualmente iniettare campioni avvelenati nel tempo, manipolando lentamente il comportamento del modello. I sistemi di apprendimento online sono particolarmente vulnerabili all’avvelenamento dei dati perché si adattano ai nuovi dati senza una convalida estesa, rendendo più facile per gli aggressori introdurre campioni dannosi, come mostrato in Figura 17.24.\nCollaborazione con addetti ai lavori per manipolare i dati: A volte, gli attacchi di avvelenamento dei dati possono comportare la collaborazione con addetti ai lavori con accesso ai dati di training. Gli addetti ai lavori malintenzionati, come dipendenti o provider di dati, possono manipolare i dati prima che vengano utilizzati per addestrare il modello. Le minacce interne sono particolarmente difficili da rilevare e prevenire, poiché gli aggressori hanno un accesso legittimo ai dati e possono elaborare attentamente la strategia di avvelenamento per eludere il rilevamento.\nQuesti sono i meccanismi chiave dell’avvelenamento dei dati nei sistemi ML. Gli aggressori spesso impiegano questi meccanismi per rendere i loro attacchi più efficaci e difficili da rilevare. Il rischio di attacchi di avvelenamento dei dati aumenta man mano che i sistemi ML diventano sempre più complessi e si basano su set di dati più grandi provenienti da fonti diverse. La difesa dall’avvelenamento dei dati richiede un approccio poliedrico. I professionisti ML e i progettisti di sistemi devono essere consapevoli dei vari meccanismi di avvelenamento dei dati e adottare un approccio completo alla sicurezza dei dati e alla resilienza del modello. Ciò include la raccolta dati sicura, la convalida dati robusta e il monitoraggio continuo delle prestazioni del modello. L’implementazione di pratiche di raccolta dati e pre-elaborazione sicure è fondamentale per prevenire l’avvelenamento dei dati alla fonte. Le tecniche di convalida dati e rilevamento anomalie possono anche aiutare a identificare e mitigare potenziali tentativi di avvelenamento. Il monitoraggio delle prestazioni del modello per segnali di avvelenamento dei dati è inoltre essenziale per rilevare e rispondere prontamente agli attacchi.\n\n\nImpatto sui Sistemi ML\nGli attacchi di avvelenamento dei dati possono avere gravi ripercussioni sui sistemi ML, compromettendone le prestazioni, l’affidabilità e la credibilità. L’impatto dell’avvelenamento dei dati può manifestarsi in vari modi, a seconda degli obiettivi dell’aggressore e del meccanismo specifico utilizzato. Analizziamo in dettaglio ciascuno dei potenziali impatti.\nDegrado delle prestazioni del modello: Uno degli impatti principali dell’avvelenamento dei dati è il degrado delle prestazioni complessive del modello. Manipolando i dati di training, gli aggressori possono introdurre rumore, distorsioni o incongruenze che ostacolano la capacità del modello di apprendere modelli accurati e fare previsioni affidabili. Ciò può ridurre accuratezza, precisione, richiamo o altre metriche delle prestazioni. Il degrado delle prestazioni del modello può avere conseguenze significative, soprattutto in applicazioni critiche come sanità, finanza o sicurezza, dove l’affidabilità delle previsioni è fondamentale.\nErrore di classificazione di target specifici: Gli attacchi di avvelenamento dei dati possono anche essere progettati per far sì che il modello classifichi in modo errato istanze target specifiche. Gli aggressori possono introdurre campioni avvelenati realizzati con cura simili alle istanze target, portando il modello ad apprendere associazioni errate. Ciò può comportare che il modello classifichi in modo errato le istanze target in modo coerente, anche se funziona bene su altri input. Tale errata classificazione mirata può avere gravi conseguenze, come far sì che un sistema di rilevamento malware trascuri file dannosi specifici o portare a una diagnosi errata in un’applicazione di imaging medico.\nBackdoor e trojan nei modelli addestrati: L’avvelenamento dei dati può introdurre backdoor o trojan nel modello addestrato. Le backdoor sono funzionalità nascoste che consentono agli aggressori di innescare comportamenti specifici o bypassare i normali meccanismi di autenticazione. D’altro canto, i trojan sono componenti dannosi insinuati nel modello che possono attivare specifici modelli di input. Avvelenando i dati di training, gli aggressori possono creare modelli che sembrano funzionare normalmente ma contengono vulnerabilità nascoste che possono essere sfruttate in seguito. Backdoor e trojan possono compromettere l’integrità e la sicurezza del sistema ML, consentendo agli aggressori di ottenere accesso non autorizzato, manipolare previsioni o esfiltrare informazioni sensibili.\nRisultati del modello distorti o ingiusti: Gli attacchi di avvelenamento dei dati possono introdurre distorsioni o ingiustizie nelle previsioni del modello. Manipolando la distribuzione dei dati di training o iniettando campioni con distorsioni specifiche, gli aggressori possono far sì che il modello apprenda e perpetui modelli discriminatori. Ciò può portare a un trattamento ingiusto di determinati gruppi o individui in base ad attributi sensibili come razza, genere o età. I modelli distorti possono avere gravi implicazioni sociali, rafforzando le disuguaglianze e le pratiche discriminatorie esistenti. Garantire l’equità e mitigare i pregiudizi è fondamentale per creare sistemi ML affidabili ed etici..\nAumento di falsi positivi o falsi negativi: L’avvelenamento dei dati può anche influire sulla capacità del modello di identificare correttamente istanze positive o negative, portando a un aumento di falsi positivi o falsi negativi. I falsi positivi si verificano quando il modello identifica erroneamente un’istanza negativa come positiva, mentre i falsi negativi si verificano quando un’istanza positiva viene classificata erroneamente come negativa. Le conseguenze dell’aumento di falsi positivi o falsi negativi possono essere significative a seconda dell’applicazione. Ad esempio, in un sistema di rilevamento delle frodi, un elevato numero di falsi positivi può portare a indagini non necessarie e frustrazione dei clienti, mentre un elevato numero di falsi negativi può consentire che le attività fraudolente passino inosservate.\nAffidabilità e fiducia del sistema compromesse: Gli attacchi di avvelenamento dei dati possono minare l’affidabilità e la fiducia complessiva dei sistemi ML. Quando i modelli vengono addestrati su dati avvelenati, le loro previsioni diventano affidabili e degne di fiducia. Ciò può erodere la fiducia dell’utente nel sistema e portare a una perdita di fiducia nelle decisioni prese dal modello. Nelle applicazioni critiche in cui si fa affidamento sui sistemi ML per il processo decisionale, come veicoli autonomi o diagnosi mediche, l’affidabilità compromessa può avere gravi conseguenze, mettendo a rischio vite e proprietà.\nPer affrontare l’impatto dell’avvelenamento dei dati è necessario un approccio proattivo alla sicurezza dei dati, ai test dei modelli e al monitoraggio. Le organizzazioni devono implementare misure robuste per garantire l’integrità e la qualità dei dati di training, impiegare tecniche per rilevare e mitigare i tentativi di avvelenamento e monitorare costantemente le prestazioni e il comportamento dei modelli distribuiti. La collaborazione tra professionisti ML, esperti di sicurezza e specialisti di dominio è essenziale per sviluppare strategie complete per prevenire e rispondere agli attacchi di avvelenamento dei dati.\n\nCaso di Studio 1\nNel 2017, i ricercatori hanno dimostrato un attacco di avvelenamento dei data contro un modello di classificazione della tossicità popolare chiamato Perspective (Hosseini et al. 2017). Questo modello ML rileva commenti tossici online.\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, e Radha Poovendran. 2017. «Deceiving google’s perspective api built for detecting toxic comments». ArXiv preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\nI ricercatori hanno aggiunto commenti tossici generati sinteticamente con lievi errori di ortografia e grammaticali ai dati di training del modello. Ciò ha lentamente corrotto il modello, facendogli classificare erroneamente un numero crescente di input gravemente tossici come non tossici nel tempo.\nDopo il ri-addestramento sui dati avvelenati, il tasso di falsi negativi del modello è aumentato dall’1,4% al 27%, consentendo ai commenti estremamente tossici di aggirare il rilevamento. I ricercatori hanno avvertito che questo furtivo “data poisoning” potrebbe consentire la diffusione di discorsi di odio, molestie e abusi se implementato contro sistemi di moderazione reali.\nQuesto caso evidenzia come l’avvelenamento dei dati possa degradare l’accuratezza e l’affidabilità del modello. Per le piattaforme di social media, un attacco di avvelenamento che compromette il rilevamento della tossicità potrebbe portare alla proliferazione di contenuti dannosi e alla sfiducia nei sistemi di moderazione ML. L’esempio dimostra perché proteggere l’integrità dei dati di training e monitorare l’avvelenamento è fondamentale in tutti i domini applicativi.\n\n\nCaso di Studio 2\n\n\n\n\n\n\nFigura 17.25: Campioni di dati avvelenati con etichette sbagliateriguardanti coppie testo/immagine non corrispondenti. Fonte: Shan\n\n\n\nÈ interessante notare che gli attacchi di “data poisoning” non sono sempre dannosi (Shan et al. 2023). Nightshade, uno strumento sviluppato da un team guidato dal professor Ben Zhao presso l’Università di Chicago, utilizza l’avvelenamento dei dati per aiutare gli artisti a proteggere la propria arte da scraping e violazioni del copyright da parte di modelli di IA generativa. Gli artisti possono utilizzare lo strumento per apportare modifiche impercettibili alle proprie immagini prima di caricarle online, come mostrato in Figura 17.25.\nSebbene queste modifiche siano impercettibili all’occhio umano, possono compromettere significativamente le prestazioni dei modelli di intelligenza artificiale generativa quando vengono incorporate nei dati di addestramento. I modelli generativi possono essere manipolati per generare allucinazioni e immagini strane. Ad esempio, con solo 300 immagini avvelenate, i ricercatori dell’Università di Chicago potrebbero ingannare l’ultimo modello Stable Diffusion per generare immagini di cani che sembrano gatti o immagini di mucche quando vengono richieste le auto.\nMan mano che aumenta il numero di immagini avvelenate su Internet, le prestazioni dei modelli che utilizzano dati acquisiti peggioreranno in modo esponenziale. In primo luogo, i dati avvelenati sono difficili da rilevare e richiedono l’eliminazione manuale. In secondo luogo, il “veleno” si diffonde rapidamente ad altre etichette perché i modelli generativi si basano su connessioni tra parole e concetti mentre generano immagini. Quindi un’immagine avvelenata di una “macchina” potrebbe diffondersi in immagini generate associate a parole come “camion”, “treno”, “autobus”, ecc.\nD’altra parte, questo strumento può essere utilizzato in modo dannoso e può influenzare le applicazioni legittime dei modelli generativi. Ciò dimostra la natura molto impegnativa e innovativa degli attacchi di apprendimento automatico.\nFigura 17.26 mostra gli effetti di diversi livelli di avvelenamento dei dati (50 campioni, 100 campioni e 300 campioni di immagini avvelenate) sulla generazione di immagini in diverse categorie. Notare come le immagini iniziano a deformarsi e deviare dalla categoria desiderata. Ad esempio, dopo 300 campioni di avvelenamento, una richiesta di un’auto genera una mucca.\n\n\n\n\n\n\nFigura 17.26: Avvelenamento dei Dati. Fonte: Shan et al. (2023))\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, e Ben Y Zhao. 2023. «Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models». ArXiv preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\n\n\n\n\n\n\nEsercizio 17.3: Attacchi Avvelenati\n\n\n\n\n\nPreparatevi a esplorare il lato oscuro della sicurezza dell’IA! In questo Colab, impareremo cos’è l’avvelenamento dei dati, ovvero come dati errati possono ingannare i modelli di IA e fargli prendere decisioni sbagliate. Ci concentreremo su un attacco reale contro una Support Vector Machine (SVM), osservando come cambia il comportamento dell’IA sotto attacco. Questo esercizio pratico metterà in evidenza perché proteggere i sistemi di IA è fondamentale, soprattutto man mano che diventano più integrati nelle nostre vite. Pensare come un hacker, comprendere la vulnerabilità e fare brainstorming su come difendere i sistemi di IA!\n\n\n\n\n\n\n\n\n17.4.3 Distribution Shift\n\nDefinizione e Caratteristiche\nLa “distribution shift” [slittamento della distribuzione] si riferisce al fenomeno in cui la distribuzione dei dati incontrata da un modello ML durante la distribuzione (inferenza) differisce dalla distribuzione su cui è stato addestrato, come mostrato in Figura 17.27. Questo non è tanto un attacco quanto il fatto che la robustezza del modello varierà nel tempo. In altre parole, le proprietà statistiche, i modelli o le ipotesi sottostanti dei dati possono cambiare tra le fasi di addestramento e di test.\n\n\n\n\n\n\nFigura 17.27: Le parentesi graffe racchiudono la “distribution shift” tra gli ambienti. Qui, z sta per la caratteristica spuria e y sta per la classe dell’etichetta. Fonte: Xin\n\n\n\nLe caratteristiche principali della “distribution shift” includono:\nDiscordanza di dominio: I dati di input durante l’inferenza provengono da un dominio o una distribuzione diversi rispetto ai dati di addestramento. Quando i dati di input durante l’inferenza provengono da un dominio o una distribuzione diversi dai dati di training, possono influenzare significativamente le prestazioni del modello. Questo perché il modello ha appreso modelli e relazioni specifici del dominio di training e, se applicati a un dominio diverso, tali pattern appresi potrebbero non essere validi. Ad esempio, si consideri un modello di analisi del sentiment addestrato sulle recensioni di film. Supponiamo che questo modello venga applicato per analizzare il sentiment nei tweet. In tal caso, potrebbe aver bisogno di aiuto per classificare accuratamente il sentiment perché la lingua, la grammatica e il contesto dei tweet possono differire dalle recensioni dei film. Questa discrepanza di dominio può causare scarse prestazioni e previsioni inaffidabili, limitando l’utilità pratica del modello.\nDeriva temporale: La distribuzione dei dati si evolve, portando a uno spostamento graduale o improvviso nelle caratteristiche di input. La deriva temporale è importante perché i modelli ML vengono spesso distribuiti in ambienti dinamici in cui la distribuzione dei dati può cambiare nel tempo. Se il modello non viene aggiornato o adattato a questi cambiamenti, le sue prestazioni possono gradualmente peggiorare. Ad esempio, i pattern e i comportamenti associati alle attività fraudolente possono evolversi in un sistema di rilevamento delle frodi man mano che i truffatori adattano le loro tecniche. Se il modello non viene riqualificato o aggiornato per catturare questi nuovi pattern, potrebbe non riuscire a rilevare efficacemente nuovi tipi di frode. La deriva temporale può portare a un calo dell’accuratezza e dell’affidabilità del modello nel tempo, rendendo cruciale il monitoraggio e l’affronto di questo tipo di spostamento della distribuzione.\nCambiamenti contestuali: Il contesto del modello ML può variare, determinando diverse distribuzioni di dati in base a fattori quali posizione, comportamento dell’utente o condizioni ambientali. I cambiamenti contestuali sono importanti perché i modelli ML vengono spesso distribuiti in vari contesti o ambienti che possono avere diverse distribuzioni di dati. Se il modello non riesce a generalizzare bene a questi diversi contesti, le sue prestazioni potrebbero migliorare. Ad esempio, si consideri un modello di visione artificiale addestrato per riconoscere oggetti in un ambiente di laboratorio controllato. Quando distribuito in un contesto reale, fattori quali condizioni di illuminazione, angoli della telecamera o confusione sullo sfondo possono variare in modo significativo, determinando una “distribution shift”. Se il modello è robusto a questi cambiamenti contestuali, potrebbe essere in grado di riconoscere accuratamente gli oggetti nel nuovo ambiente, limitandone l’utilità pratica.\nDati di addestramento non rappresentativi: I dati di addestramento potrebbero catturare solo parzialmente la variabilità e la diversità dei dati del mondo reale riscontrati durante la distribuzione. I dati di training non rappresentativi possono portare a modelli parziali o distorti che funzionano male sui dati del mondo reale. Supponiamo che i dati di training debbano catturare adeguatamente la variabilità e la diversità dei dati del mondo reale. In tal caso, il modello potrebbe apprendere pattern specifici del set di training, ma deve essere meglio generalizzato a dati nuovi e mai visti. Ciò può comportare scarse prestazioni, previsioni parziali e limitata applicabilità del modello. Ad esempio, se un modello di riconoscimento facciale viene addestrato principalmente su immagini di individui di uno specifico gruppo demografico, potrebbe avere difficoltà a riconoscere accuratamente i volti di altri gruppi demografici quando viene distribuito in un contesto reale. Garantire che i dati di training siano rappresentativi e diversificati è fondamentale per creare modelli che possano essere generalizzati bene a scenari del mondo reale.\n\n\n\n\n\n\nFigura 17.28: La deriva concettuale si riferisce a un cambiamento nei pattern e nelle relazioni dei dati nel tempo. Fonte: Evidently AI\n\n\n\nLa “distribution shift” può manifestarsi in varie forme, come:\nCovariate shift: La distribuzione delle feature di input (covariate) cambia mentre la distribuzione condizionale della variabile target dato l’input rimane la stessa. La “covariate shift” è importante perché può influire sulla capacità del modello di fare previsioni accurate quando le feature di input (covariate) differiscono tra i dati di training e quelli di test. Anche se la relazione tra le feature di input e la variabile target rimane la stessa, un cambiamento nella distribuzione delle feature di input può influire sulle prestazioni del modello. Ad esempio, si consideri un modello addestrato per prevedere i prezzi delle case in base a caratteristiche come la metratura, il numero di camere da letto e la posizione. Supponiamo che la distribuzione di queste caratteristiche nei dati di test differisca significativamente dai dati di training (ad esempio, i dati di test contengono case con una metratura molto più ampia). In tal caso, le previsioni del modello potrebbero diventare meno accurate. È importante tenere conto dei “covariate shift” per garantire la robustezza e l’affidabilità del modello quando viene applicato a nuovi dati.\nConcept drift: La relazione tra le feature di input e la variabile target cambia nel tempo, alterando il concetto sottostante che il modello sta cercando di apprendere, come mostrato in Figura 17.28. Il “concept drift” è importante perché indica cambiamenti nella relazione fondamentale tra le feature di input e la variabile target nel tempo. Quando il concetto sottostante che il modello sta cercando di apprendere cambia, le sue prestazioni possono deteriorarsi se non vengono adattate al nuovo concetto. Ad esempio, in un modello di previsione dell’abbandono dei clienti, i fattori che influenzano l’abbandono dei clienti possono evolversi a causa delle condizioni di mercato, delle offerte della concorrenza o delle preferenze dei clienti. Se il modello non viene aggiornato per catturare questi cambiamenti, le sue previsioni potrebbero diventare meno accurate e irrilevanti. Rilevare e adattarsi al “concept drift” è fondamentale per mantenere l’efficacia del modello e l’allineamento con i concetti del mondo reale in evoluzione.\nGeneralizzazione del dominio: Il modello deve generalizzare a domini o distribuzioni invisibili non presenti durante l’addestramento. La generalizzazione di dominio è importante perché consente di applicare i modelli ML a nuovi domini mai visti senza richiedere un’ampia riqualificazione o adattamento. Negli scenari del mondo reale, i dati di addestramento che coprono tutti i possibili domini o distribuzioni che il modello può incontrare sono spesso irrealizzabili. Le tecniche di generalizzazione di dominio mirano ad apprendere caratteristiche o modelli invarianti al dominio che possono essere generalizzati bene a nuovi domini. Ad esempio, si consideri un modello addestrato per classificare immagini di animali. Se il modello può apprendere caratteristiche invarianti a diversi sfondi, condizioni di illuminazione o pose, può essere generalizzato bene per classificare animali in nuovi ambienti mai visti. La generalizzazione del dominio è fondamentale per creare modelli che possono essere distribuiti in contesti reali diversi e in continua evoluzione.\nLa presenza di un “distribution shift” può avere un impatto significativo sulle prestazioni e l’affidabilità dei modelli ML, poiché i modelli potrebbero aver bisogno di aiuto per generalizzare bene alla nuova distribuzione dei dati. Rilevare e adattarsi ai “distribution shift” è fondamentale per garantire la robustezza e l’utilità pratica dei sistemi ML negli scenari del mondo reale.\n\n\nMeccanismi delle Distribution Shift\nI meccanismi della “distribution shift”, come cambiamenti nelle fonti dei dati, evoluzione temporale, variazioni specifiche del dominio, bias di selezione, cicli di feedback e manipolazioni avversarie, sono importanti da comprendere perché aiutano a identificarne le cause. Comprendendo questi meccanismi, i professionisti possono sviluppare strategie mirate per mitigarne l’impatto e migliorare la robustezza del modello. Ecco alcuni meccanismi comuni:\n\n\n\n\n\n\nFigura 17.29: Evoluzione temporale. Fonte: Białek\n\n\n\nCambiamenti nelle fonti di dati: Possono verificarsi cambiamenti di distribuzione quando le fonti di dati utilizzate per l’addestramento e l’inferenza sono diverse. Ad esempio, se un modello viene addestrato sui dati di un sensore ma distribuito sui dati di un altro sensore con caratteristiche diverse, può portare a un “distribution shift”.\nEvoluzione temporale: Nel tempo, la distribuzione dei dati sottostante può evolversi a causa di cambiamenti nel comportamento dell’utente, dinamiche di mercato o altri fattori temporali. Ad esempio, in un sistema di raccomandazione, le preferenze dell’utente possono cambiare nel tempo, portando a un “distribution shift” nei dati di input, come mostrato in Figura 17.29.\nVariazioni specifiche del dominio: Domini o contesti diversi possono avere distribuzioni di dati distinte. Un modello addestrato sui dati di un dominio può generalizzare bene con un altro dominio solo con tecniche di adattamento appropriate. Ad esempio, un modello di classificazione delle immagini addestrato su scene di interni potrebbe avere difficoltà se applicato a scene in esterno.\nBias di selezione: Un “Distribution shift” può derivare da un bias di selezione durante la raccolta o il campionamento dei dati. Se i dati di training non rappresentano la popolazione reale o determinati sottogruppi sono sovrarappresentati o sottorappresentati, si può arrivare a una mancata corrispondenza tra le distribuzioni di training e di test.\nCicli di feedback: In alcuni casi, le previsioni o le azioni intraprese da un modello ML possono influenzare la futura distribuzione dei dati. Ad esempio, in un sistema di prezzi dinamici, i prezzi stabiliti dal modello possono influire sul comportamento dei clienti, determinando uno spostamento nella distribuzione dei dati nel tempo.\nManipolazioni avversarie: Gli avversari possono manipolare intenzionalmente i dati di input per creare uno spostamento della distribuzione e ingannare il modello ML. Introducendo perturbazioni attentamente studiate o generando campioni fuori distribuzione, gli aggressori possono sfruttare le vulnerabilità del modello e fargli fare previsioni errate.\nComprendere i meccanismi del “distribution shift” è importante per sviluppare strategie efficaci per rilevare e mitigare il suo impatto sui sistemi ML. Identificando le fonti e le caratteristiche dello spostamento, i professionisti possono progettare tecniche appropriate, come l’adattamento del dominio, l’apprendimento tramite trasferimento o l’apprendimento continuo, per migliorare la robustezza e le prestazioni del modello in caso di cambiamenti distributivi.\n\n\nImpatto sui Sistemi ML\nI “distribution shift” possono avere un impatto negativo significativo sulle prestazioni e l’affidabilità dei sistemi ML. Ecco alcuni modi chiave in cui il “distribution shift” può influenzare i modelli ML:\nPrestazioni predittive degradate: Quando la distribuzione dei dati riscontrata durante l’inferenza differisce dalla distribuzione di training, l’accuratezza predittiva del modello può deteriorarsi. Il modello potrebbe aver bisogno di aiuto per generalizzare bene i nuovi dati, il che porta a un aumento degli errori e a prestazioni non ottimali.\nAffidabilità e attendibilità ridotte: Il “distribution shift” può compromettere l’affidabilità e l’attendibilità dei modelli ML. Se le previsioni del modello diventano inaffidabili o incoerenti a causa dello spostamento, gli utenti potrebbero perdere fiducia negli output del sistema, il che porta a un potenziale uso improprio o non uso del modello.\nPredizioni distorte: Lo spostamento di distribuzione può introdurre “bias” [distorsioni] nelle previsioni del modello. Se i dati di training non rappresentano la distribuzione nel mondo reale o alcuni sottogruppi sono sottorappresentati, il modello potrebbe fare previsioni distorte che discriminano determinati gruppi o perpetuano pregiudizi sociali.\nMaggiore incertezza e rischio: Lo spostamento della distribuzione introduce ulteriore incertezza e rischio nel sistema ML. Il comportamento e le prestazioni del modello potrebbero diventare meno prevedibili, rendendo difficile valutarne l’affidabilità e l’idoneità per applicazioni critiche. Questa incertezza può portare a maggiori rischi operativi e potenziali guasti.\nSfide di adattabilità: I modelli ML addestrati su una distribuzione dati specifica potrebbero aver bisogno di aiuto per adattarsi ad ambienti mutevoli o nuovi domini. La mancanza di adattabilità può limitare l’utilità e l’applicabilità del modello in scenari reali dinamici in cui la distribuzione dei dati si evolve.\nDifficoltà di manutenzione e aggiornamento: Il “distribution shift” può complicare la manutenzione e l’aggiornamento dei modelli ML. Man mano che la distribuzione dei dati cambia, il modello potrebbe richiedere frequenti riqualificazioni o ottimizzazioni per mantenere le sue prestazioni. Ciò può richiedere molto tempo e risorse, soprattutto se il cambiamento avviene rapidamente o continuamente.\nVulnerabilità agli attacchi avversari: Il “distribution shift” può rendere i modelli ML più vulnerabili agli attacchi avversari. Gli avversari possono sfruttare la sensibilità del modello ai cambiamenti distributivi creando esempi avversari al di fuori della distribuzione di addestramento, facendo sì che il modello faccia previsioni errate o si comporti in modo inaspettato.\nPer mitigare l’impatto dei “distribution shift”, è fondamentale sviluppare sistemi ML robusti che rilevino e si adattino ai cambiamenti delle distribuzioni. Tecniche come l’adattamento del dominio, l’apprendimento tramite trasferimento e l’apprendimento continuo possono aiutare a migliorare la capacità di generalizzazione del modello su diverse distribuzioni. Il monitoraggio, il test e l’aggiornamento del modello ML sono inoltre necessari per garantirne le prestazioni e l’affidabilità durante i “distribution shift”.\n\n\n\n17.4.4 Rilevamento e Mitigazione\n\nAttacchi Avversari\nCome si ricorderà da quanto sopra, gli attacchi avversari rappresentano una minaccia significativa per la robustezza e l’affidabilità dei sistemi ML. Questi attacchi comportano la creazione di input attentamente progettati, noti come “esempi avversari”, per ingannare i modelli ML e fargli fare previsioni errate. Per proteggere i sistemi ML dagli attacchi avversari, è fondamentale sviluppare tecniche efficaci per rilevare e mitigare queste minacce.\n\nTecniche di Rilevamento degli Esempi Avversari\nIl rilevamento degli esempi avversari è la prima linea di difesa contro gli attacchi avversari. Sono state proposte diverse tecniche per identificare e segnalare input sospetti che potrebbero essere avversari.\nI metodi statistici mirano a rilevare gli esempi avversari analizzando le proprietà statistiche dei dati di input. Questi metodi spesso confrontano la distribuzione dei dati di input con una di riferimento, come quella dei dati di training o una nota distribuzione benigna. Tecniche come il test Kolmogorov-Smirnov (Berger e Zhou 2014) o il test Anderson-Darling possono essere utilizzate per misurare la discrepanza tra le distribuzioni e segnalare gli input che si discostano in modo significativo dalla distribuzione prevista.\n\nBerger, Vance W, e YanYan Zhou. 2014. «Kolmogorovsmirnov test: Overview». Wiley statsref: Statistics reference online.\nKernel density estimation (KDE) è una tecnica non parametrica utilizzata per stimare la funzione di densità di probabilità di un set di dati. Nel contesto del rilevamento di esempi avversari, KDE può essere utilizzato per stimare la densità di esempi benigni nello spazio di input. Gli esempi avversari spesso si trovano in regioni a bassa densità e possono essere rilevati confrontando la loro densità stimata con una soglia. Gli input con una densità stimata al di sotto della soglia vengono segnalati come potenziali esempi avversari.\nUn’altra tecnica è la compressione delle feature (Panda, Chakraborty, e Roy 2019), che riduce la complessità dello spazio di input applicando la riduzione della dimensionalità o la discretizzazione. L’idea alla base della compressione delle feature è che gli esempi avversari spesso si basano su piccole perturbazioni impercettibili che possono essere eliminate o ridotte tramite queste trasformazioni. Le incongruenze possono essere rilevate confrontando le previsioni del modello sull’input originale e sull’input compresso, indicando la presenza di esempi avversari.\n\nPanda, Priyadarshini, Indranil Chakraborty, e Kaushik Roy. 2019. «Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks». #IEEE_O_ACC# 7: 70157–68. https://doi.org/10.1109/access.2019.2919463.\nLe tecniche di stima dell’incertezza del modello mirano a quantificare la fiducia o l’incertezza associata alle previsioni di un modello. Gli esempi avversari spesso sfruttano regioni di elevata incertezza nel confine di decisione del modello. Stimando l’incertezza utilizzando tecniche come reti neurali bayesiane, stima dell’incertezza basata su dropout o metodi di ensemble, gli input con elevata incertezza possono essere contrassegnati come potenziali esempi avversari.\n\n\nStrategie di Difesa Avversarie\nUna volta rilevati gli esempi avversari, possono essere impiegate varie strategie di difesa per mitigarne l’impatto e migliorare la robustezza dei modelli ML.\nL’addestramento avversario è una tecnica che prevede l’aumento dei dati di addestramento con esempi avversari e il riaddestramento del modello su questo set di dati aumentato. Esporre il modello a esempi avversari durante l’addestramento gli insegna a classificarli correttamente e diventa più robusto agli attacchi avversari. L’addestramento avversario può essere eseguito utilizzando vari metodi di attacco, come il Fast Gradient Sign Method (FGSM) o il Projected Gradient Descent (PGD) (Madry et al. 2017).\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, e Adrian Vladu. 2017. «Towards deep learning models resistant to adversarial attacks». arXiv preprint arXiv:1706.06083.\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, e Ananthram Swami. 2016. «Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks». In 2016 IEEE Symposium on Security and Privacy (SP), 582–97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\nLa distillazione difensiva (Papernot et al. 2016) è una tecnica che addestra un secondo modello (il modello studente) per imitare il comportamento di quello originale (il modello insegnante). Il modello studente viene addestrato sulle etichette soft prodotte dal modello insegnante, che sono meno sensibili alle piccole perturbazioni. L’utilizzo del modello studente per l’inferenza può ridurre l’impatto delle perturbazioni avversarie, poiché il modello studente impara a generalizzare meglio ed è meno sensibile al rumore avversario.\nLe tecniche di pre-elaborazione e trasformazione dell’input mirano a rimuovere o mitigare l’effetto delle perturbazioni avversarie prima di alimentare l’input nel modello ML. Queste tecniche includono la rimozione del rumore dalle immagini, la compressione JPEG, il ridimensionamento casuale, il padding o l’applicazione di trasformazioni casuali ai dati di input. Riducendo l’impatto delle perturbazioni avversarie, questi passaggi di pre-elaborazione possono aiutare a migliorare la robustezza del modello agli attacchi avversari.\nI metodi ensemble combinano più modelli per fare previsioni più robuste. L’ensemble può ridurre l’impatto degli attacchi avversari utilizzando un set diversificato di modelli con diverse architetture, dati di training o iperparametri. Esempi avversari che ingannano un modello potrebbero non ingannare gli altri nell’insieme, portando a previsioni più affidabili e robuste. Le tecniche di diversificazione del modello, come l’utilizzo di diverse tecniche di pre-elaborazione o rappresentazioni delle caratteristiche per ogni modello nell’insieme, possono migliorare ulteriormente la robustezza.\n\n\nValutazione e Test della Robustezza\nCondurre valutazioni e test approfonditi per valutare l’efficacia delle tecniche di difesa avversarie e misurare la robustezza dei modelli ML.\nLe metriche di robustezza avversaria quantificano la resilienza del modello agli attacchi avversari. Queste metriche possono includere l’accuratezza del modello sugli esempi avversari, la distorsione media richiesta per ingannare il modello o le prestazioni del modello in base a diversi livelli di attacco. Confrontando queste metriche tra diversi modelli o tecniche di difesa, i professionisti possono valutare e confrontare i loro livelli di robustezza.\nI benchmark e i set di dati standardizzati per gli attacchi avversari forniscono una base comune per valutare e confrontare la robustezza dei modelli ML. Questi benchmark includono set di dati con esempi avversari pre-generati e strumenti e framework per generare attacchi avversari. Esempi di benchmark di attacchi avversari popolari includono i set di dati MNIST-C, CIFAR-10-C e ImageNet-C (Hendrycks e Dietterich 2019), che contengono versioni corrotte o perturbate dei set di dati originali.\n\nHendrycks, Dan, e Thomas Dietterich. 2019. «Benchmarking neural network robustness to common corruptions and perturbations». arXiv preprint arXiv:1903.12261.\nI professionisti possono sviluppare sistemi ML più robusti e resilienti sfruttando queste tecniche di rilevamento di esempi avversari, strategie di difesa e metodi di valutazione della robustezza. Tuttavia, è importante notare che la robustezza avversaria è un’area di ricerca in corso e nessuna tecnica singola fornisce una protezione completa contro tutti i tipi di attacchi avversari. Un approccio completo che combina più meccanismi di difesa e test regolari è essenziale per mantenere la sicurezza e l’affidabilità dei sistemi ML di fronte alle minacce avversarie in evoluzione.\n\n\n\nAvvelenamento dei Dati\nSi ricorda che il data poisoning è un attacco che prende di mira l’integrità dei dati di training utilizzati per creare modelli ML. Manipolando o corrompendo i dati di training, gli aggressori possono influenzare il comportamento del modello e fargli fare previsioni errate o eseguire azioni indesiderate. Rilevare e mitigare gli attacchi di data poisoning è fondamentale per garantire l’affidabilità e la sicurezza dei sistemi ML, come mostrato in Figura 17.30.\n\nTecniche di rilevamento delle anomalie per identificare i Dati Avvelenati\n\n\n\n\n\n\nFigura 17.30: Iniezione di dati dannosi. Fonte: Li\n\n\n\nI metodi di rilevamento statistico degli outlier identificano i dati che si discostano in modo significativo dalla maggior parte. Questi metodi presuppongono che le istanze di dati avvelenati siano probabilmente “outlier” statistici”. Tecniche come il Metodo z-score, il Metodo di Tukey o la “[Mahalanobis] distance” possono essere utilizzate per misurare la deviazione di ogni punto dati dalla tendenza centrale del set di dati. I dati che superano una soglia predefinita vengono contrassegnati come potenziali valori anomali e considerati sospetti di avvelenamento dei dati.\nI metodi basati sul clustering raggruppano dati simili in base alle loro caratteristiche o attributi. Il presupposto è che le istanze di dati avvelenate possano formare cluster distinti o trovarsi lontano dai normali cluster di dati. Applicando algoritmi di clustering come K-means, DBSCAN o clustering gerarchico, è possibile identificare cluster anomali o dati che non appartengono a nessun cluster. Queste istanze anomale vengono poi trattate come dati potenzialmente avvelenati.\n\n\n\n\n\n\nFigura 17.31: Autoencoder. Fonte: Dertat\n\n\n\nGli autoencoder sono reti neurali addestrate per ricostruire i dati di input da una rappresentazione compressa, come mostrato in Figura 17.31. Possono essere utilizzati per il rilevamento di anomalie apprendendo i modelli normali nei dati e identificando le istanze che si discostano da essi. Durante l’addestramento, l’autoencoder viene addestrato su dati puliti e non avvelenati. Al momento dell’inferenza, viene calcolato l’errore di ricostruzione per ogni dato. I dati con errori di ricostruzione elevati sono considerati anomali e potenzialmente avvelenati, poiché non sono conformi ai pattern normali appresi.\n\n\nTecniche di Sanificazione e Preelaborazione dei Dati\nL’avvelenamento dei dati può essere evitato pulendo i dati, il che implica l’identificazione e la rimozione o la correzione di dati rumorosi, incompleti o incoerenti. Tecniche come la deduplicazione dei dati, l’imputazione dei valori mancanti e la rimozione dei valori anomali possono essere applicate per migliorare la qualità dei dati di addestramento. Eliminando o filtrando i dati sospetti o anomali, è possibile ridurre l’impatto delle istanze avvelenate.\nLa validazione dei dati implica la verifica dell’integrità e della coerenza dei dati di training. Ciò può includere il controllo della coerenza del tipo di dati, la convalida dell’intervallo e le dipendenze tra campi. Definendo e applicando le regole di validazione dei dati, i dati anomali o incoerenti indicativi di avvelenamento possono essere identificati e segnalati per ulteriori indagini.\nLa provenienza dei dati e il tracciamento della discendenza implicano il mantenimento di un registro dell’origine, delle trasformazioni e dei movimenti dei dati in tutta la pipeline ML. Documentando le fonti dei dati, i passaggi di pre-elaborazione e qualsiasi modifica apportata, i professionisti possono risalire alle anomalie o ai modelli sospetti fino alla loro origine. Ciò aiuta a identificare potenziali punti di avvelenamento dei dati e facilita il processo di indagine e mitigazione.\n\n\nTecniche di Training Robusti\nÈ possibile utilizzare tecniche di ottimizzazione robuste per modificare l’obiettivo del training per ridurre al minimo l’impatto di valori anomali o istanze avvelenate. Ciò può essere ottenuto utilizzando funzioni di perdita robuste meno sensibili ai valori estremi, come la “Huber loss” o la “modified Huber loss”. Le tecniche di regolarizzazione, come la regolarizzazione L1 o L2, possono anche aiutare a ridurre la sensibilità del modello ai dati avvelenati, limitando la complessità del modello e prevenendo l’overfitting.\nLe funzioni di “loss” [perdita] robuste sono progettate per essere meno sensibili ai valori anomali o ai dati rumorosi. Esempi includono la Huber loss modificata, la perdita di Tukey (Beaton e Tukey 1974) e la “trimmed mean loss”. Queste funzioni di perdita riducono o ignorano il contributo delle istanze anomale durante il training, riducendo il loro impatto sul processo di apprendimento del modello. Le funzioni “obiettivo” robuste, come l’obiettivo minimax o la “distributivamente robusto”, mirano a ottimizzare le prestazioni del modello negli scenari peggiori o in presenza di perturbazioni avversarie.\n\nBeaton, Albert E., e John W. Tukey. 1974. «The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data». Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\nLe tecniche di “data augmentation” comportano la generazione di esempi di addestramento aggiuntivi applicando trasformazioni o perturbazioni casuali ai dati esistenti Figura 17.32. Ciò aiuta ad aumentare la diversità e la robustezza del set di dati di addestramento. Introducendo variazioni controllate nei dati, il modello diventa meno sensibile a modelli o artefatti specifici che possono essere presenti in istanze avvelenate. Le tecniche di randomizzazione, come il sottocampionamento casuale o l’aggregazione bootstrap, possono anche aiutare a ridurre l’impatto dei dati avvelenati addestrando più modelli su diversi sottoinsiemi di dati e combinando le loro previsioni.\n\n\n\n\n\n\nFigura 17.32: Un’immagine del numero “3” nella forma originale e con applicati degli aumenti di base.\n\n\n\n\n\nApprovvigionamento di Dati Sicuro e Affidabile\nL’implementazione delle migliori pratiche di raccolta e cura dei dati può aiutare a mitigare il rischio di avvelenamento dei dati. Ciò include l’istituzione di protocolli di raccolta dati chiari, la verifica dell’autenticità e dell’affidabilità delle fonti dati e la conduzione di valutazioni regolari della qualità dei dati. L’approvvigionamento di dati da provider affidabili e rispettabili e il rispetto di pratiche di gestione dei dati sicure possono ridurre la probabilità di introdurre dati avvelenati nella pipeline di training.\nSolidi meccanismi di governance dei dati e controllo degli accessi sono essenziali per prevenire modifiche non autorizzate o manomissioni dei dati di training. Ciò implica la definizione di ruoli e responsabilità chiari per l’accesso ai dati, l’implementazione di policy di controllo degli accessi basate sul principio del privilegio minimo e il monitoraggio e il logging delle attività di accesso ai dati. Limitando l’accesso ai dati di training e mantenendo un audit trail, è possibile rilevare e investigare potenziali tentativi di avvelenamento dei dati.\nRilevare e mitigare gli attacchi di avvelenamento dei dati richiede un approccio poliedrico che combini rilevamento delle anomalie, sanificazione dei dati, tecniche di training affidabili e pratiche di approvvigionamento dei dati sicure. Implementando queste misure, i professionisti del ML possono migliorare la resilienza dei loro modelli contro l’avvelenamento dei dati e garantire l’integrità e l’affidabilità dei dati di training. Tuttavia, è importante notare che l’avvelenamento dei dati è un’area di ricerca attiva e continuano a emergere nuovi vettori di attacco e meccanismi di difesa. Rimanere informati sugli ultimi sviluppi e adottare un approccio proattivo e adattivo alla sicurezza dei dati è fondamentale per mantenere la robustezza dei sistemi ML.\n\n\n\nDistribution Shift\n\nRilevamento e Mitigazione dei “Distribution Shift”\nRicordiamo che i “distribution shift” [spostamenti di distribuzione] si verificano quando la distribuzione dei dati incontrata da un modello di machine learning (ML) durante l’implementazione differisce dalla distribuzione su cui è stato addestrato. Questi spostamenti possono avere un impatto significativo sulle prestazioni e sulla capacità di generalizzazione del modello, portando a previsioni non ottimali o errate. Rilevare e mitigare i “distribution shift” è fondamentale per garantire la robustezza e l’affidabilità dei sistemi ML in scenari reali.\n\n\nTecniche di Rilevamento per i “Distribution Shift”\nI test statistici possono essere utilizzati per confrontare le distribuzioni dei dati di training e di test per identificare differenze significative. Tecniche come il test di Kolmogorov-Smirnov o il test di Anderson-Darling misurano la discrepanza tra due distribuzioni e forniscono una valutazione quantitativa della presenza di un “distribution shift”. Applicando questi test alle funzionalità di input o alle previsioni del modello, i professionisti possono rilevare se esiste una differenza statisticamente significativa tra le distribuzioni di training e di test.\nLe metriche di divergenza quantificano la dissimilarità tra due distribuzioni di probabilità. Le metriche di divergenza comunemente utilizzate includono la divergenza Kullback-Leibler (KL) e quella di [Jensen-Shannon (JS)] (cfr.). Calcolando la divergenza tra le distribuzioni dei dati di training e di test, i professionisti possono valutare l’entità dello “spostamento della distribuzione”. Valori di divergenza elevati indicano una differenza significativa tra le distribuzioni, suggerendo la presenza di uno spostamento della distribuzione.\nLe tecniche di quantificazione dell’incertezza, come le reti neurali bayesiane o i metodi di ensemble, possono stimare l’incertezza associata alle previsioni del modello. Quando un modello viene applicato a dati da una distribuzione diversa, le sue previsioni potrebbero avere un’incertezza maggiore. Monitorando i livelli di incertezza, i professionisti possono rilevare gli spostamenti della distribuzione. Se l’incertezza supera costantemente una soglia predeterminata per i campioni di test, ciò suggerisce che il modello sta operando al di fuori della sua distribuzione addestrata.\nInoltre, i classificatori di dominio sono addestrati a distinguere tra diversi domini o distribuzioni. I professionisti possono rilevare gli spostamenti di distribuzione addestrando un classificatore a distinguere tra i domini di addestramento e di test. Se il classificatore di dominio raggiunge un’elevata accuratezza nel distinguere tra i due domini, indica una differenza significativa nelle distribuzioni sottostanti. Le prestazioni del classificatore di dominio servono come misura dello spostamento di distribuzione.\n\n\nTecniche di Mitigazione per i “Distribution Shift”\n\n\n\n\n\n\nFigura 17.33: Trasferimento dell’apprendimento. Fonte: Bhavsar\n\n\n\nIl “transfer learning” [trasferimento dell’apprendimento.] sfrutta le conoscenze acquisite da un dominio per migliorare le prestazioni in un altro, come mostrato in Figura 17.33. Utilizzando modelli pre-addestrati o trasferendo le feature apprese da un dominio di origine a un dominio di destinazione, il transfer learning può aiutare a mitigare l’impatto dei “distribution shift”. Il modello pre-addestrato può essere messo a punto su una piccola quantità di dati etichettati dal dominio target, consentendogli di adattarsi alla nuova distribuzione. Il transfer learning è particolarmente efficace quando i domini di origine e di destinazione condividono caratteristiche simili o quando i dati etichettati nel dominio di destinazione sono scarsi.\nL’apprendimento continuo, noto anche come apprendimento permanente, consente ai modelli ML di apprendere continuamente da nuove distribuzioni di dati, mantenendo al contempo le conoscenze delle distribuzioni precedenti. Tecniche come la “elastic weight consolidation (EWC)” (Kirkpatrick et al. 2017) o la “gradient episodic memory (GEM)” (Lopez-Paz e Ranzato 2017) consentono ai modelli di adattarsi alle distribuzioni di dati in evoluzione nel tempo. Queste tecniche mirano a bilanciare la plasticità del modello (capacità di apprendere da nuovi dati) con la stabilità del modello (mantenendo le conoscenze apprese in precedenza). Aggiornando gradualmente il modello con nuovi dati e mitigando l’oblio catastrofico, l’apprendimento continuo aiuta i modelli a rimanere robusti ai “distribution shift”.\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017. «Overcoming catastrophic forgetting in neural networks». Proc. Natl. Acad. Sci. 114 (13): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n\nLopez-Paz, David, e Marc’Aurelio Ranzato. 2017. «Gradient episodic memory for continual learning». Adv Neural Inf Process Syst 30.\nLe tecniche di aumento dei dati, come quelle viste in precedenza, comportano l’applicazione di trasformazioni o perturbazioni ai dati di training esistenti per aumentarne la diversità e migliorare la robustezza del modello ai “distribution shift”. Introducendo variazioni nei dati, come rotazioni, traslazioni, ridimensionamenti o aggiunta di rumore, l’aumento dei dati aiuta il modello ad apprendere caratteristiche invarianti e a generalizzare meglio a distribuzioni mai viste. Il “data augmentation” può essere eseguito durante l’addestramento e l’inferenza per migliorare la capacità del modello di gestire i “distribution shift”.\nI metodi ensemble combinano più modelli per rendere le previsioni più robuste ai “distribution shift”. Addestrando i modelli su diversi sottoinsiemi di dati, utilizzando algoritmi diversi o con diversi iperparametri, i metodi ensemble possono catturare diversi aspetti della distribuzione dei dati. Quando viene presentata una distribuzione “spostata”, l’ensemble può sfruttare i punti di forza dei singoli modelli per fare previsioni più accurate e stabili. Tecniche come il bagging, il boosting o lo stacking possono creare ensemble efficaci.\nAggiornare regolarmente i modelli con nuovi dati dalla distribuzione target è fondamentale per mitigare l’impatto dei “distribution shift”. Man mano che la distribuzione dei dati si evolve, i modelli dovrebbero essere riaddestrati o perfezionati sui dati disponibili più recenti per adattarsi ai pattern mutevoli. Il monitoraggio delle prestazioni del modello e delle caratteristiche dei dati può aiutare a rilevare quando è necessario un aggiornamento. Mantenendo aggiornati i modelli, i professionisti possono garantire che rimangano pertinenti e accurati di fronte ai distribution shift”.\nLa valutazione dei modelli utilizzando metriche robuste meno sensibili ai “distribution shift” può fornire una valutazione più affidabile delle prestazioni del modello. Metriche come l’“area under the precision-recall curve (AUPRC)” [area sotto la curva di precisione-richiamo] o il punteggio F1 sono più robuste allo squilibrio di classe e possono catturare meglio le prestazioni del modello su diverse distribuzioni. Inoltre, l’utilizzo di metriche di valutazione specifiche del dominio che si allineano con i risultati desiderati nel dominio target può fornire una misura più significativa dell’efficacia del modello.\nRilevare e mitigare i “distribution shift” è un processo continuo che richiede monitoraggio, adattamento e miglioramento continui. Utilizzando una combinazione di tecniche di rilevamento e strategie di mitigazione, i professionisti del ML possono identificare e affrontare in modo proattivo i “distribution shift”, garantendo la robustezza e l’affidabilità dei loro modelli nelle distribuzioni del mondo reale. È importante notare che i “distribution shift” possono assumere varie forme e potrebbero richiedere approcci specifici del dominio a seconda della natura dei dati e dell’applicazione. Rimanere informati sulle ultime ricerche e sulle best practice nella gestione dei “distribution shift” è essenziale per creare sistemi ML resilienti.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html#errori-software",
    "href": "contents/robust_ai/robust_ai.it.html#errori-software",
    "title": "17  IA Robusta",
    "section": "17.5 Errori Software",
    "text": "17.5 Errori Software\n\n17.5.1 Definizione e Caratteristiche\nGli errori software si riferiscono a difetti, errori o bug nei framework software runtime e nei componenti che supportano l’esecuzione e la distribuzione di modelli ML (Myllyaho et al. 2022). Questi guasti possono derivare da varie fonti, come errori di programmazione, difetti di progettazione o problemi di compatibilità (H. Zhang 2008), e possono avere implicazioni significative per le prestazioni, l’affidabilità e la sicurezza dei sistemi ML. Gli errori software nei framework ML presentano diverse caratteristiche chiave:\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen, e Tommi Mikkonen. 2022. «On misbehaviour and fault tolerance in machine learning systems». J. Syst. Software 183 (gennaio): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\nZhang, Hongyu. 2008. «On the Distribution of Software Faults». IEEE Trans. Software Eng. 34 (2): 301–2. https://doi.org/10.1109/tse.2007.70771.\n\nDiversità: Gli errori software possono manifestarsi in forme diverse, che vanno da semplici errori di logica e sintassi a problemi più complessi come perdite di memoria, condizioni di “race” e problemi di integrazione. La varietà di tipi di errori aumenta la sfida di rilevarli e mitigarli in modo efficace.\nPropagazione: Nei sistemi ML, gli errori software possono propagarsi attraverso i vari layer e componenti del framework. Un errore in un modulo può innescare una cascata di errori o comportamenti imprevisti in altre parti del sistema, rendendo difficile individuare la causa principale e valutare l’impatto completo dell’errore.\nIntermittenza: Alcuni errori software possono presentare un comportamento intermittente, che si verifica sporadicamente o in condizioni specifiche. Questi errori possono essere particolarmente difficili da riprodurre e correggere, poiché possono manifestarsi in modo incoerente durante i test o il normale funzionamento.\nInterazione con i modelli ML: Gli errori software nei framework ML possono interagire con i modelli addestrati in modi sottili. Ad esempio, un errore nella pipeline di preelaborazione dei dati può introdurre rumore o distorsione negli input del modello, causando prestazioni degradate o previsioni errate. Analogamente, gli errori nel componente di servizio del modello possono causare incongruenze tra gli ambienti di training e inferenza.\nImpatto sulle proprietà del sistema: Gli errori software possono compromettere varie proprietà desiderabili dei sistemi ML, come prestazioni, scalabilità, affidabilità e sicurezza. Gli errori possono causare rallentamenti, crash, output errati o vulnerabilità che gli aggressori possono sfruttare.\nDipendenza da fattori esterni: Il verificarsi e l’impatto degli errori software nei framework ML dipendono spesso da fattori esterni, come la scelta di hardware, sistema operativo, librerie e configurazioni. Problemi di compatibilità e mancate corrispondenze di versione possono introdurre errori difficili da anticipare e mitigare.\n\nComprendere le caratteristiche degli errori software nei framework ML è fondamentale per sviluppare strategie efficaci di prevenzione, rilevamento e mitigazione degli errori. Riconoscendo la diversità, la propagazione, l’intermittenza e l’impatto dei guasti software, i professionisti del ML possono progettare sistemi più robusti e affidabili, resilienti a questi problemi.\n\n\n17.5.2 Meccanismi degli Errori Software nei Framework ML\nI framework di apprendimento automatico, come TensorFlow, PyTorch e sci-kit-learn, forniscono potenti strumenti e astrazioni per la creazione e l’implementazione di modelli ML. Tuttavia, questi framework non sono immuni da errori software che possono influire sulle prestazioni, l’affidabilità e la correttezza dei sistemi ML. Esploriamo alcuni degli errori software comuni che possono verificarsi nei framework ML:\nMemory Leak e Problemi di Gestione delle Risorse: Una gestione della memoria non corretta, come il mancato rilascio di memoria o la chiusura di handle di file, può portare a perdite di memoria e all’esaurimento delle risorse nel tempo. Questo problema è aggravato dall’utilizzo inefficiente della memoria, in cui la creazione di copie non necessarie di grandi tensori o il mancato sfruttamento di strutture dati efficienti in termini di memoria può causare un consumo eccessivo di memoria e degradare le prestazioni del sistema. Inoltre, la mancata gestione corretta della memoria GPU può causare errori di “out-of-memory” o un utilizzo non ottimale delle risorse GPU, aggravando ulteriormente il problema come mostrato in Figura 17.34.\n\n\n\n\n\n\nFigura 17.34: Esempio di problemi di memoria e utilizzo non ottimale della GPU\n\n\n\nProblemi di Sincronizzazione e Concorrenza: Una sincronizzazione non corretta tra thread o processi può causare condizioni di “race”, deadlock o comportamento incoerente nei sistemi ML multi-thread o distribuiti. Questo problema è spesso legato alla gestione impropria delle operazioni asincrone, come I/O non bloccante o caricamento dati parallelo, che può causare problemi di sincronizzazione e influire sulla correttezza della pipeline ML. Inoltre, un coordinamento e una comunicazione adeguati tra nodi distribuiti in un cluster possono causare coerenza o dati obsoleti durante l’addestramento o l’inferenza, compromettendo l’affidabilità del sistema ML.\nProblemi di Compatibilità: Le discrepanze tra le versioni di framework, librerie o dipendenze ML possono introdurre problemi di compatibilità ed errori di runtime. L’aggiornamento o la modifica delle versioni delle librerie sottostanti senza testare a fondo l’impatto sul sistema ML può portare a comportamenti imprevisti o malfunzionamenti. Inoltre, le incongruenze tra gli ambienti di training e distribuzione, come differenze nell’hardware, nei sistemi operativi o nelle versioni dei pacchetti, possono causare problemi di compatibilità e influire sulla riproducibilità dei modelli ML, rendendo difficile garantire prestazioni coerenti sulle diverse piattaforme.\nInstabilità Numerica ed Errori di Precisione: Una gestione inadeguata delle instabilità numeriche, come la divisione per zero, l’underflow o l’overflow, può portare a calcoli errati o problemi di convergenza durante l’addestramento. Questo problema è aggravato da errori di precisione o arrotondamento insufficienti, che possono accumularsi nel tempo e influire sull’accuratezza dei modelli ML, specialmente nelle architetture di deep learning con molti livelli. Inoltre, un ridimensionamento o una normalizzazione impropri dei dati di input possono causare instabilità numeriche e influire sulla convergenza e sulle prestazioni degli algoritmi di ottimizzazione, con conseguenti prestazioni del modello non ottimali o inaffidabili.\nGestione degli Errori e delle Eccezioni Inadeguata: Una corretta gestione degli errori e delle eccezioni può impedire ai sistemi ML di bloccarsi o comportarsi in modo imprevisto quando si verificano condizioni eccezionali o input non validi. Non riuscire a catturare e gestire eccezioni specifiche o affidarsi alla gestione generica delle eccezioni può rendere difficile diagnosticare e recuperare gli errori in modo corretto, portando a instabilità del sistema e affidabilità ridotta. Inoltre, messaggi di errore incompleti o fuorvianti possono ostacolare la capacità di eseguire il debug e risolvere efficacemente gli errori software nei framework ML, prolungando il tempo necessario per identificare e risolvere i problemi.\n\n\n17.5.3 Impatto sui Sistemi ML\nGli errori software nei framework di apprendimento automatico possono avere impatti significativi e di vasta portata sulle prestazioni, l’affidabilità e la sicurezza dei sistemi ML. Esploriamo i vari modi in cui gli errori software possono influenzare i sistemi ML:\nDegrado delle Prestazioni e Rallentamenti del Sistema: Memory leak e gestione inefficiente delle risorse possono portare a un graduale degrado delle prestazioni nel tempo, poiché il sistema diventa sempre più vincolato dalla memoria e impiega più tempo nella garbage collection o nello swapping della memoria (Maas et al. 2024). Questo problema è aggravato da problemi di sincronizzazione e bug di concorrenza, che possono causare ritardi, riduzione della produttività e utilizzo non ottimale delle risorse di elaborazione, in particolare nei sistemi ML multi-thread o distribuiti. Inoltre, problemi di compatibilità o percorsi di codice inefficienti possono introdurre ulteriori overhead e rallentamenti, influenzando le prestazioni complessive del sistema ML.\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi Javanmard, Kathryn S. McKinley, e Colin Raffel. 2024. «Combining Machine Learning and Lifetime-Based Resource Management for Memory Allocation and Beyond». Commun. ACM 67 (4): 87–96. https://doi.org/10.1145/3611018.\nPrevisioni o Output Errati: Gli errori software nella pre-elaborazione dei dati, nell’ingegneria delle feature o nella valutazione del modello possono introdurre distorsioni, rumore o errori che si propagano attraverso la pipeline ML e che determinano previsioni o output errati. Nel tempo, instabilità numeriche, errori di precisione o problemi di arrotondamento possono accumularsi e portare a problemi di accuratezza o convergenza degradati nei modelli addestrati. Inoltre, gli errori nei componenti di servizio o inferenza del modello possono causare incongruenze tra gli output previsti e quelli effettivi, portando a previsioni errate o inaffidabili in produzione.\nProblemi di Affidabilità e Stabilità: Gli errori software possono causare eccezioni senza precedenti, crash o terminazioni improvvise che possono compromettere l’affidabilità e la stabilità dei sistemi ML, specialmente negli ambienti di produzione. Gli errori intermittenti o sporadici possono essere difficili da riprodurre e diagnosticare, portando a un comportamento imprevedibile e a una ridotta fiducia negli output del sistema ML. Inoltre, errori nel checkpointing, nella serializzazione del modello o nella gestione dello stato possono causare perdite di dati o incongruenze, influenzando l’affidabilità e la recuperabilità del sistema ML.\nVulnerabilità di Sicurezza: Errori software, come buffer overflow, vulnerabilità di “injection” o controllo di accesso improprio, possono introdurre rischi per la sicurezza ed esporre il sistema ML a potenziali attacchi o accessi non autorizzati. Gli avversari possono sfruttare errori nelle fasi di pre-elaborazione o estrazione delle funzionalità per manipolare i dati di input e ingannare i modelli ML, portando a comportamenti errati o dannosi. Inoltre, una protezione inadeguata dei dati sensibili, come le informazioni utente o i parametri riservati del modello, può portare a violazioni dei dati o violazioni della privacy (Q. Li et al. 2023).\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, e Bingsheng He. 2023. «A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection». IEEE Trans. Knowl. Data Eng. 35 (4): 3347–66. https://doi.org/10.1109/tkde.2021.3124599.\nDifficoltà nella Riproduzione e nel Debug: Gli errori software possono rendere difficile la riproduzione e il debug dei problemi nei sistemi ML, soprattutto quando gli errori sono intermittenti o dipendono da condizioni di runtime specifiche. Messaggi di errore incompleti o ambigui, uniti alla complessità dei framework e dei modelli ML, possono prolungare il processo di debug e ostacolare la capacità di identificare e correggere i guasti sottostanti. Inoltre, le incongruenze tra gli ambienti di sviluppo, test e produzione possono rendere difficile la riproduzione e la diagnosi dei guasti in contesti specifici.\nMaggiori Costi di Sviluppo e Manutenzione I guasti software possono comportare maggiori costi di sviluppo e manutenzione, poiché i team dedicano più tempo e risorse al debug, alla correzione e alla validazione del sistema ML. La necessità di test estesi, monitoraggio e meccanismi di tolleranza agli errori per mitigare l’impatto degli errori software può aggiungere complessità e sovraccarico al processo di sviluppo ML. Patch, aggiornamenti e correzioni di bug frequenti per risolvere gli errori software possono interrompere il flusso di lavoro di sviluppo e richiedere sforzi aggiuntivi per garantire la stabilità e la compatibilità del sistema ML.\nComprendere il potenziale impatto degli errori software sui sistemi ML è fondamentale per dare priorità agli sforzi di test, implementare progetti di tolleranza agli errori e stabilire pratiche di monitoraggio e debug efficaci. Affrontando in modo proattivo gli errori software e le loro conseguenze, i professionisti ML possono creare sistemi ML più solidi, affidabili e sicuri che forniscono risultati accurati e affidabili.\n\n\n17.5.4 Rilevamento e Mitigazione\nRilevare e mitigare i guasti software nei framework di apprendimento automatico è essenziale per garantire l’affidabilità, le prestazioni e la sicurezza dei sistemi ML. Esploriamo varie tecniche e approcci che possono essere impiegati per identificare e risolvere efficacemente i guasti software:\nTest e Validazione Approfonditi: “Unit test” completi di singoli componenti e moduli possono verificarne la correttezza e identificare potenziali guasti nelle prime fasi dello sviluppo. I test di integrazione convalidano l’interazione e la compatibilità tra diversi componenti del framework ML, garantendo un’integrazione senza soluzione di continuità. I test sistematici di casi limite, condizioni al contorno e scenari eccezionali aiutano a scoprire guasti e vulnerabilità nascosti. Il “continuous testing” e i test di regressione come mostrato in Figura 17.35 rilevano i guasti introdotti da modifiche al codice o aggiornamenti al framework ML.\n\n\n\n\n\n\nFigura 17.35: Test di regressione automatizzati. Fonte: UTOR\n\n\n\nAnalisi Statica del Codice e Linting: L’utilizzo di strumenti di analisi statica del codice identifica automaticamente potenziali problemi di codifica, come errori di sintassi, variabili non definite o vulnerabilità di sicurezza. L’applicazione di standard di codifica e best practice tramite strumenti di “linting” mantiene la qualità del codice e riduce la probabilità di comuni errori di programmazione. L’esecuzione di revisioni regolari del codice consente l’ispezione manuale della base di codice, l’identificazione di potenziali errori e garantisce l’aderenza alle linee guida di codifica e ai principi di progettazione.\nMonitoraggio e Logging in Fase di Esecuzione: L’implementazione di meccanismi di logging completi cattura informazioni rilevanti durante l’esecuzione, come dati di input, parametri del modello ed eventi di sistema. Il monitoraggio delle metriche delle prestazioni chiave, dell’utilizzo delle risorse e dei tassi di errore aiuta a rilevare anomalie, colli di bottiglia delle prestazioni o comportamenti imprevisti. L’impiego di controlli di asserzione in fase di esecuzione e invarianti, convalida le ipotesi e rileva violazioni delle condizioni previste durante l’esecuzione del programma. L’utilizzo di strumenti di profilazione consente di identificare colli di bottiglia nelle prestazioni, memory leak o percorsi di codice inefficienti che potrebbero indicare la presenza di errori software.\nDesign Pattern a Tolleranza di Errore: L’implementazione di meccanismi di gestione degli errori e delle eccezioni consente una gestione e un ripristino controllato da condizioni eccezionali o errori di runtime. L’impiego di meccanismi di ridondanza e failover, come sistemi di backup o calcoli ridondanti, garantisce la disponibilità e l’affidabilità del sistema ML in presenza di errori. La progettazione di architetture modulari e debolmente accoppiate riduce al minimo la propagazione e l’impatto dei guasti su diversi componenti del sistema ML. L’utilizzo di meccanismi di checkpointing e ripristino (Eisenman et al. 2022) consente al sistema di riprendere da uno stato stabile noto in caso di guasti o interruzioni.\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, e Murali Annavaram. 2022. «Check-N-Run: A checkpointing system for training deep learning recommendation models». In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), 929–43.\nAggiornamenti e Patch Regolari: Rimanere aggiornati con le ultime versioni e patch dei framework, delle librerie e delle dipendenze ML offre vantaggi in termini di correzioni di bug, aggiornamenti di sicurezza e miglioramenti delle prestazioni. Il monitoraggio delle note di rilascio, degli avvisi di sicurezza e dei forum della community informa i professionisti su problemi noti, vulnerabilità o problemi di compatibilità nel framework ML. L’istituzione di un processo sistematico per testare e convalidare aggiornamenti e patch prima di applicarli ai sistemi di produzione garantisce stabilità e compatibilità.\nContainerizzazione e Isolamento: Sfruttando le tecnologie di containerizzazione, come Docker o Kubernetes, si incapsulano i componenti ML e le relative dipendenze in ambienti isolati. L’utilizzo della containerizzazione garantisce ambienti di runtime coerenti e riproducibili nelle fasi di sviluppo, test e produzione, riducendo la probabilità di problemi di compatibilità o errori specifici dell’ambiente. L’impiego di tecniche di isolamento, come ambienti virtuali o sandbox, impedisce che errori o vulnerabilità in un componente influiscano su altre parti del sistema ML.\nTest Automatizzati e Continuous Integration/Continuous Deployment (CI/CD): Implementare framework e script di test automatizzati, eseguire suite di test complete e individuare gli errori nelle prime fasi dello sviluppo. L’integrazione di test automatizzati nella pipeline CI/CD, come mostrato in Figura 17.36, garantisce che le modifiche al codice siano testate a fondo prima di essere unite o distribuite in produzione. L’utilizzo di sistemi di monitoraggio continuo e di allerta automatizzati rilevano e notificano a sviluppatori e operatori potenziali guasti o anomalie in tempo reale.\n\n\n\n\n\n\nFigura 17.36: Procedura di Continuous Integration/Continuous Deployment (CI/CD). Fonte: geeksforgeeks\n\n\n\nL’adozione di un approccio proattivo e sistematico al rilevamento e alla mitigazione degli errori può migliorare significativamente la robustezza, l’affidabilità e la manutenibilità dei sistemi ML. Investendo in pratiche complete di test, monitoraggio e progettazione tollerante agli errori, le organizzazioni possono ridurre al minimo l’impatto degli errori software e garantire il regolare funzionamento dei loro sistemi ML negli ambienti di produzione.\n\n\n\n\n\n\nEsercizio 17.4: Tolleranza agli Errori\n\n\n\n\n\nPreparatevi a diventare supereroi che combattono gli errori dell’IA! I problemi software possono far deragliare i sistemi di apprendimento automatico, ma in questo Colab impareremo come renderli resilienti. Simuleremo errori software per vedere come l’IA può “rompersi”, poi esploreremo tecniche per salvare i progressi del modello ML, come i checkpoint in un gioco. Vedremo come addestrare l’IA a riprendersi dopo un crash, assicurando che rimanga sulla buona strada. Questo è fondamentale per creare un’IA affidabile e degna di fiducia, soprattutto nelle applicazioni critiche. Quindi preparatevi perché questo Colab si collega direttamente al capitolo IA Robusta: passeremo dalla teoria alla risoluzione pratica dei problemi e creeremo sistemi di intelligenza artificiale in grado di gestire l’imprevisto!",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html#strumenti-e-framework",
    "href": "contents/robust_ai/robust_ai.it.html#strumenti-e-framework",
    "title": "17  IA Robusta",
    "section": "17.6 Strumenti e Framework",
    "text": "17.6 Strumenti e Framework\nData l’importanza di sviluppare sistemi di IA robusti, negli ultimi anni ricercatori e professionisti hanno sviluppato un’ampia gamma di strumenti e framework per comprendere come i guasti hardware si manifestano e si propagano per avere un impatto sui sistemi ML. Questi strumenti e framework svolgono un ruolo cruciale nella valutazione della resilienza dei sistemi ML ai guasti hardware simulando vari scenari di guasto e analizzandone l’impatto sulle prestazioni del sistema. Ciò consente ai progettisti di identificare potenziali vulnerabilità e sviluppare strategie di mitigazione efficaci, creando in definitiva sistemi ML più robusti e affidabili in grado di funzionare in sicurezza nonostante i guasti hardware. Questa sezione fornisce una panoramica dei modelli di guasto ampiamente utilizzati nella letteratura e degli strumenti e framework sviluppati per valutare l’impatto di tali guasti sui sistemi ML.\n\n17.6.1 Modelli di Guasto e Modelli di Errore\nCome discusso in precedenza, i guasti hardware possono manifestarsi in vari modi, tra cui guasti transitori, permanenti e intermittenti. Oltre al tipo di guasto in esame, è importante anche come si manifesta il guasto. Ad esempio, l’errore si verifica in una cella di memoria o durante il calcolo di un’unità funzionale? L’impatto è su un singolo bit o su più bit? L’errore si propaga per tutto il percorso e ha un impatto sull’applicazione (causando un errore) o viene mascherato rapidamente ed è considerato benigno? Tutti questi dettagli hanno un impatto su ciò che è noto come fault model [modello di errore], che svolge un ruolo importante nella simulazione e nella misurazione di ciò che accade a un sistema quando si verifica un errore.\nPer studiare e comprendere efficacemente l’impatto degli errori hardware sui sistemi ML, è essenziale comprendere i concetti di “fault model” e “error model”. Un “fault model” [guasto] descrive come si manifesta un errore hardware nel sistema, mentre un “error model” [modello di errore] rappresenta come l’errore si propaga e influisce sul comportamento del sistema.\nI “fault model” possono essere categorizzati in base a varie caratteristiche:\n\nDurata: I guasti transitori si verificano brevemente e poi scompaiono, mentre quelli permanenti persistono indefinitamente. I guasti intermittenti si verificano sporadicamente e possono essere difficili da diagnosticare.\nPosizione: I guasti possono verificarsi in componenti hardware, come celle di memoria, unità funzionali o interconnessioni.\nGranularità: I guastipossono interessare un singolo bit (ad esempio, bitflip) o più bit (ad esempio, errori burst) all’interno di un componente hardware.\n\nD’altro canto, gli “error model” descrivono come un guasto si propaga nel sistema e si manifesta come un errore. Un errore può causare la deviazione del sistema dal comportamento previsto, portando a risultati errati o persino a guasti del sistema. I modelli di errore possono essere definiti a diversi livelli di astrazione, da quello hardware (ad esempio, bitflip a livello di registro) al livello software (ad esempio, pesi o attivazioni corrotti in un modello ML).\nIl “fault model” (o il modello di errore, in genere la terminologia più applicabile per comprendere la robustezza di un sistema ML) svolge un ruolo importante nella simulazione e nella misura di ciò che accade a un sistema quando si verifica un guasto. Il modello scelto informa le ipotesi fatte sul sistema in fase di studio. Ad esempio, un sistema incentrato su errori transitori a bit singolo (Sangchoolie, Pattabiraman, e Karlsson 2017) non sarebbe adatto a comprendere l’impatto di errori permanenti di flip multi-bit (Wilkening et al. 2014), poiché è progettato presupponendo un modello completamente diverso.\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva Gurumurthi, e David R. Kaeli. 2014. «Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults». In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 293–305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\nInoltre, anche l’implementazione di un modello di errore è una considerazione importante, in particolare per quanto riguarda il punto in cui si dice che si verifichi un errore nello stack di elaborazione. Ad esempio, un modello di flip a bit singolo a livello di registro architetturale differisce da un modello di flip a bit singolo nel peso di un modello a livello di PyTorch. Sebbene entrambi mirino a un modello di errore simile, il primo verrebbe solitamente modellato in un simulatore architetturalmente accurato (come gem5 [binkert2011gem5]), che cattura la propagazione dell’errore rispetto al secondo, concentrandosi sulla propagazione del valore attraverso un modello.\nRicerche recenti hanno dimostrato che alcune caratteristiche dei modelli di errore possono mostrare comportamenti simili a diversi livelli di astrazione (Sangchoolie, Pattabiraman, e Karlsson 2017) (Papadimitriou e Gizopoulos 2021). Ad esempio, gli errori a bit singolo sono generalmente più problematici degli errori a bit multiplo, indipendentemente dal fatto che siano modellati a livello hardware o software. Tuttavia, altre caratteristiche, come il mascheramento degli errori (Mohanram e Touba 2003) come mostrato in Figura 17.37, potrebbero non essere sempre catturate accuratamente dai modelli a livello software, poiché possono nascondere gli effetti di sistema sottostanti.\n\nSangchoolie, Behrooz, Karthik Pattabiraman, e Johan Karlsson. 2017. «One Bit is (Not) Enough: An Empirical Study of the Impact of Single and Multiple Bit-Flip Errors». In 2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 97–108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\nPapadimitriou, George, e Dimitris Gizopoulos. 2021. «Demystifying the System Vulnerability Stack: Transient Fault Effects Across the Layers». In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), 902–15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\nMohanram, K., e N. A. Touba. 2003. «Partial error masking to reduce soft error failure rate in logic circuits». In Proceedings. 16th IEEE Symposium on Computer Arithmetic, 433–40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\n\n\n\n\nFigura 17.37: Esempio di mascheramento degli errori nei componenti microarchitettonici (Ko 2021)\n\n\nKo, Yohan. 2021. «Characterizing System-Level Masking Effects against Soft Errors». Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nAlcuni strumenti, come Fidelity (He, Balaprakash, e Li 2020), mirano a colmare il divario tra modelli di errore a livello hardware e software mappando i pattern tra i due livelli di astrazione (Cheng et al. 2016). Ciò consente una modellazione più accurata dei guasti hardware negli strumenti basati su software, essenziale per lo sviluppo di sistemi ML robusti e affidabili. Gli strumenti a più basso livello in genere rappresentano caratteristiche di propagazione degli errori più accurate, ma devono essere più rapidi nella simulazione di molti errori a causa della natura complessa delle progettazioni dei sistemi hardware. D’altro canto, gli strumenti a più alto livello, come quelli implementati in framework ML come PyTorch o TensorFlow, di cui parleremo presto nelle sezioni successive, sono spesso più rapidi ed efficienti per valutare la robustezza dei sistemi ML.\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher, Hyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. «Clear: uC/u ross u-L/u ayer uE/u xploration for uA/u rchitecting uR/u esilience - Combining hardware and software techniques to tolerate soft errors in processor cores». In Proceedings of the 53rd Annual Design Automation Conference, 1–6. ACM. https://doi.org/10.1145/2897937.2897996.\nNelle sottosezioni seguenti, discuteremo vari metodi e strumenti di iniezione di guasti basati su hardware e software, evidenziandone le capacità, le limitazioni e i modelli di guasti ed errori che supportano.\n\n\n17.6.2 Injection Hardware-based di Guasti\nUno strumento di “iniezione di errori” è uno strumento che consente all’utente di implementare un particolare modello di errore, come un singolo bit flip transitorio durante l’inferenza Figura 17.38. La maggior parte degli strumenti di iniezione di errori sono basati su software, poiché sono più rapidi per gli studi di robustezza ML. Tuttavia, i metodi di iniezione di guasti basati su hardware sono ancora importanti per radicare i modelli di errore ad alto livello, poiché sono considerati il modo più accurato per studiare l’impatto dei guasti sui sistemi ML manipolando direttamente l’hardware per introdurli. Questi metodi consentono ai ricercatori di osservare il comportamento del sistema in condizioni di guasti reali. In questa sezione vengono descritti in modo più dettagliato sia gli strumenti di iniezione di errori basati su software che quelli basati su hardware.\n\n\n\n\n\n\nFigura 17.38: Gli errori hardware possono verificarsi per una serie di motivi e in momenti e/o posizioni diverse in un sistema, il che può essere esplorato quando si studia l’impatto degli errori basati sull’hardware sui sistemi (Ahmadilivani et al. 2024)\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, e Maksim Jenihhin. 2024. «A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks». ACM Comput. Surv. 56 (6): 1–39. https://doi.org/10.1145/3638242.\n\n\n\nMetodi\nDue dei metodi di iniezione di guasti basati su hardware più comuni sono quelli basati su FPGA e il test di radiazione o di fascio.\nIniezione di Guasti FPGA-based: I “Field-Programmable Gate Array (FPGA)” sono circuiti integrati riconfigurabili che possono essere programmati per implementare vari progetti hardware. Nel contesto dell’iniezione di guasti, gli FPGA offrono elevata precisione e accuratezza, poiché i ricercatori possono mirare a bit specifici o set di bit all’interno dell’hardware. Modificando la configurazione dell’FPGA, i guasti possono essere introdotti in posizioni e tempi specifici durante l’esecuzione di un modello ML. L’iniezione di guasti basata su FPGA consente un controllo dettagliato sul “fault model”, consentendo ai ricercatori di studiare l’impatto di diversi tipi di guasti, come i flip di bit singoli o gli errori multi-bit. Questo livello di controllo rende l’iniezione di guasti basata su FPGA uno strumento prezioso per comprendere la resilienza dei sistemi ML ai guasti hardware.\nTest di Radiazioni o Fasci: Il test di radiazioni o fasci (Velazco, Foucard, e Peronnard 2010) comporta l’esposizione dell’hardware che esegue un modello ML a particelle ad alta energia, come protoni o neutroni, come illustrato in Figura 17.39. Queste particelle possono causare bitflip o altri tipi di guasti nell’hardware, imitando gli effetti di quelli indotti dalle radiazioni nel mondo reale. Il test di fasci è ampiamente considerato un metodo altamente accurato per misurare il tasso di errore indotto da impatti di particelle su un’applicazione in esecuzione. Fornisce una rappresentazione realistica dei guasti in ambienti reali, in particolare in applicazioni esposte ad alti livelli di radiazioni, come sistemi spaziali o esperimenti di fisica delle particelle. Tuttavia, a differenza dell’iniezione di guasti basata su FPGA, il test di fasci potrebbe essere più preciso nel puntare a bit o componenti specifici all’interno dell’hardware, poiché potrebbe essere difficile puntare il fascio di particelle a un bit particolare nell’hardware. Nonostante sia piuttosto costoso dal punto di vista della ricerca, il test del fascio è una pratica industriale molto apprezzata per l’affidabilità.\n\nVelazco, Raoul, Gilles Foucard, e Paul Peronnard. 2010. «Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in SRAM-Based FPGAs». IEEE Trans. Nucl. Sci. 57 (6): 3500–3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\n\n\n\n\n\nFigura 17.39: Configurazione del test di radiazione per componenti semiconduttori (Lee et al. 2022) Fonte: JD Instrument\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang, e Seongik Cho. 2022. «Design of Radiation-Tolerant High-Speed Signal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear Explosion». Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\n\n\nLimitazioni\nNonostante la loro elevata accuratezza, i metodi di iniezione di guasti basati su hardware presentano diverse limitazioni che possono ostacolarne l’adozione diffusa:\nCosto: L’iniezione di guasti e il test del fascio basati su FPGA richiedono hardware e strutture specializzate, la cui configurazione e manutenzione possono essere costose. Il costo di questi metodi può rappresentare un ostacolo significativo per ricercatori e organizzazioni con risorse limitate.\nScalabilità: I metodi basati su hardware sono generalmente più lenti e meno scalabili rispetto ai metodi basati su software. L’iniezione di guasti e la raccolta di dati sull’hardware possono richiedere tempo, limitando il numero di esperimenti eseguiti in un determinato lasso di tempo. Ciò può essere particolarmente impegnativo quando si studia la resilienza di sistemi ML su larga scala o si conducono analisi statistiche che richiedono molti esperimenti di iniezione di guasti.\nFlessibilità: I metodi basati su hardware potrebbero non essere flessibili quanto quelli basati su software in termini di gamma di modelli di guasto e modelli di errore che possono supportare. Modificare la configurazione hardware o l’impostazione sperimentale per adattarsi a diversi modelli di errore può essere più impegnativo e richiedere più tempo rispetto ai metodi basati su software.\nNonostante queste limitazioni, i metodi di iniezione di errori basati su hardware rimangono strumenti essenziali per convalidare l’accuratezza dei metodi basati su software e per studiare l’impatto degli errori sui sistemi ML in contesti realistici. Combinando metodi basati su hardware e basati su software, i ricercatori possono acquisire una comprensione più completa della resilienza dei sistemi ML ai guasti hardware e sviluppare strategie di mitigazione efficaci.\n\n\n\n17.6.3 Strumenti di Injection di Guasti Software-based\nCon il rapido sviluppo di framework ML negli ultimi anni, gli strumenti di iniezione di guasti basati su software hanno guadagnato popolarità nello studio della resilienza dei sistemi ML ai guasti hardware. Questi strumenti simulano gli effetti dei guasti hardware modificando la rappresentazione software del modello ML o il grafo computazionale sottostante. L’ascesa di framework ML come TensorFlow, PyTorch e Keras ha facilitato lo sviluppo di strumenti di iniezione di guasti che sono strettamente integrati con questi framework, rendendo più facile per i ricercatori condurre esperimenti di iniezione di guasti e analizzare i risultati.\n\nVantaggi e Compromessi\nGli strumenti di iniezione di guasti basati su software offrono diversi vantaggi rispetto a quelli basati su hardware:\nVelocità: Gli strumenti basati su software sono generalmente più rapidi dei metodi basati su hardware, poiché non richiedono la modifica dell’hardware fisico o la configurazione di apparecchiature specializzate. Ciò consente ai ricercatori di condurre più esperimenti di iniezione di guasti in tempi più brevi, consentendo analisi più complete della resilienza dei sistemi ML.\nFlessibilità: Gli strumenti basati su software sono più flessibili di quelli basati su hardware in termini di gamma di modelli di guasti ed errori che possono supportare. I ricercatori possono facilmente modificare l’implementazione software dello strumento di iniezione di guasti per adattarsi a diversi modelli di guasti o per indirizzare componenti specifici del sistema ML.\nAccessibilità: Gli strumenti basati su software sono più accessibili dei metodi basati su hardware, poiché non richiedono hardware o strutture specializzate. Ciò semplifica per ricercatori e professionisti condurre esperimenti di iniezione di guasti e studiare la resilienza dei sistemi ML, anche con risorse limitate.\n\n\nLimitazioni\nGli strumenti di iniezione di guasti basati su software presentano anche alcune limitazioni rispetto ai metodi basati su hardware:\nPrecisione: Gli strumenti basati su software potrebbero non sempre catturano l’intera gamma di effetti che i guasti hardware possono avere sul sistema. Poiché questi strumenti operano a un livello di astrazione più elevato, potrebbero dover recuperare alcune delle interazioni hardware di basso livello e dei meccanismi di propagazione degli errori che possono influire sul comportamento del sistema ML.\nFedeltà: Gli strumenti basati su software potrebbero fornire un livello di fedeltà diverso rispetto ai metodi basati su hardware in termini di rappresentazione delle condizioni di guasto del mondo reale. L’accuratezza dei risultati ottenuti dagli esperimenti di iniezione di guasti basati su software potrebbe dipendere da quanto il modello software si avvicini al comportamento hardware effettivo.\n\n\n\n\n\n\nFigura 17.40: Confronto delle tecniche a livelli di astrazione. Fonte: MAVFI\n\n\n\n\n\nTipi di Strumenti di Iniezione di Guasti\nGli strumenti di iniezione di guasti basati su software possono essere categorizzati in base ai loro framework di destinazione o casi d’uso. Qui, discuteremo alcuni degli strumenti più popolari in ciascuna categoria:\nAres (Reagen et al. 2018), uno strumento di iniezione di guasti inizialmente sviluppato per il framework Keras nel 2018, è emerso come uno dei primi strumenti per studiare l’impatto dei guasti hardware sulle reti deep neural network (DNN) nel contesto della crescente popolarità dei framework ML a metà-fine anni 2010. Lo strumento è stato convalidato rispetto a un acceleratore DNN implementato in silicio, dimostrando la sua efficacia nella modellazione dei guasti hardware. Ares fornisce uno studio completo sull’impatto dei guasti hardware sia nei pesi che nei valori di attivazione, caratterizzando gli effetti dei flip di bit singoli e dei bit-error rate (BER) sulle strutture hardware. Successivamente, il framework Ares è stato esteso per supportare l’ecosistema PyTorch, consentendo ai ricercatori di investigare i guasti hardware in un contesto più moderno e ampliando ulteriormente la sua utilità sul campo.\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, e Gu-Yeon Wei. 2018. «Ares: A framework for quantifying the resilience of deep neural networks». In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\n\n\n\n\n\nFigura 17.41: I bitflip hardware nei carichi di lavoro ML possono causare oggetti fantasma e classificazioni errate, che possono essere erroneamente utilizzati a valle da sistemi più grandi, come nella guida autonoma. Quella mostrata sopra è una versione corretta e difettosa della stessa immagine che utilizza il framework di iniezione PyTorchFI.\n\n\n\nPyTorchFI (Mahmoud et al. 2020), uno strumento di iniezione di guasti progettato specificamente per il framework PyTorch, è stato sviluppato nel 2020 in collaborazione con Nvidia Research. Consente l’iniezione di guasti nei pesi, nelle attivazioni e nei gradienti dei modelli PyTorch, supportando un’ampia gamma di modelli di guasti. Sfruttando le capacità di accelerazione GPU di PyTorch, PyTorchFI fornisce un’implementazione rapida ed efficiente per condurre esperimenti di iniezione di guasti su sistemi ML su larga scala, come mostrato in Figura 17.41. La velocità e la facilità d’uso dello strumento hanno portato a un’adozione diffusa nella comunità, con conseguenti molteplici progetti guidati dagli sviluppatori, come PyTorchALFI di Intel xColabs, che si concentra sulla sicurezza negli ambienti automobilistici. Gli strumenti successivi incentrati su PyTorch per l’iniezione di guasti includono Dr. DNA di Meta (Ma et al. 2024) (che facilita ulteriormente il modello di programmazione Pythonic per una maggiore facilità d’uso) e il framework GoldenEye (Mahmoud et al. 2022), che incorpora nuovi tipi di dati numerici (come AdaptivFloat (Tambe et al. 2020) e BlockFloat nel contesto di bit flip hardware.\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez Vicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, e Siva Kumar Sastry Hari. 2020. «PyTorchFI: A Runtime Perturbation Tool for DNNs». In 2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W), 25–31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore, Sriram Sankar, e Xun Jiao. 2024. «Dr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations». In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 239–52. ACM. https://doi.org/10.1145/3620666.3651349.\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, e Gu-Yeon Wei. 2022. «GoldenEye: A Platform for Evaluating Emerging Numerical Data Formats in DNN Accelerators». In 2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 206–14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, e Gu-Yeon Wei. 2020. «Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference». In 2020 57th ACM/IEEE Design Automation Conference (DAC), 1–6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2020. «TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications». In 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE), 426–35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2019. «iBinFI/i: an efficient fault injector for safety-critical machine learning systems». In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC ’19. New York, NY, USA: ACM. https://doi.org/10.1145/3295500.3356177.\nTensorFI (Chen et al. 2020), o TensorFlow Fault Injector, è uno strumento di iniezione di guasti sviluppato specificamente per il framework TensorFlow. Analogo ad Ares e PyTorchFI, TensorFI è considerato lo strumento all’avanguardia per gli studi di robustezza ML nell’ecosistema TensorFlow. Consente ai ricercatori di iniettare guasti nel grafo computazionale di Modelli TensorFlow e studia il loro impatto sulle prestazioni del modello, supportando un’ampia gamma di modelli di errore. Uno dei principali vantaggi di TensorFI è la sua capacità di valutare la resilienza di vari modelli ML, non solo DNN. Ulteriori progressi, come BinFi (Chen et al. 2019), forniscono un meccanismo per accelerare gli esperimenti di iniezione di errori concentrandosi sui bit “importanti” nel sistema, accelerando il processo di analisi della robustezza ML e dando priorità ai componenti critici di un modello.\nNVBitFI (T. Tsai et al. 2021), uno strumento di iniezione di errori generico sviluppato da Nvidia per le sue piattaforme GPU, opera a un più basso livello rispetto a strumenti specifici del framework come Ares, PyTorchFI e TensorFlow. Mentre questi strumenti si concentrano su varie piattaforme di deep learning per implementare ed eseguire analisi di robustezza, NVBitFI mira al codice di assemblaggio hardware sottostante per l’iniezione di guasti. Ciò consente ai ricercatori di iniettare guasti in qualsiasi applicazione in esecuzione su GPU Nvidia, rendendolo uno strumento versatile per studiare la resilienza dei sistemi ML e di altre applicazioni accelerate da GPU. Consentendo agli utenti di iniettare errori a livello di architettura, NVBitFI fornisce un modello di guasto più generico che non è limitato ai soli modelli ML. Poiché i sistemi GPU di Nvidia sono comunemente utilizzati in molti sistemi basati su ML, NVBitFI è uno strumento prezioso per un’analisi completa dell’iniezione di guasti in varie applicazioni.\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa, e Stephen W. Keckler. 2021. «NVBitFI: Dynamic Fault Injection for GPUs». In 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 284–91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\nEsempi specifici di dominio\nSono stati sviluppati strumenti di iniezione di guasti specifici per dominio per affrontare le sfide e i requisiti unici di vari domini applicativi ML, come veicoli autonomi e robotica. Questa sezione evidenzia tre strumenti di iniezione di guasti specifici per dominio: DriveFI e PyTorchALFI per veicoli autonomi e MAVFI per “uncrewed aerial vehicles (UAV)” [veicoli aerei senza equipaggio]. Questi strumenti consentono ai ricercatori di iniettare guasti hardware nei sottosistemi di percezione, controllo e altri sistemi complessi, consentendo loro di studiare l’impatto dei guasti sulle prestazioni e sulla sicurezza del sistema. Lo sviluppo di questi strumenti di iniezione di guasti basati su software ha notevolmente ampliato le capacità della comunità ML di sviluppare sistemi più robusti e affidabili in grado di funzionare in modo sicuro ed efficace in presenza di guasti hardware.\nDriveFI (Jha et al. 2019) è uno strumento di iniezione di guasti progettato per veicoli autonomi. Consente l’iniezione di guasti hardware nelle pipeline di percezione e controllo dei sistemi di veicoli autonomi, consentendo ai ricercatori di studiare l’impatto di questi guasti sulle prestazioni e sulla sicurezza del sistema. DriveFI è stato integrato con piattaforme di guida autonoma standard del settore, come Nvidia DriveAV e Baidu Apollo, rendendolo uno strumento prezioso per valutare la resilienza dei sistemi di veicoli autonomi.\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, e Ravishankar K. Iyer. 2019. «ML-Based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection». In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 112–24. IEEE; IEEE. https://doi.org/10.1109/dsn.2019.00025.\n\nGräfe, Ralf, Qutub Syed Sha, Florian Geissler, e Michael Paulitsch. 2023. «Large-Scale Application of Fault Injection into PyTorch Models -an Extension to PyTorchFI for Validation Efficiency». In 2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S), 56–62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\nPyTorchALFI (Gräfe et al. 2023) è un’estensione di PyTorchFI sviluppata da Intel xColabs per il dominio dei veicoli autonomi. Si basa sulle capacità di inserimento di guasti di PyTorchFI. Aggiunge funzionalità specificamente studiate per valutare la resilienza dei sistemi di veicoli autonomi, come la capacità di inserire guasti nei dati della telecamera e del sensore LiDAR.\nMAVFI (Hsiao et al. 2023) è uno strumento di inserimento di guasti progettato per il dominio della robotica, in particolare per i veicoli aerei senza equipaggio (UAV). MAVFI è basato sul framework Robot Operating System (ROS) e consente ai ricercatori di inserire guasti nei vari componenti di un sistema UAV, come sensori, attuatori e algoritmi di controllo. Valutando l’impatto di questi guasti sulle prestazioni e sulla stabilità del UAV, i ricercatori possono sviluppare sistemi UAV più resilienti e tolleranti ai guasti.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. «MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles». In 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nLo sviluppo di strumenti di iniezione di guasti basati su software ha notevolmente ampliato le capacità di ricercatori e professionisti di studiare la resilienza dei sistemi ML ai guasti hardware. Sfruttando la velocità, la flessibilità e l’accessibilità di questi strumenti, la comunità ML può sviluppare sistemi più robusti e affidabili in grado di funzionare in modo sicuro ed efficace in presenza di guasti hardware.\n\n\n\n\n17.6.4 Colmare il Divario tra Modelli di Errore Hardware e Software\nSebbene gli strumenti di iniezione di guasti basati su software offrano molti vantaggi in termini di velocità, flessibilità e accessibilità, potrebbero non sempre catturare accuratamente l’intera gamma di effetti che i guasti hardware possono avere sul sistema. Questo perché gli strumenti basati su software operano a un livello di astrazione più alto rispetto ai metodi basati su hardware e potrebbero non rilevare alcune delle interazioni hardware di basso livello e dei meccanismi di propagazione degli errori che possono influire sul comportamento del sistema ML.\nCome illustra Bolchini et al. (2023) nel suo lavoro, gli errori hardware possono manifestarsi in complessi modelli di distribuzione spaziale che sono difficili da replicare completamente con la sola iniezione di guasti basata su software. Identificano quattro modelli distinti: (a) singolo punto, in cui il guasto corrompe un singolo valore in una feature map; (b) stessa riga, in cui il guasto corrompe una riga parziale o intera in una singola feature map; (c) bullet wake, in cui il guasto corrompe la stessa posizione su più feature map; e (d) shatter glass, che combina gli effetti dei modelli della stessa riga e bullet wake, come mostrato in Figura 17.42. Questi intricati meccanismi di propagazione degli errori evidenziano la necessità di tecniche di iniezione di guasti consapevoli dell’hardware per valutare accuratamente la resilienza dei sistemi ML.\n\n\n\n\n\n\nFigura 17.42: Gli errori hardware possono manifestarsi in modi diversi a livello software, come classificato da Bolchini et al. (Bolchini et al. 2023)\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, e Alessandro Toschi. 2023. «Fast and Accurate Error Simulation for CNNs Against Soft Errors». IEEE Trans. Comput. 72 (4): 984–97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nI ricercatori hanno sviluppato strumenti per affrontare questo problema colmando il divario tra modelli di errore hardware di basso livello e modelli di errore software di livello superiore. Uno di questi strumenti è Fidelity, progettato per mappare i pattern tra guasti a livello hardware e le loro manifestazioni a livello software.\n\nFidelity: Colmare il Gap\nFidelity (He, Balaprakash, e Li 2020) è uno strumento per modellare accuratamente i guasti hardware negli esperimenti di iniezione di guasti basati su software. Ciò avviene studiando attentamente la relazione tra i guasti a livello hardware e il loro impatto sulla rappresentazione software del sistema ML.\n\nHe, Yi, Prasanna Balaprakash, e Yanjing Li. 2020. «FIdelity: Efficient Resilience Analysis Framework for Deep Learning Accelerators». In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 270–81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\nLe intuizioni chiave alla base di Fidelity sono:\n\nPropagazione dei Guasti: Fidelity modella il modo in cui i guasti si propagano attraverso l’hardware e si manifestano come errori nello stato del sistema visibile al software. Comprendendo questi modelli di propagazione, Fidelity può simulare con maggiore accuratezza gli effetti dei guasti hardware negli esperimenti basati sul software.\nEquivalenza dei Guasti: Fidelity identifica classi equivalenti di guasti hardware che producono errori simili a livello software. Ciò consente ai ricercatori di progettare modelli di guasti basati sul software che siano rappresentativi dei guasti hardware sottostanti senza la necessità di modellare ogni possibile guasto hardware singolarmente.\nApproccio a Strati: Fidelity impiega un approccio a strati alla modellazione dei guasti, in cui gli effetti dei guasti hardware vengono propagati attraverso più livelli di astrazione, dall’hardware al livello software. Questo approccio garantisce che i modelli di guasti basati sul software siano basati sul comportamento effettivo dell’hardware.\n\nIncorporando queste informazioni, Fidelity consente agli strumenti di iniezione di guasti basati su software di catturare con precisione gli effetti dei guasti hardware sui sistemi ML. Ciò è particolarmente importante per le applicazioni critiche per la sicurezza, in cui la resilienza del sistema ai guasti hardware è fondamentale.\n\n\nL’Importanza di Catturare il Vero Comportamento Hardware\nCatturare il vero comportamento hardware negli strumenti di iniezione di guasti basati su software è fondamentale per diversi motivi:\n\nPrecisione: Modellando con precisione gli effetti dei guasti hardware, gli strumenti basati su software possono fornire informazioni più affidabili sulla resilienza dei sistemi ML. Ciò è essenziale per progettare e convalidare sistemi tolleranti ai guasti che possono funzionare in modo sicuro ed efficace in presenza di guasti hardware.\nRiproducibilità: Quando gli strumenti basati su software catturano con precisione il comportamento hardware, gli esperimenti di iniezione di guasti diventano più riproducibili su diverse piattaforme e ambienti. Ciò è importante per lo studio scientifico della resilienza del sistema ML, poiché consente ai ricercatori di confrontare e convalidare i risultati su diversi studi e implementazioni.\nEfficienza: Gli strumenti basati su software che catturano il vero comportamento dell’hardware possono essere più efficienti nei loro esperimenti di iniezione di guasti concentrandosi sui modelli di guasti più rappresentativi e impattanti. Ciò consente ai ricercatori di coprire una gamma più ampia di scenari di guasti e configurazioni di sistema con risorse computazionali limitate.\nStrategie di Mitigazione: Comprendere come i guasti hardware si manifestano a livello software è fondamentale per sviluppare strategie di mitigazione efficaci. Catturando con precisione il comportamento dell’hardware, gli strumenti di iniezione di guasti basati su software possono aiutare i ricercatori a identificare i componenti più vulnerabili del sistema ML e progettare tecniche di rafforzamento mirate per migliorare la resilienza.\n\nStrumenti come Fidelity sono essenziali per far progredire lo stato dell’arte nella ricerca sulla resilienza del sistema ML. Questi strumenti consentono ai ricercatori di condurre esperimenti di iniezione di guasti più accurati, riproducibili ed efficienti colmando il divario tra modelli di errore hardware e software. Man mano che la complessità e la criticità dei sistemi ML continuano a crescere, l’importanza di catturare il vero comportamento hardware negli strumenti di iniezione di guasti basati su software diventerà sempre più evidente.\nLa ricerca in corso in quest’area cerca di perfezionare la mappatura tra modelli di errore hardware e software e di sviluppare nuove tecniche per simulare in modo efficiente i guasti hardware negli esperimenti basati su software. Man mano che questi strumenti maturano, forniranno alla comunità ML mezzi sempre più potenti e accessibili per studiare e migliorare la resilienza dei sistemi ML ai guasti hardware.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html#conclusione",
    "href": "contents/robust_ai/robust_ai.it.html#conclusione",
    "title": "17  IA Robusta",
    "section": "17.7 Conclusione",
    "text": "17.7 Conclusione\nSviluppare un’IA solida e resiliente è fondamentale man mano che i sistemi di apprendimento automatico diventano sempre più integrati in applicazioni critiche per la sicurezza e in ambienti reali. Questo capitolo ha esplorato le principali sfide alla robustezza dell’IA derivanti da guasti hardware, attacchi dannosi, cambiamenti di distribuzione e bug software.\nAlcune delle conclusioni principali includono quanto segue:\n\nGuasti Hardware: Guasti transitori, permanenti e intermittenti nei componenti hardware possono corrompere i calcoli e degradare le prestazioni dei modelli di apprendimento automatico se non vengono rilevati e mitigati correttamente. Tecniche come ridondanza, correzione degli errori e progetti fault-tolerant svolgono un ruolo cruciale nella creazione di sistemi ML resilienti in grado di resistere ai guasti hardware.\nRobustezza del Modello: Gli attori malintenzionati possono sfruttare le vulnerabilità nei modelli ML tramite attacchi avversari e avvelenamento dei dati, mirando a indurre classificazioni errate mirate, distorcere il comportamento appreso del modello o compromettere l’integrità e l’affidabilità del sistema. Inoltre, possono verificarsi “distribution shift” quando la distribuzione dei dati riscontrata durante l’implementazione differisce da quella osservata durante il training, con conseguente degrado delle prestazioni. L’implementazione di misure difensive, tra cui training avversario, rilevamento delle anomalie, architetture di modelli robuste e tecniche come adattamento del dominio, apprendimento per trasferimento e apprendimento continuo, è essenziale per proteggersi da queste sfide e garantire l’affidabilità e la generalizzazione del modello in ambienti dinamici.\nErrori Software: Gli errori nei framework ML, nelle librerie e negli stack software possono propagarsi, degradare le prestazioni e introdurre vulnerabilità di sicurezza. Test rigorosi, monitoraggio del runtime e adozione di modelli di progettazione tolleranti agli errori sono essenziali per la creazione di un’infrastruttura software robusta che supporti sistemi ML affidabili.\n\nPoiché i sistemi ML affrontano attività sempre più complesse con conseguenze nel mondo reale, dare priorità alla resilienza diventa fondamentale. Gli strumenti e i framework discussi in questo capitolo, tra cui tecniche di “fault injection” [iniezione di guasti], metodi di analisi degli errori e framework di valutazione della robustezza, forniscono ai professionisti i mezzi per testare a fondo e rafforzare i propri sistemi ML contro varie modalità di errore e condizioni avverse.\nAndando avanti, la resilienza deve essere un obiettivo centrale durante l’intero ciclo di vita dello sviluppo dell’IA, dalla raccolta dei dati e dall’addestramento del modello all’implementazione e al monitoraggio. Affrontando in modo proattivo le molteplici sfide alla robustezza, possiamo sviluppare sistemi di apprendimento automatico affidabili e sicuri, in grado di affrontare le complessità e le incertezze degli ambienti del mondo reale.\nLa ricerca futura sul ML robusto dovrebbe continuare a far progredire le tecniche per rilevare e mitigare guasti, attacchi e “shift” delle distribuzioni. Inoltre, esplorare nuovi paradigmi per lo sviluppo di architetture IA intrinsecamente resilienti, come sistemi auto-riparanti o meccanismi a prova di errore, sarà fondamentale per spingere i confini della robustezza dell’IA. Dando priorità alla resilienza e investendo nello sviluppo di sistemi di IA robusti, possiamo liberare il pieno potenziale delle tecnologie di apprendimento automatico, garantendone al contempo un’implementazione sicura, affidabile e responsabile in applicazioni del mondo reale. Mentre l’IA continua a plasmare il nostro futuro, la creazione di sistemi resilienti in grado di resistere alle sfide del mondo reale sarà un fattore determinante per il successo e l’impatto sociale di questa tecnologia trasformativa.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.it.html#sec-robust-ai-resource",
    "href": "contents/robust_ai/robust_ai.it.html#sec-robust-ai-resource",
    "title": "17  IA Robusta",
    "section": "17.8 Risorse",
    "text": "17.8 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 17.1\nEsercizio 17.2\nEsercizio 17.3\nEsercizio 17.4\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>IA Robusta</span>"
    ]
  },
  {
    "objectID": "contents/generative_ai/generative_ai.html",
    "href": "contents/generative_ai/generative_ai.html",
    "title": "18  Generative AI",
    "section": "",
    "text": "Coming soon!\nImagine a chapter that writes itself and adapts to your curiosity, generating new insights as you read. We’re working on something extraordinary!\nThis chapter will transform how you read and learn, dynamically generating content as you go. While we fine-tune this exciting new feature, we hope users get ready for an educational experience that’s as dynamic and unique as you are. Mark your calendars for the big reveal and bookmark this page.\nThe future of generative learning is here! — Vijay Janapa Reddi",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html",
    "href": "contents/ai_for_good/ai_for_good.html",
    "title": "19  AI for Good",
    "section": "",
    "text": "19.1 Introduction\nTo give ourselves a framework around which to think about AI for social good, we will be following the UN Sustainable Development Goals (SDGs). The UN SDGs are a collection of 17 global goals, shown in Figura 19.1, adopted by the United Nations in 2015 as part of the 2030 Agenda for Sustainable Development. The SDGs address global challenges related to poverty, inequality, climate change, environmental degradation, prosperity, and peace and justice.\nWhat is special about the SDGs is that they are a collection of interlinked objectives designed to serve as a “shared blueprint for peace and prosperity for people and the planet, now and into the future.” The SDGs emphasize sustainable development’s interconnected environmental, social, and economic aspects by putting sustainability at their center.\nA recent study (Vinuesa et al. 2020) highlights the influence of AI on all aspects of sustainable development, particularly on the 17 Sustainable Development Goals (SDGs) and 169 targets internationally defined in the 2030 Agenda for Sustainable Development. The study shows that AI can act as an enabler for 134 targets through technological improvements, but it also highlights the challenges of AI on some targets. The study shows that AI can benefit 67 targets when considering AI and societal outcomes. Still, it also warns about the issues related to the implementation of AI in countries with different cultural values and wealth.\nIn our book’s context, TinyML could help advance at least some of these SDG goals.\nThe portability, lower power requirements, and real-time analytics enabled by TinyML make it well-suited for addressing several sustainability challenges developing regions face. The widespread deployment of power solutions has the potential to provide localized and cost-effective monitoring to help achieve some of the UN’s SDGs. In the rest of the sections, we will dive into how TinyML is useful across many sectors that can address the UN SDGs.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#introduction",
    "href": "contents/ai_for_good/ai_for_good.html#introduction",
    "title": "19  AI for Good",
    "section": "",
    "text": "Vinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, e Francesco Fuso Nerini. 2020. «The role of artificial intelligence in achieving the Sustainable Development Goals». Nat. Commun. 11 (1): 1–10. https://doi.org/10.1038/s41467-019-14108-y.\n\n\n\n\n\n\nFigura 19.1: United Nations Sustainable Development Goals (SDG). Source: United Nations.\n\n\n\n\n\nGoal 1 - No Poverty: TinyML could help provide low-cost solutions for crop monitoring to improve agricultural yields in developing countries.\nGoal 2 - Zero Hunger: TinyML could enable localized and precise crop health monitoring and disease detection to reduce crop losses.\nGoal 3 - Good Health and Wellbeing: TinyML could help enable low-cost medical diagnosis tools for early detection and prevention of diseases in remote areas.\nGoal 6 - Clean Water and Sanitation: TinyML could monitor water quality and detect contaminants to ensure Access to clean drinking water.\nGoal 7 - Affordable and Clean Energy: TinyML could optimize energy consumption and enable predictive maintenance for renewable energy infrastructure.\nGoal 11 - Sustainable Cities and Communities: TinyML could enable intelligent traffic management, air quality monitoring, and optimized resource management in smart cities.\nGoal 13 - Climate Action: TinyML could monitor deforestation and track reforestation efforts. It could also help predict extreme weather events.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#agriculture",
    "href": "contents/ai_for_good/ai_for_good.html#agriculture",
    "title": "19  AI for Good",
    "section": "19.2 Agriculture",
    "text": "19.2 Agriculture\nAgriculture is essential to achieving many of the UN Sustainable Development Goals, including eradicating Hunger and malnutrition, promoting economic growth, and using natural resources sustainably. TinyML can be a valuable tool to help advance sustainable agriculture, especially for smallholder farmers in developing regions.\nTinyML solutions can provide real-time monitoring and data analytics for crop health and growing conditions - all without reliance on connectivity infrastructure. For example, low-cost camera modules connected to microcontrollers can monitor for disease, pests, and nutritional deficiencies. TinyML algorithms can analyze the images to detect issues early before they spread and damage yields. Precision monitoring can optimize inputs like water, fertilizer, and pesticides - improving efficiency and sustainability.\nOther sensors, such as GPS units and accelerometers, can track microclimate conditions, soil humidity, and livestock wellbeing. Local real-time data helps farmers respond and adapt better to changes in the field. TinyML analytics at the edge avoids lag, network disruptions, and the high data costs of cloud-based systems. Localized systems allow customization of specific crops, diseases, and regional issues.\nWidespread TinyML applications can help digitize smallholder farms to increase productivity, incomes, and resilience. The low cost of hardware and minimal connectivity requirements make solutions accessible. Projects across the developing world have shown the benefits:\n\nMicrosoft’s FarmBeats project is an end-to-end approach to enable data-driven farming by using low-cost sensors, drones, and vision and machine learning algorithms. The project seeks to solve the problem of limited adoption of technology in farming due to the need for more power and internet connectivity in farms and the farmers’ limited technology savviness. The project strives to increase farm productivity and reduce costs by coupling data with farmers’ knowledge and intuition about their farms. The project has successfully enabled actionable insights from data by building artificial intelligence (AI) or machine learning (ML) models based on fused data sets.\nIn Sub-Saharan Africa, off-the-shelf cameras and edge AI have cut cassava disease losses from 40% to 5%, protecting a staple crop (Ramcharan et al. 2017).\nIn Indonesia, sensors monitor microclimates across rice paddies, optimizing water usage even with erratic rains (Tirtalistyani, Murtiningrum, e Kanwar 2022).\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg, e David P. Hughes. 2017. «Deep Learning for Image-Based Cassava Disease Detection». Front. Plant Sci. 8 (ottobre): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, e Rameshwar S. Kanwar. 2022. «Indonesia Rice Irrigation System: Time for Innovation». Sustainability 14 (19): 12477. https://doi.org/10.3390/su141912477.\nWith greater investment and integration into rural advisory services, TinyML could transform small-scale agriculture and improve farmers’ livelihoods worldwide. The technology effectively brings the benefits of precision agriculture to disconnected regions most in need.\n\n\n\n\n\n\nEsercizio 19.1: Crop Yield Modeling\n\n\n\n\n\nThis exercise teaches you how to predict crop yields in Nepal by combining satellite data (Sentinel-2), climate data (WorldClim), and on-the-ground measurements. You’ll use a machine learning algorithm called XGBoost Regressor to build a model, split the data for training and testing, and fine-tune the model parameters for the best performance. This notebook lays the foundation for implementing TinyML in the agriculture domain. Consider how you could adapt this process for smaller datasets, fewer features, and simplified models to make it compatible with the power and memory constraints of TinyML devices.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#healthcare",
    "href": "contents/ai_for_good/ai_for_good.html#healthcare",
    "title": "19  AI for Good",
    "section": "19.3 Healthcare",
    "text": "19.3 Healthcare\n\n19.3.1 Expanding Access\nUniversal health coverage and quality care remain out of reach for millions worldwide. In many regions, more medical professionals are required to Access basic diagnosis and treatment. Additionally, healthcare infrastructure like clinics, hospitals, and utilities to power complex equipment needs to be improved. These gaps disproportionately impact marginalized communities, exacerbating health disparities.\nTinyML offers a promising technological solution to help expand Access to quality healthcare globally. TinyML refers to the ability to deploy machine learning algorithms on microcontrollers, tiny chips with processing power, memory, and connectivity. TinyML enables real-time data analysis and intelligence in low-powered, compact devices.\nThis creates opportunities for transformative medical tools that are portable, affordable, and accessible. TinyML software and hardware can be optimized to run even in resource-constrained environments. For example, a TinyML system could analyze symptoms or make diagnostic predictions using minimal computing power, no continuous internet connectivity, and a battery or solar power source. These capabilities can bring medical-grade screening and monitoring directly to underserved patients.\n\n\n19.3.2 Early Diagnosis\nEarly detection of diseases is one major application. Small sensors paired with TinyML software can identify symptoms before conditions escalate or visible signs appear. For instance, cough monitors with embedded machine learning can pick up on acoustic patterns indicative of respiratory illness, malaria, or tuberculosis. Detecting diseases at onset improves outcomes and reduces healthcare costs.\nA detailed example could be given for TinyML monitoring pneumonia in children. Pneumonia is a leading cause of death for children under 5, and detecting it early is critical. A startup called Respira xColabs has developed a low-cost wearable audio sensor that uses TinyML algorithms to analyze coughs and identify symptoms of respiratory illnesses like pneumonia. The device contains a microphone sensor and microcontroller that runs a neural network model trained to classify respiratory sounds. It can identify features like wheezing, crackling, and stridor that may indicate pneumonia. The device is designed to be highly accessible - it has a simple strap, requires no battery or charging, and results are provided through LED lights and audio cues.\nAnother example involves researchers at UNIFEI in Brazil who have developed a low-cost device that leverages TinyML to monitor heart rhythms. Their innovative solution addresses a critical need - atrial fibrillation and other heart rhythm abnormalities often go undiagnosed due to the prohibitive cost and limited availability of screening tools. The device overcomes these barriers through its ingenious design. It uses an off-the-shelf microcontroller that costs only a few dollars, along with a basic pulse sensor. By minimizing complexity, the device becomes accessible to under-resourced populations. The TinyML algorithm running locally on the microcontroller analyzes pulse data in real-time to detect irregular heart rhythms. This life-saving heart monitoring device demonstrates how TinyML enables powerful AI capabilities to be deployed in cost-effective, user-friendly designs.\nTinyML’s versatility also shows promise for tackling infectious diseases. Researchers have proposed applying TinyML to identify malaria-spreading mosquitoes by their wingbeat sounds. When equipped with microphones, small microcontrollers can run advanced audio classification models to determine mosquito species. This compact, low-power solution produces results in real time, suitable for remote field use. By making entomology analytics affordable and accessible, TinyML could revolutionize monitoring insects that endanger human health. TinyML is expanding healthcare access for vulnerable communities from heart disease to malaria.\n\n\n19.3.3 Infectious Disease Control\nMosquitoes remain the most deadly disease vector worldwide, transmitting illnesses that infect over one billion people annually («Vector-borne diseases», s.d.). Diseases like malaria, dengue, and Zika are especially prevalent in resource-limited regions lacking robust infrastructure for mosquito control. Monitoring local mosquito populations is essential to prevent outbreaks and properly target interventions.\n\n«Vector-borne diseases». s.d. https://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\nTraditional monitoring methods are expensive, labor-intensive, and difficult to deploy remotely. The proposed TinyML solution overcomes these barriers. Small microphones coupled with machine learning algorithms can classify mosquitoes by species based on minute differences in wing oscillations. The TinyML software runs efficiently on low-cost microcontrollers, eliminating the need for continuous connectivity.\nA collaborative research team from the University of Khartoum and the ICTP is exploring an innovative solution using TinyML. In a recent paper, they presented a low-cost device that can identify disease-spreading mosquito species through their wing beat sounds (Altayeb, Zennaro, e Rovai 2022).\n\nAltayeb, Moez, Marco Zennaro, e Marcelo Rovai. 2022. «Classifying mosquito wingbeat sound using TinyML». In Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 132–37. ACM. https://doi.org/10.1145/3524458.3547258.\nThis portable, self-contained system shows great promise for entomology. The researchers suggest it could revolutionize insect monitoring and vector control strategies in remote areas. TinyML could significantly bolster malaria eradication efforts by providing cheaper, easier mosquito analytics. Its versatility and minimal power needs make it ideal for field use in isolated, off-grid regions with scarce resources but high disease burden.\n\n\n19.3.4 TinyML Design Contest in Healthcare\nThe first TinyML contest in healthcare, TDC’22 (Jia et al. 2023), was held in 2022 to motivate participating teams to design AI/ML algorithms for detecting life-threatening ventricular arrhythmias (VAs) and deploy them on Implantable Cardioverter Defibrillators (ICDs). VAs are the main cause of sudden cardiac death (SCD). People at high risk of SCD rely on the ICD to deliver proper and timely defibrillation treatment (i.e., shocking the heart back into normal rhythm) when experiencing life-threatening VAs.\n\nJia, Zhenge, Dawei Li, Xiaowei Xu, Na Li, Feng Hong, Lichuan Ping, e Yiyu Shi. 2023. «Life-threatening ventricular arrhythmia detection challenge in implantable cardioverterdefibrillators». Nature Machine Intelligence 5 (5): 554–55. https://doi.org/10.1038/s42256-023-00659-9.\nAn on-device algorithm for early and timely life-threatening VA detection will increase the chances of survival. The proposed AI/ML algorithm needed to be deployed and executed on an extremely low-power and resource-constrained microcontroller (MCU) (a $10 development board with an ARM Cortex-M4 core at 80 MHz, 256 kB of flash memory and 64 kB of SRAM). The submitted designs were evaluated by metrics measured on the MCU for (1) detection performance, (2) inference latency, and (3) memory occupation by the program of AI/ML algorithms.\nThe champion, GaTech EIC Lab, obtained 0.972 in \\(F_\\beta\\) (F1 score with a higher weight to recall), 1.747 ms in latency, and 26.39 kB in memory footprint with a deep neural network. An ICD with an on-device VA detection algorithm was implanted in a clinical trial.\n\n\n\n\n\n\nEsercizio 19.2: Clinical Data: Unlocking Insights with Named Entity Recognition\n\n\n\n\n\nIn this exercise, you’ll learn about Named Entity Recognition (NER), a powerful tool for extracting valuable information from clinical text. Using Spark NLP, a specialized library for healthcare NLP, we’ll explore how NER models like BiLSTM-CNN-Char and BERT can automatically identify important medical entities such as diagnoses, medications, test results, and more. You’ll get hands-on experience applying these techniques with a special focus on oncology-related data extraction, helping you unlock insights about cancer types and treatment details from patient records.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#science",
    "href": "contents/ai_for_good/ai_for_good.html#science",
    "title": "19  AI for Good",
    "section": "19.4 Science",
    "text": "19.4 Science\nIn many scientific fields, researchers are limited by the quality and resolution of data they can collect. They often must indirectly infer the true parameters of interest using approximate correlations and models built on sparse data points. This constrains the accuracy of scientific understanding and predictions.\nThe emergence of TinyML opens new possibilities for gathering high-fidelity scientific measurements. With embedded machine learning, tiny, low-cost sensors can automatically process and analyze data locally in real-time. This creates intelligent sensor networks that capture nuanced data at much greater scales and frequencies.\nFor example, monitoring environmental conditions to model climate change remains challenging due to the need for widespread, continuous data. The Ribbit Project from UC Berkeley is pioneering a crowdsourced TinyML solution (Rao 2021). They developed an open-source CO2 sensor that uses an onboard microcontroller to process the gas measurements. An extensive dataset can be aggregated by distributing hundreds of these low-cost sensors. The TinyML devices compensate for environmental factors and provide previously impossible, granular, accurate readings.\n\nRao, Ravi. 2021. «TinyML unlocks new possibilities for sustainable development technologies». www.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\nThe potential to massively scale out intelligent sensing via TinyML has profound scientific implications. Higher-resolution data can lead to discoveries and predictive capabilities in fields ranging from ecology to cosmology. Other applications could include seismic sensors for earthquake early warning systems, distributed weather monitors to track microclimate changes, and acoustic sensors to study animal populations.\nAs sensors and algorithms continue improving, TinyML networks may generate more detailed maps of natural systems than ever before. Democratizing the collection of scientific data can accelerate research and understanding across disciplines. However, it raises new challenges around data quality, privacy, and modeling unknowns. TinyML signifies a growing convergence of AI and the natural sciences to answer fundamental questions.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#conservation-and-environment",
    "href": "contents/ai_for_good/ai_for_good.html#conservation-and-environment",
    "title": "19  AI for Good",
    "section": "19.5 Conservation and Environment",
    "text": "19.5 Conservation and Environment\nTinyML is emerging as a powerful tool for environmental conservation and sustainability efforts. Recent research has highlighted numerous applications of tiny machine learning in domains such as wildlife monitoring, natural resource management, and tracking climate change.\nOne example is using TinyML for real-time wildlife tracking and protection. Researchers have developed Smart Wildlife Tracker devices that leverage TinyML algorithms to detect poaching activities. The collars contain sensors like cameras, microphones, and GPS to monitor the surrounding environment continuously. Embedded machine learning models analyze the audio and visual data to identify threats like nearby humans or gunshots. Early poaching detection gives wildlife rangers critical information to intervene and take action.\nOther projects apply TinyML to study animal behavior through sensors. The smart wildlife collar uses accelerometers and acoustic monitoring to track elephant movements, communication, and moods (Verma 2022). The low-power TinyML collar devices transmit rich data on elephant activities while avoiding burdensome Battery changes. This helps researchers unobtrusively observe elephant populations to inform conservation strategies.\n\nVerma, Team Dual_Boot: Swapnil. 2022. «Elephant AI». Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\nOn a broader scale, distributed TinyML devices are envisioned to create dense sensor networks for environmental modeling. Hundreds of low-cost air quality monitors could map pollution across cities. Underwater sensors may detect toxins and give early warning of algal blooms. Such applications underscore TinyML’s versatility in ecology, climatology, and sustainability.\nResearchers from Moulay Ismail University of Meknes in Morocco (Bamoumen et al. 2022) have published a survey on how TinyML can be used to solve environmental issues. However, thoughtfully assessing benefits, risks, and equitable Access will be vital as TinyML expands environmental research and conservation. With ethical consideration of impacts, TinyML offers data-driven solutions to protect biodiversity, natural resources, and our planet.\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, e Yousra Chtouki. 2022. «How TinyML Can be Leveraged to Solve Environmental Problems: A Survey». In 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), 338–43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#disaster-response",
    "href": "contents/ai_for_good/ai_for_good.html#disaster-response",
    "title": "19  AI for Good",
    "section": "19.6 Disaster Response",
    "text": "19.6 Disaster Response\nIn disaster response, speed and safety are paramount. But rubble and wreckage create hazardous, confined environments that impede human search efforts. TinyML enables nimble drones to assist rescue teams in these dangerous scenarios.\nWhen buildings collapse after earthquakes, small drones can prove invaluable. Equipped with TinyML navigation algorithms, micro-sized drones like the CrazyFlie can traverse cramped voids and map pathways beyond human reach (Bardienus P. Duisterhof et al. 2019). Obstacle avoidance allows the drones to weave through unstable debris. This autonomous mobility lets them rapidly sweep areas humans cannot access. Video 19.1 presents the (Bardienus P. Duisterhof et al. 2019) paper on deep reinforcement learning using drones for source-seeking.\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R Banbury, William Fu, Aleksandra Faust, Guido CHE de Croon, e Vijay Janapa Reddi. 2019. «Learning to seek: Autonomous source seeking with deep reinforcement learning onboard a nano drone microcontroller». ArXiv preprint abs/1909.11236. https://arxiv.org/abs/1909.11236.\n\n\n\n\n\n\nVideo 19.1: Learning to Seek\n\n\n\n\n\n\nCrucially, onboard sensors and TinyML processors analyze real-time data to identify signs of survivors. Thermal cameras detect body heat, microphones pick up calls for help, and gas sensors warn of leaks (Bardienus P. Duisterhof et al. 2021). Processing data locally using TinyML allows for quick interpretation to guide rescue efforts. As conditions evolve, the drones can adapt by adjusting their search patterns and priorities. Video 19.2 is an overview of autonomous drones for gas leak detection.\n\n\n\n\n\n\nVideo 19.2\n\n\n\n\n\n\nAdditionally, coordinated swarms of drones unlock new capabilities. By collaborating and sharing insights, drone teams comprehensively view the situation. Blanketing disaster sites allows TinyML algorithms to fuse and analyze data from multiple vantage points, amplifying situational awareness beyond individual drones (Bardienus P. Duisterhof et al. 2021).\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa Reddi, e Guido C. H. E. de Croon. 2021. «Sniffy Bug: A Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments». In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 9099–9106. IEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\nMost importantly, initial drone reconnaissance enhances safety for human responders. Keeping rescue teams at a safe distance until drone surveys assess hazards saves lives. Once secured, drones can guide precise personnel placement.\nBy combining agile mobility, real-time data, and swarm coordination, TinyML-enabled drones promise to transform disaster response. Their versatility, speed, and safety make them a vital asset for rescue efforts in dangerous, inaccessible environments. Integrating autonomous drones with traditional methods can accelerate responses when it matters most.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#education-and-outreach",
    "href": "contents/ai_for_good/ai_for_good.html#education-and-outreach",
    "title": "19  AI for Good",
    "section": "19.7 Education and Outreach",
    "text": "19.7 Education and Outreach\nTinyML holds immense potential to help address challenges in developing regions, but realizing its benefits requires focused education and capacity building. Recognizing this need, academic researchers have spearheaded outreach initiatives to spread TinyML education globally.\nIn 2020, Harvard University, Columbia University, the International Centre for Theoretical Physics (ICTP), and UNIFEI jointly founded the TinyML for Developing Communities (TinyML4D) network (Zennaro, Plancher, e Reddi 2022). This network empowers universities and researchers in developing countries to harness TinyML for local impact.\n\nZennaro, Marco, Brian Plancher, e V Janapa Reddi. 2022. «TinyML: Applied AI for development». In The UN 7th Multi-stakeholder Forum on Science, Technology and Innovation for the Sustainable Development Goals, 2022–05.\nA core focus is expanding Access to applied machine learning education. The TinyML4D network provides training, curricula, and lab resources to members. Hands-on workshops and data collection projects give students practical experience. Members can share best practices and build a community through conferences and academic collaborations.\nThe network prioritizes enabling locally relevant TinyML solutions. Projects address challenges like agriculture, health, and environmental monitoring based on community needs. For example, a member university in Rwanda developed a low-cost flood monitoring system using TinyML and sensors.\nTinyML4D includes over 50 member institutions across Africa, Asia, and Latin America. However, greater investments and industry partnerships are needed to reach all underserved regions. The ultimate vision is training new generations to ethically apply TinyML for sustainable development. Outreach efforts today lay the foundation for democratizing transformative technology for the future.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#accessibility",
    "href": "contents/ai_for_good/ai_for_good.html#accessibility",
    "title": "19  AI for Good",
    "section": "19.8 Accessibility",
    "text": "19.8 Accessibility\nTechnology has immense potential to break down barriers faced by people with disabilities and bridge gaps in accessibility. TinyML specifically opens new possibilities for developing intelligent, personalized assistive devices.\nWith machine learning algorithms running locally on microcontrollers, compact accessibility tools can operate in real time without reliance on connectivity. The National Institute on Deafness and Other Communication Disorders (NIDCD) states that 20% of the world’s population has some form of hearing loss. Hearing aids leveraging TinyML could recognize multiple speakers and amplify the voice of a chosen target in crowded rooms. This allows people with hearing impairments to focus on specific conversations.\nSimilarly, mobility devices could use on-device vision processing to identify obstacles and terrain characteristics. This enables enhanced navigation and safety for the visually impaired. Companies like Envision are developing smart glasses, converting visual information into speech, with embedded TinyML to guide blind people by detecting objects, text, and traffic signals. Video 19.3 below shows the different real-life use cases of the Envision visual aid glasses.\n\n\n\n\n\n\nVideo 19.3\n\n\n\n\n\n\nTinyML could even power responsive prosthetic limbs. By analyzing nerve signals and sensory data like muscle tension, prosthetics and exoskeletons with embedded ML can move and adjust grip dynamically, making control more natural and intuitive. Companies are creating affordable, everyday bionic hands using TinyML. For those with speech difficulties, voice-enabled devices with TinyML can generate personalized vocal outputs from non-verbal inputs. Pairs by Anthropic translates gestures into natural speech tailored for individual users.\nBy enabling more customizable assistive tech, TinyML makes services more accessible and tailored to individual needs. And through translation and interpretation applications, TinyML can break down communication barriers. Apps like Microsoft Translator offer real-time translation powered by TinyML algorithms.\nWith its thoughtful and inclusive design, TinyML promises more autonomy and dignity for people with disabilities. However, developers should engage communities directly, avoid compromising privacy, and consider affordability to maximize the benefits. TinyML has huge potential to contribute to a more just, equitable world.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#infrastructure-and-urban-planning",
    "href": "contents/ai_for_good/ai_for_good.html#infrastructure-and-urban-planning",
    "title": "19  AI for Good",
    "section": "19.9 Infrastructure and Urban Planning",
    "text": "19.9 Infrastructure and Urban Planning\nAs urban populations swell, cities face immense challenges in efficiently managing resources and infrastructure. TinyML presents a powerful tool for developing intelligent systems to optimize city operations and sustainability. It could revolutionize energy efficiency in smart buildings.\nMachine learning models can learn to predict and regulate energy usage based on occupancy patterns. Miniaturized sensors placed throughout buildings can provide granular, real-time data on space utilization, temperature, and more (Seyedzadeh et al. 2018). This visibility allows TinyML systems to minimize waste by optimizing heating, cooling, lighting, etc.\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, e Marc Roper. 2018. «Machine learning for estimation of building energy consumption and performance: A review». Visualization in Engineering 6 (1): 1–20. https://doi.org/10.1186/s40327-018-0064-7.\nThese examples demonstrate TinyML’s huge potential for efficient, sustainable city infrastructure. However, urban planners must consider privacy, security, and accessibility to ensure responsible adoption. With careful implementation, TinyML could profoundly modernize urban life.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#challenges-and-considerations",
    "href": "contents/ai_for_good/ai_for_good.html#challenges-and-considerations",
    "title": "19  AI for Good",
    "section": "19.10 Challenges and Considerations",
    "text": "19.10 Challenges and Considerations\nWhile TinyML presents immense opportunities, thoughtful consideration of challenges and ethical implications will be critical as adoption spreads globally. Researchers have highlighted key factors to address, especially when deploying TinyML in developing regions.\nA foremost challenge is limited Access to training and hardware (Ooko et al. 2021). Only educational programs exist tailored to TinyML, and emerging economies often need a robust electronics supply chain. Thorough training and partnerships will be needed to nurture expertise and make devices available to underserved communities. Initiatives like the TinyML4D network help provide structured learning pathways.\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, e Marco Zennaro. 2021. «TinyML in Africa: Opportunities and Challenges». In 2021 IEEE Globecom Workshops (GC Wkshps), 1–6. IEEE; IEEE. https://doi.org/10.1109/gcwkshps52748.2021.9682107.\nData limitations also pose hurdles. TinyML models require quality localized datasets, which are scarce in under-resourced environments. Creating frameworks to crowdsource data ethically could address this. However, data collection should benefit local communities directly, not just extract value.\nOptimizing power usage and connectivity will be vital for sustainability. TinyML’s low power needs make it ideal for off-grid use cases. Integrating battery or solar can enable continuous operation. Adapting devices for low-bandwidth transmission where the internet is limited also maximizes impact.\nCultural and language barriers further complicate adoption. User interfaces and devices should account for all literacy levels and avoid excluding subgroups. Voice-controllable solutions in local dialects can improve accessibility.\nAddressing these challenges requires holistic partnerships, funding, and policy support. However, inclusively and ethically scaling TinyML has monumental potential to uplift disadvantaged populations worldwide. With thoughtful implementation, the technology could profoundly democratize opportunity.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#conclusion",
    "href": "contents/ai_for_good/ai_for_good.html#conclusion",
    "title": "19  AI for Good",
    "section": "19.11 Conclusion",
    "text": "19.11 Conclusion\nTinyML presents a tremendous opportunity to harness the power of artificial intelligence to advance the UN Sustainable Development Goals and drive social impact globally, as highlighted by examples across sectors like healthcare, agriculture, conservation, and more; embedded machine learning unlocks new capabilities for low-cost, accessible solutions tailored to local contexts. TinyML circumvents barriers like poor infrastructure, limited connectivity, and high costs that often exclude developing communities from emerging technology.\nHowever, realizing TinyML’s full potential requires holistic collaboration. Researchers, policymakers, companies, and local stakeholders must collaborate to provide training, establish ethical frameworks, co-design solutions, and adapt them to community needs. Through inclusive development and deployment, TinyML can deliver on its promise to bridge inequities and uplift vulnerable populations without leaving any behind.\nIf cultivated responsibly, TinyML could democratize opportunity and accelerate progress on global priorities from poverty alleviation to climate resilience. The technology represents a new wave of applied AI to empower societies, promote sustainability, and propel humanity toward greater justice, prosperity, and peace. TinyML provides a glimpse into an AI-enabled future that is accessible to all.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#sec-ai-for-good-resource",
    "href": "contents/ai_for_good/ai_for_good.html#sec-ai-for-good-resource",
    "title": "19  AI for Good",
    "section": "19.12 Resources",
    "text": "19.12 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will be adding new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nTinyML for Social Impact.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 19.1\nVideo 19.2\nVideo 19.3\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\n\nEsercizio 19.1\nEsercizio 19.2\n\n\n\n\n\n\n\n\n\n\nLabs\n\n\n\n\n\nIn addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.\n\nComing soon.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html",
    "href": "contents/conclusion/conclusion.html",
    "title": "20  Conclusion",
    "section": "",
    "text": "20.1 Introduction\nThis book examines the rapidly evolving field of ML systems (Capitolo 2). We focus on systems because while there are many resources on ML models and algorithms, more needs to be understood about how to build the systems that run them.\nTo draw an analogy, consider the process of building a car. While many resources are available on the various components of a car, such as the engine, transmission, and suspension, there is often a need for more understanding about how to assemble these components into a functional vehicle. Just as a car requires a well-designed and properly integrated system to operate efficiently and reliably, ML models also require a robust and carefully constructed system to deliver their full potential. Moreover, there is a lot of nuance in building ML systems, given their specific use case. For example, a Formula 1 race car must be assembled differently from an everyday Prius consumer car.\nOur journey started by tracing ML’s historical trajectory, from its theoretical foundations to its current state as a transformative force across industries (Capitolo 3). This journey has highlighted the remarkable progress in the field, challenges, and opportunities.\nThroughout this book, we have looked into the intricacies of ML systems, examining the critical components and best practices necessary to create a seamless and efficient pipeline. From data preprocessing and model training to deployment and monitoring, we have provided insights and guidance to help readers navigate the complex landscape of ML system development.\nML systems involve complex workflows, spanning various topics from data engineering to model deployment on diverse systems (Capitolo 4). By providing an overview of these ML system components, we have aimed to showcase the tremendous depth and breadth of the field and expertise that is needed. Understanding the intricacies of ML workflows is crucial for practitioners and researchers alike, as it enables them to navigate the landscape effectively and develop robust, efficient, and impactful ML solutions.\nBy focusing on the systems aspect of ML, we aim to bridge the gap between theoretical knowledge and practical implementation. Just as a healthy human body system allows the organs to function optimally, a well-designed ML system enables the models to consistently deliver accurate and reliable results. This book’s goal is to empower readers with the knowledge and tools necessary to build ML systems that showcase the underlying models’ power and ensure smooth integration and operation, much like a well-functioning human body.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#knowing-the-importance-of-ml-datasets",
    "href": "contents/conclusion/conclusion.html#knowing-the-importance-of-ml-datasets",
    "title": "20  Conclusion",
    "section": "20.2 Knowing the Importance of ML Datasets",
    "text": "20.2 Knowing the Importance of ML Datasets\nOne of the key things we have emphasized is that data is the foundation upon which ML systems are built (Capitolo 5). Data is the new code that programs deep neural networks, making data engineering the first and most critical stage of any ML pipeline. That is why we began our exploration by diving into the basics of data engineering, recognizing that quality, diversity, and ethical sourcing are key to building robust and reliable machine learning models.\nThe importance of high-quality data must be balanced. Lapses in data quality can lead to significant negative consequences, such as flawed predictions, project terminations, and even potential harm to communities. These cascading effects, often called “Data Cascades,” highlight the need for diligent data management and governance practices. ML practitioners must prioritize data quality, ensure diversity and representativeness, and adhere to ethical data collection and usage standards. By doing so, we can mitigate the risks associated with poor data quality and build ML systems that are trustworthy, reliable, and beneficial to society.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#navigating-the-ai-framework-landscape",
    "href": "contents/conclusion/conclusion.html#navigating-the-ai-framework-landscape",
    "title": "20  Conclusion",
    "section": "20.3 Navigating the AI Framework Landscape",
    "text": "20.3 Navigating the AI Framework Landscape\nThere are many different ML frameworks. Therefore, we dove into the evolution of different ML frameworks, dissecting the inner workings of popular ones like TensorFlow and PyTorch, and provided insights into the core components and advanced features that define them (Capitolo 6). We also looked into the specialization of frameworks tailored to specific needs, such as those designed for embedded AI. We discussed the criteria for selecting the most suitable framework for a given project.\nOur exploration also touched upon the future trends expected to shape the landscape of ML frameworks in the coming years. As the field continues to evolve, we can anticipate the emergence of more specialized and optimized frameworks that cater to the unique requirements of different domains and deployment scenarios, as we saw with TensorFlow Lite for Microcontrollers. By staying abreast of these developments and understanding the tradeoffs involved in framework selection, we can make informed decisions and leverage the most appropriate tools to build efficient ML systems.\nMoreover, we expect to see a growing emphasis on framework interoperability and standardization efforts, such as the ONNX (Open Neural Network Exchange) format. This format allows models to be trained in one framework and deployed in another, facilitating greater collaboration and portability across different platforms and environments.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#understanding-ml-training-fundamentals",
    "href": "contents/conclusion/conclusion.html#understanding-ml-training-fundamentals",
    "title": "20  Conclusion",
    "section": "20.4 Understanding ML Training Fundamentals",
    "text": "20.4 Understanding ML Training Fundamentals\nAs ML practitioners who build ML systems, it is crucial to deeply understand the AI training process and the system challenges in scaling and optimizing it. By leveraging the capabilities of modern AI frameworks and staying up-to-date with the latest advancements in training techniques, we can build robust, efficient, and scalable ML systems that can tackle real-world problems and drive innovation across various domains.\nWe began by examining the fundamentals of AI training (Capitolo 7), which involves feeding data into ML models and adjusting their parameters to minimize the difference between predicted and actual outputs. This process is computationally intensive and requires careful consideration of various factors, such as the choice of optimization algorithms, learning rate, batch size, and regularization techniques. Understanding these concepts is crucial for developing effective and efficient training pipelines.\nHowever, training ML models at scale poses significant system challenges. As datasets’ size and models’ complexity grow, the computational resources required for training can become prohibitively expensive. This has led to the development of distributed training techniques, such as data and model parallelism, which allow multiple devices to collaborate in the training process. Frameworks like TensorFlow and PyTorch have evolved to support these distributed training paradigms, enabling practitioners to scale their training workloads across clusters of GPUs or TPUs.\nIn addition to distributed training, we discussed techniques for optimizing the training process, such as mixed-precision training and gradient compression. It’s important to note that while these techniques may seem algorithmic, they significantly impact system performance. The choice of training algorithms, precision, and communication strategies directly affects the ML system’s resource utilization, scalability, and efficiency. Therefore, adopting an algorithm-hardware or algorithm-system co-design approach is crucial, where the algorithmic choices are made in tandem with the system considerations. By understanding the interplay between algorithms and hardware, we can make informed decisions that optimize the model performance and the system efficiency, ultimately leading to more effective and scalable ML solutions.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#pursuing-efficiency-in-ai-systems",
    "href": "contents/conclusion/conclusion.html#pursuing-efficiency-in-ai-systems",
    "title": "20  Conclusion",
    "section": "20.5 Pursuing Efficiency in AI Systems",
    "text": "20.5 Pursuing Efficiency in AI Systems\nDeploying trained ML models is more complex than simply running the networks; efficiency is critical (Capitolo 8). In this chapter on AI efficiency, we emphasized that efficiency is not merely a luxury but a necessity in artificial intelligence systems. We dug into the key concepts underpinning AI systems’ efficiency, recognizing that the computational demands on neural networks can be daunting, even for minimal systems. For AI to be seamlessly integrated into everyday devices and essential systems, it must perform optimally within the constraints of limited resources while maintaining its efficacy.\nThroughout the book, we have highlighted the importance of pursuing efficiency to ensure that AI models are streamlined, rapid, and sustainable. By optimizing models for efficiency, we can widen their applicability across various platforms and scenarios, enabling AI to be deployed in resource-constrained environments such as embedded systems and edge devices. This pursuit of efficiency is crucial for the widespread adoption and practical implementation of AI technologies in real-world applications.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#optimizing-ml-model-architectures",
    "href": "contents/conclusion/conclusion.html#optimizing-ml-model-architectures",
    "title": "20  Conclusion",
    "section": "20.6 Optimizing ML Model Architectures",
    "text": "20.6 Optimizing ML Model Architectures\nWe then explored various model architectures, from the foundational perceptron to the sophisticated transformer networks, each tailored to specific tasks and data types. This exploration has showcased machine learning models’ remarkable diversity and adaptability, enabling them to tackle various problems across domains.\nHowever, when deploying these models on systems, especially resource-constrained embedded systems, model optimization becomes a necessity. The evolution of model architectures, from the early MobileNets designed for mobile devices to the more recent TinyML models optimized for microcontrollers, is a testament to the continued innovation.\nIn the chapter on model optimization (Capitolo 9), we looked into the art and science of optimizing machine learning models to ensure they are lightweight, efficient, and effective when deployed in TinyML scenarios. We explored techniques such as model compression, quantization, and architecture search, which allow us to reduce the computational footprint of models while maintaining their performance. By applying these optimization techniques, we can create models tailored to the specific constraints of embedded systems, enabling the deployment of powerful AI capabilities on edge devices. This opens many possibilities for intelligent, real-time processing and decision-making in IoT, robotics, and mobile computing applications. As we continue pushing the boundaries of AI efficiency, we expect to see even more innovative solutions for deploying machine learning models in resource-constrained environments.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#advancing-ai-processing-hardware",
    "href": "contents/conclusion/conclusion.html#advancing-ai-processing-hardware",
    "title": "20  Conclusion",
    "section": "20.7 Advancing AI Processing Hardware",
    "text": "20.7 Advancing AI Processing Hardware\nOver the years, we have witnessed remarkable strides in ML hardware, driven by the insatiable demand for computational power and the need to address the challenges of resource constraints in real-world deployments (Capitolo 10). These advancements have been crucial in enabling the deployment of powerful AI capabilities on devices with limited resources, opening up new possibilities across various industries.\nSpecialized hardware acceleration is essential to overcome these constraints and enable high-performance machine learning. Hardware accelerators, such as GPUs, FPGAs, and ASICs, optimize compute-intensive operations, particularly inference, by leveraging custom silicon designed for efficient matrix multiplications. These accelerators provide substantial speedups compared to general-purpose CPUs, enabling real-time execution of advanced ML models on devices with strict size, weight, and power limitations.\nWe have also explored the various techniques and approaches for hardware acceleration in embedded machine-learning systems. We discussed the tradeoffs in selecting the appropriate hardware for specific use cases and the importance of software optimizations to harness these accelerators’ capabilities fully. By understanding these concepts, ML practitioners can make informed decisions when designing and deploying ML systems.\nGiven the plethora of ML hardware solutions available, benchmarking has become essential to developing and deploying machine learning systems (Capitolo 11). Benchmarking allows developers to measure and compare the performance of different hardware platforms, model architectures, training procedures, and deployment strategies. By utilizing well-established benchmarks like MLPerf, practitioners gain valuable insights into the most effective approaches for a given problem, considering the unique constraints of the target deployment environment.\nAdvancements in ML hardware, combined with insights gained from benchmarking and optimization techniques, have paved the way for successfully deploying machine learning capabilities on various devices, from powerful edge servers to resource-constrained microcontrollers. As the field continues to evolve, we expect to see even more innovative hardware solutions and benchmarking approaches that will further push the boundaries of what is possible with embedded machine learning systems.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#embracing-on-device-learning",
    "href": "contents/conclusion/conclusion.html#embracing-on-device-learning",
    "title": "20  Conclusion",
    "section": "20.8 Embracing On-Device Learning",
    "text": "20.8 Embracing On-Device Learning\nIn addition to the advancements in ML hardware, we also explored on-device learning, where models can adapt and learn directly on the device (Capitolo 12). This approach has significant implications for data privacy and security, as sensitive information can be processed locally without the need for transmission to external servers.\nOn-device learning enhances privacy by keeping data within the confines of the device, reducing the risk of unauthorized access or data breaches. It also reduces reliance on cloud connectivity, enabling ML models to function effectively even in scenarios with limited or intermittent internet access. We have discussed techniques such as transfer learning and federated learning, which have expanded the capabilities of on-device learning. Transfer learning allows models to leverage knowledge gained from one task or domain to improve performance on another, enabling more efficient and effective learning on resource-constrained devices. On the other hand, Federated learning enables collaborative model updates across distributed devices without centralized data aggregation. This approach allows multiple devices to contribute to learning while keeping their data locally, enhancing privacy and security.\nThese advancements in on-device learning have paved the way for more secure, privacy-preserving, and decentralized machine learning applications. As we prioritize data privacy and security in developing ML systems, we expect to see more innovative solutions that enable powerful AI capabilities while protecting sensitive information and ensuring user privacy.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#streamlining-ml-operations",
    "href": "contents/conclusion/conclusion.html#streamlining-ml-operations",
    "title": "20  Conclusion",
    "section": "20.9 Streamlining ML Operations",
    "text": "20.9 Streamlining ML Operations\nEven if we got the above pieces right, challenges and considerations must be addressed to ensure ML models’ successful integration and operation in production environments. In the ML Ops chapter (Capitolo 13), we studied the practices and architectures necessary to develop, deploy, and manage ML models throughout their entire lifecycle. We looked at the phases of ML, from data collection and model training to evaluation, deployment, and ongoing monitoring.\nWe learned about the importance of automation, collaboration, and continuous improvement in ML Ops. By automating key processes, teams can streamline their workflows, reduce manual errors, and accelerate the deployment of ML models. Collaboration among diverse teams, including data scientists, engineers, and domain experts, ensures ML systems’ successful development and deployment.\nThe ultimate goal of this chapter was to provide readers with a comprehensive understanding of ML model management, equipping them with the knowledge and tools necessary to build and run ML applications that deliver sustained value successfully. By adopting best practices in ML Ops, organizations can ensure their ML initiatives’ long-term success and impact, driving innovation and delivering meaningful results.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#ensuring-security-and-privacy",
    "href": "contents/conclusion/conclusion.html#ensuring-security-and-privacy",
    "title": "20  Conclusion",
    "section": "20.10 Ensuring Security and Privacy",
    "text": "20.10 Ensuring Security and Privacy\nNo ML system is ever complete without thinking about security and privacy. They are of major importance when developing real-world ML systems. As machine learning finds increasing application in sensitive domains such as healthcare, finance, and personal data, safeguarding confidentiality and preventing the misuse of data and models becomes a critical imperative, and these were the concepts we discussed previously (Capitolo 14).\nTo build robust and responsible ML systems, practitioners must thoroughly understand the potential security and privacy risks. These risks include data leaks, which can expose sensitive information; model theft, where malicious actors steal trained models; adversarial attacks that can manipulate model behavior; bias in models that can lead to unfair or discriminatory outcomes; and unintended access to private information.\nMitigating these risks requires a deep understanding of best practices in security and privacy. Therefore, we have emphasized that security and privacy cannot be an afterthought—they must be proactively addressed at every stage of the ML system development lifecycle. From the initial stages of data collection and labeling, it is crucial to ensure that data is handled securely and that privacy is protected. During model training and evaluation, techniques such as differential privacy and secure multi-party computation can be employed to safeguard sensitive information.\nWhen deploying ML models, robust access controls, encryption, and monitoring mechanisms must be implemented to prevent unauthorized access and detect potential security breaches. Ongoing monitoring and auditing of ML systems as part of MLOps are also essential to identify and address emerging security or privacy vulnerabilities.\nBy embedding security and privacy considerations into each stage of building, deploying, and managing ML systems, we can safely unlock the benefits of AI while protecting individuals’ rights and ensuring the responsible use of these powerful technologies. Only through this proactive and comprehensive approach can we build ML systems that are not only technologically advanced but also ethically sound and worthy of public trust.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#upholding-ethical-considerations",
    "href": "contents/conclusion/conclusion.html#upholding-ethical-considerations",
    "title": "20  Conclusion",
    "section": "20.11 Upholding Ethical Considerations",
    "text": "20.11 Upholding Ethical Considerations\nAs we embrace ML advancements in all facets of our lives, it is crucial to remain mindful of the ethical considerations that will shape the future of AI (Capitolo 15). Fairness, transparency, accountability, and privacy in AI systems will be paramount as they become more integrated into our lives and decision-making processes.\nAs AI systems become more pervasive and influential, it is essential to ensure that they are designed and deployed in a manner that upholds ethical principles. This means actively mitigating biases, promoting fairness, and preventing discriminatory outcomes. It also ensures transparency in how AI systems make decisions, enabling users to understand and trust their outputs.\nAccountability is another critical ethical consideration. As AI systems take on more responsibilities and make decisions that impact individuals and society, there must be clear mechanisms for holding these systems and their creators accountable. This includes establishing frameworks for auditing and monitoring AI systems and defining liability and redress mechanisms in case of harm or unintended consequences.\nEthical frameworks, regulations, and standards will be essential to address these ethical challenges. These frameworks should guide the responsible development and deployment of AI technologies, ensuring that they align with societal values and promote the well-being of individuals and communities.\nMoreover, ongoing discussions and collaborations among researchers, practitioners, policymakers, and society will be crucial in navigating the ethical landscape of AI. These conversations should be inclusive and diverse, bringing together different perspectives and expertise to develop comprehensive and equitable solutions. As we move forward, it is the collective responsibility of all stakeholders to prioritize ethical considerations in the development and deployment of AI systems.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#promoting-sustainability-and-equity",
    "href": "contents/conclusion/conclusion.html#promoting-sustainability-and-equity",
    "title": "20  Conclusion",
    "section": "20.12 Promoting Sustainability and Equity",
    "text": "20.12 Promoting Sustainability and Equity\nThe increasing computational demands of machine learning, particularly for training large models, have raised concerns about their environmental impact due to high energy consumption and carbon emissions (Capitolo 16). As the scale and complexity of models continue to grow, addressing the sustainability challenges associated with AI development becomes imperative. To mitigate the environmental footprint of AI, the development of energy-efficient algorithms is crucial. This involves optimizing models and training procedures to minimize computational requirements while maintaining performance. Techniques such as model compression, quantization, and efficient neural architecture search can help reduce the energy consumption of AI systems.\nUsing renewable energy sources to power AI infrastructure is another important step towards sustainability. By transitioning to clean energy sources such as solar, wind, and hydropower, the carbon emissions associated with AI development can be significantly reduced. This requires a concerted effort from the AI community and support from policymakers and industry leaders to invest in and adopt renewable energy solutions. In addition, exploring alternative computing paradigms, such as neuromorphic and photonic computing, holds promise for developing more energy-efficient AI systems. By developing hardware and algorithms that emulate the brain’s processing mechanisms, we can potentially create AI systems that are both powerful and sustainable.\nThe AI community must prioritize sustainability as a key consideration in research and development. This involves investing in green computing initiatives, such as developing energy-efficient hardware and optimizing data centers for reduced energy consumption. It also requires collaboration across disciplines, bringing together AI, energy, and sustainability experts to develop holistic solutions.\nMoreover, it is important to acknowledge that access to AI and machine learning compute resources may not be equally distributed across organizations and regions. This disparity can lead to a widening gap between those who have the means to leverage advanced AI technologies and those who do not. Organizations like the Organisation for Economic Cooperation and Development (OECD) are actively exploring ways to address this issue and promote greater equity in AI access and adoption. By fostering international cooperation, sharing best practices, and supporting capacity-building initiatives, we can ensure that AI’s benefits are more widely accessible and that no one is left behind in the AI revolution.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#enhancing-robustness-and-resiliency",
    "href": "contents/conclusion/conclusion.html#enhancing-robustness-and-resiliency",
    "title": "20  Conclusion",
    "section": "20.13 Enhancing Robustness and Resiliency",
    "text": "20.13 Enhancing Robustness and Resiliency\nThe chapter on Robust AI dives into the fundamental concepts, techniques, and tools for building fault-tolerant and error-resilient ML systems (Capitolo 17). In that chapter, we explored how robust AI techniques can address the challenges posed by various types of hardware faults, including transient, permanent, and intermittent faults, as well as software issues such as bugs, design flaws, and implementation errors.\nBy employing robust AI techniques, ML systems can maintain their reliability, safety, and performance even in adverse conditions. These techniques enable systems to detect and recover from faults, adapt to changing environments, and make decisions under uncertainty.\nThe chapter empowers researchers and practitioners to develop AI solutions that can withstand the complexities and uncertainties of real-world environments. It provides insights into the design principles, architectures, and algorithms underpinning robust AI systems and practical guidance on implementing and validating these systems.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#shaping-the-future-of-ml-systems",
    "href": "contents/conclusion/conclusion.html#shaping-the-future-of-ml-systems",
    "title": "20  Conclusion",
    "section": "20.14 Shaping the Future of ML Systems",
    "text": "20.14 Shaping the Future of ML Systems\nAs we look to the future, the trajectory of ML systems points towards a paradigm shift from a model-centric approach to a more data-centric one. This shift recognizes that the quality and diversity of data are paramount to developing robust, reliable, and fair AI models.\nWe anticipate a growing emphasis on data curation, labeling, and augmentation techniques in the coming years. These practices aim to ensure that models are trained on high-quality, representative data that accurately reflects the complexities and nuances of real-world scenarios. By focusing on data quality and diversity, we can mitigate the risks of biased or skewed models that may perpetuate unfair or discriminatory outcomes.\nThis data-centric approach will be crucial in addressing the challenges of bias, fairness, and generalizability in ML systems. By actively seeking out and incorporating diverse and inclusive datasets, we can develop more robust, equitable, and applicable models for various contexts and populations. Moreover, the emphasis on data will drive advancements in techniques such as data augmentation, where existing datasets are expanded and diversified through data synthesis, translation, and generation. These techniques can help overcome the limitations of small or imbalanced datasets, enabling the development of more accurate and generalizable models.\nIn recent years, generative AI has taken the field by storm, demonstrating remarkable capabilities in creating realistic images, videos, and text. However, the rise of generative AI also brings new challenges for ML systems (Capitolo 18). Unlike traditional ML systems, generative models often demand more computational resources and pose challenges in terms of scalability and efficiency. Furthermore, evaluating and benchmarking generative models presents difficulties, as traditional metrics used for classification tasks may not be directly applicable. Developing robust evaluation frameworks for generative models is an active area of research.\nUnderstanding and addressing these system challenges and ethical considerations will be crucial in shaping the future of generative AI and its impact on society. As ML practitioners and researchers, we are responsible for advancing the technical capabilities of generative models and developing robust systems and frameworks that can mitigate potential risks and ensure the beneficial application of this powerful technology.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#applying-ai-for-good",
    "href": "contents/conclusion/conclusion.html#applying-ai-for-good",
    "title": "20  Conclusion",
    "section": "20.15 Applying AI for Good",
    "text": "20.15 Applying AI for Good\nThe potential for AI to be used for social good is vast, provided that responsible ML systems are developed and deployed at scale across various use cases (Capitolo 19). To realize this potential, it is essential for researchers and practitioners to actively engage in the process of learning, experimentation, and pushing the boundaries of what is possible.\nThroughout the development of ML systems, it is crucial to remember the key themes and lessons explored in this book. These include the importance of data quality and diversity, the pursuit of efficiency and robustness, the potential of TinyML and neuromorphic computing, and the imperative of security and privacy. These insights inform the work and guide the decisions of those involved in developing AI systems.\nIt is important to recognize that the development of AI is not solely a technical endeavor but also a deeply human one. It requires collaboration, empathy, and a commitment to understanding the societal implications of the systems being created. Engaging with experts from diverse fields, such as ethics, social sciences, and policy, is essential to ensure that the AI systems developed are technically sound, socially responsible, and beneficial. Embracing the opportunity to be part of this transformative field and shaping its future is a privilege and a responsibility. By working together, we can create a world where ML systems serve as tools for positive change and improving the human condition.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#congratulations",
    "href": "contents/conclusion/conclusion.html#congratulations",
    "title": "20  Conclusion",
    "section": "20.16 Congratulations",
    "text": "20.16 Congratulations\nCongratulations on coming this far, and best of luck in your future endeavors! The future of AI is bright and filled with endless possibilities, and I can’t wait to see the incredible contributions you will make.\nFeel free to reach out to me anytime at vj at eecs dot harvard dot edu.\n– Prof. Vijay Janapa Reddi, Harvard University",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/labs/labs.html",
    "href": "contents/labs/labs.html",
    "title": "Overview",
    "section": "",
    "text": "Learning Objectives\nBy completing these labs, we hope learners will:",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#learning-objectives",
    "href": "contents/labs/labs.html#learning-objectives",
    "title": "Overview",
    "section": "",
    "text": "Consiglio\n\n\n\n\nGain proficiency in setting up and deploying ML models on supported devices, enabling you to tackle real-world ML deployment scenarios with confidence.\nUnderstand the steps involved in adapting and experimenting with ML models for different applications, allowing you to optimize performance and efficiency.\nLearn troubleshooting techniques specific to embedded ML deployments equipping you with the skills to overcome common pitfalls and challenges.\nAcquire practical experience in deploying TinyML models on embedded devices bridging the gap between theory and practice.\nExplore various sensor modalities and their applications expanding your understanding of how ML can be leveraged in diverse domains.\nFoster an understanding of the real-world implications and challenges associated with ML system deployments preparing you for future projects.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#target-audience",
    "href": "contents/labs/labs.html#target-audience",
    "title": "Overview",
    "section": "Target Audience",
    "text": "Target Audience\nThese labs are designed for:\n\nBeginners in the field of machine learning who have a keen interest in exploring the intersection of ML and embedded systems.\nDevelopers and engineers looking to apply ML models to real-world applications using low-power, resource-constrained devices.\nEnthusiasts and researchers who want to gain practical experience in deploying AI on edge devices and understand the unique challenges involved.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#supported-devices",
    "href": "contents/labs/labs.html#supported-devices",
    "title": "Overview",
    "section": "Supported Devices",
    "text": "Supported Devices\n\n\n\nExercise\nNicla Vision\nXIAO ESP32S3\nRaspberry Pi\n\n\n\n\nInstallation & Setup\n\n\n\n\n\nKeyword Spotting (KWS)\n\n\n\n\n\nImage Classification\n\n\nComing soon.\n\n\nObject Detection\n\n\nComing soon.\n\n\nMotion Detection\n\n\n\n\n\nSmall Language Models (SLM)\n\n\nComing soon.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#lab-structure",
    "href": "contents/labs/labs.html#lab-structure",
    "title": "Overview",
    "section": "Lab Structure",
    "text": "Lab Structure\nEach lab follows a structured approach:\n\nIntroduction: Explore the application and its significance in real-world scenarios.\nSetup: Step-by-step instructions to configure the hardware and software environment.\nDeployment: Guidance on training and deploying the pre-trained ML models on supported devices.\nExercises: Hands-on tasks to modify and experiment with model parameters.\nDiscussion: Analysis of results, potential improvements, and practical insights.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#troubleshooting-and-support",
    "href": "contents/labs/labs.html#troubleshooting-and-support",
    "title": "Overview",
    "section": "Troubleshooting and Support",
    "text": "Troubleshooting and Support\nIf you encounter any issues during the labs, consult the troubleshooting comments or check the FAQs within each lab. For further assistance, feel free to reach out to our support team or engage with the community forums.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#credits",
    "href": "contents/labs/labs.html#credits",
    "title": "Overview",
    "section": "Credits",
    "text": "Credits\nSpecial credit and thanks to Prof. Marcelo Rovai for his valuable contributions to the development and continuous refinement of these labs.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html",
    "href": "contents/labs/getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Hardware Requirements\nTo follow along with the hands-on labs, you’ll need the following hardware:\nThe Arduino Nicla Vision is tailored for professional-grade applications, offering advanced features and performance suitable for demanding industrial projects. On the other hand, the Seeed Studio XIAO ESP32S3 Sense is geared toward makers, hobbyists, and students who want to explore edge AI applications in a more accessible and beginner-friendly format. Both boards have their strengths and target audiences, allowing users to choose the best fit for their needs and skill level. The Raspberry Pi is aimed at more advanced engineering and machine learning projects.",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#hardware-requirements",
    "href": "contents/labs/getting_started.html#hardware-requirements",
    "title": "Getting Started",
    "section": "",
    "text": "Arduino Nicla Vision board\n\nThe Arduino Nicla Vision is a powerful, compact board designed for professional-grade computer vision and audio applications. It features a high-quality camera module, a digital microphone, and an IMU, making it suitable for demanding projects in industries such as robotics, automation, and surveillance.\nArduino Nicla Vision specifications\nArduino Nicla Vision pinout diagram\n\nXIAO ESP32S3 Sense board\n\nThe Seeed Studio XIAO ESP32S3 Sense is a tiny, feature-packed board designed for makers, hobbyists, and students interested in exploring edge AI applications. It comes with a camera, microphone, and IMU, making it easy to get started with projects like image classification, keyword spotting, and motion detection.\nXIAO ESP32S3 Sense specifications\nXIAO ESP32S3 Sense pinout diagram\n\nRaspberry Pi - Single Computer board\n\n\nThe Raspberry Pi is a powerful and versatile single-board computer that has become an essential tool for engineers across various disciplines. Developed by the Raspberry Pi Foundation, these compact devices offer a unique combination of affordability, computational power, and extensive GPIO (General Purpose Input/Output) capabilities, making them ideal for prototyping, embedded systems development, and advanced engineering projects.\nRaspberry Pi Hardware Documentation\nCamera Documentation\n\n\nAdditional accessories\n\nUSB-C cable for programming and powering the XIAO\nMicro-USB cable for programming and powering the Nicla\nPower Supply for the Raspberries\nBreadboard and jumper wires (optional, for connecting additional sensors)",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#software-requirements",
    "href": "contents/labs/getting_started.html#software-requirements",
    "title": "Getting Started",
    "section": "Software Requirements",
    "text": "Software Requirements\nTo program the boards and develop embedded machine learning projects, you’ll need the following software:\n\nArduino IDE\n\nDownload and install\n\nInstall Arduino IDE\nFollow the installation guide for your specific OS.\nArduino CLI\nConfigure the Arduino IDE for the Arduino Nicla Vision and XIAO ESP32S3 Sense boards.\n\n\nOpenMV IDE (optional)\n\nDownload and install the OpenMV IDE for your operating system.\nConfigure the OpenMV IDE for the Arduino Nicla Vision.\n\nEdge Impulse Studio\n\nSign up for a free account on the Edge Impulse Studio.\nInstall Edge Impulse CLI\nFollow the guides to connect your Arduino Nicla Vision and XIAO ESP32S3 Sense boards to Edge Impulse Studio.\n\nRaspberry Pi OS\n\n\nDownload and install the Raspberry Pi Imager",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#network-connectivity",
    "href": "contents/labs/getting_started.html#network-connectivity",
    "title": "Getting Started",
    "section": "Network Connectivity",
    "text": "Network Connectivity\nSome projects may require internet connectivity for data collection or model deployment. Ensure your development environment connection is stable through Wi-Fi or Ethernet. For the Raspberry Pi, having a Wi-Fi or Ethernet connection is necessary for remote operation without the necessity to plug in a monitor, keyboard, and mouse.\n\nFor the Arduino Nicla Vision, you can use the onboard Wi-Fi module to connect to a wireless network.\nFor the XIAO ESP32S3 Sense, you can use the onboard Wi-Fi module or connect an external Wi-Fi or Ethernet module using the available pins.\nFor the Raspberry Pi, you can use the onboard Wi-Fi module to connect an external Wi-Fi or Ethernet module using the available connector.",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#conclusion",
    "href": "contents/labs/getting_started.html#conclusion",
    "title": "Getting Started",
    "section": "Conclusion",
    "text": "Conclusion\nWith your hardware and software set up, you’re ready to embark on your embedded machine learning journey. The hands-on labs will guide you through various projects, covering topics like image classification, object detection, keyword spotting, and motion classification.\nIf you encounter any issues or have questions, don’t hesitate to consult the troubleshooting guides or forums or seek support from the community.\nLet’s dive in and unlock the potential of ML on real (tiny) systems!",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Introduction\nThe Arduino Nicla Vision (sometimes called NiclaV) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the Nicla Sense ME and the Nicla Voice. The Niclas can efficiently run processes created with TensorFlow Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference), while the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the management of WiFi and Bluetooth Low Energy (BLE) connectivity simultaneously.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#hardware",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#hardware",
    "title": "Setup",
    "section": "Hardware",
    "text": "Hardware\n\nTwo Parallel Cores\nThe central processor is the dual-core STM32H747, including a Cortex M7 at 480 MHz and a Cortex M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\nArduino sketches on top of the Arm Mbed OS\nNative Mbed applications\nMicroPython / JavaScript via an interpreter\nTensorFlow Lite\n\n\n\n\nMemory\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1MB, shared by both processors. This MCU also has incorporated 2MB of FLASH, mainly for code storage.\n\n\nSensors\n\nCamera: A GC2145 2 MP Color CMOS Camera.\nMicrophone: The MP34DT05 is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n6-Axis IMU: 3D gyroscope and 3D accelerometer data from the LSM6DSOX 6-axis IMU.\nTime of Flight Sensor: The VL53L1CBV0FY Time-of-Flight sensor adds accurate and low power-ranging capabilities to the Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#arduino-ide-installation",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#arduino-ide-installation",
    "title": "Setup",
    "section": "Arduino IDE Installation",
    "text": "Arduino IDE Installation\nStart connecting the board (microUSB) to your computer:\n\nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to Tools &gt; Board &gt; Board Manager, look for Arduino Nicla Vision on the search window, and install the board.\n\nNext, go to Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards and select Arduino Nicla Vision. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\nOpen the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n\nTesting the Microphone\nOn Arduino IDE, go to Examples &gt; PDM &gt; PDMSerialPlotter, open and run the sketch. Open the Plotter and see the audio representation from the microphone:\n\n\nVary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n\n\nTesting the IMU\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. For that, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test (you can also run Gyro and board temperature):\n\n\n\nTesting the ToF (Time of Flight) Sensor\nAs we did with IMU, it is necessary to install the VL53L1X ToF library. For that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n\nNext, run the sketch proximity_detection.ino:\n\nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4m).\n\n\n\nTesting the Camera\nWe can also test the camera using, for example, the code provided on Examples &gt; Camera &gt; CameraCaptureRawBytes. We cannot see the image directly, but it is possible to get the raw image data generated by the camera.\nAnyway, the best test with the camera is to see a live image. For that, we will use another IDE, the OpenMV.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#installing-the-openmv-ide",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#installing-the-openmv-ide",
    "title": "Setup",
    "section": "Installing the OpenMV IDE",
    "text": "Installing the OpenMV IDE\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras like the one on the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\nGo to the OpenMV IDE page, download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n\nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from Files &gt; Examples &gt; HelloWord &gt; helloword.py\n\nAny messages sent through a serial connection (using print() or error messages) will be displayed on the Serial Terminal during run time. The image captured by a camera will be displayed in the Camera Viewer Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\n\nBefore connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on Examples &gt; STM_32H747_System STM32H747_manageBootloader. Upload the code to your board. The Serial Monitor will guide you.\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n\nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select Install the latest release firmware (vX.Y.Z). This action will install the latest OpenMV firmware on the Nicla Vision.\n\nYou can leave the option Erase internal file system unselected and click [OK].\nNicla’s green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n\nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, “DFU firmware update complete!”. Press [OK].\n\nA green play button appears when the Nicla Vison connects to the Tool Bar.\n\nAlso, note that a drive named “NO NAME” will appear on your computer.:\n\nEvery time you press the [RESET] button on the board, it automatically executes the main.py script stored on it. You can load the main.py code on the IDE (File &gt; Open File...).\n\n\nThis code is the “Blink” code, confirming that the HW is OK.\n\nFor testing the camera, let’s run helloword_1.py. For that, select the script on File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nWhen clicking the green play button, the MicroPython script (hellowolrd.py) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 14fps.\n\nHere is the helloworld.py script:\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\nIn GitHub, you can find the Python scripts used here.\nThe code can be split into two parts:\n\nSetup: Where the libraries are imported, initialized and the variables are defined and initiated.\nLoop: (while loop) part of the code that runs continually. The image (img variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red [X] button.\n\nNote: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the GitHub, You can find other Python scripts. Try to test the onboard sensors.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "title": "Setup",
    "section": "Connecting the Nicla Vision to Edge Impulse Studio",
    "text": "Connecting the Nicla Vision to Edge Impulse Studio\nWe will need the Edge Impulse Studio later in other exercises. Edge Impulse is a leading development platform for machine learning on edge devices.\nEdge Impulse officially supports the Nicla Vision. So, for starting, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\nDownload the most updated EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary arduino-nicla-vision.bin to your board.\n\nGo to your project on the Studio, and on the Data Acquisition tab, select WebUSB (1). A window will pop up; choose the option that shows that the Nicla is paired (2) and press [Connect] (3).\n\nIn the Collect Data section on the Data Acquisition tab, you can choose which sensor data to pick.\n\nFor example. IMU data:\n\nOr Image (Camera):\n\nAnd so on. You can also test an external sensor connected to the ADC (Nicla pin 0) and the other onboard sensors, such as the microphone and the ToF.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#expanding-the-nicla-vision-board-optional",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#expanding-the-nicla-vision-board-optional",
    "title": "Setup",
    "section": "Expanding the Nicla Vision Board (optional)",
    "text": "Expanding the Nicla Vision Board (optional)\nA last item to be explored is that sometimes, during prototyping, it is essential to experiment with external sensors and devices, and an excellent expansion to the Nicla is the Arduino MKR Connector Carrier (Grove compatible).\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\nNote that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n\nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n\nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n\nThe Grove Light Sensor would be connected to one of the single Analog pins (A0/PC4), the LoRaWAN device to the UART, and the OLED to the I2C connector.\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying “Hello Word” on the Serial Monitor. Here is the code.\n\nBelow is the Hello World code to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nFinally, here is a simple script to read the ADC value on pin “PC4” (Nicla pin A0):\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\nThe ADC can be used for other sensor variables, such as Temperature.\n\nNote that the above scripts (downloaded from Github) introduce only how to connect external devices with the Nicla Vision board using MicroPython.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#conclusion",
    "title": "Setup",
    "section": "Conclusion",
    "text": "Conclusion\nThe Arduino Nicla Vision is an excellent tiny device for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\nOn the GitHub repository, you will find the last version of all the codeused or commented on in this hands-on exercise.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#resources",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#resources",
    "title": "Setup",
    "section": "Resources",
    "text": "Resources\n\nMicropython codes\nArduino Codes",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Introduction\nAs we initiate our studies into embedded machine learning or TinyML, it’s impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\nIn the “bullseye” of the Radar is the Edge Computer Vision, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is Image Classification, a kind of ML “Hello World”!\nThis exercise will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow’s robust ecosystem, we’ll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be on optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\nWe’ll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you’ll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#computer-vision",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#computer-vision",
    "title": "Image Classification",
    "section": "Computer Vision",
    "text": "Computer Vision\nAt its core, computer vision enables machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system’s ability to understand, interpret, and react to visual stimuli.\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are Image Classification and Object Detection.\n\nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-project-goal",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-project-goal",
    "title": "Image Classification",
    "section": "Image Classification Project Goal",
    "text": "Image Classification Project Goal\nThe first step in any ML project is to define the goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). Also, we will collect images of a background where those two objects are absent.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#data-collection",
    "title": "Image Classification",
    "section": "Data Collection",
    "text": "Data Collection\nOnce you have defined your Machine Learning project goal, the next and most crucial step is the dataset collection. You can use the Edge Impulse Studio, the OpenMV IDE we installed, or even your phone for the image capture. Here, we will use the OpenMV IDE for that.\n\nCollecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, “data.” Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\nThe IDE will ask you to open the file where your data will be saved and choose the “data” folder that was created. Note that new icons will appear on the Left panel.\n\nUsing the upper icon (1), enter with the first class name, for example, “periquito”:\n\nRunning the dataset_capture_script.py and clicking on the camera icon (2), will start capturing images:\n\nRepeat the same procedure with the other classes\n\n\nWe suggest around 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and the RGB565 (color pixel format).\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\nOn your computer, you will end with a dataset that contains three classes: periquito, robot, and background.\n\nYou should return to Edge Impulse Studio and upload the dataset to your project.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio for training our model. Enter your account credentials and create a new project:\n\n\nHere, you can clone a similar project: NICLA-Vision_Image_Classification.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#dataset",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#dataset",
    "title": "Image Classification",
    "section": "Dataset",
    "text": "Dataset\nUsing the EI Studio (or Studio), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into Training, Validation, and Test. The Test Set will be divided from the beginning, and a part will reserved to be used only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about that specific data:\n\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a very simple project, the data seems OK.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#the-impulse-design",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#the-impulse-design",
    "title": "Image Classification",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model, in this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 27,640 features (96x96x3).\n\nPress [Save parameters] and Generate all features:\n\n\n\nModel Design\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to improve performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is that of thinning a network uniformly at each layer.\nEdge Impulse Studio can use both MobileNetV1 (96x96 images) and V2 (96x96 or 160x160 images), with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3MB RAM and 2.6MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\nWe will use MobileNetV2 96x96 0.1 for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-training",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-training",
    "title": "Image Classification",
    "section": "Model Training",
    "text": "Model Training\nAnother valuable technique to be used with Deep Learning is Data Augmentation. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n\nThe result is excellent, with 77ms of latency, which should result in 13fps (frames per second) during inference.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-testing",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-testing",
    "title": "Image Classification",
    "section": "Model Testing",
    "text": "Model Testing\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input:\n\nThe result is, again, excellent.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#deploying-the-model",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#deploying-the-model",
    "title": "Image Classification",
    "section": "Deploying the model",
    "text": "Deploying the model\nAt this point, we can deploy the trained model as.tflite and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n\n\nArduino Library\nFirst, Let’s deploy it as an Arduino Library:\n\nYou should install the library as.zip on the Arduino IDE and run the sketch nicla_vision_camera.ino available in Examples under your library name.\n\nNote that Arduino Nicla Vision has, by default, 512KB of RAM allocated for the M7 core and an additional 244KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nThe result is good, with 86ms of measured latency.\n\nHere is a short video showing the inference results: \n\n\nOpenMV\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware.\nThree files are generated as a library: the trained.tflite model, a list with labels, and a simple MicroPython script that can make inferences using the model.\n\nRunning this model as a .tflite directly in the Nicla was impossible. So, we can sacrifice the accuracy using a smaller model or deploy the model as an OpenMV Firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let’s explore this option.\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\nOn your computer, you will find a ZIP file. Open it:\n\nUse the Bootloader tool on the OpenMV IDE to load the FW on your board:\n\nSelect the appropriate file (.bin for Nicla-Vision):\n\nAfter the download is finished, press OK:\n\nIf a message says that the FW is outdated, DO NOT UPGRADE. Select [NO].\n\nNow, open the script ei_image_classification.py that was downloaded from the Studio and the.bin file for the Nicla.\n\nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n\n\nChanging the Code to add labels\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\nUpload the code from GitHub, or modify it as below:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nHere you can see the result:\n\nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference:\n\nThe latency will drop to only 71 ms.\n\n\nThe NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n\n\nPost-Processing with LEDs\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n\nTo accomplish that, we should upload the code from GitHub or change the last code to include the LEDs:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val &lt; 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\nLed Red 0n: Uncertain (no class is over 0.8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nAll LEDs Off: Background &gt; 0.8\n\nHere is the result:\n\nIn more detail",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-non-official-benchmark",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-non-official-benchmark",
    "title": "Image Classification",
    "section": "Image Classification (non-official) Benchmark",
    "text": "Image Classification (non-official) Benchmark\nSeveral development boards can be used for embedded machine learning (TinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n\nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino’s Library:",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\nAutonomous Vehicles: Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\nHealthcare: Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\nRetail: Automated checkout systems that identify products as they pass through a scanner.\nSecurity and Surveillance: Facial recognition, anomaly detection, and object tracking in real-time video feeds.\nAugmented Reality: Object detection and classification to overlay digital information in the real world.\nIndustrial Automation: Visual inspection of products, predictive maintenance, and robot and drone guidance.\nAgriculture: Drone-based crop monitoring and automated harvesting.\nNatural Language Processing: Image captioning and visual question answering.\nGesture Recognition: For gaming, sign language translation, and human-machine interaction.\nContent Recommendation: Image-based recommendation systems in e-commerce.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#resources",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nMicropython codes\nDataset\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "Introduction\nThis is a continuation of CV on Nicla Vision, now exploring Object Detection on microcontrollers.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#introduction",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#introduction",
    "title": "Object Detection",
    "section": "",
    "text": "Object Detection versus Image Classification\nThe main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:\n\nBut what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:\n\nAnd what happens if there is not a dominant category on the image?\n\nThe model identifies the above image completely wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in all previous examples is the MobileNet, trained with a large dataset, the ImageNet.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\nThose models used for Object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually is lower than 1M Bytes.\n\n\nAn innovative solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).\nIn this Hands-On exercise, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-object-detection-project-goal",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-object-detection-project-goal",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nBox\nWheel\n\nHere are some not labeled image samples that we should use to detect the objects (wheels and boxes):\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#data-collection",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nWe can use the Edge Impulse Studio, the OpenMV IDE, your phone, or other devices for the image capture. Here, we will use again the OpenMV IDE for our purpose.\n\nCollecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, “data.” Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\nEdge impulse suggests that the objects should be of similar size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try with mixed sizes and positions to see the result.\n\nWe will not create separate folders for our images because each contains multiple labels.\n\nConnect the Nicla Vision to the OpenMV IDE and run the dataset_capture_script.py. Clicking on the Capture Image button will start capturing images:\n\nWe suggest around 50 images mixing the objects and varying the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#edge-impulse-studio",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\nHere, you can clone the project developed for this hands-on: NICLA_Vision_Object_Detection.\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Nicla Vision as your Target Device:\n\n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer files captured.\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually.\n\n\nAll the not labeled images (51) were uploaded but they still need to be labeled appropriately before using them as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link Labeling queue (51).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels was wrong, you can edit it using the three dots menu after the sample name:\n\nYou will be guided to replace the wrong label, correcting the dataset.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-impulse-design",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-impulse-design",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterwards, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n\n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, which is suitable for use with FOMO models and Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nOne of the samples (46) apparently is in the wrong space, but clicking on it can confirm that the labeling is correct.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#model-design-training-and-test",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#model-design-training-and-test",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions which have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35`. This model uses around 250KB RAM and 80KB of ROM (Flash), which suits well with our board since it has 1MB of RAM and ROM.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with practically 1.00 in the F1 score, with a similar result when using the Test data.\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\nTest model with “Live Classification”\nSince Edge Impulse officially supports the Nicla Vision, let’s connect it to the Studio. For that, follow the steps:\n\nDownload the last EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader related to your OS:\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary (arduino-nicla-vision.bin) to your board.\n\nGo to Live classification section at EI Studio, and using webUSB, connect your Nicla Vision:\n\nOnce connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the set-up). Try with 0.8 or more.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#deploying-the-model",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#deploying-the-model",
    "title": "Object Detection",
    "section": "Deploying the Model",
    "text": "Deploying the Model\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\nWhen you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option Load a specific firmware instead.\n\nYou will find a ZIP file on your computer from the Studio. Open it:\n\nLoad the .bin file to your board:\n\nAfter the download is finished, a pop-up message will be displayed. Press OK, and open the script ei_object_detection.py downloaded from the Studio.\nBefore running the script, let’s change a few lines. Note that you can leave the window definition as 240 x 240 and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse\n# Edge Impulse - OpenMV Object Detection Example\n\nimport sensor, image, time, os, tf, math, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\nRedefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.\nmin_confidence = 0.8\nChange if necessary, the color of the circles that will be used to display the detected object’s centroid for a better contrast.\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\ncolors = [ # Add more colors if you are detecting more than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nKeep the remaining code as it is and press the green Play button to run the code:\n\nOn the camera view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window (240X240).\n\nBe ware that the coordinate origin is in the upper left corner.\n\n\nNote that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)\nHere is a short video showing the inference results:",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices, for example, to explore the Nicla doing sensor fusion (camera + microphone) and object detection. This can be very useful on projects involving bees, for example.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#resources",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Introduction\nHaving already explored the Nicla Vision board in the Image Classification and Object Detection applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).\nAs introduced in the Feature Engineering for Audio Classification Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#how-does-a-voice-assistant-work",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#how-does-a-voice-assistant-work",
    "title": "Keyword Spotting (KWS)",
    "section": "How does a voice assistant work?",
    "text": "How does a voice assistant work?\nAs said, voice assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as ” Hey Google” on the first one and “Alexa” on the second.\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\nStage 1: A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the TinyML device (Stage 1).\n\n\nTo explore the above Google Assistant project, please see the tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the Nicla Vision, which has a digital microphone that will be used to spot the keyword.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#the-kws-hands-on-project",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#the-kws-hands-on-project",
    "title": "Keyword Spotting (KWS)",
    "section": "The KWS Hands-On Project",
    "text": "The KWS Hands-On Project\nThe diagram below gives an idea of how the final KWS application should work (during inference):\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no words spoken; only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nFor real-world projects, it is always advisable to include other sounds besides the keywords, such as “Noise” (or Background) and “Unknown.”\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#dataset",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of any Machine Learning Workflow is the dataset. Once we have decided on specific keywords, in our case (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as yes and no, we can get 1,500 samples.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file to a location of your choice.\n\n\nUploading the dataset to the Edge Impulse Studio\nInitiate a new project at Edge Impulse Studio (EIS) and select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\nDefine the Label, select Automatically split between train and test, and Upload data to the EIS. Repeat for all classes.\n\nThe dataset will now appear in the Data acquisition section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets.\n\n\n\nCapturing additional Audio Data\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of sound, this is optional because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.\n\nWhen we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16KHz frequency with 16-bit per sample amplitude.\nSo, any device that can generate audio data with this basic specification (16KHz/16bits) will work fine. As a device, we can use the NiclaV, a computer, or even your mobile phone.\n\n\nUsing the NiclaV and the Edge Impulse Studio\nAs we learned in the chapter Setup Nicla Vision, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:\n\nDownload the last updated EIS Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\nPut the NiclaV in Boot Mode by pressing the reset button twice.\nUpload the binary arduino-nicla-vision.bin to your board by running the batch code corresponding to your OS.\n\nGo to your project on EIS, and on the Data Acquisition tab, select WebUSB. A window will pop up; choose the option that shows that the Nicla is paired and press [Connect].\nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab. Select: Built-in microphone, define your label (for example, yes), the sampling Frequency[16000Hz], and the Sample length (in milliseconds), for example [10s]. Start sampling.\n\nData on Pete’s dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on three dots after the sample name and select Split sample.\nA window will pop up with the Split tool.\n\nOnce inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.\n\n\nUsing a smartphone and the EI Studio\nYou can also use your PC or smartphone to capture audio data, using a sampling frequency of 16KHz and a bit depth of 16.\nGo to Devices, scan the QR Code using your phone, and click on the link. A data Collection app will appear in your browser. Select Collecting Audio, and define your Label, data capture Length, and Category.\n\nRepeat the same procedure used with the NiclaV.\n\nNote that any app, such as Audacity, can be used for audio recording, provided you use 16KHz/16-bit depth samples.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#creating-impulse-pre-process-model-definition",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#creating-impulse-pre-process-model-definition",
    "title": "Keyword Spotting (KWS)",
    "section": "Creating Impulse (Pre-Process / Model definition)",
    "text": "Creating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\nImpulse Design\n\nFirst, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‘zeros’ samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). As discussed in the Feature Engineering for Audio Classification Hands-On tutorial, we will use Audio (MFCC), which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are well suited for the human voice, our case here.\nNext, we select the Classification block to build our model from scratch using a Convolution Neural Network (CNN).\n\nAlternatively, you can use the Transfer Learning (Keyword Spotting) block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.\n\n\n\nPre-Processing (MFCC)\nThe following step is to create the features to be trained in the next phase:\nWe could keep the default parameter values, but we will use the DSP Autotune parameters option.\n\nWe will take the Raw features (our 1-second, 16KHz sampled audio data) and use the MFCC processing block to calculate the Processed features. For every 16,000 raw features (16,000 x 1 second), we will get 637 processed features (13 x 49).\n\nThe result shows that we only used a small amount of memory to pre-process data (16KB) and a latency of 34ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64MHz), the same pre-process will take around 480ms. The parameters chosen, such as the FFT length [512], will significantly impact the latency.\nNow, let’s Save parameters and move to the Generated features tab, where the actual features will be generated. Using UMAP, a dimension reduction technique, the Feature explorer shows how the features are distributed on a two-dimensional plot.\n\nThe result seems OK, with a visually clear separation between yes features (in red) and no features (in blue). The unknown features seem nearer to the no space than the yes. This suggests that the keyword no has more propensity to false positives.\n\n\nGoing under the hood\nTo understand better how the raw sound is preprocessed, look at the Feature Engineering for Audio Classification chapter. You can play with the MFCC features generation by downloading this notebook from GitHub or [Opening it In Colab]",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#model-design-and-training",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#model-design-and-training",
    "title": "Keyword Spotting (KWS)",
    "section": "Model Design and Training",
    "text": "Model Design and Training\nWe will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:\n\nAs hyper-parameters, we will have a Learning Rate of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on SpecAugment. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.\n\n\nUsing 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.\n\nIt is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for yes is 95%, and for no, 91%. That was expected by what we saw with the Feature Explorer (no and unknown at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.\n\nListen to the samples that went wrong. For example, for yes, most of the mistakes were related to a yes pronounced as “yeh”. You can acquire additional samples and then retrain your model.\n\nGoing under the hood\nIf you want to understand what is happening “under the hood,” you can download the pre-processed dataset (MFCC training data) from the Dashboard tab and run this Jupyter Notebook, playing with the code or [Opening it In Colab]. For example, you can analyze the accuracy by each epoch:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#testing",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.\n\nInspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary “trigger” for our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.\nFor NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.\n\nLive Classification\nWe can proceed to the project’s next step but also consider that it is possible to perform Live Classification using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#deploy-and-inference",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the Deployment section, select Arduino Library, and at the bottom, choose Quantized (Int8) and press Build.\n\nWhen the Build button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the Sketch tab, select the option Add .ZIP Library, and Choose the .zip file downloaded by EIS:\n\nNow, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let’s use the NiclaV code example created when we deployed the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab, look for your project, and select nicla-vision/nicla-vision_microphone (or nicla-vision_microphone_continuous)\n\nPress the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#post-processing",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#post-processing",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-processing",
    "text": "Post-processing\nNow that we know the model is working since it detects our keywords, let’s modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOW, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF.\nWe should modify one of the code examples. Let’s do it now with the nicla-vision_microphone_continuous.\nStart with initializing the LEDs:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n    \n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreate two functions, turn_off_leds() function , to turn off all RGB LEDs\n**\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nAnother turn_on_led() function is used to turn on the RGB LEDs according to the most probable result of the classifier.\n/**\n * @brief      turn_on_leds function used to turn on the RGB LEDs\n * @param[in]  pred_index     \n *             no:       [0] ==&gt; Red ON\n *             noise:    [1] ==&gt; ALL OFF \n *             unknown:  [2] ==&gt; Blue ON\n *             Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n    \n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nAnd change the // print the predictions portion of the code on loop():\n...\n\n    if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n        // print the predictions\n        ei_printf(\"Predictions \");\n        ei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n            result.timing.dsp, result.timing.classification, result.timing.anomaly);\n        ei_printf(\": \\n\");\n\n        int pred_index = 0;     // Initialize pred_index\n        float pred_value = 0;   // Initialize pred_value\n\n        for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n            if (result.classification[ix].value &gt; pred_value){\n                pred_index = ix;\n                pred_value = result.classification[ix].value;\n            }\n            // ei_printf(\"    %s: \", result.classification[ix].label);\n            // ei_printf_float(result.classification[ix].value);\n            // ei_printf(\"\\n\");\n        }\n        ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\", \n                  result.classification[pred_index].label, pred_value);\n        turn_on_leds (pred_index);\n\n        \n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nYou can find the complete code on the project’s GitHub.\nUpload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can “trigger” an external device to perform a desired action instead of turning on an LED, as we saw in the introduction.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#conclusion",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusion",
    "text": "Conclusion\n\nYou will find the notebooks and codeused in this hands-on tutorial on the GitHub repository.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection, Gunshot)\nIndustry (Anomaly Detection)\nMedical (Snore, Cough, Pulmonary diseases)\nNature (Beehive control, insect sound, pouching mitigation)",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#resources",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#resources",
    "title": "Keyword Spotting (KWS)",
    "section": "Resources",
    "text": "Resources\n\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Introduction\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers’ safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\nBy the end of this tutorial, you’ll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#introduction",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#introduction",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nSetting up the Arduino Nicla Vision Board\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#imu-installation-and-testing",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#imu-installation-and-testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "IMU Installation and testing",
    "text": "IMU Installation and testing\nFor this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, Setup Nicla Vision, the Nicla Vision Board has an onboard 6-axis IMU: 3D gyroscope and 3D accelerometer, the LSM6DSOX. Let’s verify if the LSM6DSOX IMU library is installed. If not, install it.\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n\n\nDefining the Sampling frequency:\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you’re interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\nNature of the Motion: Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\nHardware Limitations: The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\nComputational Resources: Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\nBattery Life: A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\nData Storage: More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, sampling rates of around 50 Hz to 100 Hz are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\nLet’s define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ----------------------------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines -------------------------------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n    \n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2); \n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2); \n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2); \n    }\n  }\n}\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n\n\nNote that with the Nicla board resting on a table (with the camera facing down), the z-axis measures around 9.8m/s\\(^2\\), the expected earth acceleration.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#the-case-study-simulated-container-transportation",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#the-case-study-simulated-container-transportation",
    "title": "Motion Classification and Anomaly Detection",
    "section": "The Case Study: Simulated Container Transportation",
    "text": "The Case Study: Simulated Container Transportation\nWe will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we’ll capture motion data by manually simulating the conditions of:\n\nTerrestrial Transportation (by road or train)\nMaritime-associated Transportation\nVertical Movement via Fork-Lift\nStationary (Idle) period in a Warehouse\n\n\nFrom the above images, we can define for our simulation that primarily horizontal movements (x or y axis) should be associated with the “Terrestrial class,” Vertical movements (z-axis) with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#data-collection",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection",
    "text": "Data Collection\nFor data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\nConnecting the device to Edge Impulse\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\nDownload the latest firmware and connect it directly to the Data Collection section.\nUse the CLI Data Forwarder tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the Setup Nicla Vision hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let’s do it with the last one.\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\nInstall the Edge Impulse CLI and the Node.js into your computer.\nUpload a sketch for data capture (the one discussed previously in this tutorial).\nUse the CLI Data Forwarder to capture data from the Nicla’s accelerometer and send it to the Studio, as shown in this diagram:\n\n\nStart the CLI Data Forwarder on your terminal, entering (if it is the first time) the following command:\n$ edge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables (for example, accX, accY, and accZ), and device name (for example, NiclaV:\n\nGo to the Devices section on your EI Project and verify if the device is connected (the dot should be green):\n\n\nYou can clone the project developed for this hands-on: NICLA Vision Movement Classification.\n\n\n\nData Collection\nOn the Data Acquisition section, you should see that your board [NiclaV] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50Hz]. The Studio suggests a sample length of [10000] ms (10s). The last thing left is defining the sample label. Let’s start with[terrestrial]:\n\nTerrestrial (palettes in a Truck or Train), moving horizontally. Press [Start Sample]and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n\nAs expected, the movement was captured mainly in the Y-axis (green). In the blue, we see the Z axis, around -10 m/s\\(^2\\) (the Nicla has the camera facing up).\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\nMaritime (pallets in boats into an angry ocean). The movement is captured on all three axes:\n\nLift (Palettes being handled vertically by a Forklift). Movement captured only in the Z-axis:\n\nIdle (Paletts in a warehouse). No movement detected by the accelerometer:\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the three dots menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab. Below, you can see the resulting dataset:\n\nOnce you have captured your dataset, you can explore it in more detail using the Data Explorer, a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as PCA or t-SNE to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n\nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only “vertical”.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#impulse-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Impulse Design",
    "text": "Impulse Design\nThe next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a learning block to classify new data. Go to Impulse Design and Create Impulse. The Studio will suggest the basic design. Let’s also add a second Learning Block for Anomaly Detection.\n\nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n\nThe sampling frequency should be automatically captured, if not, enter it: [50]Hz. The Studio suggests a Window Size of 2 seconds ([2000] ms) with a sliding window of [20]ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n\nLet’s dig into those steps and parameters to understand better what we are doing here.\n\nData Pre-Processing Overview\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations.\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or windows. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window increase) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of “cycles of data”.\n\nWith a sampling rate (SR) of 50Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis x 2 seconds x 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\n\nOnce the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n\nEI Studio Spectral Features\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block.\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as FFT or Wavelets.\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of [32].\nThe per axis/channel Time Domain Statistical features are:\n\nRMS: 1 feature\nSkewness: 1 feature\nKurtosis: 1 feature\n\nThe per axis/channel Frequency Domain Spectral features are:\n\nSpectral Power: 16 features (FFT Length/2)\nSkewness: 1 feature\nKurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\nYou can learn more about how each feature is calculated by downloading the notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis or opening it directly on Google CoLab.\n\n\n\nGenerating features\nOnce we understand what the pre-processing does, it is time to finish the job. So, let’s take the raw data (time-series type) and convert it to tabular data. For that, go to the Spectral Features section on the Parameters tab, define the main parameters as discussed in the previous section ([FFT] with [32] points), and select[Save Parameters]:\n\nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#models-training",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#models-training",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Models Training",
    "text": "Models Training\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\nAs hyperparameters, we will use a Learning Rate of [0.005], a Batch size of [32], and [20]% of data for validation for [30] epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n\nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be [32], as suggested by the Studio:",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#testing",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Testing",
    "text": "Testing\nWe can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\nThe default minimum threshold for a considered uncertain result is [0.6] for classification and [0.3] for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can Set confidence thresholds on the three dots menu, besides the Classy all button.\n\nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#deploy",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#deploy",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Deploy",
    "text": "Deploy\nIt is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, you can choose Quantized (Int8) or Unoptimized (float32) and [Build]. A Zip file will be created and downloaded to your computer.\n\nOn your Arduino IDE, go to the Sketch tab, select Add.ZIP Library, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: Library installed.\n\nInference\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select Nicla_vision_fusion:\n\nNote that the code created by Edge Impulse considers a sensor fusion approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n/* Includes ---------------------------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt; \n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nYou can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla [RESET] button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\nIdle and lift classes:\n\n\n\nmaritime and terrestrial:\n\n\nNote that in all situations above, the value of the anomaly score was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, “rolling” the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\nanomaly detection:\n\n\nIn this case, the anomaly is much bigger, over 1.00\n\n\nPost-processing\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if terrestrial is detected, the Green LED will light; if maritime, the Red LED will light, if it is a lift, the Blue LED will light; and if no movement is detected (idle), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously).",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#conclusion",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe notebooks and codeused in this hands-on tutorial will be found on the GitHub repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\nCase Applications\n\nIndustrial and Manufacturing\n\nPredictive Maintenance: Detecting anomalies in machinery motion to predict failures before they occur.\nQuality Control: Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\nWarehouse Logistics: Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n\n\nHealthcare\n\nPatient Monitoring: Detecting falls or abnormal movements in the elderly or those with mobility issues.\nRehabilitation: Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\nActivity Recognition: Classifying types of physical activity for fitness applications or patient monitoring.\n\n\n\nConsumer Electronics\n\nGesture Control: Interpreting specific motions to control devices, such as turning on lights with a hand wave.\nGaming: Enhancing gaming experiences with motion-controlled inputs.\n\n\n\nTransportation and Logistics\n\nVehicle Telematics: Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\nCargo Monitoring: Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n\n\nSmart Cities and Infrastructure\n\nStructural Health Monitoring: Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\nTraffic Management: Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n\n\nSecurity and Surveillance\n\nIntruder Detection: Detecting motion patterns typical of unauthorized access or other security breaches.\nWildlife Monitoring: Detecting poachers or abnormal animal movements in protected areas.\n\n\n\nAgriculture\n\nEquipment Monitoring: Tracking the performance and usage of agricultural machinery.\nAnimal Behavior Analysis: Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n\n\nEnvironmental Monitoring\n\nSeismic Activity: Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\nOceanography: Studying wave patterns or marine movements for research and safety purposes.\n\n\n\n\nNicla 3D case\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: Arduino Nicla Voice and Vision Wearable Case.\n\nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#resources",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#resources",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Resources",
    "text": "Resources\n\nArduino Code\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Introduction\nThe XIAO ESP32S3 Sense is Seeed Studio’s affordable development board, which integrates a camera sensor, digital microphone, and SD card support. Combining embedded ML computing power and photography capability, this development board is a great tool to start with TinyML (intelligent voice and vision AI).\nXIAO ESP32S3 Sense Main Features\nBelow is the general board pinout:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#introduction",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#introduction",
    "title": "Setup",
    "section": "",
    "text": "Powerful MCU Board: Incorporate the ESP32S3 32-bit, dual-core, Xtensa processor chip operating up to 240 MHz, mounted multiple development ports, Arduino / MicroPython supported\nAdvanced Functionality: Detachable OV2640 camera sensor for 1600 * 1200 resolution, compatible with OV5640 camera sensor, integrating an additional digital microphone\nElaborate Power Design: Lithium battery charge management capability offers four power consumption models, which allows for deep sleep mode with power consumption as low as 14μA\nGreat Memory for more Possibilities: Offer 8MB PSRAM and 8MB FLASH, supporting SD card slot for external 32GB FAT memory\nOutstanding RF performance: Support 2.4GHz Wi-Fi and BLE dual wireless communication, support 100m+ remote communication when connected with U.FL antenna\nThumb-sized Compact Design: 21 x 17.5mm, adopting the classic form factor of XIAO, suitable for space-limited projects like wearable devices\n\n\n\n\n\nFor more details, please refer to the Seeed Studio WiKi page:  https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "title": "Setup",
    "section": "Installing the XIAO ESP32S3 Sense on Arduino IDE",
    "text": "Installing the XIAO ESP32S3 Sense on Arduino IDE\nOn Arduino IDE, navigate to File &gt; Preferences, and fill in the URL:\nhttps://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_dev_index.json\non the field ==&gt; Additional Boards Manager URLs\n\nNext, open boards manager. Go to Tools &gt; Board &gt; Boards Manager… and enter with esp32. Select and install the most updated and stable package (avoid alpha versions) :\n\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nOn Tools, select the Board (XIAO ESP32S3):\n\nLast but not least, choose the Port where the ESP32S3 is connected.\nThat is it! The device should be OK. Let’s do some tests.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-board-with-blink",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-board-with-blink",
    "title": "Setup",
    "section": "Testing the board with BLINK",
    "text": "Testing the board with BLINK\nThe XIAO ESP32S3 Sense has a built-in LED that is connected to GPIO21. So, you can run the blink sketch as it is (using the LED_BUILTIN Arduino constant) or by changing the Blink sketch accordingly:\n#define LED_BUILT_IN 21 \n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pin work with inverted logic\n// LOW to Turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNote that the pins work with inverted logic: LOW to Turn on and HIGH to turn off.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#connecting-sense-module-expansion-board",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#connecting-sense-module-expansion-board",
    "title": "Setup",
    "section": "Connecting Sense module (Expansion Board)",
    "text": "Connecting Sense module (Expansion Board)\nWhen purchased, the expansion board is separated from the main board, but installing the expansion board is very simple. You need to align the connector on the expansion board with the B2B connector on the XIAO ESP32S3, press it hard, and when you hear a “click,” the installation is complete.\nAs commented in the introduction, the expansion board, or the “sense” part of the device, has a 1600x1200 OV2640 camera, an SD card slot, and a digital microphone.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#microphone-test",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#microphone-test",
    "title": "Setup",
    "section": "Microphone Test",
    "text": "Microphone Test\nLet’s start with sound detection. Go to the GitHub project and download the sketch: XIAOEsp2s3_Mic_Test and run it on the Arduino IDE:\n\nWhen producing sound, you can verify it on the Serial Plotter.\nSave recorded sound (.wav audio files) to a microSD card.\nNow, the onboard SD Card reader can save .wav audio files. To do that, we need to habilitate the XIAO PSRAM.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. This can be insufficient for some purposes, so up to 16 MB of external PSRAM (pseudo-static RAM) can be connected with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\n\nDownload the sketch Wav_Record, which you can find on GitHub.\nTo execute the code (Wav Record), it is necessary to use the PSRAM function of the ESP-32 chip, so turn it on before uploading.: Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\n\nRun the code Wav_Record.ino\nThis program is executed only once after the user **turns on the serial monitor. It records for 20 seconds and saves the recording file to a microSD card as “arduino_rec.wav.”\nWhen the “.” is output every 1 second in the serial monitor, the program execution is finished, and you can play the recorded sound file with the help of a card reader.\n\n\nThe sound quality is excellent!\n\nThe explanation of how the code works is beyond the scope of this tutorial, but you can find an excellent description on the wiki page.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-camera",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-camera",
    "title": "Setup",
    "section": "Testing the Camera",
    "text": "Testing the Camera\nTo test the camera, you should download the folder take_photos_command from GitHub. The folder contains the sketch (.ino) and two .h files with camera details.\n\nRun the code: take_photos_command.ino. Open the Serial Monitor and send the command capture to capture and save the image on the SD Card:\n\n\nVerify that [Both NL & CR] are selected on Serial Monitor.\n\n\nHere is an example of a taken photo:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-wifi",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-wifi",
    "title": "Setup",
    "section": "Testing WiFi",
    "text": "Testing WiFi\nOne of the XIAO ESP32S3’s differentiators is its WiFi capability. So, let’s test its radio by scanning the Wi-Fi networks around it. You can do this by running one of the code examples on the board.\nGo to Arduino IDE Examples and look for WiFI ==&gt; WiFIScan\nYou should see the Wi-Fi networks (SSIDs and RSSIs) within your device’s range on the serial monitor. Here is what I got in the lab:\n\nSimple WiFi Server (Turning LED ON/OFF)\nLet’s test the device’s capability to behave as a WiFi Server. We will host a simple page on the device that sends commands to turn the XIAO built-in LED ON and OFF.\nLike before, go to GitHub to download the folder using the sketch SimpleWiFiServer.\nBefore running the sketch, you should enter your network credentials:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nYou can monitor how your server is working with the Serial Monitor.\n\nTake the IP address and enter it on your browser:\n\nYou will see a page with links that can turn the built-in LED of your XIAO ON and OFF.\nStreaming video to Web\nNow that you know that you can send commands from the webpage to your device, let’s do the reverse. Let’s take the image captured by the camera and stream it to a webpage:\nDownload from GitHub the folder that contains the code: XIAO-ESP32S3-Streeming_Video.ino.\n\nRemember that the folder contains the.ino file and a couple of .h files necessary to handle the camera.\n\nEnter your credentials and run the sketch. On the Serial monitor, you can find the page address to enter in your browser:\n\nOpen the page on your browser (wait a few seconds to start the streaming). That’s it.\n\nStreamlining what your camera is “seen” can be important when you position it to capture a dataset for an ML project (for example, using the code “take_phots_commands.ino”.\nOf course, we can do both things simultaneously: show what the camera sees on the page and send a command to capture and save the image on the SD card. For that, you can use the code Camera_HTTP_Server_STA, which can be downloaded from GitHub.\n\nThe program will do the following tasks:\n\nSet the camera to JPEG output mode.\nCreate a web page (for example ==&gt; http://192.168.4.119//). The correct address will be displayed on the Serial Monitor.\nIf server.on (“/capture”, HTTP_GET, serverCapture), the program takes a photo and sends it to the Web.\nIt is possible to rotate the image on webPage using the button [ROTATE]\nThe command [CAPTURE] only will preview the image on the webpage, showing its size on the Serial Monitor\nThe [SAVE] command will save an image on the SD Card and show the image on the browser.\nSaved images will follow a sequential naming (image1.jpg, image2.jpg.\n\n\n\nThis program can capture an image dataset with an image classification project.\n\nInspect the code; it will be easier to understand how the camera works. This code was developed based on the great Rui Santos Tutorial ESP32-CAM Take Photo and Display in Web Server, which I invite all of you to visit.\nUsing the CameraWebServer\nIn the Arduino IDE, go to File &gt; Examples &gt; ESP32 &gt; Camera, and select CameraWebServer\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nDo not forget the Tools to enable the PSRAM.\nEnter your wifi credentials and upload the code to the device:\n\nIf the code is executed correctly, you should see the address on the Serial Monitor:\n\nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. Using the [Save] button, you can save an image to your computer download area.\n\nThat’s it! You can save the images directly on your computer for use on projects.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#conclusion",
    "title": "Setup",
    "section": "Conclusion",
    "text": "Conclusion\nThe XIAO ESP32S3 Sense is flexible, inexpensive, and easy to program. With 8 MB of RAM, memory is not an issue, and the device can handle many post-processing tasks, including communication.\nYou will find the last version of the codeon the GitHub repository: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#resources",
    "title": "Setup",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Code",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Introduction\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\nAt the forefront of the Emerging Technologies Radar is the universal language of Edge Computer Vision. When we look into Machine Learning (ML) applied to vision, the first concept that greets us is Image Classification, a kind of ML’ Hello World ’ that is both simple and profound!\nThe Seeed Studio XIAO ESP32S3 Sense is a powerful tool that combines camera and SD card support. With its embedded ML computing power and photography capability, it is an excellent starting point for exploring TinyML vision AI.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#a-tinyml-image-classification-project---fruits-versus-veggies",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#a-tinyml-image-classification-project---fruits-versus-veggies",
    "title": "Image Classification",
    "section": "A TinyML Image Classification Project - Fruits versus Veggies",
    "text": "A TinyML Image Classification Project - Fruits versus Veggies\n\nThe whole idea of our project will be to train a model and proceed with inference on the XIAO ESP32S3 Sense. For training, we should find some data (in fact, tons of data!).\nBut first of all, we need a goal! What do we want to classify?\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We will differentiate apples from bananas and potatoes (you can try other categories).\nSo, let’s find a specific dataset that includes images from those categories. Kaggle is a good start:\nhttps://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition\nThis dataset contains images of the following food items:\n\nFruits - banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango.\nVegetables - cucumber, carrot, capsicum, onion, potato, lemon, tomato, radish, beetroot, cabbage, lettuce, spinach, soybean, cauliflower, bell pepper, chili pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant.\n\nEach category is split into the train (100 images), test (10 images), and validation (10 images).\n\nDownload the dataset from the Kaggle website and put it on your computer.\n\n\nOptionally, you can add some fresh photos of bananas, apples, and potatoes from your home kitchen, using, for example, the codediscussed in the setup lab.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. As you may know, Edge Impulse is a leading development platform for machine learning on edge devices.\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\n\nData Acquisition\nNext, on the UPLOAD DATA section, upload from your computer the files from chosen categories:\n\nIt would be best if you now had your training dataset split into three classes of data:\n\n\nYou can upload extra data for further model testing or split the training data. I will leave it as it is to use the most data possible.\n\n\n\nImpulse Design\n\nAn impulse takes raw data (in this case, images), extracts features (resize pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common use of deep learning, but a lot of data should be used to accomplish this task. We have around 90 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach or model” to differentiate an apple from a banana. But, we can solve this issue by re-training a previously trained model with thousands of images. We call this technique “Transfer Learning” (TL).\n\nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\nSo, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:\n\n\nPre-processing (Feature Generation)\nBesides resizing the images, we can change them to Grayscale or keep the actual RGB color depth. Let’s start selecting Grayscale. Doing that, each one of our data samples will have dimension 9, 216 features (96x96x1). Keeping RGB, this dimension would be three times bigger. Working with Grayscale helps to reduce the amount of final memory needed for inference.\n\nRemember to [Save parameters]. This will generate the features to be used in training.\n\n\nModel Design\nTransfer Learning\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be smaller and faster. MobileNet introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.\nEdge Impulse Studio has MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images) available, with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency.\nThe smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).\nFor this first pass, we will use MobileNet V1 and α=0.10.\n\n\n\nTraining\nData Augmentation\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models, creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nUnder the rood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 16 neurons with a 10% dropout for overfitting prevention. Here is the Training output:\n\nThe result could be better. The model reached around 77% accuracy, but the amount of RAM expected to be used during the inference is relatively tiny (about 60 KBytes), which is very good.\n\n\nDeployment\nThe trained model will be deployed as a .zip Arduino library:\n\nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Please select the file you download from Edge Impulse Studio, and that’s it!\n\nUnder the Examples tab on Arduino IDE, you should find a sketch code under your project name.\n\nOpen the Static Buffer example:\n\nYou can see that the first line of code is exactly the calling of a library with all the necessary stuff for running inference on your device.\n#include &lt;XIAO-ESP32S3-CAM-Fruits-vs-Veggies_inferencing.h&gt;\nOf course, this is a generic code (a “template”) that only gets one sample of raw data (stored on the variable: features = {} and runs the classifier, doing the inference. The result is shown on the Serial Monitor.\nWe should get the sample (image) from the camera and pre-process it (resizing to 96x96, converting to grayscale, and flatting it). This will be the input tensor of our model. The output tensor will be a vector with three values (labels), showing the probabilities of each one of the classes.\n\nReturning to your project (Tab Image), copy one of the Raw Data Sample:\n\n9, 216 features will be copied to the clipboard. This is the input tensor (a flattened image of 96x96x1), in this case, bananas. Past this Input tensor onfeatures[] = {0xb2d77b, 0xb5d687, 0xd8e8c0, 0xeaecba, 0xc2cf67, ...}\n\nEdge Impulse included the library ESP NN in its SDK, which contains optimized NN (Neural Network) functions for various Espressif chips, including the ESP32S3 (running at Arduino IDE).\nWhen running the inference, you should get the highest score for “banana.”\n\nGreat news! Our device handles an inference, discovering that the input image is a banana. Also, note that the inference time was around 317ms, resulting in a maximum of 3 fps if you tried to classify images from a video.\nNow, we should incorporate the camera and classify images in real time.\nGo to the Arduino IDE Examples and download from your project the sketch esp32_camera:\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n\nThe modified sketch can be downloaded from GitHub: xiao_esp32s3_camera.\n\nNote that you can optionally keep the pins as a .h file as we did in the Setup Lab.\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start classifying your fruits and vegetables! You can check the result on Serial Monitor.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-the-model-inference",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-the-model-inference",
    "title": "Image Classification",
    "section": "Testing the Model (Inference)",
    "text": "Testing the Model (Inference)\n\nGetting a photo with the camera, the classification result will appear on the Serial Monitor:\n\nOther tests:",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-with-a-bigger-model",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-with-a-bigger-model",
    "title": "Image Classification",
    "section": "Testing with a Bigger Model",
    "text": "Testing with a Bigger Model\nNow, let’s go to the other side of the model size. Let’s select a MobilinetV2 96x96 0.35, having as input RGB images.\n\nEven with a bigger model, the accuracy could be better, and the amount of memory necessary to run the model increases five times, with latency increasing seven times.\n\nNote that the performance here is estimated with a smaller device, the ESP-EYE. The actual inference with the ESP32S3 should be better.\n\nTo improve our model, we will need to train more images.\nEven though our model did not improve accuracy, let’s test whether the XIAO can handle such a bigger model. We will do a simple inference test with the Static Buffer sketch.\nLet’s redeploy the model. If the EON Compiler is enabled when you generate the library, the total memory needed for inference should be reduced, but it does not influence accuracy.\n\n⚠️ Attention - The Xiao ESP32S3 with PSRAM enable has enought memory to run the inference, even in such bigger model. Keep the EON Compiler NOT ENABLED.\n\n\nDoing an inference with MobilinetV2 96x96 0.35, having as input RGB images, the latency was 219ms, which is great for such a bigger model.\n\nFor the test, we can train the model again, using the smallest version of MobileNet V2, with an alpha of 0.05. Interesting that the result in accuracy was higher.\n\n\nNote that the estimated latency for an Arduino Portenta (ou Nicla), running with a clock of 480MHz is 45ms.\n\nDeploying the model, we got an inference of only 135ms, remembering that the XIAO runs with half of the clock used by the Portenta/Nicla (240MHz):",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#running-inference-on-the-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#running-inference-on-the-sensecraft-web-toolkit",
    "title": "Image Classification",
    "section": "Running inference on the SenseCraft-Web-Toolkit",
    "text": "Running inference on the SenseCraft-Web-Toolkit\nOne significant limitation of viewing inference on Arduino IDE is that we can not see what the camera focuses on. A good alternative is the SenseCraft-Web-Toolkit, a visual model deployment tool provided by SSCMA(Seeed SenseCraft Model Assistant). This tool allows you to deploy models to various platforms easily through simple operations. The tool offers a user-friendly interface and does not require any coding.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn the Dashboard, download the model (“block output”): Transfer learning model - TensorFlow Lite (int8 quantized).\n\n\n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n\n\nNote that you should use the labels trained on EI Studio, entering them in alphabetic order (in our case: apple, banana, potato).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\nThe Classification result will be at the top of the image. You can also select the Confidence of your inference cursor Confidence.\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, the same that we have done with the Arduino IDE:\n\nOn Device Log, you will get information as:\n\n\nPreprocess time (image capture and Crop): 4ms;\nInference time (model latency): 106ms,\nPostprocess time (display of the image and inclusion of data): 0ms.\nOutput tensor (classes), for example: [[89,0]]; where 0 is Apple (and 1is banana and 2 is potato)\n\nHere are other screenshots:",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nThe XIAO ESP32S3 Sense is very flexible, inexpensive, and easy to program. The project proves the potential of TinyML. Memory is not an issue; the device can handle many post-processing tasks, including communication.\nYou will find the last version of the codeon the GitHub repository: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nDataset\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "Introduction\nIn the last section regarding Computer Vision (CV) and the XIAO ESP32S3, Image Classification, we learned how to set up and classify images with this remarkable development board. Continuing our CV journey, we will explore Object Detection on microcontrollers.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#introduction",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#introduction",
    "title": "Object Detection",
    "section": "",
    "text": "Object Detection versus Image Classification\nThe main task with Image Classification models is to identify the most probable object category present on an image, for example, to classify between a cat or a dog, dominant “objects” in an image:\n\nBut what happens if there is no dominant category in the image?\n\nAn image classification model identifies the above image utterly wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in the previous images is MobileNet, which is trained with a large dataset, ImageNet, running on a Raspberry Pi.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually has, at most, a few MB as in the case of the XIAO ESP32S3.\n\n\nAn Innovative Solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, such as the Nicla Vision and Portenta (Cortex M7), on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM, ESP-EYE and XIAO ESP32S3 Sense).\nIn this Hands-On project, we will explore Object Detection using FOMO.\n\nTo understand more about FOMO, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-object-detection-project-goal",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-object-detection-project-goal",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial or rural facility and must sort and count oranges (fruits) and particular frogs (bugs).\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nFruit\nBug\n\nHere are some not labeled image samples that we should use to detect the objects (fruits and bugs):\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the XIAO ESP32S3 for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#data-collection",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#data-collection",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nYou can capture images using the XIAO, your phone, or other devices. Here, we will use the XIAO with code from the Arduino IDE ESP32 library.\n\nCollecting Dataset with the XIAO ESP32S3\nOpen the Arduino IDE and select the XIAO_ESP32S3 board (and the port where it is connected). On File &gt; Examples &gt; ESP32 &gt; Camera, select CameraWebServer.\nOn the BOARDS MANAGER panel, confirm that you have installed the latest “stable” package.\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nAnd on Tools, enable the PSRAM. Enter your wifi credentials and upload the code to the device:\n\nIf the code is executed correctly, you should see the address on the Serial Monitor:\n\nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. You can save an image on your computer download area using the [Save] button.\n\nEdge impulse suggests that the objects should be similar in size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe do not need to create separate folders for our images because each contains multiple labels.\n\nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, [Stop Stream] and move your images to a folder.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#edge-impulse-studio",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\nHere, you can clone the project developed for this hands-on: XIAO-ESP32S3-Sense-Object_Detection\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Espressif ESP-EYE (most similar to our board) as your Target Device:\n\n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload files captured as a folder from your computer.\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually. We will upload all of them as training.\n\n\nAll the not-labeled images (47) were uploaded but must be labeled appropriately before being used as a project dataset. The Studio has a tool for that purpose, which you can find in the link Labeling queue (47).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, you can edit it using the three dots menu after the sample name:\n\nYou will be guided to replace the wrong label and correct the dataset.\n\n\n\nBalancing the dataset and split Train/Test\nAfter labeling all data, it was realized that the class fruit had many more samples than the bug. So, 11 new and additional bug images were collected (ending with 58 images). After labeling them, it is time to select some images and move them to the test dataset. You can do it using the three-dot menu after the image name. I selected six images, representing 13% of the total dataset.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-impulse-design",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n\n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nSome samples seem to be in the wrong space, but clicking on them confirms the correct labeling.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#model-design-training-and-test",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#model-design-training-and-test",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250KB of RAM and 80KB of ROM (Flash), which suits well with our board.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an overall F1 score of 85%, similar to the result when using the test data (83%).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\nTest model with “Live Classification”\nOnce our model is trained, we can test it using the Live Classification tool. On the correspondent section, click on Connect a development board icon (a small MCU) and scan the QR code with your phone.\n\nOnce connected, you can use the smartphone to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the setup). Try with 0.8 or more.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-arduino-ide",
    "title": "Object Detection",
    "section": "Deploying the Model (Arduino IDE)",
    "text": "Deploying the Model (Arduino IDE)\nSelect the Arduino Library and Quantized (int8) model, enable the EON Compiler on the Deploy Tab, and press [Build].\n\nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Select the file you download from Edge Impulse Studio, and that’s it!\n\nUnder the Examples tab on Arduino IDE, you should find a sketch code (esp32 &gt; esp32_camera) under your project name.\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start detecting fruits and bugs. You can check the result on Serial Monitor.\nBackground\n\nFruits\n\nBugs\n\nNote that the model latency is 143ms, and the frame rate per second is around 7 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around five times higher (around 1.5 fps).",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-sensecraft-web-toolkit",
    "title": "Object Detection",
    "section": "Deploying the Model (SenseCraft-Web-Toolkit)",
    "text": "Deploying the Model (SenseCraft-Web-Toolkit)\nAs discussed in the Image Classification chapter, verifying inference with Image models on Arduino IDE is very challenging because we can not see what the camera focuses on. Again, let’s use the SenseCraft-Web Toolkit.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn Dashboard, download the model (“block output”): Object Detection model - TensorFlow Lite (int8 quantized)\n\n\n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n\n\nNote that you should use the labels trained on EI Studio and enter them in alphabetic order (in our case, background, bug, fruit).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\nThe detected objects will be marked (the centroid). You can select the Confidence of your inference cursor Confidence. and IoU, which is used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, as we did with the Arduino IDE.\n\nOn Device Log, you will get information as:\n\nPreprocess time (image capture and Crop): 3 ms;\nInference time (model latency): 115 ms,\nPostprocess time (display of the image and marking objects): 1 ms.\nOutput tensor (boxes), for example, one of the boxes: [[30,150, 20, 20,97, 2]]; where 30,150, 20, 20 are the coordinates of the box (around the centroid); 97 is the inference result, and 2 is the class (in this case 2: fruit)\n\n\nNote that in the above example, we got 5 boxes because none of the fruits got 3 centroids. One solution will be post-processing, where we can aggregate close centroids in one.\n\nHere are other screenshots:",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Introduction\nKeyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and achievable on smaller, low-power devices. This lab will guide you through implementing a KWS system using TinyML on the XIAO ESP32S3 microcontroller board.\nThe XIAO ESP32S3, equipped with Espressif’s ESP32-S3 chip, is a compact and potent microcontroller offering a dual-core Xtensa LX7 processor, integrated Wi-Fi, and Bluetooth. Its balance of computational power, energy efficiency, and versatile connectivity make it a fantastic platform for TinyML applications. Also, with its expansion board, we will have access to the “sense” part of the device, which has a 1600x1200 OV2640 camera, an SD card slot, and a digital microphone. The integrated microphone and the SD card will be essential in this project.\nWe will use the Edge Impulse Studio, a powerful, user-friendly platform that simplifies creating and deploying machine learning models onto edge devices. We’ll train a KWS model step-by-step, optimizing and deploying it onto the XIAO ESP32S3 Sense.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions (in the case of “YES”), bringing your projects to life with voice-activated commands.\nLeveraging our experience with TensorFlow Lite for Microcontrollers (the engine “under the hood” on the EI Studio), we’ll create a KWS system capable of real-time machine learning on the device.\nAs we progress through the lab, we’ll break down each process stage - from data collection and preparation to model training and deployment - to provide a comprehensive understanding of implementing a KWS system on a microcontroller.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#introduction",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#introduction",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "How does a voice assistant work?\nKeyword Spotting (KWS) is critical to many voice assistants, enabling devices to respond to specific words or phrases. To start, it is essential to realize that Voice Assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as “ Hey Google” on the first one and “Alexa” on the second.\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\nStage 1: A smaller microprocessor inside the Echo Dot or Google Home continuously listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example where I emulate a Google Assistant on a Raspberry Pi (Stage 2), having an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n\n\nIf you want to go deeper on the full project, please see my tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this lab, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone for spotting the keyword.\n\n\nThe KWS Project\nThe below diagram will give an idea of how the final KWS application should work (during inference):\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no keywords spoken, only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nOptionally for real-world projects, it is always advised to include different words than keywords, such as “Noise” (or Background) and “Unknow.”\n\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#dataset",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of Machine Learning Workflow is the dataset. Once we have decided on specific keywords (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In other words, we can get 1,500 samples of yes and no.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file in a location of your choice.\n\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor was essential. In the case of sound, it is different because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.\n\nThe sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16KHz with a 16-bit depth.\nSo, any device that can generate audio data with this basic specification (16Khz/16bits) will work fine. As a device, we can use the proper XIAO ESP32S3 Sense, a computer, or even your mobile phone.\n\nCapturing online Audio Data with Edge Impulse and a smartphone\nIn the lab Motion Classification and Anomaly Detection, we connect our device directly to Edge Impulse Studio for data capturing (having a sampling frequency of 50Hz to 100Hz). For such low frequency, we could use the EI CLI function Data Forwarder, but according to Jan Jongboom, Edge Impulse CTO, audio (16KHz) goes too fast for the data forwarder to be captured. So, once we have the digital data captured by the microphone, we can turn it into a WAV file to be sent to the Studio via Data Uploader (same as we will do with Pete’s dataset).\n\nIf we want to collect audio data directly on the Studio, we can use any smartphone connected online with it. We will not explore this option here, but you can easily follow EI documentation.\n\n\nCapturing (offline) Audio Data with the XIAO ESP32S3 Sense\nThe built-in microphone is the MSM261D3526H1CPM, a PDM digital output MEMS microphone with Multi-modes. Internally, it is connected to the ESP32S3 via an I2S bus using pins IO41 (Clock) and IO41 (Data).\n\nWhat is I2S?\nI2S, or Inter-IC Sound, is a standard protocol for transmitting digital audio from one device to another. It was initially developed by Philips Semiconductor (now NXP Semiconductors). It is commonly used in audio devices such as digital signal processors, digital audio processors, and, more recently, microcontrollers with digital audio capabilities (our case here).\nThe I2S protocol consists of at least three lines:\n\n1. Bit (or Serial) clock line (BCLK or CLK): This line toggles to indicate the start of a new bit of data (pin IO42).\n2. Word select line (WS): This line toggles to indicate the start of a new word (left channel or right channel). The Word select clock (WS) frequency defines the sample rate. In our case, L/R on the microphone is set to ground, meaning that we will use only the left channel (mono).\n3. Data line (SD): This line carries the audio data (pin IO41)\nIn an I2S data stream, the data is sent as a sequence of frames, each containing a left-channel word and a right-channel word. This makes I2S particularly suited for transmitting stereo audio data. However, it can also be used for mono or multichannel audio with additional data lines.\nLet’s start understanding how to capture raw data using the microphone. Go to the GitHub projectand download the sketch: XIAOEsp2s3_Mic_Test:\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nThis code is a simple microphone test for the XIAO ESP32S3 using the I2S (Inter-IC Sound) interface. It sets up the I2S interface to capture audio data at a sample rate of 16 kHz with 16 bits per sample and then continuously reads samples from the microphone and prints them to the serial monitor.\nLet’s dig into the code’s main parts:\n\nInclude the I2S library: This library provides functions to configure and use the I2S interface, which is a standard for connecting digital audio devices.\nI2S.setAllPins(-1, 42, 41, -1, -1): This sets up the I2S pins. The parameters are (-1, 42, 41, -1, -1), where the second parameter (42) is the PIN for the I2S clock (CLK), and the third parameter (41) is the PIN for the I2S data (DATA) line. The other parameters are set to -1, meaning those pins are not used.\nI2S.begin(PDM_MONO_MODE, 16000, 16): This initializes the I2S interface in Pulse Density Modulation (PDM) mono mode, with a sample rate of 16 kHz and 16 bits per sample. If the initialization fails, an error message is printed, and the program halts.\nint sample = I2S.read(): This reads an audio sample from the I2S interface.\n\nIf the sample is valid, it is printed on the Serial Monitor and Plotter.\nBelow is a test “whispering” in two different tones.\n\n\n\nSave recorded sound samples (dataset) as .wav audio files to a microSD card\nLet’s use the onboard SD Card reader to save .wav audio files; we must habilitate the XIAO PSRAM first.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. It can be insufficient for some purposes so that ESP32-S3 can use up to 16 MB of external PSRAM (Psuedostatic RAM) connected in parallel with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\nTurn the PSRAM function of the ESP-32 chip on (Arduino IDE): Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\nDownload the sketch Wav_Record_dataset,which you can find on the project’s GitHub.\n\nThis code records audio using the I2S interface of the Seeed XIAO ESP32S3 Sense board, saves the recording as a.wav file on an SD card, and allows for control of the recording process through commands sent from the serial monitor. The name of the audio file is customizable (it should be the class labels to be used with the training), and multiple recordings can be made, each saved in a new file. The code also includes functionality to increase the volume of the recordings.\nLet’s break down the most essential parts of it:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nThose are the necessary libraries for the program. I2S.h allows for audio input, FS.h provides file system handling capabilities, SD.h enables the program to interact with an SD card, and SPI.h handles the SPI communication with the SD card.\n#define RECORD_TIME   10  \n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nHere, various constants are defined for the program.\n\nRECORD_TIME specifies the length of the audio recording in seconds.\nSAMPLE_RATE and SAMPLE_BITS define the audio quality of the recording.\nWAV_HEADER_SIZE specifies the size of the .wav file header.\nVOLUME_GAIN is used to increase the volume of the recording.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nThese variables keep track of the current file number (to create unique file names), the base file name, and whether the system is currently recording.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n  \n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n  \n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n  Serial.printf(\"Enter with the label name\\n\");\n}\nThe setup function initializes the serial communication, I2S interface for audio input, and SD card interface. If the I2S did not initialize or the SD card fails to mount, it will print an error message and halt execution.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n  }\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\" + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files at once\n    isRecording = false;\n  }\n}\nIn the main loop, the program waits for a command from the serial monitor. If the command is rec, the program starts recording. Otherwise, the command is assumed to be the base name for the .wav files. If it’s currently recording and a base file name is set, it records the audio and saves it as a.wav file. The file names are generated by appending the file number to the base file name.\nvoid record_wav(String fileName)\n{\n  ...\n  \n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                    rec_buffer, \n                    record_size, \n                    &sample_size, \n                    portMAX_DELAY);\n  ...\n}\nThis function records audio and saves it as a.wav file with the given name. It starts by initializing the sample_size and record_size variables. record_size is calculated based on the sample rate, size, and desired recording time. Let’s dig into the essential sections;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nThis section of the code opens the file on the SD card for writing and then generates the .wav file header using the generate_wav_header function. It then writes the header to the file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize() - ESP.getFreePsram());\nThe ps_malloc function allocates memory in the PSRAM for the recording. If the allocation fails (i.e., rec_buffer is NULL), it prints an error message and halts execution.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n         rec_buffer, \n         record_size, \n         &sample_size, \n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nThe i2s_read function reads audio data from the microphone into rec_buffer. It prints an error message if no data is read (sample_size is 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nThis section of the code increases the recording volume by shifting the sample values by VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter a new label\\n\\n\");\nFinally, the audio data is written to the .wav file. If the write operation fails, it prints an error message. After writing, the memory allocated for rec_buffer is freed, and the file is closed. The function finishes by printing a completion message and prompting the user to send a new command.\nvoid generate_wav_header(uint8_t *wav_header,  \n             uint32_t wav_size, \n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nThe generate_wav_header function creates a.wav file header based on the parameters (wav_size and sample_rate). It generates an array of bytes according to the .wav file format, which includes fields for the file size, audio format, number of channels, sample rate, byte rate, block alignment, bits per sample, and data size. The generated header is then copied into the wav_header array passed to the function.\nNow, upload the code to the XIAO and get samples from the keywords (yes and no). You can also capture noise and other words.\nThe Serial monitor will prompt you to receive the label to be recorded.\n\nSend the label (for example, yes). The program will wait for another command: rec\n\nAnd the program will start recording new samples every time a command rec is sent. The files will be saved as yes.1.wav, yes.2.wav, yes.3.wav, etc., until a new label (for example, no) is sent. In this case, you should send the command rec for each new sample, which will be saved as no.1.wav, no.2.wav, no.3.wav, etc.\n\nUltimately, we will get the saved files on the SD card.\n\nThe files are ready to be uploaded to Edge Impulse Studio\n\n\nCapturing (offline) Audio Data Apps\nAlternatively, you can also use your PC or smartphone to capture audio data with a sampling frequency 16KHz and a bit depth of 16 Bits. A good app for that is Voice Recorder Pro (IOS). You should save your records as .wav files and send them to your computer.\n\n\nNote that any app, such as Audacity, can be used for audio recording or even your computer.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#training-model-with-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#training-model-with-edge-impulse-studio",
    "title": "Keyword Spotting (KWS)",
    "section": "Training model with Edge Impulse Studio",
    "text": "Training model with Edge Impulse Studio\n\nUploading the Data\nWhen the raw dataset is defined and collected (Pete’s dataset + recorded keywords), we should initiate a new project at Edge Impulse Studio:\n\nOnce the project is created, select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\nAnd upload them to the Studio (You can automatically split data in train/test). Repete to all classes and all raw data.\n\nThe samples will now appear in the Data acquisition section.\n\nAll data on Pete’s dataset have a 1s length, but the samples recorded in the previous section have 10s and must be split into 1s samples to be compatible.\nClick on three dots after the sample name and select Split sample.\n\nOnce inside the tool, split the data into 1-second records. If necessary, add or remove segments:\n\nThis procedure should be repeated for all samples.\n\nNote: For longer audio files (minutes), first, split into 10-second segments and after that, use the tool again to get the final 1-second splits.\n\nSuppose we do not split data automatically in train/test during upload. In that case, we can do it manually (using the three dots menu, moving samples individually) or using Perform Train / Test Split on Dashboard - Danger Zone.\n\nWe can optionally check all datasets using the tab Data Explorer.\n\n\n\nCreating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\nFirst, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, I reduced the 1000 ms window on the split tool to avoid noises and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). We will use MFCC, which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are great for the human voice.\n\nNext, we select KERAS for classification and build our model from scratch by doing Image Classification using Convolution Neural Network).\n\n\nPre-Processing (MFCC)\nThe next step is to create the images to be trained in the next phase:\nWe can keep the default parameter values or take advantage of the DSP Autotuneparameters option, which we will do.\n\nThe result will not spend much memory to pre-process data (only 16KB). Still, the estimated processing time is high, 675 ms for an Espressif ESP-EYE (the closest reference available), with a 240KHz clock (same as our device), but with a smaller CPU ( XTensa LX6, versus the LX7 on the ESP32S). The real inference time should be smaller.\nSuppose we need to reduce the inference time later. In that case, we should return to the pre-processing stage and, for example, reduce the FFT length to 256, change the Number of coefficients, or another parameter.\nFor now, let’s keep the parameters defined by the Autotuning tool. Save parameters and generate the features.\n\n\nIf you want to go further with converting temporal serial data into images using FFT, Spectrogram, etc., you can play with this CoLab: Audio Raw Data Analysis.\n\n\n\nModel Design and Training\nWe will use a Convolution Neural Network (CNN) model. The basic architecture is defined with two blocks of Conv1D + MaxPooling (with 8 and 16 neurons, respectively) and a 0.25 Dropout. And on the last layer, after Flattening four neurons, one for each class:\n\nAs hyper-parameters, we will have a Learning Rate of 0.005 and a model that will be trained by 100 epochs. We will also include data augmentation, as some noise. The result seems OK:\n\nIf you want to understand what is happening “under the hood,” you can download the dataset and run a Jupyter Notebook playing with the code. For example, you can analyze the accuracy by each epoch:\n\nThis CoLab Notebook can explain how you can go further: KWS Classifier Project - Looking “Under the hood Training/xiao_esp32s3_keyword_spotting_project_nn_classifier.ipynb).”",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#testing",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data put apart before training (Test Data), we got an accuracy of approximately 87%.\n\nInspecting the F1 score, we can see that for YES. We got 0.95, an excellent result once we used this keyword to “trigger” our postprocessing stage (turn on the built-in LED). Even for NO, we got 0.90. The worst result is for unknown, what is OK.\nWe can proceed with the project, but it is possible to perform Live Classification using a smartphone before deployment on our device. Go to the Live Classification section and click on Connect a Development board:\n\nPoint your phone to the barcode and select the link.\n\nYour phone will be connected to the Studio. Select the option Classification on the app, and when it is running, start testing your keywords, confirming that the model is working with live and real data:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#deploy-and-inference",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and press the button Build.\n\nNow it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the ESP32 code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab look for your project, and select esp32/esp32_microphone:\n\nThis code was created for the ESP-EYE built-in microphone, which should be adapted for our device.\nStart changing the libraries to handle the I2S bus:\n\nBy:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInitialize the IS2 microphone at setup(), including the lines:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nOn the static void capture_samples(void* arg) function, replace the line 153 that reads data from I2S mic:\n\nBy:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                 (void*)sampleBuffer, \n                 i2s_bytes_to_read, \n                 &bytes_read, 100);\nOn function static bool microphone_inference_start(uint32_t n_samples), we should comment or delete lines 198 to 200, where the microphone initialization function is called. This is unnecessary because the I2S microphone was already initialized during the setup().\n\nFinally, on static void microphone_inference_end(void) function, replace line 243:\n\nBy:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#postprocessing",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#postprocessing",
    "title": "Keyword Spotting (KWS)",
    "section": "Postprocessing",
    "text": "Postprocessing\nNow that we know the model is working by detecting our keywords, let’s modify the code to see the internal LED going on every time a YES is detected.\nYou should initialize the LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nAnd change the // print the predictions portion of the previous code (on loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification, result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:\n\nThe idea is that the LED will be ON whenever the keyword YES is detected. In the same way, instead of turning on an LED, this could be a “trigger” for an external device, as we saw in the introduction.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#conclusion",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusion",
    "text": "Conclusion\nThe Seeed XIAO ESP32S3 Sense is a giant tiny device! However, it is powerful, trustworthy, not expensive, low power, and has suitable sensors to be used on the most common embedded machine learning applications such as vision and sound. Even though Edge Impulse does not officially support XIAO ESP32S3 Sense (yet!), we realized that using the Studio for training and deployment is straightforward.\n\nOn my GitHub repository, you will find the last version all the codeused on this project and the previous ones of the XIAO ESP32S3 series.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection)\nIndustry (Anomaly Detection)\nMedical (Snore, Toss, Pulmonary diseases)\nNature (Beehive control, insect sound)",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#resources",
    "title": "Keyword Spotting (KWS)",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS CNN training Colab Notebook\nXIAO ESP32S3 Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Introduction\nThe XIAO ESP32S3 Sense, with its built-in camera and mic, is a versatile device. But what if you need to add another type of sensor, such as an IMU? No problem! One of the standout features of the XIAO ESP32S3 is its multiple pins that can be used as an I2C bus (SDA/SCL pins), making it a suitable platform for sensor integration.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#installing-the-imu",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#installing-the-imu",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Installing the IMU",
    "text": "Installing the IMU\nWhen selecting your IMU, the market offers a wide range of devices, each with unique features and capabilities. You could choose, for example, the ADXL362 (3-axis), MAX21100 (6-axis), MPU6050 (6-axis), LIS3DHTR (3-axis), or the LCM20600Seeed Grove— (6-axis), which is part of the IMU 9DOF (lcm20600+AK09918). This variety allows you to tailor your choice to your project’s specific needs.\nFor this project, we will use an IMU, the MPU6050 (or 6500), a low-cost (less than 2.00 USD) 6-axis Accelerometer/Gyroscope unit.\n\nAt the end of the lab, we will also comment on using the LCM20600.\n\nThe MPU-6500 is a 6-axis Motion Tracking device that combines a 3-axis gyroscope, 3-axis accelerometer, and a Digital Motion ProcessorTM (DMP) in a small 3x3x0.9mm package. It also features a 4096-byte FIFO that can lower the traffic on the serial bus interface and reduce power consumption by allowing the system processor to burst read sensor data and then go into a low-power mode.\nWith its dedicated I2C sensor bus, the MPU-6500 directly accepts inputs from external I2C devices. MPU-6500, with its 6-axis integration, on-chip DMP, and run-time calibration firmware, enables manufacturers to eliminate the costly and complex selection, qualification, and system-level integration of discrete devices, guaranteeing optimal motion performance for consumers. MPU-6500 is also designed to interface with multiple non-inertial digital sensors, such as pressure sensors, on its auxiliary I2C port.\n\n\nUsually, the libraries available are for MPU6050, but they work for both devices.\n\nConnecting the HW\nConnect the IMU to the XIAO according to the below diagram:\n\nMPU6050 SCL –&gt; XIAO D5\nMPU6050 SDA –&gt; XIAO D4\nMPU6050 VCC –&gt; XIAO 3.3V\nMPU6050 GND –&gt; XIAO GND\n\n\nInstall the Library\nGo to Arduino Library Manager and type MPU6050. Install the latest version.\n\nDownload the sketch MPU6050_Acc_Data_Acquisition.in:\n/*\n * Based on I2C device class (I2Cdev) Arduino sketch for MPU6050 class \n   by Jeff Rowberg &lt;jeff@rowberg.net&gt;\n * and Edge Impulse Data Forwarder Exampe (Arduino) \n   - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * \n * Developed by M.Rovai @11May23\n */\n\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n\n// convert factor g to m/s2 ==&gt; [-32768, +32767] ==&gt; [-2g, +2g]\n#define CONVERT_G_TO_MS2    (9.81/(16384.0/(1.+ACC_RANGE))) \n\nstatic unsigned long last_interval_ms = 0;\n\nMPU6050 imu;\nint16_t ax, ay, az;\n\nvoid setup() {\n  \n    Serial.begin(115200);\n\n    \n    // initialize device\n    Serial.println(\"Initializing I2C devices...\");\n    Wire.begin();\n    imu.initialize();\n    delay(10);\n    \n//    // verify connection\n//    if (imu.testConnection()) {\n//      Serial.println(\"IMU connected\");\n//    }\n//    else {\n//      Serial.println(\"IMU Error\");\n//    }\n    delay(300);\n    \n    //Set MCU 6050 OffSet Calibration \n    imu.setXAccelOffset(-4732);\n    imu.setYAccelOffset(4703);\n    imu.setZAccelOffset(8867);\n    imu.setXGyroOffset(61);\n    imu.setYGyroOffset(-73);\n    imu.setZGyroOffset(35);\n    \n    /* Set full-scale accelerometer range.\n     * 0 = +/- 2g\n     * 1 = +/- 4g\n     * 2 = +/- 8g\n     * 3 = +/- 16g\n     */\n    imu.setFullScaleAccelRange(ACC_RANGE);\n}\n\nvoid loop() {\n\n      if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n        last_interval_ms = millis();\n        \n        // read raw accel/gyro measurements from device\n        imu.getAcceleration(&ax, &ay, &az);\n\n        // converting to m/s2\n        float ax_m_s2 = ax * CONVERT_G_TO_MS2;\n        float ay_m_s2 = ay * CONVERT_G_TO_MS2;\n        float az_m_s2 = az * CONVERT_G_TO_MS2;\n\n        Serial.print(ax_m_s2); \n        Serial.print(\"\\t\");\n        Serial.print(ay_m_s2); \n        Serial.print(\"\\t\");\n        Serial.println(az_m_s2); \n      }\n}\nSome comments about the code:\nNote that the values generated by the accelerometer and gyroscope have a range: [-32768, +32767], so for example, if the default accelerometer range is used, the range in Gs should be: [-2g, +2g]. So, “1G” means 16384.\nFor conversion to m/s2, for example, you can define the following:\n#define CONVERT_G_TO_MS2 (9.81/16384.0)\nIn the code, I left an option (ACC_RANGE) to be set to 0 (+/-2G) or 1 (+/- 4G). We will use +/-4G; that should be enough for us. In this case.\nWe will capture the accelerometer data on a frequency of 50Hz, and the acceleration data will be sent to the Serial Port as meters per squared second (m/s2).\nWhen you ran the code with the IMU resting over your table, the accelerometer data shown on the Serial Monitor should be around 0.00, 0.00, and 9.81. If the values are a lot different, you should calibrate the IMU.\nThe MCU6050 can be calibrated using the sketch: mcu6050-calibration.ino.\nRun the code. The following will be displayed on the Serial Monitor:\n\nSend any character (in the above example, “x”), and the calibration should start.\n\nNote that A message MPU6050 connection failed. Ignore this message. For some reason, imu.testConnection() is not returning a correct result.\n\nIn the end, you will receive the offset values to be used on all your sketches:\n\nTake the values and use them on the setup:\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\nNow, run the sketch MPU6050_Acc_Data_Acquisition.in:\nOnce you run the above sketch, open the Serial Monitor:\n\nOr check the Plotter:\n\nMove your device in the three axes. You should see the variation on Plotter:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#the-tinyml-motion-classification-project",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#the-tinyml-motion-classification-project",
    "title": "Motion Classification and Anomaly Detection",
    "section": "The TinyML Motion Classification Project",
    "text": "The TinyML Motion Classification Project\nFor our lab, we will simulate mechanical stresses in transport. Our problem will be to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (palettes in a Truck or Train)\nLift (Palettes being handled by Fork-Lift)\nIdle (Palettes in Storage houses)\n\nSo, to start, we should collect data. Then, accelerometers will provide the data on the palette (or container).\n\nFrom the above images, we can see that primarily horizontal movements should be associated with the “Terrestrial class,” Vertical movements with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#connecting-the-device-to-edge-impulse",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#connecting-the-device-to-edge-impulse",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Connecting the device to Edge Impulse",
    "text": "Connecting the device to Edge Impulse\nFor data collection, we should first connect our device to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\nFollow the instructions hereto install the Node.jsand Edge Impulse CLI on your computer.\n\nOnce the XIAO ESP32S3 is not a fully supported development board by Edge Impulse, we should, for example, use the CLI Data Forwarder to capture data from our sensor and send it to the Studio, as shown in this diagram:\n\n\nYou can alternately capture your data “offline,” store them on an SD card or send them to your computer via Bluetooth or Wi-Fi. In this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\nConnect your device to the serial port and run the previous code to capture IMU (Accelerometer) data, “printing them” on the serial. This will allow the Edge Impulse Studio to “capture” them.\nGo to the Edge Impulse page and create a project.\n\n\nThe maximum length for an Arduino library name is 63 characters. Note that the Studio will name the final library using your project name and include “_inference” to it. The name I chose initially did not work when I tried to deploy the Arduino library because it resulted in 64 characters. So, I need to change it by taking out the “anomaly detection” part.\n\nStart the CLI Data Forwarderon your terminal, entering (if it is the first time) the following command:\nedge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables, and device names:\n\nGo to your EI Project and verify if the device is connected (the dot should be green):",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-collection",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-collection",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection",
    "text": "Data Collection\nAs discussed before, we should capture data from all four Transportation Classes. Imagine that you have a container with a built-in accelerometer:\n\nNow imagine your container is on a boat, facing an angry ocean, on a truck, etc.:\n\nMaritime (pallets in boats)\n\nMove the XIAO in all directions, simulating an undulatory boat movement.\n\nTerrestrial (palettes in a Truck or Train)\n\nMove the XIAO over a horizontal line.\n\nLift (Palettes being handled by\n\nMove the XIAO over a vertical line.\n\nIdle (Palettes in Storage houses)\n\nLeave the XIAO over the table.\n\n\n\nBelow is one sample (raw data) of 10 seconds:\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds each) for the four classes. Using the “3 dots” after each one of the samples, select 2, moving them for the Test set (or use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab). Below, you can see the result datasets:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-pre-processing",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-pre-processing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data type captured by the accelerometer is a “time series” and should be converted to “tabular data”. We can do this conversion using a sliding window over the sample data. For example, in the below figure,\n\nWe can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50Hz. A 2-second window will capture 300 data points (3 axis x 2 seconds x 50 samples). We will slide this window each 200ms, creating a larger dataset where each instance has 300 raw features.\n\nYou should use the best SR for your case, considering Nyquist’s theorem, which states that a periodic signal must be sampled at more than twice the signal’s highest frequency component.\n\nData preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.\nOn the Studio, this dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as “FFT” or “Wavelets”. In the most common case, FFT, the Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness\nKurtosis\n\nFor example, for an FFT length of 32 points, the Spectral Analysis Block’s resulting output will be 21 features per axis (a total of 63 features).\nThose 63 features will be the Input Tensor of a Neural Network Classifier and the Anomaly Detection model (K-Means).\n\nYou can learn more by digging into the lab DSP Spectral Features",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#model-design",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#model-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Model Design",
    "text": "Model Design\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#impulse-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Impulse Design",
    "text": "Impulse Design\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\nWe also take advantage of a second model, the K-means, that can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean).\n\n\nImagine our XIAO rolling or moving upside-down, on a movement complement different from the one trained\n\n\nBelow is our final Impulse design:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#generating-features",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#generating-features",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Generating features",
    "text": "Generating features\nAt this point in our project, we have defined the pre-processing method and the model designed. Now, it is time to have the job done. First, let’s take the raw data (time-series type) and convert it to tabular data. Go to the Spectral Features tab and select Save Parameters:\n\nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but also for general non-linear dimension reduction.\n\nThe visualization allows one to verify that the classes present an excellent separation, which indicates that the classifier should work well.\n\nOptionally, you can analyze the relative importance of each feature for one class compared with other classes.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#training",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#training",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Training",
    "text": "Training\nOur model has four layers, as shown below:\n\nAs hyperparameters, we will use a Learning Rate of 0.005 and 20% of data for validation for 30 epochs. After training, we can see that the accuracy is 97%.\n\nFor anomaly detection, we should choose the suggested features that are precisely the most important in feature extraction. The number of clusters will be 32, as suggested by the Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#testing",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Testing",
    "text": "Testing\nUsing 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was not that good (8%), mainly due to the terrestrial class. Once we have four classes (which output should add 1.0), we can set up a lower threshold for a class to be considered valid (for example, 0.4):\n\nNow, the Test accuracy will go up to 97%.\n\nYou should also use your device (which is still connected to the Studio) and perform some Live Classification.\n\nBe aware that here you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#deploy",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#deploy",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Deploy",
    "text": "Deploy\nNow it is time for magic˜! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and Build. A Zip file will be created and downloaded to your computer.\n\nOn your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#inference",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#inference",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Inference",
    "text": "Inference\nNow, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select nano_ble_sense_accelerometer:\n\nOf course, this is not your board, but we can have the code working with only a few changes.\nFor example, at the beginning of the code, you have the library related to Arduino Sense IMU:\n/* Includes --------------------------------------------------------------- */\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nChange the “includes” portion with the code related to the IMU:\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\nChange the Constant Defines\n/* Constant defines ------------------------------------------------------- */\nMPU6050 imu;\nint16_t ax, ay, az;\n\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n#define CONVERT_G_TO_MS2    (9.81/(16384/(1.+ACC_RANGE)))\n#define MAX_ACCEPTED_RANGE  (2*9.81)+(2*9.81)*ACC_RANGE\nOn the setup function, initiate the IMU set the off-set values and range:\n// initialize device\nSerial.println(\"Initializing I2C devices...\");\nWire.begin();\nimu.initialize();\ndelay(10);\n\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\n\nimu.setFullScaleAccelRange(ACC_RANGE);\nAt the loop function, the buffers buffer[ix], buffer[ix + 1], and buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. On the original code, you have the line:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nChange it with this block of code:\nimu.getAcceleration(&ax, &ay, &az);       \nbuffer[ix + 0] = ax;\nbuffer[ix + 1] = ay;\nbuffer[ix + 2] = az;\nYou should change the order of the following two blocks of code. First, you make the conversion to raw data to “Meters per squared second (ms2)”, followed by the test regarding the maximum acceptance range (that here is in ms2, but on Arduino, was in Gs):\nbuffer[ix + 0] *= CONVERT_G_TO_MS2;\nbuffer[ix + 1] *= CONVERT_G_TO_MS2;\nbuffer[ix + 2] *= CONVERT_G_TO_MS2;\n\nfor (int i = 0; i &lt; 3; i++) {\n     if (fabs(buffer[ix + i]) &gt; MAX_ACCEPTED_RANGE) {\n        buffer[ix + i] = ei_get_sign(buffer[ix + i]) * MAX_ACCEPTED_RANGE;\n     }\n}\nAnd that is it! You can now upload the code to your device and proceed with the inferences. The complete code is available on the project’s GitHub.\nNow you should try your movements, seeing the result of the inference of each class on the images:\n\n\n\n\nAnd, of course, some “anomaly”, for example, putting the XIAO upside-down. The anomaly score will be over 1:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#conclusion",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\nRegarding the IMU, this project used the low-cost MPU6050 but could also use other IMUs, for example, the LCM20600 (6-axis), which is part of the Seeed Grove - IMU 9DOF (lcm20600+AK09918). You can take advantage of this sensor, which has integrated a Grove connector, which can be helpful in the case you use the XIAO with an extension board, as shown below:\n\nYou can follow the instructions here to connect the IMU with the MCU. Only note that for using the Grove ICM20600 Accelerometer, it is essential to update the files I2Cdev.cpp and I2Cdev.h that you will download from the library provided by Seeed Studio. For that, replace both files from this link. You can find a sketch for testing the IMU on the GitHub project: accelerometer_test.ino.\n\nOn the projet’s GitHub repository, you will find the last version of all codeand other docs: XIAO-ESP32S3 - IMU.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#resources",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Introduction\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with “under-the-hood” mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don’t understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "This tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#the-kws",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#the-kws",
    "title": "KWS Feature Engineering",
    "section": "The KWS",
    "text": "The KWS\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition transcribes all spoken words into text, Keyword Spotting focuses on detecting specific “keywords” or “wake words” in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n\n\nApplications of KWS\n\nVoice Assistants: In devices like Amazon’s Alexa or Google Home, KWS is used to detect the wake word (“Alexa” or “Hey Google”) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like “Start engine” or “Turn off lights.”\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\nDifferences from General Speech Recognition\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction-to-audio-signals",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction-to-audio-signals",
    "title": "KWS Feature Engineering",
    "section": "Introduction to Audio Signals",
    "text": "Introduction to Audio Signals\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals’ tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16KHz. Altough music tones can be heard at frequencies up to 20 kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8 kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs. Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal’s tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal’s constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal’s spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n\n\nWhy Not Raw Audio?\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn’t inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction-to-mfccs",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction-to-mfccs",
    "title": "KWS Feature Engineering",
    "section": "Introduction to MFCCs",
    "text": "Introduction to MFCCs\n\nWhat are MFCCs?\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal’s spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n\n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\nWhy are MFCCs important?\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear’s response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\nComputing MFCCs\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let’s walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: y(t) = x(t) - \\(\\alpha\\) x(t-1) , where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n\n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f) = |X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n\n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear’s response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n\n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "title": "KWS Feature Engineering",
    "section": "Hands-On using Python",
    "text": "Hands-On using Python\nLet’s apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#conclusion",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#conclusion",
    "title": "KWS Feature Engineering",
    "section": "Conclusion",
    "text": "Conclusion\nWhat Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\nMFCCs are particularly strong for\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\nSpectrograms or MFEs are often more suitable for\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#resources",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#resources",
    "title": "KWS Feature Engineering",
    "section": "Resources",
    "text": "Resources\n\nAudio_Data_Analysis Colab Notebook",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "Introduction\nTinyML projects related to motion (or vibration) involve data from IMUs (usually accelerometers and Gyroscopes). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block for Inertial sensors.\nBut how does it work under the hood? Let’s dig into it.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "title": "DSP Spectral Features",
    "section": "Extracting Features Review",
    "text": "Extracting Features Review\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations. Here’s a high-level overview of the process:\nData collection: First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It’s essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\nData preprocessing: Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\nThe Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentation: Depending on the nature of the data and the application, dividing the data into smaller segments or windows may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window span) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of “data cycles.”\nFeature extraction: Once the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\nLet’s explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "title": "DSP Spectral Features",
    "section": "A TinyML Motion Classification project",
    "text": "A TinyML Motion Classification project\n\nIn the hands-on project, Motion Classification and Anomaly Detection, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (pallets in a Truck or Train)\nLift (pallets being handled by Fork-Lift)\nIdle (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n\nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50Hz:\n\n\nThe result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5Hz instead of 50Hz.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "title": "DSP Spectral Features",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data captured by the accelerometer (a “time series” data) should be converted to “tabular data” using one of the typical Feature Extraction methods described in the last section.\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis x 2 seconds x 62.5 samples). The window is slid every 80ms, creating a larger dataset where each instance has 375 “raw features.”\n\nOn the Studio, the previous version (V1) of the Spectral Analysis Block extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n\nThose 33 features were the Input tensor of a Neural Network Classifier.\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\nEdge Impulse - Spectral Analysis Block V.2 under the hood\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness (in the next version)\nKurtosis (in the next version)\n\nIn this link, we can have more details about the feature extraction.\n\nClone the public project. You can also follow the explanation, playing with the code using my Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nStart importing the libraries:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nFrom the studied project, let’s choose a data sample from accelerometers as below:\n\nWindow size of 2 seconds: [2,000] ms\nSample frequency: [62.5] Hz\nWe will choose the [None] filter (for simplicity) and a\nFFT length: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n\nSelecting the Raw Features on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n\nPaste the data points to a new variable data:\ndata=[-5.6330, 0.2376, 9.8701, -5.9442, 0.4830, 9.8701, -5.4217, ...]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nThe total raw features are 375, but we will work with each axis individually, where N= 125 (number of samples per axis).\nWe aim to understand how Edge Impulse gets the processed features.\n\nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\nfeatures = [2.7322, -0.0978, -0.3813, 2.3980, 3.8924, 24.6841, 9.6303, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nThe total number of processed features is 39, which means 13 features/axis.\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n[rms] [skew] [kurtosis]\n\nand 10 for the frequency domain (we will return to this later).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSplitting raw data per sensor\nThe data has samples from all axes; let’s split and plot them separately:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ] \nplot_data(sensors, axis, 'Raw Features')\n\nSubtracting the mean\nNext, we should subtract the mean from the data. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\nHere are some specific reasons why subtracting the mean can be helpful:\n\nIt simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\nIt removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\nIt can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\nIt can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\ndtmean = [(sum(x)/len(x)) for x in sensors]\n[print('mean_'+x+'= ', round(y, 4)) for x,y in zip(axis, dtmean)][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "title": "DSP Spectral Features",
    "section": "Time Domain Statistical features",
    "text": "Time Domain Statistical features\nRMS Calculation\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the “value of the direct current that dissipates the same power in a resistor.”\nIn the case of a set of n values {𝑥1, 𝑥2, …, 𝑥𝑛}, the RMS is:\n\n\nNOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n# Using numpy and standartized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nCompared with Edge Impulse result features:\n[2.7322, 0.7833, 0.1383]\nSkewness and kurtosis calculation\nIn statistics, skewness and kurtosis are two ways to measure the shape of a distribution.\nHere, we can see the sensor values distribution:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n\nSkewness is a measure of the asymmetry of a distribution. This value can be positive or negative.\n\n\nA negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\nA positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\nA zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4)) for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nCompared with Edge Impulse result features:\n[-0.0978, 0.1735, 6.8629]\nKurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n\n\nThe kurtosis of a normal distribution is zero.\nIf a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\nIf a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4)) for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nCompared with Edge Impulse result features:\n[-0.3813, 1.1696, 65.3726]",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "title": "DSP Spectral Features",
    "section": "Spectral features",
    "text": "Spectral features\nThe filtered signal is passed to the Spectral power section, which computes the FFT to generate the spectral features.\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or “sub-windows”), and the FFT is calculated over each frame.\nFFT length - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\nThe total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\nSpectral Power - Welch’s method\nWe should use Welch’s method to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len, and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplying the above function to 3 signals:\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\nWe can plot the Power Spectrum P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n\nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nLet’s now list all Spectral features per axis and compare them with EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) #13: 3+N_feat_axis;  26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "title": "DSP Spectral Features",
    "section": "Time-frequency domain",
    "text": "Time-frequency domain\n\nWavelets\nWavelet is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a wavelet function, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\nLet’s select Wavelet on the Spectral Features block in the same project:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n\nThe Wavelet Function\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n\nAs we did before, let’s copy and past the Processed Features:\n\nfeatures = [3.6251, 0.0615, 0.0615, -7.3517, -2.7641, 2.8462, 5.0924, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse computes the Discrete Wavelet Transform (DWT) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\nIn the case of Wavelets, the extracted features are basic statistical values, crossing values, and entropy. There are, in total, 14 features per layer as below:\n\n[11] Statiscal Features: n5, n25, n75, n95, mean, median, standard deviation (std), variance (var) root mean square (rms), kurtosis, and skewness (skew).\n[2] Crossing Features: Zero crossing rate (zcross) and mean crossing rate (mcross) are the times that the signal passes through the baseline (y = 0) and the average level (y = u) per unit of time, respectively\n[1] Complexity Feature: Entropy is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\nThe total number of features varies depending on how you set the filter and the number of layers. For example, with [None] filtering and Level[1], the number of features per axis will be 14 x 2 (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n\n\nWavelet Analysis\nWavelet analysis decomposes the signal (accX, accY, and accZ) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as accX_l1, accY_l1, accZ_l1 and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as accX_d1, accY_d1, accZ_d1, permitting the extraction of features for further analysis or classification.\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the “Multilevel 1D Discrete Wavelet Transform”, the result will be a list (for detail, please see: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n\n\n\nFeature Extraction\nLet’s start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n \nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nThe Skelness and Kurtosis:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal’s frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\nMean crossing (mcross), on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) &lt; 0).su    m())/len(arr)))\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nIn wavelet analysis, entropy refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal’s uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nLet’s now list all the wavelet features and create a list by layers.\nL1_features_names = [\"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\", \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\", \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"]\n\nL0_features_names = [\"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\", \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\", \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = stat_feat_l0[i]+[skew_l0[i]]+[kurtosis_l0[i]]+[cross_l0[0][i]]+[cross_l0[1][i]]+[entropy_l0[i]]\n    [print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L0_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\nall_feat_l0 = [item for sublist in all_feat_l0 for item in sublist]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\nfeat_l1 = stat_feat_l1[i]+[skew_l1[i]]+[kurtosis_l1[i]]+[cross_l1[0][i]]+[cross_l1[1][i]]+[entropy_l1[i]]\n[print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L1_features_names, feat_l1)][0]\nall_feat_l1.append(feat_l1)\nall_feat_l1 = [item for sublist in all_feat_l1 for item in sublist]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "title": "DSP Spectral Features",
    "section": "Conclusion",
    "text": "Conclusion\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\nDaniel Situnayake wrote in his blog: “Raw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.” I recommend you read Dan’s excellent post in its totality: nn to cpp: What you need to know about porting deep learning models to the edge.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya\nMironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with\nDifferential Privacy.” In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Security, 308–18. CCS\n’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi\nSchwarzschild, Manli Shu, Christoph Studer, and Chen Zhu. 2020.\n“Headless Horseman: Adversarial Attacks on Transfer\nLearning Models.” In ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\n3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, and\nR. Venkatesh Babu. 2020. “Towards Achieving Adversarial Robustness\nby Enforcing Feature Consistency Across Bit Planes.” In 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 1020–29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\n\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, and David\nBrooks. 2016. “Fathom: Reference Workloads for Modern\nDeep Learning Methods.” In 2016 IEEE International Symposium\non Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, and\nHanna M. Wallach. 2018. “A Reductions Approach to Fair\nClassification.” In Proceedings of the 35th International\nConference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,\nSweden, July 10-15, 2018, edited by Jennifer G. Dy and Andreas\nKrause, 80:60–69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta,\nAustin Jiao, Ben Keller, Brucek Khailany, and Haoxing Ren. 2023.\n“AutoDMP: Automated DREAMPlace-Based Macro\nPlacement.” In Proceedings of the 2023 International\nSymposium on Physical Design, 149–57. ACM. https://doi.org/10.1145/3569052.3578923.\n\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, and\nBerk Sunar. 2007. “Trojan Detection Using\nIC Fingerprinting.” In 2007 IEEE Symposium on\nSecurity and Privacy (SP ’07), 29–45. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud\nDaneshtalab, and Maksim Jenihhin. 2024. “A Systematic Literature\nReview on Hardware Reliability Assessment Methods for Deep Neural\nNetworks.” ACM Comput. Surv. 56 (6): 1–39. https://doi.org/10.1145/3638242.\n\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, and Fahad Saeed. 2020.\n“Federated Learning: A Survey on Enabling\nTechnologies, Protocols, and Applications.” #IEEE_O_ACC#\n8: 140699–725. https://doi.org/10.1109/access.2020.3013541.\n\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak,\nShahab Asoodeh, and Flavio Calmon. 2022. “Beyond Adult and\nCOMPAS: Fair Multi-Class Prediction via\nInformation Projection.” Adv. Neur. In. 35: 38747–60.\n\n\nAltayeb, Moez, Marco Zennaro, and Marcelo Rovai. 2022.\n“Classifying Mosquito Wingbeat Sound Using\nTinyML.” In Proceedings of the 2022 ACM\nConference on Information Technology for Social Good, 132–37. ACM.\nhttps://doi.org/10.1145/3524458.3547258.\n\n\nAmiel, Frederic, Christophe Clavier, and Michael Tunstall. 2006.\n“Fault Analysis of DPA-Resistant Algorithms.”\nIn International Workshop on Fault Diagnosis and Tolerance in\nCryptography, 223–36. Springer.\n\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain,\nMichael Voznesensky, Bin Bao, et al. 2024. “PyTorch\n2: Faster Machine Learning Through Dynamic Python Bytecode\nTransformation and Graph Compilation.” In Proceedings of the\n29th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume 2, edited by\nHanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd’Alché-Buc, Emily B. Fox, and Roman Garnett, 8024–35. ACM. https://doi.org/10.1145/3620665.3640366.\n\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. 2020.\nICML Workshop on Challenges in Deploying and monitoring Machine Learning\nSystems.\n\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv\nBatra, C. Lawrence Zitnick, and Devi Parikh. 2015.\n“VQA: Visual Question Answering.”\nIn 2015 IEEE International Conference on Computer Vision\n(ICCV), 2425–33. IEEE. https://doi.org/10.1109/iccv.2015.279.\n\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie\nBursztein, Jaime Cochran, Zakir Durumeric, et al. 2017.\n“Understanding the Mirai Botnet.” In 26th USENIX\nSecurity Symposium (USENIX Security 17), 1093–1110.\n\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer,\nMichael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and\nGregor Weber. 2020. “Common Voice: A\nMassively-Multilingual Speech Corpus.” In Proceedings of the\nTwelfth Language Resources and Evaluation Conference, 4218–22.\nMarseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\n\n\nArifeen, Tooba, Abdus Sami Hassan, and Jeong-A Lee. 2020.\n“Approximate Triple Modular Redundancy: A\nSurvey.” #IEEE_O_ACC# 8: 139851–67. https://doi.org/10.1109/access.2020.3012673.\n\n\nAsonov, D., and R. Agrawal. 2004. “Keyboard Acoustic\nEmanations.” In IEEE Symposium on Security and Privacy, 2004.\nProceedings. 2004, 3–11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,\nDomenico Vitali, and Giovanni Felici. 2015. “Hacking Smart\nMachines with Smarter Ones: How to Extract Meaningful Data\nfrom Machine Learning Classifiers.” Int. J. Secur. Netw.\n10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\n\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman,\nSuraj Kapa, Paul A. Friedman, and Peter A. Noseworthy. 2018.\n“Noninvasive Assessment of Dofetilide Plasma Concentration Using a\nDeep Learning (Neural Network) Analysis of the Surface\nElectrocardiogram: A Proof of Concept Study.” PLOS ONE\n13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\n\nAygun, Sercan, Ece Olcay Gunes, and Christophe De Vleeschouwer. 2021.\n“Efficient and Robust Bitstream Processing in Binarised Neural\nNetworks.” Electron. Lett. 57 (5): 219–22. https://doi.org/10.1049/ell2.12045.\n\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.\n“Recent Advances in Adversarial Training for Adversarial\nRobustness.” arXiv Preprint arXiv:2102.01356.\n\n\nBains, Sunny. 2020. “The Business of Building Brains.”\nNature Electronics 3 (7): 348–51. https://doi.org/10.1038/s41928-020-0449-1.\n\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, and Yousra Chtouki. 2022.\n“How TinyML Can Be Leveraged to Solve Environmental\nProblems: A Survey.” In 2022 International\nConference on Innovation and Intelligence for Informatics, Computing,\nand Technologies (3ICT), 338–43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.\n\n\nBank, Dor, Noam Koenigstein, and Raja Giryes. 2023.\n“Autoencoders.” Machine Learning for Data Science\nHandbook: Data Mining and Knowledge Discovery Handbook, 353–74.\n\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes.\n2019. “Computer and Redundancy Solution for the Full Self-Driving\nComputer.” In 2019 IEEE Hot Chips 31 Symposium (HCS),\n1–22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\n\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro\nPellicioli, and Gerardo Pelosi. 2010. “Low Voltage Fault Attacks\nto AES.” In 2010 IEEE International Symposium on\nHardware-Oriented Security and Trust (HOST), 7–12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\n\nBarroso, Luiz André, Urs Hölzle, and Parthasarathy Ranganathan. 2019.\nThe Datacenter as a Computer: Designing Warehouse-Scale\nMachines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.\n2017. “Network Dissection: Quantifying\nInterpretability of Deep Visual Representations.” In 2017\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n3319–27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\n\nBeaton, Albert E., and John W. Tukey. 1974. “The Fitting of Power\nSeries, Meaning Polynomials, Illustrated on Band-Spectroscopic\nData.” Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\n\n\nBeck, Nathaniel, and Simon Jackman. 1998. “Beyond Linearity by\nDefault: Generalized Additive Models.” Am. J.\nPolit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\n\nBender, Emily M., and Batya Friedman. 2018. “Data Statements for\nNatural Language Processing: Toward Mitigating System Bias\nand Enabling Better Science.” Transactions of the Association\nfor Computational Linguistics 6 (December): 587–604. https://doi.org/10.1162/tacl_a_00041.\n\n\nBerger, Vance W, and YanYan Zhou. 2014.\n“Kolmogorovsmirnov Test:\nOverview.” Wiley Statsref: Statistics Reference\nOnline.\n\n\nBeyer, Lucas, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and\nAäron van den Oord. 2020. “Are We Done with Imagenet?”\nArXiv Preprint abs/2006.07159. https://arxiv.org/abs/2006.07159.\n\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, and Dawn Song. 2018.\n“Practical Black-Box Attacks on Deep Neural Networks Using\nEfficient Query Mechanisms.” In Proceedings of the European\nConference on Computer Vision (ECCV), 154–69.\n\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel\nHernández-Lobato, and Gu-Yeon Wei. 2020. “A Comprehensive\nMethodology to Determine Optimal Coherence Interfaces for\nMany-Accelerator SoCs.” In Proceedings of the\nACM/IEEE International Symposium on Low Power Electronics and\nDesign, 145–50. ACM. https://doi.org/10.1145/3370748.3406564.\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, and Paolo Napoletano. 2018.\n“Benchmark Analysis of Representative Deep Neural Network\nArchitectures.” IEEE Access 6: 64270–77.\n\n\nBiega, Asia J., Peter Potash, Hal Daumé, Fernando Diaz, and Michèle\nFinck. 2020. “Operationalizing the Legal Principle of Data\nMinimization for Personalization.” In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, edited by Jimmy Huang, Yi Chang, Xueqi\nCheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, 399–408.\nACM. https://doi.org/10.1145/3397271.3401034.\n\n\nBiggio, Battista, Blaine Nelson, and Pavel Laskov. 2012.\n“Poisoning Attacks Against Support Vector Machines.” In\nProceedings of the 29th International Conference on Machine\nLearning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1,\n2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony\nSou, Catherine Ramsdale, Ken Williamson, Richard Price, and Scott White.\n2021. “A Natively Flexible 32-Bit Arm Microprocessor.”\nNature 595 (7868): 532–36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt,\nAli Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. “The Gem5\nSimulator.” ACM SIGARCH Computer Architecture News 39\n(2): 1–7. https://doi.org/10.1145/2024716.2024718.\n\n\nBohr, Adam, and Kaveh Memarzadeh. 2020. “The Rise of Artificial\nIntelligence in Healthcare Applications.” In Artificial\nIntelligence in Healthcare, 25–60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, and Alessandro Toschi.\n2023. “Fast and Accurate Error Simulation for CNNs\nAgainst Soft Errors.” IEEE Trans. Comput. 72 (4):\n984–97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital\nShah, Robert Hannaford, Arvind Iyer, Lucas Joppa, and Milind Tambe.\n2018. “Near Real-Time Detection of Poachers from Drones in\nAirSim.” In Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artificial Intelligence, edited\nby Jérôme Lang, 5814–16. International Joint Conferences on Artificial\nIntelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\n\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo,\nHengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas\nPapernot. 2021. “Machine Unlearning.” In 2021 IEEE\nSymposium on Security and Privacy (SP), 141–59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang\nLiu. 2018. “Deeplaser: Practical Fault Attack on Deep\nNeural Networks.” ArXiv Preprint abs/1806.05859. https://arxiv.org/abs/1806.05859.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language\nModels Are Few-Shot Learners.” In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.” In Conference on Fairness, Accountability\nand Transparency, 77–91. PMLR.\n\n\nBurnet, David, and Richard Thomas. 1989. “Spycatcher:\nThe Commodification of Truth.” J. Law Soc.\n16 (2): 210. https://doi.org/10.2307/1410360.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng,\nJau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. “Recent\nProgress in Phase-Change?Pub _Newline ?Memory\nTechnology.” IEEE Journal on Emerging and Selected Topics in\nCircuits and Systems 6 (2): 146–62. https://doi.org/10.1109/jetcas.2016.2547718.\n\n\nBushnell, Michael L, and Vishwani D Agrawal. 2002. “Built-in\nSelf-Test.” Essentials of Electronic Testing for Digital,\nMemory and Mixed-Signal VLSI Circuits, 489–548.\n\n\nBuyya, Rajkumar, Anton Beloglazov, and Jemal Abawajy. 2010.\n“Energy-Efficient Management of Data Center Resources for Cloud\nComputing: A Vision, Architectural Elements, and Open\nChallenges.” https://arxiv.org/abs/1006.0308.\n\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel\nSmilkov, Martin Wattenberg, et al. 2019. “Human-Centered Tools for\nCoping with Imperfect Algorithms During Medical Decision-Making.”\nIn Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems, edited by Jennifer G. Dy and Andreas Krause,\n80:2673–82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, and Song Han. 2020.\n“TinyTL: Reduce Memory, Not Parameters\nfor Efficient on-Device Learning.” In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nCai, Han, Ligeng Zhu, and Song Han. 2019.\n“ProxylessNAS: Direct Neural\nArchitecture Search on Target Task and Hardware.” In 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, and Richard M Ryan. 2020.\n“Supporting Human Autonomy in AI Systems:\nA Framework for Ethical Enquiry.” Ethics of\nDigital Well-Being: A Multidisciplinary Approach, 31–54.\n\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah\nSherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. “Hidden\nVoice Commands.” In 25th USENIX Security Symposium (USENIX\nSecurity 16), 513–30.\n\n\nCarlini, Nicolas, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash\nSehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace.\n2023. “Extracting Training Data from Diffusion Models.” In\n32nd USENIX Security Symposium (USENIX Security 23), 5253–70.\n\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero,\nand Roberto Saia. 2020. “A Local Feature Engineering Strategy to\nImprove Network Anomaly Detection.” Future Internet 12\n(10): 177. https://doi.org/10.3390/fi12100177.\n\n\nCavoukian, Ann. 2009. “Privacy by Design.” Office of\nthe Information and Privacy Commissioner.\n\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula\nCristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, and Pablo\nR. Dias. 2021. “Eco-Friendly\nElectronicsA Comprehensive Review.”\nAdv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\n\nChallenge, WEF Net-Zero. 2021. “The Supply Chain\nOpportunity.” In World Economic Forum: Geneva,\nSwitzerland.\n\n\nChandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. “Anomaly\nDetection: A Survey.” ACM Comput. Surv. 41 (3): 1–58. https://doi.org/10.1145/1541880.1541882.\n\n\nChapelle, O., B. Scholkopf, and A. Zien Eds. 2009.\n“Semi-Supervised Learning (Chapelle, O.\nEt Al., Eds.; 2006) [Book Reviews].” IEEE Trans.\nNeural Networks 20 (3): 542–42. https://doi.org/10.1109/tnn.2009.2015974.\n\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and\nJonathan Su. 2019. “This Looks Like That: Deep\nLearning for Interpretable Image Recognition.” In Advances in\nNeural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, edited by Hanna M. Wallach, Hugo Larochelle,\nAlina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman\nGarnett, 8928–39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, and Pranav\nRajpurkar. 2023. “A Framework for Integrating Artificial\nIntelligence for Clinical Care with Continuous Therapeutic\nMonitoring.” Nature Biomedical Engineering, November. https://doi.org/10.1038/s41551-023-01115-0.\n\n\nChen, H.-W. 2006. “Gallium, Indium, and Arsenic Pollution of\nGroundwater from a Semiconductor Manufacturing Area of\nTaiwan.” B. Environ. Contam. Tox. 77 (2):\n289–96. https://doi.org/10.1007/s00128-006-1062-3.\n\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,\nHaichen Shen, Meghan Cowan, et al. 2018. “TVM:\nAn Automated End-to-End Optimizing Compiler for Deep\nLearning.” In 13th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 18), 578–94.\n\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\n“Training Deep Nets with Sublinear Memory Cost.” ArXiv\nPreprint abs/1604.06174. https://arxiv.org/abs/1604.06174.\n\n\nChen, Zhiyong, and Shugong Xu. 2023. “Learning\nDomain-Heterogeneous Speaker Recognition Systems with Personalized\nContinual Federated Learning.” EURASIP Journal on Audio,\nSpeech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.\n\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben.\n2019. “iBinFI/i: An Efficient Fault\nInjector for Safety-Critical Machine Learning Systems.” In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis. SC ’19. New York, NY,\nUSA: ACM. https://doi.org/10.1145/3295500.3356177.\n\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik\nPattabiraman, and Nathan DeBardeleben. 2020.\n“TensorFI: A Flexible Fault Injection\nFramework for TensorFlow Applications.” In 2020\nIEEE 31st International Symposium on Software Reliability Engineering\n(ISSRE), 426–35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher,\nHyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. “Clear:\nuC/u Ross u-l/u Ayer uE/u Xploration for uA/u Rchitecting uR/u Esilience\n- Combining Hardware and Software Techniques to Tolerate Soft Errors in\nProcessor Cores.” In Proceedings of the 53rd Annual Design\nAutomation Conference, 1–6. ACM. https://doi.org/10.1145/2897937.2897996.\n\n\nCheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2018. “Model\nCompression and Acceleration for Deep Neural Networks: The\nPrinciples, Progress, and Challenges.” IEEE Signal Process\nMag. 35 (1): 126–36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu,\nYu Wang, and Yuan Xie. 2016. “Prime: A Novel Processing-in-Memory\nArchitecture for Neural Network Computation in ReRAM-Based Main\nMemory.” ACM SIGARCH Computer Architecture News 44 (3):\n27–39. https://doi.org/10.1145/3007787.3001140.\n\n\nChollet, François. 2018. “Introduction to Keras.” March\n9th.\n\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\nand Dario Amodei. 2017. “Deep Reinforcement Learning from Human\nPreferences.” In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, edited by Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S.\nV. N. Vishwanathan, and Roman Garnett, 4299–4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton,\nPieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew\nHoward. 2021. “Discovering Multi-Hardware Mobile Models via\nArchitecture Search.” In 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\nChua, L. 1971. “Memristor-the Missing Circuit Element.”\n#IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\n\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and\nMosharaf Chowdhury. 2023. “Perseus: Removing Energy\nBloat from Large Model Training.” ArXiv Preprint\nabs/2312.06902. https://arxiv.org/abs/2312.06902.\n\n\nCohen, Maxime C., Ruben Lobel, and Georgia Perakis. 2016. “The\nImpact of Demand Uncertainty on Consumer Subsidies for Green Technology\nAdoption.” Manage. Sci. 62 (5): 1235–58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter\nBailis, Alexander C. Berg, Robert D. Nowak, Roshan Sumbaly, Matei\nZaharia, and I. Zeki Yalniz. 2022. “Similarity Search for\nEfficient Active Learning and Search of Rare Concepts.” In\nThirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022,\nThirty-Fourth Conference on Innovative Applications of Artificial\nIntelligence, IAAI 2022, the Twelveth Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March\n1, 2022, 6402–10. AAAI Press. https://ojs.aaai.org/index.php/AAAI/article/view/20591.\n\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao,\nJian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia.\n2019. “Analysis of DAWNBench, a Time-to-Accuracy\nMachine Learning Performance Benchmark.” ACM SIGOPS Operating\nSystems Review 53 (1): 14–25. https://doi.org/10.1145/3352020.3352024.\n\n\nConstantinescu, Cristian. 2008. “Intermittent Faults and Effects\non Reliability of Integrated Circuits.” In 2008 Annual\nReliability and Maintainability Symposium, 370–74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\n\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien\nHumbert, and Lindsay Lessard. 2011. “A Semiconductor Company’s\nExamination of Its Water Footprint Approach.” In Proceedings\nof the 2011 IEEE International Symposium on Sustainable Systems and\nTechnology, 1–6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\n\n\nCope, Gord. 2009. “Pure Water, Semiconductors and the\nRecession.” Global Water Intelligence 10 (10).\n\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and\nYoshua Bengio. 2016. “Binarized Neural Networks:\nTraining Deep Neural Networks with Weights and Activations\nConstrained to+ 1 or-1.” arXiv Preprint\narXiv:1602.02830.\n\n\nCrankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E\nGonzalez, and Ion Stoica. 2017. “Clipper: A {Low-Latency} Online Prediction Serving System.”\nIn 14th USENIX Symposium on Networked Systems Design and\nImplementation (NSDI 17), 613–27.\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism.\nMIT press.\n\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, and Farinaz Koushanfar. 2017.\n“TinyDL: Just-in-time\nDeep Learning Solution for Constrained Embedded Systems.” In\n2017 IEEE International Symposium on Circuits and Systems\n(ISCAS), 1–4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana\nTurner, Carver Middleton, Will Carroll, et al. 2023. “Closing the\nWearable Gap: Footankle\nKinematic Modeling via Deep Learning Models Based on a Smart Sock\nWearable.” Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\n\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat\nJeffries, Jian Li, Nick Kreeger, et al. 2021. “Tensorflow Lite\nMicro: Embedded Machine Learning for Tinyml\nSystems.” Proceedings of Machine Learning and Systems 3:\n800–811.\n\n\nDavies, Emma. 2011. “Endangered Elements: Critical\nThinking.” https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya,\nYongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018.\n“Loihi: A Neuromorphic Manycore Processor with\non-Chip Learning.” IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya,\nGabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, and Sumedh R.\nRisbud. 2021. “Advancing Neuromorphic Computing with Loihi:\nA Survey of Results and Outlook.” Proc.\nIEEE 109 (5): 911–34. https://doi.org/10.1109/jproc.2021.3067593.\n\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, and Max\nSmolaks. 2022. “Uptime Institute Global Data Center Survey\n2022.” Uptime Institute.\n\n\nDayarathna, Miyuru, Yonggang Wen, and Rui Fan. 2016. “Data Center\nEnergy Consumption Modeling: A Survey.” IEEE\nCommunications Surveys &Amp; Tutorials 18 (1): 732–94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc\nV. Le, Mark Z. Mao, et al. 2012. “Large Scale Distributed Deep\nNetworks.” In Advances in Neural Information Processing\nSystems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake\nTahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando\nC. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q.\nWeinberger, 1232–40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li.\n2009. “ImageNet: A Large-Scale\nHierarchical Image Database.” In 2009 IEEE Conference on\nComputer Vision and Pattern Recognition, 248–55. IEEE. https://doi.org/10.1109/cvpr.2009.5206848.\n\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. “Five\nSafes: Designing Data Access for Research.”\nEconomics Working Paper Series 1601: 28.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding.” In\nProceedings of the 2019 Conference of the North, 4171–86.\nMinneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423.\n\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh\nKurup, and Mohak Shah. 2021. “A Survey of on-Device Machine\nLearning: An Algorithms and Learning Theory Perspective.” ACM\nTransactions on Internet of Things 2 (3): 1–49. https://doi.org/10.1145/3450494.\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T.\nKung, and Ziyun Li. 2022. “SplitNets:\nDesigning Neural Architectures for Efficient Distributed\nComputing on Head-Mounted Systems.” In 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\n12549–59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\nDongarra, Jack J. 2009. “The Evolution of High Performance\nComputing on System z.” IBM J. Res. Dev. 53: 3–4.\n\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi,\nShvetank Prakash, and Vijay Janapa Reddi. 2022.\n“FastML Science Benchmarks: Accelerating\nReal-Time Scientific Edge Machine Learning.” ArXiv\nPreprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\n\nDuchi, John C., Elad Hazan, and Yoram Singer. 2010. “Adaptive\nSubgradient Methods for Online Learning and Stochastic\nOptimization.” In COLT 2010 - the 23rd Conference on Learning\nTheory, Haifa, Israel, June 27-29, 2010, edited by Adam Tauman\nKalai and Mehryar Mohri, 257–69. Omnipress. http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.\n\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R\nBanbury, William Fu, Aleksandra Faust, Guido CHE de Croon, and Vijay\nJanapa Reddi. 2019. “Learning to Seek: Autonomous\nSource Seeking with Deep Reinforcement Learning Onboard a Nano Drone\nMicrocontroller.” ArXiv Preprint abs/1909.11236. https://arxiv.org/abs/1909.11236.\n\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa\nReddi, and Guido C. H. E. de Croon. 2021. “Sniffy Bug:\nA Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in\nCluttered Environments.” In 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), 9099–9106.\nIEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\n\n\nDürr, Marc, Gunnar Nissen, Kurt-Wolfram Sühs, Philipp Schwenkenbecher,\nChristian Geis, Marius Ringelstein, Hans-Peter Hartung, et al. 2021.\n“CSF Findings in Acute NMDAR and LGI1 Antibody–Associated\nAutoimmune Encephalitis.” Neurology Neuroimmunology &Amp;\nNeuroinflammation 8 (6). https://doi.org/10.1212/nxi.0000000000001086.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n“Calibrating Noise to Sensitivity in Private Data\nAnalysis.” In Theory of Cryptography, edited by Shai\nHalevi and Tal Rabin, 265–84. Berlin, Heidelberg: Springer Berlin\nHeidelberg.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations\nof Differential Privacy.” Foundations and Trends\nin Theoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nEbrahimi, Khosrow, Gerard F. Jones, and Amy S. Fleischer. 2014. “A\nReview of Data Center Cooling Technology, Operating Conditions and the\nCorresponding Low-Grade Waste Heat Recovery Opportunities.”\nRenewable Sustainable Energy Rev. 31 (March): 622–38. https://doi.org/10.1016/j.rser.2013.12.007.\n\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, and Shiping Chen. 2013.\n“A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart\nImplementations for High Performance Computing Systems.” The\nJournal of Supercomputing 65 (3): 1302–26. https://doi.org/10.1007/s11227-013-0884-0.\n\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere,\nRaghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and\nMurali Annavaram. 2022. “Check-n-Run: A Checkpointing\nSystem for Training Deep Learning Recommendation Models.” In\n19th USENIX Symposium on Networked Systems Design and Implementation\n(NSDI 22), 929–43.\n\n\nEldan, Ronen, and Mark Russinovich. 2023. “Who’s Harry Potter?\nApproximate Unlearning in LLMs.” ArXiv\nPreprint abs/2310.02238. https://arxiv.org/abs/2310.02238.\n\n\nEl-Rayis, A. O. 2014. “Reconfigurable Architectures for the Next\nGeneration of Mobile Device Telecommunications Systems.” :\nhttps://www.researchgate.net/publication/292608967.\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor\nLenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu.\n2023. “Training Spiking Neural Networks Using Lessons from Deep\nLearning.” Proc. IEEE 111 (9): 1016–54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M.\nSwetter, Helen M. Blau, and Sebastian Thrun. 2017.\n“Dermatologist-Level Classification of Skin Cancer with Deep\nNeural Networks.” Nature 542 (7639): 115–18. https://doi.org/10.1038/nature21056.\n\n\n“EuroSoil 2021 (O205).” 2021. In EuroSoil 2021\n(O205). DS12902. STMicroelectronics; Frontiers Media SA. https://doi.org/10.3389/978-2-88966-997-4.\n\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,\nChaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017.\n“Robust Physical-World Attacks on Deep Learning Models.”\nArXiv Preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo\nJindariani, Nhan Tran, Luca P. Carloni, et al. 2021. “Hls4ml:\nAn Open-Source Codesign Workflow to Empower Scientific\nLow-Power Machine Learning Devices.” https://arxiv.org/abs/2103.05579.\n\n\nFarah, Martha J. 2005. “Neuroethics: The Practical\nand the Philosophical.” Trends Cogn. Sci. 9 (1): 34–40.\nhttps://doi.org/10.1016/j.tics.2004.12.001.\n\n\nFarwell, James P., and Rafal Rohozinski. 2011. “Stuxnet and the\nFuture of Cyber War.” Survival 53 (1): 23–40. https://doi.org/10.1080/00396338.2011.555586.\n\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill,\nMing Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. “A Configurable\nCloud-Scale DNN Processor for Real-Time\nAI.” In 2018 ACM/IEEE 45th Annual International\nSymposium on Computer Architecture (ISCA), 1–14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard,\nIan Cassar, Dario Della Monica, and Anna Ingólfsdóttir. 2017. “A\nFoundation for Runtime Monitoring.” In International\nConference on Runtime Verification, 8–29. Springer.\n\n\nFrankle, Jonathan, and Michael Carbin. 2019. “The Lottery Ticket\nHypothesis: Finding Sparse, Trainable Neural\nNetworks.” In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\n\n\nFriedman, Batya. 1996. “Value-Sensitive Design.”\nInteractions 3 (6): 16–23. https://doi.org/10.1145/242485.242493.\n\n\nFurber, Steve. 2016. “Large-Scale Neuromorphic Computing\nSystems.” J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun,\nRodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey\nZaytsev, and Evgeny Burnaev. 2021. “Adversarial Attacks on Deep\nModels for Financial Transaction Records.” In Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery &Amp; Data\nMining, 2868–78. ACM. https://doi.org/10.1145/3447548.3467145.\n\n\nGale, Trevor, Erich Elsen, and Sara Hooker. 2019. “The State of\nSparsity in Deep Neural Networks.” ArXiv Preprint\nabs/1902.09574. https://arxiv.org/abs/1902.09574.\n\n\nGandolfi, Karine, Christophe Mourtel, and Francis Olivier. 2001.\n“Electromagnetic Analysis: Concrete Results.”\nIn Cryptographic Hardware and Embedded SystemsCHES\n2001: Third International Workshop Paris, France, May 1416,\n2001 Proceedings 3, 251–61. Springer.\n\n\nGannot, G., and M. Ligthart. 1994. “Verilog HDL Based\nFPGA Design.” In International Verilog HDL\nConference, 86–92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\n\nGao, Yansong, Said F. Al-Sarawi, and Derek Abbott. 2020. “Physical\nUnclonable Functions.” Nature Electronics 3 (2): 81–91.\nhttps://doi.org/10.1038/s41928-020-0372-5.\n\n\nGates, Byron D. 2009. “Flexible Electronics.”\nScience 323 (5921): 1566–67. https://doi.org/10.1126/science.1171230.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Commun. ACM 64 (12):\n86–92. https://doi.org/10.1145/3458723.\n\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts. 2021.\n“Causal Abstractions of Neural Networks.” In Advances\nin Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 9574–86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\n\nGholami, Dong Kim, Mahoney Yao, and Keutzer. 2021. “A Survey of\nQuantization Methods for Efficient Neural Network Inference).”\nArXiv Preprint. https://arxiv.org/abs/2103.13630.\n\n\nGlorot, Xavier, and Yoshua Bengio. 2010. “Understanding the\nDifficulty of Training Deep Feedforward Neural Networks.” In\nProceedings of the Thirteenth International Conference on Artificial\nIntelligence and Statistics, 249–56. http://proceedings.mlr.press/v9/glorot10a.html.\n\n\nGnad, Dennis R. E., Fabian Oboril, and Mehdi B. Tahoori. 2017.\n“Voltage Drop-Based Fault Attacks on FPGAs Using\nValid Bitstreams.” In 2017 27th International Conference on\nField Programmable Logic and Applications (FPL), 1–7. IEEE; IEEE.\nhttps://doi.org/10.23919/fpl.2017.8056840.\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.\n“Generative Adversarial Networks.” Commun. ACM 63\n(11): 139–44. https://doi.org/10.1145/3422622.\n\n\nGoodyear, Victoria A. 2017. “Social Media, Apps and Wearable\nTechnologies: Navigating Ethical Dilemmas and\nProcedures.” Qualitative Research in Sport, Exercise and\nHealth 9 (3): 285–302. https://doi.org/10.1080/2159676x.2017.1303790.\n\n\nGoogle. n.d. “Information Quality Content Moderation.” https://blog.google/documents/83/.\n\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang,\nand Edward Choi. 2018. “MorphNet: Fast\n&Amp; Simple Resource-Constrained Structure Learning of Deep\nNetworks.” In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 1586–95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\n\n\nGräfe, Ralf, Qutub Syed Sha, Florian Geissler, and Michael Paulitsch.\n2023. “Large-Scale Application of Fault Injection into\nPyTorch Models -an Extension to PyTorchFI for\nValidation Efficiency.” In 2023 53rd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks -\nSupplemental Volume (DSN-s), 56–62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\n\n\nGreengard, Samuel. 2015. The Internet of Things. The MIT Press.\nhttps://doi.org/10.7551/mitpress/10277.001.0001.\n\n\nGrossman, Elizabeth. 2007. High Tech Trash: Digital\nDevices, Hidden Toxics, and Human Health. Island press.\n\n\nGruslys, Audrunas, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex\nGraves. 2016. “Memory-Efficient Backpropagation Through\nTime.” In Advances in Neural Information Processing Systems\n29: Annual Conference on Neural Information Processing Systems 2016,\nDecember 5-10, 2016, Barcelona, Spain, edited by Daniel D. Lee,\nMasashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett,\n4125–33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\nGu, Ivy. 2023. “Deep Learning Model Compression (Ii) by Ivy Gu\nMedium.” https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\nGujarati, Arpan, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann,\nYmir Vigfusson, and Jonathan Mace. 2020. “Serving DNNs Like\nClockwork: Performance Predictability from the Bottom Up.” In\n14th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI 20), 443–62. https://www.usenix.org/conference/osdi20/presentation/gujarati.\n\n\nGuo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian\nWeinberger. 2019. “Simple Black-Box Adversarial Attacks.”\nIn International Conference on Machine Learning, 2484–93. PMLR.\n\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia,\nLi Yan, et al. 2019. “Mobile Photoplethysmographic Technology to\nDetect Atrial Fibrillation.” Journal of the American College\nof Cardiology 74 (19): 2365–75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and\nLopamudra Praharaj. 2023. “From ChatGPT to\nThreatGPT: Impact of Generative\nAI in Cybersecurity and Privacy.”\n#IEEE_O_ACC# 11: 80218–45. https://doi.org/10.1109/access.2023.3300381.\n\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin\nCanini, Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van\nEsbroeck. 2016. “Monotonic Calibrated Interpolated Look-up\nTables.” The Journal of Machine Learning Research 17\n(1): 3790–3836.\n\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee,\nDavid Brooks, and Carole-Jean Wu. 2022. “Act: Designing\nSustainable Computer Systems with an Architectural Carbon Modeling\nTool.” In Proceedings of the 49th Annual International\nSymposium on Computer Architecture, 784–99. ACM. https://doi.org/10.1145/3470496.3527408.\n\n\nGwennap, Linley. n.d. “Certus-NX Innovates\nGeneral-Purpose FPGAs.”\n\n\nHaensch, Wilfried, Tayfun Gokmen, and Ruchir Puri. 2019. “The Next\nGeneration of Deep Learning Hardware: Analog\nComputing.” Proc. IEEE 107 (1): 108–22. https://doi.org/10.1109/jproc.2018.2871057.\n\n\nHamming, R. W. 1950. “Error Detecting and Error Correcting\nCodes.” Bell Syst. Tech. J. 29 (2): 147–60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\nHan, Song, Huizi Mao, and William J Dally. 2015. “Deep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.” arXiv Preprint\narXiv:1510.00149.\n\n\nHan, Song, Huizi Mao, and William J. Dally. 2016. “Deep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.” https://arxiv.org/abs/1510.00149.\n\n\nHandlin, Oscar. 1965. “Science and Technology in Popular\nCulture.” Daedalus-Us., 156–70.\n\n\nHardt, Moritz, Eric Price, and Nati Srebro. 2016. “Equality of\nOpportunity in Supervised Learning.” In Advances in Neural\nInformation Processing Systems 29: Annual Conference on Neural\nInformation Processing Systems 2016, December 5-10, 2016, Barcelona,\nSpain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett, 3315–23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\n\n\nHawks, Benjamin, Javier Duarte, Nicholas J. Fraser, Alessandro\nPappalardo, Nhan Tran, and Yaman Umuroglu. 2021. “Ps and Qs: Quantization-aware Pruning for Efficient Low\nLatency Neural Network Inference.” Frontiers in Artificial\nIntelligence 4 (July). https://doi.org/10.3389/frai.2021.676564.\n\n\nHazan, Avi, and Elishai Ezra Tsur. 2021. “Neuromorphic Analog\nImplementation of Neural Engineering Framework-Inspired Spiking Neuron\nfor High-Dimensional Representation.” Front. Neurosci.\n15 (February): 627221. https://doi.org/10.3389/fnins.2021.627221.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.\n“Delving Deep into Rectifiers: Surpassing Human-Level Performance\non ImageNet Classification.” In 2015 IEEE International\nConference on Computer Vision (ICCV), 1026–34. IEEE. https://doi.org/10.1109/iccv.2015.123.\n\n\n———. 2016. “Deep Residual Learning for Image Recognition.”\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\n\nHe, Yi, Prasanna Balaprakash, and Yanjing Li. 2020.\n“FIdelity: Efficient Resilience Analysis\nFramework for Deep Learning Accelerators.” In 2020 53rd\nAnnual IEEE/ACM International Symposium on Microarchitecture\n(MICRO), 270–81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\n\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju,\nNishant Patil, and Yanjing Li. 2023. “Understanding and Mitigating\nHardware Failures in Deep Learning Training Systems.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1–16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\n\n\nHébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, and Guy N.\nRothblum. 2018. “Multicalibration: Calibration for\nthe (Computationally-Identifiable) Masses.” In\nProceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,\n2018, edited by Jennifer G. Dy and Andreas Krause, 80:1944–53.\nProceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\n\nHegde, Sumant. 2023. “An Introduction to Separable Convolutions -\nAnalytics Vidhya.” https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky,\nand Joelle Pineau. 2020. “Towards the Systematic Reporting of the\nEnergy and Carbon Footprints of Machine Learning.” The\nJournal of Machine Learning Research 21 (1): 10039–81.\n\n\nHendrycks, Dan, and Thomas Dietterich. 2019. “Benchmarking Neural\nNetwork Robustness to Common Corruptions and Perturbations.”\narXiv Preprint arXiv:1903.12261.\n\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn\nSong. 2021. “Natural Adversarial Examples.” In 2021\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 15262–71. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\n\nHennessy, John L., and David A. Patterson. 2019. “A New Golden Age\nfor Computer Architecture.” Commun. ACM 62 (2): 48–60.\nhttps://doi.org/10.1145/3282307.\n\n\nHimmelstein, Gracie, David Bates, and Li Zhou. 2022. “Examination\nof Stigmatizing Language in the Electronic Health Record.”\nJAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\n\n\nHinton, Geoffrey. 2005. “Van Nostrand’s Scientific Encyclopedia.” Wiley.\nhttps://doi.org/10.1002/0471743984.vse0673.\n\n\n———. 2017. “Overview of Minibatch Gradient Descent.”\nUniversity of Toronto; University Lecture.\n\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song,\nJun Yeong Seok, Kyung Jean Yoon, et al. 2012. “Frontiers in\nElectronic Materials.” Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nHoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and\nAlexandra Peste. 2021. “Sparsity in Deep Learning: Pruning and\nGrowth for Efficient Inference and Training in Neural Networks,”\nJanuary. http://arxiv.org/abs/2102.00554v1.\n\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia\nChmielinski. 2020. “The Dataset Nutrition Label: A Framework to\nDrive Higher Data Quality Standards.” In Data Protection and\nPrivacy. Hart Publishing. https://doi.org/10.5040/9781509932771.ch-001.\n\n\nHong, Sanghyun, Nicholas Carlini, and Alexey Kurakin. 2023.\n“Publishing Efficient on-Device Models Increases Adversarial\nVulnerability.” In 2023 IEEE Conference on Secure and\nTrustworthy Machine Learning (SaTML), 271–90. IEEE; IEEE. https://doi.org/10.1109/satml54575.2023.00026.\n\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, and Radha Poovendran.\n2017. “Deceiving Google’s Perspective Api Built for Detecting\nToxic Comments.” ArXiv Preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\n\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.\n“MobileNets: Efficient Convolutional\nNeural Networks for Mobile Vision Applications.” ArXiv\nPreprint. https://arxiv.org/abs/1704.04861.\n\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman\nMahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay\nJanapa Reddi. 2023. “MAVFI: An\nEnd-to-End Fault Analysis Framework with Anomaly Detection and Recovery\nfor Micro Aerial Vehicles.” In 2023 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\n\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting\nChan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, and\nYu-Min Tzou. 2016. “Accumulation of Heavy Metals and Trace\nElements in Fluvial Sediments Received Effluents from Traditional and\nSemiconductor Industries.” Scientific Reports 6 (1):\n34250. https://doi.org/10.1038/srep34250.\n\n\nHu, Jie, Li Shen, and Gang Sun. 2018. “Squeeze-and-Excitation\nNetworks.” In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 7132–41. IEEE. https://doi.org/10.1109/cvpr.2018.00745.\n\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, and Jian Shi. 2023.\n“Halide Perovskite Semiconductors.” Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi\nSekitani, Takao Someya, and Kwang-Ting Cheng. 2011.\n“Pseudo-CMOS: A Design Style for\nLow-Cost and Robust Flexible Electronics.” IEEE Trans.\nElectron Devices 58 (1): 141–50. https://doi.org/10.1109/ted.2010.2088127.\n\n\nHutter, Michael, Jorn-Marc Schmidt, and Thomas Plos. 2009.\n“Contact-Based Fault Injections and Power Analysis on\nRFID Tags.” In 2009 European Conference on\nCircuit Theory and Design, 409–12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf,\nWilliam J Dally, and Kurt Keutzer. 2016. “SqueezeNet:\nAlexnet-level Accuracy with 50x Fewer\nParameters and 0.5 MB Model Size.” ArXiv\nPreprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim\nHartley, and Luc Van Gool. 2018. “AI Benchmark:\nRunning Deep Neural Networks on Android\nSmartphones,” 0–0.\n\n\nImani, Mohsen, Abbas Rahimi, and Tajana S. Rosing. 2016.\n“Resistive Configurable Associative Memory for Approximate\nComputing.” In Proceedings of the 2016 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1327–32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\nIntelLabs. 2023. “Knowledge Distillation - Neural Network\nDistiller.” https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew\nJagielski, Katherine Lee, Christopher Choquette Choo, and Nicholas\nCarlini. 2023. “Preventing Generation of Verbatim Memorization in\nLanguage Models Gives a False Sense of Privacy.” In\nProceedings of the 16th International Natural Language Generation\nConference, 5253–70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nIrimia-Vladu, Mihai. 2014.\n““Green” Electronics:\nBiodegradable and Biocompatible Materials and Devices for\nSustainable Future.” Chem. Soc. Rev. 43 (2): 588–610. https://doi.org/10.1039/c3cs60235d.\n\n\nIsscc. 2014. “Computing’s Energy Problem (and What We Can Do about\nIt).” https://ieeexplore.ieee.org/document/6757323.\n\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,\nAndrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018.\n“Quantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference.” In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\n2704–13.\n\n\nJaderberg, Max, Valentin Dalibard, Simon Osindero, Wojciech M.\nCzarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, et al. 2017.\n“Population Based Training of Neural Networks.” arXiv\nPreprint arXiv:1711.09846, November. http://arxiv.org/abs/1711.09846v2.\n\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler,\nDaniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. “Edge\nImpulse: An MLOps Platform for Tiny Machine Learning.”\nProceedings of Machine Learning and Systems 5.\n\n\nJha, A. R. 2014. Rare Earth Materials: Properties and\nApplications. CRC Press. https://doi.org/10.1201/b17045.\n\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B.\nSullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, and Ravishankar K.\nIyer. 2019. “ML-Based Fault Injection for Autonomous\nVehicles: A Case for Bayesian Fault\nInjection.” In 2019 49th Annual IEEE/IFIP International\nConference on Dependable Systems and Networks (DSN), 112–24. IEEE;\nIEEE. https://doi.org/10.1109/dsn.2019.00025.\n\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\nLong, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014.\n“Caffe: Convolutional Architecture for Fast Feature\nEmbedding.” In Proceedings of the 22nd ACM International\nConference on Multimedia, 675–78. ACM. https://doi.org/10.1145/2647868.2654889.\n\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, and Daniele P. Scarpazza.\n2018. “Dissecting the NVIDIA Volta\nGPU Architecture via Microbenchmarking.” ArXiv\nPreprint. https://arxiv.org/abs/1804.06826.\n\n\nJia, Zhenge, Dawei Li, Xiaowei Xu, Na Li, Feng Hong, Lichuan Ping, and\nYiyu Shi. 2023. “Life-Threatening Ventricular Arrhythmia Detection\nChallenge in Implantable\nCardioverterdefibrillators.” Nature Machine\nIntelligence 5 (5): 554–55. https://doi.org/10.1038/s42256-023-00659-9.\n\n\nJia, Zhihao, Matei Zaharia, and Alex Aiken. 2019. “Beyond Data and\nModel Parallelism for Deep Neural Networks.” In Proceedings\nof Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA,\nMarch 31 - April 2, 2019, edited by Ameet Talwalkar, Virginia\nSmith, and Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, and Qiang Yang. 2020. “Towards\nUtilizing Unlabeled Data in Federated Learning: A Survey\nand Prospective.” arXiv Preprint arXiv:2002.11545.\n\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur\nSridhar, Karl Rosaen, and Ram Vasudevan. 2017. “Driving in the\nMatrix: Can Virtual Worlds Replace Human-Generated\nAnnotations for Real World Tasks?” In 2017 IEEE International\nConference on Robotics and Automation (ICRA), 746–53. Singapore,\nSingapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. “In-Datacenter\nPerformance Analysis of a Tensor Processing Unit.” In\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\n———, et al. 2017b. “In-Datacenter Performance Analysis of a Tensor\nProcessing Unit.” In Proceedings of the 44th Annual\nInternational Symposium on Computer Architecture, 1–12. ISCA ’17.\nNew York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng\nNai, Nishant Patil, et al. 2023. “TPU V4:\nAn Optically Reconfigurable Supercomputer for Machine\nLearning with Hardware Support for Embeddings.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture. ISCA ’23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\n\n\nJoye, Marc, and Michael Tunstall. 2012. Fault Analysis in\nCryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\n\nKairouz, Peter, Sewoong Oh, and Pramod Viswanath. 2015. “Secure\nMulti-Party Differential Privacy.” In Advances in Neural\nInformation Processing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, edited by Corinna Cortes, Neil D. Lawrence, Daniel\nD. Lee, Masashi Sugiyama, and Roman Garnett, 2008–16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das,\nKunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019.\n“A Study of BFLOAT16 for Deep Learning\nTraining.” https://arxiv.org/abs/1905.12322.\n\n\nKao, Sheng-Chun, Geonhwa Jeong, and Tushar Krishna. 2020.\n“ConfuciuX: Autonomous Hardware Resource\nAssignment for DNN Accelerators Using Reinforcement\nLearning.” In 2020 53rd Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO), 622–36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\n\nKao, Sheng-Chun, and Tushar Krishna. 2020. “Gamma: Automating the\nHW Mapping of DNN Models on Accelerators via Genetic Algorithm.”\nIn Proceedings of the 39th International Conference on\nComputer-Aided Design, 1–9. ACM. https://doi.org/10.1145/3400302.3415639.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. “Scaling Laws for Neural Language Models.”\nArXiv Preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nKarargyris, Alexandros, Renato Umeton, Micah J Sheller, Alejandro\nAristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023.\n“Federated Benchmarking of Medical Artificial Intelligence with\nMedPerf.” Nature Machine Intelligence 5\n(7): 799–810. https://doi.org/10.1038/s42256-023-00652-2.\n\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna\nWallach, and Jennifer Wortman Vaughan. 2020. “Interpreting\nInterpretability: Understanding Data Scientists’ Use of\nInterpretability Tools for Machine Learning.” In Proceedings\nof the 2020 CHI Conference on Human Factors in Computing Systems,\nedited by Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh\nAndres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14.\nACM. https://doi.org/10.1145/3313831.3376219.\n\n\nKawazoe Aguilera, Marcos, Wei Chen, and Sam Toueg. 1997.\n“Heartbeat: A Timeout-Free Failure Detector for\nQuiescent Reliable Communication.” In Distributed Algorithms:\n11th International Workshop, WDAG’97 Saarbrücken, Germany, September\n2426, 1997 Proceedings 11, 126–40. Springer.\n\n\nKhan, Mohammad Emtiyaz, and Siddharth Swaroop. 2021.\n“Knowledge-Adaptation Priors.” In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 19757–70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\nZhengxuan Wu, Bertie Vidgen, et al. 2021. “Dynabench:\nRethinking Benchmarking in NLP.” In\nProceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language\nTechnologies, 4110–24. Online: Association for Computational\nLinguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\n\nKim, Jungrae, Michael Sullivan, and Mattan Erez. 2015. “Bamboo\nECC: Strong, Safe, and Flexible Codes for\nReliable Computer Memory.” In 2015 IEEE 21st International\nSymposium on High Performance Computer Architecture (HPCA), 101–12.\nIEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk\nPark, Sangjun Choi, Seungwon Kim, Kwonchul Ha, and Won Kim. 2018.\n“Chemical Use in the Semiconductor Manufacturing Industry.”\nInt. J. Occup. Env. Heal. 24 (3-4): 109–18. https://doi.org/10.1080/10773525.2018.1519957.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A Method for\nStochastic Optimization.” Edited by Yoshua Bengio and Yann LeCun,\nDecember. http://arxiv.org/abs/1412.6980v9.\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\nGuillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017.\n“Overcoming Catastrophic Forgetting in Neural Networks.”\nProc. Natl. Acad. Sci. 114 (13): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n\n\nKo, Yohan. 2021. “Characterizing System-Level Masking Effects\nAgainst Soft Errors.” Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss,\nWerner Haas, Mike Hamburg, et al. 2019a. “Spectre Attacks:\nExploiting Speculative Execution.” In 2019 IEEE\nSymposium on Security and Privacy (SP). IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\n———, et al. 2019b. “Spectre Attacks: Exploiting\nSpeculative Execution.” In 2019 IEEE Symposium on Security\nand Privacy (SP). IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\nKocher, Paul, Joshua Jaffe, and Benjamin Jun. 1999. “Differential\nPower Analysis.” In Advances in\nCryptologyCRYPTO’99: 19th Annual International Cryptology\nConference Santa Barbara, California, USA, August 1519,\n1999 Proceedings 19, 388–97. Springer.\n\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, and Pankaj Rohatgi. 2011.\n“Introduction to Differential Power Analysis.” Journal\nof Cryptographic Engineering 1 (1): 5–27. https://doi.org/10.1007/s13389-011-0006-y.\n\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma\nPierson, Been Kim, and Percy Liang. 2020. “Concept Bottleneck\nModels.” In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\n119:5338–48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin\nZhang, Akshay Balsubramani, Weihua Hu, et al. 2021.\n“WILDS: A Benchmark of in-the-Wild\nDistribution Shifts.” In Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, edited by Marina Meila and Tong Zhang,\n139:5637–64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\n\nKoren, Yehuda, Robert Bell, and Chris Volinsky. 2009. “Matrix\nFactorization Techniques for Recommender Systems.”\nComputer 42 (8): 30–37. https://doi.org/10.1109/mc.2009.263.\n\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh\nDwivedi, André van Schaik, Mahesh Mehendale, and Chetan Singh Thakur.\n2023. “RAMAN: A Re-Configurable and\nSparse TinyML Accelerator for Inference on Edge.” https://arxiv.org/abs/2306.06493.\n\n\nKrishnamoorthi. 2018. “Quantizing Deep Convolutional Networks for\nEfficient Inference: A Whitepaper.” ArXiv\nPreprint. https://arxiv.org/abs/1806.08342.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022.\n“Self-Supervised Learning in Medicine and Healthcare.”\nNat. Biomed. Eng. 6 (12): 1346–52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang,\nIzzeddin Gur, Vijay Janapa Reddi, and Aleksandra Faust. 2022.\n“Multi-Agent Reinforcement Learning for Microprocessor Design\nSpace Exploration.” https://arxiv.org/abs/2211.16385.\n\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour,\nIkechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023.\n“ArchGym: An Open-Source Gymnasium for\nMachine Learning Assisted Architecture Design.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1–16. ACM. https://doi.org/10.1145/3579371.3589049.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012.\n“ImageNet Classification with Deep Convolutional\nNeural Networks.” In Advances in Neural Information\nProcessing Systems 25: 26th Annual Conference on Neural Information\nProcessing Systems 2012. Proceedings of a Meeting Held December 3-6,\n2012, Lake Tahoe, Nevada, United States, edited by Peter L.\nBartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou,\nand Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\n\n———. 2017. “ImageNet Classification with Deep\nConvolutional Neural Networks.” Edited by F. Pereira, C. J.\nBurges, L. Bottou, and K. Q. Weinberger. Commun. ACM 60 (6):\n84–90. https://doi.org/10.1145/3065386.\n\n\nKung, Hsiang Tsung, and Charles E Leiserson. 1979. “Systolic\nArrays (for VLSI).” In Sparse Matrix Proceedings\n1978, 1:256–82. Society for industrial; applied mathematics\nPhiladelphia, PA, USA.\n\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak,\nMorteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Anima\nAnandkumar. 2023. “FourCastNet:\nAccelerating Global High-Resolution Weather Forecasting\nUsing Adaptive Fourier Neural Operators.” In\nProceedings of the Platform for Advanced Scientific Computing\nConference, 1–11. ACM. https://doi.org/10.1145/3592979.3593412.\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,\nand Tijmen Blankevoort. 2022. “FP8 Quantization:\nThe Power of the Exponent.” https://arxiv.org/abs/2208.09225.\n\n\nKuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\nKrasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. “The Open\nImages Dataset V4: Unified Image Classification, Object\nDetection, and Visual Relationship Detection at Scale.”\nInternational Journal of Computer Vision 128 (7): 1956–81.\n\n\nKwon, Jisu, and Daejin Park. 2021. “Hardware/Software\nCo-Design for TinyML Voice-Recognition Application on\nResource Frugal Edge Devices.” Applied Sciences 11 (22):\n11073. https://doi.org/10.3390/app112211073.\n\n\nKwon, Sun Hwa, and Lin Dong. 2022. “Flexible Sensors and Machine\nLearning for Heart Monitoring.” Nano Energy 102\n(November): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\n\nKwon, Young D, Rui Li, Stylianos I Venieris, Jagmohan Chauhan, Nicholas\nD Lane, and Cecilia Mascolo. 2023. “TinyTrain:\nDeep Neural Network Training at the Extreme Edge.”\nArXiv Preprint abs/2307.09988. https://arxiv.org/abs/2307.09988.\n\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018a. “Cmsis-Nn:\nEfficient Neural Network Kernels for Arm Cortex-m\nCpus.” ArXiv Preprint abs/1801.06601. https://arxiv.org/abs/1801.06601.\n\n\n———. 2018b. “CMSIS-NN:\nEfficient Neural Network Kernels for Arm Cortex-m\nCPUs.” https://arxiv.org/abs/1801.06601.\n\n\nLakkaraju, Himabindu, and Osbert Bastani. 2020.\n“”How Do i Fool You?”:\nManipulating User Trust via Misleading Black Box Explanations.”\nIn Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety, 79–85. ACM. https://doi.org/10.1145/3375627.3375833.\n\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger,\nMeire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. “Learning\nSkillful Medium-Range Global Weather Forecasting.”\nScience 382 (6677): 1416–21. https://doi.org/10.1126/science.adi2336.\n\n\nLannelongue, Loı̈c, Jason Grealey, and Michael Inouye. 2021. “Green\nAlgorithms: Quantifying the Carbon Footprint of\nComputation.” Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\nLeCun, Yann, John Denker, and Sara Solla. 1989. “Optimal Brain\nDamage.” Adv Neural Inf Process Syst 2.\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang,\nand Seongik Cho. 2022. “Design of Radiation-Tolerant High-Speed\nSignal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear\nExplosion.” Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\nLeRoy Poff, N, MM Brinson, and JW Day. 2002. “Aquatic Ecosystems\n& Global Climate Change.” Pew Center on Global Climate\nChange.\n\n\nLi, En, Liekang Zeng, Zhi Zhou, and Xu Chen. 2020. “Edge\nAI: On-demand Accelerating Deep\nNeural Network Inference via Edge Computing.” IEEE Trans.\nWireless Commun. 19 (1): 447–57. https://doi.org/10.1109/twc.2019.2946140.\n\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai,\nKarthik Pattabiraman, Joel Emer, and Stephen W. Keckler. 2017.\n“Understanding Error Propagation in Deep Learning Neural Network\n(DNN) Accelerators and Applications.” In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, 1–12. ACM. https://doi.org/10.1145/3126908.3126964.\n\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, and\nZedong Nie. 2021. “Non-Invasive Monitoring of Three Glucose Ranges\nBased on ECG by Using DBSCAN-CNN.” IEEE Journal of Biomedical\nand Health Informatics 25 (9): 3340–50. https://doi.org/10.1109/jbhi.2021.3072628.\n\n\nLi, Mu, David G. Andersen, Alexander J. Smola, and Kai Yu. 2014.\n“Communication Efficient Distributed Machine Learning with the\nParameter Server.” In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Montreal, Quebec,\nCanada, edited by Zoubin Ghahramani, Max Welling, Corinna Cortes,\nNeil D. Lawrence, and Kilian Q. Weinberger, 19–27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu,\nand Bingsheng He. 2023. “A Survey on Federated Learning Systems:\nVision, Hype and Reality for Data Privacy and\nProtection.” IEEE Trans. Knowl. Data Eng. 35 (4):\n3347–66. https://doi.org/10.1109/tkde.2021.3124599.\n\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.\n“Federated Learning: Challenges, Methods, and Future\nDirections.” IEEE Signal Process Mag. 37 (3): 50–60. https://doi.org/10.1109/msp.2020.2975749.\n\n\nLi, Xiang, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016.\n“LightRNN: Memory and\nComputation-Efficient Recurrent Neural Networks.” In Advances\nin Neural Information Processing Systems 29: Annual Conference on Neural\nInformation Processing Systems 2016, December 5-10, 2016, Barcelona,\nSpain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett, 4385–93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\n\n\nLi, Yuhang, Xin Dong, and Wei Wang. 2020. “Additive Powers-of-Two\nQuantization: An Efficient Non-Uniform Discretization for\nNeural Networks.” In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\n\nLi, Zhizhong, and Derek Hoiem. 2018. “Learning Without\nForgetting.” IEEE Trans. Pattern Anal. Mach. Intell. 40\n(12): 2935–47. https://doi.org/10.1109/tpami.2017.2773081.\n\n\nLi, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin\nJin, Yanping Huang, et al. 2023. “{AlpaServe}:\nStatistical Multiplexing with Model Parallelism for Deep Learning\nServing.” In 17th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 23), 663–79.\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han.\n2020. “MCUNet: Tiny Deep Learning on\nIoT Devices.” In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song\nHan. 2022. “On-Device Training Under 256kb Memory.”\nAdv. Neur. In. 35: 22941–54.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, and Song Han. 2023.\n“Tiny Machine Learning: Progress and Futures Feature.”\nIEEE Circuits Syst. Mag. 23 (3): 8–34. https://doi.org/10.1109/mcas.2023.3302182.\n\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona,\nDeva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.\n“Microsoft Coco: Common Objects in Context.”\nIn Computer VisionECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part v 13,\n740–55. Springer.\n\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial\nIntelligence. Edward Elgar Publishing.\n\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, and Thomas B. Schon.\n2019. “Data Consistency Approach to Model Validation.”\n#IEEE_O_ACC# 7: 59788–96. https://doi.org/10.1109/access.2019.2915109.\n\n\nLindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008.\n“NVIDIA Tesla: A Unified Graphics and\nComputing Architecture.” IEEE Micro 28 (2): 39–55. https://doi.org/10.1109/mm.2008.31.\n\n\nLin, Tang Tang, Dang Yang, and Han Gan. 2023. “AWQ:\nActivation-aware Weight Quantization for\nLLM Compression and Acceleration.” ArXiv\nPreprint. https://arxiv.org/abs/2306.00978.\n\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, and Yun Tian.\n2020. “Energy Consumption and Emission Mitigation Prediction Based\non Data Center Traffic and PUE for Global Data\nCenters.” Global Energy Interconnection 3 (3): 272–82.\nhttps://doi.org/10.1016/j.gloei.2020.07.008.\n\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella\nJensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022.\n“Monitoring Gait at Home with Radio Waves in Parkinson’s Disease:\nA Marker of Severity, Progression, and Medication Response.”\nScience Translational Medicine 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\n\n\nLoh, Gabriel H. 2008. “3D-Stacked Memory\nArchitectures for Multi-Core Processors.” ACM SIGARCH\nComputer Architecture News 36 (3): 453–64. https://doi.org/10.1145/1394608.1382159.\n\n\nLopez-Paz, David, and Marc’Aurelio Ranzato. 2017. “Gradient\nEpisodic Memory for Continual Learning.” Adv Neural Inf\nProcess Syst 30.\n\n\nLou, Yin, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013.\n“Accurate Intelligible Models with Pairwise Interactions.”\nIn Proceedings of the 19th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, edited by Inderjit S. Dhillon,\nYehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh,\nJingrui He, Robert L. Grossman, and Ramasamy Uthurusamy, 623–31. ACM. https://doi.org/10.1145/2487575.2487579.\n\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, and\nAhmad Beirami. 2021. “Fermi: Fair Empirical Risk\nMinimization via Exponential Rényi Mutual Information.”\n\n\nLubana, Ekdeep Singh, and Robert P Dick. 2020. “A Gradient Flow\nFramework for Analyzing Network Pruning.” arXiv Preprint\narXiv:2009.11839.\n\n\nLuebke, David. 2008. “CUDA: Scalable\nParallel Programming for High-Performance Scientific Computing.”\nIn 2008 5th IEEE International Symposium on Biomedical Imaging: From\nNano to Macro, 836–38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to\nInterpreting Model Predictions.” In Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett,\n4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore,\nSriram Sankar, and Xun Jiao. 2024. “Dr.\nDNA: Combating Silent Data Corruptions in Deep\nLearning Using Distribution of Neuron Activations.” In\nProceedings of the 29th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Systems,\nVolume 3, 239–52. ACM. https://doi.org/10.1145/3620666.3651349.\n\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi\nJavanmard, Kathryn S. McKinley, and Colin Raffel. 2024. “Combining\nMachine Learning and Lifetime-Based Resource Management for Memory\nAllocation and Beyond.” Commun. ACM 67 (4): 87–96. https://doi.org/10.1145/3611018.\n\n\nMaass, Wolfgang. 1997. “Networks of Spiking Neurons:\nThe Third Generation of Neural Network Models.”\nNeural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,\nand Adrian Vladu. 2017. “Towards Deep Learning Models Resistant to\nAdversarial Attacks.” arXiv Preprint arXiv:1706.06083.\n\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez\nVicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, and Siva\nKumar Sastry Hari. 2020. “PyTorchFI: A\nRuntime Perturbation Tool for DNNs.” In 2020\n50th Annual IEEE/IFIP International Conference on Dependable Systems and\nNetworks Workshops (DSN-w), 25–31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\n\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher,\nSarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael\nB. Sullivan, Timothy Tsai, and Stephen W. Keckler. 2021.\n“Optimizing Selective Protection for CNN\nResilience.” In 2021 IEEE 32nd International Symposium on\nSoftware Reliability Engineering (ISSRE), 127–38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, and\nGu-Yeon Wei. 2022. “GoldenEye: A\nPlatform for Evaluating Emerging Numerical Data Formats in\nDNN Accelerators.” In 2022 52nd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks (DSN),\n206–14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\n\nMarković, Danijela, Alice Mizrahi, Damien Querlioz, and Julie Grollier.\n2020. “Physics for Neuromorphic Computing.” Nature\nReviews Physics 2 (9): 499–510. https://doi.org/10.1038/s42254-020-0208-2.\n\n\nMartin, C. Dianne. 1993. “The Myth of the Awesome Thinking\nMachine.” Commun. ACM 36 (4): 120–33. https://doi.org/10.1145/255950.153587.\n\n\nMarulli, Fiammetta, Stefano Marrone, and Laura Verde. 2022.\n“Sensitivity of Machine Learning Approaches to Fake and Untrusted\nData in Healthcare Domain.” Journal of Sensor and Actuator\nNetworks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy,\nKatrina Ligett, Terah Lyons, James Manyika, et al. 2023.\n“Artificial Intelligence Index Report 2023.” ArXiv\nPreprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg\nDiamos, David Kanter, Paulius Micikevicius, et al. 2020a.\n“MLPerf: An Industry Standard Benchmark\nSuite for Machine Learning Performance.” IEEE Micro 40\n(2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\n———, et al. 2020b. “MLPerf: An Industry\nStandard Benchmark Suite for Machine Learning Performance.”\nIEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan\nManuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021.\n“Multilingual Spoken Words Corpus.” In Thirty-Fifth\nConference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2).\n\n\nMcCarthy, John. 1981. “Epistemological Problems of Artificial\nIntelligence.” In Readings in Artificial Intelligence,\n459–65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\n\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise\nAgüera y Arcas. 2017. “Communication-Efficient Learning of Deep\nNetworks from Decentralized Data.” In Proceedings of the 20th\nInternational Conference on Artificial Intelligence and Statistics,\nAISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, edited by\nAarti Singh and Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine\nLearning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\nMiller, Charlie. 2019. “Lessons Learned from Hacking a\nCar.” IEEE Design &Amp; Test 36 (6): 7–9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\nMiller, Charlie, and Chris Valasek. 2015. “Remote Exploitation of\nan Unaltered Passenger Vehicle.” Black Hat USA 2015 (S\n91): 1–91.\n\n\nMiller, D. A. B. 2000. “Optical Interconnects to Silicon.”\n#IEEE_J_JSTQE# 6 (6): 1312–17. https://doi.org/10.1109/2944.902184.\n\n\nMills, Andrew, and Stephen Le Hunte. 1997. “An Overview of\nSemiconductor Photocatalysis.” J. Photochem. Photobiol.,\nA 108 (1): 1–35. https://doi.org/10.1016/s1010-6030(97)00118-4.\n\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang,\nEbrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. “A Graph\nPlacement Methodology for Fast Chip Design.” Nature 594\n(7862): 207–12. https://doi.org/10.1038/s41586-021-03544-w.\n\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan\nStosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021.\n“Accelerating Sparse Deep Neural Networks.” CoRR\nabs/2104.08378. https://arxiv.org/abs/2104.08378.\n\n\nMittal, Sparsh, Gaurav Verma, Brajesh Kaushik, and Farooq A. Khanday.\n2021. “A Survey of SRAM-Based in-Memory Computing\nTechniques and Applications.” J. Syst. Architect. 119\n(October): 102276. https://doi.org/10.1016/j.sysarc.2021.102276.\n\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos,\nRathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta,\net al. 2023. “Neural Inference at the Frontier of Energy, Space,\nand Time.” Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\n\nMohanram, K., and N. A. Touba. 2003. “Partial Error Masking to\nReduce Soft Error Failure Rate in Logic Circuits.” In\nProceedings. 16th IEEE Symposium on Computer Arithmetic,\n433–40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\nMonyei, Chukwuka G., and Kirsten E. H. Jenkins. 2018. “Electrons\nHave No Identity: Setting Right Misrepresentations in\nGoogle and Apple’s Clean Energy Purchasing.”\nEnergy Research &Amp; Social Science 46 (December): 48–51.\nhttps://doi.org/10.1016/j.erss.2018.06.015.\n\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim,\nand Ali Raad. 2023. “Reviewing Federated Learning Aggregation\nAlgorithms; Strategies, Contributions, Limitations and Future\nPerspectives.” Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\nMukherjee, S. S., J. Emer, and S. K. Reinhardt. 2005. “The Soft\nError Problem: An Architectural Perspective.” In\n11th International Symposium on High-Performance Computer\nArchitecture, 243–47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\n\n\nMunshi, Aaftab. 2009. “The OpenCL\nSpecification.” In 2009 IEEE Hot Chips 21 Symposium\n(HCS), 1–314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\n\nMusk, Elon et al. 2019. “An Integrated Brain-Machine Interface\nPlatform with Thousands of Channels.” J. Med. Internet\nRes. 21 (10): e16194. https://doi.org/10.2196/16194.\n\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen,\nand Tommi Mikkonen. 2022. “On Misbehaviour and Fault Tolerance in\nMachine Learning Systems.” J. Syst. Software 183\n(January): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\n\nNakano, Jane. 2021. The Geopolitics of Critical Minerals Supply\nChains. JSTOR.\n\n\nNarayanan, Arvind, and Vitaly Shmatikov. 2006. “How to Break\nAnonymity of the Netflix Prize Dataset.” arXiv Preprint\nCs/0610105.\n\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, and Maggie Shen\nQiao. 2021. “AI Literacy: Definition,\nTeaching, Evaluation and Ethical Issues.” Proceedings of the\nAssociation for Information Science and Technology 58 (1): 504–9.\n\n\nNgo, Richard, Lawrence Chan, and Sören Mindermann. 2022. “The\nAlignment Problem from a Deep Learning Perspective.” ArXiv\nPreprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and\nNgai-Man Cheung. 2023. “Re-Thinking Model Inversion Attacks\nAgainst Deep Neural Networks.” In 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 16384–93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\n\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li,\nJames Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021.\n“The Design Process for Google’s Training Chips:\nTpuv2 and TPUv3.” IEEE Micro\n41 (2): 56–63. https://doi.org/10.1109/mm.2021.3058217.\n\n\nNorthcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021.\n“Pervasive Label Errors in Test Sets Destabilize Machine Learning\nBenchmarks.” arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749\narXiv-issued DOI via DataCite.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil\nMullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used\nto Manage the Health of Populations.” Science 366\n(6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nOecd. 2023. “A Blueprint for Building National Compute Capacity\nfor Artificial Intelligence.” 350. Organisation for Economic\nCo-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\n\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael\nPetrov, and Shan Carter. 2020. “Zoom in: An\nIntroduction to Circuits.” Distill 5 (3): e00024–001. https://doi.org/10.23915/distill.00024.001.\n\n\nOliynyk, Daryna, Rudolf Mayer, and Andreas Rauber. 2023. “I Know\nWhat You Trained Last Summer: A Survey on Stealing Machine\nLearning Models and Defences.” ACM Comput. Surv. 55\n(14s): 1–41. https://doi.org/10.1145/3595292.\n\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, and Marco\nZennaro. 2021. “TinyML in Africa:\nOpportunities and Challenges.” In 2021 IEEE\nGlobecom Workshops (GC Wkshps), 1–6. IEEE; IEEE. https://doi.org/10.1109/gcwkshps52748.2021.9682107.\n\n\nOprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022.\n“Poisoning Attacks Against Machine Learning: Can\nMachine Learning Be Trustworthy?” Computer 55 (11):\n94–99. https://doi.org/10.1109/mc.2022.3190787.\n\n\nPan, Sinno Jialin, and Qiang Yang. 2010. “A Survey on Transfer\nLearning.” IEEE Trans. Knowl. Data Eng. 22 (10):\n1345–59. https://doi.org/10.1109/tkde.2009.191.\n\n\nPanda, Priyadarshini, Indranil Chakraborty, and Kaushik Roy. 2019.\n“Discretization Based Solutions for Secure Machine Learning\nAgainst Adversarial Attacks.” #IEEE_O_ACC# 7: 70157–68.\nhttps://doi.org/10.1109/access.2019.2919463.\n\n\nPapadimitriou, George, and Dimitris Gizopoulos. 2021.\n“Demystifying the System Vulnerability Stack:\nTransient Fault Effects Across the Layers.” In\n2021 ACM/IEEE 48th Annual International Symposium on Computer\nArchitecture (ISCA), 902–15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram\nSwami. 2016. “Distillation as a Defense to Adversarial\nPerturbations Against Deep Neural Networks.” In 2016 IEEE\nSymposium on Security and Privacy (SP), 582–97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\n\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max\nBartolo, Oana Inel, Juan Ciro, et al. 2023. “Adversarial Nibbler:\nA Data-Centric Challenge for Improving the Safety of\nText-to-Image Models.” ArXiv Preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\nPatterson, David A, and John L Hennessy. 2016. Computer Organization\nand Design ARM Edition: The Hardware Software\nInterface. Morgan kaufmann.\n\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang,\nLluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and\nJeff Dean. 2022. “The Carbon Footprint of Machine Learning\nTraining Will Plateau, Then Shrink.” Computer 55 (7):\n18–28. https://doi.org/10.1109/mc.2022.3148714.\n\n\nPeters, Dorian, Rafael A. Calvo, and Richard M. Ryan. 2018.\n“Designing for Motivation, Engagement and Wellbeing in Digital\nExperience.” Front. Psychol. 9 (May): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A\nBroniatowski, and Mark A Przybocki. 2020. “Four Principles of\nExplainable Artificial Intelligence.” Gaithersburg,\nMaryland 18.\n\n\nPlank, James S. 1997. “A Tutorial on\nReedSolomon Coding for Fault-Tolerance in\nRAID-Like Systems.” Software: Practice and\nExperience 27 (9): 995–1012.\n\n\nPont, Michael J, and Royan HL Ong. 2002. “Using Watchdog Timers to\nImprove the Reliability of Single-Processor Embedded Systems:\nSeven New Patterns and a Case Study.” In\nProceedings of the First Nordic Conference on Pattern Languages of\nPrograms, 159–200. Citeseer.\n\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan\nV. Green, Pete Warden, Tim Ansell, and Vijay Janapa Reddi. 2023.\n“CFU Playground: Full-stack Open-Source Framework for Tiny Machine\nLearning (TinyML) Acceleration on\nFPGAs.” In 2023 IEEE International Symposium on\nPerformance Analysis of Systems and Software (ISPASS). Vol.\nabs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete\nWarden, Brian Plancher, and Vijay Janapa Reddi. 2023. “Is\nTinyML Sustainable? Assessing the Environmental Impacts of\nMachine Learning on Microcontrollers.” ArXiv Preprint.\nhttps://arxiv.org/abs/2301.11899.\n\n\nPsoma, Sotiria D., and Chryso Kanthou. 2023. “Wearable Insulin\nBiosensors for Diabetes Management: Advances and Challenges.”\nBiosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022.\n“Data Cards: Purposeful and Transparent Dataset\nDocumentation for Responsible AI.” In 2022 ACM\nConference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros\nConstantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. “A\nReconfigurable Fabric for Accelerating Large-Scale Datacenter\nServices.” ACM SIGARCH Computer Architecture News 42\n(3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang,\nand Honggang Zhang. 2021. “An Efficient Pruning Scheme of Deep\nNeural Networks for Internet of Things Applications.” EURASIP\nJournal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\nQian, Yu, Xuegong Zhou, Hao Zhou, and Lingli Wang. 2024. “An\nEfficient Reinforcement Learning Based Framework for Exploring Logic\nSynthesis.” ACM Trans. Des. Autom. Electron. Syst. 29\n(2): 1–33. https://doi.org/10.1145/3632174.\n\n\nR. V., Rashmi, and Karthikeyan A. 2018. “Secure Boot of Embedded\nApplications - a Review.” In 2018 Second International\nConference on Electronics, Communication and Aerospace Technology\n(ICECA), 291–98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\nRachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler,\nMorgane Ayle, and Stephan Günnemann. 2022. “Winning the Lottery\nAhead of Time: Efficient Early Network Pruning.” In\nInternational Conference on Machine Learning, 18293–309. PMLR.\n\n\nRaina, Rajat, Anand Madhavan, and Andrew Y. Ng. 2009. “Large-Scale\nDeep Unsupervised Learning Using Graphics Processors.” In\nProceedings of the 26th Annual International Conference on Machine\nLearning, edited by Andrea Pohoreckyj Danyluk, Léon Bottou, and\nMichael L. Littman, 382:873–80. ACM International Conference Proceeding\nSeries. ACM. https://doi.org/10.1145/1553374.1553486.\n\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, and Olga Russakovsky.\n2023a. “Overlooked Factors in Concept-Based Explanations:\nDataset Choice, Concept Learnability, and Human\nCapability.” In 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 10932–41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\n\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, and Olga Russakovsky.\n2023b. “UFO: A Unified Method for\nControlling Understandability and Faithfulness Objectives in\nConcept-Based Explanations for CNNs.” ArXiv\nPreprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed,\nJames Legg, and David P. Hughes. 2017. “Deep Learning for\nImage-Based Cassava Disease Detection.” Front. Plant\nSci. 8 (October): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,\nAlec Radford, Mark Chen, and Ilya Sutskever. 2021. “Zero-Shot\nText-to-Image Generation.” In Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, edited by Marina Meila and Tong Zhang,\n139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\n\nRanganathan, Parthasarathy. 2011. “From Microprocessors to\nNanostores: Rethinking Data-Centric Systems.”\nComputer 44 (1): 39–48. https://doi.org/10.1109/mc.2011.18.\n\n\nRao, Ravi. 2021. “TinyML Unlocks New Possibilities\nfor Sustainable Development Technologies.”\nWww.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\n\n\nRashid, Layali, Karthik Pattabiraman, and Sathish Gopalakrishnan. 2012.\n“Intermittent Hardware Errors Recovery: Modeling and\nEvaluation.” In 2012 Ninth International Conference on\nQuantitative Evaluation of Systems, 220–29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\n\n\n———. 2015. “Characterizing the Impact of Intermittent Hardware\nFaults on Programs.” IEEE Trans. Reliab. 64 (1):\n297–310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and\nChristopher Ré. 2018. “Snorkel MeTaL: Weak\nSupervision for Multi-Task Learning.” In Proceedings of the\nSecond Workshop on Data Management for End-to-End Machine Learning.\nACM. https://doi.org/10.1145/3209889.3209898.\n\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu\nLee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. “Ares:\nA Framework for Quantifying the Resilience of Deep Neural\nNetworks.” In 2018 55th ACM/ESDA/IEEE Design Automation\nConference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael\nGelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. “A\nCase for Efficient Accelerator Design Space Exploration via\nBayesian Optimization.” In 2017 IEEE/ACM\nInternational Symposium on Low Power Electronics and Design\n(ISLPED), 1–6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\n\nReddi, Sashank J., Satyen Kale, and Sanjiv Kumar. 2019. “On the\nConvergence of Adam and Beyond.” arXiv Preprint\narXiv:1904.09237, April. http://arxiv.org/abs/1904.09237v1.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson,\nGuenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020.\n“MLPerf Inference Benchmark.” In 2020\nACM/IEEE 47th Annual International Symposium on Computer Architecture\n(ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\n\nReddi, Vijay Janapa, and Meeta Sharma Gupta. 2013. Resilient\nArchitecture Design for Voltage Variation. Springer International\nPublishing. https://doi.org/10.1007/978-3-031-01739-1.\n\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, and D. I. August.\n2005. “SWIFT: Software Implemented Fault\nTolerance.” In International Symposium on Code Generation and\nOptimization, 243–54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016.\n“” Why Should i Trust You?” Explaining\nthe Predictions of Any Classifier.” In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 1135–44.\n\n\nRobbins, Herbert, and Sutton Monro. 1951. “A Stochastic\nApproximation Method.” The Annals of Mathematical\nStatistics 22 (3): 400–407. https://doi.org/10.1214/aoms/1177729586.\n\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and\nBjorn Ommer. 2022. “High-Resolution Image Synthesis with Latent\nDiffusion Models.” In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\n\n\nRomero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, and Christos\nKozyrakis. 2021. “INFaaS: Automated Model-Less Inference\nServing.” In 2021 USENIX Annual Technical Conference (USENIX\nATC 21), 397–411. https://www.usenix.org/conference/atc21/presentation/romero.\n\n\nRosa, Gustavo H. de, and João P. Papa. 2021. “A Survey on Text\nGeneration Using Generative Adversarial Networks.” Pattern\nRecogn. 119 (November): 108098. https://doi.org/10.1016/j.patcog.2021.108098.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and\nRecognizing Automaton Project Para. Cornell Aeronautical\nLaboratory.\n\n\nRoskies, Adina. 2002. “Neuroethics for the New Millenium.”\nNeuron 35 (1): 21–23. https://doi.org/10.1016/s0896-6273(02)00763-8.\n\n\nRuder, Sebastian. 2016. “An Overview of Gradient Descent\nOptimization Algorithms.” ArXiv Preprint abs/1609.04747\n(September). http://arxiv.org/abs/1609.04747v2.\n\n\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning\nModels for High Stakes Decisions and Use Interpretable Models\nInstead.” Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,\nSean Ma, Zhiheng Huang, et al. 2015. “ImageNet Large\nScale Visual Recognition Challenge.” Int. J. Comput.\nVision 115 (3): 211–52. https://doi.org/10.1007/s11263-015-0816-y.\n\n\nRussell, Stuart. 2021. “Human-Compatible Artificial\nIntelligence.” Human-Like Machine Intelligence, 3–23.\n\n\nRyan, Richard M., and Edward L. Deci. 2000. “Self-Determination\nTheory and the Facilitation of Intrinsic Motivation, Social Development,\nand Well-Being.” Am. Psychol. 55 (1): 68–78. https://doi.org/10.1037/0003-066x.55.1.68.\n\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar\nKrishna. 2018. “Scale-Sim: Systolic Cnn Accelerator\nSimulator.” ArXiv Preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,\nPraveen Paritosh, and Lora M Aroyo. 2021a.\n““Everyone Wants to Do the Model Work,\nNot the Data Work”: Data Cascades in\nHigh-Stakes AI.” In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems, 1–15.\n\n\n———. 2021b. “‘Everyone Wants to Do the Model Work, Not the\nData Work’: Data Cascades in High-Stakes AI.” In\nProceedings of the 2021 CHI Conference on Human Factors in Computing\nSystems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSangchoolie, Behrooz, Karthik Pattabiraman, and Johan Karlsson. 2017.\n“One Bit Is (Not) Enough: An Empirical\nStudy of the Impact of Single and Multiple Bit-Flip Errors.” In\n2017 47th Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 97–108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\n\nSchäfer, Mike S. 2023. “The Notorious GPT:\nScience Communication in the Age of Artificial\nIntelligence.” Journal of Science Communication 22 (02):\nY02. https://doi.org/10.22323/2.22020402.\n\n\nSchizas, Nikolaos, Aristeidis Karras, Christos Karras, and Spyros\nSioutas. 2022. “TinyML for Ultra-Low Power\nAI and Large Scale IoT Deployments:\nA Systematic Review.” Future Internet 14\n(12): 363. https://doi.org/10.3390/fi14120363.\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker\nMitchell, Prasanna Date, and Bill Kay. 2022. “Opportunities for\nNeuromorphic Computing Algorithms and Applications.” Nature\nComputational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, and\nAndreas Paepcke. 2021. “Deployment of Embedded\nEdge-AI for Wildlife Monitoring in Remote Regions.”\nIn 2021 20th IEEE International Conference on Machine Learning and\nApplications (ICMLA), 1035–42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020.\n“Green AI.” Commun. ACM 63 (12):\n54–63. https://doi.org/10.1145/3381831.\n\n\nSegal, Mark, and Kurt Akeley. 1999. “The OpenGL\nGraphics System: A Specification (Version 1.1).”\n\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, and P. W. C.\nPrasad. 2017. “Ethical Implications of User Perceptions of\nWearable Devices.” Sci. Eng. Ethics 24 (1): 1–28. https://doi.org/10.1007/s11948-017-9872-8.\n\n\nSeide, Frank, and Amit Agarwal. 2016. “Cntk: Microsoft’s\nOpen-Source Deep-Learning Toolkit.” In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 2135–35. ACM. https://doi.org/10.1145/2939672.2945397.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna\nVedantam, Devi Parikh, and Dhruv Batra. 2017.\n“Grad-CAM: Visual Explanations from Deep\nNetworks via Gradient-Based Localization.” In 2017 IEEE\nInternational Conference on Computer Vision (ICCV), 618–26. IEEE.\nhttps://doi.org/10.1109/iccv.2017.74.\n\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers,\nand Hsien-Hsin S. Lee. 2010. “SAFER: Stuck-at-fault Error Recovery for\nMemories.” In 2010 43rd Annual IEEE/ACM International\nSymposium on Microarchitecture, 115–24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, and Marc Roper.\n2018. “Machine Learning for Estimation of Building Energy\nConsumption and Performance: A Review.”\nVisualization in Engineering 6 (1): 1–20. https://doi.org/10.1186/s40327-018-0064-7.\n\n\nShalev-Shwartz, Shai, Shaked Shammah, and Amnon Shashua. 2017. “On\na Formal Model of Safe and Scalable Self-Driving Cars.” ArXiv\nPreprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y\nZhao. 2023. “Prompt-Specific Poisoning Attacks on Text-to-Image\nGenerative Models.” ArXiv Preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H.\nP. Pernice, Harish Bhaskaran, C. D. Wright, and Paul R. Prucnal. 2021.\n“Photonics for Artificial Intelligence and Neuromorphic\nComputing.” Nat. Photonics 15 (2): 102–14. https://doi.org/10.1038/s41566-020-00754-y.\n\n\nSheaffer, Jeremy W, David P Luebke, and Kevin Skadron. 2007. “A\nHardware Redundancy and Recovery Mechanism for Reliable Scientific\nComputation on Graphics Processors.” In Graphics\nHardware, 2007:55–64. Citeseer.\n\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin,\nJonathan Koomey, Eric Masanet, Nathaniel Horner, Inês Azevedo, and\nWilliam Lintner. 2016. “United States Data Center Energy Usage\nReport.”\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichael W. Mahoney, and Kurt Keutzer. 2020. “Q-BERT:\nHessian Based Ultra Low Precision Quantization of\nBERT.” Proceedings of the AAAI Conference on\nArtificial Intelligence 34 (05): 8815–21. https://doi.org/10.1609/aaai.v34i05.6409.\n\n\nSheng, Victor S., and Jing Zhang. 2019. “Machine Learning with\nCrowdsourcing: A Brief Summary of the Past Research and\nFuture Directions.” Proceedings of the AAAI Conference on\nArtificial Intelligence 33 (01): 9837–43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\n\nShi, Hongrui, and Valentin Radu. 2022. “Data Selection for\nEfficient Model Update in Federated Learning.” In Proceedings\nof the 2nd European Workshop on Machine Learning and Systems,\n72–78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\nShneiderman, Ben. 2020. “Bridging the Gap Between Ethics and\nPractice: Guidelines for Reliable, Safe, and Trustworthy Human-Centered\nAI Systems.” ACM Trans. Interact. Intell. Syst. 10 (4):\n1–31. https://doi.org/10.1145/3419764.\n\n\n———. 2022. Human-Centered AI. Oxford University\nPress.\n\n\nShokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.\n2017. “Membership Inference Attacks Against Machine Learning\nModels.” In 2017 IEEE Symposium on Security and Privacy\n(SP), 3–18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\n\nSiddik, Md Abu Bakar, Arman Shehabi, and Landon Marston. 2021.\n“The Environmental Footprint of Data Centers in the United\nStates.” Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, and Alexandre\nAntonelli. 2022. “Improving Biodiversity Protection Through\nArtificial Intelligence.” Nature Sustainability 5 (5):\n415–24. https://doi.org/10.1038/s41893-022-00851-6.\n\n\nSingh, Narendra, and Oladele A. Ogunseitan. 2022. “Disentangling\nthe Worldwide Web of e-Waste and Climate Change Co-Benefits.”\nCircular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\n\n\nSkorobogatov, Sergei. 2009. “Local Heating Attacks on Flash Memory\nDevices.” In 2009 IEEE International Workshop on\nHardware-Oriented Security and Trust, 1–6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\n\nSkorobogatov, Sergei P, and Ross J Anderson. 2003. “Optical Fault\nInduction Attacks.” In Cryptographic Hardware and Embedded\nSystems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA,\nAugust 1315, 2002 Revised Papers 4, 2–12. Springer.\n\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin\nWattenberg. 2017. “Smoothgrad: Removing Noise by\nAdding Noise.” ArXiv Preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\n\nSnoek, Jasper, Hugo Larochelle, and Ryan P. Adams. 2012.\n“Practical Bayesian Optimization of Machine Learning\nAlgorithms.” In Advances in Neural Information Processing\nSystems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake\nTahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando\nC. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q.\nWeinberger, 2960–68. https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html.\n\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever,\nand Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent\nNeural Networks from Overfitting.” J. Mach. Learn. Res.\n15 (1): 1929–58. https://doi.org/10.5555/2627435.2670313.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy\nand Policy Considerations for Deep Learning in NLP.”\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 3645–50. Florence, Italy: Association\nfor Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\n\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma,\nSarma Vrudhula, Jae-sun Seo, and Yu Cao. 2016.\n“Throughput-Optimized OpenCL-Based FPGA\nAccelerator for Large-Scale Convolutional Neural Networks.” In\nProceedings of the 2016 ACM/SIGDA International Symposium on\nField-Programmable Gate Arrays, 16–25. ACM. https://doi.org/10.1145/2847263.2847276.\n\n\nSudhakar, Soumya, Vivienne Sze, and Sertac Karaman. 2023. “Data\nCenters on Wheels: Emissions from Computing Onboard\nAutonomous Vehicles.” IEEE Micro 43 (1): 29–39. https://doi.org/10.1109/mm.2022.3219803.\n\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017.\n“Efficient Processing of Deep Neural Networks: A\nTutorial and Survey.” Proc. IEEE 105 (12): 2295–2329. https://doi.org/10.1109/jproc.2017.2761740.\n\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\nDumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.\n“Intriguing Properties of Neural Networks.” In 2nd\nInternational Conference on Learning Representations, ICLR 2014, Banff,\nAB, Canada, April 14-16, 2014, Conference Track Proceedings, edited\nby Yoshua Bengio and Yann LeCun. http://arxiv.org/abs/1312.6199.\n\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa\nReddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020.\n“Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings\nfor Resilient Deep Learning Inference.” In 2020 57th ACM/IEEE\nDesign Automation Conference (DAC), 1–6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler,\nAndrew Howard, and Quoc V. Le. 2019. “MnasNet: Platform-aware Neural Architecture Search for\nMobile.” In 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2820–28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\n\nTan, Mingxing, and Quoc V. Le. 2023. “Demystifying Deep\nLearning.” Wiley. https://doi.org/10.1002/9781394205639.ch6.\n\n\nTang, Xin, Yichun He, and Jia Liu. 2022. “Soft Bioelectronics for\nCardiac Interfaces.” Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\n\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, and Jia Liu. 2023.\n“Flexible Braincomputer Interfaces.”\nNature Electronics 6 (2): 109–18. https://doi.org/10.1038/s41928-022-00913-9.\n\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, and Mohan\nKankanhalli. 2022. “Deep Regression Unlearning.” ArXiv\nPreprint abs/2210.08196. https://arxiv.org/abs/2210.08196.\n\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad\nAlmahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et\nal. 2016. “Theano: A Python Framework for Fast\nComputation of Mathematical Expressions.” https://arxiv.org/abs/1605.02688.\n\n\n“The Ultimate Guide to Deep Learning Model Quantization and\nQuantization-Aware Training.” n.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F.\nManso. 2021. “Deep Learning’s Diminishing Returns:\nThe Cost of Improvement Is Becoming Unsustainable.”\nIEEE Spectr. 58 (10): 50–55. https://doi.org/10.1109/mspec.2021.9563954.\n\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, and Samuel B. Fey. 2019.\n“Fish Die-Offs Are Concurrent with Thermal Extremes in North\nTemperate Lakes.” Nat. Clim. Change 9 (8): 637–41. https://doi.org/10.1038/s41558-019-0520-y.\n\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, and Rameshwar S. Kanwar.\n2022. “Indonesia Rice Irrigation System:\nTime for Innovation.” Sustainability 14\n(19): 12477. https://doi.org/10.3390/su141912477.\n\n\nTokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa,\nShunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki\nYamazaki Vincent. 2019. “Chainer: A Deep Learning Framework for\nAccelerating the Research Cycle.” In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery &Amp;\nData Mining, 5:1–6. ACM. https://doi.org/10.1145/3292500.3330756.\n\n\nTramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, and Dan\nBoneh. 2019. “AdVersarial: Perceptual Ad Blocking\nMeets Adversarial Machine Learning.” In Proceedings of the\n2019 ACM SIGSAC Conference on Computer and Communications Security,\n2005–21. ACM. https://doi.org/10.1145/3319535.3354222.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022.\n“Pruning Has a Disparate Impact on Model Accuracy.” Adv\nNeural Inf Process Syst 35: 17652–64.\n\n\nTsai, Min-Jen, Ping-Yi Lin, and Ming-En Lee. 2023. “Adversarial\nAttacks on Medical Image Classification.” Cancers 15\n(17): 4228. https://doi.org/10.3390/cancers15174228.\n\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa,\nand Stephen W. Keckler. 2021. “NVBitFI:\nDynamic Fault Injection for GPUs.” In\n2021 51st Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 284–91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\n\nUddin, Mueen, and Azizah Abdul Rahman. 2012. “Energy Efficiency\nand Low Carbon Enabler Green IT Framework for Data Centers\nConsidering Green Metrics.” Renewable Sustainable Energy\nRev. 16 (6): 4078–94. https://doi.org/10.1016/j.rser.2012.03.014.\n\n\nUn, and World Economic Forum. 2019. A New Circular Vision for\nElectronics, Time for a Global Reboot. PACE - Platform for\nAccelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\nValenzuela, Christine L, and Pearl Y Wang. 2000. “A Genetic\nAlgorithm for VLSI Floorplanning.” In Parallel\nProblem Solving from Nature PPSN VI: 6th International Conference Paris,\nFrance, September 1820, 2000 Proceedings 6, 671–80.\nSpringer.\n\n\nVan Noorden, Richard. 2016. “ArXiv Preprint Server\nPlans Multimillion-Dollar Overhaul.” Nature 534 (7609):\n602–2. https://doi.org/10.1038/534602a.\n\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar,\nRam Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, and\nChris H. Kim. 2021. “Wide-Range Many-Core SoC Design\nin Scaled CMOS: Challenges and\nOpportunities.” IEEE Trans. Very Large Scale Integr. VLSI\nSyst. 29 (5): 843–56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Adv Neural Inf Process\nSyst 30.\n\n\n“Vector-Borne Diseases.” n.d.\nhttps://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\n\n\nVelazco, Raoul, Gilles Foucard, and Paul Peronnard. 2010.\n“Combining Results of Accelerated Radiation Tests and Fault\nInjections to Predict the Error Rate of an Application Implemented in\nSRAM-Based FPGAs.” IEEE Trans.\nNucl. Sci. 57 (6): 3500–3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay,\nLung-Yen Chen, Bonan Zhang, and Peter Deaville. 2019. “In-Memory\nComputing: Advances and Prospects.” IEEE\nSolid-State Circuits Mag. 11 (3): 43–55. https://doi.org/10.1109/mssc.2019.2922889.\n\n\nVerma, Team Dual_Boot: Swapnil. 2022. “Elephant\nAI.” Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\n\n\nVinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam,\nVirginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans,\nMax Tegmark, and Francesco Fuso Nerini. 2020. “The Role of\nArtificial Intelligence in Achieving the Sustainable Development\nGoals.” Nat. Commun. 11 (1): 1–10. https://doi.org/10.1038/s41467-019-14108-y.\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar\nFuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021.\n“IntAct: A 96-Core Processor with Six\nChiplets 3D-Stacked on an Active Interposer with\nDistributed Interconnects and Integrated Power Management.”\nIEEE J. Solid-State Circuits 56 (1): 79–97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.\n“Counterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR.”\nSSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\n\nWald, Peter H., and Jeffrey R. Jones. 1987. “Semiconductor\nManufacturing: An Introduction to Processes and\nHazards.” Am. J. Ind. Med. 11 (2): 203–21. https://doi.org/10.1002/ajim.4700110209.\n\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi,\nand Arijit Raychowdhury. 2021. “Analyzing and Improving Fault\nTolerance of Learning-Based Navigation Systems.” In 2021 58th\nACM/IEEE Design Automation Conference (DAC), 841–46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\n\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, and Y Zhu. 2023.\n“Vpp: The Vulnerability-Proportional Protection\nParadigm Towards Reliable Autonomous Machines.” In\nProceedings of the 5th International Workshop on Domain Specific\nSystem Architecture (DOSSA), 1–6.\n\n\nWang, LingFeng, and YaQing Zhan. 2019a. “A Conceptual Peer Review\nModel for arXiv and Other Preprint\nDatabases.” Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\n\n———. 2019b. “A Conceptual Peer Review Model for arXiv and Other Preprint Databases.”\nLearn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang,\nYujun Lin, and Song Han. 2020. “APQ:\nJoint Search for Network Architecture, Pruning and\nQuantization Policy.” In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2075–84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\n\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for\nLimited-Vocabulary Speech Recognition.” arXiv Preprint\narXiv:1804.03209.\n\n\nWarden, Pete, and Daniel Situnayake. 2019. Tinyml:\nMachine Learning with Tensorflow Lite on Arduino and\nUltra-Low-Power Microcontrollers. O’Reilly Media.\n\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital\nComputing Systems. Ballistic Research Laboratories.\n\n\nWeiser, Mark. 1991. “The Computer for the 21st Century.”\nSci. Am. 265 (3): 94–104. https://doi.org/10.1038/scientificamerican0991-94.\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, and Anvesh Nookala.\n2020. “ANNETTE: Accurate Neural Network\nExecution Time Estimation with Stacked Models.” IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nWiener, Norbert. 1960. “Some Moral and Technical Consequences of\nAutomation: As Machines Learn They May Develop Unforeseen Strategies at\nRates That Baffle Their Programmers.” Science 131\n(3410): 1355–58. https://doi.org/10.1126/science.131.3410.1355.\n\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva\nGurumurthi, and David R. Kaeli. 2014. “Calculating Architectural\nVulnerability Factors for Spatial Multi-Bit Transient Faults.” In\n2014 47th Annual IEEE/ACM International Symposium on\nMicroarchitecture, 293–305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\n\n\nWinkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilariño,\nSivan Kartha, and Joana Portugal-Pereira. 2022. “Examples of\nShifting Development Pathways: Lessons on How to Enable\nBroader, Deeper, and Faster Climate Action.” Climate\nAction 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu,\nPang-Shiu Chen, Byoungil Lee, Frederick T. Chen, and Ming-Jinn Tsai.\n2012. “MetalOxide\nRRAM.” Proc. IEEE 100 (6): 1951–70. https://doi.org/10.1109/jproc.2012.2190369.\n\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, and Yangqing Jia. 2019.\n“FBNet: Hardware-aware\nEfficient ConvNet Design via Differentiable Neural\nArchitecture Search.” In 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 10734–42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nWu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury,\nMarat Dukhan, Kim Hazelwood, et al. 2019. “Machine Learning at\nFacebook: Understanding Inference at the Edge.” In 2019 IEEE\nInternational Symposium on High Performance Computer Architecture\n(HPCA), 331–44. IEEE; IEEE. https://doi.org/10.1109/hpca.2019.00048.\n\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha\nArdalani, Kiwan Maeng, Gloria Chang, et al. 2022. “Sustainable Ai:\nEnvironmental Implications, Challenges and\nOpportunities.” Proceedings of Machine Learning and\nSystems 4: 795–813.\n\n\nWu, Zhang Judd, and Micikevicius Isaev. 2020. “Integer\nQuantization for Deep Learning Inference: Principles and\nEmpirical Evaluation).” ArXiv Preprint. https://arxiv.org/abs/2004.09602.\n\n\nXiao, Seznec Lin, Demouth Wu, and Han. 2022.\n“SmoothQuant: Accurate and Efficient\nPost-Training Quantization for Large Language Models.” ArXiv\nPreprint. https://arxiv.org/abs/2211.10438.\n\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and\nQuoc V. Le. 2020. “Adversarial Examples Improve Image\nRecognition.” In 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 816–25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\n\n\nXie, Saining, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He.\n2017. “Aggregated Residual Transformations for Deep Neural\nNetworks.” In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 1492–1500. IEEE. https://doi.org/10.1109/cvpr.2017.634.\n\n\nXinyu, Chen. n.d.\n\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei\nCao, Xuegong Zhou, et al. 2021. “MRI-Based Brain\nTumor Segmentation Using FPGA-Accelerated Neural\nNetwork.” BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\n\n\nXiu, Liming. 2019. “Time Moore: Exploiting Moore’s Law from the Perspective of Time.”\nIEEE Solid-State Circuits Mag. 11 (1): 39–55. https://doi.org/10.1109/mssc.2018.2882285.\n\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong\nWang, and Hongbin Zha. 2018. “Alternating Multi-Bit Quantization\nfor Recurrent Neural Networks.” In 6th International\nConference on Learning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes,\nVasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph\nFeichtenhofer. 2023. “Demystifying CLIP Data.”\nArXiv Preprint abs/2309.16671. https://arxiv.org/abs/2309.16671.\n\n\nXu, Ying, Xu Zhong, Antonio Jimeno Yepes, and Jey Han Lau. 2021.\n“Grey-Box Adversarial Attack and Defence for\nSentiment Classification.” arXiv Preprint\narXiv:2103.11576.\n\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A Choquette-Choo,\nPeter Kairouz, H Brendan McMahan, Jesse Rosenstock, and Yuanbo Zhang.\n2023. “Federated Learning of Gboard Language Models with\nDifferential Privacy.” ArXiv Preprint abs/2305.18465. https://arxiv.org/abs/2305.18465.\n\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Françoise Beaufays, Rajiv\nMathews, and Mingqing Chen. 2023. “Online Model Compression for\nFederated Learning with Large Models.” In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 1–5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric\nTan, Leyuan Wang, et al. 2021. “Hawq-V3: Dyadic\nNeural Network Quantization.” In International Conference on\nMachine Learning, 11875–86. PMLR.\n\n\nYe, Linfeng, and Shayan Mohajer Hamidi. 2021. “Thundernna:\nA White Box Adversarial Attack.” arXiv Preprint\narXiv:2111.12305.\n\n\nYeh, Y. C. 1996. “Triple-Triple Redundant 777 Primary Flight\nComputer.” In 1996 IEEE Aerospace Applications Conference.\nProceedings, 1:293–307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\n\n\nYik, Jason, Soikat Hasan Ahmed, Zergham Ahmed, Brian Anderson, Andreas\nG. Andreou, Chiara Bartolozzi, Arindam Basu, et al. 2023.\n“NeuroBench: Advancing Neuromorphic\nComputing Through Collaborative, Fair and Representative\nBenchmarking.” https://arxiv.org/abs/2304.04640.\n\n\nYou, Jie, Jae-Won Chung, and Mosharaf Chowdhury. 2023. “Zeus:\nUnderstanding and Optimizing GPU Energy\nConsumption of DNN Training.” In 20th USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 23),\n119–39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\n\nYou, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.\n2017. “ImageNet Training in Minutes,” September. http://arxiv.org/abs/1709.05011v10.\n\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018.\n“Recent Trends in Deep Learning Based Natural Language Processing\n[Review Article].” IEEE Comput. Intell.\nMag. 13 (3): 55–75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nYu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy\nDavis, Jeff Dean, et al. 2018. “Dynamic Control Flow in\nLarge-Scale Machine Learning.” In Proceedings of the\nThirteenth EuroSys Conference, 265–83. ACM. https://doi.org/10.1145/3190508.3190551.\n\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019.\n“Q8BERT: Quantized 8Bit\nBERT.” In 2019 Fifth Workshop on Energy\nEfficient Machine Learning and Cognitive Computing - NeurIPS Edition\n(EMC2-NIPS), 36–39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\nZeiler, Matthew D. 2012. “ADADELTA: An Adaptive Learning Rate\nMethod,” December, 119–49. https://doi.org/10.1002/9781118266502.ch6.\n\n\nZennaro, Marco, Brian Plancher, and V Janapa Reddi. 2022.\n“TinyML: Applied AI for\nDevelopment.” In The UN 7th Multi-Stakeholder Forum on\nScience, Technology and Innovation for the Sustainable Development\nGoals, 2022–05.\n\n\nZhang, Chengliang, Minchen Yu, Wei Wang 0030, and Feng Yan 0001. 2019.\n“MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware\nMachine Learning Inference Serving.” In 2019 USENIX Annual\nTechnical Conference (USENIX ATC 19), 1049–62. https://www.usenix.org/conference/atc19/presentation/zhang-chengliang.\n\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason\nOptimizing Cong. 2015. “FPGA-Based Accelerator Design\nfor Deep Convolutional Neural Networks Proceedings of the 2015\nACM.” In SIGDA International Symposium on\nField-Programmable Gate Arrays-FPGA, 15:161–70.\n\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna\nGoldie, and Azalia Mirhoseini. 2022. “A Full-Stack Search\nTechnique for Domain Optimized Deep Learning Accelerators.” In\nProceedings of the 27th ACM International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems, 27–42. ASPLOS ’22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\n\n\nZhang, Dongxia, Xiaoqing Han, and Chunyu Deng. 2018. “Review on\nthe Research and Practice of Deep Learning and Reinforcement Learning in\nSmart Grids.” CSEE Journal of Power and Energy Systems 4\n(3): 362–70. https://doi.org/10.17775/cseejpes.2018.00520.\n\n\nZhang, Hongyu. 2008. “On the Distribution of Software\nFaults.” IEEE Trans. Software Eng. 34 (2): 301–2. https://doi.org/10.1109/tse.2007.70771.\n\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, and Siddharth Garg. 2018.\n“Analyzing and Mitigating the Impact of Permanent Faults on a\nSystolic Array Based Neural Network Accelerator.” In 2018\nIEEE 36th VLSI Test Symposium (VTS), 1–6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\n\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. 2018.\n“ThUnderVolt: Enabling Aggressive\nVoltage Underscaling and Timing Error Resilience for Energy Efficient\nDeep Learning Accelerators.” In 2018 55th ACM/ESDA/IEEE\nDesign Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu.\n2020. “Fast Hardware-Aware Neural Architecture Search.” In\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\nZhang, Qingxue, Dian Zhou, and Xuan Zeng. 2017. “Highly Wearable\nCuff-Less Blood Pressure and Heart Rate Monitoring with Single-Arm\nElectrocardiogram and Photoplethysmogram Signals.” BioMedical\nEngineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai\nHelen Li, and Yiran Chen. 2020. “AutoShrink:\nA Topology-Aware NAS for Discovering Efficient\nNeural Architecture.” In The Thirty-Fourth AAAI Conference on\nArtificial Intelligence, AAAI 2020, the Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, the Tenth\nAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, 6829–36. AAAI Press.\nhttps://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\nZhao, Mark, and G. Edward Suh. 2018. “FPGA-Based\nRemote Power Side-Channel Attacks.” In 2018 IEEE Symposium on\nSecurity and Privacy (SP), 229–44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\n\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas\nChandra. 2018. “Federated Learning with Non-Iid Data.”\nArXiv Preprint abs/1806.00582. https://arxiv.org/abs/1806.00582.\n\n\nZhou, Bolei, Yiyou Sun, David Bau, and Antonio Torralba. 2018.\n“Interpretable Basis Decomposition for Visual Explanation.”\nIn Proceedings of the European Conference on Computer Vision\n(ECCV), 119–34.\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat,\nXavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian,\nManuel Le Gallo, and Paul N. Whatmough. 2021.\n“AnalogNets: Ml-hw\nCo-Design of Noise-Robust TinyML Models and Always-on\nAnalog Compute-in-Memory Accelerator.” https://arxiv.org/abs/2111.06503.\n\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2018.\n“Learning Rich Features for Image Manipulation Detection.”\nIn 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1053–61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand\nJayarajan, Amar Phanishayee, Bianca Schroeder, and Gennady Pekhimenko.\n2018. “Benchmarking and Analyzing Deep Neural Network\nTraining.” In 2018 IEEE International Symposium on Workload\nCharacterization (IISWC), 88–100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\n\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang\nGan, and Song Han. 2023. “PockEngine:\nSparse and Efficient Fine-Tuning in a Pocket.” In\n56th Annual IEEE/ACM International Symposium on\nMicroarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. 2021. “A Comprehensive Survey on\nTransfer Learning.” Proc. IEEE 109 (1): 43–76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nZoph, Barret, and Quoc V. Le. 2016. “Neural Architecture Search\nwith Reinforcement Learning,” November, 367–92. https://doi.org/10.1002/9781394217519.ch17.",
    "crumbs": [
      "REFERENCES",
      "References"
    ]
  },
  {
    "objectID": "contents/tools.html",
    "href": "contents/tools.html",
    "title": "Appendice A: Tools",
    "section": "",
    "text": "A.1 Hardware Kits",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "contents/tools.html#hardware-kits",
    "href": "contents/tools.html#hardware-kits",
    "title": "Appendice A: Tools",
    "section": "",
    "text": "A.1.1 Microcontrollers and Development Boards\n\n\n\n\n\n\n\n\n\n\nNo\nHardware\nProcessor\nFeatures\nTinyML Compatibility\n\n\n\n\n1\nArduino Nano 33 BLE Sense\nARM Cortex-M4\nOnboard sensors, Bluetooth connectivity\nTensorFlow Lite Micro\n\n\n2\nRaspberry Pi Pico\nDual-core Arm Cortex-M0+\nLow-cost, large community support\nTensorFlow Lite Micro\n\n\n3\nSparkFun Edge\nAmbiq Apollo3 Blue\nUltra-low power consumption, onboard microphone\nTensorFlow Lite Micro\n\n\n4\nAdafruit EdgeBadge\nATSAMD51 32-bit Cortex M4\nCompact size, integrated display and microphone\nTensorFlow Lite Micro\n\n\n5\nGoogle Coral Development Board\nNXP i.MX 8M SOC (quad Cortex-A53, Cortex-M4F)\nEdge TPU, Wi-Fi, Bluetooth\nTensorFlow Lite for Coral\n\n\n6\nSTM32 Discovery Kits\nVarious (e.g., STM32F7, STM32H7)\nDifferent configurations, Cube.AI software support\nSTM32Cube.AI\n\n\n7\nArduino Nicla Vision\nSTM32H747AII6 Dual Arm Cortex M7/M4\nIntegrated camera, low power, compact design\nTensorFlow Lite Micro\n\n\n8\nArduino Nicla Sense ME\n64 MHz Arm Cortex M4 (nRF52832)\nMulti-sensor platform, environment sensing, BLE, Wi-Fi\nTensorFlow Lite Micro\n\n\n9\nXIAO ESP32S3 Sense\nXtensa LX7 dual-core (ESP32-S3R8)\nIntegrated camera, microphone, BLE, Wi-Fi and the most compact design\nTensorFlow Lite Micro",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "contents/tools.html#software-tools",
    "href": "contents/tools.html#software-tools",
    "title": "Appendice A: Tools",
    "section": "A.2 Software Tools",
    "text": "A.2 Software Tools\n\nA.2.1 Machine Learning Frameworks\n\n\n\n\n\n\n\n\n\nNo\nMachine Learning Framework\nDescription\nUse Cases\n\n\n\n\n1\nTensorFlow Lite\nLightweight library for running machine learning models on constrained devices\nImage recognition, voice commands, anomaly detection\n\n\n2\nEdge Impulse\nA platform providing tools for creating machine learning models optimized for edge devices\nData collection, model training, deployment on tiny devices\n\n\n3\nONNX Runtime\nA performance-optimized engine for running ONNX models, fine-tuned for edge devices\nCross-platform deployment of machine learning models\n\n\n\n\n\nA.2.2 Libraries and APIs\n\n\n\n\n\n\n\n\n\nNo\nLibrary/API\nDescription\nUse Cases\n\n\n\n\n1\nCMSIS-NN\nA collection of efficient neural network kernels optimized for Cortex-M processors\nEmbedded vision and AI applications\n\n\n2\nARM NN\nAn inference engine for CPUs, GPUs, and NPUs, enabling the translation of neural network frameworks\nAccelerating machine learning model inference on ARM-based devices",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "contents/tools.html#ides-and-development-environments",
    "href": "contents/tools.html#ides-and-development-environments",
    "title": "Appendice A: Tools",
    "section": "A.3 IDEs and Development Environments",
    "text": "A.3 IDEs and Development Environments\n\n\n\n\n\n\n\n\n\nNo\nIDE/Development Environment\nDescription\nFeatures\n\n\n\n\n1\nPlatformIO\nAn open-source ecosystem for IoT development catering to various boards & platforms\nCross-platform build system, continuous testing, firmware updates\n\n\n2\nEclipse Embedded CDT\nA plugin for Eclipse facilitating embedded systems development\nSupports various compilers and debuggers, integrates with popular build tools\n\n\n3\nArduino IDE\nOfficial development environment for Arduino supporting various boards & languages\nUser-friendly interface, large community support, extensive library collection\n\n\n4\nMbed Studio\nARM’s IDE for developing robust embedded software with Mbed OS\nIntegrated debugger, Mbed OS integration, version control support\n\n\n5\nSegger Embedded Studio\nA powerful IDE for ARM microcontrollers supporting a wide range of development boards\nAdvanced code editor, project management, debugging capabilities",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "contents/zoo_datasets.html",
    "href": "contents/zoo_datasets.html",
    "title": "Appendice B: Datasets",
    "section": "",
    "text": "Google Speech Commands Dataset\n\nDescription: A set of one-second .wav audio files, each containing a single spoken English word.\nLink to the Dataset\n\nVisualWakeWords Dataset\n\nDescription: A dataset tailored for TinyML vision applications, consisting of binary labeled images indicating whether a person is in the image or not.\nLink to the Dataset\n\nEMNIST Dataset\n\nDescription: A dataset containing 28x28 pixel images of handwritten characters and digits, which is an extension of the MNIST dataset but includes letters.\nLink to the Dataset\n\nUCI Machine Learning Repository: Human Activity Recognition Using Smartphones\n\nDescription: A dataset with the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors.\nLink to the Dataset\n\nPlantVillage Dataset\n\nDescription: A dataset comprising of images of healthy and diseased crop leaves categorized based on the crop type and disease type, which could be used in a TinyML agricultural project.\nLink to the Dataset\n\nGesture Recognition using 3D Motion Sensing (3D Gesture Database)\n\nDescription: This dataset contains 3D gesture data recorded using a Leap Motion Controller, which might be useful for gesture recognition projects.\nLink to the Dataset\n\nMultilingual Spoken Words Corpus\n\nDescription: A dataset containing recordings of common spoken words in various languages, useful for speech recognition projects targeting multiple languages.\nLink to the Dataset\n\nWake Vision\n\nDescription: A dataset containing over 6 million images for binary person classification. In addition, it includes a fine-grain benchmark suite for evaluating the fairness and robustness of models.\nLink to the Dataset\n\n\nRemember to verify the dataset’s license or terms of use to ensure it can be used for your intended purpose.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html",
    "href": "contents/learning_resources.html",
    "title": "Appendice D: Resources",
    "section": "",
    "text": "D.1 Books\nHere is a list of recommended books for learning about TinyML or embedded AI:\nThese books cover a range of topics related to TinyML and embedded AI, including:\nIn addition to the above books, there are a number of other resources available for learning about TinyML and embedded AI, including online courses, tutorials, and blog posts. Some of these are listed below. Another great way to learn is by joining the community of embedded AI developers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html#books",
    "href": "contents/learning_resources.html#books",
    "title": "Appendice D: Resources",
    "section": "",
    "text": "TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers by Pete Warden and Daniel Situnayake\nAI at the Edge: Solving Real-World Problems with Embedded Machine Learning by Daniel Situnayake and Jenny Plunkett\nTinyML Cookbook: Combine artificial intelligence and ultra-low-power embedded devices to make the world smarter by Gian Marco Iodice\nIntroduction to TinyML by Rohit Sharma\nIntegrated camera, microphone, BLE, Wi-Fi and the most compact design by Lei Feng(Seeed Studio), Marcelo Rovai\n\n\n\nThe fundamentals of machine learning and TinyML\nHow to choose the right hardware and software for your project\nHow to train and deploy TinyML models on embedded devices\nReal-world examples of TinyML applications",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html#tutorials",
    "href": "contents/learning_resources.html#tutorials",
    "title": "Appendice D: Resources",
    "section": "D.2 Tutorials",
    "text": "D.2 Tutorials",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html#frameworks",
    "href": "contents/learning_resources.html#frameworks",
    "title": "Appendice D: Resources",
    "section": "D.3 Frameworks",
    "text": "D.3 Frameworks\n\nGitHub Description: There are various GitHub repositories dedicated to TinyML where you can contribute or learn from existing projects. Some popular organizations/repos to check out are:\n\n\nTensorFlow Lite Micro: GitHub Repository\nTinyML4D: GitHub Repository\nEdge Impulse Expert Network: Repository\n\n\nStack Overflow Tags: tinyml Description: Use the “tinyml” tag on Stack Overflow to ask technical questions and find answers from the community.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html#courses-and-learning-platforms",
    "href": "contents/learning_resources.html#courses-and-learning-platforms",
    "title": "Appendice D: Resources",
    "section": "D.4 Courses and Learning Platforms",
    "text": "D.4 Courses and Learning Platforms\n\nCoursera Course: Introduction to Embedded Machine Learning Description: A dedicated course on Coursera to learn the basics and advances of TinyML.\nEdX Course: Intro to TinyML Description: Learn about TinyML with this HarvardX course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/community.html",
    "href": "contents/community.html",
    "title": "Appendice E: Communities",
    "section": "",
    "text": "E.1 Online Forums",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/community.html#online-forums",
    "href": "contents/community.html#online-forums",
    "title": "Appendice E: Communities",
    "section": "",
    "text": "TinyML Forum Website: TinyML Forum Description: A dedicated forum for discussions, news, and updates on TinyML.\nReddit Subreddits: r/TinyML Description: Reddit community discussing various topics related to TinyML.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/community.html#blogs-and-websites",
    "href": "contents/community.html#blogs-and-websites",
    "title": "Appendice E: Communities",
    "section": "E.2 Blogs and Websites",
    "text": "E.2 Blogs and Websites\n\nTinyML Foundation Website: TinyML Foundation Description: The official website offers a wealth of information including research, news, and events.\nEdge Impulse Blog Website: Blog Description: Contains several articles, tutorials, and resources on TinyML.\nTiny Machine Learning Open Education Initiative (TinyMLedu) Website: TinyML Open Education Initiative Description: The website offers links to educational materials on TinyML, training events and research papers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/community.html#social-media-groups",
    "href": "contents/community.html#social-media-groups",
    "title": "Appendice E: Communities",
    "section": "E.3 Social Media Groups",
    "text": "E.3 Social Media Groups\n\nLinkedIn Groups Description: Join TinyML groups on LinkedIn to connect with professionals and enthusiasts in the field.\nTwitter Description: Follow TinyML enthusiasts, organizations, and experts on Twitter for the latest news and updates. Example handles to follow:\n\nTwitter\nEdgeImpulse",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/community.html#conferences-and-meetups",
    "href": "contents/community.html#conferences-and-meetups",
    "title": "Appendice E: Communities",
    "section": "E.4 Conferences and Meetups",
    "text": "E.4 Conferences and Meetups\n\nTinyML Summit Website: TinyML Summit Description: Annual event where professionals and enthusiasts gather to discuss the latest developments in TinyML.\nMeetup Website: Meetup Description: Search for TinyML groups on Meetup to find local or virtual gatherings.\n\nRemember to always check the credibility and activity level of the platforms and groups before diving in to ensure a productive experience.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/case_studies.html",
    "href": "contents/case_studies.html",
    "title": "Appendice F: Case Studies",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nComing soon.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html",
    "title": "Nicla Vision",
    "section": "",
    "text": "Pre-requisites",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#pre-requisites",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#pre-requisites",
    "title": "Nicla Vision",
    "section": "",
    "text": "Nicla Vision Board: Ensure you have the Nicla Vision board.\nUSB Cable: For connecting the board to your computer.\nNetwork: With internet access for downloading necessary software.",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#setup",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#setup",
    "title": "Nicla Vision",
    "section": "Setup",
    "text": "Setup\n\nSetup Nicla Vision",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#exercises",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#exercises",
    "title": "Nicla Vision",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "Pre-requisites",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#pre-requisites",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#pre-requisites",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "XIAO ESP32S3 Sense Board: Ensure you have the XIAO ESP32S3 Sense Board.\nUSB-C Cable: This is for connecting the board to your computer.\nNetwork: With internet access for downloading necessary software.\nSD Card and an SD card Adapter: This saves audio and images (optional).",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#setup",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#setup",
    "title": "XIAO ESP32S3",
    "section": "Setup",
    "text": "Setup\n\nSetup XIAO ESP32S3",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#exercises",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#exercises",
    "title": "XIAO ESP32S3",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/shared/shared.html",
    "href": "contents/labs/shared/shared.html",
    "title": "Shared Labs",
    "section": "",
    "text": "The labs in this section cover topics and techniques that are applicable across different hardware platforms. These labs are designed to be independent of specific boards, allowing you to focus on the fundamental concepts and algorithms used in (tiny) ML applications.\nBy exploring these shared labs, you’ll gain a deeper understanding of the common challenges and solutions in embedded machine learning. The knowledge and skills acquired here will be valuable regardless of the specific hardware you work with in the future.\n\n\n\nExercise\nNicla Vision\nXIAO ESP32S3\n\n\n\n\nKWS Feature Engineering\n✔ Link\n✔ Link\n\n\nDSP Spectral Features Block\n✔ Link\n✔ Link",
    "crumbs": [
      "Shared Labs"
    ]
  }
]