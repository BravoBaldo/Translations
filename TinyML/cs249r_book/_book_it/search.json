[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Systems",
    "section": "",
    "text": "Prefazione\nBenvenuti in Machine Learning Systems. Questo libro è la porta d’accesso al mondo frenetico dei sistemi di intelligenza artificiale. È un’estensione del corso CS249r alla Harvard University.\n\nAbbiamo creato questo libro open source come sforzo collaborativo per riunire spunti di studenti, professionisti e la più ampia comunità di professionisti dell’IA. Il nostro obiettivo è sviluppare una guida completa che esplori le complessità dei sistemi di intelligenza artificiale e le loro numerose applicazioni.\n\n“Se vuoi andare veloce, vai da solo. Se vuoi andare lontano, vai insieme ad altre persone”. – Proverbio africano\n\nQuesto non è un libro statico; è un documento vivo e pulsante. Lo stiamo rendendo open source e lo aggiorniamo costantemente per soddisfare le esigenze in continua evoluzione di questo campo dinamico. Contiene un ricco mix di conoscenze specialistiche che guideranno attraverso la complessa interazione tra algoritmi all’avanguardia e i principi fondamentali che li fanno funzionare. Stiamo preparando il terreno per il prossimo grande balzo nell’innovazione dell’IA.\n\n\nPerché Abbiamo Scritto Questo Libro\nViviamo in un’epoca in cui la tecnologia è in continua evoluzione. La collaborazione aperta e la condivisione delle conoscenze sono gli elementi costitutivi della vera innovazione. Questo è lo spirito alla base di questo lavoro. Andiamo oltre il tradizionale modello di libro di testo per creare un hub di conoscenza vivo, in modo che possiamo tutti condividere e imparare gli uni dagli altri.\nIl libro si concentra sui principi e sui casi di studio dei sistemi di IA, con l’obiettivo di fornire una comprensione approfondita che aiuterà a navigare nel panorama in continua evoluzione dei sistemi di IA. Mantenendolo “open source”, non stiamo solo rendendo accessibile l’apprendimento, ma stiamo anche invitando nuove idee e miglioramenti continui. In breve, stiamo costruendo una comunità in cui la conoscenza è libera di crescere e illuminare la strada verso la tecnologia AI globale.\n\n\nCosa c’è da Sapere\nPer immergersi in questo libro, non si dev’essere un esperto di AI. Tutto ciò di cui c’è bisogno è una conoscenza di base dei concetti di informatica e la curiosità di esplorare su come funzionano i sistemi AI. È qui che avviene l’innovazione e una conoscenza di base della programmazione e delle strutture dati sarà la bussola.\n\n\nConvenzioni del Libro\nPer i dettagli sulle convenzioni utilizzate in questo libro, consultare la sezione Convenzioni.\n\n\nDichiarazione di Trasparenza dei Contenuti\nQuesto libro è un progetto guidato dalla comunità, con contenuti generati da numerosi collaboratori nel tempo. Il processo di creazione dei contenuti potrebbe aver coinvolto vari strumenti di editing, tra cui la tecnologia AI generativa. In qualità di autore principale, editore e curatore, il Prof. Vijay Janapa Reddi mantiene la supervisione umana e la supervisione editoriale per garantire che il contenuto sia accurato e pertinente. Tuttavia, nessuno è perfetto, quindi potrebbero comunque esserci delle inesattezze. Apprezziamo molto i feedback e invitiamo a fornire correzioni e suggerimenti. Questo approccio collaborativo è fondamentale per migliorare e mantenere la qualità del contenuto e rendere le informazioni accessibili a livello globale.\n\n\nPer dare una mano\nSe si è interessati a contribuire, le linee guida si trovano qui.\n\n\nContatti\nCi sono domande o feedback? Si è liberi di inviare una e-mail al Prof. Vijay Janapa Reddi direttamente, oppure si può avviare un thread di discussione su GitHub.\n\n\nCollaboratori\nUn grande ringraziamento a tutti coloro che hanno contribuito a rendere questo libro quello che è! L’elenco completo dei singoli collaboratori è qui e ulteriori dettagli sullo stile GitHub qui. Benvenuti come collaboratori!",
    "crumbs": [
      "PREFAZIONE",
      "Prefazione"
    ]
  },
  {
    "objectID": "contents/dedication.it.html",
    "href": "contents/dedication.it.html",
    "title": "Dedica",
    "section": "",
    "text": "Questo libro è una testimonianza dell’idea che, nell’immensità della tecnologia e dell’innovazione, non sono sempre i sistemi più grandi, ma quelli più piccoli, a poter cambiare il mondo.",
    "crumbs": [
      "PREFAZIONE",
      "Dedica"
    ]
  },
  {
    "objectID": "contents/acknowledgements/acknowledgements.it.html",
    "href": "contents/acknowledgements/acknowledgements.it.html",
    "title": "Ringraziamenti",
    "section": "",
    "text": "Singoli Collaboratori\nEstendiamo la nostra sincera gratitudine alla comunità open source di studenti, insegnanti e contributori. Che abbiate contribuito con un’intera sezione, una singola frase o semplicemente corretto un errore di battitura, i vostri sforzi hanno migliorato questo libro. Apprezziamo profondamente il tempo, la competenza e l’impegno di tutti. Questo libro è tanto vostro quanto nostro.\nUn ringraziamento speciale va al professor Vijay Janapa Reddi, la cui convinzione nel potere trasformativo delle comunità open source e la cui guida inestimabile sono state la nostra luce guida fin dall’inizio.\nDobbiamo molto anche al team di GitHub e di Quarto. Avete rivoluzionato il modo in cui le persone collaborano e questo libro è una testimonianza di ciò che si può ottenere quando vengono rimosse le barriere alla cooperazione globale.",
    "crumbs": [
      "PREFAZIONE",
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/acknowledgements/acknowledgements.it.html#agenzie-e-aziende-finanziatrici",
    "href": "contents/acknowledgements/acknowledgements.it.html#agenzie-e-aziende-finanziatrici",
    "title": "Ringraziamenti",
    "section": "Agenzie e Aziende Finanziatrici",
    "text": "Agenzie e Aziende Finanziatrici\nSiamo immensamente grati per il generoso supporto delle varie agenzie e aziende finanziatrici che hanno supportato gli assistenti didattici coinvolti in questo lavoro. Le organizzazioni elencate di seguito hanno svolto un ruolo cruciale nel dare vita a questo progetto con i loro contributi.",
    "crumbs": [
      "PREFAZIONE",
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/acknowledgements/acknowledgements.it.html#ai-nostri-lettori",
    "href": "contents/acknowledgements/acknowledgements.it.html#ai-nostri-lettori",
    "title": "Ringraziamenti",
    "section": "Ai Nostri Lettori",
    "text": "Ai Nostri Lettori\nA tutti coloro che acquisteranno questo libro, vogliamo ringraziarvi! L’abbiamo scritto pensando a voi, sperando di provocare riflessioni, ispirare domande e forse anche accendere una scintilla di ispirazione. Dopo tutto, che senso ha scrivere se nessuno legge?",
    "crumbs": [
      "PREFAZIONE",
      "Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/contributors.it.html",
    "href": "contents/contributors.it.html",
    "title": "Collaboratori e Ringraziamenti",
    "section": "",
    "text": "Estendiamo i nostri più sinceri ringraziamenti al gruppo eterogeneo di persone che hanno generosamente contribuito con la loro competenza, intuizioni, tempo e supporto per migliorare sia il contenuto che la base di codice di questo progetto. Ciò include non solo coloro che hanno contribuito direttamente tramite codice e scrittura, ma anche coloro che hanno aiutato identificando problemi, fornendo feedback e offrendo suggerimenti. Di seguito c’è l’elenco di tutti i collaboratori. Per contribuire a questo progetto, visitare la nostra pagina GitHub per maggiori informazioni.\n\n\n\n\n\n\n\n\nVijay Janapa Reddi\n\n\nIkechukwu Uchendu\n\n\nNaeem Khoshnevis\n\n\nDouwe den Blanken\n\n\njasonjabbour\n\n\n\n\nshanzehbatool\n\n\nkai4avaya\n\n\nElias Nuwara\n\n\nJared Ping\n\n\nMatthew Stewart\n\n\n\n\nItai Shapira\n\n\nMarcelo Rovai\n\n\nMaximilian Lam\n\n\nJayson Lin\n\n\nJeffrey Ma\n\n\n\n\nSophia Cho\n\n\nAndrea\n\n\nAlex Rodriguez\n\n\nKorneel Van den Berghe\n\n\nZishen Wan\n\n\n\n\nColby Banbury\n\n\nSara Khosravi\n\n\nDivya Amirtharaj\n\n\nSrivatsan Krishnan\n\n\nAbdulrahman Mahmoud\n\n\n\n\nAghyad Deeb\n\n\narnaumarin\n\n\nEmeka Ezike\n\n\nAditi Raju\n\n\nEmil Njor\n\n\n\n\noishib\n\n\nJared Ni\n\n\nMichael Schnebly\n\n\nELSuitorHarvard\n\n\nHenry Bae\n\n\n\n\nYu-Shun Hsiao\n\n\nJae-Won Chung\n\n\nMark Mazumder\n\n\nJennifer Zhou\n\n\nAndrew Bass\n\n\n\n\neurashin\n\n\nMarco Zennaro\n\n\nPong Trairatvorakul\n\n\nShvetank Prakash\n\n\nAlex Oesterling\n\n\n\n\nAllen-Kuang\n\n\nBruno Scaglione\n\n\ngnodipac886\n\n\nGauri Jain\n\n\nFin Amin\n\n\n\n\nSercan Aygün\n\n\nBaldassarre Cesarano\n\n\nabigailswallow\n\n\nYang Zhou\n\n\nyanjingl\n\n\n\n\nJason Yik\n\n\nhappyappledog\n\n\nEmmanuel Rassou\n\n\nCurren Iyer\n\n\nJessica Quaye\n\n\n\n\nShreya Johri\n\n\nVijay Edupuganti\n\n\nSonia Murthy\n\n\nThe Random DIY\n\n\nCostin-Andrei Oncescu\n\n\n\n\nAnnie Laurie Cook\n\n\nJothi Ramaswamy\n\n\nBatur Arslan\n\n\na-saraf\n\n\nsonghan\n\n\n\n\nZishen",
    "crumbs": [
      "PREFAZIONE",
      "Collaboratori e Ringraziamenti"
    ]
  },
  {
    "objectID": "contents/copyright.it.html",
    "href": "contents/copyright.it.html",
    "title": "Copyright",
    "section": "",
    "text": "Questo libro è open source e sviluppato in modo collaborativo tramite GitHub. Salvo diversa indicazione, questo lavoro è concesso in licenza con Creative Commons Attribuzione-Non commerciale-Condividi allo stesso modo 4.0 Internazionale (CC BY-NC-SA 4.0 CC BY-SA 4.0). Il testo completo della licenza si trova qui.\nI collaboratori di questo progetto hanno dedicato i loro contributi al pubblico dominio o con la stessa licenza aperta del progetto originale. Sebbene i contributi siano collaborativi, ogni collaboratore mantiene il copyright sui rispettivi contributi.\nPer i dettagli sulla paternità, i contributi e come contribuire, consultare il repository del progetto su GitHub.\nTutti i marchi e i marchi registrati menzionati in questo libro sono di proprietà dei rispettivi proprietari.\nLe informazioni fornite in questo libro sono ritenute accurate e affidabili. Tuttavia, gli autori, i curatori e gli editori non possono essere ritenuti responsabili per eventuali danni causati o presumibilmente causati, direttamente o indirettamente, dalle informazioni contenute nel presente libro.",
    "crumbs": [
      "PREFAZIONE",
      "Copyright"
    ]
  },
  {
    "objectID": "contents/about.it.html",
    "href": "contents/about.it.html",
    "title": "Informazioni sul Libro",
    "section": "",
    "text": "Panoramica\nBenvenuti a questo progetto collaborativo avviato dalla classe CS249r Machine Learning Systems presso l’Università di Harvard. Il nostro obiettivo è rendere questo libro una risorsa della comunità che aiuti educatori e studenti a comprendere i sistemi di ML. Il libro verrà aggiornato regolarmente per riflettere nuove intuizioni sui sistemi ML e metodi di insegnamento efficaci.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/about.it.html#argomenti-esplorati",
    "href": "contents/about.it.html#argomenti-esplorati",
    "title": "Informazioni sul Libro",
    "section": "Argomenti Esplorati",
    "text": "Argomenti Esplorati\nQuesto libro offre una panoramica completa di vari aspetti dei sistemi di apprendimento automatico. Copriamo l’intero flusso di lavoro dei sistemi ML end-to-end, iniziando con concetti fondamentali e procedendo attraverso l’ingegneria dei dati, i framework AI e il training dei modelli.\nSi imparerà ad ottimizzare i modelli per l’efficienza, a distribuire l’AI su varie piattaforme hardware e a confrontare le prestazioni. Il libro esplora anche argomenti più avanzati come la sicurezza, la privacy, l’intelligenza artificiale responsabile e sostenibile, l’IA solida e generativa e l’impatto sociale dell’IA. Alla fine, si avranno solide basi e approfondimenti pratici sia sulle dimensioni tecniche che etiche dell’apprendimento automatico.\nCi auguriamo che una volta terminato questo libro si abbia una conoscenza di base dell’apprendimento automatico e delle sue applicazioni. Si impareranno anche le implementazioni reali dei sistemi di apprendimento automatico e si acquisirà esperienza pratica tramite laboratori e compiti basati su progetti.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/about.it.html#chi-dovrebbe-leggerlo",
    "href": "contents/about.it.html#chi-dovrebbe-leggerlo",
    "title": "Informazioni sul Libro",
    "section": "Chi Dovrebbe Leggerlo",
    "text": "Chi Dovrebbe Leggerlo\nQuesto libro è pensato per chi è alle prime armi con l’entusiasmante campo dei sistemi di apprendimento automatico. Inizia con le basi dell’apprendimento automatico e passa ad argomenti più avanzati rilevanti per la comunità ML e aree di ricerca più ampie. Il libro è particolarmente utile per:\n\nStudenti di Informatica e Ingegneria Elettrica: Questo libro è una risorsa utile per gli studenti di informatica e di ingegneria elettrica. Li introduce alle tecniche utilizzate nei sistemi ML, preparandoli alle sfide del mondo reale nell’apprendimento automatico.\nIngegneri di Sistema: Per gli ingegneri di vari settori, questo libro funge da guida ai sistemi ML, aiutandoli a creare applicazioni intelligenti, in particolare su piattaforme con risorse limitate.\nRicercatori e Accademici: Coloro che sono coinvolti nella ricerca su apprendimento automatico, visione artificiale ed elaborazione del segnale potrebbero trovare questo libro interessante. Fa luce sulle sfide uniche dell’esecuzione di algoritmi di apprendimento automatico su diverse piattaforme.\nProfessionisti del Settore: Se si lavora in settori come IoT, robotica, tecnologia indossabile o dispositivi intelligenti, questo libro fornirà le conoscenze necessarie per aggiungere funzionalità di apprendimento automatico ai prodotti.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/about.it.html#principali-risultati-dellapprendimento",
    "href": "contents/about.it.html#principali-risultati-dellapprendimento",
    "title": "Informazioni sul Libro",
    "section": "Principali Risultati dell’Apprendimento",
    "text": "Principali Risultati dell’Apprendimento\nI lettori acquisiranno competenze nel training e nell’implementazione di modelli di reti neurali profonde su diverse piattaforme, oltre a comprendere le sfide più ampie coinvolte nella loro progettazione, sviluppo e implementazione. Nello specifico, si imparerà:\n\nConcetti Fondamentali nel Machine Learning [apprendimento automatico]\nFondamenti dei sistemi di Intelligenza Artificiale\nPiattaforme Hardware Adatte all’Implementazione dell’Intelligenza Artificiale\nTecniche per Modelli di Addestramento per Diversi Sistemi di Intelligenza Artificiale\nStrategie per l’Ottimizzazione dei Modelli di Intelligenza Artificiale\nApplicazioni Reali dei Sistemi di Intelligenza Artificiale\nSfide Attuali e Tendenze Future nei Sistemi di Intelligenza Artificiale\n\nIl nostro obiettivo è rendere questo libro una risorsa per chiunque sia interessato a sviluppare applicazioni intelligenti su vari sistemi. Dopo aver completato il libro, si sarà ben equipaggiati per progettare e implementare progetti abilitati all’apprendimento automatico.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/about.it.html#prerequisiti-per-i-lettori",
    "href": "contents/about.it.html#prerequisiti-per-i-lettori",
    "title": "Informazioni sul Libro",
    "section": "Prerequisiti per i Lettori",
    "text": "Prerequisiti per i Lettori\n\nCompetenze di programmazione di base: Consigliamo di avere una certa esperienza di programmazione, idealmente in Python. Una conoscenza delle variabili, tipi di dati e strutture di controllo faciliterà l’interazione col libro.\nAlcune Conoscenze di Machine Learning: Sebbene non sia obbligatorio, una conoscenza di base dei concetti di apprendimento automatico aiuterà ad assorbire il materiale più facilmente. Se si è nuovi nel campo, il libro fornisce sufficienti informazioni di base per mettersi al passo.\nConoscenza di Base dei Sistemi: Si consiglia un livello di conoscenza di base dei sistemi a livello universitario junior o senior. Sarà utile comprendere l’architettura di sistema, i sistemi operativi e le reti di base.\nProgrammazione Python (Facoltativo): Se si ha familiarità con Python, si troverà più facile interagire con le sezioni di codifica del libro. Conoscere librerie come NumPy, scikit-learn e TensorFlow sarà particolarmente utile.\nVoglia di Imparare: Il libro è progettato per essere accessibile a un vasto pubblico, con diversi livelli di competenza tecnica. La volontà di sfidare se stessi e di impegnarsi in esercizi pratici aiuterà a trarne il massimo vantaggio.\nDisponibilità delle Risorse: Per gli aspetti pratici, ci sarà bisogno di un computer con Python e le librerie pertinenti installate. L’accesso facoltativo a schede di sviluppo o hardware specifico sarà utile anche per sperimentare la distribuzione del modello di apprendimento automatico.\n\nSoddisfacendo questi prerequisiti, si sarà ben posizionati per approfondire la comprensione dei sistemi di apprendimento automatico, impegnarsi in esercizi di codifica e persino implementare applicazioni pratiche su vari dispositivi.",
    "crumbs": [
      "PREFAZIONE",
      "Informazioni sul Libro"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html",
    "href": "contents/introduction/introduction.it.html",
    "title": "1  Introduzione",
    "section": "",
    "text": "1.1 Panoramica\nAll’inizio degli anni ’90, Mark Weiser, un pioniere dell’informatica, ha introdotto il mondo a un concetto rivoluzionario che avrebbe cambiato per sempre il modo in cui interagiamo con la tecnologia. Immaginava un futuro in cui l’informatica sarebbe stata perfettamente integrata nei nostri ambienti, diventando una parte invisibile e integrante della vita quotidiana. Questa visione, che lui chiamava “ubiquitous computing” [informatica ovunque], prometteva un mondo in cui la tecnologia ci avrebbe servito senza richiedere la nostra costante attenzione o interazione. Facciamo un salto in avanti fino a oggi, e ci troviamo sul punto di realizzare la visione di Weiser, grazie all’avvento e alla proliferazione dei sistemi di machine learning [apprendimento automatico].\nNella visione dell’ubiquitous computing (Weiser 1991), l’integrazione dei processori negli oggetti di uso quotidiano è solo un aspetto di un più ampio cambiamento di paradigma. La vera essenza di questa visione risiede nella creazione di un ambiente intelligente in grado di anticipare le nostre esigenze e agire per nostro conto, migliorando le nostre esperienze senza richiedere comandi espliciti. Per raggiungere questo livello di intelligenza pervasiva, è fondamentale sviluppare e distribuire sistemi di machine learning che coprano l’intero ecosistema, dal cloud all’edge e persino ai più piccoli dispositivi IoT.\nDistribuendo le capacità di apprendimento automatico nel continuum di elaborazione, possiamo sfruttare i punti di forza di ogni livello mitigandone al contempo i limiti. Il cloud, con le sue vaste risorse di elaborazione e capacità di archiviazione, è ideale per addestrare modelli complessi su grandi set di dati ed eseguire attività che richiedono molte risorse. I dispositivi edge, come gateway e smartphone, possono elaborare i dati localmente, consentendo tempi di risposta più rapidi, una migliore privacy e requisiti di larghezza di banda ridotti. Infine, i dispositivi IoT più piccoli, dotati di capacità di apprendimento automatico, possono prendere decisioni rapide in base ai dati dei sensori, consentendo sistemi altamente reattivi ed efficienti.\nQuesta intelligenza distribuita è particolarmente cruciale per le applicazioni che richiedono elaborazione in tempo reale, come veicoli autonomi, automazione industriale e assistenza sanitaria smart [intelligente]. Elaborando i dati al livello più appropriato del continuum informatico, possiamo garantire che le decisioni vengano prese in modo rapido e accurato, senza fare affidamento su una comunicazione costante con un server centrale.\nLa migrazione dell’intelligenza di apprendimento automatico nell’ecosistema consente inoltre esperienze più personalizzate e consapevoli del contesto. Imparando dal comportamento e dalle preferenze degli utenti all’edge, i dispositivi possono adattarsi alle esigenze individuali senza compromettere la privacy. Questa intelligenza localizzata può quindi essere aggregata e perfezionata nel cloud, creando un ciclo di feedback che migliora costantemente il sistema complessivo.\nTuttavia, l’implementazione di sistemi di apprendimento automatico nel continuum informatico presenta diverse sfide. Garantire l’interoperabilità e l’integrazione senza soluzione di continuità di questi sistemi richiede protocolli e interfacce standardizzati. È inoltre necessario affrontare i problemi di sicurezza e privacy, poiché la distribuzione dell’intelligenza su più livelli aumenta la superficie di attacco e il potenziale di violazioni dei dati.\nInoltre, le diverse capacità computazionali e i vincoli energetici dei dispositivi a diversi livelli del continuum informatico richiedono lo sviluppo di modelli di apprendimento automatico efficienti e adattabili. Tecniche come la compressione del modello, il “federated learning” [apprendimento federato] e il transfer learning [apprendimento tramite trasferimento] possono aiutare ad affrontare queste sfide, consentendo l’implementazione dell’intelligenza su un’ampia gamma di dispositivi.\nMentre ci avviciniamo alla realizzazione della visione di Weiser dell’ubiquitous computing, lo sviluppo e l’implementazione di sistemi di apprendimento automatico nell’intero ecosistema saranno fondamentali. Sfruttando i punti di forza di ogni layer [livello] del continuum informatico, possiamo creare un ambiente intelligente che si integra perfettamente con la nostra vita quotidiana, anticipando le nostre esigenze e migliorando le nostre esperienze in modi che un tempo erano inimmaginabili. Mentre continuiamo a spingere i confini di ciò che è possibile con l’apprendimento automatico distribuito, ci avviciniamo sempre di più a un futuro in cui la tecnologia diventa una parte invisibile ma integrante del nostro mondo.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#panoramica",
    "href": "contents/introduction/introduction.it.html#panoramica",
    "title": "1  Introduzione",
    "section": "",
    "text": "Figura 1.1: Ubiqutous computing.\n\n\n\n\n\nWeiser, Mark. 1991. «The Computer for the 21st Century». Sci. Am. 265 (3): 94–104. https://doi.org/10.1038/scientificamerican0991-94.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#cosa-cè-nel-libro",
    "href": "contents/introduction/introduction.it.html#cosa-cè-nel-libro",
    "title": "1  Introduzione",
    "section": "1.2 Cosa c’è nel Libro",
    "text": "1.2 Cosa c’è nel Libro\nIn questo libro, esploreremo le basi tecniche dei sistemi di apprendimento automatico onnipresenti, le sfide della creazione e distribuzione di questi sistemi nel continuum informatico e la vasta gamma di applicazioni che consentono. Un aspetto unico di questo libro è la sua funzione di canale verso opere accademiche fondamentali e documenti di ricerca accademica, mirati ad arricchire la comprensione del lettore e incoraggiare un’esplorazione più approfondita dell’argomento. Questo approccio cerca di colmare il divario tra materiali pedagogici e tendenze di ricerca all’avanguardia, offrendo una guida completa che è al passo con l’evoluzione del campo dell’apprendimento automatico applicato.\nPer migliorare l’esperienza di apprendimento, abbiamo incluso una varietà di materiali supplementari. In tutto il libro, si troveranno slide che riassumono i concetti chiave, video che forniscono spiegazioni e dimostrazioni approfondite, esercizi che rafforzano la comprensione ed esercizi pratici che offrono esperienza pratica con gli strumenti e le tecniche discussi. Queste risorse aggiuntive sono progettate per soddisfare diversi stili di apprendimento e contribuire ad acquisire una comprensione più profonda e pratica dell’argomento.\nIniziamo con i fondamenti, introducendo concetti chiave nei sistemi e nell’apprendimento automatico e fornendo un avvio al deep learning. Poi guidiamo attraverso il flusso di lavoro dell’IA, dall’ingegneria dei dati alla selezione dei framework di IA giusti. La sezione sul training copre le diverse tecniche di training dell’IA efficienti, ottimizzazioni dei modelli e accelerazione dell’IA tramite hardware specializzato. Successivamente si affronta il deployment [distribuzione], con capitoli sul benchmarking dell’IA, l’apprendimento distribuito e le operazioni di ML. Argomenti avanzati come sicurezza, privacy, IA responsabile, IA sostenibile, IA robusta e IA generativa vengono quindi esplorati in profondità. Il libro si conclude evidenziando l’impatto positivo dell’IA e il suo potenziale per il bene.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#come-orientarsi-in-questo-libro",
    "href": "contents/introduction/introduction.it.html#come-orientarsi-in-questo-libro",
    "title": "1  Introduzione",
    "section": "1.3 Come Orientarsi in Questo Libro",
    "text": "1.3 Come Orientarsi in Questo Libro\nPer ottenere il massimo da questo libro, consigliamo un approccio di apprendimento strutturato che sfrutti le varie risorse fornite. Ogni capitolo include slide, video, esercizi e laboratori per soddisfare diversi stili di apprendimento e rafforzare la comprensione. Inoltre, un bot tutor AI (SocratiQ AI) è prontamente disponibile per guidare attraverso i contenuti e fornire assistenza personalizzata.\n\nI Fondamenti (Capitoli 1-3): Si inizia costruendo una solida base con i primi capitoli, che forniscono un’introduzione all’intelligenza artificiale embedded e trattano argomenti fondamentali come sistemi embedded e deep learning.\nFlusso di Lavoro (Capitoli 4-6): Con questa base, si passa ai capitoli incentrati sugli aspetti pratici del processo di creazione del modello AI come flussi di lavoro, ingegneria dei dati e framework.\nTraining (Capitoli 7-10): Questi capitoli offrono approfondimenti su come addestrare efficacemente i modelli AI, comprese tecniche per efficienza, ottimizzazioni e accelerazione.\nDeployment (Capitoli 11-13): Si esamina come distribuire l’IA sui dispositivi e monitorarne l’operatività tramite metodi come benchmarking, on-device learning e MLOps.\nArgomenti Avanzati (Capitoli 14-18): Si esaminano criticamente argomenti come sicurezza, privacy, etica, sostenibilità, robustezza e IA generativa.\nImpatto Sociale (Capitolo 19): Esplora le applicazioni positive e il potenziale dell’IA per il bene della società.\nConclusione (Capitolo 20): Riflessioni sui principali risultati e sulle direzioni future nell’IA embedded.\n\nSebbene il libro sia progettato per un apprendimento progressivo, incoraggiamo un approccio di apprendimento interconnesso che consente di navigare tra i capitoli in base ai propri interessi e alle proprie esigenze. In tutto il libro si trovano casi di studio ed esercizi pratici che aiuteranno a mettere in relazione la teoria con le applicazioni del mondo reale. Consigliamo inoltre di partecipare a forum e gruppi per partecipare a discussioni, discutere concetti e condividere approfondimenti con altri studenti. Rivedere regolarmente i capitoli può aiutare a rafforzare l’apprendimento e offrire nuove prospettive sui concetti trattati. Adottando questo approccio strutturato ma flessibile e interagendo attivamente con i contenuti e la community, si farà un’esperienza di apprendimento appagante e arricchente che massimizza la comprensione.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#suddivisione-dei-capitoli",
    "href": "contents/introduction/introduction.it.html#suddivisione-dei-capitoli",
    "title": "1  Introduzione",
    "section": "1.4 Suddivisione dei Capitoli",
    "text": "1.4 Suddivisione dei Capitoli\nEcco uno sguardo più da vicino a cosa tratta ogni capitolo. Abbiamo strutturato il libro in sei sezioni principali: Nozioni Fondamentali, Flusso di lavoro, Training, Deployment, Argomenti avanzati e Impatto. Queste sezioni riflettono da vicino i componenti principali di una tipica pipeline di machine learning, dalla comprensione dei concetti di base al la deploy e alla manutenzione dei sistemi di intelligenza artificiale in applicazioni del mondo reale. Organizzando il contenuto in questo modo, miriamo a fornire una progressione logica che rispecchi il processo effettivo di sviluppo e implementazione di soluzioni di intelligenza artificiale embedded.\n\n1.4.1 Nozioni Fondamentali\nNella sezione Nozioni Fondamentali, poniamo le basi per comprendere l’intelligenza artificiale embedded. Introduciamo concetti chiave, forniamo una panoramica dei sistemi di apprendimento automatico e approfondiamo i principi e gli algoritmi di deep learning che alimentano le applicazioni di intelligenza artificiale nei sistemi embedded. Questa sezione fornisce le conoscenze essenziali necessarie per comprendere i capitoli successivi.\n\nIntroduzione: Questo capitolo prepara il terreno, fornendo una panoramica dell’intelligenza artificiale embedded e gettando le basi per i capitoli successivi.\nSistemi di ML: Introduciamo le basi dei sistemi di machine learning [apprendimento automatico], le piattaforme in cui gli algoritmi di intelligenza artificiale sono ampiamente applicati.\nAvvio al Deep Learning: Questo capitolo offre un’introduzione completa agli algoritmi e ai principi alla base delle applicazioni AI nei sistemi embedded.\n\n\n\n1.4.2 Workflow\nLa sezione Workflow [Flusso di lavoro] guida attraverso gli aspetti pratici della creazione di modelli AI. Analizziamo il flusso di lavoro AI, discutiamo le “best practice” di data engineering e passiamo in rassegna i framework AI più diffusi. Alla fine di questa sezione, si avrà una chiara comprensione dei passaggi coinvolti nello sviluppo di applicazioni AI competenti e degli strumenti disponibili per semplificare il processo.\n\nWorkflow IA: Questo capitolo analizza il flusso di lavoro di apprendimento automatico, offrendo approfondimenti sui passaggi che portano ad applicazioni AI competenti.\nIngegneria dei Dati: Ci concentriamo sull’importanza dei dati nei sistemi di IA, discutendo su come gestire e organizzare efficacemente i dati.\nFramework di IA: Questo capitolo esamina diversi framework per lo sviluppo di modelli di apprendimento automatico, guidando nella scelta di quello più adatto ai propri progetti.\n\n\n\n1.4.3 Training\nNella sezione Training, esploriamo tecniche per il training efficiente e affidabile di modelli di IA. Trattiamo strategie per raggiungere efficienza, ottimizzazioni dei modelli e il ruolo dell’hardware specializzato nell’accelerazione IA. Questa sezione fornisce le conoscenze per sviluppare modelli ad alte prestazioni che integrabili senza problemi nei sistemi embedded.\n\nTraining IA: Questo capitolo approfondisce il training [addestramento] dei modelli, esplorando tecniche per sviluppare modelli efficienti e affidabili.\nIA Efficiente: Qui, discutiamo strategie per raggiungere l’efficienza nelle applicazioni di IA, dall’ottimizzazione delle risorse computazionali al miglioramento delle prestazioni.\nOttimizzazioni dei Modelli: Esploriamo vari modi per ottimizzare i modelli di IA per un’integrazione senza soluzione di continuità nei sistemi embedded.\nAccelerazione IA: Discutiamo il ruolo dell’hardware specializzato nel migliorare le prestazioni dei sistemi IA embedded.\n\n\n\n1.4.4 Deployment\nLa sezione Deployment [distribuzione] si concentra sulle sfide e sulle soluzioni per l’implementazione di modelli AI su dispositivi embedded. Discutiamo metodi di benchmarking per valutare le prestazioni del sistema AI, tecniche per l’apprendimento “on-device” per migliorare l’efficienza e la privacy e i processi coinvolti nelle operazioni di ML. Questa sezione fornisce le competenze per implementare e mantenere in modo efficace le funzionalità di IA nei sistemi embedded.\n\nBenchmark dell’IA: Questo capitolo si concentra su come valutare i sistemi di IA tramite metodi di benchmarking sistematici.\nApprendimento On-Device: Esploriamo tecniche per l’apprendimento localizzato, che migliora sia l’efficienza che la privacy.\nOperazioni di ML: Questo capitolo esamina i processi coinvolti nell’integrazione, nel monitoraggio e nella manutenzione senza soluzione di continuità delle funzionalità di IA nei sistemi embedded.\n\n\n\n1.4.5 Argomenti Avanzati\nNella sezione Argomenti avanzati, studieremo le problematiche critiche che circondano l’intelligenza artificiale embedded. Affrontiamo le preoccupazioni relative a privacy e sicurezza, esploriamo i principi etici dell’intelligenza artificiale responsabile, discutiamo strategie per uno sviluppo sostenibile dell’intelligenza artificiale, esaminiamo tecniche per la creazione di modelli di intelligenza artificiale solidi e introduciamo l’entusiasmante campo dell’intelligenza artificiale generativa. Questa sezione amplia la comprensione del complesso panorama dell’intelligenza artificiale embedded e prepara ad affrontarne le sfide.\n\nSicurezza e Privacy: Man mano che l’intelligenza artificiale diventa sempre più onnipresente, questo capitolo affronta gli aspetti cruciali della privacy e della sicurezza nei sistemi di intelligenza artificiale embedded.\nIA Responsabile: Discutiamo i principi etici che guidano l’uso responsabile dell’intelligenza artificiale, concentrandoci sulla correttezza, responsabilità e trasparenza.\nIA Sostenibile: Questo capitolo esplora pratiche e strategie per un’intelligenza artificiale sostenibile, garantendo fattibilità a lungo termine e un impatto ambientale ridotto.\nIA Robusta: Parliamo di tecniche per sviluppare modelli di IA affidabili e robusti che possano funzionare in modo coerente in varie condizioni.\nIA Generativa: Questo capitolo esplora gli algoritmi e le tecniche alla base dell’IA generativa, aprendo strade all’innovazione e alla creatività.\n\n\n\n1.4.6 Impatto Sociale\nLa sezione Impatto Sociale evidenzia il potenziale trasformativo dell’IA embedded in vari domini. Presentiamo applicazioni reali di TinyML in sanità, agricoltura, conservazione e altre aree in cui l’IA sta facendo una positiva differenza. Questa sezione invoglia a sfruttare la potenza dell’AI embedded per il bene della società e a contribuire allo sviluppo di soluzioni di impatto.\n\nAI per il Bene: Evidenziamo applicazioni positive di TinyML in aree come sanità, agricoltura e la conservazione.\n\n\n\n1.4.7 Chiusura\nNella sezione Chiusura, riflettiamo sugli insegnamenti chiave del libro e guardiamo al futuro dell’IA embedded. Sintetizziamo i concetti trattati, discutiamo le tendenze emergenti e forniamo indicazioni su come proseguire nel percorso di apprendimento in questo campo in rapida evoluzione. Questa sezione lascia con una comprensione completa dell’intelligenza artificiale embedded e l’entusiasmo di applicare le conoscenze in modi innovativi.\n\nConclusione: Il libro si conclude con una riflessione sugli apprendimenti chiave e sulle direzioni future nel campo dell’intelligenza artificiale embedded.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/introduction/introduction.it.html#contributi-dei-lettori",
    "href": "contents/introduction/introduction.it.html#contributi-dei-lettori",
    "title": "1  Introduzione",
    "section": "1.5 Contributi dei Lettori",
    "text": "1.5 Contributi dei Lettori\nL’apprendimento nel mondo frenetico dell’intelligenza artificiale è un viaggio collaborativo. Ci siamo prefissati di coltivare una vivace comunità di studenti, innovatori e collaboratori. Esplorando i concetti e impegnandosi con gli esercizi, incoraggiamo a condividere le intuizioni ed esperienze personali. Che si tratti di un approccio innovativo, di un’applicazione interessante o di una domanda stimolante, i contributi dei singoli possono arricchire l’ecosistema di apprendimento. Partecipare alle discussioni, offrire e cercare indicazioni e collaborare a progetti per promuovere una cultura di crescita e apprendimento reciproci. Condividendo la conoscenza, si svolge un ruolo importante nel promuovere una comunità connessa, informata e potenziata a livello globale.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduzione</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html",
    "href": "contents/ml_systems/ml_systems.it.html",
    "title": "2  Sistemi di ML",
    "section": "",
    "text": "2.1 Introduzione\nIl ML si sta evolvendo rapidamente, con nuovi paradigmi che plasmano il modo in cui i modelli vengono sviluppati, addestrati e implementati. Uno di questi paradigmi è l’apprendimento automatico embedded, che sta vivendo un’innovazione significativa guidata dalla proliferazione di sensori intelligenti, dispositivi edge e microcontrollori. Il machine learning embedded si riferisce all’integrazione di algoritmi di apprendimento automatico nell’hardware di un dispositivo, consentendo l’elaborazione e l’analisi dei dati in tempo reale senza fare affidamento sulla connettività cloud. Questo capitolo esplora il panorama dell’apprendimento automatico embedded, coprendo gli approcci chiave di Cloud ML, Edge ML e TinyML (Figura 2.1).\nL’apprendimento automatico è iniziato con Cloud ML, dove potenti server nel cloud venivano utilizzati per addestrare ed eseguire grandi modelli di machine learning. Tuttavia, con l’aumento della necessità di elaborazione in tempo reale e a bassa latenza, è emerso l’Edge ML, avvicinando le capacità di inferenza alla fonte dei dati su dispositivi edge come gli smartphone. L’ultimo sviluppo in questa progressione è TinyML, che consente ai modelli ML di funzionare su microcontrollori con risorse estremamente limitate e piccoli sistemi embedded. TinyML consente l’inferenza sul dispositivo senza fare affidamento sulla connettività al cloud o all’edge, aprendo nuove possibilità per dispositivi intelligenti alimentati a batteria.\nFigura 2.2 mostra le principali differenze tra Cloud ML, Edge ML e TinyML in termini di hardware, latenza, connettività, requisiti di alimentazione e complessità del modello. Questa significativa disparità nelle risorse disponibili pone delle sfide quando si tenta di distribuire modelli di deep learning su microcontrollori, poiché questi modelli spesso richiedono memoria e storage sostanziali. Ad esempio, modelli di deep learning ampiamente utilizzati come ResNet-50 superano i limiti di risorse dei microcontrollori di un fattore di circa 100, mentre modelli più efficienti come MobileNet-V2 superano comunque questi vincoli di un fattore di circa 20. Anche se quantizzato per utilizzare interi a 8 bit (int8) per un utilizzo di memoria ridotto, MobileNetV2 richiede più di 5 volte la memoria solitamente disponibile su un microcontrollore, rendendo difficile adattare il modello a questi dispositivi minuscoli.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#introduzione",
    "href": "contents/ml_systems/ml_systems.it.html#introduzione",
    "title": "2  Sistemi di ML",
    "section": "",
    "text": "Figura 2.1: Cloud vs. Edge vs. TinyML: Lo Spettro dell’Intelligenza Distribuita. Fonte: ABI Research – TinyML.\n\n\n\n\n\n\n\n\n\n\n\nFigura 2.2: Dalle GPU cloud ai microcontrollori: Navigazione nel panorama della memoria e dell’archiviazione tra dispositivi di elaborazione. Fonte: (Lin et al. 2023)\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, e Song Han. 2023. «Tiny Machine Learning: Progress and Futures Feature». IEEE Circuits Syst. Mag. 23 (3): 8–34. https://doi.org/10.1109/mcas.2023.3302182.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#cloud-ml",
    "href": "contents/ml_systems/ml_systems.it.html#cloud-ml",
    "title": "2  Sistemi di ML",
    "section": "2.2 Cloud ML",
    "text": "2.2 Cloud ML\nCloud ML sfrutta potenti server nel cloud per il training e l’esecuzione di modelli ML complessi e di grandi dimensioni e si basa sulla connettività Internet.\n\n2.2.1 Caratteristiche\nDefinizione di Cloud ML\nIl Cloud Machine Learning (Cloud ML) è un sottocampo del machine learning che sfrutta la potenza e la scalabilità dell’infrastruttura di cloud computing per sviluppare, addestrare e distribuire modelli di machine learning. Utilizzando le vaste risorse computazionali disponibili nel cloud, Cloud ML consente la gestione efficiente di set di dati su larga scala e algoritmi di machine learning complessi.\nInfrastruttura Centralizzata\nUna delle caratteristiche principali di Cloud ML è la sua infrastruttura centralizzata. I provider di servizi cloud offrono una piattaforma virtuale composta da server ad alta capacità, soluzioni di storage espansive e architetture di rete robuste, tutte ospitate in data center distribuiti in tutto il mondo (Figura 2.3). Questa configurazione centralizzata consente la messa in comune e la gestione efficiente delle risorse computazionali, semplificando la scalabilità dei progetti di machine learning in base alle esigenze.\nElaborazione Dati Scalabile e Addestramento dei Modelli\nIl Cloud ML eccelle nella sua capacità di elaborare e analizzare enormi volumi di dati. L’infrastruttura centralizzata è progettata per gestire calcoli complessi e attività di model training che richiedono una notevole potenza di calcolo. Sfruttando la scalabilità del cloud, i modelli di apprendimento automatico possono essere addestrati su grandi quantità di dati, con conseguente miglioramento delle capacità di apprendimento e delle prestazioni predittive.\nDeployment Flessibile e Accessibilità\nUn altro vantaggio di Cloud ML è la flessibilità che offre in termini di deployment [distribuzione] e accessibilità. Una volta che un modello di machine learning è stato addestrato e convalidato, può essere facilmente distribuito e reso accessibile agli utenti tramite servizi basati su cloud. Ciò consente un’integrazione perfetta delle funzionalità di apprendimento automatico in varie applicazioni e servizi, indipendentemente dalla posizione o dal dispositivo dell’utente.\nCollaborazione e Condivisione delle Risorse\nIl Cloud ML promuove la collaborazione e la condivisione delle risorse tra team e organizzazioni. La natura centralizzata dell’infrastruttura cloud consente a più utenti di accedere e lavorare contemporaneamente sugli stessi progetti di apprendimento automatico. Questo approccio collaborativo facilita la condivisione delle conoscenze, accelera il processo di sviluppo e ottimizza l’utilizzo delle risorse.\nEfficacia dei Costi e Scalabilità\nSfruttando il modello di prezzo “pay-as-you-go” offerto dai provider di servizi cloud, Cloud ML consente alle organizzazioni di evitare i costi iniziali associati alla creazione e alla manutenzione della propria infrastruttura di machine learning. La capacità di aumentare o diminuire le risorse in base alla domanda garantisce economicità e flessibilità nella gestione dei progetti di apprendimento automatico.\nIl Cloud ML ha rivoluzionato il modo in cui ci si approccia all’apprendimento automatico, rendendolo più accessibile, scalabile ed efficiente. Ha aperto nuove possibilità per le organizzazioni di sfruttare la potenza dell’apprendimento automatico senza la necessità di investimenti significativi in hardware e infrastruttura.\n\n\n\n\n\n\nFigura 2.3: Data center Cloud TPU presso Google. Fonte: Google.\n\n\n\n\n\n2.2.2 Vantaggi\nIl Cloud ML offre diversi vantaggi significativi che lo rendono una scelta potente per i progetti di apprendimento automatico:\nImmensa Potenza di Calcolo\nUno dei principali vantaggi del Cloud ML è la sua capacità di fornire vaste risorse di calcolo. L’infrastruttura cloud è progettata per gestire algoritmi complessi ed elaborare grandi set di dati in modo efficiente. Ciò è particolarmente vantaggioso per i modelli di apprendimento automatico che richiedono una notevole potenza di calcolo, come reti di deep learning o modelli addestrati su enormi set di dati. Sfruttando le capacità di calcolo del cloud, le organizzazioni possono superare i limiti delle configurazioni hardware locali e ridimensionare i loro progetti di apprendimento automatico per soddisfare requisiti esigenti.\nScalabilità Dinamica\nIl Cloud ML offre scalabilità dinamica, consentendo alle organizzazioni di adattarsi facilmente alle mutevoli esigenze di calcolo. Man mano che il volume dei dati aumenta o la complessità dei modelli di apprendimento automatico aumenta, l’infrastruttura cloud può essere ridimensionata senza problemi verso l’alto o verso il basso per adattarsi a questi cambiamenti. Questa flessibilità garantisce prestazioni costanti e consente alle organizzazioni di gestire carichi di lavoro variabili senza la necessità di ingenti investimenti hardware. Col Cloud ML, le risorse possono essere allocate su richiesta, fornendo una soluzione conveniente ed efficiente per la gestione di progetti di machine learning.\nAccesso a Strumenti e Algoritmi Avanzati\nLe piattaforme Cloud ML forniscono accesso a un’ampia gamma di strumenti e algoritmi avanzati specificamente progettati per l’apprendimento automatico. Questi strumenti spesso includono librerie, framework e API predefiniti che semplificano lo sviluppo e l’implementazione di modelli di apprendimento automatico. Gli sviluppatori possono sfruttare queste risorse per accelerare la creazione, il training e l’ottimizzazione di modelli sofisticati. Utilizzando gli ultimi progressi negli algoritmi e nelle tecniche di apprendimento automatico, le organizzazioni possono rimanere all’avanguardia dell’innovazione e ottenere risultati migliori nei loro progetti di apprendimento automatico.\nAmbiente Collaborativo\nIl Cloud ML promuove un ambiente collaborativo che consente ai team di lavorare insieme senza problemi. La natura centralizzata dell’infrastruttura cloud consente a più utenti di accedere e contribuire agli stessi progetti di apprendimento automatico contemporaneamente. Questo approccio collaborativo facilita la condivisione delle conoscenze, promuove la collaborazione interfunzionale e accelera lo sviluppo e l’iterazione dei modelli di apprendimento automatico. I team possono condividere facilmente codice, set di dati e risultati, consentendo una collaborazione efficiente e guidando l’innovazione in tutta l’organizzazione.\nEfficacia in Termini di Costi\nL’adozione del Cloud ML può essere una soluzione conveniente per le organizzazioni, soprattutto rispetto alla creazione e alla manutenzione di un’infrastruttura di apprendimento automatico in sede. I provider di servizi cloud offrono modelli di prezzo flessibili, come piani pay-as-you-go o basati su abbonamento, consentendo alle organizzazioni di pagare solo per le risorse che consumano. Ciò elimina la necessità di investimenti di capitale iniziali in hardware e infrastruttura, riducendo il costo complessivo dell’implementazione di progetti di apprendimento automatico. Inoltre, la scalabilità di Cloud ML garantisce che le organizzazioni possano ottimizzare l’utilizzo delle risorse ed evitare l’eccesso di provisioning [fornitura], migliorando ulteriormente l’efficienza in termini di costi.\nI vantaggi di Cloud ML, tra cui l’immensa potenza di calcolo, la scalabilità dinamica, l’accesso a strumenti e algoritmi avanzati, l’ambiente collaborativo e la convenienza, lo rendono una scelta interessante per le organizzazioni che desiderano sfruttare il potenziale del machine learning. Sfruttando le capacità del cloud, le organizzazioni possono accelerare le proprie iniziative di machine learning, guidare l’innovazione e ottenere un vantaggio competitivo nell’attuale panorama basato sui dati.\n\n\n2.2.3 Sfide\nSebbene il Cloud ML offra numerosi vantaggi, presenta anche alcune sfide che le organizzazioni devono considerare:\nProblemi di Latenza\nUna delle principali sfide del Cloud ML è il potenziale di problemi della latenza, in particolare nelle applicazioni che richiedono risposte in tempo reale. Poiché i dati devono essere inviati dall’origine dei dati ai server cloud centralizzati per l’elaborazione e quindi di nuovo all’applicazione, potrebbero verificarsi ritardi dovuti alla trasmissione in rete. Questa latenza può rappresentare un notevole svantaggio in scenari sensibili al fattore tempo, come veicoli autonomi, rilevamento delle frodi in tempo reale o sistemi di controllo industriale, in cui è fondamentale prendere decisioni immediate. Gli sviluppatori devono progettare attentamente i propri sistemi per ridurre al minimo la latenza e garantire tempi di risposta accettabili.\nProblemi di Sicurezza e Privacy dei Dati\nLa centralizzazione dell’elaborazione e dell’archiviazione dei dati nel cloud può sollevare preoccupazioni sulla privacy e sulla sicurezza dei dati. Quando i dati sensibili vengono trasmessi e archiviati in data center remoti, diventano vulnerabili a potenziali attacchi informatici e accessi non autorizzati. I data center cloud possono diventare obiettivi interessanti per gli hacker che cercano di sfruttare le vulnerabilità e ottenere l’accesso a informazioni preziose. Le organizzazioni devono investire in misure di sicurezza robuste, come crittografia, controlli di accesso e monitoraggio continuo, per proteggere i propri dati nel cloud. Anche la conformità alle normative sulla privacy dei dati, come GDPR o HIPAA, diventa una considerazione critica quando si gestiscono dati sensibili nel cloud.\nConsiderazioni sui Costi\nCon l’aumento delle esigenze di elaborazione dei dati, i costi associati all’utilizzo dei servizi cloud possono aumentare. Mentre il Cloud ML offre scalabilità e flessibilità, le organizzazioni che gestiscono grandi volumi di dati potrebbero dover affrontare costi crescenti man mano che consumano più risorse cloud. Il modello di prezzo pay-as-you-go dei servizi cloud implica che i costi possono aumentare rapidamente, soprattutto per attività ad alta intensità di elaborazione come l’addestramento e l’inferenza dei modelli. Le organizzazioni devono monitorare e ottimizzare attentamente l’utilizzo del cloud per garantirne la convenienza. Potrebbero dover prendere in considerazione strategie come la compressione dei dati, la progettazione efficiente degli algoritmi e l’ottimizzazione dell’allocazione delle risorse per ridurre al minimo i costi pur ottenendo le prestazioni desiderate.\nDipendenza dalla Connettività Internet\nIl Cloud ML si basa su una connettività Internet stabile e affidabile per funzionare in modo efficace. Poiché i dati devono essere trasmessi da e verso il cloud, eventuali interruzioni o limitazioni nella connettività di rete possono influire sulle prestazioni e sulla disponibilità del sistema di apprendimento automatico. Questa dipendenza dalla connettività Internet può rappresentare un problema in scenari in cui l’accesso alla rete è limitato, inaffidabile o costoso. Le organizzazioni devono garantire un’infrastruttura di rete solida e considerare meccanismi di “failover” o capacità offline per mitigare l’impatto dei problemi di connettività.\nVendor Lock-In\nQuando si adotta il Cloud ML, le organizzazioni spesso diventano dipendenti dagli strumenti, dalle API e dai servizi specifici forniti dal fornitore cloud prescelto. Questo vendor lock-in [blocco da fornitore] può rendere difficile cambiare fornitore o migrare verso piattaforme diverse in futuro. Le organizzazioni possono affrontare sfide in termini di portabilità, interoperabilità e costi quando prendono in considerazione un cambiamento nel loro fornitore di Cloud ML. È importante valutare attentamente le offerte del fornitore, considerare obiettivi strategici a lungo termine e pianificare potenziali scenari di migrazione per ridurre al minimo i rischi associati al vendor lock-in.\nAffrontare queste sfide richiede un’attenta pianificazione, progettazione architettonica e strategie di mitigazione del rischio. Le organizzazioni devono soppesare i vantaggi del Cloud ML rispetto ai potenziali problemi e prendere decisioni informate in base ai loro requisiti specifici, alla sensibilità dei dati e agli obiettivi aziendali. Affrontando proattivamente queste sfide, le organizzazioni possono sfruttare efficacemente la potenza del Cloud ML garantendo al contempo la privacy dei dati, la sicurezza, l’economicità e l’affidabilità complessiva del sistema.\n\n\n2.2.4 Casi d’Uso di Esempio\nIl Cloud ML ha trovato ampia adozione in vari domini, rivoluzionando il modo in cui le aziende operano e gli utenti interagiscono con la tecnologia. Esploriamo alcuni esempi notevoli del Cloud ML in azione:\nAssistenti Virtuali\nIl Cloud ML svolge un ruolo cruciale nel potenziamento di assistenti virtuali come Siri e Alexa. Questi sistemi sfruttano le immense capacità computazionali del cloud per elaborare e analizzare gli input vocali in tempo reale. Sfruttando la potenza dell’elaborazione del linguaggio naturale e degli algoritmi di apprendimento automatico, gli assistenti virtuali possono comprendere le domande degli utenti, estrarre informazioni rilevanti e generare risposte intelligenti e personalizzate. La scalabilità e la potenza di elaborazione del cloud consentono a questi assistenti di gestire un vasto numero di interazioni utente contemporaneamente, offrendo un’esperienza utente fluida e reattiva.\nSistemi di Raccomandazione Commerciali\nIl Cloud ML costituisce la spina dorsale dei sistemi di raccomandazione avanzati utilizzati da piattaforme come Netflix e Amazon. Questi sistemi sfruttano la capacità del cloud di elaborare e analizzare enormi set di dati per scoprire modelli, preferenze e comportamenti degli utenti. Sfruttando il filtraggio collaborativo e altre tecniche di apprendimento automatico, i sistemi di raccomandazione possono offrire contenuti personalizzati o suggerimenti di prodotti su misura per gli interessi di ciascun utente. La scalabilità del cloud consente a questi sistemi di aggiornare e perfezionare continuamente le proprie raccomandazioni in base alla quantità sempre crescente di dati utente, migliorandone il coinvolgimento e la soddisfazione.\nRilevamento delle Frodi\nNel settore finanziario, il Cloud ML ha rivoluzionato i sistemi di rilevamento delle frodi. Sfruttando la potenza di calcolo del cloud, questi sistemi possono analizzare grandi quantità di dati transazionali in tempo reale per identificare potenziali attività fraudolente. Gli algoritmi di apprendimento automatico addestrati su modelli di frode storici possono rilevare anomalie e comportamenti sospetti, consentendo agli istituti finanziari di adottare misure proattive per prevenire le frodi e ridurre al minimo le perdite finanziarie. La capacità del cloud di elaborare e archiviare grandi volumi di dati lo rende una piattaforma ideale per implementare sistemi di rilevamento delle frodi robusti e scalabili.\nEsperienze Utente Personalizzate\nIl Cloud ML è profondamente integrato nelle nostre esperienze online, plasmando il modo in cui interagiamo con le piattaforme digitali. Dagli annunci personalizzati sui feed dei social media alle funzionalità di testo predittivo nei servizi di posta elettronica, il Cloud ML alimenta algoritmi intelligenti che migliorano il coinvolgimento e la praticità dell’utente. Consente ai siti di e-commerce di consigliare prodotti in base alla cronologia di navigazione e acquisto di un utente, ottimizza i motori di ricerca per fornire risultati accurati e pertinenti e automatizza il tagging e la categorizzazione delle foto su piattaforme come Facebook. Sfruttando le risorse di calcolo del cloud, questi sistemi possono apprendere e adattarsi continuamente alle preferenze dell’utente, offrendo un’esperienza utente più intuitiva e personalizzata.\nSicurezza e Rilevamento delle Anomalie\nIl Cloud ML svolge un ruolo nel rafforzare la sicurezza dell’utente alimentando i sistemi di rilevamento delle anomalie. Questi sistemi monitorano costantemente le attività dell’utente e i log di sistema per identificare pattern insoliti o comportamenti sospetti. Analizzando grandi quantità di dati in tempo reale, gli algoritmi Cloud ML possono rilevare potenziali minacce informatiche, come tentativi di accesso non autorizzati, infezioni da malware o violazioni dei dati. La scalabilità e la potenza di elaborazione del cloud consentono a questi sistemi di gestire la crescente complessità e il volume dei dati di sicurezza, fornendo un approccio proattivo per proteggere utenti e sistemi da potenziali minacce.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#edge-ml",
    "href": "contents/ml_systems/ml_systems.it.html#edge-ml",
    "title": "2  Sistemi di ML",
    "section": "2.3 Edge ML",
    "text": "2.3 Edge ML\n\n2.3.1 Caratteristiche\nDefinizione di Edge ML\nL’Edge Machine Learning (Edge ML) esegue algoritmi di apprendimento automatico direttamente sui dispositivi endpoint o più vicini al luogo in cui vengono generati i dati anziché affidarsi a server cloud centralizzati. Questo approccio mira ad avvicinare il calcolo alla fonte dei dati, riducendo la necessità di inviare grandi volumi di dati sulle reti, spesso con conseguente riduzione della latenza e miglioramento della privacy dei dati.\nElaborazione Dati Decentralizzata\nIn Edge ML, l’elaborazione dei dati avviene in modo decentralizzato. Invece di inviare dati a server remoti, i dati vengono elaborati localmente su dispositivi come smartphone, tablet o dispositivi Internet of Things (IoT) (Figura 2.4). Questa elaborazione locale consente ai dispositivi di prendere decisioni rapide in base ai dati che raccolgono senza affidarsi pesantemente alle risorse di un server centrale. Questa decentralizzazione è particolarmente importante nelle applicazioni in tempo reale in cui anche un leggero ritardo può avere conseguenze significative.\nArchiviazione e Calcolo dei Dati Locali\nL’archiviazione e il calcolo dei dati locali sono caratteristiche chiave di Edge ML. Questa configurazione garantisce che i dati possano essere archiviati e analizzati direttamente sui dispositivi, mantenendo così la privacy dei dati e riducendo la necessità di una connettività Internet costante. Inoltre, questo spesso porta a un calcolo più efficiente, poiché i dati non devono percorrere lunghe distanze e i calcoli vengono eseguiti con una comprensione più consapevole del contesto locale, che a volte può portare ad analisi più approfondite.\n\n\n\n\n\n\nFigura 2.4: Esempi di Edge ML. Fonte: Edge Impulse.\n\n\n\n\n\n2.3.2 Vantaggi\nLatenza Ridotta\nUno dei principali vantaggi di Edge ML è la significativa riduzione della latenza rispetto al Cloud ML. Questa ridotta latenza può essere un vantaggio fondamentale in situazioni in cui i millisecondi contano, come nei veicoli autonomi, dove un rapido processo decisionale può fare la differenza tra sicurezza e incidente.\nPrivacy dei Dati Migliorata\nEdge ML offre anche una migliore privacy dei dati, poiché i dati vengono principalmente archiviati ed elaborati localmente. Ciò riduce al minimo il rischio di violazioni dei dati, più comuni nelle soluzioni di archiviazione dati centralizzate. Le informazioni sensibili possono essere mantenute più sicure, poiché non vengono inviate su reti che potrebbero essere intercettate.\nMinore Utilizzo della Larghezza di Banda\nOperare più vicino alla fonte dei dati significa che meno dati devono essere inviati sulle reti, riducendo l’utilizzo della larghezza di banda. Ciò può comportare risparmi sui costi e guadagni di efficienza, soprattutto in ambienti in cui la larghezza di banda è limitata o costosa.\n\n\n2.3.3 Sfide\nRisorse di Calcolo Limitate Rispetto al Cloud ML\nTuttavia, Edge ML presenta le sue sfide. Una delle principali preoccupazioni sono le risorse di calcolo limitate rispetto alle soluzioni basate su cloud. I dispositivi endpoint possono avere una potenza di elaborazione o una capacità di archiviazione diverse rispetto ai server cloud, limitando la complessità dei modelli di apprendimento automatico che possono essere distribuiti.\nComplessità nella Gestione dei Nodi Edge\nLa gestione di una rete di nodi Edge può introdurre complessità, soprattutto per quanto riguarda coordinamento, aggiornamenti e manutenzione. Garantire che tutti i nodi funzionino senza problemi e siano aggiornati con gli algoritmi e i protocolli di sicurezza più recenti può essere una sfida logistica.\nProblemi di Sicurezza nei Nodi Edge\nSebbene Edge ML offra una maggiore privacy dei dati, i nodi Edge possono talvolta essere più vulnerabili ad attacchi fisici e informatici. Sviluppare protocolli di sicurezza affidabili che proteggano i dati su ogni nodo senza compromettere l’efficienza del sistema, resta una sfida significativa nell’implementazione di soluzioni Edge ML.\n\n\n2.3.4 Casi d’Uso di Esempio\nEdge ML ha molte applicazioni, dai veicoli autonomi e dalle case intelligenti all’IoT industriale. Questi esempi sono stati scelti per evidenziare scenari in cui l’elaborazione dei dati in tempo reale, la latenza ridotta e la privacy migliorata non sono solo vantaggiose, ma spesso fondamentali per il funzionamento e il successo di queste tecnologie. Dimostrano il ruolo che Edge ML può svolgere nel guidare i progressi in vari settori, promuovendo l’innovazione e aprendo la strada a sistemi più intelligenti, reattivi e adattabili.\nVeicoli Autonomi\nI veicoli autonomi sono un esempio lampante del potenziale di Edge ML. Questi veicoli si affidano in larga misura all’elaborazione dei dati in tempo reale per navigare e prendere decisioni. I modelli di apprendimento automatico localizzati aiutano ad analizzare rapidamente i dati da vari sensori per prendere decisioni di guida immediate, garantendo sicurezza e funzionamento regolare.\nCase ed Edifici Intelligenti\nEdge ML svolge un ruolo cruciale nella gestione efficiente di vari sistemi in case ed edifici intelligenti, dall’illuminazione e dal riscaldamento alla sicurezza. Elaborando i dati localmente, questi sistemi possono funzionare in modo più reattivo e armonioso con le abitudini e le preferenze degli occupanti, creando un ambiente di vita più confortevole.\nIoT industriale\nL’Internet of Things (IoT) industriale sfrutta Edge ML per monitorare e controllare processi industriali complessi. Qui, i modelli di apprendimento automatico possono analizzare i dati da numerosi sensori in tempo reale, consentendo la manutenzione predittiva, ottimizzando le operazioni e migliorando le misure di sicurezza. Questa rivoluziona l’automazione e l’efficienza industriale.\nL’applicabilità di Edge ML è vasta e non si limita a questi esempi. Vari altri settori, tra cui sanità, agricoltura e pianificazione urbana, stanno esplorando e integrando l’Edge ML per sviluppare soluzioni innovative che rispondono alle esigenze e alle sfide del mondo reale, annunciando una nuova era di sistemi intelligenti e interconnessi.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#tiny-ml",
    "href": "contents/ml_systems/ml_systems.it.html#tiny-ml",
    "title": "2  Sistemi di ML",
    "section": "2.4 Tiny ML",
    "text": "2.4 Tiny ML\n\n2.4.1 Caratteristiche\nDefinizione di TinyML\nTinyML si colloca all’incrocio tra sistemi embedded e apprendimento automatico, rappresentando un campo in rapida crescita che porta algoritmi intelligenti direttamente a microcontrollori e sensori minuscoli. Questi microcontrollori operano con gravi limitazioni di risorse, in particolare per quanto riguarda memoria, archiviazione e potenza di calcolo (vedere un esempio di kit TinyML in Figura 2.5).\nMachine Learning On-Device\nIn TinyML, l’attenzione è rivolta all’apprendimento automatico sul dispositivo. Ciò significa che i modelli di apprendimento automatico vengono distribuiti e addestrati sul dispositivo, eliminando la necessità di server esterni o infrastrutture cloud. Ciò consente a TinyML di abilitare un processo decisionale intelligente proprio dove vengono generati i dati, rendendo possibili approfondimenti e azioni in tempo reale, anche in contesti in cui la connettività è limitata o non disponibile.\nAmbienti a Basso Consumo Energetico e con Risorse Limitate\nTinyML eccelle in contesti a basso consumo energetico e con risorse limitate. Questi ambienti richiedono soluzioni altamente ottimizzate che funzionino entro le risorse disponibili. TinyML soddisfa questa esigenza tramite algoritmi e modelli specializzati progettati per offrire prestazioni decenti consumando energia minima, garantendo così periodi operativi prolungati, anche nei dispositivi alimentati a batteria.\n\n\n\n\n\n\nFigura 2.5: Esempi di kit di dispositivi TinyML. Fonte: Widening Access to Applied Machine Learning with TinyML.\n\n\n\n\n\n\n\n\n\nEsercizio 2.1: TinyML con Arduino\n\n\n\n\n\nPrepararsi a portare l’apprendimento automatico sui dispositivi più piccoli! Nel mondo dell’apprendimento automatico embedded, TinyML è il luogo in cui i vincoli di risorse incontrano l’ingegnosità. Questo notebook Colab guiderà nella creazione di un modello di riconoscimento dei gesti progettato su una scheda Arduino. Si imparerà come addestrare una piccola ma efficace rete neurale, ottimizzarla per un utilizzo minimo di memoria e distribuirla al proprio microcontrollore. Se si è entusiasti di rendere più intelligenti gli oggetti di uso quotidiano, è qui che si inizia!\n\n\n\n\n\n\n2.4.2 Vantaggi\nLatenza Estremamente Bassa\nUno dei vantaggi più importanti di TinyML è la sua capacità di offrire una latenza estremamente bassa. Poiché il calcolo avviene direttamente sul dispositivo, il tempo necessario per inviare dati a server esterni e ricevere una risposta viene eliminato. Ciò è fondamentale nelle applicazioni che richiedono un processo decisionale immediato, consentendo risposte rapide a condizioni mutevoli.\nElevata Sicurezza dei Dati\nTinyML migliora intrinsecamente la sicurezza dei dati. Poiché l’elaborazione e l’analisi dei dati avvengono sul dispositivo, il rischio di intercettazione dei dati durante la trasmissione viene praticamente eliminato. Questo approccio localizzato alla gestione dei dati garantisce che le informazioni sensibili rimangano sul dispositivo, rafforzando la sicurezza dei dati dell’utente.\nEfficienza Energetica\nTinyML opera all’interno di un framework efficiente dal punto di vista energetico, una necessità dati i suoi ambienti con risorse limitate. Utilizzando algoritmi snelli e metodi di calcolo ottimizzati, TinyML garantisce che i dispositivi possano eseguire attività complesse senza esaurire rapidamente la durata della batteria, il che lo rende un’opzione sostenibile per le distribuzioni a lungo termine.\n\n\n2.4.3 Sfide\nCapacità di Calcolo Limitate\nTuttavia, il passaggio a TinyML comporta una serie di ostacoli. La limitazione principale sono le capacità di calcolo limitate dei dispositivi. La necessità di operare entro tali limiti implica che i modelli distribuiti debbano essere semplificati, il che potrebbe influire sull’accuratezza e la complessità delle soluzioni.\nCiclo di Sviluppo Complesso\nTinyML introduce anche un ciclo di sviluppo complicato. La creazione di modelli leggeri ed efficaci richiede una profonda comprensione dei principi di apprendimento automatico e competenza nei sistemi embedded. Questa complessità richiede un approccio di sviluppo collaborativo, in cui la competenza multi-dominio è essenziale per il successo.\nOttimizzazione e Compressione del Modello\nUna sfida centrale in TinyML è l’ottimizzazione e la compressione del modello. La creazione di modelli di machine learning in grado di operare efficacemente all’interno della memoria limitata e della potenza di calcolo dei microcontrollori richiede approcci innovativi alla progettazione del modello. Gli sviluppatori si trovano spesso ad affrontare la sfida di trovare un delicato equilibrio e ottimizzare i modelli per mantenere l’efficacia, pur rispettando rigidi vincoli di risorse.\n\n\n2.4.4 Casi d’Uso di Esempio\nDispositivi Indossabili\nNei dispositivi indossabili, TinyML apre le porte a gadget più intelligenti e reattivi. Dai fitness tracker che offrono feedback in tempo reale sugli allenamenti agli occhiali intelligenti che elaborano dati visivi al volo, TinyML trasforma il modo in cui interagiamo con la tecnologia indossabile, offrendo esperienze personalizzate direttamente dal dispositivo.\nManutenzione Predittiva\nNegli ambienti industriali, TinyML svolge un ruolo significativo nella manutenzione predittiva. Implementando algoritmi TinyML su sensori che monitorano lo stato di salute delle apparecchiature, le aziende possono identificare preventivamente potenziali problemi, riducendo i tempi di inattività e prevenendo costosi guasti. L’analisi dei dati in loco garantisce risposte rapide, impedendo potenzialmente a piccoli problemi di diventare problemi gravi.\nRilevamento delle Anomalie\nTinyML può essere impiegato per creare modelli di rilevamento delle anomalie che identificano pattern di dati insoliti. Ad esempio, una fabbrica intelligente potrebbe usare TinyML per monitorare i processi industriali e individuare anomalie, aiutando a prevenire incidenti e migliorare la qualità del prodotto. Allo stesso modo, un’azienda di sicurezza potrebbe usare TinyML per monitorare il traffico di rete per modelli insoliti, aiutando a rilevare e prevenire attacchi informatici. TinyML potrebbe monitorare i dati dei pazienti per anomalie nell’assistenza sanitaria, aiutando a rilevare precocemente le malattie e a migliorare il trattamento dei pazienti.\nMonitoraggio Ambientale\nNel monitoraggio ambientale, TinyML consente l’analisi dei dati in tempo reale da vari sensori distribuiti sul campo. Questi potrebbero spaziare dal monitoraggio della qualità dell’aria in città al tracciamento della fauna selvatica nelle aree protette. Tramite TinyML, i dati possono essere elaborati localmente, consentendo risposte rapide alle mutevoli condizioni e fornendo una comprensione adeguata dei modelli ambientali, cruciale per un processo decisionale informato.\nIn sintesi, TinyML funge da pioniere nell’evoluzione dell’apprendimento automatico, promuovendo l’innovazione in vari campi portando l’intelligenza direttamente nell’Edge. Il suo potenziale di trasformare la nostra interazione con la tecnologia e il mondo è immenso, promettendo un futuro in cui i dispositivi sono connessi, intelligenti e capaci di prendere decisioni e rispondere in tempo reale.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#confronto",
    "href": "contents/ml_systems/ml_systems.it.html#confronto",
    "title": "2  Sistemi di ML",
    "section": "2.5 Confronto",
    "text": "2.5 Confronto\nFino a questo punto, abbiamo esplorato singolarmente ciascuna delle diverse varianti di ML. Ora, mettiamole insieme per una visione completa. Tabella 2.1 offre un’analisi comparativa di Cloud ML, Edge ML e TinyML basata su varie caratteristiche e aspetti. Questo confronto mira a fornire una chiara prospettiva sui vantaggi unici e sui fattori distintivi, aiutando a prendere decisioni informate in base alle esigenze e ai vincoli specifici di una determinata applicazione o progetto.\n\n\n\nTabella 2.1: Confronto degli aspetti delle funzionalità tra Cloud ML, Edge ML e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nUbicazione dell’elaborazione\nServer centralizzati (Data Center)\nDispositivi locali (più vicini alle fonti di dati)\nSul dispositivo (microcontrollori, sistemi embedded)\n\n\nLatenza\nAlta (dipende dalla connettività Internet)\nModerata (latenza ridotta rispetto a Cloud ML)\nBassa (elaborazione immediata senza ritardo di rete)\n\n\nPrivacy dei dati\nModerata (dati trasmessi tramite reti)\nAlta (i dati rimangono sulle reti locali)\nMolto alta (dati elaborati sul dispositivo, non trasmessi)\n\n\nPotenza di calcolo\nAlta (usa una potente infrastruttura del data center)\nModerata (utilizza le capacità del dispositivo locale)\nBassa (limitata alla potenza del sistema embedded )\n\n\nConsumo energetico\nAlto (i data center consumano molta energia)\nModerato (meno dei data center, più di TinyML)\nBasso (alta efficienza energetica, progettato per bassi consumi)\n\n\nScalabilità\nAlto (facile da scalare con risorse server aggiuntive)\nModerato (dipende dalle capacità del dispositivo locale)\nBasso (limitato dalle risorse hardware del dispositivo)\n\n\nCosto\nAlto (costi ricorrenti per l’uso del server, manutenzione)\nVariabile (dipende dalla complessità della configurazione locale)\nBasso (principalmente costi iniziali per i componenti hardware)\n\n\nConnettività\nAlto (richiede una connettività Internet stabile)\nBasso (può funzionare con connettività intermittente)\nMolto basso (può funzionare senza alcuna connettività di rete)\n\n\nElaborazione in tempo reale\nModerata (può essere influenzata dalla latenza di rete)\nAlta (capace di elaborazione in tempo reale localmente)\nMolto alta (elaborazione immediata con latenza minima)\n\n\nEsempi di applicazione\nAnalisi di Big Data, Assistenti virtuali\nVeicoli autonomi, Case intelligenti\nDispositivi indossabili, Reti di sensori\n\n\nComplessità\nDa moderata ad alta (richiede conoscenza del cloud computing)\nModerata (richiede conoscenza della configurazione della rete locale)\nDa moderata ad alta (richiede competenza nei sistemi embedded)",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#conclusione",
    "href": "contents/ml_systems/ml_systems.it.html#conclusione",
    "title": "2  Sistemi di ML",
    "section": "2.6 Conclusione",
    "text": "2.6 Conclusione\nIn questo capitolo, abbiamo offerto una panoramica in evoluzione dell’apprendimento automatico, che copre i paradigmi cloud, edge e tiny ML. L’apprendimento automatico basato su cloud sfrutta le immense risorse computazionali delle piattaforme cloud per abilitare modelli potenti e accurati, ma presenta delle limitazioni, tra cui problemi di latenza e privacy. Edge ML mitiga queste limitazioni portando l’inferenza direttamente sui dispositivi edge, offrendo una latenza inferiore e ridotte esigenze di connettività. TinyML va oltre, miniaturizzando i modelli ML per eseguirli direttamente su dispositivi con risorse altamente limitate, aprendo una nuova categoria di applicazioni intelligenti.\nOgni approccio ha i suoi compromessi, tra cui complessità del modello, latenza, privacy e costi dell’hardware. Nel tempo, prevediamo la convergenza di questi approcci ML embedded, col pre-training cloud che facilita implementazioni edge e tiny ML più sofisticate. Progressi come l’apprendimento federato e l’apprendimento “on-device” consentiranno ai dispositivi embedded di perfezionare i propri modelli imparando dai dati del mondo reale.\nIl panorama ML embedded si sta evolvendo rapidamente ed è pronto a consentire applicazioni intelligenti su un ampio spettro di dispositivi e casi d’uso. Questo capitolo funge da “istantanea” dello stato attuale del ML embedded. Man mano che algoritmi, hardware e connettività continuano a migliorare, possiamo aspettarci che i dispositivi embedded di tutte le dimensioni diventino sempre più capaci, sbloccando nuove applicazioni trasformative per l’intelligenza artificiale.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/ml_systems/ml_systems.it.html#sec-ml-systems-resource",
    "href": "contents/ml_systems/ml_systems.it.html#sec-ml-systems-resource",
    "title": "2  Sistemi di ML",
    "section": "2.7 Risorse",
    "text": "2.7 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nEmbedded Systems Overview.\nEmbedded Computer Hardware.\nEmbedded I/O.\nEmbedded systems software.\nEmbedded ML software.\nEmbedded Inference.\nTinyML on Microcontrollers.\nTinyML as a Service (TinyMLaaS):\n\nTinyMLaaS: Introduction.\nTinyMLaaS: Design Overview.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sistemi di ML</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html",
    "href": "contents/dl_primer/dl_primer.it.html",
    "title": "3  Avvio al Deep Learning",
    "section": "",
    "text": "3.1 Introduzione",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html#introduzione",
    "href": "contents/dl_primer/dl_primer.it.html#introduzione",
    "title": "3  Avvio al Deep Learning",
    "section": "",
    "text": "3.1.1 Definizione e Importanza\nIl deep learning, un’area specializzata nell’apprendimento automatico e nell’intelligenza artificiale (IA), utilizza algoritmi modellati sulla struttura e la funzione del cervello umano, noti come reti neurali artificiali. Questo campo è un elemento fondamentale nell’IA, che guida il progresso in diversi settori come la visione artificiale, l’elaborazione del linguaggio naturale e i veicoli a guida autonoma. La sua importanza nei sistemi di IA embedded è evidenziata dalla sua capacità di gestire calcoli e previsioni intricati, ottimizzando le risorse limitate nelle impostazioni embedded. La Figura 3.1 illustra lo sviluppo cronologico e la segmentazione relativa dei tre campi.\n\n\n\n\n\n\nFigura 3.1: Il diagramma illustra l’intelligenza artificiale come campo onnicomprensivo che comprende tutti i metodi computazionali che imitano le funzioni cognitive umane. Il Machine learning [apprendimento automatico] è un sottoinsieme dell’IA che include algoritmi in grado di apprendere dai dati. Il deep learning, un ulteriore sottoinsieme del ML, coinvolge specificamente reti neurali in grado di apprendere pattern [schemi] più complessi in grandi volumi di dati. Fonte: NVIDIA.\n\n\n\n\n\n3.1.2 Breve Storia del Deep Learning\nL’idea del deep learning ha origine nelle prime reti neurali artificiali. Ha vissuto diversi cicli di interesse, a partire dall’introduzione del Perceptron negli anni ’50 (Rosenblatt 1957), seguita dall’invenzione degli algoritmi di backpropagation negli anni ’80 (Rumelhart, Hinton, e Williams 1986).\n\nRosenblatt, Frank. 1957. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory.\n\nRumelhart, David E., Geoffrey E. Hinton, e Ronald J. Williams. 1986. «Learning representations by back-propagating errors». Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. «ImageNet Classification with Deep Convolutional Neural Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\nIl termine “deep learning” è diventato importante negli anni 2000, caratterizzato da progressi nella potenza di calcolo e nell’accessibilità dei dati. Traguardi importanti includono l’addestramento di successo di reti profonde come AlexNet (Krizhevsky, Sutskever, e Hinton 2012) da parte di Geoffrey Hinton, una figura di spicco nell’intelligenza artificiale, e il rinnovato focus sulle reti neurali come strumenti efficaci per l’analisi e la modellazione dei dati.\nIl deep learning ha recentemente registrato una crescita esponenziale, trasformando vari settori. La crescita computazionale ha seguito un modello di raddoppio di 18 mesi dal 1952 al 2010, che poi ha accelerato fino a un ciclo di 6 mesi dal 2010 al 2022, come mostrato in Figura 3.2. Contemporaneamente, abbiamo assistito all’emergere di modelli su larga scala tra il 2015 e il 2022, che sono apparsi da 2 a 3 ordini di grandezza più veloci e hanno seguito un ciclo di raddoppio di 10 mesi.\n\n\n\n\n\n\nFigura 3.2: Crescita dei modelli di deep learning.\n\n\n\nMolteplici fattori hanno contribuito a questa impennata, tra cui i progressi nella potenza computazionale, l’abbondanza di big data e i miglioramenti nei progetti algoritmici. In primo luogo, la crescita delle capacità computazionali, in particolare l’arrivo delle Graphics Processing Units (GPU) [unità di elaborazione grafica] e delle Tensor Processing Units (TPU) [unità di elaborazione tensoriale] (Jouppi et al. 2017), ha accelerato notevolmente i tempi di training e inferenza dei modelli di apprendimento profondo. Questi miglioramenti hardware hanno consentito la costruzione e il training di reti più complesse e profonde di quanto fosse possibile negli anni precedenti.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\nIn secondo luogo, la rivoluzione digitale ha prodotto una grande quantità di big data, offrendo materiale ricco da cui i modelli di deep learning possono imparare e distinguersi in attività quali il riconoscimento di immagini e parlato, la traduzione linguistica e il gioco. I grandi set di dati etichettati sono stati fondamentali per perfezionare e distribuire con successo applicazioni di deep learning in contesti reali.\nInoltre, le collaborazioni e gli sforzi open source hanno alimentato una comunità dinamica di ricercatori e professionisti, accelerando i progressi nelle tecniche di deep learning. Innovazioni come il “deep reinforcement learning”, il “transfer learning” e l’intelligenza artificiale generativa hanno ampliato la portata di ciò che è realizzabile col deep learning, aprendo nuove possibilità in vari settori, tra cui sanità, finanza, trasporti e intrattenimento.\nLe organizzazioni di tutto il mondo riconoscono il potenziale trasformativo del deep learning e investono molto in ricerca e sviluppo per sfruttare le sue capacità nel fornire soluzioni innovative, ottimizzare le operazioni e creare nuove opportunità di business. Mentre il deep learning continua la sua traiettoria ascendente, è destinato a ridefinire il modo in cui interagiamo con la tecnologia, migliorando la praticità, la sicurezza e la connettività nelle nostre vite.\n\n\n3.1.3 Applicazioni del Deep Learning\nIl deep learning è oggi ampiamente utilizzato in numerosi settori e il suo impatto trasformativo sulla società è evidente. Nella finanza, alimenta le previsioni del mercato azionario, la valutazione del rischio e il rilevamento delle frodi. Ad esempio, gli algoritmi di deep learning possono prevedere le tendenze del mercato azionario, guidare le strategie di investimento e migliorare le decisioni finanziarie. Nel marketing, guida la segmentazione dei clienti, la personalizzazione e l’ottimizzazione dei contenuti. Il deep learning analizza il comportamento e le preferenze dei consumatori per abilitare pubblicità altamente mirate e la distribuzione di contenuti personalizzati. Nella produzione, il deep learning semplifica i processi di produzione e migliora il controllo di qualità analizzando continuamente grandi volumi di dati. Ciò consente alle aziende di aumentare la produttività e ridurre al minimo gli sprechi, portando alla produzione di beni di qualità superiore a costi inferiori. Nell’assistenza sanitaria, il machine learning aiuta nella diagnosi, nella pianificazione del trattamento e nel monitoraggio dei pazienti. Allo stesso modo, il deep learning può fare previsioni mediche che migliorano la diagnosi dei pazienti e salvano vite. I vantaggi sono chiari: il machine learning prevede con maggiore accuratezza degli esseri umani e lo fa molto più rapidamente.\nIl deep learning migliora i prodotti di uso quotidiano, come il rafforzamento dei sistemi di raccomandazione di Netflix per fornire agli utenti più consigli personalizzati. In Google, i modelli di deep learning hanno portato a notevoli miglioramenti in Google Translate, consentendogli di gestire oltre 100 lingue. I veicoli autonomi di aziende come Waymo, Cruise e Motional sono diventati realtà grazie all’uso del deep learning nel loro sistema di percezione. Inoltre, Amazon impiega l’edge deep learning nei suoi dispositivi Alexa per eseguire l’individuazione delle parole chiave.\n\n\n3.1.4 Rilevanza per l’IA Embedded\nL’IA embedded, l’integrazione di algoritmi di intelligenza artificiale direttamente nei dispositivi hardware, trae naturalmente vantaggio dalle capacità del deep learning. La combinazione di algoritmi di deep learning e sistemi embedded ha gettato le basi per dispositivi intelligenti e autonomi in grado di analisi avanzate on-device [sul dispositivo]. Il deep learning aiuta a estrarre pattern e informazioni complesse dai dati di input, il che è essenziale nello sviluppo di sistemi embedded intelligenti, dagli elettrodomestici ai macchinari industriali. Questa collaborazione mira a inaugurare una nuova era di dispositivi intelligenti e interconnessi in grado di apprendere e adattarsi al comportamento dell’utente e alle condizioni ambientali, ottimizzando le prestazioni e offrendo praticità ed efficienza senza precedenti.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html#reti-neurali",
    "href": "contents/dl_primer/dl_primer.it.html#reti-neurali",
    "title": "3  Avvio al Deep Learning",
    "section": "3.2 Reti Neurali",
    "text": "3.2 Reti Neurali\nIl deep learning trae ispirazione dalle reti neurali del cervello umano per creare modelli decisionali. Questa sezione approfondisce i concetti fondamentali del deep learning, offrendo approfondimenti sugli argomenti più complessi trattati più avanti in questa introduzione.\nLe reti neurali fungono da fondamento del deep learning, ispirate alle reti neurali biologiche nel cervello umano per elaborare e analizzare i dati in modo gerarchico. Le reti neurali sono composte da unità di base chiamate perceptron, che sono solitamente organizzate in layer [strati]. Ogni layer è costituito da diversi perceptron e più layer sono impilati per formare l’intera rete. Le connessioni tra questi layer sono definite da insiemi di pesi o parametri che determinano come i dati vengono elaborati mentre fluiscono dall’input all’output della rete.\nDi seguito, esaminiamo i componenti e le strutture primarie nelle reti neurali.\n\n3.2.1 Perceptron\nIl Perceptron è l’unità di base o il nodo che costituisce la base per strutture più complesse. Funziona prendendo più input, ognuno dei quali rappresenta una caratteristica dell’oggetto in analisi, come le caratteristiche di una casa per prevederne il prezzo o gli attributi di una canzone per prevederne la popolarità nei servizi di streaming musicale. Questi input sono indicati come \\(x_1, x_2, ..., x_n\\).\nCiascun input \\(x_i\\) ha un peso corrispondente \\(w_{ij}\\) e il perceptron moltiplica semplicemente ogni input per il suo peso corrispondente. Questa operazione è simile alla regressione lineare, dove l’output intermedio, \\(z\\), è calcolato come la somma dei prodotti degli input e dei loro pesi:\n\\[\nz = \\sum (x_i \\cdot w_{ij})\n\\]\nA questo calcolo intermedio, viene aggiunto un termine di bias \\(b\\), che consente al modello di adattarsi meglio ai dati spostando la funzione di output lineare verso l’alto o verso il basso. Pertanto, la combinazione lineare intermedia calcolata dal perceptron, incluso il bias, diventa:\n\\[\nz = \\sum (x_i \\cdot w_{ij}) + b\n\\]\nQuesta forma base di un perceptron può modellare solo relazioni lineari tra input e output. I pattern trovati in natura sono spesso complessi e si estendono oltre le relazioni lineari. Per consentire al perceptron di gestire relazioni non lineari, una funzione di attivazione viene applicata all’output lineare \\(z\\).\n\\[\n\\hat{y} = \\sigma(z)\n\\]\nFigura 3.3 illustra un esempio in cui i dati presentano un andamento non lineare che non potrebbe essere modellato adeguatamente con un approccio lineare. La funzione di attivazione, come la sigmoide, la tanh o la ReLU, trasforma la somma di input lineare in un output non lineare. L’obiettivo principale di questa funzione è introdurre la non linearità nel modello, consentendogli di apprendere ed eseguire attività più sofisticate. Pertanto, l’output finale del perceptron, inclusa la funzione di attivazione, può essere espresso come:\n\n\n\n\n\n\nFigura 3.3: Le funzioni di attivazione consentono la modellazione di relazioni non lineari complesse. Fonte: Medium - Sachin Kaushik.\n\n\n\nUn perceptron può essere configurato per eseguire attività di regressione o classificazione. Per la regressione, viene utilizzato l’output numerico effettivo \\(\\hat{y}\\). Per la classificazione, l’output dipende dal fatto che \\(\\hat{y}\\) superi una determinata soglia. Se \\(\\hat{y}\\) supera questa soglia, il perceptron potrebbe restituire una classe (ad esempio, ‘yes’) e, in caso contrario, un’altra classe (ad esempio, ‘no’).\n\n\n\n\n\n\nFigura 3.4: Perceptron. Concepiti negli anni ’50, i perceptron hanno aperto la strada allo sviluppo di reti neurali più complesse e sono stati un elemento fondamentale nel deep learning. Fonte: Wikimedia - Chrislb.\n\n\n\nFigura 3.4 illustra gli elementi fondamentali di un perceptron, che funge da fondamento per reti neurali più complesse. Un perceptron può essere pensato come un decisore in miniatura, che utilizza i suoi pesi, il sui bias [polarizzazione] e la sua funzione di attivazione per elaborare input e generare output in base ai parametri appresi. Questo concetto costituisce la base per comprendere architetture di reti neurali più complesse, come i perceptron multilayer [multistrato]. In queste strutture avanzate, i layer di perceptron lavorano di concerto, con l’output di ogni layer che funge da input per il layer successivo. Questa disposizione gerarchica crea un modello di deep learning in grado di comprendere e modellare pattern complessi e astratti all’interno dei dati. Impilando queste semplici unità, le reti neurali acquisiscono la capacità di affrontare attività sempre più sofisticate, dal riconoscimento delle immagini all’elaborazione del linguaggio naturale.\n\n\n3.2.2 Perceptron Multilayer\nI “Multilayer perceptron” (MLP) sono un’evoluzione del modello del perceptron a singolo layer, caratterizzato da più layer di nodi collegati in modo “feedforward”. In una rete feedforward, le informazioni si muovono in una sola direzione: dal layer di input, attraverso i layer nascosti, al layer di output, senza cicli o loop. Questa struttura è illustrata in Figura 3.5. I layer di rete includono un layer di input per la ricezione dei dati, diversi layer nascosti per l’elaborazione dei dati e un layer di output per la generazione del risultato finale.\nMentre un singolo perceptron è limitato nella sua capacità di modellare pattern complessi, la vera forza delle reti neurali emerge dall’assemblaggio di più layer. Ciascun layer è costituito da numerosi perceptron che lavorano insieme, consentendo alla rete di catturare relazioni intricate e non lineari all’interno dei dati. Con sufficiente profondità e ampiezza, queste reti possono approssimare praticamente qualsiasi funzione, indipendentemente da quanto sia complessa.\n\n\n\n\n\n\nFigura 3.5: Perceptron Multilayer. Fonte: Wikimedia - Charlie.\n\n\n\n\n\n3.2.3 Processo di Training\nUna rete neurale riceve un input, esegue un calcolo e produce una previsione. La previsione è determinata dai calcoli eseguiti all’interno dei set di perceptron trovati tra i layer di input e output. Questi calcoli dipendono principalmente dall’input e dai pesi. Poiché non si ha il controllo sull’input, l’obiettivo durante il training [addestramento] è quello di regolare i pesi in modo tale che l’output della rete fornisca la previsione più accurata.\nIl processo di addestramento prevede diversi passaggi chiave, a partire dal passaggio in avanti (forward), in cui i pesi esistenti della rete vengono utilizzati per calcolare l’output per un dato input. Questo output viene poi confrontato con i veri valori target per calcolare un errore, che misura quanto bene la previsione della rete corrisponde al risultato previsto. In seguito, viene eseguito un passaggio all’indietro (backward). Ciò comporta l’utilizzo dell’errore per apportare modifiche ai pesi della rete tramite un processo chiamato “backpropagation”. Questa regolazione mira a ridurre l’errore nelle previsioni successive. Il ciclo di passaggio forward [in avanti], calcolo dell’errore e passaggio backward [all’indietro] viene ripetuto iterativamente. Questo processo continua finché le previsioni della rete non sono sufficientemente accurate o non viene raggiunto un numero predefinito di iterazioni, riducendo al minimo la “funzione di perdita” utilizzata per misurare l’errore.\n\nForward Pass\nIl “forward pass” [passo in avanti] è la fase iniziale in cui i dati si spostano attraverso la rete dal layer di input a quello di output. All’inizio dell’addestramento, i pesi della rete vengono inizializzati in modo casuale, impostando le condizioni iniziali. Durante il “forward pass”, ogni layer esegue calcoli specifici sui dati di input utilizzando questi pesi e il bias, e i risultati vengono poi passati al layer successivo. L’output finale di questa fase è la “prediction” [previsione] della rete. Questa “prediction” viene confrontata con i valori target effettivi presenti nel set di dati per calcolare la “loss” [perdita], che può essere considerata come la differenza tra gli output previsti e i valori target. La perdita quantifica le prestazioni della rete in questa fase, fornendo una metrica cruciale per la successiva regolazione dei pesi durante il backward pass.\nVideo 3.1 di seguito spiega come funzionano le reti neurali utilizzando il riconoscimento delle cifre scritte a mano come applicazione di esempio. Affronta anche la matematica alla base delle reti neurali.\n\n\n\n\n\n\nVideo 3.1: Reti Neurali\n\n\n\n\n\n\n\n\nBackward Pass (Backpropagation)\nDopo aver completato il forward pass e calcolato la perdita, che misura quanto le previsioni del modello si discostano dai valori target effettivi, il passo successivo è migliorare le prestazioni del modello regolando i pesi della rete. Poiché non possiamo controllare gli input del modello, la regolazione dei pesi diventa il nostro metodo principale per perfezionare il modello.\nDeterminiamo come regolare i pesi del nostro modello tramite un algoritmo chiave chiamato “backpropagation”. La backpropagation utilizza la perdita calcolata per determinare il gradiente di ciascun peso. Questi gradienti descrivono la direzione e l’entità in cui i pesi devono essere regolati. Regolando i pesi in base a questi gradienti, il modello è meglio posizionato per fare previsioni più vicine ai valori target effettivi nel successivo “forward pass”.\nComprendere questi concetti fondamentali apre la strada alla comprensione di architetture e tecniche di deep learning più complesse, favorendo lo sviluppo di applicazioni più sofisticate e produttive, in particolare all’interno di sistemi di intelligenza artificiale embedded.\nVideo 3.2 and Video 3.3 build upon Video 3.1. Riguardano la “gradient descent” [discesa del gradiente] e la backpropagation nelle reti neurali.\n\n\n\n\n\n\nVideo 3.2: Gradient descent\n\n\n\n\n\n\n\n\n\n\n\n\nVideo 3.3: Backpropagation\n\n\n\n\n\n\n\n\n\n3.2.4 Architetture dei Modelli\nLe architetture di deep learning si riferiscono ai vari approcci strutturati che stabiliscono come i neuroni e i layer sono organizzati e interagiscono nelle reti neurali. Queste architetture si sono evolute per affrontare efficacemente diversi problemi e diversi tipi di dati. Questa sezione fornisce una panoramica di alcune note architetture di deep learning e delle loro caratteristiche.\n\nMultilayer Perceptron (MLP)\nGli MLP sono architetture di deep learning di base che comprendono tre layer: uno di input, uno o più layer nascosti e un layer di output. Questi layer sono completamente connessi, il che significa che ogni neurone in uno layer è collegato a ogni neurone nei layer precedenti e successivi. Gli MLP possono modellare funzioni complesse e sono utilizzati in varie attività, come regressione, classificazione e riconoscimento di pattern. La loro capacità di apprendere relazioni non lineari tramite backpropagation li rende uno strumento versatile nel toolkit di deep learning.\nNei sistemi di intelligenza artificiale embedded, gli MLP possono funzionare come modelli compatti per attività più semplici come l’analisi dei dati dei sensori o il riconoscimento di pattern di base, in cui le risorse computazionali sono limitate. La loro capacità di apprendere relazioni non lineari con una complessità relativamente minore li rende una scelta adatta per i sistemi embedded.\n\n\n\n\n\n\nEsercizio 3.1: Multilayer Perceptron (MLP)\n\n\n\n\n\nAbbiamo appena scalfito la superficie delle reti neurali. Ora, proveremo ad applicare questi concetti in esempi pratici. Nei notebook Colab forniti, si esploreranno:\nPrevisione dei prezzi delle case: Scoprire come le reti neurali possono analizzare i dati sugli alloggi per stimare i valori delle proprietà. \nClassificazione delle immagini: Scoprire come creare una rete per comprendere il famoso set di dati di cifre scritte a mano MNIST. \nDiagnosi medica nel mondo reale: Usare il deep learning per affrontare l’importante compito della classificazione del cancro al seno. \n\n\n\n\n\nConvolutional Neural Networks (CNNs)\nLe CNN [reti neurali convoluzionali] sono utilizzate principalmente in attività di riconoscimento di immagini e video. Questa architettura è composta da due parti principali: la base convoluzionale e i layer completamente connessi. Nella base convoluzionale, i layer convoluzionali filtrano i dati di input per identificare caratteristiche come bordi, angoli e texture [trame]. Dopo ogni layer convoluzionale, è possibile applicare un layer di pooling [raggruppamento] per ridurre le dimensioni spaziali dei dati, diminuendo così il carico computazionale e concentrando le feature estratte. A differenza degli MLP, che trattano le feature di input come entità piatte e indipendenti, le CNN mantengono le relazioni spaziali tra i pixel, rendendole particolarmente efficaci per i dati di immagini e video. Le feature estratte dalla base convoluzionale vengono poi passate ai layer completamente connessi, simili a quelli utilizzati negli MLP, che eseguono la classificazione in base alle feature estratte dai layer di convoluzione. Le CNN si sono dimostrate altamente efficaci nel riconoscimento delle immagini, nel rilevamento di oggetti e in altre applicazioni di visione artificiale.\nNell’intelligenza artificiale embedded, le CNN sono fondamentali per le attività di riconoscimento di immagini e video, in cui è spesso necessaria l’elaborazione in tempo reale. Possono essere ottimizzate per i sistemi embedded utilizzando tecniche come la quantizzazione e il “pruning” [potatura] per ridurre al minimo l’utilizzo della memoria e le richieste computazionali, consentendo funzionalità efficienti di rilevamento di oggetti e riconoscimento facciale in dispositivi con risorse computazionali limitate.\n\n\n\n\n\n\nEsercizio 3.2: Convolutional Neural Networks (CNNs)\n\n\n\n\n\nAbbiamo discusso del fatto che le CNN [Reti neurali convoluzionali] sono eccellenti nell’identificare le caratteristiche delle immagini, il che le rende ideali per attività come la classificazione degli oggetti. Ora, si potrà mettere in pratica questa conoscenza! Questo notebook Colab si concentra sulla creazione di una CNN per classificare le immagini dal set di dati CIFAR-10, che include oggetti come aeroplani, automobili e animali. Si impareranno le principali differenze tra CIFAR-10 e il set di dati MNIST che abbiamo esplorato in precedenza e come queste differenze influenzano la scelta del modello. Alla fine di questo notebook, avremo compreso le CNN per il riconoscimento delle immagini e saremo sulla buona strada per diventare esperti di TinyML!\n\n\n\n\n\n\nRecurrent Neural Networks (RNN)\nLe RNN [Reti Neurali Ricorrenti] sono adatte per l’analisi di dati sequenziali, come la previsione di serie temporali e l’elaborazione del linguaggio naturale. In questa architettura, le connessioni tra i nodi formano un grafo diretto lungo una sequenza temporale, consentendo il trasporto delle informazioni attraverso le sequenze tramite vettori di stato nascosti. Le varianti delle RNN includono le Long Short-Term Memory (LSTM) e le Gated Recurrent Units (GRU), progettate per catturare dipendenze più lunghe nei dati sequenziali.\nQueste reti possono essere utilizzate nei sistemi di riconoscimento vocale, nella manutenzione predittiva o nei dispositivi IoT in cui sono comuni i pattern di dati sequenziali. Le ottimizzazioni specifiche per le piattaforme embedded possono aiutare a gestirne i requisiti di elaborazione e memoria tipicamente elevati.\n\n\nGenerative Adversarial Networks (GAN)\nLe GAN [Reti Generative Avversarie] sono costituite da due reti, un generatore e un discriminatore, addestrate simultaneamente tramite l’addestramento adversarial [avversario] (Goodfellow et al. 2020). Il generatore produce dati che cercano di imitare la distribuzione dei dati reali, mentre il discriminatore mira a distinguere tra dati reali e generati. Le GAN sono ampiamente utilizzate nella generazione di immagini, nel trasferimento di stile e nell’aumento dei dati.\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Commun. ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\nIn contesti embedded, le reti GAN potrebbero essere utilizzate per l’aumento dei dati sul dispositivo per migliorare il training dei modelli direttamente sul dispositivo embedded, consentendo un apprendimento continuo e un adattamento ai nuovi dati senza la necessità di risorse di cloud computing.\n\n\nAutoencoder\nGli autoencoder sono reti neurali per la compressione dei dati e la riduzione del rumore (Bank, Koenigstein, e Giryes 2023). Sono strutturati per codificare i dati di input in una rappresentazione a dimensione inferiore e quindi decodificarli nella loro forma originale. Varianti come gli Variational Autoencoders (VAE) [Autoencoder Variazionali] introducono livelli probabilistici che consentono proprietà generative, trovando applicazioni nella generazione di immagini e nel rilevamento di anomalie.\n\nBank, Dor, Noam Koenigstein, e Raja Giryes. 2023. «Autoencoders». Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, 353–74.\nL’uso degli autoencoder può aiutare nella trasmissione e nell’archiviazione efficiente dei dati, migliorando le prestazioni complessive dei sistemi embedded con risorse di calcolo e di memoria limitate.\n\n\nTransformer Network\nLe “Transformer network” [reti di trasformatori] sono emerse come un’architettura potente, specialmente nell’elaborazione del linguaggio naturale (Vaswani et al. 2017). Queste reti utilizzano meccanismi di auto-attenzione per soppesare l’influenza di diverse parole di input su ogni parola di output, consentendo il calcolo parallelo e catturando pattern intricati nei dati. Le reti di trasformatori hanno portato a risultati all’avanguardia in attività come la traduzione linguistica, la sintesi e la generazione di testo.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, e Illia Polosukhin. 2017. «Attention is all you need». Adv Neural Inf Process Syst 30.\nQueste reti possono essere ottimizzate per eseguire attività correlate alla lingua direttamente sul dispositivo. Ad esempio, i trasformatori possono essere utilizzati nei sistemi embedded per servizi di traduzione in tempo reale o interfacce assistite dalla voce, dove latenza ed efficienza computazionale sono cruciali. Tecniche come la distillazione del modello possono essere impiegate per distribuire queste reti su dispositivi embedded con risorse limitate.\nQueste architetture servono a scopi specifici ed eccellono in diversi domini, offrendo un ricco toolkit per affrontare diversi problemi nei sistemi di intelligenza artificiale embedded. Comprendere le sfumature di queste architetture è fondamentale nella progettazione di modelli di deep learning efficaci ed efficienti per varie applicazioni.\n\n\n\n3.2.5 ML Tradizionale vs Deep Learning\nIl deep learning estende il machine learning tradizionale utilizzando reti neurali per discernere i pattern nei dati. Al contrario, il machine learning tradizionale si basa su un set di algoritmi consolidati come alberi decisionali, k-nearest neighbor e macchine a vettori di supporto, ma non coinvolge le reti neurali. Per evidenziare brevemente le differenze, Tabella 3.1 illustra le caratteristiche contrastanti tra il ML tradizionale e il deep learning:\n\n\n\nTabella 3.1: Confronto tra machine learning tradizionale e deep learning.\n\n\n\n\n\n\n\n\n\n\nAspetto\nML tradizionale\nDeep Learning\n\n\n\n\nRequisiti dei dati\nDa basso a moderato (efficiente con set di dati più piccoli)\nAlto (richiede set di dati di grandi dimensioni per un apprendimento adeguato)\n\n\nComplessità del modello\nModerata (adatta a problemi ben definiti)\nAlta (rileva modelli intricati, adatta a compiti complessi)\n\n\nRisorse di calcolo\nDa basse a moderate (economiche, meno dispendiose in termini di risorse)\nAlta (richiede una potenza di calcolo e risorse sostanziali)\n\n\nVelocità di distribuzione\nVeloce (cicli di training e distribuzione più rapidi)\nLento (tempi di training prolungati, in particolare con set di dati più grandi)\n\n\nInterpretabilità\nAlta (chiare intuizioni sui percorsi decisionali)\nBassa (strutture complesse a layer, natura “scatola nera”)\n\n\nManutenzione\nPiù facile (semplice da aggiornare e mantenere)\nComplesso (richiede più sforzi nella manutenzione e negli aggiornamenti)\n\n\n\n\n\n\n\n\n3.2.6 Scelta tra ML tradizionale e DL\n\nDisponibilità e Volume dei Dati\nQuantità di Dati: Gli algoritmi di machine learning tradizionali, come gli alberi decisionali o Naive Bayes, sono spesso più adatti quando la disponibilità dei dati è limitata. Offrono previsioni affidabili anche con set di dati più piccoli. Ciò è particolarmente vero nella diagnostica medica per la previsione delle malattie e nella segmentazione dei clienti nel marketing.\nDiversità e Qualità dei Dati: Gli algoritmi di machine learning tradizionali spesso funzionano bene con dati strutturati (l’input del modello è un set di funzionalità, idealmente indipendenti l’una dall’altra) ma possono richiedere un notevole sforzo di pre-elaborazione (ad esempio, la “feature engineering” [progettazione delle funzionalità]). D’altro canto, il deep learning adotta l’approccio di eseguire automaticamente la progettazione delle funzionalità come parte dell’architettura del modello. Questo approccio consente la costruzione di modelli end-to-end in grado di mappare direttamente da dati di input non strutturati (come testo, audio e immagini) all’output desiderato senza fare affidamento su euristiche semplicistiche con efficacia limitata. Tuttavia, ciò si traduce in modelli più grandi che richiedono più dati e risorse computazionali. Nei dati rumorosi, la necessità di set di dati più grandi è ulteriormente enfatizzata quando si utilizza il Deep Learning.\n\n\nComplessità del Problema\nGranularità del Problema: I problemi che sono semplici o moderatamente complessi, che possono coinvolgere relazioni lineari o polinomiali tra variabili, spesso trovano una migliore aderenza ai metodi tradizionali di apprendimento automatico.\nRappresentazione Gerarchica delle Feature: I modelli di deep learning sono eccellenti in attività che richiedono una rappresentazione gerarchica delle feature [caratteristiche], come il riconoscimento di immagini e voce. Tuttavia, non tutti i problemi richiedono questa complessità e gli algoritmi tradizionali di apprendimento automatico possono talvolta offrire soluzioni più semplici e ugualmente efficaci.\n\n\nRisorse Hardware e Computazionali\nVincoli di Risorse: La disponibilità di risorse computazionali spesso influenza la scelta tra ML tradizionale e deep learning. Il primo è generalmente meno dispendioso in termini di risorse e quindi preferibile in ambienti con limitazioni hardware o vincoli di budget.\nScalabilità e Velocità: Gli algoritmi tradizionali di apprendimento automatico, come le Support Vector Machines (SVM) [macchine a vettori di supporto ], spesso consentono tempi di training più rapidi e una scalabilità più semplice, il che è particolarmente vantaggioso nei progetti con tempistiche ristrette e volumi di dati in crescita.\n\n\nNormativa di Conformità\nLa conformità normativa è fondamentale in vari settori, e richiede l’aderenza a linee guida e “best practice” come il General Data Protection Regulation (GDPR) [Regolamento generale sulla protezione dei dati] nell’UE. I modelli ML tradizionali, grazie alla loro intrinseca interpretabilità, spesso si allineano meglio a queste normative, soprattutto in settori come la finanza e l’assistenza sanitaria.\n\n\nInterpretabilità\nComprendere il processo decisionale è più facile con le tecniche tradizionali di apprendimento automatico rispetto ai modelli di deep learning, che funzionano come “scatole nere”, rendendo difficile tracciare i percorsi decisionali.\n\n\n\n3.2.7 Fare una Scelta Informata\nConsiderati i vincoli dei sistemi di intelligenza artificiale embedded, comprendere le differenze tra le tecniche di ML tradizionali e il deep learning diventa essenziale. Entrambe le strade offrono vantaggi unici e le loro caratteristiche distintive spesso determinano la scelta dell’una rispetto all’altra in diversi scenari.\nNonostante ciò, il deep learning ha costantemente superato i metodi tradizionali di apprendimento automatico in diverse aree chiave grazie all’abbondanza di dati, ai progressi computazionali e alla comprovata efficacia in attività complesse. Ecco alcuni motivi specifici per cui ci concentriamo sul deep learning:\n\nPrestazioni Superiori in Attività Complesse: I modelli di deep learning, in particolare le reti neurali profonde, eccellono in attività in cui le relazioni tra i punti dati sono incredibilmente intricate. Attività come il riconoscimento di immagini e parlato, la traduzione linguistica e la riproduzione di giochi complessi come Go e Scacchi hanno visto progressi significativi principalmente attraverso algoritmi di deep learning.\nGestione Efficiente dei Dati non Strutturati: A differenza dei metodi tradizionali di apprendimento automatico, il deep learning può elaborare in modo più efficace i dati non strutturati. Ciò è fondamentale nel panorama dei dati odierno, in cui la stragrande maggioranza dei dati, come testo, immagini e video, non è strutturata.\nSfruttamento dei Big Data: Con la disponibilità dei Big Data, i modelli di deep learning possono apprendere e migliorare continuamente. Questi modelli eccellono nell’utilizzare grandi set di dati per migliorare la loro accuratezza predittiva, un limite degli approcci tradizionali di machine-learning.\nProgressi Hardware e Calcolo Parallelo: L’avvento di potenti GPU e la disponibilità di piattaforme di cloud computing hanno consentito il rapido training di modelli di deep learning. Questi progressi hanno affrontato una delle sfide significative del deep learning: la necessità di risorse computazionali sostanziali.\nAdattabilità Dinamica e Apprendimento Continuo: I modelli di deep learning possono adattarsi dinamicamente a nuove informazioni o dati. Possono essere addestrati per generalizzare il loro apprendimento a nuovi dati mai visti, cruciali in campi in rapida evoluzione come la guida autonoma o la traduzione linguistica in tempo reale.\n\nSebbene il deep learning abbia guadagnato una notevole popolarità, è essenziale comprendere che il machine learning tradizionale è ancora rilevante. Man mano che ci addentriamo nei meandri del deep learning, evidenzieremo anche le situazioni in cui i metodi tradizionali di machine learning potrebbero essere più appropriati, grazie alla loro semplicità, efficienza e interpretabilità. Concentrandoci in questo testo sul deep learning, intendiamo fornire ai lettori le conoscenze e gli strumenti per affrontare problemi moderni e complessi in vari ambiti, fornendo al contempo approfondimenti sui vantaggi comparativi e sugli scenari applicativi appropriati per il deep learning e le tecniche tradizionali di machine learning.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html#conclusione",
    "href": "contents/dl_primer/dl_primer.it.html#conclusione",
    "title": "3  Avvio al Deep Learning",
    "section": "3.3 Conclusione",
    "text": "3.3 Conclusione\nIl deep learning è diventato un potente set di tecniche per affrontare le complesse sfide del riconoscimento di pattern e della previsione. Iniziando con una panoramica, abbiamo delineato i concetti e i principi fondamentali che governano il deep learning, gettando le basi per studi più avanzati.\nAl centro del deep learning, abbiamo esplorato le idee di base delle reti neurali, potenti modelli computazionali ispirati alla struttura neuronale interconnessa del cervello umano. Questa esplorazione ci ha permesso di apprezzare le capacità e il potenziale delle reti neurali nella creazione di algoritmi sofisticati in grado di apprendere e adattarsi dai dati.\nComprendere il ruolo delle librerie e dei framework è stata una parte fondamentale della nostra discussione. Abbiamo offerto approfondimenti sugli strumenti che possono facilitare lo sviluppo e l’implementazione di modelli di deep learning. Queste risorse semplificano l’implementazione delle reti neurali e aprono strade all’innovazione e all’ottimizzazione.\nSuccessivamente, abbiamo affrontato le sfide che si potrebbero incontrare quando si racchiudono algoritmi di deep learning nei sistemi embedded, fornendo una prospettiva critica sulle complessità e sulle considerazioni relative all’introduzione dell’intelligenza artificiale nei dispositivi edge.\nInoltre, abbiamo esaminato i limiti del deep learning. Attraverso le discussioni, abbiamo svelato le sfide affrontate nelle applicazioni del deep learning e delineato scenari in cui l’apprendimento automatico tradizionale potrebbe superare il deep learning. Queste sezioni sono fondamentali per promuovere una visione equilibrata delle capacità e dei limiti del deep learning.\nIn questo “Avviamento”, abbiamo fornito le conoscenze per fare scelte informate tra l’implementazione dell’apprendimento automatico tradizionale o delle tecniche di deep learning, a seconda delle esigenze e dei vincoli unici di un problema specifico.\nConcludendo questo capitolo, ci auguriamo che sia stato acquisito il “linguaggio” di base del deep learning e si sia pronti ad approfondire i capitoli successivi con una solida comprensione e una prospettiva critica. Il viaggio che è pieno di entusiasmanti opportunità e sfide nel racchiudere l’intelligenza artificiale nei sistemi.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/dl_primer/dl_primer.it.html#sec-deep-learning-primer-resource",
    "href": "contents/dl_primer/dl_primer.it.html#sec-deep-learning-primer-resource",
    "title": "3  Avvio al Deep Learning",
    "section": "3.4 Risorse",
    "text": "3.4 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nPast, Present, and Future of ML.\nThinking About Loss.\nMinimizing Loss.\nFirst Neural Network.\nUnderstanding Neurons.\nIntro to CLassification.\nTraining, Validation, and Test Data.\nIntro to Convolutions.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 3.1\nVideo 3.2\nVideo 3.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 3.1\nEsercizio 3.2\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\n\nProssimamente.",
    "crumbs": [
      "Nozioni Fondamentali",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Avvio al Deep Learning</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html",
    "href": "contents/workflow/workflow.it.html",
    "title": "4  Workflow dell’IA",
    "section": "",
    "text": "4.1 Panoramica\nLo sviluppo di un modello di apprendimento automatico di successo richiede un flusso di lavoro sistematico. Questo processo end-to-end consente di creare, distribuire e gestire modelli in modo efficace. Come mostrato in Figura 4.1, in genere prevede i seguenti passaggi chiave:\nSeguire questo flusso di lavoro ML strutturato ci guida attraverso le fasi chiave dello sviluppo. Garantisce di creare modelli efficaci e robusti pronti per la distribuzione nel mondo reale, con conseguenti modelli di qualità superiore che risolvono le varie esigenze.\nIl flusso di lavoro ML è iterativo, richiede un monitoraggio continuo e potenziali aggiustamenti. Ulteriori considerazioni includono:",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#panoramica",
    "href": "contents/workflow/workflow.it.html#panoramica",
    "title": "4  Workflow dell’IA",
    "section": "",
    "text": "Figura 4.1: Metodologia di progettazione multi-step per lo sviluppo di un modello di machine learning. Comunemente denominato ciclo di vita del machine learning\n\n\n\n\n\nDefinizione del Problema - Si inizia articolando chiaramente il problema specifico da risolvere. Questo si concentra sui problemi durante la raccolta dati e la creazione del modello.\nRaccolta e Preparazione dei Dati: Raccogliere dati di training pertinenti e di alta qualità che catturino tutti gli aspetti del problema. Pulire e pre-elaborare i dati per prepararli alla modellazione.\nSelezione e Training del Modello: Scegliere un algoritmo di apprendimento automatico adatto al tipo di problema e ai dati. Considerare i pro e i contro dei diversi approcci. Inserire i dati preparati nel modello per addestrarlo. Il tempo di addestramento varia in base alle dimensioni dei dati e alla complessità del modello.\nValutazione del Modello: Testare il modello addestrato su nuovi dati non ancora esaminati per misurarne l’accuratezza predittiva. Identificare eventuali limitazioni.\nDistribuzione del Modello: Integrare il modello convalidato in applicazioni o sistemi per avviarne l’operatività.\nMonitoraggio e Manutenzione: Tenere traccia delle prestazioni del modello in produzione. Ri-addestrare periodicamente su nuovi dati per mantenerli aggiornati.\n\n\n\n\nControllo della Versione: Tenere traccia delle modifiche al codice e ai dati per riprodurre i risultati e ripristinare le versioni precedenti se necessario.\nDocumentazione: Mantenere una documentazione dettagliata per la comprensione e la riproduzione del flusso di lavoro.\nTest: Testare rigorosamente il flusso di lavoro per garantirne la funzionalità.\nSicurezza: Proteggere il flusso di lavoro e i dati quando si distribuiscono modelli in contesti di produzione.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#ia-tradizionale-o-embedded",
    "href": "contents/workflow/workflow.it.html#ia-tradizionale-o-embedded",
    "title": "4  Workflow dell’IA",
    "section": "4.2 IA Tradizionale o Embedded",
    "text": "4.2 IA Tradizionale o Embedded\nIl flusso di lavoro ML è una guida universale applicabile su diverse piattaforme, tra cui soluzioni basate su cloud, edge computing e TinyML. Tuttavia, il flusso di lavoro per l’IA Embedded introduce complessità e sfide uniche, rendendolo un dominio accattivante e aprendo la strada a innovazioni straordinarie.\n\n4.2.1 Ottimizzazione delle Risorse\n\nFlusso di Lavoro ML Tradizionale: Questo workflow dà priorità all’accuratezza e alle prestazioni del modello, spesso sfruttando abbondanti risorse di calcolo in ambienti cloud o data center.\nFlusso di Lavoro IA Embedded: Dati i vincoli di risorse dei sistemi embedded, questo flusso di lavoro richiede un’attenta pianificazione per ottimizzare le dimensioni del modello e le richieste di calcolo. Tecniche come la quantizzazione e il pruning [potatura] del modello sono fondamentali.\n\n\n\n4.2.2 Elaborazione in Real-time\n\nFlusso di Lavoro ML Tradizionale: Meno enfasi sull’elaborazione in tempo reale, spesso basata sull’elaborazione di dati in batch.\nFlusso di Lavoro IA Embedded: Dà priorità all’elaborazione dei dati in tempo reale, rendendo essenziali bassa latenza ed esecuzione rapida, soprattutto in applicazioni come veicoli autonomi e automazione industriale.\n\n\n\n4.2.3 Gestione dei Dati e Privacy\n\nFlusso di Lavoro ML Tradizionale: Elabora i dati in posizioni centralizzate, spesso richiedendo un ampio trasferimento di dati e concentrandosi sulla sicurezza dei dati durante il transito e l’archiviazione.\nFlusso di Lavoro IA Embedded: Questo workflow sfrutta l’edge computing per elaborare i dati più vicino alla fonte, riducendo la trasmissione dei dati e migliorando la privacy tramite la localizzazione dei dati.\n\n\n\n4.2.4 Integrazione Hardware-Software\n\nFlusso di Lavoro ML Tradizionale: In genere funziona su hardware generico, con sviluppo di software indipendente.\nFlusso di Lavoro IA Embedded: Questo flusso di lavoro prevede un approccio più integrato allo sviluppo hardware e software, spesso incorporando chip personalizzati o acceleratori hardware per ottenere prestazioni ottimali.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#ruoli-e-responsabilità",
    "href": "contents/workflow/workflow.it.html#ruoli-e-responsabilità",
    "title": "4  Workflow dell’IA",
    "section": "4.3 Ruoli e responsabilità",
    "text": "4.3 Ruoli e responsabilità\nLa creazione di una soluzione ML, in particolare per l’intelligenza artificiale embedded, è uno sforzo multidisciplinare che coinvolge vari specialisti. A differenza dello sviluppo software tradizionale, la creazione di una soluzione ML richiede un approccio multidisciplinare a causa della natura sperimentale dello sviluppo del modello e dei requisiti ad alta intensità di risorse per il training e l’implementazione di questi modelli.\nC’è una forte necessità di ruoli incentrati sui dati per il successo delle pipeline di apprendimento automatico. Gli scienziati dei dati e gli ingegneri dei dati gestiscono la raccolta dei dati, creano pipeline di dati e ne garantiscono la qualità. Poiché la natura dei modelli di apprendimento automatico dipende dai dati che consumano, i modelli sono unici e variano a seconda delle diverse applicazioni, il che richiede un’ampia sperimentazione. I ricercatori e gli ingegneri di apprendimento automatico guidano questa fase sperimentale attraverso test continui, convalida e iterazione per ottenere prestazioni ottimali.\nLa fase di implementazione richiede spesso hardware e infrastrutture specializzati, poiché i modelli di machine learning possono essere ad alta intensità di risorse, richiedendo un’elevata potenza di calcolo e una gestione efficiente delle risorse. Ciò richiede la collaborazione con gli ingegneri hardware per garantire che l’infrastruttura possa supportare le esigenze computazionali di training e inferenza del modello.\nPoiché i modelli prendono decisioni che possono avere un impatto sugli individui e sulla società, gli aspetti etici e legali dell’apprendimento automatico stanno diventando sempre più importanti. Sono necessari esperti di etica e consulenti legali per garantire la conformità agli standard etici e alle normative legali.\nTabella 4.1 mostra una panoramica dei ruoli tipici coinvolti. Sebbene i confini tra questi ruoli possano a volte confondersi, la tabella seguente fornisce una panoramica generale.\n\n\n\nTabella 4.1: Ruoli e responsabilità delle persone coinvolte in Operazioni di ML.\n\n\n\n\n\n\n\n\n\nRuolo\nResponsabilità\n\n\n\n\nProject Manager\nSupervisiona il progetto, assicurando che le tempistiche e le milestone siano rispettate.\n\n\nEsperti di Dominio\nOffrono approfondimenti specifici del dominio per definire i requisiti del progetto.\n\n\nData Scientist\nSpecializzati nell’analisi dei dati e nello sviluppo di modelli.\n\n\nIngegneri di Apprendimento Automatico\nConcentrati sullo sviluppo e l’implementazione del modello.\n\n\nData Scientist\nSpecializzati nell’analisi dei dati e nello sviluppo di modelli.\n\n\nEmbedded Systems Engineer\nIntegra modelli ML in sistemi embedded.\n\n\nSoftware Developer\nSviluppa componenti software per l’integrazione del sistema IA.\n\n\nHardware Engineer\nProgetta e ottimizza l’hardware per il sistema AI embedded.\n\n\nUI/UX Designer\nConcentrato sulla progettazione incentrata sull’utente.\n\n\nQA Engineer\nAssicura che il sistema soddisfi gli standard di qualità.\n\n\nEticisti e Consulenti Legali\nConsulenti sulla conformità etica e legale.\n\n\nPersonale Operativo e di Manutenzione\nMonitora e mantiene il sistema distribuito.\n\n\nSpecialisti della sicurezza\nGarantiscono la sicurezza del sistema.\n\n\n\n\n\n\nComprendere questi ruoli è fondamentale per completare un progetto ML. Nei prossimi capitoli esploreremo l’essenza e le competenze di ciascun ruolo, favorendo una comprensione completa delle complessità implicite nei progetti di intelligenza artificiale embedded. Questa visione olistica facilita una collaborazione senza soluzione di continuità e alimenta un ambiente maturo per innovazione e scoperte.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#conclusione",
    "href": "contents/workflow/workflow.it.html#conclusione",
    "title": "4  Workflow dell’IA",
    "section": "4.4 Conclusione",
    "text": "4.4 Conclusione\nQuesto capitolo ha gettato le basi per comprendere il flusso di lavoro dell’apprendimento automatico, un approccio strutturato fondamentale per lo sviluppo, l’implementazione e la manutenzione dei modelli ML. Esplorando le diverse fasi del ciclo di vita ML, abbiamo acquisito informazioni sulle sfide uniche affrontate dai flussi di lavoro ML tradizionali e IA embedded, in particolare in termini di ottimizzazione delle risorse, elaborazione in tempo reale, gestione dei dati e integrazione hardware-software. Queste distinzioni sottolineano l’importanza di adattare i flussi di lavoro per soddisfare le esigenze specifiche dell’ambiente applicativo.\nIl capitolo ha sottolineato l’importanza della collaborazione multidisciplinare nei progetti ML. La comprensione dei diversi ruoli fornisce una visione completa del lavoro di squadra necessario per navigare nella natura sperimentale e ad alta intensità di risorse dello sviluppo ML. Mentre andiamo avanti verso discussioni più dettagliate nei capitoli successivi, questa panoramica di alto livello ci fornisce una prospettiva globale sul flusso di lavoro ML e sui vari ruoli coinvolti.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/workflow/workflow.it.html#sec-ai-workflow-resource",
    "href": "contents/workflow/workflow.it.html#sec-ai-workflow-resource",
    "title": "4  Workflow dell’IA",
    "section": "4.5 Risorse",
    "text": "4.5 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nML Workflow.\nML Lifecycle.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html",
    "href": "contents/data_engineering/data_engineering.it.html",
    "title": "5  Data Engineering",
    "section": "",
    "text": "5.1 Introduzione\nSi immagini un mondo in cui l’intelligenza artificiale può diagnosticare malattie con una precisione senza precedenti, ma solo se i dati utilizzati per addestrarla sono imparziali e affidabili. È qui che entra in gioco il “data engineering” [ingegneria dei dati]. Sebbene oltre il 90% dei dati mondiali sia stato creato negli ultimi due decenni, questa enorme quantità di informazioni è utile solo per creare modelli di intelligenza artificiale efficaci con un’elaborazione e una preparazione adeguate. L’ingegneria dei dati colma questa lacuna trasformando i dati grezzi in un formato di alta qualità che alimenta l’innovazione dell’intelligenza artificiale. Nel mondo odierno basato sui dati, proteggere la privacy degli utenti è fondamentale. Che siano obbligatorie per legge o guidate dalle preoccupazioni degli utenti, le tecniche di anonimizzazione come la privacy differenziale e l’aggregazione sono fondamentali per mitigare i rischi per la privacy. Tuttavia, un’implementazione attenta è fondamentale per garantire che questi metodi non compromettano l’utilità dei dati. I creatori di set di dati affrontano complesse sfide di privacy e rappresentazione quando creano dati di addestramento di alta qualità, in particolare per domini sensibili come l’assistenza sanitaria. Dal punto di vista legale, i creatori potrebbero dover rimuovere identificatori diretti come nomi ed età. Anche senza obblighi legali, la rimozione di tali informazioni può aiutare a creare fiducia negli utenti. Tuttavia, un’eccessiva anonimizzazione può compromettere l’utilità del set di dati. Tecniche come la privacy differenziale\\(^{1}\\), l’aggregazione e la riduzione dei dettagli forniscono alternative per bilanciare privacy e utilità, ma hanno degli svantaggi. I creatori devono trovare un equilibrio ponderato in base al caso d’uso.\nSebbene la privacy sia fondamentale, garantire modelli di intelligenza artificiale equi e solidi richiede di affrontare le lacune (gap) della rappresentazione nei dati. È fondamentale ma non sufficiente garantire la diversità tra variabili individuali come genere, razza e accento. Queste combinazioni, a volte chiamate lacune (gap) di ordine superiore, possono influire in modo significativo sulle prestazioni del modello. Ad esempio, un set di dati medico potrebbe avere dati bilanciati su genere, età e diagnosi individualmente, ma non ha abbastanza casi per catturare donne anziane con una condizione specifica. Tali higher-order gaps [lacune di ordine superiore] non sono immediatamente evidenti, ma possono influire in modo critico sulle prestazioni del modello.\nLa creazione di dati di training utili ed etici richiede una considerazione globale dei rischi per la privacy e delle lacune di rappresentazione. Le soluzioni perfette elusive necessitano di pratiche di ingegneria dei dati coscienziose come l’anonimizzazione, l’aggregazione, il sotto-campionamento di gruppi sovrarappresentati e la generazione di dati sintetizzati per bilanciare esigenze contrastanti. Ciò facilita modelli che sono sia accurati che socialmente responsabili. La collaborazione interfunzionale e i controlli esterni possono anche rafforzare i dati di training. Le sfide sono molteplici ma superabili con uno sforzo ponderato.\nIniziamo discutendo della raccolta dati: Dove reperiamo i dati e come li raccogliamo? Le opzioni spaziano dall’estrazione di dati dal web, all’accesso alle API e all’utilizzo di sensori e dispositivi IoT, fino alla conduzione di sondaggi e alla raccolta di input dagli utenti. Questi metodi riflettono pratiche del mondo reale. Successivamente, approfondiremo l’etichettatura dei dati, tenendo conto anche del coinvolgimento umano. Discuteremo i compromessi e le limitazioni dell’etichettatura umana ed esploreremo i metodi emergenti per l’etichettatura automatizzata. Successivamente, affronteremo la pulizia e la preelaborazione dei dati, un passaggio cruciale ma spesso sottovalutato nella preparazione dei dati grezzi per l’addestramento del modello di intelligenza artificiale. Segue l’aumento dei dati, una strategia per migliorare set di dati limitati generando campioni sintetici. Ciò è particolarmente pertinente per i sistemi embedded, poiché molti casi d’uso necessitano di ampi repository di dati prontamente disponibili per la cura [https://it.wikipedia.org/wiki/Data_curation]. La generazione di dati sintetici emerge come un’alternativa praticabile con vantaggi e svantaggi. Parleremo anche del versioning del dataset, sottolineando l’importanza di tracciare le modifiche dei dati nel tempo. I dati sono in continua evoluzione; quindi, è fondamentale ideare strategie per gestire e archiviare dataset espansivi. Alla fine di questa sezione, si avrà una comprensione completa dell’intera pipeline di dati, dalla raccolta all’archiviazione, essenziale per rendere operativi i sistemi di intelligenza artificiale. Intraprendiamo questo viaggio!",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#definizione-del-problema",
    "href": "contents/data_engineering/data_engineering.it.html#definizione-del-problema",
    "title": "5  Data Engineering",
    "section": "5.2 Definizione del Problema",
    "text": "5.2 Definizione del Problema\nIn molti domini di machine learning, algoritmi sofisticati sono al centro dell’attenzione, mentre l’importanza fondamentale della qualità dei dati viene spesso trascurata. Questa negligenza dà origine alle “Data Cascades” di Sambasivan et al. (2021) (cfr. Figura 5.1)—eventi in cui le carenze nella qualità dei dati si sommano, portando a conseguenze negative a valle come previsioni errate, cessazioni di progetti e persino potenziali danni alle comunità. In Figura 5.1, abbiamo un’illustrazione delle potenziali insidie dei dati in ogni fase e di come influenzano l’intero processo lungo la linea. L’influenza degli errori nella raccolta dei dati è particolarmente pronunciata. Eventuali lacune in questa fase diventeranno evidenti in fasi successive (nella valutazione e nell’implementazione del modello) e potrebbero comportare conseguenze costose, come l’abbandono dell’intero modello e il riavvio da zero. Pertanto, investire in tecniche di data engineering sin dall’inizio ci aiuterà a rilevare gli errori in anticipo.\n\n\n\n\n\n\nFigura 5.1: Data cascades: costi composti. Fonte: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. «“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI». In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–15.\n\n\nNonostante molti professionisti del ML riconoscano l’importanza dei dati, altri segnalano di dover affrontare queste “cascate”. Ciò evidenzia un problema sistemico: mentre il fascino dello sviluppo di modelli avanzati rimane, i dati spesso devono essere maggiormente apprezzati.\nPrendiamo, ad esempio, le “Keyword Spotting” (KWS) (cfr. Figura 5.2). KWS è un ottimo esempio di TinyML in azione ed è una tecnologia fondamentale alla base delle interfacce di abilitazione alla voce su dispositivi endpoint come gli smartphone. Questi sistemi, che in genere funzionano come motori leggeri di “wake-word” [parole di attivazione], sono costantemente attivi, in ascolto di una frase specifica per attivare ulteriori azioni. Quando diciamo “OK, Google” o “Alexa”, questo avvia un processo su un microcontrollore embedded nel dispositivo. Nonostante le loro risorse limitate, questi microcontrollori svolgono un ruolo importante nel consentire interazioni vocali senza interruzioni con i dispositivi, spesso operando in ambienti con elevato rumore ambientale. L’unicità della wake-word aiuta a ridurre al minimo i falsi positivi, assicurando che il sistema non venga attivato inavvertitamente.\nÈ importante comprendere che queste tecnologie di individuazione delle “parole chiave” non sono isolate; si integrano perfettamente in sistemi più grandi, elaborando segnali in modo continuo e gestendo al contempo un basso consumo energetico. Questi sistemi vanno oltre il semplice riconoscimento delle parole chiave, evolvendosi per facilitare diversi rilevamenti di suoni, come la rottura di un vetro. Questa evoluzione è orientata alla creazione di dispositivi intelligenti in grado di comprendere e rispondere ai comandi vocali, annunciando un futuro in cui anche gli elettrodomestici possono essere controllati tramite interazioni vocali.\n\n\n\n\n\n\nFigura 5.2: Esempio di individuazione delle “Keyword Spotting”: interazione con Alexa. Fonte: Amazon.\n\n\n\nCreare un modello KWS affidabile è un compito complesso. Richiede una profonda comprensione dello scenario di distribuzione, che comprenda dove e come funzioneranno questi dispositivi. Ad esempio, l’efficacia di un modello KWS non riguarda solo il riconoscimento di una parola; riguarda la sua distinzione tra vari accenti e rumori di sottofondo, che si tratti di un bar affollato o del suono stridulo di una televisione in un soggiorno o in una cucina dove questi dispositivi sono comunemente presenti. Riguarda la garanzia che un sussurrato “Alexa” nel cuore della notte o un urlato “OK Google” in un mercato rumoroso vengano riconosciuti con la stessa precisione.\nInoltre, molti degli attuali assistenti vocali KWS supportano un numero limitato di lingue, lasciando una parte sostanziale della diversità linguistica mondiale non rappresentata. Questa limitazione è in parte dovuta alla difficoltà di raccogliere e monetizzare i dati per le lingue parlate da popolazioni più piccole. La distribuzione “long-tail” [https://it.wikipedia.org/wiki/Coda_lunga] delle lingue implica che molte lingue hanno dati limitati, rendendo difficile lo sviluppo di tecnologie di supporto.\nQuesto livello di accuratezza e robustezza dipende dalla disponibilità e dalla qualità dei dati, dalla capacità di etichettare correttamente i dati e dalla trasparenza dei dati per l’utente finale prima che vengano utilizzati per addestrare il modello. Tuttavia, tutto inizia con una chiara comprensione della dichiarazione o definizione del problema.\nIn genere, in ML, la definizione del problema ha alcuni passaggi chiave:\n\nIdentificare chiaramente la definizione del problema\nDefinire obiettivi chiari\nStabilire un benchmark [riferimento] di successo\nComprendere l’impegno/l’uso dell’utente finale\nComprendere i vincoli e le limitazioni dell’implementazione\nSeguito infine dalla raccolta dati.\n\nUna solida base di progetto è essenziale per la sua traiettoria e il suo successo finale. Al centro di questa base c’è innanzitutto l’identificazione di un problema chiaro, come garantire che i comandi vocali nei sistemi di assistenza vocale siano riconosciuti in modo coerente in diversi ambienti. Obiettivi chiari, come la creazione di set di dati rappresentativi per scenari diversi, forniscono una direzione unificata. I benchmark, come l’accuratezza del sistema nel rilevamento delle parole chiave, offrono risultati misurabili per valutare i progressi. Il coinvolgimento delle parti interessate, dagli utenti finali agli investitori, fornisce informazioni preziose e garantisce l’allineamento con le esigenze del mercato. Inoltre, quando si esplorano ambiti come l’assistenza vocale, è importante comprendere i limiti della piattaforma. I sistemi embedded, come i microcontrollori, sono dotati di limitazioni intrinseche di potenza di elaborazione, memoria ed efficienza energetica. Riconoscere queste limitazioni garantisce che le funzionalità, come il rilevamento delle parole chiave, siano personalizzate per funzionare in modo ottimale, bilanciando le prestazioni col risparmio delle risorse.\nIn questo contesto, usando KWS come esempio, possiamo suddividere ciascuno dei passaggi come segue:\n\nIdentificazione del Problema: In sostanza, KWS mira a rilevare parole chiave specifiche tra suoni ambientali e altre parole pronunciate. Il problema principale è progettare un sistema in grado di riconoscere queste parole chiave con elevata accuratezza, bassa latenza e minimi falsi positivi o negativi, soprattutto se distribuito su dispositivi con risorse di elaborazione limitate.\nImpostazione di Obiettivi Chiari: Gli obiettivi per un sistema KWS potrebbero includere:\n\nRaggiungimento di un tasso di accuratezza specifico (ad esempio, accuratezza del 98% nel rilevamento delle parole chiave).\nGaranzia di bassa latenza (ad esempio, rilevamento delle parole chiave e risposta entro 200 millisecondi).\nRiduzione al minimo del consumo di energia per estendere la durata della batteria sui dispositivi embedded.\nGaranzia che le dimensioni del modello siano ottimizzate per la memoria disponibile sul dispositivo.\n\nBenchmark per il successo: Stabilire metriche chiare per misurare il successo del sistema KWS. Questo potrebbe includere:\n\nTasso di Veri Positivi: La percentuale di parole chiave identificate correttamente.\nTasso di Falsi Positivi: La percentuale di parole chiave non identificate erroneamente come parole chiave.\nTempo di Risposta: Il tempo impiegato dall’enunciazione della parola chiave alla risposta del sistema.\nConsumo Energetico: Potenza media utilizzata durante il rilevamento della parola chiave.\n\nCoinvolgimento e Comprensione delle Parti Interessate:: Coinvolgere le parti interessate, tra cui produttori di dispositivi, sviluppatori di hardware e software e utenti finali. Comprendere le loro esigenze, capacità e vincoli. Ad esempio:\n\nI produttori di dispositivi potrebbero dare priorità al basso consumo energetico.\nGli sviluppatori di software potrebbero enfatizzare la facilità di integrazione.\nGli utenti finali darebbero priorità all’accuratezza e alla reattività.\n\nComprensione dei Vincoli e delle Limitazioni dei Sistemi Embedded: I dispositivi embedded presentano una serie di problematiche:\n\nLimiti della Memoria: I modelli KWS devono essere leggeri per adattarsi ai vincoli di memoria dei dispositivi embedded. In genere, i modelli KWS devono essere piccoli quanto 16 KB per adattarsi alla “isola always-on” [porzione sempre attiva] del SoC. Inoltre, questa è solo la dimensione del modello. Anche il codice applicativo aggiuntivo per la pre-elaborazione potrebbe dover rientrare nei vincoli di memoria.\nPotenza di Elaborazione: Le capacità di calcolo dei dispositivi embedded sono limitate (alcune centinaia di MHz di velocità di clock), quindi il modello KWS deve essere ottimizzato per l’efficienza.\nConsumo Energetico: Poiché molti dispositivi embedded sono alimentati a batteria, il sistema KWS deve essere efficiente dal punto di vista energetico.\nVincoli Ambientali: I dispositivi potrebbero essere distribuiti in vari ambienti, dalle silenziose camere da letto agli ambienti industriali rumorosi. Il sistema KWS deve essere sufficientemente robusto per funzionare efficacemente in questi scenari.\n\nRaccolta e Analisi dei Dati: Per un sistema KWS, la qualità e la diversità dei dati sono fondamentali. Le considerazioni potrebbero includere:\n\nVarietà di Accenti: Raccogliere dati da parlanti con accenti diversi per garantire un riconoscimento ad ampio raggio.\nRumori di Sottofondo: Includere campioni di dati con diversi rumori ambientali per addestrare il modello per scenari del mondo reale.\nVariazioni delle Parole Chiave: Le persone potrebbero pronunciare le parole chiave in modo diverso o avere leggere variazioni nella parola di attivazione stessa. Assicurarsi che il set di dati catturi queste sfumature.\n\nFeedback e Perfezionamento Iterativo: Una volta sviluppato un prototipo di sistema KWS, è fondamentale testarlo in scenari del mondo reale, raccogliere feedback e perfezionare iterativamente il modello. Ciò garantisce che il sistema rimanga allineato con il problema e gli obiettivi definiti. Ciò è importante perché gli scenari di distribuzione cambiano nel tempo man mano che le cose si evolvono.\n\n\n\n\n\n\n\nEsercizio 5.1: Keyword Spotting con TensorFlow Lite Micro\n\n\n\n\n\nEsplorare una guida pratica per la creazione e l’implementazione di sistemi Keyword Spotting (KWS) utilizzando TensorFlow Lite Micro. Seguire i passaggi dalla raccolta dati all’addestramento del modello e all’implementazione nei microcontrollori. Imparare a creare modelli KWS efficienti che riconoscono parole chiave specifiche in mezzo al rumore di fondo. Perfetto per chi è interessato all’apprendimento automatico sui sistemi embedded. Sbloccare il potenziale dei dispositivi “voice-enabled” con TensorFlow Lite Micro!\n\n\n\n\nIl capitolo corrente sottolinea il ruolo essenziale della qualità dei dati nell’apprendimento automatico, utilizzando come esempio i sistemi Keyword Spotting (KWS). Descrive i passaggi chiave, dalla definizione del problema al coinvolgimento delle parti interessate, sottolineando il feedback iterativo. Il prossimo capitolo approfondirà la gestione della qualità dei dati, discutendone le conseguenze e le tendenze future, concentrandosi sull’importanza di dati diversificati e di alta qualità nello sviluppo di sistemi di intelligenza artificiale, affrontando considerazioni etiche e metodi di reperimento dei dati.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#ricerca-dei-dati.",
    "href": "contents/data_engineering/data_engineering.it.html#ricerca-dei-dati.",
    "title": "5  Data Engineering",
    "section": "5.3 Ricerca dei Dati.",
    "text": "5.3 Ricerca dei Dati.\nLa qualità e la diversità dei dati raccolti sono importanti per sviluppare sistemi di intelligenza artificiale accurati e robusti. Il reperimento di dati di training di alta qualità richiede un’attenta considerazione degli obiettivi, delle risorse e delle implicazioni etiche. I dati possono essere ottenuti da varie fonti a seconda delle esigenze del progetto:\n\n5.3.1 Dataset preesistenti\nPiattaforme come Kaggle e UCI Machine Learning Repository forniscono un comodo punto di partenza. I dataset preesistenti sono preziosi per ricercatori, sviluppatori e aziende. Uno dei loro principali vantaggi è l’efficienza dei costi. Creare un set di dati da zero può richiedere molto tempo ed essere costoso, quindi accedere a dati già pronti può far risparmiare risorse significative. Inoltre, molti set di dati, come ImageNet, sono diventati parametri di riferimento standard nella comunità di apprendimento automatico, consentendo confronti di prestazioni coerenti tra diversi modelli e algoritmi. Questa disponibilità di dati significa che gli esperimenti possono essere avviati immediatamente senza ritardi nella raccolta e nella preelaborazione dei dati. In un campo in rapida evoluzione come il ML, questa praticità è importante.\nLa garanzia di qualità che deriva dai dataset preesistenti più diffusi è importante da considerare perché diversi set di dati contengono errori. Ad esempio, nel set di dati ImageNet è stato riscontrato oltre il 6,4% di errori. Dato il loro uso diffuso, la comunità spesso identifica e corregge eventuali errori o distorsioni in questi set di dati. Questa garanzia è particolarmente utile per studenti e nuovi arrivati nel campo, in quanto possono concentrarsi sull’apprendimento e sulla sperimentazione senza preoccuparsi dell’integrità dei dati. La documentazione di supporto che spesso accompagna i set di dati esistenti è inestimabile, sebbene ciò si applichi generalmente solo a quelli ampiamente utilizzati. Una buona documentazione fornisce approfondimenti sul processo di raccolta dati e sulle definizioni delle variabili e talvolta offre persino prestazioni del modello di base. Queste informazioni non solo aiutano la comprensione, ma promuovono anche la riproducibilità nella ricerca, un pilastro dell’integrità scientifica; attualmente, c’è una crisi attorno al miglioramento della riproducibilità nei sistemi di apprendimento automatico. Quando altri ricercatori hanno accesso agli stessi dati, possono convalidare i risultati, testare nuove ipotesi o applicare metodologie diverse, consentendoci così di basarci più rapidamente sul lavoro reciproco.\nSebbene piattaforme come Kaggle e UCI Machine Learning Repository siano risorse inestimabili, è essenziale comprendere il contesto in cui sono stati raccolti i dati. I ricercatori dovrebbero fare attenzione al potenziale “overfitting” quando utilizzano set di dati popolari, poiché potrebbero essere stati addestrati più modelli su di essi, portando a metriche di prestazioni gonfiate. A volte, questi set di dati non riflettono i dati del mondo reale.\nInoltre, in questi set di dati possono esserci problemi di distorsione, validità e riproducibilità e negli ultimi anni si è sviluppata una crescente consapevolezza di questi problemi. Inoltre, l’utilizzo dello stesso set di dati per addestrare modelli diversi, come mostrato in Figura 5.3, può talvolta creare un disallineamento: addestrare più modelli utilizzando lo stesso set di dati comporta un “disallineamento” tra i modelli e il mondo, in cui un intero ecosistema di modelli riflette solo un sottoinsieme ristretto di dati del mondo reale.\n\n\n\n\n\n\nFigura 5.3: Addestrare modelli diversi con lo stesso set di dati. Fonte: (icons from left to right: Becris; Freepik; Freepik; Paul J; SBTS2018).\n\n\n\n\n\n5.3.2 Web Scraping\nIl “web scraping” si riferisce a tecniche automatizzate per l’estrazione di dati dai siti Web. In genere comporta l’invio di richieste HTTP ai server Web, il recupero di contenuti HTML e l’analisi di tali contenuti per estrarre informazioni rilevanti. Gli strumenti e i framework più diffusi per il web scraping includono Beautiful Soup, Scrapy e Selenium. Questi strumenti offrono diverse funzionalità, dall’analisi dei contenuti HTML all’automazione delle interazioni con i browser Web, in particolare per i siti Web che caricano i contenuti in modo dinamico tramite JavaScript.\nIl web scraping può raccogliere efficacemente grandi set di dati per l’addestramento di modelli di apprendimento automatico, in particolare quando i dati etichettati da esseri umani sono scarsi. Per la ricerca sulla visione artificiale, il web scraping consente la raccolta di enormi volumi di immagini e video. I ricercatori hanno utilizzato questa tecnica per creare set di dati influenti come ImageNet e OpenImages. Ad esempio, si potrebbero effettuare scraping di siti di e-commerce per accumulare foto di prodotti per il riconoscimento di oggetti o piattaforme di social media per raccogliere caricamenti di utenti per l’analisi facciale. Anche prima di ImageNet, il progetto LabelMe di Stanford ha raschiato (scraped) Flickr per oltre 63.000 immagini annotate che coprono centinaia di categorie di oggetti.\nOltre alla visione artificiale, lo scraping web supporta la raccolta di dati testuali per il linguaggio naturale. I ricercatori possono “raschiare” siti di notizie per dati di analisi del “sentiment”, forum e siti di recensioni per la ricerca sui sistemi di dialogo o social media per la modellazione di argomenti. Ad esempio, i dati di training per il chatbot ChatGPT sono stati ottenuti tramite scraping di gran parte dell’Internet pubblico. I repository GitHub sono stati sottoposti a scraping per addestrare l’assistente di codifica Copilot AI di GitHub.\nIl web scraping può anche raccogliere dati strutturati, come prezzi delle azioni, dati meteorologici o informazioni sui prodotti, per applicazioni analitiche. Una volta che i dati sono stati “raschiati”, è essenziale archiviarli in modo strutturato, spesso utilizzando database o data warehouse. Una corretta gestione dei dati garantisce l’usabilità dei dati raccolti per analisi e applicazioni future.\nTuttavia, mentre il web scraping offre numerosi vantaggi, ci sono limitazioni significative e considerazioni etiche da sostenere. Non tutti i siti Web consentono lo scraping e la violazione di queste restrizioni può portare a ripercussioni legali. Anche lo scraping di materiale protetto da copyright o comunicazioni private è immorale e potenzialmente illegale. Il web scraping etico impone l’aderenza al file ‘robots.txt’ di un sito web, che delinea le sezioni del sito a cui è possibile accedere e che possono essere scansionate dai bot automatizzati.\nPer scoraggiare lo scraping automatizzato, molti siti web implementano limiti di velocità. Se un bot invia troppe richieste in un breve periodo, potrebbe essere temporaneamente bloccato, limitando la velocità di accesso ai dati. Inoltre, la natura dinamica dei contenuti web implica che i dati estratti a intervalli diversi potrebbero richiedere maggiore coerenza, ponendo sfide per gli studi a lungo termine. Tuttavia, ci sono tendenze emergenti come la Web Navigation in cui gli algoritmi di apprendimento automatico possono navigare automaticamente nel sito web per accedere ai contenuti dinamici.\nIl volume di dati pertinenti disponibili per lo scraping potrebbe essere limitato per argomenti di nicchia. Ad esempio, mentre lo scraping per argomenti comuni come immagini di gatti e cani potrebbe produrre dati abbondanti, la ricerca di condizioni mediche rare potrebbe essere meno fruttuosa. Inoltre, i dati ottenuti tramite scraping sono spesso non strutturati e rumorosi, il che richiede un’accurata pre-elaborazione e pulizia. È fondamentale comprendere che non tutti i dati raccolti saranno di alta qualità o accuratezza. L’impiego di metodi di verifica, come il riferimento incrociato con fonti alternative di dati, può migliorare l’affidabilità dei dati.\nQuando si esegue lo scraping di dati personali, sorgono problemi di privacy, sottolineando la necessità di anonimizzazione. Pertanto, è fondamentale aderire ai “Termini del Servizio” di un sito Web, limitare la raccolta di dati a quelli di dominio pubblico e garantire l’anonimato di tutti i dati personali acquisiti.\nMentre il web scraping può essere un metodo scalabile per accumulare grandi set di dati di training per sistemi di intelligenza artificiale, la sua applicabilità è limitata a tipi di dati specifici. Ad esempio, il web scraping rende più complessa la ricerca di dati per unità di misura inerziali (IMU) per il riconoscimento dei gesti. Al massimo, si può effettuare lo scraping di un set di dati esistente.\nLa raccolta dal Web può produrre dati incoerenti o imprecisi. Ad esempio, la foto in Figura 5.4 viene visualizzata quando si cerca “semaforo” su Google Images. È un’immagine del 1914 che mostra semafori obsoleti, che sono anche appena distinguibili a causa della scarsa qualità dell’immagine. Questo può essere problematico per i set di dati estratti dal Web, poiché lo inquina con campioni di dati non applicabili (vecchi).\n\n\n\n\n\n\nFigura 5.4: Un’immagine di vecchi semafori (1914). Fonte: Vox.\n\n\n\n\n\n\n\n\n\nEsercizio 5.2: Web Scraping\n\n\n\n\n\nScoprire la potenza del web scraping con Python usando librerie come Beautiful Soup e Pandas. Questo esercizio estrarrà la documentazione Python per i nomi e le descrizioni delle funzioni ed esplorerà le statistiche dei giocatori NBA. Alla fine, si avranno le competenze per estrarre e analizzare dati da siti Web reali. Pronti all’immersione? Accedere al notebook Google Colab qui sotto e iniziare a fare pratica!\n\n\n\n\n\n\n5.3.3 Crowdsourcing\nIl crowdsourcing per i dataset è la pratica di ottenere dati utilizzando i servizi di molte persone, sia da una comunità specifica che dal pubblico in generale, in genere tramite Internet. Invece di affidarsi a un piccolo team o a un’organizzazione specifica per raccogliere o etichettare i dati, il crowdsourcing sfrutta lo sforzo collettivo di un vasto gruppo distribuito di partecipanti. Servizi come Amazon Mechanical Turk consentono la distribuzione di attività di annotazione a una forza lavoro ampia e diversificata. Questo facilita la raccolta di etichette per attività complesse come l’analisi del “sentiment” o il riconoscimento delle immagini che richiedono il giudizio umano.\nIl crowdsourcing è emerso come un approccio efficace per la raccolta di dati e la risoluzione dei problemi. Uno dei principali vantaggi del crowdsourcing è la scalabilità: distribuendo le attività a un ampio pool globale di collaboratori su piattaforme digitali, i progetti possono elaborare rapidamente enormi volumi di dati. Ciò rende il crowdsourcing ideale per l’etichettatura, la raccolta e l’analisi di dati su larga scala.\nInoltre, il crowdsourcing attinge a un gruppo eterogeneo di partecipanti, apportando un’ampia gamma di prospettive, intuizioni culturali e capacità linguistiche che possono arricchire i dati e migliorare la risoluzione creativa dei problemi in modi che un gruppo più omogeneo potrebbe non fare. Poiché il crowdsourcing attinge da un vasto pubblico oltre i canali tradizionali, è più conveniente rispetto ai metodi convenzionali, soprattutto per microattività più semplici.\nLe piattaforme di crowdsourcing consentono anche una grande flessibilità, poiché i parametri delle attività possono essere modificati in tempo reale in base ai risultati iniziali. Ciò crea un ciclo di feedback per miglioramenti iterativi al processo di raccolta dati. I lavori complessi possono essere suddivisi in microattività e distribuiti a più persone, con risultati convalidati in modo incrociato assegnando versioni ridondanti della stessa attività. Se gestito in modo ponderato, il crowdsourcing consente il coinvolgimento della comunità attorno a un progetto collaborativo, in cui i partecipanti trovano una ricompensa nel contribuire.\nTuttavia, mentre il crowdsourcing offre numerosi vantaggi, è essenziale affrontarlo con una strategia chiara. Mentre fornisce l’accesso a un set diversificato di annotatori, introduce anche variabilità nella qualità delle annotazioni. Inoltre, piattaforme come Mechanical Turk potrebbero non sempre catturano uno spettro demografico completo; spesso, gli individui esperti di tecnologia sono sovra-rappresentati, mentre i bambini e gli anziani potrebbero essere sotto-rappresentati. Fornire istruzioni chiare e formazione per gli annotatori è fondamentale. Controlli periodici e convalide dei dati etichettati aiutano a mantenere la qualità. Ciò si ricollega all’argomento della chiara definizione del problema di cui abbiamo discusso in precedenza. Il crowdsourcing per i set di dati richiede anche una particolare attenzione alle considerazioni etiche. È fondamentale assicurarsi che i partecipanti siano informati su come verranno utilizzati i loro dati e che la loro privacy sia protetta. Il controllo di qualità tramite protocolli dettagliati, trasparenza nell’approvvigionamento e verifica è essenziale per garantire risultati affidabili.\nPer TinyML, il crowdsourcing può presentare alcune sfide uniche. I dispositivi TinyML sono altamente specializzati per attività particolari entro vincoli rigorosi. Di conseguenza, i dati di cui hanno bisogno tendono a essere molto specifici. Ottenere tali dati specializzati da un pubblico generico può essere difficile tramite crowdsourcing. Ad esempio, le applicazioni TinyML spesso si basano su dati raccolti da determinati sensori o hardware. Il crowdsourcing richiederebbe ai partecipanti di avere accesso a dispositivi molto specifici e coerenti, come i microfoni, con le stesse frequenze di campionamento. Queste sfumature hardware presentano ostacoli anche per semplici attività audio come l’individuazione di parole chiave.\nOltre all’hardware, i dati stessi necessitano di elevata granularità e qualità, dati i limiti di TinyML. Può essere difficile garantire ciò quando si fa crowdsourcing da chi non ha familiarità con il contesto e i requisiti dell’applicazione. Ci sono anche potenziali problemi relativi alla privacy, alla raccolta in tempo reale, alla standardizzazione e alle competenze tecniche. Inoltre, la natura ristretta di molte attività TinyML semplifica l’etichettatura accurata dei dati con la giusta comprensione. I partecipanti potrebbero aver bisogno di un contesto completo per fornire annotazioni affidabili.\nPertanto, mentre il crowdsourcing può funzionare bene in molti casi, le esigenze specializzate di TinyML introducono sfide uniche per i dati. È richiesta un’attenta pianificazione per linee guida, targeting e controllo di qualità. Per alcune applicazioni, il crowdsourcing potrebbe essere fattibile, ma altre potrebbero richiedere più lavoro per la raccolta dati più mirati per ottenere dati di training pertinenti e di alta qualità.\n\n\n5.3.4 Dati Sintetici\nLa generazione di dati sintetici può essere utile per affrontare alcune delle limitazioni della raccolta dati. Comporta la creazione di dati che non sono stati originariamente catturati o osservati, ma vengono generati utilizzando algoritmi, simulazioni o altre tecniche per assomigliare ai dati del mondo reale. Come mostrato in Figura 5.5, i dati sintetici vengono uniti ai dati storici e quindi utilizzati come input per l’addestramento del modello. È diventato uno strumento prezioso in vari campi, in particolare quando i dati del mondo reale sono scarsi, costosi o eticamente difficili (ad esempio, TinyML). Varie tecniche, come le “Generative Adversarial Networks” (GANs) [reti generative avversarie], possono produrre dati sintetici di alta qualità quasi indistinguibili dai dati reali. Queste tecniche hanno fatto notevoli progressi, rendendo la generazione di dati sintetici sempre più realistica e affidabile.\nPotrebbe essere necessario disporre di più dati del mondo reale per l’analisi o l’addestramento di modelli di apprendimento automatico in molti domini, in particolare quelli emergenti. I dati sintetici possono colmare questa lacuna producendo grandi volumi di dati che imitano scenari del mondo reale. Ad esempio, rilevare il suono di un vetro che si rompe potrebbe essere difficile nelle applicazioni di sicurezza in cui un dispositivo TinyML sta cercando di identificare le effrazioni. La raccolta di dati del mondo reale richiederebbe la rottura di numerose finestre, il che è poco pratico e costoso.\nInoltre, avere un set di dati diversificato è fondamentale nell’apprendimento automatico, in particolare nel deep learning. I dati sintetici possono aumentare i set di dati esistenti introducendo varianti, migliorando così la robustezza dei modelli. Ad esempio, SpecAugment è un’eccellente tecnica di aumento dei dati per i sistemi di “Automatic Speech Recognition” (ASR).\nAnche la privacy e la riservatezza sono grandi problemi. I set di dati contenenti informazioni sensibili o personali sollevano problemi di privacy quando vengono condivisi o utilizzati. I dati sintetici, essendo generati artificialmente, non hanno questi legami diretti con individui reali, consentendo un utilizzo più sicuro preservando al contempo le proprietà statistiche essenziali.\nLa generazione di dati sintetici, in particolare una volta stabiliti i meccanismi di generazione, può essere un’alternativa più conveniente. I dati sintetici eliminano la necessità di rompere più finestre per raccogliere dati rilevanti nello scenario applicativo di sicurezza di cui sopra.\nMolti casi d’uso embedded riguardano situazioni uniche, come gli impianti di produzione, che sono difficili da simulare. I dati sintetici consentono ai ricercatori il controllo completo sul processo di generazione dei dati, consentendo la creazione di scenari o condizioni specifici che sono difficili da catturare nella vita reale.\nSebbene i dati sintetici offrano numerosi vantaggi, è essenziale utilizzarli giudiziosamente. Bisogna fare attenzione a garantire che i dati generati rappresentino accuratamente le distribuzioni sottostanti del mondo reale e non introducano distorsioni indesiderate.\n\n\n\n\n\n\nFigura 5.5: Aumento delle dimensioni dei dati di training con la generazione di dati sintetici. Fonte: AnyLogic.\n\n\n\n\n\n\n\n\n\nEsercizio 5.3: Dati Sintetici\n\n\n\n\n\nScopriamo la generazione di dati sintetici utilizzando le Generative Adversarial Networks (GANs) su dati tabellari. Adotteremo un approccio pratico, immergendoci nel funzionamento del modello CTGAN e applicandolo al set di dati Synthea dal dominio sanitario. Dalla pre-elaborazione dei dati al training e valutazione del modello, procederemo passo dopo passo, imparando come creare dati sintetici, valutarne la qualità e sbloccare il potenziale delle GAN per l’aumento dei dati e le applicazioni del mondo reale.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#archiviazione-dati",
    "href": "contents/data_engineering/data_engineering.it.html#archiviazione-dati",
    "title": "5  Data Engineering",
    "section": "5.4 Archiviazione Dati",
    "text": "5.4 Archiviazione Dati\nL’approvvigionamento e l’archiviazione dei dati vanno di pari passo e i dati devono essere archiviati in un formato che faciliti l’accesso e l’elaborazione. A seconda del caso d’uso, possono essere utilizzati vari tipi di sistemi di archiviazione dati per archiviare i set di dati. Alcuni esempi sono mostrati in Tabella 5.1.\n\n\n\nTabella 5.1: Panoramica comparativa del database, del data warehouse e del data lake.\n\n\n\n\n\n\n\n\n\n\nDatabase\nData Warehouse\nData Lake\n\n\n\n\nScopo\nOperativo e transazionale\nAnalitico\n\n\nTipo di dati\nStrutturato\nStrutturato, semi-strutturato e/o non strutturato\n\n\nScala\nDa piccoli a grandi volumi di dati\nGrandi volumi di dati integrati Grandi volumi di dati diversi\n\n\nEsempi\nMySQL\nGoogle BigQuery, Amazon Redshift, Microsoft Azure Synapse, Google Cloud Storage, AWS S3, Azure Data Lake Storage\n\n\n\n\n\n\nI dati archiviati sono spesso accompagnati da metadati, definiti come “dati sui dati”. Forniscono informazioni contestuali dettagliate sui dati, come mezzi di creazione dei dati, ora di creazione, licenza di utilizzo dei dati allegata, ecc. Ad esempio, Hugging Face ha Dataset Cards. Per promuovere un uso responsabile dei dati, i creatori di dataset dovrebbero rivelare potenziali “bias” [pregiudizi] tramite le “dataset cards” [schede dei dataset]. Queste schede possono istruire gli utenti sui contenuti e le limitazioni di un dataset. Le schede forniscono anche un contesto essenziale sull’uso appropriato del dataset evidenziando bias [pregiudizi] e altri dettagli importanti. Avere questo tipo di metadati può anche consentire un rapido recupero se strutturato correttamente. Una volta che il modello è stato sviluppato e distribuito su dispositivi edge, i sistemi di archiviazione possono continuare a memorizzare dati in arrivo, aggiornamenti del modello o risultati analitici.\nData Governance: Con una grande quantità di archiviazione dati, è anche fondamentale disporre di policy e pratiche (ad esempio, “governance” [gestione] dei dati) che aiutino a gestire i dati durante il loro ciclo di vita, dall’acquisizione allo smaltimento. La governance dei dati descrive il modo in cui i dati vengono gestiti e include l’adozione di decisioni chiave in merito al loro accesso e controllo. Figura 5.6 illustra i diversi domini coinvolti nella governance dei dati. Implica l’esercizio dell’autorità e l’assunzione di decisioni sui dati per mantenerne la qualità, garantire la conformità, mantenere la sicurezza e ricavarne valore. La governance dei dati è resa operativa sviluppando politiche, incentivi e sanzioni, coltivando una cultura che percepisce i dati come un bene prezioso. Procedure specifiche e autorità assegnate vengono implementate per salvaguardare la qualità dei dati e monitorarne l’utilizzo e i rischi correlati.\nLa governance dei dati utilizza tre approcci integrativi: pianificazione e controllo, organizzativo e basato sul rischio.\n\nL’approccio di pianificazione e controllo, comune nell’IT, allinea business e tecnologia attraverso cicli annuali e continui aggiustamenti, concentrandosi su una governance verificabile e basata su policy.\nL’approccio organizzativo enfatizza la struttura, stabilendo ruoli autorevoli come Chief Data Officer e garantendo responsabilità e rendicontazione nella governance.\nL’approccio basato sul rischio, intensificato dai progressi dell’IA, si concentra sull’identificazione e la gestione dei rischi intrinseci nei dati e negli algoritmi. Affronta in particolare i problemi specifici dell’IA attraverso valutazioni regolari e strategie di gestione proattiva del rischio, consentendo azioni incidentali e preventive per mitigare gli impatti indesiderati degli algoritmi.\n\n\n\n\n\n\n\nFigura 5.6: Una panoramica del framework di governance dei dati. Fonte: StarCIO..\n\n\n\nEcco alcuni esempi di governance dei dati in diversi settori:\n\nMedicina: Gli Health Information Exchanges (HIE) [scambi di informazioni sanitarie] consentono la condivisione di informazioni sanitarie tra diversi operatori sanitari per migliorare l’assistenza ai pazienti. Implementano rigorose pratiche di governance dei dati per mantenere l’accuratezza, l’integrità, la privacy e la sicurezza dei dati, rispettando normative come l’Health Insurance Portability and Accountability Act (HIPAA). Le policy di governance assicurano che i dati dei pazienti siano condivisi solo con entità autorizzate e che i pazienti possano controllare l’accesso alle proprie informazioni.\nFinanza: Basilea III Framework è un quadro normativo internazionale per le banche. Garantisce che le banche stabiliscano policy, pratiche e responsabilità chiare per la gestione dei dati, assicurandone accuratezza, completezza e tempestività. Non solo consente alle banche di soddisfare la conformità normativa, ma previene anche le crisi finanziarie gestendo i rischi in modo più efficace.\nGoverno: Le agenzie governative che gestiscono i dati dei cittadini, i registri pubblici e le informazioni amministrative implementano la governance dei dati per gestire i dati in modo trasparente e sicuro. Il sistema di previdenza sociale negli Stati Uniti e il sistema Aadhar in India sono buoni esempi di tali sistemi di governance.\n\nConsiderazioni speciali sull’archiviazione dei dati per TinyML\nFormati di Archiviazione Audio Efficienti: I sistemi di individuazione delle parole chiave necessitano di formati di archiviazione audio specializzati per consentire una rapida ricerca delle parole chiave nei dati audio. I formati tradizionali come WAV e MP3 archiviano forme d’onda audio complete, che richiedono un’elaborazione estesa per la ricerca. L’individuazione delle parole chiave utilizza un archivio compresso ottimizzato per la ricerca basata su frammenti. Un approccio consiste nell’archiviazione di caratteristiche acustiche compatte anziché audio grezzo. Tale flusso di lavoro implicherebbe:\n\nEstrazione di Caratteristiche Acustiche: I coefficienti Mel-frequency cepstral (MFCC) rappresentano comunemente importanti caratteristiche audio.\nCreazione di Embedding: Gli “embedding” trasformano le caratteristiche acustiche estratte in spazi vettoriali continui, consentendo un’archiviazione dei dati più compatta e rappresentativa. Questa rappresentazione è essenziale per convertire dati ad alta dimensionalità, come l’audio, in un formato più gestibile ed efficiente per l’elaborazione e l’archiviazione.\nQuantizzazione vettoriale: Questa tecnica rappresenta dati ad alta dimensionalità, come gli embedding, con vettori a bassa dimensionalità, riducendo le esigenze di archiviazione. Inizialmente, un codebook viene generato dai dati di training per definire un set di vettori di codice che rappresentano i vettori di dati originali. Successivamente, ogni vettore di dati viene abbinato alla “codeword” più vicina in base al codebook, garantendo una perdita minima di informazioni.\nArchiviazione sequenziale: L’audio viene frammentato in frame brevi e le feature [caratteristiche] quantizzate (o embedded) per ogni frame vengono archiviate in sequenza per mantenere l’ordine temporale, preservando la coerenza e il contesto dei dati audio.\n\nQuesto formato consente di decodificare le feature frame per frame per la corrispondenza delle parole chiave. La ricerca delle caratteristiche è più rapida della decompressione dell’audio completo.\nSelective Network Output Storage: [Archiviazione selettiva dell’output di rete] Un’altra tecnica per ridurre l’archiviazione consiste nell’eliminare le caratteristiche audio intermedie archiviate durante l’addestramento ma non richieste durante l’inferenza. La rete viene eseguita su audio completo durante l’addestramento. Tuttavia, solo gli output finali vengono archiviati durante l’inferenza.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#elaborazione-dei-dati",
    "href": "contents/data_engineering/data_engineering.it.html#elaborazione-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.5 Elaborazione dei Dati",
    "text": "5.5 Elaborazione dei Dati\nIl “Data processing” elaborazione dei dati si riferisce ai passaggi necessari per trasformare i dati grezzi in un formato adatto per l’inserimento negli algoritmi di apprendimento automatico. È una fase cruciale in qualsiasi flusso di lavoro ML, ma spesso trascurata. Con un’elaborazione dei dati adeguata, è probabile che i modelli ML raggiungano prestazioni ottimali. Figura 5.7 mostra una ripartizione dell’allocazione del tempo di uno scienziato dei dati, evidenziando la parte significativa spesa per la pulizia e l’organizzazione dei dati (%60).\n\n\n\n\n\n\nFigura 5.7: Ripartizione delle attività dei “Data scientist” in base al tempo impiegato. Fonte: Forbes.\n\n\n\nUna corretta pulizia dei dati è un passaggio cruciale che influisce direttamente sulle prestazioni del modello. I dati del mondo reale sono spesso sporchi, contengono errori, valori mancanti, rumore, anomalie e incongruenze. La pulizia dei dati comporta il rilevamento e la correzione di questi problemi per preparare dati di alta qualità per la modellazione. Selezionando attentamente le tecniche appropriate, i data scientist possono migliorare l’accuratezza del modello, ridurre l’overfitting e addestrare gli algoritmi per apprendere pattern più solidi. Nel complesso, un’elaborazione dei dati ponderata consente ai sistemi di apprendimento automatico di scoprire meglio le informazioni e di fare previsioni dai dati del mondo reale.\nI dati spesso provengono da fonti diverse e possono essere non strutturati o semi-strutturati. Pertanto, elaborarli e standardizzarli è essenziale, assicurando che aderiscano a un formato uniforme. Tali trasformazioni possono includere:\n\nNormalizzazione di variabili numeriche\nCodifica di variabili categoriali\nUtilizzo di tecniche come la riduzione della dimensionalità\n\nLa convalida dei dati svolge un ruolo più ampio rispetto alla garanzia di aderenza a determinati standard, come impedire che i valori di temperatura scendano sotto lo zero assoluto. Questi problemi si verificano in TinyML perché i sensori potrebbero funzionare male o produrre temporaneamente letture errate; tali transienti non sono rari. Pertanto, è fondamentale rilevare gli errori nei dati in anticipo prima che si propaghino attraverso la pipeline dei dati. Rigorosi processi di convalida, tra cui la verifica delle pratiche di annotazione iniziali, il rilevamento di valori anomali e la gestione dei valori mancanti tramite tecniche come l’imputazione della media, contribuiscono direttamente alla qualità dei set di dati. Ciò, a sua volta, influisce sulle prestazioni, la correttezza e la sicurezza dei modelli addestrati su di essi. Diamo un’occhiata a Figura 5.8 per un esempio di pipeline di elaborazione dei dati. Nel contesto di TinyML, il Multilingual Spoken Words Corpus (MSWC) è un esempio di pipeline di elaborazione dei dati, flussi di lavoro sistematici e automatizzati per la trasformazione, l’archiviazione e l’elaborazione dei dati. I dati di input (che sono una raccolta di brevi registrazioni) attraversano diverse fasi di elaborazione, come l’allineamento audio-parola e l’estrazione di parole chiave. Semplificando il flusso di dati, dai dati grezzi ai set di dati utilizzabili, le pipeline di dati migliorano la produttività e facilitano lo sviluppo rapido di modelli di apprendimento automatico. MSWC è una raccolta ampia e in continua espansione di registrazioni audio di parole pronunciate in 50 lingue diverse, utilizzate collettivamente da oltre 5 miliardi di persone. Questo set di dati è destinato allo studio accademico e all’uso aziendale in aree come l’identificazione di parole chiave e la ricerca basata sul parlato. È concesso in licenza aperta con Creative Commons Attribution 4.0 per un ampio utilizzo.\n\n\n\n\n\n\nFigura 5.8: Una panoramica della pipeline di elaborazione dati del Multilingual Spoken Words Corpus (MSWC). Fonte: Mazumder et al. (2021).\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. «Multilingual spoken words corpus». In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\n\nIl MSWC ha utilizzato un metodo di allineamento forzato per estrarre automaticamente singole registrazioni di parole per addestrare modelli di individuazione delle parole chiave dal progetto Common Voice, che presenta registrazioni a livello di frase in crowdsourcing. L’allineamento forzato si riferisce a metodi di lunga data nell’elaborazione del parlato che prevedono quando fenomeni del parlato come sillabe, parole o frasi iniziano e finiscono all’interno di una registrazione audio. Nei dati MSWC, le registrazioni in crowdsourcing spesso presentano rumori di sottofondo, come elettricità statica e vento. A seconda dei requisiti del modello, questi rumori possono essere rimossi o mantenuti intenzionalmente.\nMantenere l’integrità dell’infrastruttura dati è uno lavoro continuo. Ciò comprende archiviazione dei dati, sicurezza, gestione degli errori e rigoroso controllo delle versioni. Gli aggiornamenti periodici sono fondamentali, soprattutto in ambiti dinamici come l’individuazione delle parole chiave, per adattarsi alle tendenze linguistiche in evoluzione e alle integrazioni dei dispositivi.\nC’è un boom nelle pipeline di elaborazione dati, comunemente presenti nelle toolchain delle operazioni ML, di cui parleremo nel capitolo MLOps. In breve, questi includono framework come MLOps di Google Cloud. Fornisce metodi per l’automazione e il monitoraggio in tutte le fasi della costruzione del sistema ML, tra cui integrazione, test, rilascio, distribuzione e gestione dell’infrastruttura. Diversi meccanismi si concentrano sull’elaborazione dati, parte integrante di questi sistemi.\n\n\n\n\n\n\nEsercizio 5.4: Elaborazione dei Dati\n\n\n\n\n\nEsploriamo due progetti significativi nell’elaborazione dei dati vocali e nell’apprendimento automatico. MSWC è un vasto set di dati audio con oltre 340.000 parole chiave e 23,4 milioni di esempi parlati di 1 secondo. Viene utilizzato in varie applicazioni come dispositivi voice-enabled e automazione dei call center. Il progetto Few-Shot Keyword Spotting introduce un nuovo approccio per l’individuazione delle parole chiave in diverse lingue, ottenendo risultati impressionanti con dati di addestramento minimi. Esamineremo il set di dati MSWC, impareremo come strutturarlo in modo efficace e poi addestreremo un modello di individuazione di parole chiave con la tecnica “few-shot” [https://www.ibm.com/it-it/topics/few-shot-learning]. Cominciamo!",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#etichettatura-dei-dati",
    "href": "contents/data_engineering/data_engineering.it.html#etichettatura-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.6 Etichettatura dei Dati",
    "text": "5.6 Etichettatura dei Dati\nIl “Data labeling” etichettatura dei dati è importante per creare set di dati di training di alta qualità per modelli di apprendimento automatico. Le etichette forniscono informazioni di base, consentendo ai modelli di apprendere relazioni tra input e output desiderati. Questa sezione copre considerazioni chiave per la selezione di tipi di etichette, formati e contenuti per acquisire le informazioni necessarie per le attività. Discute approcci di annotazione comuni, dall’etichettatura manuale al crowdsourcing ai metodi assistiti dall’intelligenza artificiale, e le “best practice” per garantire la qualità delle etichette tramite formazione, linee guida e controlli di qualità. Sottolineiamo anche il trattamento etico degli annotatori umani. Viene anche esplorata l’integrazione dell’intelligenza artificiale per accelerare e aumentare l’annotazione umana. Comprendere le esigenze, le sfide e le strategie di etichettatura è essenziale per costruire dataset affidabili e utili per addestrare sistemi di apprendimento automatico performanti e affidabili.\n\n5.6.1 Tipi di Etichette\nLe etichette contengono informazioni su attività o concetti chiave. Figura 5.9 include alcuni tipi di etichette comuni: una “classification label” [etichetta di classificazione] viene utilizzata per categorizzare le immagini con etichette (etichettando un’immagine con “dog” [cane] e presenta un cane); un “bounding box” [riquadro delimitatore] identifica la posizione dell’oggetto (disegnando un riquadro attorno al cane); una “segmentation map” [mappa di segmentazione] classifica gli oggetti a livello di pixel (evidenziando il cane con un colore distinto); una “caption” [didascalia] fornisce annotazioni descrittive (descrivendo le azioni, la posizione, il colore, ecc. del cane); e una “transcript” [trascrizione] denota il contenuto audio. La scelta del formato dell’etichetta dipende dal caso d’uso e dai vincoli di risorse, poiché etichette più dettagliate richiedono un lavoro maggiore per la raccolta (Johnson-Roberson et al. 2017).\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, e Ram Vasudevan. 2017. «Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?» In 2017 IEEE International Conference on Robotics and Automation (ICRA), 746–53. Singapore, Singapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\n\n\n\n\n\nFigura 5.9: Una panoramica dei tipi di etichette comuni.\n\n\n\nA meno che non si concentri sull’apprendimento auto-supervisionato, un set di dati fornirà probabilmente etichette che affrontano una o più attività di interesse. Date le loro limitazioni di risorse uniche, i creatori di set di dati devono considerare quali informazioni le etichette dovrebbero catturare e come possono ottenere praticamente le etichette necessarie. I creatori devono prima decidere quali tipi di etichette di contenuto dovrebbero catturare. Ad esempio, un creatore interessato al rilevamento delle auto vorrebbe etichettare le auto nel suo dataset. Tuttavia, potrebbe anche considerare se raccogliere simultaneamente etichette per altre attività per cui il set di dati potrebbe essere potenzialmente utilizzato, come il rilevamento dei pedoni.\nInoltre, gli annotatori possono fornire metadati per le informazioni su come il set di dati rappresenta diverse caratteristiche di interesse (cfr. Sezione 5.9). Il dataset Common Voice, ad esempio, include vari tipi di metadati che forniscono informazioni sugli oratori, sulle registrazioni e sulla qualità del set di dati per ciascuna lingua rappresentata (Ardila et al. 2020). Includono suddivisioni demografiche che mostrano il numero di registrazioni per fascia di età e genere del parlante. Questo ci consente di vedere chi ha contribuito alle registrazioni per ogni lingua. Includono anche statistiche come la durata media delle registrazioni e il numero totale di ore di registrazioni convalidate. Queste forniscono informazioni sulla natura e le dimensioni dei set di dati per ogni lingua. Inoltre, le metriche di controllo qualità come la percentuale di registrazioni convalidate sono utili per sapere quanto siano completi e puliti i set di dati. I metadati includono anche suddivisioni demografiche normalizzate scalate al 100% per il confronto tra le lingue. Questo evidenzia le differenze di rappresentazione tra lingue con risorse più elevate e più basse.\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, e Gregor Weber. 2020. «Common Voice: A Massively-Multilingual Speech Corpus». In Proceedings of the Twelfth Language Resources and Evaluation Conference, 4218–22. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\nSuccessivamente, i creatori devono determinare il formato di tali etichette. Ad esempio, un creatore interessato al rilevamento delle auto potrebbe scegliere tra etichette di classificazione binaria che indicano se è presente un’auto, riquadri di delimitazione che mostrano le posizioni generali di tutte le auto o etichette di segmentazione pixel per pixel che mostrano la posizione esatta di ogni auto. La scelta del formato dell’etichetta può dipendere dal caso d’uso e dai vincoli di risorse, poiché le etichette più dettagliate sono in genere più costose e richiedono più tempo per essere acquisite.\n\n\n5.6.2 Metodi di Annotazione\nGli approcci comuni all’annotazione includono etichettatura manuale, crowdsourcing e tecniche semi-automatiche. L’etichettatura manuale da parte di esperti produce alta qualità ma necessita di maggiore scalabilità. Il crowdsourcing consente ai non esperti di distribuire annotazioni, spesso tramite piattaforme dedicate (Sheng e Zhang 2019). Metodi debolmente supervisionati e programmatici possono ridurre il lavoro manuale generando etichette in modo euristico o automatico (Ratner et al. 2018).\n\nSheng, Victor S., e Jing Zhang. 2019. «Machine Learning with Crowdsourcing: A Brief Summary of the Past Research and Future Directions». Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 9837–43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, e Christopher Ré. 2018. «Snorkel MeTaL: Weak Supervision for Multi-Task Learning». In Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning. ACM. https://doi.org/10.1145/3209889.3209898.\nDopo aver deciso il contenuto e il formato desiderati per le etichette, i creatori iniziano il processo di annotazione. Per raccogliere un gran numero di etichette da annotatori umani, i creatori si affidano spesso a piattaforme di annotazione dedicate, che possono metterli in contatto con team di annotatori umani. Quando utilizzano queste piattaforme, i creatori potrebbero aver bisogno di maggiori informazioni sui background degli annotatori e sui livelli di esperienza con argomenti di interesse. Tuttavia, alcune piattaforme offrono l’accesso ad annotatori con competenze specifiche (ad esempio, medici).\n\n\n\n\n\n\nEsercizio 5.5: Etichette Autoprodotte\n\n\n\n\n\nEsploriamo Wake Vision, un set di dati completo progettato per il rilevamento di persone con TinyML. Questo set di dati deriva da un set di dati più ampio e generico, Open Images (Kuznetsova et al. 2020), e specificamente adattato per il rilevamento binario di persone.\nIl processo di trasformazione comporta il filtraggio e la rietichettatura delle etichette e dei riquadri di delimitazione esistenti in Open Images utilizzando una pipeline automatizzata. Questo metodo non solo consente di risparmiare tempo e risorse, ma garantisce anche che il set di dati soddisfi i requisiti specifici delle applicazioni TinyML.\nInoltre, generiamo metadati per confrontare la correttezza e la robustezza dei modelli in scenari difficili.\nCominciamo!\n\n\n\n\n\nKuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. «The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale». International journal of computer vision 128 (7): 1956–81.\n\n\n5.6.3 Garantire la Qualità dell’Etichetta\nNon vi è alcuna garanzia che le etichette dei dati siano effettivamente corrette. Figura 5.10 mostra alcuni esempi di casi di etichettatura rigida: alcuni errori derivano da immagini sfocate che le rendono difficili da identificare (l’immagine della rana), e altri derivano da una mancanza di conoscenza del dominio (il caso della cicogna nera). È possibile che nonostante le migliori istruzioni fornite agli etichettatori, etichettino ancora in modo errato alcune immagini (Northcutt, Athalye, e Mueller 2021). Strategie come controlli di qualità, formazione degli annotatori e raccolta di più etichette per ciascun elemento possono aiutare a garantire la qualità delle etichette. Per attività ambigue, più annotatori possono aiutare a identificare i punti dati controversi e quantificare i livelli di disaccordo.\n\n\n\n\n\n\nFigura 5.10: Alcuni esempi di casi di etichettatura rigida. Fonte: Northcutt, Athalye, e Mueller (2021).\n\n\nNorthcutt, Curtis G, Anish Athalye, e Jonas Mueller. 2021. «Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks». arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite.\n\n\nQuando si lavora con annotatori umani, è importante offrire un compenso equo e dare priorità al trattamento etico, poiché gli annotatori possono essere sfruttati o danneggiati durante il processo di etichettatura (Perrigo, 2023). Ad esempio, se è probabile che un set di dati contenga contenuti inquietanti, gli annotatori potrebbero trarre vantaggio dall’avere la possibilità di visualizzare le immagini in scala di grigi (Google, s.d.).\n\nGoogle. s.d. «Information quality content moderation». https://blog.google/documents/83/.\n\n\n5.6.4 Annotazione assistita dall’intelligenza artificiale\nIl ML ha una domanda insaziabile di dati. Pertanto, sono necessari più dati. Ciò solleva la questione di come possiamo ottenere più dati etichettati. Invece di generare e curare sempre i dati manualmente, possiamo fare affidamento sui modelli di intelligenza artificiale esistenti per etichettare i set di dati in modo più rapido ed economico, anche se spesso con una qualità inferiore rispetto all’annotazione umana. Questo può essere fatto in vari modi come mostrato in Figura 5.11, tra cui i seguenti:\n\nPre-annotazione: I modelli di intelligenza artificiale possono generare etichette preliminari per un set di dati utilizzando metodi come l’apprendimento semi-supervisionato (Chapelle, Scholkopf, e Zien 2009), che gli esseri umani possono poi esaminare e correggere. Questo può far risparmiare una notevole quantità di tempo, soprattutto per set di dati di grandi dimensioni.\nApprendimento attivo: I modelli di intelligenza artificiale possono identificare i dati più informativi in un dataset, che possono quindi essere riordinati per priorità per l’annotazione umana. Questo può aiutare a migliorare la qualità del set di dati etichettato riducendo al contempo il tempo di annotazione complessivo.\nControllo qualità: I modelli di intelligenza artificiale possono identificare e segnalare potenziali errori nelle annotazioni umane, contribuendo a garantire l’accuratezza e la coerenza del set di dati etichettato.\n\n\nChapelle, O., B. Scholkopf, e A. Zien Eds. 2009. «Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]». IEEE Trans. Neural Networks 20 (3): 542–42. https://doi.org/10.1109/tnn.2009.2015974.\nEcco alcuni esempi di come l’annotazione assistita dall’intelligenza artificiale è stata proposta come utile:\n\nImmagini mediche: L’annotazione assistita dall’intelligenza artificiale etichetta le immagini mediche, come scansioni MRI (Magnetic Resonance Imaging) e raggi X (Krishnan, Rajpurkar, e Topol 2022). Annotare attentamente i set di dati medici è estremamente impegnativo, soprattutto su larga scala, poiché gli esperti del settore sono scarsi e diventano costosi. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale per diagnosticare malattie e altre condizioni mediche in modo più accurato ed efficiente.\nAuto a guida autonoma: L’annotazione assistita dall’intelligenza artificiale viene utilizzata per etichettare immagini e video di auto a guida autonoma. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale per identificare oggetti sulla strada, come altri veicoli, pedoni e segnali stradali.\nSocial media: L’annotazione assistita dall’intelligenza artificiale etichetta i post sui social media come immagini e video. Ciò può aiutare ad addestrare i modelli di intelligenza artificiale a identificare e classificare diversi tipi di contenuti, come notizie, pubblicità e post personali.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, e Eric J. Topol. 2022. «Self-supervised learning in medicine and healthcare». Nat. Biomed. Eng. 6 (12): 1346–52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\n\n\n\n\nFigura 5.11: Strategie per acquisire ulteriori dati di addestramento etichettati. Fonte: Standford AI Lab.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#controllo-della-versione-dei-dati",
    "href": "contents/data_engineering/data_engineering.it.html#controllo-della-versione-dei-dati",
    "title": "5  Data Engineering",
    "section": "5.7 Controllo della Versione dei Dati",
    "text": "5.7 Controllo della Versione dei Dati\nI sistemi di produzione sono costantemente inondati da volumi di dati fluttuanti e in aumento, che determinano la rapida comparsa di numerose repliche di dati. Questi dati in aumento servono come base per l’addestramento di modelli di apprendimento automatico. Ad esempio, un’azienda di vendita globale impegnata nella previsione delle vendite riceve continuamente dati sul comportamento dei consumatori. Allo stesso modo, i sistemi sanitari che formulano modelli predittivi per la diagnosi delle malattie acquisiscono costantemente nuovi dati sui pazienti. Le applicazioni TinyML, come l’individuazione delle parole chiave, sono molto affamate di dati per quanto riguarda la quantità di dati generati. Di conseguenza, è fondamentale un monitoraggio meticoloso delle versioni dei dati e delle prestazioni del modello corrispondente.\nIl “Data Version Control” [controllo delle versioni dei dati] offre una metodologia strutturata per gestire in modo efficiente alterazioni e versioni di set di dati. Facilita il monitoraggio delle modifiche, conserva più versioni e garantisce riproducibilità e tracciabilità nei progetti incentrati sui dati. Inoltre, il controllo delle versioni dei dati offre la versatilità di rivedere e utilizzare versioni specifiche in base alle necessità, garantendo che ogni fase dell’elaborazione dei dati e dello sviluppo del modello possa essere riesaminata e verificata in modo preciso e semplice. Ha una varietà di usi pratici -\nGestione del Rischio: Il controllo della versione dei dati consente trasparenza e responsabilità monitorando le versioni del set di dati.\nCollaborazione ed Efficienza: Un facile accesso a diverse versioni del set di dati in un unico posto può migliorare la condivisione dei dati di controllo specifici e consentire una collaborazione efficiente.\nRiproducibilità: Il controllo della versione dei dati consente di monitorare le prestazioni dei modelli riguardanti diverse versioni dei dati, e quindi di abilitare la riproducibilità.\nConcetti Chiave\n\nCommit: È un’istantanea immutabile dei dati in un momento specifico, che rappresenta una versione univoca. Ogni commit è associato a un identificatore univoco per consentire\nBranch: I “branch” [rami] consentono a sviluppatori e specialisti dei data di discostarsi dalla linea di sviluppo principale e continuare a lavorare in modo indipendente senza influenzare altri rami. Ciò è particolarmente utile quando si sperimentano nuove funzionalità o modelli, consentendo sviluppo e sperimentazione paralleli senza il rischio di danneggiare il ramo principale stabile.\nMerge: I “Merge” [unioni] aiutano a integrare le modifiche da rami diversi mantenendo l’integrità dei dati.\n\nCon il controllo della versione dei dati in atto, possiamo tracciare le modifiche mostrate in Figura 5.12, riprodurre i risultati precedenti ripristinando le versioni precedenti e collaborare in modo sicuro ramificando e isolando le modifiche.\n\n\n\n\n\n\nFigura 5.12: Data versioning.\n\n\n\nSistemi di Data Version Control più Diffusi\nDVC: È un’abbreviazione di “Data Version Control” ed è uno strumento open source e leggero che funziona su Git Hub e supporta tutti i tipi di formati di dati. Può integrarsi perfettamente nel flusso di lavoro se Git viene utilizzato per gestire il codice. Cattura le versioni dei dati e dei modelli nei commit Git mentre li archivia in locale o sul cloud (ad esempio, AWS, Google Cloud, Azure). Questi dati e modelli (ad esempio, artefatti di ML) sono definiti nei file di metadati, che vengono aggiornati a ogni commit. Può consentire il monitoraggio delle metriche dei modelli su diverse versioni dei dati.\nlakeFS: È uno strumento open source che supporta il controllo della versione dei dati sui “data lake”. Supporta molte operazioni simili a git, come i “branch” e il “merge” dei dati, nonché il ripristino delle versioni precedenti dei dati. Ha anche una funzionalità UI unica, che semplifica notevolmente l’esplorazione e la gestione dei dati.\nGit LFS: È utile per il controllo della versione dei dataset di dimensioni ridotte. Utilizza le funzionalità di “branch” e “merge” native di Git, ma è limitato nel tracciamento delle metriche, nel ripristino delle versioni precedenti o nell’integrazione con i “data lake”.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#ottimizzazione-dei-dati-per-lia-embedded",
    "href": "contents/data_engineering/data_engineering.it.html#ottimizzazione-dei-dati-per-lia-embedded",
    "title": "5  Data Engineering",
    "section": "5.8 Ottimizzazione dei Dati per l’IA Embedded",
    "text": "5.8 Ottimizzazione dei Dati per l’IA Embedded\nI creatori che lavorano su sistemi embedded potrebbero avere priorità insolite quando puliscono i loro dataset. Da un lato, i modelli potrebbero essere sviluppati per casi d’uso insolitamente specifici, che richiedono un filtraggio intensivo dei dataset. Mentre altri modelli di linguaggio naturale possono essere in grado di trasformare qualsiasi discorso in testo, un modello per un sistema embedded può essere incentrato su un singolo compito limitato, come il rilevamento di una parola chiave. Di conseguenza, i creatori possono filtrare in modo aggressivo grandi quantità di dati perché devono affrontare un determinato compito. Un sistema di intelligenza artificiale embedded può anche essere legato a specifici dispositivi hardware o ambienti. Ad esempio, un modello video potrebbe dover elaborare immagini da un singolo tipo di telecamera, che verrà montata solo sui campanelli nei quartieri residenziali. In questo scenario, i creatori possono scartare le immagini se provengono da un diverso tipo di telecamera, mostrano il tipo sbagliato di scenario o sono state scattate dall’altezza o dall’angolazione sbagliate.\nD’altra parte, ci si aspetta spesso che i sistemi di IA embedded forniscano prestazioni particolarmente accurate in contesti imprevedibili del mondo reale. Ciò può portare i creatori a progettare set di dati per rappresentare variazioni nei potenziali input e promuovere la robustezza del modello. Di conseguenza, possono definire un ambito ristretto per il loro progetto ma poi puntare a una copertura approfondita entro quei limiti. Ad esempio, i creatori del modello del campanello menzionato sopra potrebbero provare a coprire le variazioni nei dati derivanti da:\n\nQuartieri geograficamente, socialmente e architettonicamente diversi\nDiversi tipi di illuminazione artificiale e naturale\nDiverse stagioni e condizioni meteorologiche\nOstruzioni (ad esempio gocce di pioggia o scatole di consegna che oscurano la visuale della telecamera)\n\nCome descritto sopra, i creatori possono prendere in considerazione il crowdsourcing o la generazione sintetica di dati per includere queste varianti.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#sec-data-transparency",
    "href": "contents/data_engineering/data_engineering.it.html#sec-data-transparency",
    "title": "5  Data Engineering",
    "section": "5.9 Trasparenza dei Dati",
    "text": "5.9 Trasparenza dei Dati\nFornendo una documentazione chiara e dettagliata, i creatori possono aiutare gli sviluppatori a capire come utilizzare al meglio i loro set di dati. Diversi gruppi hanno suggerito formati di documentazione standardizzati per i set di dati, come Data Cards (Pushkarna, Zaldivar, e Kjartansson 2022), datasheet (Gebru et al. 2021), data statement (Bender e Friedman 2018), o Data Nutrition Labels (Holland et al. 2020). Quando rilasciano un dataset, i creatori possono descrivere quali tipi di dati hanno raccolto, come li hanno raccolti ed etichettati e quali tipi di casi d’uso potrebbero essere adatti o meno al set di dati. Quantitativamente, potrebbe essere opportuno mostrare quanto bene il set di dati rappresenti gruppi diversi (ad esempio, gruppi di genere diversi, telecamere diverse).\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, e Kate Crawford. 2021. «Datasheets for datasets». Commun. ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\nBender, Emily M., e Batya Friedman. 2018. «Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science». Transactions of the Association for Computational Linguistics 6 (dicembre): 587–604. https://doi.org/10.1162/tacl_a_00041.\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, e Kasia Chmielinski. 2020. «The Dataset Nutrition Label: A Framework to Drive Higher Data Quality Standards». In Data Protection and Privacy. Hart Publishing. https://doi.org/10.5040/9781509932771.ch-001.\nFigura 5.13 mostra un esempio di una scheda dati per un set di dati di computer vision (CV). Include alcune informazioni di base sul set di dati e istruzioni su come utilizzarlo, inclusi i “bias” noti.\n\n\n\n\n\n\nFigura 5.13: Data card che descrive un dataset CV. Fonte: Pushkarna, Zaldivar, e Kjartansson (2022).\n\n\nPushkarna, Mahima, Andrew Zaldivar, e Oddur Kjartansson. 2022. «Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI». In 2022 ACM Conference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nTenere traccia della provenienza dei dati, essenzialmente le origini e il viaggio di ogni dato attraverso la pipeline dei dati, non è solo una buona pratica, ma un requisito essenziale per la qualità. La provenienza dei dati contribuisce in modo significativo alla trasparenza dei sistemi di machine learning. I sistemi trasparenti semplificano l’analisi dei dati, consentendo una migliore identificazione e rettifica di errori, bias o incongruenze. Ad esempio, se un modello di ML addestrato su dati medici non è performante in aree specifiche, tracciare la provenienza può aiutare a identificare se il problema riguarda i metodi di raccolta dati, i gruppi demografici rappresentati nei dati o altri fattori. Questo livello di trasparenza non aiuta solo a eseguire il debug del sistema, ma svolge anche un ruolo cruciale nel migliorare la qualità complessiva dei dati. Migliorando l’affidabilità e la credibilità del set di dati, la provenienza dei dati migliora anche le prestazioni del modello e la sua accettabilità tra gli utenti finali.\nQuando si produce la documentazione, i creatori devono anche specificare come gli utenti possono accedere al dataset e come questo verrà mantenuto nel tempo. Ad esempio, gli utenti potrebbero dover sottoporsi a una formazione o ricevere un’autorizzazione speciale dai creatori prima di accedere a un set di dati di informazioni protette, come con molti dataset medici. In alcuni casi, gli utenti potrebbero non accedere direttamente ai dati. Devono invece inviare il loro modello per essere addestrato sull’hardware dei creatori del set di dati, seguendo una configurazione di apprendimento “federato” (Aledhari et al. 2020). I creatori possono anche descrivere per quanto tempo il dataset rimarrà accessibile, come gli utenti possono inviare feedback su eventuali errori che scoprono e se ci sono piani per aggiornare il set di dati.\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, e Fahad Saeed. 2020. «Federated Learning: A Survey on Enabling Technologies, Protocols, and Applications». #IEEE_O_ACC# 8: 140699–725. https://doi.org/10.1109/access.2020.3013541.\nAlcune leggi e normative promuovono anche la trasparenza dei dati attraverso nuovi requisiti per le organizzazioni:\n\nIl “General Data Protection Regulation (GDPR)” nell’Unione Europea: Stabilisce requisiti rigorosi per l’elaborazione e la protezione dei dati personali dei cittadini dell’UE. Impone policy sulla privacy in linguaggio semplice che spiegano chiaramente quali dati vengono raccolti, perché vengono utilizzati, per quanto tempo vengono archiviati e con chi vengono condivisi. Il GDPR impone inoltre che le informative sulla privacy debbano includere dettagli sulla base giuridica per l’elaborazione, i trasferimenti di dati, i periodi di conservazione, i diritti di accesso e cancellazione e le informazioni di contatto per i responsabili del trattamento dei dati.\nIl “California’s Consumer Privacy Act” (CCPA): Il CCPA richiede policy sulla privacy chiare e diritti di esclusione per vendere dati personali. In modo significativo, stabilisce anche i diritti dei consumatori di essere interpellati per la divulgazione dei propri dati specifici. Le aziende devono fornire copie delle informazioni personali raccolte e dettagli su come vengono utilizzate, quali categorie vengono raccolte e cosa ricevono le terze parti. I consumatori possono identificare dati che ritengono debbano essere più accurati. La legge rappresenta un importante passo avanti nel potenziamento dell’accesso ai dati personali.\n\nGarantire la trasparenza dei dati presenta diverse sfide, soprattutto perché richiede molto tempo e risorse finanziarie. I sistemi di dati sono anche piuttosto complessi e la trasparenza completa può richiedere tempo. La trasparenza completa può anche sopraffare i consumatori con troppi dettagli. Infine, è anche importante bilanciare il compromesso tra trasparenza e privacy.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#licenze",
    "href": "contents/data_engineering/data_engineering.it.html#licenze",
    "title": "5  Data Engineering",
    "section": "5.10 Licenze",
    "text": "5.10 Licenze\nMolti dataset di alta qualità provengono da fonti proprietarie o contengono informazioni protette da copyright. Ciò introduce le licenze come una competenza legale impegnativa. Le aziende desiderose di addestrare sistemi di ML devono impegnarsi in trattative per ottenere licenze che garantiscano l’accesso legale a questi dataset. Inoltre, i termini delle licenze possono imporre restrizioni sulle applicazioni dei dati e sui metodi di condivisione. Il mancato rispetto di queste licenze può avere gravi conseguenze.\nAd esempio, ImageNet, uno dei dataset più ampiamente utilizzati per la ricerca sulla visione artificiale, è un caso emblematico. La maggior parte delle sue immagini è stata ottenuta da fonti online pubbliche senza esplicita autorizzazione, suscitando preoccupazioni etiche (Prabhu e Birhane, 2020). L’accesso al set di dati ImageNet per le aziende richiede la registrazione e l’adesione ai suoi termini di utilizzo, che limitano l’uso commerciale (ImageNet, 2021). I principali attori come Google e Microsoft investono in modo significativo nella concessione di licenze per i set di dati per migliorare i loro sistemi di visione di ML. Tuttavia, il fattore costo limita l’accessibilità per i ricercatori di aziende più piccole con budget limitati.\nIl dominio legale della concessione di licenze per i dati ha visto casi importanti che aiutano a definire i parametri di utilizzo corretto. Un esempio importante è Authors Guild, Inc. contro Google, Inc. Questa causa del 2005 sosteneva che il progetto di scansione di libri di Google violava i diritti d’autore visualizzando frammenti senza autorizzazione. Tuttavia, i tribunali alla fine si sono pronunciati a favore di Google, sostenendo il “fair use” [correttezza] in base alla natura trasformativa della creazione di un indice ricercabile e della visualizzazione di estratti limitati di testo. Questo precedente fornisce alcune basi legali per sostenere che le protezioni del “fair use” si applicano all’indicizzazione di set di dati e alla generazione di campioni rappresentativi per l’apprendimento automatico. Tuttavia, le restrizioni di licenza rimangono vincolanti, quindi un’analisi completa dei termini di licenza è fondamentale. Il caso dimostra perché le negoziazioni con i fornitori di dati sono importanti per consentire un utilizzo legale entro limiti accettabili.\nNuove Normative sui Dati e le Loro Implicazioni\nAnche le nuove normative sui dati hanno un impatto sulle pratiche di licenza. Il panorama legislativo si sta evolvendo con normative come l’Artificial Intelligence Act dell’UE, che è pronto a regolamentare lo sviluppo e l’uso dei sistemi di intelligenza artificiale all’interno dell’Unione Europea (UE). Questa legislazione:\n\nClassifica i sistemi di IA in base al rischio.\nImpone prerequisiti di sviluppo e utilizzo.\nSottolinea la qualità dei dati, la trasparenza, la supervisione umana e la responsabilità.\n\nInoltre, l’EU Act affronta le dimensioni etiche e le sfide operative in settori quali sanità e finanza. Gli elementi chiave includono il divieto di sistemi di intelligenza artificiale che presentano rischi “inaccettabili”, condizioni rigorose per sistemi ad alto rischio e obblighi minimi per sistemi di intelligenza artificiale a “rischio limitato”. Il proposto “European AI Board” supervisionerà e garantirà l’implementazione di una regolamentazione efficiente.\nProblemi nell’Assemblaggio di Dataset di Training ML\nProblemi complessi di licenza relativi a dati proprietari, leggi sul copyright e normative sulla privacy limitano le opzioni per l’assemblaggio dei set di dati di training ML. Tuttavia, espandere l’accessibilità tramite licenze più aperte o collaborazioni di dati pubblico-private potrebbe accelerare notevolmente il progresso del settore e gli standard etici.\nA volte, alcune parti di un dataset potrebbero dover essere rimosse o oscurate per rispettare gli accordi di utilizzo dei dati o proteggere informazioni sensibili. Ad esempio, un set di dati di informazioni utente potrebbe contenere nomi, dettagli di contatto e altri dati identificativi che potrebbero dover essere rimossi dal set di dati; questo avviene molto tempo dopo che il set di dati è già stato attivamente reperito e utilizzato per l’addestramento dei modelli. Analogamente, un dataset che include contenuti protetti da copyright o segreti commerciali potrebbe dover filtrare tali parti prima di essere distribuito. Leggi come il General Data Protection Regulation (GDPR), il California Consumer Privacy Act (CCPA) e L’Amended Act on the Protection of Personal Information (APPI) sono state approvate per garantire il diritto all’oblio. Queste normative impongono legalmente ai fornitori di modelli di cancellare i dati degli utenti su richiesta.\nI raccoglitori e i fornitori di dati devono essere in grado di adottare misure appropriate per de-identificare o filtrare qualsiasi informazione proprietaria, concessa in licenza, riservata o regolamentata, se necessario. A volte, gli utenti possono richiedere esplicitamente che i loro dati vengano rimossi.\nLa possibilità di aggiornare il set di dati rimuovendo i dati consentirà ai creatori di rispettare gli obblighi legali ed etici relativi al loro utilizzo e alla privacy. Tuttavia, la capacità di rimuovere i dati presenta alcune limitazioni importanti. Dobbiamo considerare che alcuni modelli potrebbero essere già stati addestrati sul dataset e non esiste un modo chiaro o noto per eliminare l’effetto di un particolare campione di dati dalla rete addestrata. Non esiste un meccanismo di cancellazione. Quindi, ciò solleva la questione: il modello dovrebbe essere riaddestrato da zero ogni volta che viene rimosso un campione? Questa è un’opzione costosa. Una volta che i dati sono stati utilizzati per addestrare un modello, la semplice rimozione dal set di dati originale potrebbe non eliminare completamente il suo impatto sul comportamento del modello. Sono necessarie nuove ricerche sugli effetti della rimozione dei dati sui modelli già addestrati e se sia necessario un ri-addestramento completo per evitare di conservare artefatti di dati eliminati. Ciò presenta una considerazione importante quando si bilanciano gli obblighi di licenza dei dati con l’efficienza e la praticità in un sistema di ML in evoluzione e distribuito.\nLa licenza del dataset è un dominio poliedrico che interseca tecnologia, etica e legge. Comprendere queste complessità diventa fondamentale per chiunque crei set di dati durante l’ingegneria dei dati, man mano che il mondo si evolve.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#conclusione",
    "href": "contents/data_engineering/data_engineering.it.html#conclusione",
    "title": "5  Data Engineering",
    "section": "5.11 Conclusione",
    "text": "5.11 Conclusione\nI dati sono il componente fondamentale dei sistemi di intelligenza artificiale. Senza dati di qualità, anche gli algoritmi di apprendimento automatico più avanzati falliranno. L’ingegneria dei dati comprende il processo end-to-end di raccolta, archiviazione, elaborazione e gestione dei dati per alimentare lo sviluppo di modelli di apprendimento automatico. Si inizia con la definizione chiara del problema principale e degli obiettivi, che guidano una raccolta dati efficace. I dati possono essere reperiti da diversi mezzi, tra cui dataset esistenti, web scraping, crowdsourcing e generazione di dati sintetici. Ogni approccio comporta compromessi tra costi, velocità, privacy e specificità. Una volta raccolti i dati, un’etichettatura ponderata tramite annotazione manuale o assistita dall’intelligenza artificiale consente la creazione di set di dati di training di alta qualità. Un’archiviazione adeguata in database, “warehouse” o “lake” facilita l’accesso e l’analisi. I metadati forniscono dettagli contestuali sui dati. L’elaborazione dei dati trasforma i dati grezzi in un formato pulito e coerente per lo sviluppo di modelli di apprendimento automatico. In tutta questa pipeline, la trasparenza attraverso la documentazione e il tracciamento della provenienza è fondamentale per l’etica, la verificabilità e la riproducibilità. I protocolli di licenza dei dati regolano anche l’accesso e l’uso legale dei dati. Le principali sfide nell’ingegneria dei dati includono rischi per la privacy, lacune di rappresentazione, restrizioni legali sui dati proprietari e la necessità di bilanciare vincoli concorrenti come velocità e qualità. Progettando attentamente dati di training di alta qualità, i professionisti dell’apprendimento automatico possono sviluppare sistemi di intelligenza artificiale accurati, robusti e responsabili, tra cui applicazioni embedded e TinyML.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/data_engineering/data_engineering.it.html#sec-data-engineering-resource",
    "href": "contents/data_engineering/data_engineering.it.html#sec-data-engineering-resource",
    "title": "5  Data Engineering",
    "section": "5.12 Risorse",
    "text": "5.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nData Engineering: Overview.\nFeature engineering.\nData Standards: Speech Commands.\nCrowdsourcing Data for the Long Tail.\nReusing and Adapting Existing Datasets.\nResponsible Data Collection.\nRilevamento Dati Anomali:\n\nAnomaly Detection: Overview.\nAnomaly Detection: Challenges.\nAnomaly Detection: Datasets.\nAnomaly Detection: Using Autoencoders.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 5.1\nEsercizio 5.2\nEsercizio 5.3\nEsercizio 5.4\nEsercizio 5.5\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html",
    "href": "contents/frameworks/frameworks.it.html",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "",
    "text": "6.1 Introduzione\nI framework di machine learning [apprendimento automatico] forniscono gli strumenti e l’infrastruttura per creare, addestrare e distribuire in modo efficiente modelli di apprendimento automatico. In questo capitolo esploreremo l’evoluzione e le capacità chiave dei principali framework come TensorFlow (TF), PyTorch e framework specializzati per dispositivi embedded. Ci immergeremo nei componenti come grafi computazionali, algoritmi di ottimizzazione, accelerazione hardware e altro che consentono agli sviluppatori di creare rapidamente modelli performanti. Comprendere questi framework è essenziale per sfruttare la potenza del deep learning in tutto lo spettro, dal cloud ai dispositivi edge [periferici].\nI framework di apprendimento automatico gestiscono gran parte della complessità dello sviluppo di modelli tramite API di alto livello e linguaggi specifici per dominio che consentono ai professionisti di creare rapidamente modelli combinando componenti e astrazioni predefiniti. Ad esempio, framework come TensorFlow e PyTorch forniscono API Python per definire architetture di reti neurali utilizzando livelli, ottimizzatori, set di dati e altro. Ciò consente un’iterazione rapida rispetto alla codifica di ogni dettaglio del modello partendo da zero.\nUna capacità chiave offerta da questi framework è rappresentata dai motori di training distribuiti che possono scalare l’addestramento del modello su cluster di GPU e TPU. Ciò rende possibile il training di modelli all’avanguardia con miliardi o trilioni di parametri su vasti set di dati. I framework si integrano anche con hardware specializzato come le GPU NVIDIA per accelerare ulteriormente il training tramite ottimizzazioni come la parallelizzazione ed efficienti operazioni matriciali.\nInoltre, i framework semplificano il deploy [distribuzione] di modelli finiti in produzione tramite strumenti come TensorFlow Serving per il model serving scalabile e TensorFlow Lite per l’ottimizzazione su dispositivi mobili ed edge. Altre capacità preziose includono visualizzazione, tecniche di ottimizzazione del modello come quantizzazione e potatura e monitoraggio delle metriche durante il training.\nI principali framework open source come TensorFlow, PyTorch e MXNet alimentano gran parte della ricerca e dello sviluppo dell’IA oggi. Offerte commerciali come Amazon SageMaker e Microsoft Azure Machine Learning integrano questi framework open source con funzionalità proprietarie e strumenti aziendali.\nGli ingegneri e i professionisti del machine learning sfruttano questi framework robusti per concentrarsi su attività di alto valore come architettura del modello, progettazione delle feature e ottimizzazione degli iperparametri anziché sull’infrastruttura. L’obiettivo è creare e distribuire modelli performanti che risolvano in modo efficiente i problemi del mondo reale.\nIn questo capitolo, esploreremo i principali framework cloud odierni e il modo in cui hanno adattato modelli e strumenti specificamente per la distribuzione embedded ed edge. Confronteremo modelli di programmazione, hardware supportato, capacità di ottimizzazione e altro ancora per comprendere appieno in che modo i framework consentono un apprendimento automatico scalabile dal cloud all’edge.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#evoluzione-dei-framework",
    "href": "contents/frameworks/frameworks.it.html#evoluzione-dei-framework",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.2 Evoluzione dei Framework",
    "text": "6.2 Evoluzione dei Framework\nI framework di apprendimento automatico si sono evoluti in modo significativo per soddisfare le diverse esigenze dei professionisti del machine learning e i progressi nelle tecniche di intelligenza artificiale. Qualche decennio fa, la creazione e l’addestramento di modelli di apprendimento automatico richiedevano un’ampia codifica e infrastruttura di basso livello. Oltre alla necessità di una codifica di basso livello, la prima ricerca sulle reti neurali era limitata da dati e potenza di calcolo insufficienti. Tuttavia, i framework di apprendimento automatico si sono evoluti notevolmente nell’ultimo decennio per soddisfare le crescenti esigenze dei professionisti e i rapidi progressi nelle tecniche di deep learning [apprendimento profondo]. Il rilascio di grandi set di dati come ImageNet (Deng et al. 2009) e i progressi nel calcolo parallelo con GPU hanno sbloccato il potenziale per reti neurali molto più profonde.\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, e Fei-Fei Li. 2009. «ImageNet: A large-scale hierarchical image database». In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–55. IEEE. https://doi.org/10.1109/cvpr.2009.5206848.\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. «Theano: A Python framework for fast computation of mathematical expressions». https://arxiv.org/abs/1605.02688.\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, e Trevor Darrell. 2014. «Caffe: Convolutional Architecture for Fast Feature Embedding». In Proceedings of the 22nd ACM international conference on Multimedia, 675–78. ACM. https://doi.org/10.1145/2647868.2654889.\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2012. «ImageNet Classification with Deep Convolutional Neural Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\nChollet, François. 2018. «Introduction to keras». March 9th.\n\nTokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, e Hiroyuki Yamazaki Vincent. 2019. «Chainer: A Deep Learning Framework for Accelerating the Research Cycle». In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 5:1–6. ACM. https://doi.org/10.1145/3292500.3330756.\n\nSeide, Frank, e Amit Agarwal. 2016. «Cntk: Microsoft’s Open-Source Deep-Learning Toolkit». In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2135–35. ACM. https://doi.org/10.1145/2939672.2945397.\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. «PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation». In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, e Roman Garnett, 8024–35. ACM. https://doi.org/10.1145/3620665.3640366.\nI primi framework di apprendimento automatico, Theano di Team et al. (2016) e Caffe di Jia et al. (2014), sono stati sviluppati da istituzioni accademiche. Theano è stato creato dal Montreal Institute for Learning Algorithms, mentre Caffe è stato sviluppato dal Berkeley Vision and Learning Center. Nel crescente interesse per il deep learning dovuto alle prestazioni all’avanguardia di AlexNet Krizhevsky, Sutskever, e Hinton (2012) sul dataset ImageNet, aziende private e singole persone hanno iniziato a sviluppare framework di ML, dando vita a Keras di Chollet (2018), Chainer di Tokui et al. (2019), TensorFlow di Google (Yu et al. 2018), CNTK di Microsoft (Seide e Agarwal 2016) e PyTorch di Facebook (Ansel et al. 2024).\nMolti di questi framework ML possono essere suddivisi in framework di alto livello, di basso livello e di grafi computazionali statici e dinamici. I framework di alto livello forniscono un livello di astrazione più elevato rispetto a quelli di basso livello. I framework di alto livello hanno funzioni e moduli predefiniti per attività ML comuni, come la creazione, l’addestramento e la valutazione di modelli ML comuni, la preelaborazione dei dati, le funzionalità di progettazione e la visualizzazione dei dati, che i framework di basso livello non hanno. Pertanto, i framework di alto livello possono risultare più facili da usare ma sono meno personalizzabili rispetto a quelli di basso livello (ad esempio, gli utenti di framework di basso livello possono definire livelli personalizzati, funzioni “loss” [di perdita], algoritmi di ottimizzazione, ecc.). Esempi di framework di alto livello sono TensorFlow/Keras e PyTorch. Esempi di framework ML di basso livello includono TensorFlow con API di basso livello, Theano, Caffe, Chainer e CNTK.\nFramework come Theano e Caffe utilizzavano grafi computazionali statici, che richiedevano la definizione anticipata dell’architettura completa del modello, limitandone così la flessibilità. Al contrario, i grafici dinamici vengono costruiti al volo per uno sviluppo più iterativo. Intorno al 2016, framework come PyTorch e TensorFlow 2.0 hanno iniziato ad adottare grafici dinamici, offrendo maggiore flessibilità per lo sviluppo del modello. Discuteremo di questi concetti e dettagli più avanti nella sezione Training dell’IA.\nLo sviluppo di questi framework ha suscitato un’esplosione di dimensioni e complessità del modello nel tempo, dai primi perceptron multistrato e reti convoluzionali ai moderni trasformatori con miliardi o trilioni di parametri. Nel 2016, i modelli ResNet di He et al. (2016) hanno raggiunto un’accuratezza ImageNet record con oltre 150 livelli e 25 milioni di parametri. Poi, nel 2020, il modello linguistico GPT-3 di OpenAI (Brown et al. 2020) ha spinto i parametri a un sorprendente numero di 175 miliardi utilizzando il parallelismo del modello nei framework per addestrare migliaia di GPU e TPU.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. «Deep Residual Learning for Image Recognition». In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nOgni generazione di framework ha sbloccato nuove capacità che hanno alimentato il progresso:\n\nTheano e TensorFlow (2015) hanno introdotto grafi computazionali e differenziazione automatica per semplificare la creazione di modelli.\nCNTK (2016) ha aperto la strada a un addestramento distribuito efficiente combinando parallelismo di modelli e dati.\nPyTorch (2016) ha fornito programmazione imperativa e grafici dinamici per una sperimentazione flessibile.\nTensorFlow 2.0 (2019) ha impostato di default l’esecuzione Eager per intuitività e debug.\nTensorFlow Graphics (2020) ha aggiunto strutture dati 3D per gestire nuvole di punti e mesh.\n\nNegli ultimi anni, i framework sono convergenti. Figura 6.1 mostra che TensorFlow e PyTorch sono diventati i framework ML più dominanti, rappresentando oltre il 95% dei framework ML utilizzati nella ricerca e nella produzione. Keras è stato integrato in TensorFlow nel 2019; Preferred Networks ha trasferito Chainer a PyTorch nel 2019; e Microsoft ha smesso di sviluppare attivamente CNTK nel 2022 per supportare PyTorch su Windows.\n\n\n\n\n\n\nFigura 6.1: Popolarità dei framework ML negli Stati Uniti misurata dalle ricerche web di Google. Fonte: Google.\n\n\n\nUn approccio unico non funziona bene in tutto lo spettro, dal cloud ai piccoli dispositivi edge. Diversi framework rappresentano varie filosofie sull’esecuzione di grafici, API dichiarative rispetto a quelle imperative e altro ancora. Le dichiarative definiscono cosa dovrebbe fare il programma, mentre le imperative si concentrano su come dovrebbe essere fatto passo dopo passo. Ad esempio, TensorFlow utilizza l’esecuzione di grafici e la modellazione in stile dichiarativo, mentre PyTorch adotta l’esecuzione rapida e la modellazione imperativa per una maggiore flessibilità con Python. Ogni approccio comporta dei compromessi che discuteremo in Sezione 6.3.7.\nGli attuali framework avanzati consentono ai professionisti di sviluppare e distribuire modelli sempre più complessi, un fattore chiave dell’innovazione nel campo dell’intelligenza artificiale. Questi framework continuano a evolversi ed espandere le loro capacità per la prossima generazione di machine learning. Per capire come questi sistemi continuano a evolversi, approfondiremo TensorFlow come esempio di come il framework sia cresciuto in complessità nel tempo.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#sec-deep_dive_into_tensorflow",
    "href": "contents/frameworks/frameworks.it.html#sec-deep_dive_into_tensorflow",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.3 Approfondimento su TensorFlow",
    "text": "6.3 Approfondimento su TensorFlow\nTensorFlow è stato sviluppato dal team di Google Brain ed è stato rilasciato come libreria software open source il 9 novembre 2015. È stato progettato per il calcolo numerico utilizzando grafici di flusso di dati e da allora è diventato popolare per un’ampia gamma di applicazioni di apprendimento automatico e deep learning.\nTensorFlow è un framework di training e inferenza che fornisce funzionalità integrate per gestire tutto, dalla creazione e training del modello alla distribuzione, come mostrato in Figura 6.2. Sin dal suo sviluppo iniziale, l’ecosistema TensorFlow è cresciuto fino a includere molte diverse “varietà” di TensorFlow, ciascuna pensata per consentire agli utenti di supportare ML su diverse piattaforme. In questa sezione, discuteremo principalmente solo del pacchetto core.\n\n6.3.1 Ecosistema TF\n\nTensorFlow Core: pacchetto principale con cui interagiscono la maggior parte degli sviluppatori. Fornisce una piattaforma completa e flessibile per definire, addestrare e distribuire modelli di apprendimento automatico. Include tf.keras come API di alto livello.\nTensorFlow Lite: progettato per distribuire modelli leggeri su dispositivi mobili, embedded ed edge. Offre strumenti per convertire i modelli TensorFlow in un formato più compatto adatto a dispositivi con risorse limitate e fornisce modelli pre-addestrati ottimizzati per dispositivi mobili.\nTensorFlow Lite Micro: progettato per eseguire modelli di apprendimento automatico su microcontrollori con risorse minime. Funziona senza la necessità di supporto del sistema operativo, librerie C o C++ standard o allocazione dinamica della memoria, utilizzando solo pochi kilobyte di memoria.\nTensorFlow.js: libreria JavaScript che consente l’addestramento e la distribuzione di modelli di apprendimento automatico direttamente nel browser o su Node.js. Fornisce inoltre strumenti per il porting di modelli TensorFlow pre-addestrati nel formato browser-friendly.\nTensorFlow su dispositivi edge (Coral): piattaforma di componenti hardware e strumenti software di Google che consente l’esecuzione di modelli TensorFlow su dispositivi edge, sfruttando Edge TPU per l’accelerazione.\nTensorFlow Federated (TFF): framework per l’apprendimento automatico e altri calcoli su dati decentralizzati. TFF facilita l’apprendimento “federato”, consentendo l’addestramento del modello su molti dispositivi senza centralizzare i dati.\nTensorFlow Graphics: libreria per l’utilizzo di TensorFlow per svolgere attività correlate alla grafica, tra cui l’elaborazione di forme 3D e nuvole di punti, utilizzando il deep learning.\nTensorFlow Hub: repository di componenti di modelli di apprendimento automatico riutilizzabili che consente agli sviluppatori di riutilizzare componenti di modelli pre-addestrati, facilitando l’apprendimento per trasferimento e la composizione del modello.\nTensorFlow Serving: framework progettato per servire e distribuire modelli di apprendimento automatico per l’inferenza in ambienti di produzione. Fornisce strumenti per il versioning e l’aggiornamento dinamico dei modelli distribuiti senza interruzione del servizio.\nTensorFlow Extended (TFX): piattaforma end-to-end progettata per distribuire e gestire pipeline di apprendimento automatico in ambienti di produzione. TFX comprende validazione dei dati, pre-elaborazione, addestramento del modello, convalida e componenti di servizio.\n\n\n\n\n\n\n\nFigura 6.2: Panoramica dell’architettura di TensorFlow 2.0. Fonte: Tensorflow.\n\n\n\nTensorFlow è stato sviluppato per affrontare le limitazioni di DistBelief (Yu et al. 2018)—il framework in uso presso Google dal 2011 al 2015—offrendo flessibilità lungo tre direttrici: 1) definizione di nuovi livelli [livelli], 2) perfezionamento degli algoritmi di training e 3) definizione di nuovi algoritmi di training. Per comprendere quali limitazioni di DistBelief hanno portato allo sviluppo di TensorFlow, faremo prima una breve panoramica dell’architettura del server dei parametri utilizzata da DistBelief (Dean et al. 2012).\n\nYu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, et al. 2018. «Dynamic control flow in large-scale machine learning». In Proceedings of the Thirteenth EuroSys Conference, 265–83. ACM. https://doi.org/10.1145/3190508.3190551.\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. «Large Scale Distributed Deep Networks». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 1232–40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\nL’architettura Parameter Server (PS) è un design popolare per distribuire il training di modelli di apprendimento automatico, in particolare reti neurali profonde, su più macchine. L’idea fondamentale è di separare l’archiviazione e la gestione dei parametri del modello dal calcolo utilizzato per aggiornare tali parametri. In genere, i server dei parametri gestiscono l’archiviazione e la gestione dei parametri del modello, suddividendoli su più server. I processi worker eseguono le attività di calcolo, tra cui l’elaborazione dei dati e il calcolo dei gradienti, che vengono poi inviati ai server dei parametri per l’aggiornamento.\nStorage: I processi del server dei parametri stateful [con stato] gestivano l’archiviazione e la gestione dei parametri del modello. Data l’ampia scala dei modelli e la natura distribuita del sistema, questi parametri erano condivisi tra più server dei parametri. Ogni server manteneva una parte dei parametri del modello, rendendolo \"stateful\" poiché doveva mantenere e gestire questo stato durante il processo di training.\nComputation: I processi worker, che potevano essere eseguiti in parallelo, erano senza stato e puramente computazionali. Elaboravano dati e calcolavano gradienti senza mantenere alcuno stato o memoria a lungo termine (M. Li et al. 2014). I worker non conservavano informazioni tra le diverse attività. Invece, comunicavano periodicamente con i server dei parametri per recuperare i parametri più recenti e restituire i gradienti calcolati.\n\nLi, Mu, David G. Andersen, Alexander J. Smola, e Kai Yu. 2014. «Communication Efficient Distributed Machine Learning with the Parameter Server». In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, a cura di Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, e Kilian Q. Weinberger, 19–27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\n\n\n\n\nEsercizio 6.1: TensorFlow Core\n\n\n\n\n\nAndiamo a comprendere in modo completo gli algoritmi di apprendimento automatico di base utilizzando TensorFlow e le loro applicazioni pratiche nell’analisi dei dati e nella modellazione predittiva. Inizieremo con la regressione lineare per prevedere i tassi di sopravvivenza dal set di dati del Titanic. Poi, utilizzando TensorFlow, costruiremo classificatori per identificare diverse specie di fiori in base ai loro attributi. Successivamente, utilizzeremo l’algoritmo K-Means e la sua applicazione nella segmentazione dei set di dati in cluster coesi. Infine, applicheremo modelli hidden [nascosti] di Markov (HMM) per prevedere i modelli meteorologici.\n\n\n\n\n\n\n\n\n\n\nEsercizio 6.2: TensorFlow Lite\n\n\n\n\n\nQui vedremo come costruire un modello di apprendimento automatico in miniatura per microcontrollori. Costruiremo una mini rete neurale semplificata per apprendere dai dati anche con risorse limitate e ottimizzata per l’implementazione riducendo il nostro modello per un uso efficiente sui microcontrollori. TensorFlow Lite, una potente tecnologia derivata da TensorFlow, riduce i modelli per dispositivi minuscoli e aiuta ad abilitare funzionalità sul dispositivo come il riconoscimento delle immagini nei dispositivi smart [intelligenti]. Viene utilizzato nell’edge computing per consentire analisi e decisioni più rapide nei dispositivi che elaborano i dati localmente.\n\n\n\n\nDistBelief e la sua architettura definita sopra sono stati fondamentali per abilitare il deep learning distribuito in Google, ma hanno anche introdotto delle limitazioni che hanno motivato lo sviluppo di TensorFlow:\n\n\n6.3.2 Grafico di Calcolo Statico\nI parametri del modello sono distribuiti su vari server di parametri nell’architettura del server di parametri. Poiché DistBelief è stato progettato principalmente per il paradigma della rete neurale, i parametri corrispondevano a una struttura di rete neurale fissa. Se il computation graph [grafico di calcolo] fosse dinamico, la distribuzione e il coordinamento dei parametri diventerebbero significativamente più complicati. Ad esempio, una modifica nel grafico potrebbe richiedere l’inizializzazione di nuovi parametri o la rimozione di quelli esistenti, complicando le attività di gestione e sincronizzazione dei server di parametri. Ciò ha reso più difficile implementare modelli al di fuori del framework neurale o modelli che richiedevano grafici di calcolo dinamici.\nTensorFlow è stato progettato come un framework di calcolo più generale che esprime il calcolo come un grafico del flusso di dati. Ciò consente una più ampia varietà di modelli e algoritmi di apprendimento automatico al di fuori delle reti neurali e fornisce flessibilità nel perfezionamento dei modelli.\n\n\n6.3.3 Usabilità & Distribuzione\nIl modello del server dei parametri delinea i ruoli (nodi worker e server dei parametri) ed è ottimizzato per i deployment [distribuzioni] dei data center, che potrebbero essere ottimali solo per alcuni casi d’uso. Ad esempio, questa divisione introduce overhead o complessità sui dispositivi edge o in altri ambienti non data center.\nTensorFlow è stato creato per funzionare su più piattaforme, dai dispositivi mobili e edge all’infrastruttura cloud. Mirava anche a essere più leggero e intuitivo per gli sviluppatori e a fornire facilità d’uso tra il training locale e quello distribuito.\n\n\n6.3.4 Progettazione dell’Architettura\nInvece di utilizzare l’architettura del server dei parametri, TensorFlow distribuisce i task [attività] su un cluster. Queste attività sono processi denominati che possono comunicare su una rete e ciascuna può eseguire la struttura principale di TensorFlow, il grafico del flusso di dati e l’interfaccia con vari dispositivi di elaborazione (come CPU o GPU). Questo grafico [grafo] è una rappresentazione diretta in cui i nodi simboleggiano le operazioni di elaborazione e gli edge rappresentano i tensori (dati) che scorrono tra queste operazioni.\nNonostante l’assenza di server di parametri tradizionali, alcuni “task PS” memorizzano e gestiscono parametri che ricordano i server di parametri di altri sistemi. I task rimanenti, che di solito gestiscono calcoli, elaborazione dati e gradienti, sono denominati “task worker”. I task PS di TensorFlow possono eseguire qualsiasi calcolo rappresentabile dal grafico del flusso di dati, il che significa che non sono limitati solo all’archiviazione dei parametri e il calcolo può essere distribuito. Questa capacità li rende significativamente più versatili e offre agli utenti il potere di programmare i task PS utilizzando l’interfaccia TensorFlow standard, la stessa che userebbero per definire i loro modelli. Come accennato in precedenza, la struttura dei grafici del flusso di dati li rende anche intrinsecamente buoni per il parallelismo, consentendo l’elaborazione di grandi set di dati.\n\n\n6.3.5 Funzionalità Native & Keras\nTensorFlow include librerie per aiutare gli utenti a sviluppare e distribuire più modelli specifici per i casi d’uso e, poiché questo framework è open source, questo elenco continua a crescere. Queste librerie affrontano l’intero ciclo di vita dello sviluppo ML: preparazione dei dati, creazione di modelli, distribuzione e IA responsabile.\nUno dei maggiori vantaggi di TensorFlow è la sua integrazione con Keras, anche se, come vedremo nella prossima sezione, Pytorch ha recentemente aggiunto un’integrazione Keras. Keras è un altro framework ML creato per essere estremamente intuitivo e, di conseguenza, ha un alto livello di astrazione. Parleremo di Keras più approfonditamente più avanti in questo capitolo. Tuttavia, quando si discute della sua integrazione con TensorFlow, è importante notare che era stato originariamente creato per essere indipendente dal backend. Ciò significa che gli utenti potrebbero astrarre queste complessità, offrendo un modo più pulito e intuitivo per definire e addestrare modelli senza preoccuparsi di problemi di compatibilità con diversi backend. Gli utenti di TensorFlow hanno evidenziato alcuni problemi sull’usabilità e la leggibilità dell’API di TensorFlow, quindi, man mano che TF acquisiva importanza, ha integrato Keras come API di alto livello. Questa integrazione ha offerto grandi vantaggi agli utenti di TensorFlow poiché ha introdotto una leggibilità e una portabilità più intuitive dei modelli, sfruttando comunque le potenti funzionalità di backend, il supporto di Google e l’infrastruttura per distribuire i modelli su varie piattaforme.\n\n\n\n\n\n\nEsercizio 6.3: Esplorazione di Keras: Creazione, Addestramento e Valutazione di Reti Neurali\n\n\n\n\n\nQui, impareremo come utilizzare Keras, un’API di reti neurali di alto livello, per lo sviluppo e l’addestramento (training) di modelli. Esploreremo l’API funzionale per la creazione di modelli concisi, comprenderemo le classi “loss” e metriche per la valutazione dei modelli e utilizzeremo gli ottimizzatori nativi per aggiornare i parametri del modello durante l’addestramento. Inoltre, scopriremo come definire layer e metriche personalizzati su misura per le nostre esigenze. Infine, esamineremo i cicli di addestramento di Keras per semplificare il processo di addestramento delle reti neurali su grandi set di dati. Questa conoscenza ci consentirà di costruire e ottimizzare modelli di reti neurali in varie applicazioni di machine learning e intelligenza artificiale.\n\n\n\n\n\n\n6.3.6 Limitazioni e Sfide\nTensorFlow è uno dei framework di deep learning più popolari, ma ha dovuto affrontare critiche e debolezze, principalmente legate all’usabilità e all’utilizzo delle risorse. Sebbene vantaggioso, il ritmo rapido degli aggiornamenti tramite il supporto di Google ha talvolta portato a problemi di retrocompatibilità, funzioni deprecate e documentazione instabile. Inoltre, anche con l’implementazione di Keras, la sintassi e la curva di apprendimento di TensorFlow possono risultare difficili per i nuovi utenti. Un’altra critica importante di TensorFlow è il suo elevato overhead e consumo di memoria dovuto alla gamma di librerie integrate e al supporto. Sebbene le versioni ridotte possano risolvere alcuni di questi problemi, potrebbero comunque essere limitate in ambienti con risorse limitate.\n\n\n6.3.7 PyTorch & TensorFlow\nPyTorch e TensorFlow si sono affermati come leader nel settore. Entrambi i framework offrono funzionalità robuste ma differiscono per filosofie di progettazione, facilità d’uso, ecosistema e capacità di distribuzione.\nFilosofia di Progettazione e Paradigma di Programmazione: PyTorch utilizza un grafo computazionale dinamico denominato eager execution [esecuzione rapida]. Ciò lo rende intuitivo e facilita il debug poiché le operazioni vengono eseguite immediatamente e possono essere ispezionate al volo. Al contrario, le versioni precedenti di TensorFlow erano incentrate su un grafo computazionale statico, che richiedeva la definizione completa del grafico prima dell’esecuzione. Tuttavia, TensorFlow 2.0 ha introdotto la “eager execution” per default, rendendolo più allineato con PyTorch. La natura dinamica di PyTorch e l’approccio basato su Python hanno consentito la sua semplicità e flessibilità, in particolare per la prototipazione rapida. L’approccio grafico statico di TensorFlow nelle sue versioni precedenti aveva una curva di apprendimento più ripida; l’introduzione di TensorFlow 2.0, con la sua integrazione Keras come API di alto livello, ha semplificato notevolmente il processo di sviluppo.\nDeployment: PyTorch è fortemente favorito negli ambienti di ricerca, ma la distribuzione dei modelli PyTorch in contesti di produzione è sempre stata un problema. Tuttavia, la distribuzione è diventata più fattibile con l’introduzione di TorchScript, lo strumento TorchServe e PyTorch Mobile. TensorFlow si distingue per la sua forte scalabilità e capacità di distribuzione, in particolare su piattaforme embedded e mobili con TensorFlow Lite. TensorFlow Serving e TensorFlow.js facilitano ulteriormente la distribuzione in vari ambienti, conferendogli così una portata più ampia nell’ecosistema.\nPrestazioni: Entrambi i framework offrono un’accelerazione hardware efficiente per le loro operazioni. Tuttavia, TensorFlow ha un flusso di lavoro di ottimizzazione leggermente più robusto, come il compilatore XLA (Accelerated Linear Algebra), che può aumentare ulteriormente le prestazioni. Il suo grafo computazionale statico era anche vantaggioso per alcune ottimizzazioni nelle prime versioni.\nEcosistema: PyTorch ha un ecosistema in crescita con strumenti come TorchServe per servire modelli e librerie come TorchVision, TorchText e TorchAudio per domini specifici. Come abbiamo detto prima, TensorFlow ha un ecosistema ampio e maturo. TensorFlow Extended (TFX) fornisce una piattaforma end-to-end per distribuire pipeline di apprendimento automatico di produzione. Altri strumenti e librerie includono TensorFlow Lite, TensorFlow Lite Micro, TensorFlow.js, TensorFlow Hub e TensorFlow Serving.\nTabella 6.1 fornisce un’analisi comparativa:\n\n\n\nTabella 6.1: Confronto tra PyTorch e TensorFlow.\n\n\n\n\n\n\n\n\n\n\nAspetto\nPytorch\nTensorFlow\n\n\n\n\nFilosofia di Progettazione\nGrafo computazionale dinamico (eager execution)\nGrafo computazionale statico (prime versioni); Esecuzione rapida in TensorFlow 2.0\n\n\nDeployment\nTradizionalmente impegnativa; Migliorata con TorchScript e TorchServe\nScalabile, specialmente su piattaforme embedded con TensorFlow Lite\n\n\nPrestazioni e Ottimizzazione\nAccelerazione GPU efficiente\nOttimizzazione robusta con compilatore XLA\n\n\nEcosistema\nTorchServe, TorchVision, TorchText, TorchAudio, PyTorch Mobile\nTensorFlow Extended (TFX), TensorFlow Lite, TensorFlow Lite Micro TensorFlow.js, TensorFlow Hub, TensorFlow Serving\n\n\nFacilità d’uso\nPreferito per il suo approccio Pythonic e la prototipazione rapida\nCurva di apprendimento inizialmente ripida; Semplificato con Keras in TensorFlow 2.0",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#componenti-di-base-del-framework",
    "href": "contents/frameworks/frameworks.it.html#componenti-di-base-del-framework",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.4 Componenti di Base del Framework",
    "text": "6.4 Componenti di Base del Framework\nDopo aver introdotto i popolari framework di machine learning e aver fornito un confronto di alto livello, questa sezione presenterà le funzionalità principali che formano la struttura di questi framework. Tratterà la struttura speciale chiamata tensori, che questi framework utilizzano per gestire più facilmente dati multidimensionali complessi. Si vedrà anche come questi framework rappresentano diversi tipi di architetture di reti neurali e le loro operazioni richieste tramite grafi computazionali. Inoltre, si vedrà come offrono strumenti che rendono lo sviluppo di modelli di machine learning più astratto ed efficiente, come caricatori di dati, algoritmi di ottimizzazione delle perdite implementate, tecniche di differenziazione efficienti e la capacità di accelerare il processo di training su acceleratori hardware.\n\n6.4.1 Strutture Dati Tensoriali\nPer comprendere i tensori, partiamo dai concetti familiari dell’algebra lineare. Come mostrato in Figura 6.4, i vettori possono essere rappresentati come una pila di numeri in un array unidimensionale. Le matrici seguono la stessa idea e si possono pensare a loro come a molti vettori impilati l’uno sull’altro, rendendoli bidimensionali. I tensori di dimensioni superiori funzionano allo stesso modo. Un tensore tridimensionale è semplicemente un insieme di matrici impilate l’una sull’altra in una direzione aggiuntiva. Pertanto, vettori e matrici possono essere considerati casi speciali di tensori con dimensioni 1D e 2D, rispettivamente.\n\n\n\n\n\n\nFigura 6.3: Visualizzazione della Struttura Dati del Tensore.\n\n\n\nI tensori offrono una struttura flessibile che può rappresentare dati in dimensioni superiori. Ad esempio, per rappresentare i dati di un’immagine, i pixel in ogni posizione di un’immagine sono strutturati come matrici. Tuttavia, le immagini non sono rappresentate da una sola matrice di valori di pixel; in genere hanno tre canali in cui ogni canale è una matrice contenente valori di pixel che rappresentano l’intensità di rosso, verde o blu. Insieme, questi canali creano un’immagine colorata. Senza i tensori, archiviare tutte queste informazioni da più matrici può risultare complesso. Con i tensori, è facile contenere i dati dell’immagine in un singolo tensore tridimensionale, con ogni numero che rappresenta un certo valore di colore in una posizione specifica nell’immagine.\n\n\n\n\n\n\nFigura 6.4: Visualizzazione della struttura dell’immagine colorata che può essere facilmente memorizzata come un Tensore 3D. Credito: Niklas Lang\n\n\n\nNon finisce qui. Se volessimo archiviare una serie di immagini, potremmo usare un tensore quadridimensionale, in cui la nuova dimensione rappresenta immagini diverse. Ciò significa che si stanno archiviando più immagini, ciascuna con tre matrici che rappresentano i tre canali del colore. Questo dà un’idea dell’utilità dei tensori quando si gestiscono dati multidimensionali in modo efficiente.\nI tensori hanno anche un attributo unico che consente ai framework di calcolare automaticamente i gradienti, semplificando l’implementazione di modelli complessi e algoritmi di ottimizzazione. Nel machine learning, come discusso nel Capitolo 3, la backpropagation richiede di prendere la derivata delle equazioni. Una delle caratteristiche principali dei tensori in PyTorch e TensorFlow è la loro capacità di tracciare i calcoli e calcolare i gradienti. Ciò è fondamentale per la backpropagation nelle reti neurali. Ad esempio, in PyTorch, si può usare l’attributo requires_grad, che consente di calcolare e memorizzare automaticamente i gradienti durante il “backward pass”, facilitando il processo di ottimizzazione. Analogamente, in TensorFlow, tf.GradientTape registra le operazioni per la differenziazione automatica.\nSi consideri questa semplice equazione matematica che si vuole differenziare. Matematicamente, il calcolo del gradiente si effettua nel modo seguente:\nDato: \\[\ny = x^2\n\\]\nLa derivata di \\(y\\) rispetto a \\(x\\) è: \\[\n\\frac{dy}{dx} = 2x\n\\]\nQuando \\(x = 2\\): \\[\n\\frac{dy}{dx} = 2*2 = 4\n\\]\nIl gradiente di \\(y\\) rispetto a \\(x\\), con \\(x = 2\\), è 4.\nUna potente caratteristica dei tensori in PyTorch e TensorFlow è la loro capacità di calcolare facilmente le derivate (gradienti). Ecco gli esempi di codice corrispondenti in PyTorch e TensorFlow:\n\nPyTorchTensorFlow\n\n\nimport torch\n\n# Create a tensor with gradient tracking\nx = torch.tensor(2.0, requires_grad=True)\n\n# Define a simple function\ny = x ** 2\n\n# Compute the gradient\ny.backward()\n\n# Print the gradient\nprint(x.grad)\n\n# Output\ntensor(4.0)\n\n\nimport tensorflow as tf\n\n# Create a tensor with gradient tracking\nx = tf.Variable(2.0)\n\n# Define a simple function\nwith tf.GradientTape() as tape:\n    y = x ** 2\n\n# Compute the gradient\ngrad = tape.gradient(y, x)\n\n# Print the gradient\nprint(grad)\n\n# Output\ntf.Tensor(4.0, shape=(), dtype=float32)\n\n\n\nQuesta differenziazione automatica è una potente funzionalità dei tensori in framework come PyTorch e TensorFlow, che semplifica l’implementazione e l’ottimizzazione di modelli complessi di apprendimento automatico.\n\n\n6.4.2 Grafi computazionali\n\nDefinizione di Grafico\nI grafi computazionali sono una componente chiave di framework di deep learning come TensorFlow e PyTorch. Ci consentono di esprimere architetture di reti neurali complesse in modo efficiente e differenziato. Un grafo computazionale è costituito da un grafo aciclico diretto (directed acyclic graph, DAG) in cui ogni nodo rappresenta un’operazione o una variabile e gli spigoli rappresentano le dipendenze dei dati tra di essi.\nÈ importante differenziare i grafi computazionali dai diagrammi di reti neurali, come quelli per i perceptron multistrato (multilayer perceptrons, MLP), che rappresentano nodi e layer. I diagrammi di reti neurali, come illustrato nel Capitolo 3, visualizzano l’architettura e il flusso di dati attraverso nodi e layer, fornendo una comprensione intuitiva della struttura del modello. Al contrario, i grafi computazionali forniscono una rappresentazione di basso livello delle operazioni matematiche sottostanti e delle dipendenze dei dati necessarie per implementare e addestrare queste reti.\nAd esempio, un nodo potrebbe rappresentare un’operazione di moltiplicazione di matrici, prendendo due matrici di input (o tensori) e producendo una matrice di output (o tensore). Per visualizzarlo, si consideri il semplice esempio in Figura 7.3. Il grafo aciclico orientato sopra calcola \\(z = x \\times y\\), dove ogni variabile è composta solo da numeri.\n\n\n\n\n\n\nFigura 6.5: Esempio elementare di un grafo computazionale.\n\n\n\nFramework come TensorFlow e PyTorch creano grafi computazionali per implementare le architetture delle reti neurali che in genere rappresentiamo con diagrammi. Quando si definisce un layer di rete neurale nel codice (ad esempio, un “layer denso” in TensorFlow), il framework costruisce un grafo computazionale che include tutte le operazioni necessarie (come moltiplicazione di matrici, addizione e funzioni di attivazione) e le relative dipendenze dai dati. Questo grafo consente al framework di gestire in modo efficiente il flusso di dati, ottimizzare l’esecuzione delle operazioni e calcolare automaticamente i gradienti per l’addestramento. Internamente, i grafi computazionali rappresentano astrazioni per layer comuni come quelli convoluzionali, di pooling, ricorrenti e densi, con dati che includono attivazioni, pesi e bias rappresentati in tensori. Questa rappresentazione consente un calcolo efficiente, sfruttando la struttura del grafico per parallelizzare le operazioni e applicare ottimizzazioni.\nAlcuni livelli comuni che i grafi computazionali potrebbero implementare includono layer convoluzionali, di attenzione, ricorrenti e densi. I layer fungono da astrazioni di livello superiore che definiscono calcoli specifici in cima alle operazioni di base rappresentate nel grafo. Ad esempio, un layer Denso esegue la moltiplicazione e l’addizione di matrici tra tensori di input, peso e bias. È importante notare che un layer opera su tensori come input e output; il layer stesso non è un tensore. Alcune differenze chiave tra layer e tensori sono:\n\nI layer contengono stati come pesi e bias. I tensori sono senza stato, contengono solo dati.\nI layer possono modificare lo stato interno durante l’addestramento. I tensori sono immutabili/di sola lettura.\nI layer sono astrazioni di livello superiore. I tensori sono a un livello inferiore e rappresentano direttamente dati e operazioni matematiche.\nI layer definiscono modelli di calcolo fissi. I tensori scorrono tra i livelli durante l’esecuzione.\nI layer vengono utilizzati indirettamente durante la creazione di modelli. I tensori scorrono tra i livelli durante l’esecuzione.\n\nQuindi, mentre i tensori sono una struttura dati fondamentale che i layer consumano e producono, i layer hanno funzionalità aggiuntive per definire operazioni parametrizzate e addestramento. Mentre un layer configura le operazioni tensoriali in background, il layer rimane distinto dagli oggetti tensoriali. L’astrazione del layer rende la creazione e l’addestramento di reti neurali molto più intuitive. Questa astrazione consente agli sviluppatori di creare modelli impilando insieme questi layer senza implementare la logica del layer. Ad esempio, la chiamata di tf.keras.layers.Conv2D in TensorFlow crea un layer convoluzionale. Il framework gestisce il calcolo delle convoluzioni, la gestione dei parametri, ecc. Ciò semplifica lo sviluppo del modello, consentendo agli sviluppatori di concentrarsi sull’architettura anziché sulle implementazioni di basso livello. Le astrazioni dei layer utilizzano implementazioni altamente ottimizzate per le prestazioni. Consentono inoltre la portabilità, poiché la stessa architettura può essere eseguita su backend hardware diversi come GPU e TPU.\nInoltre, i grafi computazionali includono funzioni di attivazione come ReLU, sigmoide e tanh che sono essenziali per le reti neurali e molti framework le forniscono come astrazioni standard. Queste funzioni introducono non linearità che consentono ai modelli di approssimare funzioni complesse. I framework le forniscono come operazioni semplici e predefinite che possono essere utilizzate durante la costruzione di modelli, ad esempio if.nn.relu in TensorFlow. Questa astrazione consente flessibilità, poiché gli sviluppatori possono facilmente scambiare le funzioni di attivazione per ottimizzare le prestazioni. Le attivazioni predefinite sono inoltre ottimizzate dal framework per un’esecuzione più rapida.\nNegli ultimi anni, modelli come ResNets e MobileNets sono emersi come architetture popolari, con i framework attuali che li pre-confezionano come grafi computazionali. Invece di preoccuparsi dei dettagli, gli sviluppatori possono utilizzarli come punto di partenza, personalizzandoli secondo necessità sostituendo i layer. Ciò semplifica e velocizza lo sviluppo del modello, evitando di reinventare le architetture da zero. I modelli predefiniti includono implementazioni ben collaudate e ottimizzate che garantiscono buone prestazioni. Il loro design modulare consente inoltre di trasferire le funzionalità apprese a nuove attività tramite apprendimento tramite trasferimento. Queste architetture predefinite forniscono i mattoni ad alte prestazioni per creare rapidamente modelli robusti.\nQueste astrazioni di layer, funzioni di attivazione e architetture predefinite fornite dai framework costituiscono un grafo computazionale. Quando un utente definisce un layer in un framework (ad esempio, tf.keras.layers.Dense()), il framework configura nodi e bordi del grafo computazionale per rappresentare tale layer. I parametri del layer come pesi e bias diventano variabili nel grafo. I calcoli del layer diventano nodi operativi (come x e y nella figura sopra). Quando si chiama una funzione di attivazione come tf.nn.relu(), il framework aggiunge un nodo operativo ReLU al grafo. Le architetture predefinite sono solo sottografi preconfigurati che possono essere inseriti nel grafo del modello. Quindi, la definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i livelli, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafo.\nCostruiamo implicitamente un grafo computazionale quando definiamo un’architettura di rete neurale in un framework. Il framework utilizza questo grafo per determinare le operazioni da eseguire durante l’addestramento e l’inferenza. I grafi computazionali offrono diversi vantaggi rispetto al codice grezzo e questa è una delle funzionalità principali offerte da un buon framework di ML:\n\nRappresentazione esplicita del flusso di dati e delle operazioni\nCapacità di ottimizzare il grafo prima dell’esecuzione\nDifferenziazione automatica per il training\nAgnosticismo linguistico: il grafo può essere tradotto per essere eseguito su GPU, TPU, ecc.\nPortabilità: il grafo può essere serializzato, salvato e ripristinato in seguito\n\nI grafi computazionali sono i componenti fondamentali dei framework di ML. La definizione del modello tramite astrazioni di alto livello crea un grafo computazionale: i layer, le attivazioni e le architetture che utilizziamo diventano nodi e rami del grafico. I compilatori e gli ottimizzatori del framework operano su questo grafo per generare codice eseguibile. Le astrazioni forniscono un’API intuitiva per gli sviluppatori per la creazione di grafi computazionali. Sotto, ci sono ancora grafi! Quindi, anche se non si possono manipolare direttamente i grafi come utente del framework, consentono di eseguire ad alto livello e in modo efficiente le specifiche del modello. Le astrazioni semplificano la creazione del modello, mentre i grafi computazionali la rendono possibile.\n\n\nGrafi Statici vs. Dinamici\nI framework di deep learning hanno tradizionalmente seguito uno dei due approcci per esprimere grafi computazionali.\nGrafi statici (declare-then-execute): Con questo modello, l’intero grafo computazionale deve essere definito in anticipo prima di eseguirlo. Tutte le operazioni e le dipendenze dei dati devono essere specificate durante la fase di dichiarazione. TensorFlow originariamente seguiva questo approccio statico: i modelli venivano definiti in un contesto separato e poi veniva creata una sessione per eseguirli. Il vantaggio dei grafi statici è che consentono un’ottimizzazione più aggressiva poiché il framework può vedere il grafo completo. Tuttavia, tende anche a essere meno flessibile per la ricerca e l’interattività. Le modifiche al grafo richiedono la nuova dichiarazione del modello completo.\nPer esempio:\nx = tf.placeholder(tf.float32)\ny = tf.matmul(x, weights) + biases\nIn questo esempio, x è un segnaposto per i dati di input e y è il risultato di un’operazione di moltiplicazione di matrici seguita da un’addizione. Il modello è definito in questa fase di dichiarazione, in cui tutte le operazioni e le variabili devono essere specificate in anticipo.\nUna volta definito l’intero grafo, il framework lo compila e lo ottimizza. Ciò significa che i passaggi computazionali sono definitivamente “scolpiti” e il framework può applicare varie ottimizzazioni per migliorare l’efficienza e le prestazioni. Quando in seguito si esegue il grafo, si forniscono i tensori di input effettivi e le operazioni predefinite vengono eseguite nella sequenza ottimizzata.\nQuesto approccio è simile alla creazione di un progetto in cui ogni dettaglio è pianificato prima dell’inizio della costruzione. Sebbene ciò consenta potenti ottimizzazioni, significa anche che qualsiasi modifica al modello richiede la ridefinizione dell’intero grafo da zero.\nGrafi dinamici (define-by-run): A differenza della dichiarazione (di tutto) prima e dell’esecuzione poi, il grafo viene creato dinamicamente durante l’esecuzione. Non esiste una fase di dichiarazione separata: le operazioni vengono eseguite immediatamente come definite. Questo stile è imperativo e flessibile, facilitando la sperimentazione.\nPyTorch utilizza grafi dinamici, creandoli al volo mentre avviene l’esecuzione. Ad esempio, si consideri il seguente frammento di codice, in cui il grafo viene creato durante l’esecuzione:\nx = torch.randn(4,784)\ny = torch.matmul(x, weights) + biases\nL’esempio sopra non ha fasi separate di compilazione/build/esecuzione. Le operazioni definiscono ed eseguono immediatamente. Con i grafi dinamici, la definizione è intrecciata con l’esecuzione, fornendo un flusso di lavoro più intuitivo e interattivo. Tuttavia, lo svantaggio è che c’è meno potenziale di ottimizzazione poiché il framework vede solo il grafo mentre viene creato.\nDi recente, la distinzione si è offuscata poiché i framework adottano entrambe le modalità. TensorFlow 2.0 passa automaticamente alla modalità di grafo dinamico, consentendo agli utenti di lavorare con quelli statici quando necessario. La dichiarazione dinamica offre flessibilità e facilità d’uso, rendendo i framework più intuitivi, mentre i grafi statici forniscono vantaggi di ottimizzazione a scapito dell’interattività. Il framework ideale bilancia questi approcci. Tabella 6.2 confronta i pro e i contro dei grafi di esecuzione statici e dinamici:\n\n\n\nTabella 6.2: Confronto tra Grafi di Esecuzione Statici (Declare-then-execute) e Dinamici (Define-by-run), evidenziandone i rispettivi pro e contro.\n\n\n\n\n\n\n\n\n\n\nGrafo di esecuzione\nPro\nContro\n\n\n\n\nStatico (Declare-then-execute)\n\nAbilita le ottimizzazioni del grafo visualizzando il modello completo in anticipo\nPuò esportare e distribuire grafici congelati\nIl grafo è impacchettato indipendentemente dal codice\n\n\nMeno flessibile per la ricerca e l’iterazione\nLe modifiche richiedono la ricostruzione del grafo\nL’esecuzione ha fasi di compilazione ed esecuzione separate\n\n\n\nDinamico (Define-by-run)\n\nStile imperativo intuitivo come il codice Python\nAlterna la creazione del grafo con l’esecuzione\nFacile da modificare i grafi\nIl debug si adatta perfettamente al flusso di lavoro\n\n\nPiù difficile da ottimizzare senza un grafo completo\nPossibili rallentamenti dalla creazione del grafo durante l’esecuzione\nPuò richiedere più memoria\n\n\n\n\n\n\n\n\n\n\n6.4.3 Tool della Pipeline dei Dati\nI grafi computazionali possono essere validi solo quanto i dati da cui apprendono e su cui lavorano. Pertanto, alimentare i dati di training in modo efficiente è fondamentale per ottimizzare le prestazioni della “deep neural network” [rete neurale profonda], sebbene spesso venga trascurata come una delle funzionalità principali. Molti framework di IA moderni forniscono pipeline specializzate per acquisire, elaborare e aumentare i set di dati per il training del modello.\n\nLoader dei Dati\nAl centro di queste pipeline ci sono i “data loader”, che gestiscono esempi di training di lettura da fonti come file, database e storage di oggetti. I data loader facilitano il caricamento e la pre-elaborazione efficienti dei dati, cruciali per i modelli di deep learning. Ad esempio, la pipeline di caricamento dati tf.data di TensorFlow è progettata per gestire questo processo. A seconda dell’applicazione, i modelli di deep learning richiedono diversi formati di dati come file CSV o cartelle di immagini. Alcuni formati popolari includono:\n\nCSV, un formato versatile e semplice spesso utilizzato per dati tabellari.\nTFRecord: Formato proprietario di TensorFlow, ottimizzato per le prestazioni.\nParquet: Storage a colonne, che offre compressione e recupero dati efficienti.\nJPEG/PNG: Comunemente utilizzato per dati di immagini.\nWAV/MP3: Formati prevalenti per dati audio.\n\nEsempi di batch di data loader per sfruttare il supporto di vettorizzazione nell’hardware. Il “batching” si riferisce al raggruppamento di più dati per l’elaborazione simultanea, sfruttando le capacità di calcolo vettorizzate di hardware come le GPU. Sebbene le dimensioni tipiche dei batch siano comprese tra 32 e 512 esempi, la dimensione ottimale spesso dipende dall’ingombro di memoria dei dati e dai vincoli hardware specifici. I loader avanzati possono trasmettere in streaming set di dati virtualmente illimitati da dischi e archivi cloud. Trasmettono in streaming grandi dataset da dischi o reti anziché caricarli completamente in memoria, consentendo dimensioni illimitate.\nI data loader possono anche mescolare i dati tra “epoche” per la randomizzazione e le funzionalità di preelaborazione in parallelo con l’addestramento del modello per accelerarne il processo. Mescolare casualmente l’ordine degli esempi tra epoche di training riduce il bias e migliora la generalizzazione.\nI data loader supportano anche strategie di “caching” e “prefetching” per ottimizzare la distribuzione dei dati per un addestramento del modello rapido e fluido. Il caching [memorizzazione nella cache] dei batch preelaborati consente di riutilizzarli in modo efficiente durante più fasi di addestramento ed elimina l’elaborazione ridondante. Il prefetching, al contrario, comporta il precaricamento dei batch successivi, assicurando che il modello non resti mai inattivo in attesa di dati.\n\n\n\n6.4.4 Data Augmentation\nFramework di apprendimento automatico come TensorFlow e PyTorch forniscono strumenti per semplificare e snellire il processo di “data augmentation” [aumento dei dati], migliorando l’efficienza dell’espansione sintetica dei set di dati. Questi framework offrono funzionalità integrate per applicare trasformazioni casuali, come capovolgimento, ritaglio, rotazione, modifica del colore e aggiunta di rumore per le immagini. Per i dati audio, gli aumenti comuni comportano la miscelazione di clip con rumore di fondo o la modulazione di velocità, tono e volume.\nIntegrando gli strumenti di “augmentation” nella pipeline di dati, i framework consentono di applicare queste trasformazioni al volo durante ogni epoca di addestramento. Questo approccio incrementa la variazione nella distribuzione dei dati di addestramento, riducendo così l’overfitting e migliorando la generalizzazione del modello. L’uso di “data loader” performanti in combinazione con ampie capacità di “augmentation” consente ai professionisti di alimentare in modo efficiente set di dati massicci e vari alle reti neurali.\nQueste pipeline di dati “hands-off” rappresentano un miglioramento significativo in termini di usabilità e produttività. Consentono agli sviluppatori di concentrarsi maggiormente sull’architettura del modello e meno sulla manipolazione dei dati durante l’addestramento di modelli di deep learning.\n\n\n6.4.5 Funzioni Loss e Algoritmi di Ottimizzazione\nL’addestramento di una rete neurale è fondamentalmente un processo iterativo che cerca di minimizzare una funzione di loss [perdita]. L’obiettivo è di mettere a punto i pesi e i parametri del modello per produrre previsioni vicine alle vere etichette target. I framework di apprendimento automatico hanno notevolmente semplificato questo processo offrendo funzioni di loss [perdita] e algoritmi di ottimizzazione.\nI framework di apprendimento automatico forniscono funzioni di perdita implementate che sono necessarie per quantificare la differenza tra le previsioni del modello e i valori reali. Diversi set di dati richiedono una diversa funzione di perdita per funzionare correttamente, poiché tale funzione indica al computer l’“obiettivo” a cui mirare. Le funzioni di perdita comunemente utilizzate includono il “Mean Squared Error (MSE)” [errore quadratico medio] per le attività di regressione, la “Cross-Entropy Loss” per le attività di classificazione, e la Kullback-Leibler (KL) per i modelli probabilistici. Ad esempio, tf.keras.losses di TensorFlow contiene una serie di queste funzioni di perdita comunemente utilizzate.\nGli algoritmi di ottimizzazione vengono utilizzati per trovare in modo efficiente il set di parametri del modello che minimizzano la funzione di perdita, assicurando che il modello funzioni bene sui dati di training e si generalizzi a nuovi dati. I framework moderni sono dotati di implementazioni efficienti di diversi algoritmi di ottimizzazione, molti dei quali sono varianti della “discesa del gradiente” con metodi stocastici e tassi di apprendimento adattivo. Alcuni esempi di queste varianti sono Stochastic Gradient Descent, Adagrad, Adadelta e Adam. L’implementazione di tali varianti è fornita in tf.keras.optimizers. Ulteriori informazioni con esempi chiari sono disponibili nella sezione Training dell’IA.\n\n\n6.4.6 Supporto al Training del Modello\nÈ richiesta una fase di compilazione prima di addestrare un modello di rete neurale definito. Durante questa fase, l’architettura di alto livello della rete neurale viene trasformata in un formato eseguibile ottimizzato. Questo processo comprende diverse fasi. La prima fase consiste nel costruire il grafo computazionale, che rappresenta tutte le operazioni matematiche e il flusso di dati all’interno del modello. Ne abbiamo discusso in precedenza.\nDurante l’addestramento, l’attenzione è rivolta all’esecuzione del grafo computazionale. A ogni parametro all’interno del grafo, come pesi e bias, viene assegnato un valore iniziale. A seconda del metodo di inizializzazione scelto, questo valore potrebbe essere casuale o basato su una logica predefinita.\nIl passaggio critico successivo è l’allocazione della memoria. La memoria essenziale è riservata alle operazioni del modello sia su CPU che su GPU, garantendo un’elaborazione efficiente dei dati. Le operazioni del modello vengono poi mappate sulle risorse hardware disponibili, in particolare GPU o TPU, per accelerare l’elaborazione. Una volta completata la compilazione, il modello viene preparato per l’addestramento.\nIl processo di addestramento impiega vari strumenti per migliorare l’efficienza. L’elaborazione batch è comunemente utilizzata per massimizzare la produttività computazionale. Tecniche come la vettorizzazione consentono operazioni su interi array di dati anziché procedere elemento per elemento, il che aumenta la velocità. Ottimizzazioni come la “kernel fusion” (fare riferimento al capitolo Ottimizzazioni) amalgamano più operazioni in un’unica azione, riducendo al minimo il sovraccarico computazionale. Le operazioni possono anche essere segmentate in fasi, facilitando l’elaborazione simultanea di diversi mini-batch in varie parti.\nI framework eseguono costantemente il checkpoint dello stato, preservando le versioni intermedie del modello durante l’addestramento. Ciò garantisce che i progressi vengano recuperati in caso di interruzione e che l’addestramento possa essere ripreso dall’ultimo checkpoint. Inoltre, il sistema monitora attentamente le prestazioni del modello rispetto a un set di dati di convalida. Se il modello inizia a sovradimensionarsi (se le sue prestazioni sul set di convalida diminuiscono), l’addestramento viene automaticamente interrotto, conservando risorse computazionali e tempo.\nI framework di ML incorporano una combinazione di compilazione del modello, metodi di elaborazione batch avanzati e utilità come il checkpoint e l’arresto anticipato. Queste risorse gestiscono gli aspetti complessi delle prestazioni, consentendo ai professionisti di concentrarsi sullo sviluppo e l’addestramento del modello. Di conseguenza, gli sviluppatori sperimentano sia velocità che facilità quando utilizzano le capacità delle reti neurali.\n\n\n6.4.7 Validazione e Analisi\nDopo aver addestrato i modelli di deep learning, i framework forniscono utilità per valutare le prestazioni e ottenere informazioni sul funzionamento dei modelli. Questi strumenti consentono una sperimentazione e un debug disciplinati.\n\nMetriche di Valutazione\nI framework includono implementazioni di comuni metriche di valutazione per la convalida:\n\nAccuratezza: Frazione di previsioni corrette complessive. Sono ampiamente utilizzate per la classificazione.\nPrecisione: Delle previsioni positive, quante erano positive. Utile per set di dati sbilanciati.\nRichiamo: Dei positivi effettivi, quanti ne abbiamo previsti correttamente? Misura della Completezza.\nPunteggio F1: Media armonica di precisione e richiamo. Combina entrambe le metriche.\nAUC-ROC - Area sotto la curva ROC. Sono utilizzate per l’analisi della soglia di classificazione.\nMAP - Mean Average Precision. Valuta le previsioni classificate nel recupero/rilevamento.\nMatrice di Confusione: Matrice che mostra i veri positivi, i veri negativi, i falsi positivi e i falsi negativi. Fornisce una visione più dettagliata delle prestazioni di classificazione.\n\nQueste metriche quantificano le prestazioni del modello sui dati di convalida per il confronto.\n\n\nVisualizzazione\nGli strumenti di visualizzazione forniscono informazioni sui modelli:\n\nCurve di perdita: Tracciano la perdita di training e validazione nel tempo per individuare l’overfitting.\nLoss curves [Griglie di attivazione]: Illustrano le funzionalità apprese dai filtri convoluzionali.\nProjection [Proiezione]: Riduce la dimensionalità per una visualizzazione intuitiva.\nPrecision-recall curves [Curve di richiamo della precisione]: Valutano i compromessi di classificazione.\n\nStrumenti come TensorBoard per TensorFlow e TensorWatch per PyTorch consentono metriche e visualizzazioni in tempo reale durante il training.\n\n\n\n6.4.8 Programmazione differenziabile\nI metodi di addestramento per il machine learning come la backpropagation si basano sulla modifica della funzione di perdita rispetto alla modifica dei pesi (che essenzialmente è la definizione di derivata). Pertanto, la capacità di addestrare rapidamente ed efficientemente grandi modelli di machine learning si basa sulla capacità del computer di prendere derivate. Ciò rende la programmazione differenziabile uno degli elementi più importanti di un framework di apprendimento automatico.\nPossiamo utilizzare quattro metodi principali per far sì che i computer prendano derivate. Innanzitutto, possiamo calcolare manualmente le derivate a mano e inserirle nel computer. Questo diventerebbe rapidamente un incubo con molti layer di reti neurali se dovessimo calcolare manualmente tutte le derivate nei passaggi di backpropagation. Un altro metodo è la differenziazione simbolica utilizzando sistemi di computer algebrici come Mathematica, che può introdurre un layer di inefficienza, poiché è necessario un livello di astrazione per prendere le derivate. Le derivate numeriche, la pratica di approssimare i gradienti utilizzando metodi di differenze finite, soffrono di molti problemi, tra cui elevati costi computazionali e dimensioni della griglia più grandi, che portano a molti errori. Ciò porta alla differenziazione automatica, che sfrutta le funzioni primitive che i computer utilizzano per rappresentare le operazioni per ottenere una derivata esatta. Con la differenziazione automatica, la complessità computazionale del calcolo del gradiente è proporzionale al calcolo della funzione stessa. Le complessità della differenziazione automatica non sono gestite dagli utenti finali al momento, ma le risorse per saperne di più possono essere trovate ampiamente, ad esempio qui. La differenziazione automatica e la programmazione differenziabile di oggi sono onnipresenti e vengono eseguite in modo efficiente e automatico dai moderni framework di machine learning.\n\n\n6.4.9 Accelerazione Hardware\nLa tendenza a formare e distribuire continuamente modelli di apprendimento automatico più grandi ha reso necessario il supporto dell’accelerazione hardware per le piattaforme di machine-learning. Figura 6.6 mostra il gran numero di aziende che offrono acceleratori hardware in diversi domini, come il machine learning “Very Low Power” e quello “Embedded”. I “deep layer” delle reti neurali richiedono molte moltiplicazioni di matrici, che attraggono hardware in grado di calcolare rapidamente e in parallelo tali operazioni. In questo panorama, due architetture hardware, GPU e TPU, sono emerse come scelte principali per l’addestramento di modelli di apprendimento automatico.\nL’uso di acceleratori hardware è iniziato con AlexNet, che ha aperto la strada a lavori futuri per utilizzare le GPU come acceleratori hardware per l’addestramento di modelli di visione artificiale. Le GPU, o “Graphics Processing Units” [unità di elaborazione grafica], eccellono nella gestione di molti calcoli contemporaneamente, il che le rende ideali per le operazioni matriciali centrali per l’addestramento delle reti neurali. La loro architettura, progettata per il rendering della grafica, è perfetta per le operazioni matematiche richieste nell’apprendimento automatico. Sebbene siano molto utili per le attività di apprendimento automatico e siano state implementate in molte piattaforme hardware, le GPU sono comunque di uso generale in quanto possono essere utilizzate per altre applicazioni.\nD’altro canto, le Tensor Processing Units (TPU) sono unità hardware progettate specificamente per le reti neurali. Si concentrano sull’operazione di “moltiplicazione e accumulazione” (MAC) e il loro hardware è costituito da una grande matrice hardware che contiene elementi che calcolano in modo efficiente l’operazione MAC. Questo concetto, chiamato systolic array architecture, è stato ideato da Kung e Leiserson (1979), ma ha dimostrato di essere una struttura utile per calcolare in modo efficiente i prodotti matriciali e altre operazioni all’interno delle reti neurali (come le convoluzioni).\n\nKung, Hsiang Tsung, e Charles E Leiserson. 1979. «Systolic arrays (for VLSI)». In Sparse Matrix Proceedings 1978, 1:256–82. Society for industrial; applied mathematics Philadelphia, PA, USA.\nSebbene le TPU possano ridurre drasticamente i tempi di addestramento, presentano anche degli svantaggi. Ad esempio, molte operazioni all’interno dei framework di apprendimento automatico (principalmente TensorFlow in questo caso, poiché la TPU si integra direttamente con esso) non sono supportate dalle TPU. Non possono inoltre supportare operazioni personalizzate dai framework di apprendimento automatico e la progettazione della rete deve essere strettamente allineata alle capacità hardware.\nOggi, le GPU NVIDIA dominano il training, supportate da librerie software come CUDA, cuDNN e TensorRT. I framework includono anche ottimizzazioni per massimizzare le prestazioni su questi tipi di hardware, come l’eliminazione di connessioni non importanti e la fusione di layer. La combinazione di queste tecniche con l’accelerazione hardware fornisce una maggiore efficienza. Per l’inferenza, l’hardware si sta spostando sempre di più verso ASIC e SoC ottimizzati. Le TPU di Google accelerano i modelli nei data center, mentre Apple, Qualcomm, la famiglia NVIDIA Jetson e altri ora producono chip “mobili” incentrati sull’intelligenza artificiale.\n\n\n\n\n\n\nFigura 6.6: Aziende che offrono acceleratori hardware di ML. Fonte: Gradient Flow.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#sec-ai_frameworks-advanced",
    "href": "contents/frameworks/frameworks.it.html#sec-ai_frameworks-advanced",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.5 Funzionalità Avanzate",
    "text": "6.5 Funzionalità Avanzate\nOltre a fornire gli strumenti essenziali per il training di modelli di apprendimento automatico, i framework offrono anche funzionalità avanzate. Queste funzionalità includono la distribuzione del training su diverse piattaforme hardware, la facile messa a punto di grandi modelli pre-addestrati e l’esemplificazione del “federated learning”. L’implementazione di queste funzionalità in modo indipendente sarebbe altamente complessa e richiederebbe molte risorse, ma i framework semplificano questi processi, rendendo le tecniche avanzate di apprendimento automatico più accessibili.\n\n6.5.1 Training distribuito\nPoiché i modelli di apprendimento automatico sono diventati più grandi nel corso degli anni, è diventato essenziale per i modelli di grandi dimensioni utilizzare più nodi di elaborazione nel processo di training. Questo processo, l’apprendimento distribuito, ha consentito maggiori capacità di training, ma ha anche imposto problemi nell’implementazione.\nPossiamo considerare tre diversi modi per distribuire il lavoro di training dei modelli di apprendimento automatico su più nodi di elaborazione. Il partizionamento dei dati di input (o parallelismo dei dati) si riferisce a più processori che eseguono lo stesso modello su diverse partizioni di input. Questa è l’implementazione più semplice ed è disponibile per molti framework di machine learning. La distribuzione più impegnativa del lavoro è rappresentata dal parallelismo del modello, che si riferisce a più nodi di elaborazione che lavorano su parti diverse del modello, e dal parallelismo del modello pipelined, che si riferisce a più nodi di elaborazione che lavorano su diversi layer del modello sullo stesso input. Gli ultimi due menzionati qui sono aree di ricerca attive.\nI framework di ML che supportano l’apprendimento distribuito includono TensorFlow (tramite il suo modulo tf.distribute), PyTorch (tramite i suoi moduli torch.nn.DataParallel e torch.nn.DistributedDataParallel) e MXNet (tramite la sua API gluon).\n\n\n6.5.2 Conversione del Modello\nI modelli di machine learning hanno vari metodi per essere rappresentati e utilizzati in diversi framework e per diversi tipi di dispositivi. Ad esempio, un modello può essere convertito per essere compatibile con i framework di inferenza all’interno del dispositivo mobile. Il formato di default per i modelli TensorFlow sono i file di checkpoint contenenti pesi e architetture, necessari per riaddestrare i modelli. Tuttavia, i modelli vengono in genere convertiti nel formato TensorFlow Lite per la distribuzione mobile. TensorFlow Lite utilizza una rappresentazione compatta del “flat buffer” e ottimizzazioni per un’inferenza rapida su hardware mobile, eliminando tutto il bagaglio non necessario associato ai metadati di addestramento, come le strutture dei file di checkpoint.\nLe ottimizzazioni del modello come la quantizzazione (vedere il capitolo Ottimizzazioni) possono ottimizzare ulteriormente i modelli per architetture target come i dispositivi mobili. Ciò riduce la precisione di pesi e attivazioni a uint8 o a int8 per un ingombro ridotto e un’esecuzione più rapida con acceleratori hardware supportati. Per la quantizzazione post-training, il convertitore di TensorFlow gestisce automaticamente analisi e conversione.\nFramework come TensorFlow semplificano la distribuzione di modelli addestrati su dispositivi IoT mobili ed embedded tramite API di conversione semplici per il formato TFLite e la quantizzazione. La conversione pronta all’uso consente un’inferenza ad alte prestazioni su dispositivi mobili senza l’onere dell’ottimizzazione manuale. Oltre a TFLite, altri target comuni includono TensorFlow.js per la distribuzione Web, TensorFlow Serving per i servizi cloud e TensorFlow Hub per l’apprendimento tramite trasferimento. Le utility di conversione di TensorFlow gestiscono questi scenari per semplificare i flussi di lavoro end-to-end.\nUlteriori informazioni sulla conversione dei modelli in TensorFlow sono linkate qui.\n\n\n6.5.3 AutoML, No-Code/Low-Code ML\nIn molti casi, l’apprendimento automatico può avere una barriera d’ingresso relativamente alta rispetto ad altri campi. Per addestrare e distribuire con successo modelli, è necessario avere una comprensione critica di una varietà di discipline, dalla scienza dei dati (elaborazione dei dati, pulizia dei dati), strutture di modelli (ottimizzazione degli iperparametri, architettura delle reti neurali), hardware (accelerazione, elaborazione parallela) e altro a seconda del problema in questione. La complessità di questi problemi ha portato all’introduzione di framework come AutoML, che mira a rendere “il Machine learning disponibile per gli esperti e i non esperti di Machine learning” e ad “automatizzare la ricerca nel machine learning”. Hanno costruito AutoWEKA, che aiuta nel complesso processo di selezione degli iperparametri, e Auto-sklearn e Auto-pytorch, un’estensione di AutoWEKA nelle popolari Librerie sklearn e PyTorch.\nMentre questi sforzi per automatizzare parti delle attività di apprendimento automatico sono in corso, altri si sono concentrati sulla semplificazione dei modelli tramite l’implementazione di apprendimento automatico “no-code” [senza codice]/low-code [a basso codice], utilizzando un’interfaccia drag-and-drop con un’interfaccia utente di facile navigazione. Aziende come Apple, Google e Amazon hanno già creato queste piattaforme di facile utilizzo per consentire agli utenti di costruire modelli di apprendimento automatico che possono essere integrati nel loro ecosistema.\nQuesti passaggi per rimuovere le barriere all’ingresso continuano a democratizzare il machine learning, semplificano l’accesso per i principianti e semplificano il flusso di lavoro per gli esperti.\n\n\n6.5.4 Metodi di Apprendimento Avanzati\n\nIl Transfer Learning\nIl “transfer learning” è la pratica di utilizzare le conoscenze acquisite da un modello pre-addestrato per addestrare e migliorare le prestazioni di un modello per un’attività diversa. Ad esempio, modelli come MobileNet e ResNet vengono addestrati sul set di dati ImageNet. Per fare ciò, si può congelare il modello pre-addestrato, utilizzandolo come un estrattore di feature per addestrare un modello molto più piccolo costruito sopra l’estrazione di feature. Si può anche mettere a punto l’intero modello per adattarlo al nuovo compito. I framework di apprendimento automatico semplificano il caricamento di modelli pre-addestrati, il congelamento di layer specifici e l’addestramento di layer personalizzati in cima. Semplificano questo processo fornendo API intuitive e un facile accesso a grandi repository di modelli pre-addestrati.\nL’apprendimento tramite trasferimento presenta delle sfide, come l’incapacità del modello modificato di svolgere le sue attività originali dopo l’apprendimento tramite trasferimento. Articoli come “Learning without Forgetting” di Z. Li e Hoiem (2018) mirano ad affrontare queste sfide e sono stati implementati in moderne piattaforme di machine learning.\n\nLi, Zhizhong, e Derek Hoiem. 2018. «Learning without Forgetting». IEEE Trans. Pattern Anal. Mach. Intell. 40 (12): 2935–47. https://doi.org/10.1109/tpami.2017.2773081.\n\n\nIl Federated Learning\nIl “Federated learning” di McMahan et al. (2017) è una forma di elaborazione distribuita che prevede l’addestramento di modelli su dispositivi personali anziché la centralizzazione dei dati su un singolo server (Figura 6.7). Inizialmente, un modello globale di base viene addestrato su un server centrale per essere distribuito a tutti i dispositivi. Utilizzando questo modello di base, i dispositivi calcolano individualmente i gradienti e li inviano all’hub centrale. Intuitivamente, questo trasferisce i parametri del modello anziché i dati stessi. L’apprendimento federato migliora la privacy mantenendo i dati sensibili sui dispositivi locali e condividendo gli aggiornamenti del modello solo con un server centrale. Questo metodo è particolarmente utile quando si gestiscono dati sensibili o quando un’infrastruttura su larga scala non è praticabile.\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, e Blaise Agüera y Arcas. 2017. «Communication-Efficient Learning of Deep Networks from Decentralized Data». In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, a cura di Aarti Singh e Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\n\n\n\n\nFigura 6.7: Un approccio con server centralizzato al “federated learning”. Fonte: NVIDIA.\n\n\n\nTuttavia, il federated learning deve affrontare sfide come garantire l’accuratezza dei dati, gestire dati non-IID (independent and identically distributed) [indipendenti e distribuiti in modo identico], gestire la produzione di dati non bilanciata e superare il sovraccarico della comunicazione e l’eterogeneità dei dispositivi. Anche i problemi di privacy e sicurezza, come gli attacchi di inversione del gradiente, pongono sfide significative.\nI framework di apprendimento automatico semplificano l’implementazione dell’apprendimento federato fornendo gli strumenti e le librerie necessarie. Ad esempio, TensorFlow Federated (TFF) offre un framework open source per supportare l’apprendimento federato. TFF consente agli sviluppatori di simulare e implementare algoritmi di apprendimento federato, offrendo un core federato per operazioni di basso livello e API di alto livello per attività federate comuni. Si integra perfettamente con TensorFlow, consentendo l’uso di modelli e ottimizzatori TensorFlow in un ambiente federato. TFF supporta tecniche di aggregazione sicure per migliorare la privacy e consente la personalizzazione degli algoritmi di apprendimento federato. Sfruttando questi strumenti, gli sviluppatori possono distribuire in modo efficiente il training, perfezionare i modelli pre-addestrati e gestire le complessità intrinseche dell’apprendimento federato.\nSono stati sviluppati anche altri programmi open source come Flower per semplificare l’implementazione dell’apprendimento federato con vari framework di machine learning.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#specializzazione-del-framework",
    "href": "contents/frameworks/frameworks.it.html#specializzazione-del-framework",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.6 Specializzazione del Framework",
    "text": "6.6 Specializzazione del Framework\nFinora abbiamo parlato in generale dei framework di ML. Tuttavia, in genere, i framework sono ottimizzati in base alle capacità computazionali e ai requisiti applicativi dell’ambiente target, che vanno dal cloud all’edge ai dispositivi minuscoli. La scelta del framework giusto è fondamentale in base all’ambiente target per la distribuzione. Questa sezione fornisce una panoramica dei principali tipi di framework di IA su misura per ambienti cloud, edge e TinyML per aiutare a comprendere le somiglianze e le differenze tra questi ecosistemi.\n\n6.6.1 Cloud\nI framework di IA basati su cloud presuppongono l’accesso a un’ampia potenza di calcolo, memoria e risorse di archiviazione nel cloud. In genere supportano sia il training che l’inferenza. I framework di IA basati su cloud sono adatti per applicazioni in cui i dati possono essere inviati al cloud per l’elaborazione, come servizi di IA basati su cloud, analisi di dati su larga scala e applicazioni Web. I framework di IA cloud più diffusi includono quelli che abbiamo menzionato in precedenza, come TensorFlow, PyTorch, MXNet, Keras, ecc. Questi framework utilizzano GPU, TPU, training distribuito e AutoML per fornire IA scalabile. Concetti come model serving, MLOps e AIOps sono correlati all’operatività dell’IA nel cloud. L’IA cloud alimenta servizi come Google Cloud AI e consente il “transfer learning” tramite modelli pre-addestrati.\n\n\n6.6.2 Edge\nI framework Edge AI sono pensati per distribuire modelli di IA su dispositivi IoT, smartphone e server edge. I framework Edge AI sono ottimizzati per dispositivi con risorse di calcolo moderate, bilanciando potenza e prestazioni. I framework Edge AI sono ideali per applicazioni che richiedono elaborazione in tempo reale o quasi reale, tra cui robotica, veicoli autonomi e dispositivi intelligenti. I principali framework Edge AI includono TensorFlow Lite, PyTorch Mobile, CoreML e altri. Impiegano ottimizzazioni come compressione del modello, quantizzazione ed architetture di reti neurali efficienti. Il supporto hardware include CPU, GPU, NPU e acceleratori come Edge TPU. Edge AI consente casi d’uso come visione mobile, riconoscimento vocale e rilevamento di anomalie in tempo reale.\n\n\n6.6.3 Embedded\nI framework TinyML sono specializzati per distribuire modelli AI su dispositivi con risorse estremamente limitate, in particolare microcontrollori e sensori all’interno dell’ecosistema IoT. I framework TinyML sono progettati per dispositivi con risorse limitate, enfatizzando memoria minima e consumo energetico. I framework TinyML sono specializzati per casi d’uso su dispositivi IoT con risorse limitate per applicazioni di manutenzione predittiva, riconoscimento dei gesti e monitoraggio ambientale. I principali framework TinyML includono TensorFlow Lite Micro, uTensor e ARM NN. Ottimizzano modelli complessi per adattarli a kilobyte di memoria tramite tecniche come l’addestramento consapevole della quantizzazione e la precisione ridotta. TinyML consente il rilevamento intelligente su dispositivi alimentati a batteria, consentendo l’apprendimento collaborativo tramite apprendimento federato. La scelta del framework implica il bilanciamento delle prestazioni del modello e dei vincoli computazionali della piattaforma target, che sia cloud, edge o TinyML. Tabella 6.3 confronta i principali framework di IA negli ambienti cloud, edge e TinyML:\n\n\n\nTabella 6.3: Confronto dei tipi di framework per Cloud AI, Edge AI e TinyML.\n\n\n\n\n\n\n\n\n\n\n\nTipo di framework\nEsempi\nTecnologie chiave\nCasi d’uso\n\n\n\n\nCloud AI\nTensorFlow, PyTorch, MXNet, Keras\nGPU, TPU, addestramento distribuito, AutoML, MLOps\nServizi cloud, app Web, analisi di big data\n\n\nEdge AI\nTensorFlow Lite, PyTorch Mobile, Core ML\nOttimizzazione del modello, compressione, quantizzazione, architetture NN efficienti\nApp mobili, sistemi autonomi, elaborazione in tempo reale\n\n\nTinyML\nTensorFlow Lite Micro, uTensor, ARM NN\nTraining consapevole della quantizzazione, precisione ridotta, ricerca di architettura neurale\nSensori IoT, dispositivi indossabili, manutenzione predittiva, riconoscimento dei gesti\n\n\n\n\n\n\nDifferenze principali:\n\nCloud AI sfrutta un’enorme potenza di calcolo per modelli complessi utilizzando GPU/TPU e training distribuito.\nEdge AI ottimizza i modelli per l’esecuzione locale su dispositivi edge con risorse limitate.\nTinyML adatta i modelli a una memoria estremamente bassa e calcola ambienti come i microcontrollori.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#sec-ai_frameworks_embedded",
    "href": "contents/frameworks/frameworks.it.html#sec-ai_frameworks_embedded",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.7 Framework di Intelligenza Artificiale Embedded",
    "text": "6.7 Framework di Intelligenza Artificiale Embedded\n\n6.7.1 Vincoli di Risorse\nI sistemi embedded affrontano gravi limitazioni di risorse che pongono sfide uniche quando si distribuiscono modelli di machine learning rispetto alle piattaforme di elaborazione tradizionali. Ad esempio, le unità microcontrollore (MCU) comunemente utilizzate nei dispositivi IoT hanno spesso:\n\nRAM varia da decine di kilobyte a pochi megabyte. Il popolare MCU ESP8266 ha circa 80 KB di RAM a disposizione degli sviluppatori. Ciò contrasta con 8 GB o più su laptop e desktop tipici odierni.\nMemoria Flash varia da centinaia di kilobyte a pochi megabyte. Il microcontrollore Arduino Uno fornisce solo 32 KB di archiviazione del codice. I computer standard odierni hanno un’archiviazione su disco nell’ordine dei terabyte.\nPotenza di elaborazione da pochi MHz a circa 200 MHz. L’ESP8266 funziona a 80 MHz. Questo è di diversi ordini di grandezza più lento delle CPU multi-core multi-GHz nei server e nei laptop di fascia alta.\n\nQuesti vincoli rigorosi spesso rendono impossibile l’addestramento di modelli di apprendimento automatico direttamente sui microcontrollori. La RAM limitata impedisce la gestione di grandi set di dati per il training. L’uso di energia per l’addestramento esaurirebbe rapidamente anche i dispositivi alimentati a batteria. Al contrario, i modelli vengono addestrati su sistemi ricchi di risorse e distribuiti su microcontrollori per un’inferenza ottimizzata. Ma anche l’inferenza pone delle sfide:\n\nDimensioni del Modello: I modelli di intelligenza artificiale sono troppo grandi per adattarsi a dispositivi IoT ed embedded. Ciò richiede tecniche di compressione del modello, come quantizzazione, potatura e “knowledge distillation” [distillazione della conoscenza]. Inoltre, come vedremo, molti dei framework utilizzati dagli sviluppatori di intelligenza artificiale hanno grandi quantità di overhead e librerie integrate che i sistemi embedded non possono supportare.\nComplessità delle Attività: Con solo decine di KB o pochi MB di RAM, i dispositivi IoT e i sistemi embedded sono limitati nella complessità delle attività che possono gestire. Le attività che richiedono grandi set di dati o algoritmi sofisticati, ad esempio LLM, che verrebbero eseguiti senza problemi su piattaforme di elaborazione tradizionali potrebbero non essere fattibili su sistemi embedded senza compressione o altre tecniche di ottimizzazione a causa delle limitazioni di memoria.\nArchiviazione ed Elaborazione dei Dati: I sistemi embedded spesso elaborano i dati in tempo reale e potrebbero archiviarne solo piccole quantità localmente. Al contrario, i sistemi di elaborazione tradizionali possono contenere ed elaborare grandi set di dati in memoria, consentendo un’analisi più rapida delle operazioni sui dati e aggiornamenti in tempo reale.\nSicurezza e Privacy: La poca memoria limita anche la complessità degli algoritmi e dei protocolli di sicurezza, la crittografia dei dati, le protezioni da reverse engineering e altro che può essere implementato sul dispositivo. Ciò potrebbe rendere alcuni dispositivi IoT più vulnerabili agli attacchi.\n\nDi conseguenza, le ottimizzazioni software specializzate e i framework ML su misura per i microcontrollori devono funzionare entro questi stretti limiti delle risorse. Tecniche di ottimizzazione intelligenti come quantizzazione, potatura e distillazione della conoscenza comprimono i modelli per adattarli alla memoria limitata (vedere la sezione Ottimizzazioni). Gli insegnamenti tratti dalla ricerca di architettura neurale aiutano a guidare la progettazione dei modelli.\nI miglioramenti hardware come gli acceleratori ML dedicati sui microcontrollori aiutano anche ad alleviare i vincoli. Ad esempio, Hexagon DSP di Qualcomm accelera i modelli TensorFlow Lite sui chip mobili Snapdragon. Google Edge TPU racchiude le prestazioni ML in un piccolo ASIC per dispositivi edge. ARM Ethos-U55 offre un’inferenza efficiente sui microcontrollori di classe Cortex-M. Questi chip ML personalizzati sbloccano funzionalità avanzate per applicazioni con risorse limitate.\nA causa della potenza di elaborazione limitata, è quasi sempre impossibile addestrare modelli di intelligenza artificiale su IoT o sistemi embedded. Invece, i modelli vengono addestrati su potenti computer tradizionali (spesso con GPU) e poi distribuiti sul dispositivo embedded per l’inferenza. TinyML si occupa specificamente di questo, assicurando che i modelli siano sufficientemente leggeri per l’inferenza in tempo reale su questi dispositivi limitati.\n\n\n6.7.2 Framework e Librerie\nI framework di intelligenza artificiale embedded sono strumenti software e librerie progettati per abilitare funzionalità di intelligenza artificiale e ML su sistemi embedded. Questi framework sono essenziali per portare l’intelligenza artificiale su dispositivi IoT, robotica e altre piattaforme di edge computing e sono progettati per funzionare dove risorse di elaborazione, memoria e consumo energetico sono limitati.\n\n\n6.7.3 Sfide\nSebbene i sistemi embedded rappresentino un’enorme opportunità per l’implementazione dell’apprendimento automatico per abilitare capacità intelligenti in edge, questi ambienti con risorse limitate pongono sfide significative. A differenza dei tipici ambienti cloud o desktop ricchi di risorse computazionali, i dispositivi embedded introducono gravi limitazioni in termini di memoria, potenza di elaborazione, efficienza energetica e hardware specializzato. Di conseguenza, le tecniche e i framework di apprendimento automatico esistenti progettati per cluster di server con risorse abbondanti non si traducono direttamente nei sistemi embedded. Questa sezione svela alcune delle sfide e delle opportunità per i sistemi embedded e i framework ML.\n\nEcosistema Frammentato\nLa mancanza di un framework ML unificato ha portato a un ecosistema altamente frammentato. Gli ingegneri di aziende come STMicroelectronics, NXP Semiconductors e Renesas hanno dovuto sviluppare soluzioni personalizzate su misura per le loro specifiche architetture di microcontrollori e DSP. Questi framework ad hoc richiedevano un’ampia ottimizzazione manuale per ogni piattaforma hardware di basso livello. Ciò ha reso estremamente difficile il porting dei modelli, richiedendo la riqualificazione per nuove architetture Arm, RISC-V o proprietarie.\n\n\nEsigenze Hardware Disparate\nSenza un framework condiviso, non esisteva un modo standard per valutare le capacità dell’hardware. Fornitori come Intel, Qualcomm e NVIDIA crearono soluzioni integrate, combinando modelli e migliorando software e hardware. Ciò rese difficile discernere i motivi del guadagni di prestazioni, se fosse merito dei nuovi progetti di chip come i core x86 a basso consumo di Intel o le ottimizzazioni software. Era necessario un framework standard affinché i fornitori potessero valutare le capacità del loro hardware in modo equo e riproducibile.\n\n\nMancanza di Portabilità\nCon strumenti standardizzati, adattare modelli addestrati in framework comuni come TensorFlow o PyTorch per funzionare in modo efficiente sui microcontrollori era più facile. Richiedeva una traduzione manuale dispendiosa, in termini di tempo, dei modelli per l’esecuzione su DSP specializzati di aziende come CEVA o core Arm M-series a basso consumo. Nessuno strumento immediato consentiva l’implementazione portatile su diverse architetture.\n\n\nInfrastruttura Incompleta\nL’infrastruttura per supportare i flussi di lavoro di sviluppo dei modelli chiave doveva essere migliorata. È necessario un maggiore supporto per le tecniche di compressione per adattare modelli di grandi dimensioni a budget di memoria limitati. Mancavano strumenti per la quantizzazione per ridurre la precisione per un’inferenza più rapida. Le API standardizzate per l’integrazione nelle applicazioni erano incomplete. Mancavano funzionalità essenziali come il debugging sul dispositivo, le metriche e la profilazione delle prestazioni. Queste lacune hanno aumentato i costi e la difficoltà dello sviluppo ML embedded.\n\n\nNessun Benchmark Standard\nSenza benchmark unificati, non esisteva un modo standard per valutare e confrontare le capacità di diverse piattaforme hardware di fornitori come NVIDIA, Arm e Ambiq Micro.. Le valutazioni esistenti si basavano su benchmark proprietari pensati per mostrare i punti di forza di specifici chip. Ciò rendeva impossibile misurare i miglioramenti hardware in modo oggettivo, imparziale e imparziale. Il capitolo Benchmarking AI affronta questo argomento in modo più dettagliato.\n\n\nTest Minimi del Mondo Reale\nGran parte dei benchmark si basava su dati sintetici. Testare rigorosamente i modelli su applicazioni embedded nel mondo reale era difficile senza set di dati e benchmark standardizzati, sollevando dubbi su come le dichiarazioni sulle prestazioni si sarebbero tradotte in un utilizzo nel mondo reale. Erano necessari test più approfonditi per convalidare i chip in casi di utilizzo reali.\nLa mancanza di framework e infrastrutture condivisi ha rallentato l’adozione di TinyML, ostacolandone l’integrazione nei prodotti embedded. I recenti framework standard hanno iniziato ad affrontare questi problemi attraverso una migliore portabilità, profilazione delle prestazioni e supporto per il benchmarking. Tuttavia, è ancora necessaria un’innovazione continua per consentire un’implementazione fluida e conveniente dell’IA nei dispositivi edge.\n\n\nRiepilogo\nL’assenza di framework, benchmark e infrastrutture standardizzati per ML embedded ne ha tradizionalmente ostacolato l’adozione. Tuttavia, sono stati compiuti recenti progressi nello sviluppo di framework condivisi come TensorFlow Lite Micro e suite di benchmark come MLPerf Tiny che mirano ad accelerare la proliferazione di soluzioni TinyML. Tuttavia, superare la frammentazione e la difficoltà dell’implementazione embedded rimane un processo in corso.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#esempi",
    "href": "contents/frameworks/frameworks.it.html#esempi",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.8 Esempi",
    "text": "6.8 Esempi\nIl deployment [distribuzione] di machine learning su microcontrollori e altri dispositivi embedded richiede spesso librerie software e framework appositamente ottimizzati per funzionare entro vincoli rigorosi di memoria, elaborazione e potenza. Esistono diverse opzioni per eseguire l’inferenza su hardware con risorse limitate, ciascuna con il proprio approccio all’ottimizzazione dell’esecuzione del modello. Questa sezione esplorerà le caratteristiche chiave e i principi di progettazione alla base di TFLite Micro, TinyEngine e CMSIS-NN, fornendo informazioni su come ogni framework affronta il complesso problema dell’esecuzione di reti neurali molto accurata ma efficiente sui microcontrollori. Mostrerà inoltre diversi approcci per l’implementazione di framework TinyML efficienti.\nTabella 6.4 riassume le principali differenze e somiglianze tra questi tre framework di inferenza di apprendimento automatico specializzati per sistemi embedded e microcontrollori.\n\n\n\nTabella 6.4: Confronto dei framework: TensorFlow Lite Micro, TinyEngine e CMSIS-NN\n\n\n\n\n\n\n\n\n\n\n\nFramework\nTensorFlow Lite Micro\nTinyEngine\nCMSIS-NN\n\n\n\n\nApproccio\nBasato su interprete\nCompilazione statica\nKernel di reti neurali ottimizzati\n\n\nFocus sull’hardware\nDispositivi embedded generali\nMicrocontrollori\nProcessori ARM Cortex-M\n\n\nSupporto aritmetico\nVirgola mobile\nVirgola mobile, virgola fissa\nVirgola mobile, virgola fissa\n\n\nSupporto del modello\nModelli di rete neurale generale\nModelli co-progettati con TinyNAS\nTipi di livelli di rete neurale comuni\n\n\nImpronta del codice\nPiù grande grazie all’inclusione di interprete e operazioni\nPiccola, include solo le operazioni necessarie per il modello\nNativamente leggera\n\n\nLatenza\nPiù alta grazie a overhead di interpretazione\nMolto bassa grazie al modello compilato\nfocalizzato sulla bassa latenza\n\n\nGestione della memoria\nGestita dinamicamente da interprete\nOttimizzazione a livello di modello\nStrumenti per un’allocazione efficiente\n\n\nApproccio di ottimizzazione\nAlcune funzionalità di e generazione del codice\nKernel specializzati, fusione di operatori\nOttimizzazioni di assemblaggio specifiche dell’architettura\n\n\nPrincipali vantaggi\nFlessibilità, portabilità, facile aggiornamento dei modelli\nMassimizza le prestazioni, ottimizza l’utilizzo della memoria\nAccelerazione hardware, API standardizzata, portabilità\n\n\n\n\n\n\nNe comprenderemo ciascuno in modo più dettagliato nelle sezioni seguenti.\n\n6.8.1 Interprete\nTensorFlow Lite Micro (TFLM) è un framework di inferenza di apprendimento automatico progettato per dispositivi embedded con risorse limitate. Utilizza un interprete per caricare ed eseguire modelli di apprendimento automatico, il che fornisce flessibilità e facilità di aggiornamento dei modelli sul campo (David et al. 2021).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\nGli interpreti tradizionali spesso hanno un overhead di branching [diramazione] significativo, che può ridurre le prestazioni. Tuttavia, l’interpretazione del modello di machine learning trae vantaggio dall’efficienza dei kernel di lunga durata, in cui ogni runtime del kernel è relativamente grande e aiuta a mitigare l’overhead dell’interprete.\nUn’alternativa a un motore di inferenza basato su interprete è quella di generare codice nativo da un modello durante l’esportazione. Ciò può migliorare le prestazioni, ma sacrifica portabilità e flessibilità, poiché il codice generato deve essere ricompilato per ogni piattaforma target e deve essere sostituito completamente per modificare un modello.\nTFLM bilancia la semplicità della compilazione del codice e la flessibilità di un approccio basato su interprete includendo alcune funzionalità di generazione del codice. Ad esempio, la libreria può essere costruita esclusivamente da file sorgenti, offrendo gran parte della semplicità della compilazione associata alla generazione di codice, pur mantenendo i vantaggi di un framework che esegue il modello interpretandolo.\nUn approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice per l’inferenza di apprendimento automatico su dispositivi embedded:\n\nFlessibilità: I modelli possono essere aggiornati sul campo senza ricompilare l’intera applicazione.\nPortabilità: L’interprete può essere utilizzato per eseguire modelli su diverse piattaforme target senza dover effettuare il porting del codice.\nEfficienza della Memoria: L’interprete può condividere il codice su più modelli, riducendo l’utilizzo della memoria.\nFacilità di sviluppo: Gli interpreti sono più facili da sviluppare e gestire rispetto ai generatori di codice.\n\nTensorFlow Lite Micro è un framework potente e flessibile per l’inferenza di apprendimento automatico su dispositivi embedded. Il suo approccio basato su interprete offre diversi vantaggi rispetto alla generazione di codice, tra cui flessibilità, portabilità, efficienza della memoria e facilità di sviluppo.\n\n\n6.8.2 Basati su Compilatore\nTinyEngine è un framework di inferenza ML progettato specificamente per microcontrollori con risorse limitate. Utilizza diverse ottimizzazioni per consentire l’esecuzione di reti neurali molto accurate entro i vincoli rigorosi di memoria, elaborazione e archiviazione sui microcontrollori (Lin et al. 2020).\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\nMentre framework di inferenza come TFLite Micro utilizzano interpreti per eseguire il grafo della rete neurale in modo dinamico in fase di esecuzione, ciò aggiunge un overhead significativo per quanto riguarda l’utilizzo della memoria per archiviare metadati, latenza di interpretazione e mancanza di ottimizzazioni. Tuttavia, TFLite sostiene che l’overhead è piccolo. TinyEngine elimina questo overhead utilizzando un approccio di generazione del codice. Analizza il grafo di rete durante la compilazione e genera codice specializzato per eseguire solo quel modello. Questo codice viene compilato in modo nativo nel binario dell’applicazione, evitando i costi di interpretazione in fase di esecuzione.\nI framework ML convenzionali pianificano la memoria per layer, cercando di ridurre al minimo l’utilizzo per ogni layer separatamente. TinyEngine esegue la pianificazione a livello di modello anziché analizzare l’utilizzo della memoria tra i layer. Assegna una dimensione di buffer comune in base alle esigenze massime di memoria di tutti i layer. Questo buffer viene quindi condiviso in modo efficiente tra i layer per aumentare il riutilizzo dei dati.\nTinyEngine è inoltre specializzato nei kernel per ogni layer tramite tecniche come operatori di tiling, unrolling e fusing. Ad esempio, genererà kernel di calcolo unrolled [srotolato] con il numero di loop necessari per una convoluzione 3x3 o 5x5. Questi kernel specializzati estraggono le massime prestazioni dall’hardware del microcontrollore. Utilizza convoluzioni depthwise [in profondità] ottimizzate per ridurre al minimo le allocazioni di memoria calcolando l’output di ogni canale posizionato sui dati del canale di input. Questa tecnica sfrutta la natura separabile dei canali delle convoluzioni depthwise per ridurre le dimensioni di picco della memoria.\nCome TFLite Micro, il binario TinyEngine compilato include solo le operazioni necessarie per un modello specifico anziché tutte le operazioni possibili. Ciò si traduce in un footprint binario molto piccolo, mantenendo basse le dimensioni del codice per i dispositivi con limiti di memoria.\nUna differenza tra TFLite Micro e TinyEngine è che quest’ultimo è co-progettato con “TinyNAS”, un metodo di ricerca di architettura per modelli di microcontrollori simile al NAS differenziale per microcontrollori. L’efficienza di TinyEngine consente di esplorare modelli più grandi e accurati tramite NAS. Fornisce inoltre feedback a TinyNAS su quali modelli possono rientrare nei vincoli hardware.\nAttraverso varie tecniche personalizzate, come la compilazione statica, la pianificazione basata sul modello, kernel specializzati e la co-progettazione con NAS, TinyEngine consente un’inferenza di deep learning ad alta precisione entro i vincoli di risorse rigorosi dei microcontrollori.\n\n\n6.8.3 Libreria\nCMSIS-NN, acronimo di Cortex Microcontroller Software Interface Standard for Neural Networks, è una libreria software ideata da ARM. Offre un’interfaccia standardizzata per distribuire l’inferenza di reti neurali su microcontrollori e sistemi embedded, concentrandosi sull’ottimizzazione per i processori ARM Cortex-M (Lai, Suda, e Chandra 2018).\n\nLai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. «Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus». ArXiv preprint abs/1801.06601. https://arxiv.org/abs/1801.06601.\nKernel di Reti Neurali: CMSIS-NN ha kernel altamente efficienti che gestiscono operazioni fondamentali di reti neurali come convoluzione, pooling, layer completamente connessi e funzioni di attivazione. Si rivolge a un’ampia gamma di modelli di reti neurali supportando l’aritmetica a virgola mobile e fissa. Quest’ultima è particolarmente utile per i dispositivi con risorse limitate in quanto riduce i requisiti di memoria e di calcolo (Quantization).\nAccelerazione Hardware: CMSIS-NN sfrutta la potenza delle istruzioni SIMD (Single Instruction, Multiple Data) disponibili su molti processori Cortex-M. Ciò consente l’elaborazione parallela di più elementi di dati all’interno di una singola istruzione, aumentando così l’efficienza computazionale. Alcuni processori Cortex-M dispongono di estensioni di Digital Signal Processing (DSP) che CMSIS-NN può sfruttare per l’esecuzione accelerata della rete neurale. La libreria include anche ottimizzazioni a livello di assembly su misura per specifiche architetture di microcontrollori per migliorare ulteriormente le prestazioni.\nAPI standardizzata: CMSIS-NN offre un’API coerente e astratta che protegge gli sviluppatori dalle complessità dei dettagli hardware di basso livello. Ciò semplifica l’integrazione dei modelli di rete neurale nelle applicazioni. Può anche comprendere strumenti o utilità per convertire i formati di modelli di rete neurale più diffusi in un formato compatibile con CMSIS-NN.\nGestione della Memoria: CMSIS-NN fornisce funzioni per un’allocazione e una gestione efficienti della memoria, il che è fondamentale nei sistemi embedded in cui le risorse di memoria sono scarse. Garantisce un utilizzo ottimale della memoria durante l’inferenza e, in alcuni casi, consente operazioni in loco per ridurre il sovraccarico di memoria.\nPortabilità: CMSIS-NN è progettato per la portabilità su vari processori Cortex-M. Questo consente agli sviluppatori di scrivere codice che possa funzionare su diversi microcontrollori senza modifiche significative.\nBassa Latenza: CMSIS-NN riduce al minimo la latenza di inferenza, rendendolo una scelta ideale per applicazioni in tempo reale in cui è fondamentale prendere decisioni rapide.\nEfficienza Energetica: La libreria è progettata con un focus sull’efficienza energetica, rendendola adatta per dispositivi alimentati a batteria e con vincoli energetici.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#scelta-del-framework-giusto",
    "href": "contents/frameworks/frameworks.it.html#scelta-del-framework-giusto",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.9 Scelta del Framework Giusto",
    "text": "6.9 Scelta del Framework Giusto\nLa scelta del framework di machine learning giusto per una determinata applicazione richiede un’attenta valutazione di modelli, hardware e considerazioni software. Analizzando questi tre aspetti (modelli, hardware e software), gli ingegneri di ML possono selezionare il framework ottimale e personalizzarlo in base alle esigenze per applicazioni ML su dispositivo efficienti e performanti. L’obiettivo è bilanciare complessità del modello, limitazioni hardware e integrazione software per progettare una pipeline ML su misura per dispositivi embedded e edge.\n\n\n\n\n\n\nFigura 6.8: Confronto tra Framework TensorFlow - Generale. Fonte: TensorFlow.\n\n\n\n\n6.9.1 Modello\nTensorFlow supporta molte più operazioni (op) rispetto a TensorFlow Lite e TensorFlow Lite Micro, in quanto viene solitamente utilizzato per la ricerca o l’implementazione cloud, che richiedono un numero elevato di operatori e una maggiore flessibilità (vedere Figura 6.8). TensorFlow Lite supporta operazioni selezionate per il training sul dispositivo, mentre TensorFlow Micro no. TensorFlow Lite supporta anche forme dinamiche e training consapevole della quantizzazione, mentre TensorFlow Micro no. Al contrario, TensorFlow Lite e TensorFlow Micro offrono strumenti e supporto di quantizzazione nativi, dove la quantizzazione si riferisce alla trasformazione di un programma ML in una rappresentazione approssimata con operazioni di precisione inferiore disponibili.\n\n\n6.9.2 Software\n\n\n\n\n\n\nFigura 6.9: Confronto tra Framework TensorFlow - Software. Fonte: TensorFlow.\n\n\n\nTensorFlow Lite Micro non supporta il sistema operativo, mentre TensorFlow e TensorFlow Lite sì, per ridurre il sovraccarico di memoria, velocizzare i tempi di avvio e consumare meno energia (vedere Figura 6.9). TensorFlow Lite Micro può essere utilizzato insieme a sistemi operativi in tempo reale (RTOS) come FreeRTOS, Zephyr e Mbed OS. TensorFlow Lite e TensorFlow Lite Micro supportano la mappatura della memoria del modello, consentendo l’accesso diretto ai modelli dalla memoria flash anziché caricarli nella RAM, cosa che TensorFlow non fa. TensorFlow e TensorFlow Lite supportano la “accelerator delegation” per pianificare il codice su diversi acceleratori, mentre TensorFlow Lite Micro no, poiché i sistemi embedded tendono ad avere una gamma limitata di acceleratori specializzati.\n\n\n6.9.3 Hardware\n\n\n\n\n\n\nFigura 6.10: Confronto tra Framework TensorFlow - Hardware. Fonte: TensorFlow.\n\n\n\nTensorFlow Lite e TensorFlow Lite Micro hanno dimensioni binarie di base e footprint di memoria significativamente più piccoli rispetto a TensorFlow (vedere Figura 6.10). Ad esempio, un tipico binario TensorFlow Lite Micro è inferiore a 200 KB, mentre TensorFlow è molto più grande. Ciò è dovuto agli ambienti con risorse limitate dei sistemi embedded. TensorFlow supporta x86, TPU e GPU come NVIDIA, AMD e Intel. TensorFlow Lite supporta i processori Arm Cortex-A e x86 comunemente utilizzati su telefoni cellulari e tablet. Quest’ultimo è privo di tutta la logica di training non necessaria per l’implementazione sul dispositivo. TensorFlow Lite Micro fornisce supporto per core Arm Cortex M focalizzati sui microcontrollori come M0, M3, M4 e M7, nonché DSP come Hexagon e SHARC e MCU come STM32, NXP Kinetis, Microchip AVR.\n\n\n6.9.4 Altri Fattori\nLa selezione del framework di IA appropriato è essenziale per garantire che i sistemi embedded possano eseguire in modo efficiente i modelli di IA. Diversi fattori chiave oltre a modelli, hardware e software dovrebbero essere presi in considerazione quando si valutano i framework IA per i sistemi embedded. Altri fattori chiave da considerare quando si sceglie un framework di apprendimento automatico sono prestazioni, scalabilità, facilità d’uso, integrazione con strumenti di ingegneria dei dati, integrazione con strumenti di ottimizzazione dei modelli e supporto della community. Comprendendo questi fattori, si possono prendere decisioni informate e massimizzare il potenziale delle iniziative di machine-learning.\n\nPrestazioni\nLe prestazioni sono fondamentali nei sistemi embedded in cui le risorse di calcolo sono limitate. Valutare la capacità del framework di ottimizzare l’inferenza del modello per l’hardware embedded. La quantizzazione del modello e il supporto dell’accelerazione hardware sono cruciali per ottenere un’inferenza efficiente.\n\n\nScalabilità\nLa scalabilità è essenziale quando si considera la potenziale crescita di un progetto di IA embedded. Il framework dovrebbe supportare l’implementazione di modelli su vari dispositivi embedded, dai microcontrollori ai processori più potenti. Dovrebbe inoltre gestire senza problemi sia le distribuzioni su piccola che su larga scala.\n\n\nIntegrazione con Strumenti di Data Engineering\nGli strumenti di ingegneria dei dati sono essenziali per la pre-elaborazione dei dati e la gestione della pipeline. Un framework di intelligenza artificiale ideale per sistemi embedded dovrebbe integrarsi perfettamente con questi strumenti, consentendo un’efficiente acquisizione dei dati, trasformazione e addestramento del modello.\n\n\nIntegrazione con Strumenti di Ottimizzazione del Modello\nL’ottimizzazione del modello garantisce che i modelli di intelligenza artificiale siano adatti per la distribuzione embedded. Valutare se il framework si integra con strumenti di ottimizzazione del modello come TensorFlow Lite Converter o ONNX Runtime per facilitare la quantizzazione del modello e la riduzione delle dimensioni.\n\n\nFacilità d’Uso\nLa facilità d’uso di un framework di IA ha un impatto significativo sull’efficienza dello sviluppo. Un framework con un’interfaccia intuitiva e una documentazione chiara riduce la curva di apprendimento degli sviluppatori. Si dovrebbe considerare se il framework supporta API di alto livello, consentendo agli sviluppatori di concentrarsi sulla progettazione del modello piuttosto che sui dettagli di implementazione di basso livello. Questo fattore è incredibilmente importante per i sistemi embedded, che hanno meno funzionalità di quelle a cui gli sviluppatori tipici potrebbero essere abituati.\n\n\nSupporto della Community\nIl supporto della community gioca un altro fattore essenziale. I framework con community attive e coinvolte spesso hanno basi di codice ben mantenute, ricevono aggiornamenti regolari e forniscono forum preziosi per la risoluzione dei problemi. Di conseguenza, anche il supporto della community gioca un ruolo nella facilità d’uso perché garantisce che gli sviluppatori abbiano accesso a una vasta gamma di risorse, tra cui tutorial e progetti di esempio. Il supporto della community fornisce una certa garanzia che il framework continuerà a essere supportato per futuri aggiornamenti. Ci sono solo pochi framework che soddisfano le esigenze di TinyML. TensorFlow Lite Micro è il più popolare e ha il maggior supporto della comunità.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#tendenze-future-nei-framework-ml",
    "href": "contents/frameworks/frameworks.it.html#tendenze-future-nei-framework-ml",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.10 Tendenze Future nei Framework ML",
    "text": "6.10 Tendenze Future nei Framework ML\n\n6.10.1 Decomposizione\nAttualmente, lo stack del sistema ML è costituito da quattro astrazioni come mostrato in Figura 6.11, vale a dire (1) grafi computazionali, (2) programmi tensoriali, (3) librerie e runtime e (4) primitive hardware.\n\n\n\n\n\n\nFigura 6.11: Quattro astrazioni negli attuali stack dei sistemi ML. Fonte: TVM.\n\n\n\nCiò ha portato a confini verticali (ad esempio, tra i livelli di astrazione) e orizzontali (ad esempio, approcci basati sulla libreria rispetto a quelli basati sulla compilazione per il calcolo dei tensori), che ostacolano l’innovazione per il ML. Il lavoro futuro nei framework ML può guardare alla rottura di questi confini. A dicembre 2021 è stato proposto Apache TVM Unity, che mirava a facilitare le interazioni tra i diversi livelli di astrazione (nonché le persone dietro di essi, come scienziati ML, ingegneri ML e ingegneri hardware) e a co-ottimizzare le decisioni in tutti e quattro i livelli di astrazione.\n\n\n6.10.2 Compilatori e Librerie ad Alte Prestazioni\nCon l’ulteriore sviluppo dei framework ML, continueranno a emergere compilatori e librerie ad alte prestazioni. Alcuni esempi attuali includono TensorFlow XLA e CUTLASS di Nvidia, che accelerano le operazioni di algebra lineare nei grafi computazionali, e TensorRT di Nvidia, che accelera e ottimizza l’inferenza.\n\n\n6.10.3 ML per Framework ML\nPossiamo anche usare il ML per migliorare i framework di ML in futuro. Alcuni usi correnti di ML per framework ML includono:\n\nOttimizzazione degli iperparametri tramite tecniche quali ottimizzazione bayesiana, ricerca casuale e ricerca a griglia\nNeural Architecture Search (NAS) per cercare automaticamente architetture di rete ottimali\nAutoML, che come descritto in Sezione 6.5, automatizza la pipeline ML.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#conclusione",
    "href": "contents/frameworks/frameworks.it.html#conclusione",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.11 Conclusione",
    "text": "6.11 Conclusione\nIn sintesi, la selezione del framework di machine learning ottimale richiede una valutazione approfondita di varie opzioni in base a criteri quali usabilità, supporto della community, prestazioni, compatibilità hardware e capacità di conversione del modello. Non esiste una soluzione adatta a tutti, poiché il framework giusto dipende da vincoli e casi d’uso specifici.\nAbbiamo prima introdotto la necessità di framework di apprendimento automatico come TensorFlow e PyTorch. Questi framework offrono funzionalità quali tensori per la gestione di dati multidimensionali, grafi computazionali per la definizione e l’ottimizzazione delle operazioni del modello e una suite di strumenti tra cui funzioni di perdita, ottimizzatori e caricatori di dati che semplificano lo sviluppo del modello.\nLe funzionalità avanzate migliorano ulteriormente l’usabilità di questi framework, consentendo attività come la messa a punto di grandi modelli pre-addestrati e la facilitazione del “federated learning”. Queste funzionalità sono fondamentali per sviluppare modelli di apprendimento automatico sofisticati in modo efficiente.\nI framework di intelligenza artificiale embedded, come TensorFlow Lite Micro, forniscono strumenti specializzati per la distribuzione di modelli su piattaforme con risorse limitate. TensorFlow Lite Micro, ad esempio, offre strumenti di ottimizzazione completi, tra cui la mappatura della quantizzazione e le ottimizzazioni del kernel, per garantire prestazioni elevate su piattaforme basate su microcontrollori come i processori Arm Cortex-M e RISC-V. I framework creati appositamente per hardware specializzato come CMSIS-NN su processori Cortex-M possono massimizzare ulteriormente le prestazioni ma sacrificare la portabilità. I framework integrati dei fornitori di processori adattano lo stack alle loro architetture, liberando il pieno potenziale dei loro chip ma ci si blocca nel loro ecosistema.\nIn definitiva, la scelta del framework giusto implica la ricerca della migliore corrispondenza tra le sue capacità e i requisiti della piattaforma target. Ciò richiede un bilanciamento tra esigenze di prestazioni, vincoli hardware, complessità del modello e altri fattori. Una valutazione approfondita dei modelli e dei casi d’uso previsti e la valutazione delle opzioni rispetto alle metriche chiave guideranno gli sviluppatori nella selezione del framework ideale per le loro applicazioni di machine learning.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/frameworks/frameworks.it.html#sec-ai-frameworks-resource",
    "href": "contents/frameworks/frameworks.it.html#sec-ai-frameworks-resource",
    "title": "6  Framework di Intelligenza Artificiale",
    "section": "6.12 Risorse",
    "text": "6.12 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nFrameworks overview.\nEmbedded systems software.\nInference engines: TF vs. TFLite.\nTF flavors: TF vs. TFLite vs. TFLite Micro.\nTFLite Micro:\n\nTFLite Micro Big Picture.\nTFLite Micro Interpreter.\nTFLite Micro Model Format.\nTFLite Micro Memory Allocation.\nTFLite Micro NN Operations.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 6.1\nEsercizio 6.2\nEsercizio 6.3\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Framework di Intelligenza Artificiale</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html",
    "href": "contents/training/training.it.html",
    "title": "7  Addestramento dell’IA",
    "section": "",
    "text": "7.1 Introduzione\nIl training è fondamentale per sviluppare sistemi di intelligenza artificiale accurati e utili tramite il machine learning. Il training mira a creare un modello di apprendimento automatico che possa generalizzare a dati nuovi e mai visti anziché memorizzare gli esempi dell’addestramento. Ciò avviene inserendo dati di training in algoritmi che apprendono pattern da questi esempi regolando i parametri interni.\nGli algoritmi riducono al minimo una “funzione loss” [perdita], che confronta le loro previsioni sui dati di training con le etichette o le soluzioni note, guidando l’apprendimento. Un training efficace richiede spesso set di dati rappresentativi di alta qualità sufficientemente grandi da catturare la variabilità nei casi d’uso del mondo reale.\nRichiede inoltre la scelta di un algoritmo adatto all’attività, che si tratti di una rete neurale per la visione artificiale, un algoritmo di apprendimento di rinforzo per il controllo robotico o un metodo basato su alberi per la previsione categoriale. È necessaria un’attenta messa a punto per la struttura del modello, come la profondità e la larghezza della rete neurale e i parametri di apprendimento come la dimensione del passo e la forza della regolarizzazione.\nSono importanti anche le tecniche per prevenire l’overfitting, come le penalità di regolarizzazione nonché la convalida con dati trattenuti. L’overfitting può verificarsi quando un modello si adatta troppo ai dati di training, non riuscendo a generalizzare con i nuovi dati. Ciò può accadere se il modello è troppo complesso o è stato addestrato troppo a lungo.\nPer evitare l’overfitting, le tecniche di regolarizzazione possono aiutare a vincolare il modello. Un metodo di regolarizzazione consiste nell’aggiungere un termine di penalità alla funzione di perdita che scoraggia la complessità, come la norma L2 dei pesi. Questo penalizza i valori dei parametri elevati. Un’altra tecnica è il dropout, in cui una percentuale di neuroni viene impostata casualmente a zero durante l’addestramento. Ciò riduce il co-adattamento dei neuroni.\nI metodi di validazione aiutano anche a rilevare ed evitare l’overfitting. Una parte dei dati di training viene tenuta fuori dal ciclo di training come un set di validazione. Il modello viene valutato su questi dati. Se l’errore di convalida aumenta mentre l’errore di training diminuisce, si verifica un overfitting. Il training può quindi essere interrotto in anticipo o regolarizzato in modo più forte. La regolarizzazione e la convalida consentono ai modelli di addestrarsi alla massima capacità senza overfitting [sovra-adattare] i dati di training.\nIl training richiede notevoli risorse di elaborazione, in particolare per le reti neurali profonde (deep) utilizzate nella visione artificiale, nell’elaborazione del linguaggio naturale e in altre aree. Queste reti hanno milioni di pesi regolabili che devono essere regolati tramite un training esteso. I miglioramenti hardware e le tecniche di training distribuite hanno consentito di addestrare reti neurali sempre più grandi che possono raggiungere prestazioni di livello umano in alcune attività.\nIn sintesi, alcuni punti chiave sul training:\nGuideremo attraverso questi dettagli nelle restanti sezioni. Comprendere come sfruttare in modo efficace dati, algoritmi, ottimizzazione dei parametri e generalizzazione attraverso il training è essenziale per sviluppare sistemi di intelligenza artificiale capaci e distribuibili che funzionino in modo robusto nel mondo reale.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#introduzione",
    "href": "contents/training/training.it.html#introduzione",
    "title": "7  Addestramento dell’IA",
    "section": "",
    "text": "I Dati sono cruciali: I modelli di machine learning apprendono dagli esempi nei dati di training. Dati più rappresentativi e di qualità elevata portano a migliori prestazioni del modello. I dati devono essere elaborati e formattati per il training.\nGli algoritmi imparano dai dati: Diversi algoritmi (reti neurali, alberi decisionali, ecc.) hanno approcci diversi per trovare dei pattern nei dati. È importante scegliere l’algoritmo giusto per l’attività.\nL’addestramento affina i parametri del modello: L’addestramento del modello regola i parametri interni per trovare pattern nei dati. I modelli avanzati come le reti neurali hanno molti pesi regolabili. L’addestramento regola iterativamente i pesi per ridurre al minimo una funzione di perdita.\nLa generalizzazione è l’obiettivo: Un modello che sovra-adatta i dati di addestramento non generalizzerà bene. Le tecniche di regolarizzazione (dropout, early stopping arresto anticipato, ecc.) riducono il sovra-adattamento. I dati di validazione vengono utilizzati per valutare la generalizzazione.\nL’addestramento richiede risorse di elaborazione: L’addestramento di modelli complessi richiede una notevole potenza di elaborazione e tempo. I miglioramenti hardware e il training distribuito su GPU/TPU hanno consentito dei progressi.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#matematica-delle-reti-neurali",
    "href": "contents/training/training.it.html#matematica-delle-reti-neurali",
    "title": "7  Addestramento dell’IA",
    "section": "7.2 Matematica delle Reti Neurali",
    "text": "7.2 Matematica delle Reti Neurali\nIl deep learning ha rivoluzionato l’apprendimento automatico e l’intelligenza artificiale, consentendo ai computer di apprendere modelli complessi e prendere decisioni intelligenti. La rete neurale è al centro della rivoluzione del deep learning e, come discusso nella sezione 3, “Avvio al Deep Learning”, è un pilastro in alcuni di questi progressi.\nLe reti neurali sono costituite da semplici funzioni stratificate l’una sull’altra. Ogni layer acquisisce alcuni dati, esegue alcuni calcoli e li passa al layer successivo. Questi layer apprendono progressivamente funzionalità di alto livello utili per le attività che la rete è addestrata a svolgere. Ad esempio, in una rete addestrata per il riconoscimento delle immagini, il layer di input può acquisire valori di pixel, mentre i layer successivi possono rilevare forme semplici come i bordi. I layer successivi possono rilevare forme più complesse come nasi, occhi, ecc. Il layer di output finale classifica l’immagine nel suo complesso.\nLa rete, in una rete neurale, si riferisce al modo in cui questi layer sono connessi. L’output di ogni layer è considerato un set di neuroni, che sono collegati ai neuroni nei layer successivi, formando una “rete”. Il modo in cui questi neuroni interagiscono è determinato dai pesi tra di loro, che modellano le forze sinaptiche simili a quelle di un neurone del cervello. La rete neurale viene addestrata regolando questi pesi. Concretamente, i pesi vengono inizialmente impostati in modo casuale, quindi viene immesso l’input, l’output viene confrontato con il risultato desiderato e, infine, i pesi vengono modificati per migliorare la rete. Questo processo viene ripetuto finché la rete non riduce al minimo in modo affidabile la perdita (loss), indicando di aver appreso i pattern nei dati.\nCome viene definito matematicamente questo processo? Formalmente, le reti neurali sono modelli matematici costituiti da operazioni lineari e non lineari alternate, parametrizzate da un set di pesi apprendibili che vengono addestrati per minimizzare una qualche funzione di perdita (loss). Questa funzione di perdita misura quanto è buono il nostro modello per quanto riguarda l’adattamento dei nostri dati di addestramento e produce un valore numerico quando viene valutato sul nostro modello rispetto ai dati di addestramento. L’addestramento delle reti neurali comporta la valutazione ripetuta della funzione di perdita su molti dati diversi per misurare quanto è buono il nostro modello, quindi la modifica continua dei pesi del nostro modello utilizzando la backpropagation in modo che la perdita diminuisca, ottimizzando infine il modello per adattarlo ai nostri dati.\n\n7.2.1 Notazione delle Reti Neurali\nEntrando nei dettagli, il nucleo di una rete neurale può essere visto come una sequenza di operazioni alternate lineari e non lineari, come mostrato in Figura 7.1:\n\n\n\n\n\n\nFigura 7.1: Diagramma della rete neurale. Fonte: astroML.\n\n\n\nLa rete neurale funziona prendendo un vettore di input \\(x_i\\) e passandolo attraverso una serie di layer, ognuno dei quali esegue operazioni lineari e non lineari. L’output della rete a ogni layer \\(A_j\\) può essere rappresentato come:\n\\[\nA_j = f\\left(\\sum_{i=1}^{N} w_{ij} x_i\\right)\n\\]\nDove:\n\n\\(N\\) - Il numero totale di feature di input.\n\\(x_{i}\\) - La singola feature di input, dove \\(i\\) varia da \\(1\\) a \\(N\\).\n\\(w_{ij}\\) - I pesi che collegano il neurone \\(i\\) in uno layer al neurone \\(j\\) nel layer successivo, che vengono aggiustati durante l’addestramento.\n\\(f(\\theta)\\) - La funzione di attivazione non lineare applicata a ogni layer (ad esempio, ReLU, softmax, ecc.).\n\\(A_{j}\\) - L’output della rete neurale a ogni layer \\(j\\), dove \\(j\\) indica il numero del layer.\n\nNel contesto di Figura 7.1, \\(x_1, x_2, x_3, x_4,\\) e \\(x_5\\) rappresentano le caratteristiche di input. Ogni neurone di input \\(x_i\\) corrisponde a una feature dei dati di input. Le frecce dal layer di input al layer nascosto indicano le connessioni tra i neuroni di input e i neuroni nascosti, con ogni connessione associata a un peso \\(w_{ij}\\).\nIl layer nascosto è costituito dai neuroni \\(a_1, a_2, a_3,\\) e \\(a_4\\), ognuno dei quali riceve input da tutti i neuroni nello layer di input. I pesi \\(w_{ij}\\) collegano i neuroni di input ai neuroni nascosti. Ad esempio, \\(w_{11}\\) è il peso che collega l’input \\(x_1\\) al neurone nascosto \\(a_1\\).\nIl numero di nodi in ogni layer e il numero totale di layer insieme definiscono l’architettura della rete neurale. Nel primo layer (layer di input), il numero di nodi corrisponde alla dimensionalità dei dati di input, mentre nell’ultimo layer (layer di output), il numero di nodi corrisponde alla dimensionalità dell’output. Il numero di nodi nei layer intermedi può essere impostato arbitrariamente, consentendo flessibilità nella progettazione dell’architettura di rete.\nI pesi, che determinano il modo in cui ogni layer della rete neurale interagisce con gli altri, sono matrici di numeri reali. Inoltre, ogni layer in genere include un vettore di bias [polarizzazione], ma qui lo ignoriamo per semplicità. La matrice dei pesi \\(W_j\\) che collega il layer \\(j-1\\) al layer \\(j\\) ha le dimensioni:\n\\[\nW_j \\in \\mathbb{R}^{d_j \\times d_{j-1}}\n\\]\ndove \\(d_j\\) è il numero di nodi nel layer \\(j\\) e \\(d_{j-1}\\) è il numero di nodi nel layer precedente \\(j-1\\).\nL’output finale \\(y_k\\) della rete si ottiene applicando un’altra funzione di attivazione \\(g(\\theta)\\) alla somma ponderata degli output del layer nascosto:\n\\[\ny = g\\left(\\sum_{j=1}^{M} w_{jk} A_j\\right)\n\\]\nDove:\n\n\\(M\\) - Il numero di neuroni nascosti nel layer finale prima dell’output.\n\\(w_{jk}\\) - Il peso tra il neurone nascosto \\(a_j\\) e il neurone di output \\(y_k\\).\n\\(g(\\theta)\\) - La funzione di attivazione applicata alla somma ponderata degli output del layer nascosto.\n\nLa nostra rete neurale, come definita, esegue una sequenza di operazioni lineari e non lineari sui dati di input (\\(x_{i}\\)) per ottenere previsioni (\\(y_{i}\\)), che si spera siano una buona risposta a ciò che vogliamo che la rete neurale faccia sull’input (ad esempio, classificare se l’immagine di input è un gatto o meno). La nostra rete neurale può quindi essere rappresentata succintamente come una funzione \\(N\\) che accetta un input \\(x \\in \\mathbb{R}^{d_0}\\) parametrizzato da \\(W_1, ..., W_n\\) e produce l’output finale \\(y\\):\n\\[\ny = N(x; W_1, ..., W_n) \\quad \\text{where } A_0 = x\n\\]\nQuesta equazione indica che la rete inizia con l’input \\(A_0 = x\\) e calcola iterativamente \\(A_j\\) a ogni layer utilizzando i parametri \\(W_j\\) fino a produrre l’output finale \\(y\\) al layer di output.\nSuccessivamente vedremo come valutare questa rete neurale rispetto ai dati di addestramento introducendo una funzione di perdita.\n\n\n\n\n\n\nNota\n\n\n\nPerché sono necessarie le operazioni non lineari? Se avessimo solo layer lineari, l’intera rete sarebbe equivalente a un singolo layer lineare costituito dal prodotto degli operatori lineari. Quindi, le funzioni non lineari svolgono un ruolo chiave nella potenza delle reti neurali poiché migliorano la capacità della rete neurale di adattare le funzioni.\n\n\n\n\n\n\n\n\nNota\n\n\n\nAnche le convoluzioni sono operatori lineari e possono essere convertite in una moltiplicazione di matrici.\n\n\n\n\n7.2.2 Funzione Loss come Misura della Bontà di Adattamento Rispetto ai Dati di Addestramento\nDopo aver definito la nostra rete neurale, ci vengono forniti alcuni dati di addestramento, ovvero un set di punti \\({(x_j, y_j)}\\) per \\(j=1 \\rightarrow M\\), dove \\(M\\) è il numero totale di campioni nel set di dati e \\(j\\) indicizza ogni campione. Vogliamo valutare quanto è buona la nostra rete neurale nell’adattare questi dati. Per fare ciò, introduciamo una funzione di perdita, ovvero una funzione che prende l’output della rete neurale su un particolare punto dati \\(\\hat{y_j} = N(x_j; W_1, ..., W_n)\\) e lo confronta con la “etichetta” di quel particolare dato (il corrispondente \\(y_j\\)) e restituisce un singolo scalare numerico (ovvero un numero reale) che rappresenta quanto è “bene” la rete neurale adatta quel particolare dato; la misura finale di quanto è buona la rete neurale sull’intero set di dati è quindi solo la media delle perdite su tutti i dati.\nEsistono molti tipi diversi di funzioni di perdita; ad esempio, nel caso della classificazione delle immagini, potremmo usare la funzione di “cross-entropy loss” [perdita di entropia incrociata], che ci dice quanto bene si confrontano due vettori che rappresentano le previsioni di classificazione (ad esempio, se la nostra previsione prevede che un’immagine sia più probabilmente un cane, ma l’etichetta dice che è un gatto, restituirà una “perdita” elevata, che indica un cattivo adattamento).\nMatematicamente, una funzione di perdita è una funzione che prende due vettori con valori reali, uno che rappresenta gli output previsti della rete neurale e l’altro che rappresenta le etichette vere, e restituisce un singolo scalare numerico che rappresenta l’errore o la “perdita”.\n\\[\nL: \\mathbb{R}^{d_{n}} \\times \\mathbb{R}^{d_{n}} \\longrightarrow \\mathbb{R}\n\\]\nPer un singolo esempio di training, la perdita è data da:\n\\[\nL(N(x_j; W_1, ..., W_n), y_j)\n\\]\ndove \\(\\hat{y}_j = N(x_j; W_1, ..., W_n)\\) è l’output previsto della rete neurale per l’input \\(x_j\\), and \\(y_j\\) è la vera etichetta.\nLa perdita totale nell’intero set di dati, \\(L_{full}\\), viene quindi calcolata come la perdita media in tutti i dati di training:\n\nFunzione di Perdita per l’Ottimizzazione del Modello di Rete Neurale su Dataset \\[\nL_{full} = \\frac{1}{M} \\sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)\n\\]\n\n\n\n7.2.3 Addestramento di Reti Neurali con Discesa del Gradiente\nOra che possiamo misurare quanto bene la nostra rete si adatta ai dati di training, possiamo ottimizzare i pesi della rete neurale per ridurre al minimo questa perdita. In questo contesto, stiamo denotando \\(W_i\\) come pesi per ogni layer \\(i\\) nella rete. Ad alto livello, modifichiamo i parametri delle matrici a valori reali \\(W_i\\) per ridurre al minimo la funzione di perdita \\(L_{full}\\). Nel complesso, il nostro obiettivo matematico è\n\nObiettivo dell’Addestramento della Rete Neurale \\[\nmin_{W_1, ..., W_n} L_{full}\n\\] \\[\n= min_{W_1, ..., W_n} \\frac{1}{M} \\sum_{j=1}^{M} L(N(x_j; W_1,...W_n), y_j)\n\\]\n\nQuindi, come ottimizziamo questo obiettivo? Ricordiamo dal calcolo che la minimizzazione di una funzione può essere eseguita prendendo la derivata della funzione relativa ai parametri di input e modificando i parametri nella direzione del gradiente. Questa tecnica è chiamata a “discesa del gradiente” e concretamente comporta il calcolo della derivata della funzione di perdita \\(L_{full}\\) relativa a \\(W_1, ..., W_n\\) per ottenere un gradiente per questi parametri per fare un passo avanti, poi aggiornare questi parametri nella direzione del gradiente. Quindi, possiamo addestrare la nostra rete neurale utilizzando la discesa del gradiente, che applica ripetutamente la regola di aggiornamento.\n\nRegola di Aggiornamento della Discesa del Gradiente \\[\nW_i := W_i - \\lambda \\frac{\\partial L_{full}}{\\partial W_i} \\mbox{ for } i=1..n\n\\]\n\n\n\n\n\n\n\nNota\n\n\n\nIn pratica, il gradiente viene calcolato su un mini-batch di punti dati per migliorare l’efficienza computazionale. Questo processo è chiamato “discesa del gradiente stocastico” o “discesa del gradiente batch”.\n\n\nDove \\(\\lambda\\) è la dimensione del passo o il tasso di apprendimento delle nostre modifiche, nell’addestramento della nostra rete neurale, eseguiamo ripetutamente il passaggio precedente fino alla convergenza, o quando la perdita non diminuisce più. Figura 7.2 illustra questo processo: vogliamo raggiungere il punto minimo, il che si ottiene seguendo il gradiente (come illustrato con le frecce blu nella figura). Questo precedente approccio è noto come discesa del gradiente completa poiché stiamo calcolando la derivata relativa a tutti i dati di addestramento e solo dopo eseguiamo un singolo passaggio del gradiente; un approccio più efficiente è quello di calcolare il gradiente relativo solo a un batch casuale di dati e poi eseguire un passaggio, un processo noto come discesa del gradiente batch o discesa del gradiente stocastica (Robbins e Monro 1951), che è più efficiente poiché ora eseguiamo molti più passi per passaggio di tutti i dati di addestramento. Successivamente, tratteremo la matematica alla base del calcolo del gradiente della funzione di perdita relativa a \\(W_i\\), un processo noto come backpropagation.\n\nRobbins, Herbert, e Sutton Monro. 1951. «A Stochastic Approximation Method». The Annals of Mathematical Statistics 22 (3): 400–407. https://doi.org/10.1214/aoms/1177729586.\n\n\n\n\n\n\nFigura 7.2: Discesa del gradiente. Fonte: Towards Data Science.\n\n\n\n\n\n7.2.4 Backpropagation\nL’addestramento delle reti neurali comporta ripetute applicazioni dell’algoritmo di discesa del gradiente, che prevede il calcolo della derivata della funzione di perdita rispetto alle \\(W_i\\). Come calcoliamo la derivata della perdita relativa alle \\(W_i\\), dato che le \\(W_i\\) sono funzioni annidate l’una dell’altra in una rete neurale profonda? Il trucco è sfruttare la regola della catena: possiamo calcolare la derivata della perdita relativa alle \\(W_i\\) applicando ripetutamente la regola della catena in un processo completo noto come backpropagation. In particolare, possiamo calcolare i gradienti calcolando la derivata della perdita relativa agli output dell’ultimo layer, poi usarla progressivamente per calcolare la derivata della perdita relativa a ciascun layer precedente a quello di input. Questo processo inizia dalla fine della rete (il layer più vicino all’output) e procede all’indietro, e quindi prende il nome di backpropagation.\nAnalizziamolo. Possiamo calcolare la derivata della perdita relativa agli output di ciascun layer della rete neurale utilizzando applicazioni ripetute della regola della catena.\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{n}} = \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}\n\\]\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{n-1}} = \\frac{\\partial A_{n-1}}{\\partial L_{n-1}} \\frac{\\partial L_{n}}{\\partial A_{n-1}} \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}  \n\\]\no più in generale\n\\[\n\\frac{\\partial L_{full}}{\\partial L_{i}} = \\frac{\\partial A_{i}}{\\partial L_{i}} \\frac{\\partial L_{i+1}}{\\partial A_{i}} ... \\frac{\\partial A_{n}}{\\partial L_{n}} \\frac{\\partial L_{full}}{\\partial A_{n}}  \n\\]\n\n\n\n\n\n\nNota\n\n\n\nIn quale ordine dovremmo eseguire questo calcolo? Da una prospettiva computazionale, è preferibile eseguire i calcoli dalla fine alla parte frontale. (ad esempio: prima si calcola \\(\\frac{\\partial L_{full}}{\\partial A_{n}}\\), poi i termini precedenti, anziché iniziare dal centro) poiché ciò evita di materializzare e calcolare grandi jacobiani. Questo perché \\(\\ \\frac {\\partial L_{full}}{\\partial A_{n}}\\) è un vettore; quindi, qualsiasi operazione di matrice che include questo termine ha un output che è compresso per essere un vettore. Quindi, eseguire il calcolo dalla fine evita grandi moltiplicazioni matrice-matrice assicurando che i prodotti intermedi siano vettori.\n\n\n\n\n\n\n\n\nNota\n\n\n\nNella nostra notazione, assumiamo che le attivazioni intermedie \\(A_{i}\\) siano vettori colonna, anziché vettori riga, quindi la regola della catena è \\(\\frac{\\partial L}{\\partial L_{i}} = \\frac{\\partial L_{i+1}}{\\partial L_{i}} ... \\frac{\\partial L}{\\partial L_{n}}\\) piuttosto che \\(\\frac{\\partial L}{\\partial L_{i}} = \\frac{\\partial L}{\\partial L_{n}} ... \\frac{\\partial L_{i+1}}{\\partial L_{i}}\\)\n\n\nDopo aver calcolato la derivata della perdita relativa all’output di ogni layer, possiamo facilmente ottenere la derivata della perdita relativa ai parametri, utilizzando di nuovo la regola della catena:\n\\[\n\\frac{\\partial L_{full}}{W_{i}} = \\frac{\\partial L_{i}}{\\partial W_{i}} \\frac{\\partial L_{full}}{\\partial L_{i}}\n\\]\nEd è in definitiva così che le derivate dei pesi dei layer vengono calcolate usando la backpropagation! Come appare concretamente in un esempio specifico? Di seguito, esaminiamo un esempio specifico di una semplice rete neurale a 2 layer su un’attività di regressione usando una funzione di perdita MSE con input a 100 dimensioni e uno layer nascosto a 30 dimensioni:\n\nEsempio di backpropagation\nSupponiamo di avere una rete neurale a due layer \\[\nL_1 = W_1 A_{0}\n\\] \\[\nA_1 = ReLU(L_1)\n\\] \\[\nL_2 = W_2 A_{1}\n\\] \\[\nA_2 = ReLU(L_2)\n\\] \\[\nNN(x) = \\mbox{Let } A_{0} = x \\mbox{ then output } A_2\n\\] dove \\(W_1 \\in \\mathbb{R}^{30 \\times 100}\\) e \\(W_2 \\in \\mathbb{R}^{1 \\times 30}\\). Inoltre, supponiamo di utilizzare la funzione di perdita MSE: \\[\nL(x, y) = (x-y)^2\n\\] Vogliamo calcolare \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_i} \\mbox{ for } i=1,2\n\\] Notare quanto segue: \\[\n\\frac{\\partial L(x, y)}{\\partial x} = 2 \\times (x-y)\n\\] \\[\n\\frac{\\partial ReLU(x)}{\\partial x} \\delta  = \\left\\{\\begin{array}{lr}\n0 & \\text{for } x \\leq 0 \\\\\n1 & \\text{for } x \\geq 0 \\\\\n\\end{array}\\right\\} \\odot \\delta\n\\] \\[\n\\frac{\\partial WA}{\\partial A} \\delta = W^T \\delta\n\\] \\[\n\\frac{\\partial WA}{\\partial W} \\delta = \\delta A^T\n\\] Quindi abbiamo \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_2} = \\frac{\\partial L_2}{\\partial W_2} \\frac{\\partial A_2}{\\partial L_2} \\frac{\\partial L(NN(x), y)}{\\partial A_2}\n\\] \\[\n= (2L(NN(x) - y) \\odot ReLU'(L_2)) A_1^T\n\\] and \\[\n\\frac{\\partial L(NN(x), y)}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial A_1}{\\partial L_1} \\frac{\\partial L_2}{\\partial A_1} \\frac{\\partial A_2}{\\partial L_2} \\frac{\\partial L(NN(x), y)}{\\partial A_2}\n\\] \\[\n= [ReLU'(L_1) \\odot (W_2^T [2L(NN(x) - y) \\odot ReLU'(L_2)])] A_0^T\n\\]\n\n\n\n\n\n\n\nConsiglio\n\n\n\nRicontrollare il lavoro assicurandosi che le forme siano corrette!\n\nTutti i prodotti di Hadamard (\\(\\odot\\)) dovrebbero operare su tensori della stessa forma\nTutte le moltiplicazioni di matrici dovrebbero operare su matrici che condividono una dimensione comune (ad esempio, m per n, n per k)\nTutti i gradienti relativi ai pesi dovrebbero avere la stessa forma delle stesse matrici dei pesi\n\n\n\nL’intero processo di backpropagation può essere complesso, specialmente per reti molto profonde. Fortunatamente, framework di machine learning come PyTorch supportano la differenziazione automatica, che esegue la backpropagation. In questi framework, dobbiamo semplicemente specificare il passaggio in avanti e le derivate ci verranno calcolate automaticamente. Tuttavia, è utile comprendere il processo teorico che avviene internamente in questi framework di apprendimento automatico.\n\n\n\n\n\n\nNota\n\n\n\nCome visto sopra, le attivazioni intermedie \\(A_i\\) vengono riutilizzate nella backpropagation. Per migliorare le prestazioni, queste attivazioni vengono memorizzate nella cache dal passaggio in avanti per evitare di essere ricalcolate. Tuttavia, le attivazioni devono essere mantenute in memoria tra i passaggi in avanti e indietro, il che comporta un maggiore utilizzo della memoria. Se la rete e le dimensioni del batch sono grandi, ciò potrebbe causare problemi di memoria. Analogamente, le derivate rispetto agli output di ogni layer vengono memorizzate nella cache per evitare il ricalcolo.\n\n\n\n\n\n\n\n\nEsercizio 7.1: Reti Neurali con Backpropagation e Discesa del Gradiente\n\n\n\n\n\nScoprire la matematica dietro le potenti reti neurali! Il deep learning potrebbe sembrare magico, ma è radicato nei principi matematici. In questo capitolo, abbiamo scomposto la notazione delle reti neurali, le funzioni di perdita e la potente tecnica della backpropagation. Ora, prepariamoci a implementare questa teoria con questi notebook Colab. Immergersi nel cuore di come le reti neurali apprendono. Si vedrà la matematica dietro la backpropagation e la discesa del gradiente, aggiornando quei pesi passo dopo passo.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#grafi-del-calcolo-differenziabili",
    "href": "contents/training/training.it.html#grafi-del-calcolo-differenziabili",
    "title": "7  Addestramento dell’IA",
    "section": "7.3 Grafi del Calcolo Differenziabili",
    "text": "7.3 Grafi del Calcolo Differenziabili\nIn generale, la discesa del gradiente stocastico mediante backpropagation può essere eseguita su qualsiasi grafo computazionale che un utente può definire, a condizione che le operazioni del calcolo siano differenziabili. Pertanto, le librerie generiche di deep learning come PyTorch e Tensorflow consentono agli utenti di specificare il loro processo computazionale (ad esempio, reti neurali) come grafo computazionale. La backpropagation viene eseguita automaticamente tramite differenziazione automatica quando la discesa del gradiente stocastico viene eseguita su questi grafi computazionali. Inquadrare l’addestramento dell’IA come un problema di ottimizzazione su grafi di calcolo differenziabili è un modo generale per comprendere cosa sta accadendo internamente con i sistemi di deep learning.\nLa struttura raffigurata in Figura 7.3 mostra un segmento di un grafo computazionale differenziabile. In questo grafo, l’input ‘x’ viene elaborato tramite una serie di operazioni: viene prima moltiplicato per una matrice di pesi ‘W’ (MatMul), poi aggiunto a un bias ‘b’ (Add) e infine passato a una funzione di attivazione, Rectified Linear Unit (ReLU). Questa sequenza di operazioni ci fornisce l’output C. La natura differenziabile del grafo significa che ogni operazione ha un gradiente ben definito. La differenziazione automatica, come implementata nei framework ML, sfrutta questa proprietà per calcolare in modo efficiente i gradienti della perdita rispetto a ciascun parametro nella rete (ad esempio, ‘W’ e ‘b’).\n\n\n\n\n\n\nFigura 7.3: Grafo Computazionale. Fonte: TensorFlow.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#dati-di-training",
    "href": "contents/training/training.it.html#dati-di-training",
    "title": "7  Addestramento dell’IA",
    "section": "7.4 Dati di Training",
    "text": "7.4 Dati di Training\nPer consentire un training efficace della rete neurale, i dati disponibili devono essere suddivisi in set di training, di validazione e di test. Il set di training viene utilizzato per addestrare i parametri del modello. Il set di validazione valuta il modello durante il training per ottimizzare gli iperparametri e prevenire l’overfitting. Il set di test fornisce una valutazione finale imparziale delle prestazioni del modello addestrato.\nMantenere chiare suddivisioni tra training, validation e test con dati rappresentativi è fondamentale per addestrare, ottimizzare e valutare correttamente i modelli per ottenere le migliori prestazioni nel mondo reale. A tal fine, scopriremo le insidie o gli errori comuni che le persone commettono quando creano queste suddivisioni dei dati.\nTabella 7.1 confronta le differenze tra le suddivisioni dei dati di training, validazione e test:\n\n\n\nTabella 7.1: Confronto tra suddivisioni di dati di training, validazione e test.\n\n\n\n\n\n\n\n\n\n\nSuddivisione\nScopo\nDimensioni tipiche\n\n\n\n\nSet di addestramento\nAddestrare i parametri del modello\n60-80% dei dati totali\n\n\nSet di validazione\nValutare il modello durante l’addestramento per ottimizzare gli iperparametri e prevenire l’overfitting\n∼20% dei dati totali\n\n\nSet di test\nFornire una valutazione imparziale del modello finale addestrato\n∼20% dei dati totali\n\n\n\n\n\n\n\n7.4.1 Suddivisioni di Dataset\n\nSet di Training\nIl set di training viene utilizzato per addestrare il modello. È il sottoinsieme più grande, in genere il 60-80% dei dati totali. Il modello vede e impara dai dati di addestramento per fare previsioni. È necessario un set di training sufficientemente grande e rappresentativo affinché il modello apprenda efficacemente i pattern sottostanti.\n\n\nSet di Validazione\nIl set di validazione valuta il modello durante l’addestramento, in genere dopo ogni epoca. Solitamente, il 20% dei dati viene assegnato a questo set. Il modello non impara né aggiorna i suoi parametri in base ai dati di validazione. Vengono usati per ottimizzare gli iperparametri e apportare altre modifiche per migliorare l’addestramento. Il monitoraggio di metriche come perdita e accuratezza sul set di validazione impedisce l’overfitting solo sui dati di addestramento.\n\n\nSet di Test\nIl set di test agisce come un dataset che il modello non ha visto durante l’addestramento. Viene utilizzato per fornire una valutazione imparziale del modello addestrato finale. In genere, il 20% dei dati è riservato ai test. Mantenere un set di test “hold-out” [esterno] è fondamentale per ottenere una stima accurata di come il modello addestrato si comporterebbe su dati non ancora visti del mondo reale. La mancanza di dati dal set di test deve essere evitata a tutti i costi.\nLe proporzioni relative dei set di training, validazione e test possono variare in base alle dimensioni dei dati e all’applicazione. Tuttavia, seguire le linee guida generali per una suddivisione 60/20/20 è un buon punto di partenza. Un’attenta suddivisione dei dati garantisce che i modelli siano adeguatamente addestrati, ottimizzati e valutati per ottenere le prestazioni migliori.\nVideo 7.1 spiega come suddividere correttamente il dataset in set di training, validazione e test, assicurando un processo di training ottimale.\n\n\n\n\n\n\nVideo 7.1: Train/Dev/Test Sets\n\n\n\n\n\n\n\n\n\n7.4.2 Errori e Insidie Comuni\n\nDati di Training Insufficienti\nAssegnare troppo pochi dati al set di training è un errore comune quando si suddividono i dati, il che può avere un impatto significativo sulle prestazioni del modello. Se il set di training è troppo piccolo, il modello non avrà campioni sufficienti per apprendere in modo efficace i veri pattern nei dati. Ciò comporta un’elevata varianza e impedisce al modello di generalizzare bene ai nuovi dati.\nAd esempio, se si addestra un modello di classificazione delle immagini per riconoscere cifre scritte a mano, fornire solo 10 o 20 immagini per classe di cifre sarebbe del tutto inadeguato. Il modello avrebbe bisogno di più esempi per catturare le ampie varianze negli stili di scrittura, rotazioni, larghezze dei tratti e altre varianti.\nCome regola generale, la dimensione del training set dovrebbe essere di almeno centinaia o migliaia di esempi affinché la maggior parte degli algoritmi di apprendimento automatico funzioni in modo efficace. A causa dell’elevato numero di parametri, il set di training spesso deve essere di decine o centinaia di migliaia per le reti neurali profonde, in particolare quelle che utilizzano layer convoluzionali.\nDati di training insufficienti si manifestano in genere in sintomi quali alti tassi di errore su set di validazione/test, bassa accuratezza del modello, alta varianza e overfitting su campioni di set di training di piccole dimensioni. La soluzione è raccogliere più dati di training di qualità. Le tecniche di data augmentation possono anche aiutare ad aumentare virtualmente le dimensioni dei dati di training per immagini, audio, ecc.\nÈ importante considerare attentamente la complessità del modello e la difficoltà del problema quando si assegnano i campioni di training per garantire che siano disponibili dati sufficienti affinché il modello possa apprendere correttamente. Si consiglia inoltre di seguire le linee guida sulle dimensioni minime dei set di training per diversi algoritmi. Sono necessari più dati di training per mantenere il successo complessivo di qualsiasi applicazione di machine learning.\nSi consideri Figura 7.4 dove proviamo a classificare/suddividere i dati in due categorie (qui, per colore): a sinistra, l’overfitting è rappresentato da un modello che ha appreso troppo bene le sfumature nei dati di training (o il set di dati era troppo piccolo o abbiamo eseguito il modello per troppo tempo), facendo sì che segua il rumore insieme al segnale, come indicato dalle eccessive curve della linea. Il lato destro mostra l’underfitting, dove la semplicità del modello gli impedisce di catturare la struttura sottostante del dataset, con conseguente linea che non si adatta bene ai dati. Il grafico centrale rappresenta un adattamento ideale, dove il modello bilancia bene tra generalizzazione e adattamento, catturando la tendenza principale dei dati senza essere influenzato da valori anomali. Sebbene il modello non sia un adattamento perfetto (manca di alcuni punti), ci interessa di più la sua capacità di riconoscere modelli generali piuttosto che valori anomali idiosincratici.\n\n\n\n\n\n\nFigura 7.4: Adattamento dei dati: overfitting, right fit e underfitting. Fonte: MathWorks.\n\n\n\nFigura 7.5 il processo di adattamento dei dati nel tempo. Durante l’addestramento, cerchiamo il “punto ottimale” tra underfitting e overfitting. Inizialmente, quando il modello non ha avuto abbastanza tempo per apprendere i pattern nei dati, ci troviamo nella zona di underfitting, indicata da alti tassi di errore sul set di convalida (da ricordare che il modello è addestrato sul set di addestramento e testiamo la sua generalizzabilità sul set di convalida o sui dati che non ha mai visto prima). A un certo punto, raggiungiamo un minimo globale per i tassi di errore e idealmente vogliamo interrompere l’addestramento lì. Se continuiamo l’addestramento, il modello inizierà a “memorizzare” o a conoscere i dati troppo bene, tanto che il tasso di errore inizierà a risalire, poiché il modello non riuscirà a generalizzare a dati che non ha mai visto prima.\n\n\n\n\n\n\nFigura 7.5: Adattamento dei dati nel tempo. Fonte: IBM.\n\n\n\nIl Video 7.2 fornisce una panoramica di bias e varianza e la relazione tra i due concetti e l’accuratezza del modello.\n\n\n\n\n\n\nVideo 7.2: Bias/Varianza\n\n\n\n\n\n\n\n\nPerdita di Dati Tra Set\nIl “data leakage” [perdita di dati] si riferisce al trasferimento involontario di informazioni tra i set di training, convalida e test. Ciò viola il presupposto fondamentale che le divisioni siano reciprocamente esclusive. La perdita di dati porta a risultati di valutazione seriamente compromessi e metriche di prestazioni gonfiate.\nUn modo comune in cui si verifica la perdita di dati è se alcuni campioni del set di test vengono inavvertitamente inclusi nei dati di training. Quando si valuta il set di test, il modello ha già visto alcuni dati, il che fornisce punteggi eccessivamente ottimistici. Ad esempio, se il 2% dei dati di test trapelano nel set di training di un classificatore binario, può comportare un aumento della precisione fino al 20%!\nSe le divisioni dei dati non vengono eseguite con attenzione, possono verificarsi forme di perdita più sottili. Se le divisioni non vengono randomizzate e mescolate correttamente, i campioni che sono vicini tra loro nel set di dati potrebbero finire nella stessa divisione, portando a distorsioni della distribuzione. Ciò crea una fuga di informazioni basata sulla prossimità nel set di dati.\nUn altro caso è quando i set di dati hanno campioni collegati, intrinsecamente connessi, come grafici, reti o dati di serie temporali. La suddivisione “ingenua” può isolare nodi o intervalli di tempo connessi in set diversi. I modelli possono fare ipotesi non valide basate su informazioni parziali.\nPer prevenire la perdita di dati è necessario creare una solida separazione tra le suddivisioni: nessun campione dovrebbe esistere in più di una suddivisione. Il mescolamento e la suddivisione randomizzata aiutano a creare divisioni robuste. Le tecniche di “cross-validation” [validazione incrociata] possono essere utilizzate per una valutazione più rigorosa. Rilevare la perdita è difficile, ma i segnali rivelatori includono modelli che funzionano molto meglio sui dati di test rispetto a quelli di validazione.\nLa perdita di dati compromette gravemente la validità della valutazione perché il modello ha già visto parzialmente i dati di test. Nessuna quantità di messa a punto o architetture complesse può sostituire le suddivisioni nette dei dati. È meglio essere prudenti e creare una separazione completa tra le suddivisioni per evitare questo errore fondamentale nelle pipeline di machine learning.\n\n\nSet di Validazione Piccolo o Non Rappresentativo\nIl set di validazione viene utilizzato per valutare le prestazioni del modello durante l’addestramento e per ottimizzare gli iperparametri. Per valutazioni affidabili e stabili, il set di validazione deve essere sufficientemente ampio e rappresentativo della distribuzione dei dati reali. Tuttavia, ciò può rendere più impegnativa la selezione e l’ottimizzazione del modello.\nAd esempio, se il set di validazione contiene solo 100 campioni, le metriche calcolate avranno un’elevata varianza. A causa del rumore, l’accuratezza può variare fino al 5-10% tra le epoche. Questo rende difficile sapere se un calo nell’accuratezza della validazione è dovuto a un overfitting o a una varianza naturale. Con un set di validazione più ampio, diciamo 1000 campioni, le metriche saranno molto più stabili.\nInoltre, se il set di validazione non è rappresentativo, forse mancano alcune sottoclassi, la capacità stimata del modello potrebbe essere gonfiata. Ciò potrebbe portare a scelte di iperparametri scadenti o a interruzioni premature dell’addestramento. I modelli selezionati in base a tali set di validazione distorti non si generalizzano bene ai dati reali.\nUna buona regola pratica è che la dimensione del set di convalida dovrebbe essere di almeno diverse centinaia di campioni e fino al 10-20% del set di addestramento, lasciando comunque campioni sufficienti per l’addestramento. Le divisioni dovrebbero anche essere stratificate, il che significa che le proporzioni di classe nel set di validazione dovrebbero corrispondere a quelle nel set di dati completo, soprattutto se si lavora con set di dati sbilanciati. Un set di validazione più ampio che rappresenti le caratteristiche dei dati originali è essenziale per una corretta selezione e messa a punto del modello.\n\n\nRiutilizzo del Set di Test Più Volte\nIl set di test è progettato per fornire una valutazione imparziale del modello completamente addestrato solo una volta alla fine del processo di sviluppo del modello. Riutilizzare il set di test più volte durante lo sviluppo per la valutazione del modello, la messa a punto degli iperparametri, la selezione del modello, ecc., può causare un overfitting sui dati di test. Invece, si deve riservare il set di test per una valutazione finale del modello completamente addestrato, trattandolo come una scatola nera per simularne le prestazioni su dati reali. Questo approccio fornisce metriche affidabili per determinare se il modello è pronto per la distribuzione in produzione.\nSe il set di test viene riutilizzato come parte del processo di validazione, il modello potrebbe iniziare a vedere e imparare dai campioni di test. Questo, insieme all’ottimizzazione intenzionale o meno delle prestazioni del modello sul set di test, può gonfiare artificialmente metriche come l’accuratezza.\nAd esempio, supponiamo che il set di test venga utilizzato ripetutamente per la selezione del modello su 5 architetture. In tal caso, il modello potrebbe raggiungere il 99% di accuratezza del test memorizzando i campioni anziché apprendere modelli generalizzabili. Tuttavia, quando implementati nel mondo reale, l’accuratezza dei nuovi dati potrebbe scendere del 60%.\nLa prassi migliore è interagire con il set di test solo una volta alla fine per segnalare metriche imparziali su come il modello finale ottimizzato si comporterebbe nel mondo reale. Durante lo sviluppo del modello, il set di convalida dovrebbe essere utilizzato per tutte le attività di ottimizzazione dei parametri, selezione del modello, arresto anticipato e simili. È importante riservare una parte, come il 20-30% dell’intero set di dati, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l’ottimizzazione o la selezione del modello durante lo sviluppo.\nNon mantenere un set “hold-out” non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrità finale sull’efficacia nel mondo reale. Mantenere la completa separazione di addestramento/validazione dal set di test è essenziale per ottenere stime accurate delle prestazioni del modello. Anche piccole deviazioni da un singolo utilizzo del set di test potrebbero falsare positivamente i risultati e le metriche, fornendo una visione eccessivamente ottimistica dell’efficacia nel mondo reale.\n\n\nStesse Suddivisioni dei Dati negli Esperimenti\nQuando si confrontano diversi modelli di machine learning o si sperimentano varie architetture e iperparametri, utilizzare le stesse suddivisioni dei dati per l’addestramento, la validazione e il test nei diversi esperimenti può introdurre distorsioni e invalidare le comparazioni.\nSe le stesse suddivisioni vengono riutilizzate, i risultati della valutazione potrebbero essere più bilanciati e misurare accuratamente quale modello funziona meglio. Ad esempio, una certa suddivisione casuale dei dati potrebbe favorire il modello A rispetto al modello B indipendentemente dagli algoritmi. Riutilizzare questa suddivisione causerà quindi distorsioni a favore del modello A.\nInvece, le suddivisioni dei dati dovrebbero essere randomizzate o mescolate per ogni iterazione sperimentale. Ciò garantisce che la casualità nel campionamento delle suddivisioni non conferisca un vantaggio ingiusto a nessun modello.\nCon diverse suddivisioni per esperimento, la valutazione diventa più solida. Ogni modello viene testato su un’ampia gamma di set di test estratti casualmente dalla popolazione complessiva, attenuando la variazione e rimuovendo la correlazione tra i risultati.\nLa prassi corretta è quella di impostare un “seed” casuale prima di suddividere i dati per ogni esperimento. La suddivisione dovrebbe avvenire dopo il rimescolamento/ricampionamento come parte della pipeline sperimentale. Eseguire confronti sulle stesse suddivisioni viola l’ipotesi i.i.d (indipendenti e identicamente distribuite) richiesta per la validità statistica.\nLe suddivisioni univoche sono essenziali per confronti di modelli equi. Sebbene richieda un’elaborazione più intensiva, l’allocazione randomizzata per esperimento rimuove la distorsione del campionamento e consente un benchmarking valido. Ciò evidenzia le vere differenze nelle prestazioni del modello indipendentemente dalle caratteristiche di una particolare suddivisione.\n\n\nMancata Stratificazione delle Suddivisioni\nQuando si suddividono i dati in set di training, validazione e test, la mancata stratificazione delle suddivisioni può comportare una rappresentazione non uniforme delle classi target tra le suddivisioni e introdurre un bias di campionamento. Ciò è particolarmente problematico per i set di dati sbilanciati.\nLa suddivisione stratificata implica il campionamento dei dati in modo che la proporzione di classi di output sia approssimativamente preservata in ogni suddivisione. Ad esempio, se si esegue una suddivisione training-test 70/30 su un set di dati con campioni negativi al 60% e positivi al 40%, la stratificazione garantisce esempi negativi al ~60% e positivi al ~40% sia nei set di training che nei set di test.\nSenza stratificazione, la casualità potrebbe comportare che la suddivisione di training abbia campioni positivi al 70% mentre il test ha campioni positivi al 30%. Il modello addestrato su questa distribuzione di training distorta non si generalizzerà bene. Lo squilibrio delle classi compromette anche le metriche del modello come l’accuratezza.\nLa stratificazione funziona meglio quando viene eseguita utilizzando etichette, sebbene proxy come il clustering possano essere utilizzati per l’apprendimento non supervisionato. Diventa essenziale per set di dati altamente distorti con classi rare che potrebbero essere facilmente omesse dalle suddivisioni.\nLibrerie come Scikit-Learn hanno metodi di suddivisione stratificati nativi. Non utilizzarli potrebbe inavvertitamente introdurre bias di campionamento e danneggiare le prestazioni del modello sui gruppi minoritari. Dopo aver eseguito le suddivisioni, il bilanciamento complessivo delle classi dovrebbe essere esaminato per garantire una rappresentazione uniforme tra le suddivisioni.\nLa stratificazione fornisce un set di dati bilanciato sia per l’addestramento del modello che per la valutazione. Sebbene la semplice suddivisione casuale sia facile, tenendo conto delle esigenze di stratificazione, specialmente per dati sbilanciati nel mondo reale, si traduce in uno sviluppo e una valutazione del modello più solidi.\n\n\nIgnorare le Dipendenze delle Serie Temporali\nI dati delle serie temporali hanno una struttura temporale intrinseca con osservazioni dipendenti dal contesto passato. Suddividere ingenuamente i dati delle serie temporali in set di training e test senza tenere conto di questa dipendenza porta a perdite di dati e bias di lookahead.\nAd esempio, suddividere semplicemente una serie temporale nel primo 70% di training e nell’ultimo 30% come dati di test contaminerà i dati di training con dati futuri. Il modello può usare queste informazioni per “sbirciare” in avanti durante il training.\nCiò si traduce in una valutazione eccessivamente ottimistica delle prestazioni del modello. Il modello può sembrare che preveda il futuro in modo accurato, ma in realtà ha appreso implicitamente in base ai dati futuri, il che non si traduce in prestazioni nel mondo reale.\nDovrebbero essere utilizzate tecniche di validazione incrociata delle serie temporali appropriate, come il concatenamento in avanti, per preservare l’ordine e la dipendenza. Il set di test dovrebbe contenere solo dati da una finestra temporale futura a cui il modello non è stato esposto per il training.\nNon tenere conto delle relazioni temporali porta a ipotesi di causalità non valide. Se i dati di training contengono dati futuri, il modello potrebbe anche dover imparare come estrapolare ulteriormente le previsioni.\nMantenere il flusso temporale degli eventi ed evitare il bias di lookahead è fondamentale per addestrare e testare correttamente i modelli di serie temporali. Ciò garantisce che possano davvero prevedere modelli futuri e non solo memorizzare i dati di training passati.\n\n\nNessun Dato Non Visto per la Valutazione Finale\nUn errore comune quando si suddividono i dati è non metterne da parte una porzione solo per la valutazione finale del modello completato. Tutti i dati vengono utilizzati per training, validazione e set di test durante lo sviluppo.\nQuesto non lascia dati non visti per ottenere una stima imparziale di come il modello finale ottimizzato si comporterebbe nel mondo reale. Le metriche sul set di test utilizzate durante lo sviluppo potrebbero riflettere solo parzialmente le reali capacità del modello.\nAd esempio, scelte come l’arresto anticipato e l’ottimizzazione degli iperparametri sono spesso ottimizzate in base alle prestazioni del set di test. Questo accoppia il modello ai dati di test. È necessario un set di dati non visto per interrompere questo accoppiamento e ottenere metriche reali del mondo reale.\nLa “best practice” è quella di riservare una parte, come il 20-30% del set di dati completo, esclusivamente per la valutazione finale del modello. Questi dati non dovrebbero essere utilizzati per la convalida, l’ottimizzazione o la selezione del modello durante lo sviluppo.\nIl salvataggio di alcuni dati non visti consente di valutare il modello completamente addestrato come una scatola nera su dati del mondo reale. Questo fornisce metriche affidabili per decidere se il modello è pronto per la distribuzione in produzione.\nNon mantenere un set “hold-out” non visto per la convalida finale rischia di ottimizzare i risultati e trascurare potenziali errori prima del rilascio del modello. Avere alcuni dati nuovi fornisce un controllo di integrità finale sull’efficacia nel mondo reale.\n\n\nSovra-ottimizzazione del Set di Validazione\nIl set di validazione è pensato per guidare il processo di training del modello, non per fungere da dati di training aggiuntivi. L’eccessiva ottimizzazione del set di validazione per massimizzare le metriche delle prestazioni lo tratta più come un set di training secondario, portando a metriche gonfiate e scarsa generalizzazione.\nAd esempio, tecniche come l’ottimizzazione estensiva degli iperparametri o l’aggiunta di incrementi di dati mirati a migliorare l’accuratezza della convalida possono far sì che il modello si adatti troppo ai dati di validazione. Il modello può raggiungere un’accuratezza di validazione del 99% ma solo un’accuratezza di test del 55%.\nAnalogamente, riutilizzare il set di validazione per un arresto anticipato può anche ottimizzare il modello specificamente per quei dati. L’arresto alle migliori prestazioni di validazione sovra-adatta il rumore e le fluttuazioni causate dalle piccole dimensioni di validazione.\nIl set di validazione funge da proxy per ottimizzare e selezionare i modelli. Tuttavia, l’obiettivo rimane massimizzare le prestazioni dei dati del mondo reale, non il set di validazione. Ridurre al minimo la perdita o l’errore sui dati di validazione non si traduce automaticamente in una buona generalizzazione.\nUn buon approccio è quello di mantenere l’uso del set di validazione al minimo: gli iperparametri possono essere regolati grossolanamente prima sui dati di training, ad esempio. Il set di validazione guida il training ma non dovrebbe influenzare o alterare il modello stesso. È uno strumento diagnostico, non di ottimizzazione.\nQuando si valutano le prestazioni sul set di validazione, bisogna fare attenzione a non sovra-adattare. Sono necessari dei compromessi per costruire modelli che funzionino bene sulla popolazione complessiva e non siano eccessivamente regolati sui campioni di validazione.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#algoritmi-di-ottimizzazione",
    "href": "contents/training/training.it.html#algoritmi-di-ottimizzazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.5 Algoritmi di Ottimizzazione",
    "text": "7.5 Algoritmi di Ottimizzazione\nStochastic gradient descent (SGD) è un algoritmo di ottimizzazione semplice ma potente per l’addestramento di modelli di machine learning. Funziona stimando il gradiente della funzione di perdita relativa ai parametri del modello utilizzando un singolo esempio di addestramento e poi aggiornando i parametri nella direzione che riduce la perdita.\nSebbene concettualmente semplice, SGD necessita di alcune aree di miglioramento. Innanzitutto, scegliere un tasso di apprendimento appropriato può essere difficile: troppo piccolo e i progressi sono molto lenti; troppo grande e i parametri possono oscillare e non convergere. In secondo luogo, SGD tratta tutti i parametri in modo uguale e indipendente, il che potrebbe non essere l’ideale in tutti i casi. Infine, SGD vanilla [standard] utilizza solo informazioni sul gradiente di primo ordine, il che si traduce in progressi lenti su problemi mal condizionati.\n\n7.5.1 Ottimizzazioni\nNel corso degli anni, sono state proposte varie ottimizzazioni per accelerare e migliorare l’SGD vanilla. Ruder (2016) fornisce un’eccellente panoramica dei diversi ottimizzatori. In breve, diverse tecniche di ottimizzazione SGD comunemente utilizzate includono:\n\nRuder, Sebastian. 2016. «An overview of gradient descent optimization algorithms». ArXiv preprint abs/1609.04747 (settembre). http://arxiv.org/abs/1609.04747v2.\nMomentum: Accumula un vettore di velocità in direzioni di gradiente persistente attraverso le iterazioni. Ciò aiuta ad accelerare i progressi smorzando le oscillazioni e mantiene i progressi in direzioni coerenti.\nNesterov Accelerated Gradient (NAG): Una variante di momentum che calcola i gradienti in “look ahead” anziché nella posizione del parametro corrente. Questo aggiornamento anticipatorio impedisce l’overshooting mentre il momentum mantiene il progresso accelerato.\nAdagrad: Un algoritmo di velocità di apprendimento adattivo che mantiene una velocità di apprendimento per parametro ridotta proporzionalmente alla somma storica dei gradienti di ciascun parametro. Aiuta a eliminare la necessità di regolare manualmente i tassi di apprendimento (Duchi, Hazan, e Singer 2010).\n\nDuchi, John C., Elad Hazan, e Yoram Singer. 2010. «Adaptive Subgradient Methods for Online Learning and Stochastic Optimization». In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, a cura di Adam Tauman Kalai e Mehryar Mohri, 257–69. Omnipress. http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.\n\nZeiler, Matthew D. 2012. «ADADELTA: An Adaptive Learning Rate Method», dicembre, 119–49. https://doi.org/10.1002/9781118266502.ch6.\nAdadelta: Una modifica ad Adagrad limita la finestra dei gradienti passati accumulati, riducendo così il decadimento aggressivo dei tassi di apprendimento (Zeiler 2012).\nRMSProp: Divide il tasso di apprendimento per una media esponenzialmente decrescente dei gradienti quadrati. Ciò ha un effetto di normalizzazione simile ad Adagrad ma non accumula i gradienti nel tempo, evitando un rapido decadimento dei tassi di apprendimento (Hinton 2017).\n\nHinton, Geoffrey. 2017. «Overview of Minibatch Gradient Descent». University of Toronto; University Lecture.\n\nKingma, Diederik P., e Jimmy Ba. 2014. «Adam: A Method for Stochastic Optimization». A cura di Yoshua Bengio e Yann LeCun, dicembre. http://arxiv.org/abs/1412.6980v9.\nAdam: Combinazione di momentum e rmsprop dove rmsprop modifica il tasso di apprendimento in base alla media delle recenti ampiezze dei gradienti. Mostra un progresso iniziale molto rapido e regola automaticamente le dimensioni dei passi (Kingma e Ba 2014).\nAMSGrad: Una variante di Adam che assicura una convergenza stabile mantenendo il massimo dei gradienti quadratici passati, impedendo al tasso di apprendimento di aumentare durante l’addestramento (Reddi, Kale, e Kumar 2019).\n\nReddi, Sashank J., Satyen Kale, e Sanjiv Kumar. 2019. «On the Convergence of Adam and Beyond». arXiv preprint arXiv:1904.09237, aprile. http://arxiv.org/abs/1904.09237v1.\nTra questi metodi, Adam è ampiamente considerato l’algoritmo di ottimizzazione di riferimento per molte attività di deep-learning. Supera costantemente SGD vanilla in termini di velocità di addestramento e prestazioni. Altri ottimizzatori potrebbero essere più adatti in alcuni casi, in particolare per modelli più semplici.\n\n\n7.5.2 Compromessi\nTabella 7.2 è una tabella di pro e contro per alcuni dei principali algoritmi di ottimizzazione per l’addestramento di reti neurali:\n\n\n\nTabella 7.2: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione.\n\n\n\n\n\n\n\n\n\n\nAlgoritmo\nPro\nContro\n\n\n\n\nMomentum\n\nConvergenza più rapida dovuta all’accelerazione lungo i gradienti\nMinore oscillazione rispetto a SGD vanilla\n\n\nRichiede la messa a punto del parametro momentum\n\n\n\nNesterov Accelerated Gradient (NAG)\n\nPiù veloce dello slancio standard in alcuni casi\nGli aggiornamenti anticipati impediscono il superamento\n\n\nPiù complesso da comprendere intuitivamente\n\n\n\nAdagrad\n\nElimina la necessità di regolare manualmente i tassi di apprendimento\nFunziona bene su gradienti radi\n\n\nIl tasso di apprendimento potrebbe decadere troppo rapidamente su gradienti densi\n\n\n\nAdadelta\n\nDecadimento del tasso di apprendimento meno aggressivo rispetto ad Adagrad\n\n\nAncora sensibile al valore iniziale del tasso di apprendimento\n\n\n\nRMSProp\n\nRegola automaticamente i tassi di apprendimento\nFunziona bene nella pratica\n\n\nNessun aspetto negativo importante\n\n\n\nAdam\n\nCombinazione di momentum e tassi di apprendimento adattivo\nConvergenza efficiente e veloce\n\n\nPrestazioni di generalizzazione leggermente peggiori in alcuni casi\n\n\n\nAMSGrad\n\nMiglioramento di Adam che affronta il problema della generalizzazione\n\n\nNon è stato utilizzato/testato così ampiamente come Adam\n\n\n\n\n\n\n\n\n\n7.5.3 Algoritmi di Benchmarking\nNon esiste un singolo metodo migliore per tutti i tipi di problemi. Ciò significa che abbiamo bisogno di un benchmarking completo per identificare l’ottimizzatore più efficace per set di dati e modelli specifici. Le prestazioni di algoritmi come Adam, RMSProp e Momentum variano a seconda delle dimensioni del batch, dei programmi di apprendimento, dell’architettura del modello, della distribuzione dei dati e della regolarizzazione. Queste variazioni sottolineano l’importanza di valutare ogni ottimizzatore in diverse condizioni.\nPrendiamo ad esempio Adam, che spesso eccelle nelle attività di visione artificiale, a differenza di RMSProp, che potrebbe mostrare una migliore generalizzazione in determinate attività di elaborazione del linguaggio naturale. La forza di Momentum risiede nella sua accelerazione in scenari con direzioni di gradiente coerenti, mentre i tassi di apprendimento adattivo di Adagrad sono più adatti per problemi di gradiente sparso.\nQuesta vasta gamma di interazioni tra ottimizzatori dimostra la difficoltà di dichiarare un singolo algoritmo universalmente superiore. Ogni ottimizzatore ha punti di forza unici, rendendo fondamentale valutare vari metodi per scoprire empiricamente le loro condizioni di applicazione ottimali.\nUn approccio di benchmarking completo dovrebbe valutare la velocità di convergenza e fattori come errore di generalizzazione, stabilità, sensibilità degli iperparametri ed efficienza computazionale, tra gli altri. Ciò comporta il monitoraggio delle curve di apprendimento di training e convalida su più esecuzioni e il confronto degli ottimizzatori su vari set di dati e modelli per comprenderne i punti di forza e di debolezza.\nAlgoPerf, introdotto da Dürr et al. (2021), risponde alla necessità di un sistema di benchmarking robusto. Questa piattaforma valuta le prestazioni dell’ottimizzatore utilizzando criteri quali curve di loss [perdita] di training, errore di generalizzazione, sensibilità agli iperparametri ed efficienza computazionale. AlgoPerf testa vari metodi di ottimizzazione, tra cui Adam, LAMB e Adafactor, su diversi tipi di modelli come CNN e RNN/LSTM su set di dati stabiliti. Utilizza la “containerizzazione” e la raccolta automatica di metriche per ridurre al minimo le incongruenze e consente esperimenti controllati su migliaia di configurazioni, fornendo una base affidabile per confrontare gli ottimizzatori.\n\nDürr, Marc, Gunnar Nissen, Kurt-Wolfram Sühs, Philipp Schwenkenbecher, Christian Geis, Marius Ringelstein, Hans-Peter Hartung, et al. 2021. «CSF Findings in Acute NMDAR and LGI1 Antibody–Associated Autoimmune Encephalitis». Neurology Neuroimmunology &amp; Neuroinflammation 8 (6). https://doi.org/10.1212/nxi.0000000000001086.\nLe informazioni ottenute da AlgoPerf e benchmark simili sono inestimabili per guidare la scelta ottimale o la messa a punto degli ottimizzatori. Abilitando valutazioni riproducibili, questi benchmark contribuiscono a una comprensione più approfondita delle prestazioni di ciascun ottimizzatore, aprendo la strada a innovazioni future e progressi accelerati nel settore.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#ottimizzazione-degli-iperparametri",
    "href": "contents/training/training.it.html#ottimizzazione-degli-iperparametri",
    "title": "7  Addestramento dell’IA",
    "section": "7.6 Ottimizzazione degli Iperparametri",
    "text": "7.6 Ottimizzazione degli Iperparametri\nGli iperparametri sono impostazioni importanti nei modelli di machine learning che incidono notevolmente sulle prestazioni finali dei modelli. A differenza di altri parametri del modello che vengono appresi durante l’addestramento, gli iperparametri vengono specificati dai “data scientist” o dagli ingegneri del machine learning prima dell’addestramento del modello.\nLa scelta dei valori degli iperparametri corretti consente ai modelli di apprendere pattern dai dati in modo efficace. Alcuni esempi di iperparametri chiave negli algoritmi di apprendimento automatico includono:\n\nReti neurali: Velocità di apprendimento, dimensione del batch, numero di unità nascoste, funzioni di attivazione\nMacchine a vettori di supporto: Forza di regolarizzazione, tipo di kernel e parametri\nRandom forest: Numero di alberi, profondità dell’albero\nK-means: Numero di cluster\n\nIl problema è che non ci sono regole pratiche affidabili per scegliere configurazioni ottimali degli iperparametri: in genere si devono provare valori diversi e valutare le prestazioni. Questo processo è chiamato “hyperparameter tuning” ottimizzazione degli iperparametri.\nNei primi anni del moderno deep learning, i ricercatori erano ancora alle prese con problemi di convergenza instabile e lenta. I punti dolenti comuni includevano perdite di training che fluttuavano selvaggiamente, gradienti che esplodevano o svanivano e un’ampia serie di tentativi ed errori necessari per addestrare le reti in modo affidabile. Di conseguenza, un punto focale iniziale era l’utilizzo di iperparametri per controllare l’ottimizzazione del modello. Ad esempio, tecniche seminali come la normalizzazione batch consentivano una convergenza più rapida del modello regolando gli aspetti dello spostamento interno delle covariate. I metodi di velocità di apprendimento adattivo hanno anche mitigato la necessità di estese pianificazioni manuali. Questi affrontavano problemi di ottimizzazione durante l’addestramento, come la divergenza incontrollata del gradiente. Le velocità di apprendimento adattate con attenzione sono anche il fattore di controllo primario per ottenere una convergenza rapida e stabile anche oggi.\nCon l’espansione esponenziale della capacità computazionale negli anni successivi, modelli molto più grandi potevano essere addestrati senza cadere preda di problemi di pura ottimizzazione numerica. L’attenzione si è spostata verso la generalizzazione, sebbene una convergenza efficiente fosse un prerequisito fondamentale. Tecniche all’avanguardia come “Transformers” hanno introdotto miliardi di parametri. A tali dimensioni, gli iperparametri relativi a capacità, regolarizzazione, ensembling [raggruppamento], ecc., hanno assunto un ruolo centrale per la messa a punto, anziché solo le metriche di convergenza grezze.\nLa lezione è che comprendere l’accelerazione e la stabilità del processo di ottimizzazione stesso costituisce il lavoro di base. Schemi di inizializzazione, dimensioni dei batch, decadimenti di peso e altri iperparametri di training rimangono indispensabili oggi. Dominare una convergenza rapida e impeccabile consente ai professionisti di espandere la propria attenzione sulle esigenze emergenti relative alla messa a punto di parametri quali accuratezza, robustezza ed efficienza su larga scala.\n\n7.6.1 Algoritmi di Ricerca\nQuando si tratta del processo critico di ottimizzazione degli iperparametri, ci sono diversi algoritmi sofisticati su cui gli specialisti del machine learning si affidano per cercare sistematicamente nel vasto spazio di possibili configurazioni dei modelli. Alcuni degli algoritmi di ricerca degli iperparametri più importanti includono:\n\nGrid Search: Il metodo di ricerca più elementare, in cui si definisce manualmente una griglia di valori da controllare per ogni iperparametro. Ad esempio, controllando velocità di apprendimento = [0.01, 0.1, 1] e dimensioni batch = [32, 64, 128]. Il vantaggio principale è la semplicità, ma può portare a un’esplosione esponenziale nello spazio di ricerca, rendendolo dispendioso in termini di tempo. È più adatto per l’ottimizzazione di un piccolo numero di parametri.\nRandom Search: Invece di definire una griglia, si selezionano casualmente valori per ogni iperparametro da un intervallo o set predefinito. Questo metodo è più efficiente nell’esplorazione di un vasto spazio di iperparametri perché non richiede una ricerca esaustiva. Tuttavia, potrebbe comunque non trovare parametri ottimali poiché non esplora sistematicamente tutte le possibili combinazioni.\nBayesian Optimization: Questo è un approccio probabilistico avanzato per l’esplorazione adattiva basato su una funzione surrogata per modellare le prestazioni su iterazioni. È semplice ed efficiente: trova iperparametri altamente ottimizzati in meno passaggi di valutazione. Tuttavia, richiede un maggiore investimento nella configurazione (Snoek, Larochelle, e Adams 2012).\nEvolutionary Algorithms: Questi algoritmi imitano i principi della selezione naturale. Generano popolazioni di combinazioni di iperparametri e le evolvono nel tempo in base alle prestazioni. Questi algoritmi offrono solide capacità di ricerca più adatte per superfici di risposta complesse. Tuttavia, sono necessarie molte iterazioni per una convergenza ragionevole.\nPopulation Based Training (PBT): Un metodo che ottimizza gli iperparametri addestrando più modelli in parallelo, consentendo loro di condividere e adattare configurazioni di successo durante l’addestramento, combinando elementi di ricerca casuale e algoritmi evolutivi (Jaderberg et al. 2017).\nNeural Architecture Search: Un approccio alla progettazione di architetture ad alte prestazioni per reti neurali. Tradizionalmente, gli approcci NAS utilizzano una qualche forma di apprendimento di rinforzo per proporre architetture di reti neurali, che vengono poi ripetutamente valutate (Zoph e Le 2016).\n\n\nSnoek, Jasper, Hugo Larochelle, e Ryan P. Adams. 2012. «Practical Bayesian Optimization of Machine Learning Algorithms». In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, a cura di Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, e Kilian Q. Weinberger, 2960–68. https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html.\n\nJaderberg, Max, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, et al. 2017. «Population Based Training of Neural Networks». arXiv preprint arXiv:1711.09846, novembre. http://arxiv.org/abs/1711.09846v2.\n\nZoph, Barret, e Quoc V. Le. 2016. «Neural Architecture Search with Reinforcement Learning», novembre, 367–92. https://doi.org/10.1002/9781394217519.ch17.\n\n\n7.6.2 Implicazioni di Sistema\nLa messa a punto degli iperparametri può avere un impatto significativo sul tempo di convergenza durante l’addestramento del modello, influenzando direttamente il runtime complessivo. I valori corretti per gli iperparametri chiave di training sono cruciali per un’efficiente convergenza del modello. Ad esempio, la velocità di apprendimento dell’iperparametro controlla la dimensione del passo durante l’ottimizzazione della discesa del gradiente. Impostando correttamente uno scheduling della velocità di apprendimento assicura che l’algoritmo di ottimizzazione converga rapidamente verso un buon minimo. Una velocità di apprendimento troppo bassa porta a una convergenza dolorosamente lenta, mentre un valore troppo grande causa una fluttuazione selvaggia delle perdite. Una messa a punto corretta assicura un rapido movimento verso pesi e bias ottimali.\nAnalogamente, la dimensione del batch per la discesa del gradiente stocastica influisce sulla stabilità della convergenza. La giusta dimensione del batch attenua le fluttuazioni negli aggiornamenti dei parametri per avvicinarsi più rapidamente al minimo. Sono necessarie più dimensioni del batch per evitare una convergenza rumorosa, mentre le dimensioni maggiori del batch non riescono a generalizzare e rallentano la convergenza a causa di aggiornamenti dei parametri meno frequenti. La messa a punto degli iperparametri per una convergenza più rapida e una durata di addestramento ridotta ha implicazioni dirette sui costi e sui requisiti di risorse per il ridimensionamento dei sistemi di machine learning:\n\nCosti computazionali inferiori: Tempi di convergenza più brevi significano costi computazionali inferiori per i modelli di training. Il training ML sfrutta spesso grandi istanze di cloud computing come cluster GPU e TPU che comportano pesanti costi orari. Ridurre al minimo i tempi di training riduce direttamente questo costo di noleggio delle risorse, che tende a dominare i budget ML per le organizzazioni. Un’iterazione più rapida consente inoltre agli esperti di dati di sperimentare più liberamente all’interno dello stesso budget.\nTempo di training ridotto: Un tempo di training ridotto sblocca opportunità per addestrare più modelli utilizzando lo stesso budget computazionale. Gli iperparametri ottimizzati estendono ulteriormente le risorse disponibili, consentendo alle aziende di sviluppare e sperimentare più modelli con vincoli di risorse per massimizzare le prestazioni.\nEfficienza delle risorse: Un training più rapido consente di allocare istanze di calcolo più piccole nel cloud poiché i modelli richiedono l’accesso alle risorse per una durata più breve. Ad esempio, un job di training di un’ora consente di utilizzare istanze GPU meno potenti rispetto a un training di più ore, che richiede un accesso di elaborazione sostenuto su intervalli più lunghi. Ciò consente di risparmiare sui costi, soprattutto per carichi di lavoro di grandi dimensioni.\n\nCi sono anche altri vantaggi. Ad esempio, una convergenza più rapida riduce la pressione sui team di ingegneria ML in merito al provisioning delle risorse di training. Le semplici routine di riaddestramento del modello possono utilizzare risorse meno potenti anziché richiedere l’accesso a code ad alta priorità per cluster GPU di livello di produzione vincolati, liberando risorse di distribuzione per altre applicazioni.\n\n\n7.6.3 Gli Auto Tuner\nData la sua importanza, esiste un’ampia gamma di offerte commerciali per aiutare con l’ottimizzazione degli iperparametri. Toccheremo brevemente due esempi: uno incentrato sull’ottimizzazione per ML su scala cloud e l’altro per modelli di apprendimento automatico mirati ai microcontrollori. Tabella 7.3 delinea le principali differenze:\n\n\n\nTabella 7.3: Confronto di piattaforme di ottimizzazione per diversi casi d’uso di machine learning.\n\n\n\n\n\n\n\n\n\n\n\nPiattaforma\nCaso d’Uso Target\nTecniche di ottimizzazione\nVantaggi\n\n\n\n\nVertex AI di Google\nApprendimento automatico su scala cloud\nOttimizzazione bayesiana, addestramento Population-Based\nNasconde la complessità, consentendo modelli rapidi e pronti per l’implementazione con ottimizzazione iperparametrica all’avanguardia\n\n\nEON Tuner di Edge Impulse\nModelli di microcontrollori (TinyML)\nOttimizzazione bayesiana\nAdatta i modelli per dispositivi con risorse limitate, semplifica l’ottimizzazione per l’implementazione embedded\n\n\n\n\n\n\n\nBigML\nSono disponibili diverse piattaforme commerciali di auto-tuning per risolvere questo problema. Una soluzione è Vertex AI Cloud di Google, che offre un ampio supporto integrato per tecniche di ottimizzazione all’avanguardia.\nUna delle funzionalità più importanti della piattaforma di apprendimento automatico gestita da Vertex AI di Google è l’ottimizzazione efficiente e integrata degli iperparametri per lo sviluppo del modello. Per addestrare con successo modelli ML performanti è necessario identificare configurazioni ottimali per un set di iperparametri esterni che determinano il comportamento del modello, ponendo un problema di ricerca ad alta dimensione impegnativo. Vertex AI mira a semplificare questo problema tramite strumenti di Automated Machine Learning (AutoML).\nIn particolare, gli scienziati dei dati possono sfruttare i motori di ottimizzazione degli iperparametri di Vertex AI fornendo un set di dati etichettato e scegliendo un tipo di modello come un classificatore di reti neurali o Random Forest. Vertex avvia un job di “Hyperparameter Search” in modo trasparente sul backend, gestendo completamente il provisioning delle risorse, l’addestramento del modello, il monitoraggio delle metriche e l’analisi dei risultati automaticamente utilizzando algoritmi di ottimizzazione avanzati.\nInternamente, Vertex AutoML impiega varie strategie di ricerca per esplorare in modo intelligente le configurazioni di iperparametri più promettenti in base ai risultati delle valutazioni precedenti. Tra queste, l’ottimizzazione bayesiana è offerta in quanto fornisce un’efficienza di campionamento superiore, richiedendo meno iterazioni di training per ottenere una qualità del modello ottimizzata rispetto ai metodi standard di Grid Search o di Random Search. Per spazi di ricerca di architettura neurale più complessi, Vertex AutoML utilizza il Population-Based Training, che addestra simultaneamente più modelli e regola dinamicamente i loro iperparametri sfruttando le prestazioni di altri modelli nella popolazione, analogamente ai principi di selezione naturale.\nVertex AI mira a democratizzare le tecniche di ricerca di iperparametri all’avanguardia su scala cloud per tutti gli sviluppatori ML, astraendo l’orchestrazione sottostante e la complessità di esecuzione. Gli utenti si concentrano esclusivamente sul loro set di dati, sui requisiti del modello e sugli obiettivi di accuratezza, mentre Vertex gestisce il ciclo di ottimizzazione, l’allocazione delle risorse, il training del modello, il monitoraggio dell’accuratezza e l’archiviazione degli artefatti internamente. Il risultato è ottenere modelli ML ottimizzati e pronti per la distribuzione più velocemente per il problema target.\n\n\nTinyML\nEdge Impulse’s Efficient On-device Neural Network Tuner (EON Tuner) è uno strumento di ottimizzazione automatizzata degli iperparametri progettato per sviluppare modelli di apprendimento automatico per microcontrollori. Semplifica il processo di sviluppo del modello trovando automaticamente la migliore configurazione di rete neurale per un’implementazione efficiente e accurata su dispositivi con risorse limitate.\nLa funzionalità chiave di EON Tuner è la seguente. Innanzitutto, gli sviluppatori definiscono gli iperparametri del modello, come numero di layer, nodi per layer, funzioni di attivazione e pianificazione della velocità di “annealing” [https://it.wikipedia.org/wiki/Ricottura_simulata] dell’apprendimento. Questi parametri costituiscono lo spazio di ricerca che verrà ottimizzato. Successivamente, viene selezionata la piattaforma del microcontrollore target, fornendo vincoli hardware embedded. L’utente può anche specificare obiettivi di ottimizzazione, come la riduzione dell’ingombro di memoria, la riduzione della latenza, la riduzione del consumo energetico o la massimizzazione della precisione.\nCon lo spazio di ricerca definito e gli obiettivi di ottimizzazione, EON Tuner sfrutta l’ottimizzazione degli iperparametri bayesiani per esplorare in modo intelligente possibili configurazioni. Ogni configurazione potenziale viene automaticamente implementata come specifica di modello completa, addestrata e valutata per metriche di qualità. Il processo continuo bilancia esplorazione e sfruttamento per arrivare a impostazioni ottimizzate su misura per l’architettura del chip scelta dallo sviluppatore e i requisiti di prestazioni.\nEON Tuner libera gli esperti di machine learning dal processo iterativo esigente di messa a punto manuale dei modelli, regolando automaticamente i modelli per il deployment embedded. Lo strumento si integra perfettamente nel flusso di lavoro Edge Impulse, portando i modelli dal concetto a implementazioni ottimizzate in modo efficiente sui microcontrollori. L’esperienza racchiusa in EON Tuner per quanto riguarda l’ottimizzazione del modello ML per i microcontrollori garantisce che sia gli sviluppatori principianti che quelli esperti possano rapidamente iterare per ottenere modelli adatti alle esigenze del loro progetto.\n\n\n\n\n\n\nEsercizio 7.2: Ottimizzazione degli Iperparametri\n\n\n\n\n\nPrepariamoci a scoprire i segreti della messa a punto degli iperparametri e portiamo i modelli PyTorch al livello successivo! Gli iperparametri sono come i quadranti e le manopole nascosti che controllano i superpoteri di apprendimento del modello. In questo notebook Colab, si collaborerà con Ray Tune per trovare le combinazioni perfette di iperparametri. Scopriamo come definire quali valori cercare, impostare il codice di training per l’ottimizzazione e lasciare che Ray Tune faccia il grosso del lavoro. Alla fine, si diventerà professionisti della messa a punto degli iperparametri!\n\n\n\n\nVideo 7.3 spiega l’organizzazione sistematica del processo di ottimizzazione degli iperparametri.\n\n\n\n\n\n\nVideo 7.3: Iperparametro",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#regolarizzazione",
    "href": "contents/training/training.it.html#regolarizzazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.7 Regolarizzazione",
    "text": "7.7 Regolarizzazione\nLa regolarizzazione è una tecnica critica per migliorare le prestazioni e la generalizzabilità dei modelli di machine learning in impostazioni applicate. Si riferisce alla limitazione matematica o alla penalizzazione della complessità del modello per evitare il sovra-adattamento dei dati di training. Senza regolarizzazione, i modelli ML complessi sono inclini al sovra-adattamento del set di dati e alla memorizzazione di peculiarità e rumore nel set di training anziché all’apprendimento di modelli significativi. Possono raggiungere un’elevata accuratezza di training ma hanno prestazioni scadenti quando valutano nuovi input non ancora visti.\nLa regolarizzazione aiuta ad affrontare questo problema ponendo vincoli che favoriscono modelli più semplici e più generalizzabili che non si agganciano a errori di campionamento. Tecniche come la regolarizzazione L1/L2 penalizzano direttamente valori di parametri elevati durante il training, costringendo il modello a utilizzare i parametri più piccoli che possono spiegare adeguatamente il segnale. Le regole di arresto anticipato interrompono il training quando le prestazioni del set di validazione smettono di migliorare, prima che il modello inizi a sovra-adattarsi.\nUna regolarizzazione appropriata è fondamentale quando si distribuiscono modelli a nuove popolazioni di utenti e ambienti in cui sono probabili cambiamenti di distribuzione. Ad esempio, un modello irregolare di rilevamento delle frodi addestrato presso una banca potrebbe funzionare inizialmente, ma accumulare debiti tecnici nel tempo man mano che emergono nuovi pattern di frode.\nLa regolarizzazione di reti neurali complesse offre anche vantaggi computazionali: modelli più piccoli richiedono meno “data augmentation”, potenza di calcolo e archiviazione dei dati. La regolarizzazione consente anche sistemi di intelligenza artificiale più efficienti, in cui accuratezza, robustezza e gestione delle risorse sono attentamente bilanciate rispetto alle limitazioni del set di addestramento.\nDiverse potenti tecniche di regolarizzazione sono comunemente utilizzate per migliorare la generalizzazione del modello. L’architettura della strategia ottimale richiede la comprensione di come ogni metodo influisce sull’apprendimento e sulla complessità del modello.\n\n7.7.1 L1 e L2\nDue delle forme di regolarizzazione più ampiamente utilizzate sono la regolarizzazione L1 e la L2. Entrambe penalizzano la complessità del modello aggiungendo un termine extra alla funzione di costo ottimizzata durante l’addestramento. Questo termine cresce all’aumentare dei parametri del modello.\nLa regolarizzazione L2, nota anche come “ridge regression” [https://it.wikipedia.org/wiki/Regolarizzazione_di_Tichonov], aggiunge la somma delle grandezze al quadrato di tutti i parametri moltiplicata per un coefficiente α. Questa penalità quadratica riduce i valori dei parametri estremi in modo più aggressivo rispetto alle tecniche L1. L’implementazione richiede solo la modifica della funzione di costo e la messa a punto di α.\n\\[R_{L2}(\\Theta) = \\alpha \\sum_{i=1}^{n}\\theta_{i}^2\\]\nDove:\n\n\\(R_{L2}(\\Theta)\\) - Il termine di regolarizzazione L2 che viene aggiunto alla funzione di costo\n\\(\\alpha\\) - L’iperparametro di regolarizzazione L2 che controlla la forza della regolarizzazione\n\\(\\theta_{i}\\) - L’i-esimo parametro del modello\n\\(n\\) - Il numero di parametri nel modello\n\\(\\theta_{i}^2\\) - Il quadrato di ciascun parametro\n\nE la funzione di costo regolarizzata L2 completa è:\n\\[J(\\theta) = L(\\theta) + R_{L2}(\\Theta)\\]\nDove:\n\n\\(L(\\theta)\\) - La funzione di costo non regolarizzata originale\n\\(J(\\theta)\\) - La nuova funzione di costo regolarizzata\n\nSia la regolarizzazione L1 che L2 penalizzano i pesi elevati nella rete neurale. Tuttavia, la differenza fondamentale tra la regolarizzazione L1 e L2 è che la regolarizzazione L2 penalizza i quadrati dei parametri anziché i valori assoluti. Questa differenza fondamentale ha un impatto considerevole sui pesi regolarizzati risultanti. La regolarizzazione L1, o regressione LASSO [https://it.wikipedia.org/wiki/Regolarizzazione_(matematica)], utilizza la somma assoluta delle grandezze anziché il quadrato moltiplicato per α. La penalizzazione del valore assoluto dei pesi induce scarsità poiché il gradiente degli errori estrapola linearmente mentre i termini dei pesi tendono a zero; questo è diverso dalla penalizzazione del valore al quadrato dei pesi, dove la penalità si riduce man mano che i pesi tendono a 0. Inducendo scarsità nel vettore dei parametri, la regolarizzazione L1 esegue automaticamente la selezione delle feature, impostando i pesi delle feature irrilevanti a zero. A differenza della regolarizzazione L2, la L1 porta alla scarsità poiché i pesi sono impostati su 0; nella regolarizzazione L2, i pesi sono impostati su un valore molto vicino a 0 ma generalmente non raggiungono mai esattamente 0. La regolarizzazione L1 incoraggia la scarsità ed è stata utilizzata in alcuni lavori per addestrare reti sparse che potrebbero essere più efficienti in termini di hardware (Hoefler et al. 2021).\n\nHoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, e Alexandra Peste. 2021. «Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks», gennaio. http://arxiv.org/abs/2102.00554v1.\n\\[R_{L1}(\\Theta) = \\alpha \\sum_{i=1}^{n}||\\theta_{i}||\\]\nDove:\n\n\\(R_{L1}(\\Theta)\\) - Il termine di regolarizzazione L1 che viene aggiunto alla funzione di costo\n\\(\\alpha\\) - L’iperparametro di regolarizzazione L1 che controlla la forza della regolarizzazione\n\\(\\theta_{i}\\) - L’i-esimo parametro del modello\n\\(n\\) - Il numero di parametri nel modello\n\\(||\\theta_{i}||\\) - La norma L1, che assume il valore assoluto di ciascun parametro\n\nE la funzione di costo regolarizzata L1 completa è:\n\\[J(\\theta) = L(\\theta) + R_{L1}(\\Theta)\\]\nDove:\n\n\\(L(\\theta)\\) - La funzione di costo non regolarizzata originale\n\\(J(\\theta)\\) - La nuova funzione di costo regolarizzata\n\nLa scelta tra L1 e L2 dipende dalla complessità del modello prevista e dalla necessità o meno di una selezione di feature intrinseche. Entrambi richiedono una messa a punto iterativa su un set di validazione per selezionare l’iperparametro α ottimale.\nVideo 7.4 e Video 7.5 spiegano come funziona la regolarizzazione.\n\n\n\n\n\n\nVideo 7.4: Regolarizzazione\n\n\n\n\n\n\nVideo 7.5 spiega come la regolarizzazione può aiutare a ridurre l’overfitting del modello per migliorare le prestazioni.\n\n\n\n\n\n\nVideo 7.5: Perché la Regolarizzazione Riduce l’Overfitting\n\n\n\n\n\n\n\n\n7.7.2 Dropout\nUn altro metodo di regolarizzazione ampiamente adottato è “dropout” (Srivastava et al. 2014). Durante l’addestramento, dropout imposta casualmente una frazione \\(p\\) di output del nodo o attivazioni nascoste a zero. Questo incoraggia una maggiore distribuzione delle informazioni su più nodi anziché affidarsi a un piccolo numero di nodi. Al momento della previsione, viene utilizzata l’intera rete neurale, con attivazioni intermedie scalate di \\(1 - p\\) per mantenere le ampiezze di output. Le ottimizzazioni GPU semplificano l’implementazione efficiente di dropout tramite framework come PyTorch e TensorFlow.\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, e Ruslan Salakhutdinov. 2014. «Dropout: a simple way to prevent neural networks from overfitting.» J. Mach. Learn. Res. 15 (1): 1929–58. https://doi.org/10.5555/2627435.2670313.\nSiamo più precisi. Durante l’addestramento con dropout, l’output di ogni nodo \\(a_i\\) viene passato attraverso una maschera di dropout \\(r_i\\) prima di essere utilizzato dal layer successivo:\n\\[ ã_i = r_i \\odot a_i \\]\nDove:\n\n\\(a_i\\) - output del nodo \\(i\\)\n\\(ã_i\\) - output del nodo \\(i\\) dopo il dropout\n\\(r_i\\) - variabile casuale di Bernoulli indipendente con probabilità \\(1 - p\\) di essere 1\n\\(\\odot\\) - moltiplicazione elemento per elemento\n\nPer capire come funziona il dropout, è importante sapere che la maschera di dropout \\(r_i\\) è basata sulle variabili casuali di Bernoulli. Una variabile casuale di Bernoulli assume un valore di 1 con probabilità \\(1-p\\) (mantenendo l’attivazione) e un valore di 0 con probabilità \\(p\\) (dropping [perdendo] l’attivazione). Ciò significa che l’attivazione di ciascun nodo viene mantenuta o eliminata indipendentemente durante l’addestramento. Questa maschera di dropout \\(r_i\\) imposta casualmente una frazione \\(p\\) di attivazioni a 0 durante l’addestramento, costringendo la rete a creare rappresentazioni ridondanti.\nAl momento del test, la maschera di dropout viene rimossa e le attivazioni vengono ridimensionate di \\(1 - p\\) per mantenere le ampiezze di output previste:\n\\[ a_i^{test} = (1 - p)  a_i\\]\nDove:\n\n\\(a_i^{test}\\) - output del nodo al momento del test\n\\(p\\) - la probabilità di effettuare il dropping [eliminare] di un nodo.\n\nL’iperparametro chiave è \\(p\\), la probabilità di eliminare ogni nodo, spesso impostata tra 0.2 e 0.5. Le reti più grandi tendono a trarre vantaggio da un dropout maggiore, mentre le reti più piccole rischiano di non adattarsi se vengono eliminati troppi nodi. Tentativi ed errori combinati con il monitoraggio delle prestazioni di validazione aiutano a regolare il livello di dropout.\nVideo 7.6 discute l’intuizione alla base della tecnica di regolarizzazione del dropout e il suo funzionamento.\n\n\n\n\n\n\nVideo 7.6: Dropout\n\n\n\n\n\n\n\n\n7.7.3 Arresto Anticipato\nL’intuizione alla base di “early stopping” arresto anticipato implica il monitoraggio delle prestazioni del modello su un set di validazione “held-out” [esterno] in epoche di addestramento. Inizialmente, gli aumenti nell’idoneità del set di addestramento accompagnano i guadagni nell’accuratezza della validazione man mano che il modello rileva modelli generalizzabili. Dopo un certo punto, tuttavia, il modello inizia a sovradimensionarsi, agganciandosi a peculiarità e rumore nei dati di addestramento che non si applicano più in generale. Le prestazioni di validazione raggiungono il picco e poi si degradano se l’addestramento continua. Le regole di “arresto anticipato” interrompono l’addestramento a questo picco per evitare il sovradimensionamento. Questa tecnica dimostra come le pipeline ML debbano monitorare il feedback del sistema, non solo massimizzare incondizionatamente le prestazioni su un set di addestramento statico. Lo stato del sistema evolve e gli endpoint ottimali cambiano.\nPertanto, i metodi formali di arresto anticipato richiedono il monitoraggio di una metrica come l’accuratezza o la perdita di validazione dopo ogni epoca. Le curve comuni mostrano rapidi guadagni iniziali che si riducono gradualmente, alla fine raggiungendo un plateau e diminuiscono leggermente man mano che si verifica il sovradimensionamento. Il punto di arresto ottimale è spesso compreso tra 5 e 15 epoche oltre il picco, a seconda dei “patient threshold” [limiti della pazienza!]. Il monitoraggio di più metriche può migliorare il segnale poiché esiste una varianza tra le misure.\nLe semplici regole di arresto anticipato si interrompono immediatamente alla prima degradazione post-picco. Metodi più robusti introducono un parametro di “pazienza”, ovvero il numero di epoche di degradazione consentite prima dell’arresto. Ciò evita di interrompere prematuramente l’addestramento a causa di fluttuazioni transitorie. Le finestre di “pazienza” tipiche vanno da 50 a 200 batch di validazione. Finestre più ampie comportano il rischio di overfitting. Le strategie di ottimizzazione formali possono determinare la “pazienza” ottimale.\n\n\n\n\n\n\nEsercizio 7.3: Regolarizzazione\n\n\n\n\n\nCombattere l’Overfitting: Scoprire i Segreti della Regolarizzazione! L’overfitting è come se il modello memorizzasse le risposte a un test, per poi fallire l’esame reale. Le tecniche di regolarizzazione sono le guide di studio che aiutano il modello a generalizzare e ad affrontare nuovi problemi. In questo notebook Colab, impareremo come ottimizzare i parametri di regolarizzazione per risultati ottimali utilizzando la regolarizzazione L1 e L2, il dropout e l’arresto anticipato.\n\n\n\n\nVideo 7.7 tratta alcuni altri metodi di regolarizzazione che possono ridurre l’overfitting del modello.\n\n\n\n\n\n\nVideo 7.7: Altri Metodi di Regolarizzazione",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#funzioni-di-attivazione",
    "href": "contents/training/training.it.html#funzioni-di-attivazione",
    "title": "7  Addestramento dell’IA",
    "section": "7.8 Funzioni di Attivazione",
    "text": "7.8 Funzioni di Attivazione\nLe funzioni di attivazione svolgono un ruolo cruciale nelle reti neurali. Introducono comportamenti non lineari che consentono alle reti neurali di modellare pattern complessi. Le funzioni di attivazione elemento per elemento vengono applicate alle somme ponderate che arrivano a ciascun neurone nella rete. Senza funzioni di attivazione, le reti neurali sarebbero ridotte a modelli di regressione lineare.\nIdealmente, le funzioni di attivazione possiedono alcune qualità desiderabili:\n\nNon lineari: Consentono di modellare relazioni complesse tramite trasformazioni non lineari della somma degli input.\nDifferenziabili: Devono avere derivate prime ben definite per abilitare la retropropagazione e l’ottimizzazione basata sul gradiente durante l’addestramento.\nLimitazione dell’Intervallo: Limitano il segnale di output, impedendo un’esplosione. Ad esempio, la sigmoide schiaccia gli input a (0,1).\n\nInoltre, proprietà come efficienza computazionale, monotonicità e fluidità rendono alcune attivazioni più adatte di altre in base all’architettura di rete e alla complessità del problema.\nEsamineremo brevemente alcune delle funzioni di attivazione più ampiamente adottate e i loro punti di forza e limiti. Forniremo anche linee guida per la selezione di funzioni appropriate abbinate ai vincoli del sistema ML e alle esigenze dei casi d’uso.\n\n7.8.1 Sigmoide\nL’attivazione sigmoide applica una curva a forma di S schiacciante che lega strettamente l’output tra 0 e 1. Ha la forma matematica:\n\\[ sigmoid(x) = \\frac{1}{1+e^{-x}} \\]\nLa trasformazione esponenziale consente alla funzione di passare gradualmente da quasi 0 a quasi 1 quando l’input passa da molto negativo a molto positivo. L’aumento monotono copre l’intero intervallo (0,1).\nLa funzione sigmoide presenta diversi vantaggi. Fornisce sempre un gradiente uniforme per la retropropagazione e il suo output è limitato tra 0 e 1, il che aiuta a prevenire valori “esplosivi” durante l’addestramento. Inoltre, ha una semplice formula matematica che è facile da calcolare.\nTuttavia, la funzione sigmoide presenta anche alcuni svantaggi. Tende a saturarsi a valori di input estremi, il che può causare la “scomparsa” dei gradienti, rallentando o addirittura interrompendo il processo di apprendimento. Inoltre, la funzione non è centrata sullo zero, il che significa che i suoi output non sono distribuiti simmetricamente attorno allo zero, il che può portare ad aggiornamenti inefficienti durante l’addestramento.\n\n\n7.8.2 Tanh\nAnche Tanh o “tangente iperbolica” assume una forma a S ma è centrata sullo zero, il che significa che il valore medio dell’output è 0.\n\\[ tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\]\nLa trasformazione numeratore/denominatore sposta l’intervallo da (0,1) in Sigmoide a (-1, 1) in tanh.\nLa maggior parte dei pro/contro sono condivisi con la Sigmoide, ma Tanh evita alcuni problemi di saturazione dell’output essendo centrata. Tuttavia, soffre ancora di gradienti che svaniscono con molti layer.\n\n\n7.8.3 ReLU\nLa Rectified Linear Unit (ReLU) introduce un semplice comportamento di soglia con la sua forma matematica:\n\\[ ReLU(x) = max(0, x) \\]\nLascia tutti gli input positivi invariati mentre taglia tutti i valori negativi a 0. Questa attivazione sparsa e il calcolo economico rendono ReLU ampiamente favorito rispetto a sigmoide/tanh.\nFigura 7.6 dimostra le 3 funzioni di attivazione di cui abbiamo discusso sopra in confronto a una funzione lineare:\n\n\n\n\n\n\nFigura 7.6: Funzioni di Attivazione Comuni. Fonte: AI Wiki.\n\n\n\n\n\n7.8.4 Softmax\nLa funzione di attivazione softmax è generalmente utilizzata come ultimo layer per le attività di classificazione per normalizzare il vettore del valore di attivazione in modo che i suoi elementi sommino a 1. Questo è utile per le attività di classificazione in cui vogliamo imparare a prevedere probabilità specifiche per classe di un input particolare, nel qual caso la probabilità cumulativa tra le classi è uguale a 1. La funzione di attivazione softmax è definita come\n\\[\\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K\\]\n\n\n7.8.5 Pro e Contro\nTabella 7.4 sono i pro e i contro riassuntivi di queste varie funzioni di attivazione standard:\n\n\n\nTabella 7.4: Confronto dei pro e dei contro di diversi algoritmi di ottimizzazione.\n\n\n\n\n\n\n\n\n\n\nAttivazione\nPro\nContro\n\n\n\n\nSigmoide\n\nGradiente uniforme per il backdrop [sfondo]\nOutput limitato tra 0 e 1\n\n\nLa saturazione elimina i gradienti\nNon centrato sullo zero\n\n\n\nTanh\n\nGradiente più uniforme della sigmoide\nOutput centrato sullo zero [-1, 1]\n\n\nSoffre ancora di problemi di gradiente evanescente\n\n\n\nReLU\n\nEfficiente dal punto di vista computazionale\nIntroduce la “sparsity” [scarsità]\nEvita gradienti evanescenti\n\n\nUnità “ReLU morenti”\nNon limitato\n\n\n\nSoftmax\n\nUtilizzato per l’ultimo livello per normalizzare gli output in modo che siano una distribuzione\nIn genere utilizzato per attività di classificazione\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsercizio 7.4: Funzioni di Attivazione\n\n\n\n\n\nSblocchiamo la potenza delle funzioni di attivazione! Questi piccoli “muletti” matematici sono ciò che rende le reti neurali così incredibilmente flessibili. In questo notebook Colab, ci si cimenterà con funzioni come Sigmoid, tanh e la superstar ReLU. Guardiamo come trasformano gli input e scopriamo quale funziona meglio in diverse situazioni. È la chiave per costruire reti neurali in grado di affrontare problemi complessi!",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#inizializzazione-dei-pesi",
    "href": "contents/training/training.it.html#inizializzazione-dei-pesi",
    "title": "7  Addestramento dell’IA",
    "section": "7.9 Inizializzazione dei Pesi",
    "text": "7.9 Inizializzazione dei Pesi\nLa corretta inizializzazione dei pesi in una rete neurale prima dell’addestramento è un passaggio fondamentale che ha un impatto diretto sulle prestazioni del modello. L’inizializzazione casuale dei pesi a valori molto grandi o molto piccoli può portare a problemi come gradienti che svaniscono/esplodono, convergenza lenta dell’addestramento o intrappolati in minimi locali scadenti. La corretta inizializzazione del peso accelera la convergenza del modello durante l’addestramento e comporta implicazioni per le prestazioni del sistema al momento dell’inferenza negli ambienti di produzione. Alcuni aspetti chiave sono:\n\nTempo di Accuratezza più Rapido: Un’inizializzazione attentamente calibrata porta a una convergenza più rapida, che si traduce nel raggiungimento da parte dei modelli di traguardi di accuratezza target in anticipo nel ciclo di training. Ad esempio, l’inizializzazione Xavier potrebbe ridurre il tempo di accuratezza del 20% rispetto a un’inizializzazione casuale errata. Poiché l’addestramento è in genere la fase più dispendiosa in termini di tempo e calcolo, ciò migliora direttamente la velocità e la produttività del sistema ML.\nEfficienza del Ciclo di Iterazione del Modello: Se i modelli vengono addestrati più rapidamente, il tempo di risposta complessivo per le iterazioni di sperimentazione, valutazione e progettazione del modello diminuisce in modo significativo. I sistemi hanno maggiore flessibilità per esplorare architetture, pipeline di dati, ecc., entro determinati intervalli di tempo.\nImpatto sulle Epoche di Addestramento Necessarie: Il processo di addestramento viene eseguito per più epoche, con ogni passaggio completo attraverso i dati che rappresenta un’epoca. Una buona inizializzazione può ridurre le epoche necessarie per far convergere le curve di perdita e accuratezza sul set di addestramento del 10-30%. Ciò significa risparmi tangibili sui costi di risorse e infrastruttura.\nEffetto sugli Iperparametri di Addestramento: I parametri di inizializzazione del peso interagiscono fortemente con determinati iperparametri di regolarizzazione che governano le dinamiche di addestramento, come i programmi di velocità di apprendimento e le probabilità di abbandono. Trovare la giusta combinazione di impostazioni non è banale. Un’inizializzazione appropriata semplifica questa ricerca.\n\nL’inizializzazione dei pesi ha vantaggi a cascata per l’efficienza ingegneristica dell’apprendimento automatico e un overhead di risorse di sistema ridotto al minimo. È una tattica facilmente trascurata che ogni professionista dovrebbe padroneggiare. La scelta di quale tecnica di inizializzazione del peso utilizzare dipende da fattori come l’architettura del modello (numero di layer, pattern di connettività, ecc.), le funzioni di attivazione e il problema specifico da risolvere. Nel corso degli anni, i ricercatori hanno sviluppato e verificato empiricamente diverse strategie di inizializzazione mirate alle comuni architetture di reti neurali, di cui parleremo qui.\n\n7.9.1 Inizializzazione Uniforme e Normale\nQuando si inizializzano pesi in modo casuale, vengono comunemente utilizzate due distribuzioni di probabilità standard: uniforme e Gaussiana (normale). La distribuzione uniforme imposta una probabilità uguale che i parametri di peso iniziali rientrino in qualsiasi punto entro i limiti minimi e massimi impostati. Ad esempio, i limiti potrebbero essere -1 e 1, portando a una distribuzione uniforme dei pesi tra questi limiti. La distribuzione gaussiana, d’altra parte, concentra la probabilità attorno a un valore medio, seguendo la forma di una curva a campana. La maggior parte dei valori di peso si raggrupperà nella regione della media specificata, con meno campioni verso le estremità. Il parametro di deviazione standard controlla la distribuzione attorno alla media.\nLa scelta tra inizializzazione uniforme o normale dipende dall’architettura di rete e dalle funzioni di attivazione. Per reti poco profonde, si consiglia una distribuzione normale con una deviazione standard relativamente piccola (ad esempio, 0.01). La curva a campana impedisce valori di peso elevati che potrebbero innescare l’instabilità di addestramento in reti piccole. Per reti più profonde, una distribuzione normale con deviazione standard più elevata (diciamo 0.5 o superiore) o una distribuzione uniforme può essere preferita per tenere conto dei problemi di gradiente evanescente su molti layer. La maggiore diffusione determina una maggiore differenziazione tra i comportamenti dei neuroni. La messa a punto dei parametri di distribuzione di inizializzazione è fondamentale per una convergenza stabile e rapida del modello. Il monitoraggio dei trend di “loss” [perdita] di addestramento può diagnosticare i problemi per modificare i parametri in modo iterativo.\n\n\n7.9.2 Inizializzazione Xavier\nProposta da Glorot e Bengio (2010), questa tecnica di inizializzazione è progettata appositamente per le funzioni di attivazione sigmoide e tanh. Queste attivazioni saturate possono causare gradienti evanescenti o esplosivi durante la retro-propagazione su molti layer.\n\nGlorot, Xavier, e Yoshua Bengio. 2010. «Understanding the difficulty of training deep feedforward neural networks.» In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249–56. http://proceedings.mlr.press/v9/glorot10a.html.\nIl metodo Xavier imposta in modo intelligente la varianza della distribuzione dei pesi in base al numero di input e output per ciascun layer. L’intuizione è che questo bilancia il flusso di informazioni e gradienti in tutta la rete. Ad esempio, si consideri un layer con 300 unità di input e 100 unità di output. Inserendo questo nella formula varianza = 2/(#inputs + #outputs) si ottiene una varianza di 2/(300+100) = 0.01.\nIl campionamento dei pesi iniziali da una distribuzione uniforme o normale centrata su 0 con questa varianza fornisce una convergenza di addestramento molto più fluida per reti sigmoide/tanh profonde. I gradienti sono ben condizionati, impedendo la scomparsa o la crescita esponenziale.\n\n\n7.9.3 Inizializzazione He\nCome proposto da He et al. (2015), questa tecnica di inizializzazione è adattata alle funzioni di attivazione ReLU (Rectified Linear Unit). Le ReLU introducono il problema del neurone morente in cui le unità rimangono bloccate e producono solo 0 se inizialmente ricevono forti input negativi. Ciò rallenta e ostacola l’addestramento.\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2015. «Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification». In 2015 IEEE International Conference on Computer Vision (ICCV), 1026–34. IEEE. https://doi.org/10.1109/iccv.2015.123.\n“He” supera questo problema campionando i pesi da una distribuzione con un set di varianza basato solo sul numero di input per layer, ignorando gli output. Ciò mantiene i segnali in arrivo sufficientemente piccoli da attivare le ReLU nel loro regime lineare dall’inizio, evitando unità morte. Per un layer con 1024 input, la formula varianza = 2/1024 = 0.002 mantiene la maggior parte dei pesi concentrati strettamente attorno a 0.\nQuesta inizializzazione specializzata consente alle reti ReLU di convergere in modo efficiente fin dall’inizio. La scelta tra Xavier e He deve corrispondere alla funzione di attivazione della rete prevista.\n\n\n\n\n\n\nEsercizio 7.5: Inizializzazione dei Pesi\n\n\n\n\n\nFacciamo partire la rete neurale col piede giusto con l’inizializzazione dei pesi! Il modo in cui si impostano quei pesi iniziali può fare la differenza nell’addestramento del modello. Si immagini di accordare gli strumenti di un’orchestra prima del concerto. In questo notebook Colab, si imparerà che la giusta strategia di inizializzazione può far risparmiare tempo, migliorare le prestazioni del modello e rendere il percorso di deep-learning molto più fluido.\n\n\n\n\nVideo 7.8 sottolinea l’importanza di selezionare deliberatamente i valori di peso iniziale rispetto a scelte casuali.\n\n\n\n\n\n\nVideo 7.8: Inizializzazione dei Pesi",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#colli-di-bottiglia-del-sistema",
    "href": "contents/training/training.it.html#colli-di-bottiglia-del-sistema",
    "title": "7  Addestramento dell’IA",
    "section": "7.10 “Colli di Bottiglia” del Sistema",
    "text": "7.10 “Colli di Bottiglia” del Sistema\nCome introdotto in precedenza, le reti neurali comprendono operazioni lineari (moltiplicazioni di matrici) intervallate da funzioni di attivazione non lineari elemento per elemento. La parte computazionalmente più costosa delle reti neurali sono le trasformazioni lineari, in particolare le moltiplicazioni di matrici tra ogni layer. Questi layer lineari mappano le attivazioni dal layer precedente a uno spazio dimensionale superiore che funge da input per la funzione di attivazione del layer successivo.\n\n7.10.1 Complessità a Runtime della Moltiplicazione di Matrici\n\nMoltiplicazioni di Layer vs. Attivazioni\nLa maggior parte del calcolo nelle reti neurali deriva dalle moltiplicazioni di matrici tra layer. Si consideri un layer di rete neurale con una dimensione di input di \\(M\\) = 500 e una dimensione di output di \\(N\\) = 1000; la moltiplicazione di matrici richiede \\(O(N \\cdot M) = O(1000 \\cdot 500) = 500,000\\) operazioni di moltiplicazione-accumulazione (MAC) tra quei layer.\nConfronta questo col layer precedente, che aveva \\(M\\) = 300 input, che richiedevano \\(O(500 \\cdot 300) = 150,000\\) operazioni. Man mano che le dimensioni dei layer aumentano, i requisiti computazionali aumentano quadraticamente con la dimensione del layer. I calcoli totali su \\(L\\) layer possono essere espressi come \\(\\sum_{l=1}^{L-1} O\\big(N^{(l)} \\cdot M^{(l-1)}\\big)\\), dove il calcolo richiesto per ogni layer dipende dal prodotto delle dimensioni di input e output delle matrici che vengono moltiplicate.\nOra, confrontando la moltiplicazione della matrice con la funzione di attivazione, che richiede solo \\(O(N) = 1000\\) non linearità elemento per elemento per \\(N = 1000\\) output, possiamo vedere le trasformazioni lineari che dominano le attivazioni computazionalmente.\nQueste grandi moltiplicazioni di matrici influiscono sulle scelte hardware, sulla latenza dell’inferenza e sui vincoli di potenza per le applicazioni di reti neurali nel mondo reale. Ad esempio, un tipico layer DNN potrebbe richiedere 500,000 moltiplicazioni-accumulazioni rispetto a solo 1000 attivazioni non lineari, dimostrando un aumento di 500x nelle operazioni matematiche.\nQuando si addestrano reti neurali, in genere utilizziamo la discesa del gradiente in mini-batch, operando su piccoli batch di dati contemporaneamente. Considerando una dimensione batch di \\(B\\) esempi di addestramento, l’input per la moltiplicazione di matrice diventa una matrice \\(M \\times B\\), mentre l’output è una matrice \\(N \\times B\\).\n\n\nMini-batch\nNell’addestramento delle reti neurali, dobbiamo stimare ripetutamente il gradiente della funzione di perdita rispetto ai parametri di rete (ad esempio, pesi e bias). Questo gradiente indica in quale direzione i parametri devono essere aggiornati per ridurre al minimo la perdita. Come introdotto in precedenza, eseguiamo aggiornamenti su un batch di dati a ogni aggiornamento, noto anche come discesa del gradiente stocastico o “discesa del gradiente mini-batch”.\nL’approccio più semplice è stimare il gradiente in base a un singolo esempio di addestramento, calcolare l’aggiornamento dei parametri, riassettare tutto e ripetere per l’esempio successivo. Tuttavia, ciò comporta aggiornamenti dei parametri molto piccoli e frequenti che possono essere computazionalmente inefficienti e potrebbero dover essere più accurati in termini di convergenza a causa della stocasticità dell’utilizzo di un solo dato per un aggiornamento del modello.\nInvece, la discesa del gradiente in mini-batch bilancia la stabilità della convergenza e l’efficienza computazionale. Invece di calcolare il gradiente su singoli esempi, stimiamo il gradiente in base a piccoli “mini-batch” di dati, solitamente tra 8 e 256 esempi in pratica.\nCiò fornisce una stima del gradiente rumorosa ma coerente che porta a una convergenza più stabile. Inoltre, l’aggiornamento dei parametri deve essere eseguito solo una volta per mini-batch anziché una volta per ogni esempio, riducendo il sovraccarico computazionale.\nRegolando la dimensione del mini-batch, possiamo controllare il compromesso tra la fluidità della stima (i batch più grandi sono generalmente migliori) e la frequenza degli aggiornamenti (i batch più piccoli consentono aggiornamenti più frequenti). Le dimensioni del mini-batch sono solitamente potenze di 2, quindi possono sfruttare in modo efficiente il parallelismo tra i core GPU.\nQuindi, il calcolo totale esegue una moltiplicazione di matrici \\(N \\times M\\) per \\(M \\times B\\), producendo \\(O(N \\cdot M \\cdot B)\\) operazioni in virgola mobile. Come esempio numerico, \\(N=1000\\) unità nascoste, \\(M=500\\) unità di input e una dimensione del batch \\(B=64\\) equivale a 1000 x 500 x 64 = 32 milioni di moltiplicazioni-accumulazioni per iterazione di training!\nAl contrario, le funzioni di attivazione vengono applicate elemento per elemento alla matrice di output \\(N \\times B\\), richiedendo solo \\(O(N \\cdot B)\\) calcoli. Per \\(N=1000\\) e \\(B=64\\), si tratta di sole 64,000 non linearità, ovvero 500 volte meno lavoro della moltiplicazione di matrici.\nMan mano che aumentiamo le dimensioni del batch per sfruttare appieno hardware parallelo come le GPU, la discrepanza tra la moltiplicazione di matrici e il costo della funzione di attivazione aumenta ulteriormente. Ciò rivela come l’ottimizzazione delle operazioni di algebra lineare offra enormi guadagni di efficienza.\nPertanto, la moltiplicazione di matrici è fondamentale nell’analisi di dove e come le reti neurali impiegano i calcoli. Ad esempio, le moltiplicazioni di matrici spesso rappresentano oltre il 90% della latenza di inferenza e del tempo di addestramento nelle comuni reti neurali convoluzionali e ricorrenti.\n\n\nOttimizzazione della Moltiplicazione di Matrici\nDiverse tecniche migliorano l’efficienza delle operazioni generali matrice-matrice densa/sparsa e matrice-vettore per migliorare l’efficienza complessiva. Alcuni metodi chiave sono:\n\nSfruttamento di librerie matematiche ottimizzate come cuBLAS per l’accelerazione GPU\nAbilitazione di formati di precisione inferiore come FP16 o INT8 dove l’accuratezza lo consente\nUtilizzo di Tensor Processing Unit con moltiplicazione di matrici in hardware\nCalcoli consapevoli della sparsità e formati di archiviazione dati per sfruttare i parametri zero\nApprossimazione delle moltiplicazioni di matrici con algoritmi come le Fast Fourier Transform\nProgettazione dell’architettura del modello per ridurre le larghezze e le attivazioni degli layer\nQuantizzazione, pruning [potatura], distillazione e altre tecniche di compressione\nParallelizzazione del calcolo sull’hardware disponibile\nRisultati di caching/pre-calcolo ove possibile per ridurre le operazioni ridondanti\n\nLe potenziali tecniche di ottimizzazione sono vaste, data la porzione sproporzionata di tempo che i modelli trascorrono nella matematica di matrici e vettori. Anche i miglioramenti incrementali velocizzano i tempi di esecuzione e riducono il consumo di energia. Trovare nuovi modi per migliorare queste primitive di algebra lineare rimane un’area di ricerca attiva allineata con le future esigenze di machine learning. Ne parleremo in dettaglio nei capitoli Ottimizzazioni e Accelerazione IA.\n\n\n\n7.10.2 Calcolo vs. Collo di Bottiglia della Memoria\nA questo punto, la moltiplicazione matrice-matrice è l’operazione matematica fondamentale alla base delle reti neurali. Sia l’addestramento che l’inferenza per le reti neurali utilizzano ampiamente queste operazioni di moltiplicazione di matrici. L’analisi mostra che oltre il 90% dei requisiti computazionali nelle reti neurali attuali derivano da moltiplicazioni di matrici. Di conseguenza, le prestazioni della moltiplicazione di matrici hanno un’enorme influenza sul tempo complessivo di addestramento o inferenza del modello.\n\nAddestramento vs. Inferenza\nMentre l’addestramento e l’inferenza si basano ampiamente sulle prestazioni della moltiplicazione di matrici, i loro profili computazionali precisi differiscono. In particolare, l’inferenza della rete neurale tende a essere più legata al calcolo rispetto all’addestramento per una dimensione di batch equivalente. La differenza fondamentale risiede nel passaggio di backpropagation, che è richiesto solo durante l’addestramento. La backpropagation implica una sequenza di operazioni di moltiplicazione di matrici per calcolare i gradienti rispetto alle attivazioni su ogni layer della rete. Tuttavia, è fondamentale che qui non sia necessaria alcuna larghezza di banda di memoria aggiuntiva: gli input, gli output e i gradienti vengono letti/scritti dalla cache o dai registri.\nDi conseguenza, l’addestramento mostra intensità aritmetiche inferiori, con calcoli del gradiente limitati dall’accesso alla memoria anziché dai FLOP (Floating Point Operations Per Second), una misura delle prestazioni computazionali che indica quanti calcoli in virgola mobile un sistema può eseguire al secondo. Al contrario, la propagazione in avanti domina l’inferenza della rete neurale, che corrisponde a una serie di moltiplicazioni matrice-matrice. Senza una retrospettiva del gradiente che richiede molta memoria, le dimensioni dei batch più grandi spingono facilmente l’inferenza a essere estremamente limitata dal calcolo. Le elevate intensità aritmetiche misurate mostrano questo. I tempi di risposta possono essere critici per alcune applicazioni di inferenza, costringendo il fornitore dell’applicazione a utilizzare una dimensione di batch inferiore per soddisfare questi requisiti di tempo di risposta, riducendo così l’efficienza dell’hardware; quindi, le inferenze potrebbero vedere un utilizzo inferiore dell’hardware.\nLe implicazioni sono che il provisioning hardware e i compromessi tra larghezza di banda e FLOP differiscono a seconda che un sistema miri al training o all’inferenza. I server ad alta produttività e bassa latenza per l’inferenza dovrebbero enfatizzare la potenza di calcolo anziché la memoria, mentre i cluster di training richiedono un’architettura più bilanciata.\nTuttavia, la moltiplicazione di matrici mostra un’interessante tensione: la larghezza di banda della memoria dell’hardware sottostante o le capacità di throughput aritmetico possono vincolarla. La capacità del sistema di recuperare e fornire dati matriciali rispetto alla sua capacità di eseguire operazioni di calcolo determina questa direzione.\nQuesto fenomeno ha impatti profondi; l’hardware deve essere progettato giudiziosamente e devono essere prese in considerazione le ottimizzazioni del software. Ottimizzare e bilanciare il calcolo rispetto alla memoria per alleviare questo collo di bottiglia della moltiplicazione di matrici è fondamentale per un training un deployment efficienti del modello.\nInfine, la dimensione del batch può avere un impatto sui tassi di convergenza durante l’addestramento della rete neurale, un’altra considerazione importante. Ad esempio, ci sono generalmente rendimenti decrescenti nei benefici della convergenza con dimensioni di batch estremamente grandi (ad esempio, &gt; 16384). Al contrario, dimensioni di batch estremamente grandi possono essere sempre più vantaggiose da una prospettiva di intensità hardware/aritmetica; l’utilizzo di batch così grandi potrebbe non tradursi in una convergenza più rapida rispetto al tempo a causa dei loro benefici decrescenti per la convergenza. Questi compromessi fanno parte delle decisioni di progettazione fondamentali per i sistemi per il tipo di ricerca basata sull’apprendimento automatico.\n\n\nDimensione del Batch\nLa dimensione del batch utilizzata durante l’addestramento e l’inferenza della rete neurale ha un impatto significativo sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria. In concreto, la dimensione del batch si riferisce al numero di campioni propagati assieme attraverso la rete in un passaggio avanti/indietro. La moltiplicazione di matrici equivale a dimensioni di matrice maggiori.\nIn particolare, diamo un’occhiata all’intensità aritmetica della moltiplicazione di matrici durante l’addestramento della rete neurale. Questa misura il rapporto tra operazioni computazionali e trasferimenti di memoria. La moltiplicazione di due matrici di dimensione \\(N \\times M\\) e \\(M \\times B\\) richiede \\(N \\times M \\times B\\) operazioni di moltiplicazione-accumulo, ma solo trasferimenti di \\(N \\times M + M \\times B\\) elementi di matrice.\nMan mano che aumentiamo la dimensione del batch \\(B\\), il numero di operazioni aritmetiche cresce più velocemente dei trasferimenti di memoria. Ad esempio, con una dimensione del batch di 1, abbiamo bisogno di \\(N \\times M\\) operazioni e \\(N + M\\) trasferimenti, dando un rapporto di intensità aritmetica di circa \\(\\frac{N \\times M}{N+M}\\). Ma con una dimensione del batch di grandi dimensioni di 128, il rapporto di intensità diventa \\(\\frac{128 \\times N \\times M}{N \\times M + M \\times 128} \\approx 128\\). L’utilizzo di una dimensione del batch più grande sposta il calcolo complessivo da vincolato alla memoria a più vincolato al calcolo. L’addestramento IA utilizza grandi dimensioni del batch ed è generalmente limitato dalle massime prestazioni di calcolo aritmetiche, ovvero l’Applicazione 3 in Figura 7.7.\nPertanto, la moltiplicazione di matrici in batch è molto più intensiva dal punto di vista computazionale rispetto al limite di accesso alla memoria. Ciò ha implicazioni per la progettazione hardware e le ottimizzazioni software, che tratteremo in seguito. L’intuizione chiave è che possiamo modificare in modo significativo il profilo computazionale e i colli di bottiglia posti dall’addestramento e dall’inferenza della rete neurale regolando la dimensione del batch.\n\n\n\n\n\n\nFigura 7.7: Modello a profilo a di tetto per il training di IA.\n\n\n\n\n\nCaratteristiche Hardware\nL’hardware moderno come CPU e GPU è altamente ottimizzato per la produttività computazionale piuttosto che per la larghezza di banda della memoria. Ad esempio, le GPU H100 Tensor Core di fascia alta possono fornire oltre 60 TFLOPS di prestazioni a doppia precisione, ma forniscono solo fino a 3 TB/s di larghezza di banda della memoria. Ciò significa che c’è uno squilibrio di quasi 20 volte tra unità aritmetiche e accesso alla memoria; di conseguenza, per hardware come gli acceleratori GPU, i carichi di lavoro di addestramento della rete neurale devono essere resi il più intensivi possibile dal punto di vista computazionale per utilizzare appieno le risorse disponibili.\nCiò motiva ulteriormente la necessità di utilizzare batch di grandi dimensioni durante l’addestramento. Quando si utilizza un batch di piccole dimensioni, la moltiplicazione della matrice è limitata dalla larghezza di banda della memoria, sottoutilizzando le abbondanti risorse di elaborazione. Tuttavia, possiamo spostare il collo di bottiglia verso l’elaborazione e ottenere un’intensità aritmetica molto più elevata con batch sufficientemente grandi. Ad esempio, potrebbero essere necessari batch di 256 o 512 campioni per saturare una GPU di fascia alta. Lo svantaggio è che batch più grandi forniscono aggiornamenti dei parametri meno frequenti, il che può influire sulla convergenza. Tuttavia, il parametro funge da importante manopola di sintonizzazione per bilanciare le limitazioni di memoria e quelle di elaborazione.\nPertanto, date le architetture di elaborazione-memoria sbilanciate dell’hardware moderno, l’impiego di batch di grandi dimensioni è essenziale per alleviare i colli di bottiglia e massimizzare la produttività. Come accennato, anche il software e gli algoritmi successivi devono adattarsi a tali dimensioni di batch, poiché dimensioni di batch più grandi possono avere rendimenti decrescenti verso la convergenza della rete. L’utilizzo di dimensioni di batch molto piccole può portare a un utilizzo non ottimale dell’hardware, limitando in ultima analisi l’efficienza del training. L’aumento di dimensioni dei batch di grandi dimensioni è un argomento di ricerca esplorato in vari lavori che mirano a eseguire una training su larga scala (You et al. 2017).\n\nYou, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, e Kurt Keutzer. 2017. «ImageNet Training in Minutes», settembre. http://arxiv.org/abs/1709.05011v10.\n\n\nArchitetture dei Modelli\nL’architettura della rete neurale influisce anche sul fatto che la moltiplicazione di matrici rappresenti un collo di bottiglia computazionale o di memoria maggiore durante l’esecuzione. I trasformatori e gli MLP sono molto più vincolati al calcolo rispetto alle reti neurali convoluzionali CNN. Ciò deriva dai tipi di operazioni di moltiplicazione di matrici coinvolte in ciascun modello. I trasformatori si basano sull’auto-attenzione, moltiplicando grandi matrici di attivazione per enormi matrici di parametri per correlare gli elementi. Gli MLP impilano layer completamente connessi, richiedendo anche grandi moltiplicazioni matriciali.\nAl contrario, i layer convoluzionali nelle CNN hanno una finestra scorrevole che riutilizza attivazioni e parametri nell’input, il che significa che sono necessarie meno operazioni matriciali univoche. Tuttavia, le convoluzioni richiedono l’accesso ripetuto a piccole parti di input e lo spostamento di somme parziali per popolare ciascuna finestra. Sebbene le operazioni aritmetiche nelle convoluzioni siano intense, questo spostamento di dati e la manipolazione del buffer impongono enormi overhead di accesso alla memoria. Le CNN comprendono diverse fasi a strati, quindi gli output intermedi devono materializzarsi frequentemente nella memoria.\nDi conseguenza, l’addestramento CNN tende a essere più vincolato alla larghezza di banda della memoria rispetto al limite aritmetico in confronto a Transformers e MLP. Pertanto, il profilo di moltiplicazione della matrice e, a sua volta, il collo di bottiglia posto, varia in modo significativo in base alla scelta del modello. Hardware e sistemi devono essere progettati con un appropriato equilibrio di larghezza di banda di elaborazione-memoria a seconda dell’implementazione del modello target. I modelli che si basano maggiormente sull’attenzione e sui layer MLP richiedono una maggiore produttività aritmetica rispetto alle CNN, il che richiede un’elevata larghezza di banda della memoria.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#parallelizzazione-del-training",
    "href": "contents/training/training.it.html#parallelizzazione-del-training",
    "title": "7  Addestramento dell’IA",
    "section": "7.11 Parallelizzazione del Training",
    "text": "7.11 Parallelizzazione del Training\nL’addestramento delle reti neurali comporta richieste di calcolo e memoria intensive. L’algoritmo di backpropagation per il calcolo dei gradienti e l’aggiornamento dei pesi consiste in ripetute moltiplicazioni di matrici e operazioni aritmetiche sull’intero set di dati. Ad esempio, un passaggio di backpropagation scala in complessità temporale con \\(O(num\\_parameters \\times batch\\_size \\times sequence\\_length)\\).\nI requisiti di calcolo aumentano rapidamente con l’aumento delle dimensioni del modello in parametri e layer. Inoltre, l’algoritmo richiede l’archiviazione di output di attivazione e parametri del modello per la fase di backward, che cresce con le dimensioni del modello.\nI modelli più grandi non possono adattarsi e addestrarsi su un singolo dispositivo acceleratore come una GPU e l’ingombro di memoria diventa proibitivo. Pertanto, dobbiamo parallelizzare l’addestramento del modello su più dispositivi per fornire elaborazione e memoria sufficienti per addestrare reti neurali all’avanguardia.\nCome mostrato in Figura 7.8, i due approcci principali sono il parallelismo dei dati, che replica il modello su più dispositivi suddividendo i dati di input in batch, e il parallelismo del modello, che suddivide l’architettura del modello stesso su diversi dispositivi. Tramite il training in parallelo, possiamo sfruttare maggiori risorse aggregate di elaborazione e memoria per superare le limitazioni del sistema e accelerare i carichi di lavoro di deep learning.\n\n\n\n\n\n\nFigura 7.8: Parallelismo dei dati e parallelismo del modello.\n\n\n\n\n7.11.1 Parallelismo dei Dati\nLa parallelizzazione dei dati è un approccio comune per parallelizzare il training di apprendimento automatico su più unità di elaborazione, come GPU o risorse di elaborazione distribuite. Il set di dati di addestramento è suddiviso in batch nel parallelismo dei dati e un’unità di elaborazione separata elabora ogni batch. I parametri del modello vengono poi aggiornati in base ai gradienti calcolati dall’elaborazione di ogni batch. Ecco una descrizione dettagliata della parallelizzazione dei dati per il training ML:\n\nDivisione del Dataset: Il set di dati di addestramento è suddiviso in batch più piccoli, ciascuno contenente un sottoinsieme degli esempi di training.\nReplica del Modello: Il modello di rete neurale è replicato su tutte le unità di elaborazione e ogni unità di elaborazione ha la sua copia.\nCalcolo Parallelo: Ogni unità di elaborazione prende un batch diverso e calcola in modo indipendente i passaggi in forward e backward. Durante il passaggio forward [in avanti], il modello fa delle previsioni sui dati di input. La funzione di loss [perdita] determina i gradienti per i parametri del modello durante il passaggio backward [all’indietro].\nAggregazione dei Gradienti: Dopo l’elaborazione dei rispettivi batch, i gradienti di ogni unità di elaborazione vengono aggregati. I metodi di aggregazione comuni includono la sommatoria o la media dei gradienti.\nAggiornamento dei Parametri: I gradienti aggregati aggiornano i parametri del modello. L’aggiornamento può essere eseguito utilizzando algoritmi di ottimizzazione come SGD o varianti come Adam.\nSincronizzazione: Dopo l’aggiornamento, tutte le unità di elaborazione sincronizzano i parametri del modello, assicurandosi che ciascuna ne abbia la versione più recente.\n\nI passaggi precedenti vengono ripetuti per diverse iterazioni o fino alla convergenza.\nPrendiamo un esempio specifico. Abbiamo 256 dimensioni di batch e 8 GPU; ogni GPU riceverà un micro-batch di 32 campioni. I loro passaggi forward e backward calcolano perdite e gradienti solo in base ai 32 campioni locali. I gradienti vengono aggregati tra i dispositivi con un server dei parametri o una libreria di comunicazioni collettiva per ottenere il gradiente effettivo per il batch globale. Gli aggiornamenti dei pesi avvengono indipendentemente su ogni GPU in base a questi gradienti. Dopo un numero configurato di iterazioni, i pesi aggiornati si sincronizzano e si equalizzano tra i dispositivi prima di passare alle iterazioni successive.\nIl parallelismo dei dati è efficace quando il modello è grande e il set di dati è sostanziale, poiché consente l’elaborazione parallela di diverse parti dei dati. È ampiamente utilizzato in framework e librerie di deep learning che supportano il training distribuito, come TensorFlow e PyTorch. Tuttavia, per garantire una parallelizzazione efficiente, è necessario prestare attenzione a gestire problemi come l sovraccarico della comunicazione, bilanciamento del carico e sincronizzazione.\n\n\n7.11.2 Parallelismo del Modello\nIl parallelismo del modello si riferisce alla distribuzione del modello di rete neurale su più dispositivi anziché alla replica del modello completo come il parallelismo dei dati. Ciò è particolarmente utile quando un modello è troppo grande per essere inserito nella memoria di una singola GPU o di un dispositivo acceleratore. Sebbene ciò potrebbe non essere specificamente applicabile per casi d’uso embedded o TinyML poiché la maggior parte dei modelli è relativamente piccola, è comunque utile saperlo.\nNell’addestramento parallelo del modello, diverse parti o layer del modello vengono assegnati a dispositivi separati. Le attivazioni di input e gli output intermedi vengono partizionati e passati tra questi dispositivi durante i passaggi forward e backward per coordinare i calcoli del gradiente tra le partizioni del modello.\nIl “footprint” [impronta] di memoria e le operazioni di calcolo vengono distribuite suddividendo l’architettura del modello su più dispositivi anziché concentrarsi su uno. Ciò consente l’addestramento di modelli molto grandi con miliardi di parametri che altrimenti supererebbero la capacità di un singolo dispositivo. Esistono diversi modi in cui possiamo eseguire il partizionamento:\n\nParallelismo di Layer: I layer consecutivi sono distribuiti su dispositivi diversi. Ad esempio, il dispositivo 1 contiene i layer 1-3; il dispositivo 2 contiene i layer 4-6. Le attivazioni di output dal layer 3 verrebbero trasferite al dispositivo 2 per avviare i layer successivi per i calcoli della fase di forward.\nParallelismo a Livello di Filtro: Nei layer convoluzionali, i filtri di output possono essere suddivisi tra più dispositivi. Ogni dispositivo calcola gli output di attivazione per un sottoinsieme di filtri, che vengono concatenati prima di propagarsi ulteriormente.\nParallelismo Spaziale: Le immagini di input vengono divise spazialmente, quindi ogni dispositivo elabora una determinata regione come il quarto in alto a sinistra delle immagini. Le regioni di output si combinano poi per formare l’output completo.\n\nInoltre, le combinazioni ibride possono suddividere il modello a livello di layer e i dati in batch. Il tipo appropriato di parallelismo del modello dipende dai vincoli specifici dell’architettura neurale e dalla configurazione hardware. Ottimizzare il partizionamento e la comunicazione per la topologia del modello è fondamentale per ridurre al minimo il sovraccarico.\nTuttavia, poiché le parti del modello vengono eseguite su dispositivi fisicamente separati, devono comunicare e sincronizzare i loro parametri durante ogni fase di addestramento. La fase di backward deve garantire che gli aggiornamenti del gradiente si propaghino accuratamente tra le partizioni del modello. Quindi, il coordinamento e l’interconnessione ad alta velocità tra i dispositivi sono fondamentali per ottimizzare le prestazioni dell’addestramento parallelo. Sono necessari dei buoni protocolli di partizionamento e comunicazione per ridurre al minimo il sovraccarico di trasferimento.\n\n\n7.11.3 Confronto\nRiassumendo, Tabella 7.5 illustra alcune delle caratteristiche chiave per confrontare il parallelismo dei dati e quello dei modelli.\n\n\n\nTabella 7.5: Confronto tra parallelismo dei dati e parallelismo del modello.\n\n\n\n\n\n\n\n\n\n\nCaratteristica\nParallelismo dei dati\nParallelismo del modello\n\n\n\n\nDefinizione\nDistribuisce i dati tra i dispositivi con repliche\nDistribuisce il modello tra i dispositivi\n\n\nObiettivo\nAccelera il training tramite il ridimensionamento del calcolo\nAbilita un training del modello più ampio\n\n\nMetodo di Ridimensionamento\nDispositivi/workers in scala\nDimensioni modello in scala\n\n\nVincolo Principale\nDimensione del modello per ogni dispositivo\nOverhead di coordinamento dispositivo\n\n\nRequisiti Hardware\nPiù GPU/TPU\nSpesso interconnessione specializzata\n\n\nDifficoltà Principale\nSincronizzazione dei parametri\nPartizionamento e comunicazione complicati\n\n\nTipi\nN/D\nPer livello, per filtro, spaziale\n\n\nComplessità del Codice\nModifiche minime\nIntervento più significativa sul modello\n\n\nLibrerie Popolari\nHorovod, PyTorch Distributed\nMesh TensorFlow",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#conclusione",
    "href": "contents/training/training.it.html#conclusione",
    "title": "7  Addestramento dell’IA",
    "section": "7.12 Conclusione",
    "text": "7.12 Conclusione\nIn questo capitolo abbiamo trattato le basi fondamentali che consentono un training efficace dei modelli di intelligenza artificiale. Abbiamo esplorato concetti matematici come funzioni di perdita, backpropagation e discesa del gradiente che rendono possibile l’ottimizzazione delle reti neurali. Abbiamo anche discusso tecniche pratiche per sfruttare i dati di training, la regolarizzazione, la messa a punto degli iperparametri, l’inizializzazione dei pesi e strategie di parallelizzazione distribuita che migliorano convergenza, generalizzazione e scalabilità.\nQueste metodologie costituiscono il fondamento attraverso cui è stato raggiunto il successo del deep learning nell’ultimo decennio. Padroneggiare questi fondamenti prepara i professionisti a progettare sistemi e perfezionare modelli su misura per il loro contesto. Tuttavia, man mano che modelli e set di dati crescono in modo esponenziale, i sistemi di training devono ottimizzare parametri come tempo, costo e “carbon footprint” [impatto ambientale]. Il ridimensionamento hardware tramite grosse warehouse consente un throughput computazionale enorme, ma le ottimizzazioni relative a efficienza e specializzazione saranno fondamentali. Tecniche software come compressione e sfruttamento delle matrici sparse possono aumentare i guadagni hardware. Ne discuteremo diverse nei prossimi capitoli.\nNel complesso, i fondamenti trattati in questo capitolo preparano i professionisti a costruire, perfezionare e distribuire modelli. Tuttavia, le competenze interdisciplinari che abbracciano teoria, sistemi e hardware differenzieranno gli esperti in grado di portare l’IA al livello successivo in modo sostenibile e responsabile, come richiesto dalla società. Comprendere l’efficienza insieme all’accuratezza costituisce l’approccio ingegneristico bilanciato necessario per addestrare sistemi intelligenti che si integrano senza problemi in molti contesti del mondo reale.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/training/training.it.html#sec-ai-training-resource",
    "href": "contents/training/training.it.html#sec-ai-training-resource",
    "title": "7  Addestramento dell’IA",
    "section": "7.13 Risorse",
    "text": "7.13 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nThinking About Loss.\nMinimizing Loss.\nTraining, Validation, and Test Data.\nContinuous Training:\n\nRetraining Trigger.\nData Processing Overview.\nData Ingestion.\nData Validation.\nData Transformation.\nTraining with AutoML.\nContinuous Training with Transfer Learning.\nContinuous Training Use Case Metrics.\nContinuous Training Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 7.1\nVideo 7.2\nVideo 7.3\nVideo 7.4\nVideo 7.5\nVideo 7.6\nVideo 7.7\nVideo 7.8\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 7.1\nEsercizio 7.2\nEsercizio 7.3\nEsercizio 7.5\nEsercizio 7.4\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Addestramento dell'IA</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html",
    "href": "contents/efficient_ai/efficient_ai.it.html",
    "title": "8  IA Efficiente",
    "section": "",
    "text": "8.1 Introduzione\nI modelli di training possono consumare molta energia, a volte equivalente all’impatto ambientale di processi industriali considerevoli. Tratteremo alcuni di questi dettagli sulla sostenibilità nel capitolo Sostenibilità dell’IA. Dal punto di vista dell’implementazione, se questi modelli non sono ottimizzati per l’efficienza, possono esaurire rapidamente le batterie dei dispositivi, richiedere una memoria eccessiva o non soddisfare le esigenze di elaborazione in tempo reale. In questo capitolo, miriamo a chiarire le sfumature dell’efficienza, gettando le basi per un’esplorazione completa nei capitoli successivi.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#la-necessità-di-unia-efficiente",
    "href": "contents/efficient_ai/efficient_ai.it.html#la-necessità-di-unia-efficiente",
    "title": "8  IA Efficiente",
    "section": "8.2 La Necessità di un’IA Efficiente",
    "text": "8.2 La Necessità di un’IA Efficiente\nL’efficienza assume connotazioni diverse a seconda di dove si verificano i calcoli dell’IA. Rivediamo Cloud, Edge e TinyML (come discusso in Sistemi di ML) e distinguiamoli in termini di efficienza. Figura 8.1 fornisce un confronto generale delle tre diverse piattaforme.\n\n\n\n\n\n\nFigura 8.1: Cloud, Mobile e TinyML. Fonte: Schizas et al. (2022).\n\n\nSchizas, Nikolaos, Aristeidis Karras, Christos Karras, e Spyros Sioutas. 2022. «TinyML for Ultra-Low Power AI and Large Scale IoT Deployments: A Systematic Review». Future Internet 14 (12): 363. https://doi.org/10.3390/fi14120363.\n\n\nIA Cloud: I modelli IA tradizionali vengono spesso eseguiti in data center su larga scala dotati di potenti GPU e TPU (Barroso, Hölzle, e Ranganathan 2019). Qui, l’efficienza riguarda l’ottimizzazione delle risorse di calcolo, la riduzione dei costi e la garanzia di elaborazione e restituzione tempestive dei dati. Tuttavia, fare affidamento sul cloud introduce latenza, soprattutto quando si ha a che fare con flussi di dati di grandi dimensioni che richiedono caricamento, elaborazione e download.\n\nBarroso, Luiz André, Urs Hölzle, e Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\nLi, En, Liekang Zeng, Zhi Zhou, e Xu Chen. 2020. «Edge AI: On-demand Accelerating Deep Neural Network Inference via Edge Computing». IEEE Trans. Wireless Commun. 19 (1): 447–57. https://doi.org/10.1109/twc.2019.2946140.\nIA Edge: L’edge computing avvicina l’intelligenza artificiale alla fonte dei dati, elaborando le informazioni direttamente su dispositivi locali come smartphone, fotocamere o macchine industriali (Li et al. 2020). Qui, l’efficienza comprende risposte rapide in tempo reale e ridotte esigenze di trasmissione dei dati. Tuttavia, i vincoli sono più severi: questi dispositivi, sebbene più potenti dei microcontrollori, hanno una potenza di calcolo limitata rispetto alle configurazioni cloud.\nTinyML: TinyML supera i limiti consentendo ai modelli di intelligenza artificiale di funzionare su microcontrollori o ambienti con risorse estremamente limitate. La differenza di prestazioni del processore e della memoria tra TinyML e i sistemi cloud o mobili può essere di diversi ordini di grandezza (Warden e Situnayake 2019). L’efficienza in TinyML consiste nell’assicurare che i modelli siano sufficientemente leggeri da adattarsi a questi dispositivi, consumino il minimo di energia (fondamentale per i dispositivi alimentati a batteria) e continuino a svolgere le loro attività in modo efficace.\n\nWarden, Pete, e Daniel Situnayake. 2019. Tinyml: Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers. O’Reilly Media.\nLo spettro da Cloud a TinyML rappresenta un passaggio da vaste risorse di elaborazione centralizzate ad ambienti distribuiti, localizzati e limitati. Passando dall’uno all’altro, i problemi e le strategie relative all’efficienza evolvono, sottolineando la necessità di approcci specializzati su misura per ogni scenario. Dopo aver stabilito la necessità di un’intelligenza artificiale efficiente, in particolare nel contesto di TinyML, passeremo all’esplorazione delle metodologie ideate per rispondere a queste sfide. Le sezioni seguenti delineano i concetti principali che approfondiremo in seguito. Dimostreremo l’ampiezza e la profondità dell’innovazione necessarie per ottenere un’intelligenza artificiale efficiente mentre esploriamo queste strategie.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#architetture-di-modelli-efficienti",
    "href": "contents/efficient_ai/efficient_ai.it.html#architetture-di-modelli-efficienti",
    "title": "8  IA Efficiente",
    "section": "8.3 Architetture di Modelli Efficienti",
    "text": "8.3 Architetture di Modelli Efficienti\nSelezionare un’architettura del modello ottimale è tanto cruciale quanto ottimizzarla. Negli ultimi anni, i ricercatori hanno compiuto passi da gigante nell’esplorazione di architetture innovative che possono avere intrinsecamente meno parametri pur mantenendo prestazioni elevate.\nMobileNet: MobileNet sono modelli di applicazioni di visione mobile ed embedded efficienti (Howard et al. 2017). L’idea chiave che ha portato al loro successo sono le convoluzioni separabili in profondità, che riducono significativamente il numero di parametri e calcoli nella rete. MobileNetV2 e V3 migliorano ulteriormente questo design introducendo residui invertiti e colli di bottiglia lineari.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. «SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and 0.5 MB model size». ArXiv preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\nSqueezeNet: SqueezeNet è una classe di modelli ML noti per le sue dimensioni ridotte senza sacrificare la precisione. Ciò si ottiene utilizzando un “modulo fire” che riduce il numero di canali di input a filtri 3x3, riducendo così i parametri (Iandola et al. 2016). Inoltre, impiega il downsampling [sottocampionamento] ritardato per aumentare la precisione mantenendo una mappa delle feature più ampia.\nVarianti ResNet: L’architettura Residual Network (ResNet) consente l’introduzione di connessioni skip o scorciatoie (He et al. 2016). Alcune varianti di ResNet sono progettate per essere più efficienti. Ad esempio, ResNet-SE incorpora il meccanismo “squeeze and excitation” per ricalibrare le feature map (Hu, Shen, e Sun 2018), mentre ResNeXt offre convoluzioni raggruppate per l’efficienza (Xie et al. 2017).\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, e Jian Sun. 2016. «Deep Residual Learning for Image Recognition». In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\nHu, Jie, Li Shen, e Gang Sun. 2018. «Squeeze-and-Excitation Networks». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7132–41. IEEE. https://doi.org/10.1109/cvpr.2018.00745.\n\nXie, Saining, Ross Girshick, Piotr Dollar, Zhuowen Tu, e Kaiming He. 2017. «Aggregated Residual Transformations for Deep Neural Networks». In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1492–1500. IEEE. https://doi.org/10.1109/cvpr.2017.634.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-model-compression",
    "href": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-model-compression",
    "title": "8  IA Efficiente",
    "section": "8.4 Compressione Efficiente del Modello",
    "text": "8.4 Compressione Efficiente del Modello\nI metodi di compressione dei modelli sono essenziali per portare i modelli di apprendimento profondo su dispositivi con risorse limitate. Queste tecniche riducono le dimensioni dei modelli, il consumo energetico e le richieste di elaborazione senza perdere significativamente la precisione. Ad alto livello, i metodi possono essere categorizzati nei seguenti metodi fondamentali:\nPruning: L’Abbiamo menzionato un paio di volte nei capitoli precedenti, ma non l’abbiamo ancora formalmente introdotta. Il pruning è simile alla potatura dei rami di un albero. Questo è stato pensato per la prima volta nel documento Optimal Brain Damage (LeCun, Denker, e Solla 1989) ed è stato successivamente reso popolare nel contesto del deep learning da Han, Mao, e Dally (2016). Determinati pesi o interi neuroni vengono rimossi dalla rete nella potatura in base a criteri specifici. Questo può ridurre significativamente le dimensioni del modello. In Sezione 9.2.1 esploreremo due delle principali strategie di potatura, quella strutturata e quella non-strutturata. Figura 8.2 è un esempio di potatura della rete neurale, in cui la rimozione di alcuni nodi negli strati interni (in base a criteri specifici) riduce il numero di rami tra i nodi e, a sua volta, le dimensioni del modello.\n\nLeCun, Yann, John Denker, e Sara Solla. 1989. «Optimal brain damage». Adv Neural Inf Process Syst 2.\n\nHan, Song, Huizi Mao, e William J. Dally. 2016. «Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding». https://arxiv.org/abs/1510.00149.\n\n\n\n\n\n\nFigura 8.2: Neural Network Pruning.\n\n\n\nQuantizzazione: La quantizzazione è il processo di limitazione di un input da un set ampio a un output in un set più piccolo, principalmente nel deep learning; ciò significa ridurre il numero di bit che rappresentano i pesi e i bias del modello. Ad esempio, l’utilizzo di rappresentazioni a 16 o 8 bit anziché a 32 bit può ridurre la dimensione del modello e velocizzare i calcoli, con un piccolo compromesso in termini di accuratezza. Esploreremo questi aspetti più in dettaglio in Sezione 9.3.4. Figura 8.3 mostra un esempio di quantizzazione mediante arrotondamento al numero più vicino. La conversione da virgola mobile a 32 bit a 16 bit riduce l’utilizzo della memoria del 50%. Passare da un intero a 32 bit a uno a 8 bit riduce l’utilizzo della memoria del 75%. Mentre la perdita di precisione numerica e, di conseguenza, di prestazioni del modello è minima, l’efficienza nell’utilizzo della memoria è significativa.\n\n\n\n\n\n\nFigura 8.3: Diverse forme di quantizzazione.\n\n\n\nKnowledge Distillation: La “distillazione della conoscenza” comporta l’addestramento di un modello più piccolo (studente) per replicare il comportamento di un modello più grande (insegnante). L’idea è quella di trasferire la conoscenza dal modello ingombrante a quello leggero. Quindi, il modello più piccolo raggiunge prestazioni vicine alla sua controparte più grande ma con parametri significativamente inferiori. Esploreremo la “distillazione della conoscenza” in modo più dettagliato in Sezione 9.2.2.1.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#hardware-di-inferenza-efficiente",
    "href": "contents/efficient_ai/efficient_ai.it.html#hardware-di-inferenza-efficiente",
    "title": "8  IA Efficiente",
    "section": "8.5 Hardware di Inferenza Efficiente",
    "text": "8.5 Hardware di Inferenza Efficiente\nNel capitolo Training, abbiamo discusso il processo di training dei modelli di intelligenza artificiale. Ora, dal punto di vista dell’efficienza, è importante notare che il training è un’attività che richiede molte risorse e molto tempo, spesso richiede hardware potente e impiega da ore a settimane per essere completato. L’inferenza, d’altra parte, deve essere il più veloce possibile, soprattutto nelle applicazioni in tempo reale. È qui che entra in gioco un hardware di inferenza efficiente. Ottimizzando l’hardware specificamente per le attività di inferenza, possiamo ottenere tempi di risposta rapidi e un funzionamento efficiente dal punto di vista energetico, il che è particolarmente cruciale per i dispositivi edge e i sistemi embedded.\nTPU (Tensor Processing Unit): Le TPU sono ASIC (Application-Specific Integrated Circuits) personalizzati da Google per accelerare i carichi di lavoro di apprendimento automatico (Jouppi et al. 2017). Sono ottimizzate per le operazioni tensoriali, offrono un throughput elevato per l’aritmetica a bassa precisione e sono progettate specificamente per il machine learning delle reti neurali. Le TPU accelerano significativamente l’addestramento e l’inferenza del modello rispetto alle GPU/CPU generiche. Questo potenziamento si traduce in un addestramento più rapido dei modelli e in capacità di inferenza in tempo reale o quasi reale, fondamentali per applicazioni come la ricerca vocale e la realtà aumentata.\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\nLe Edge TPU sono una versione più piccola e a basso consumo delle TPU di Google, studiate appositamente per i dispositivi edge. Forniscono un’inferenza ML veloce sul dispositivo per i modelli TensorFlow Lite. Le Edge TPU consentono un’inferenza a bassa latenza e ad alta efficienza su dispositivi edge come smartphone, dispositivi IoT e sistemi embedded. Le capacità di IA possono essere implementate in applicazioni in tempo reale senza comunicare con un server centrale, risparmiando così larghezza di banda e riducendo la latenza. Si consideri la tabella in Figura 8.4. Mostra le differenze di prestazioni tra l’esecuzione di modelli diversi su CPU rispetto a un acceleratore Coral USB. L’acceleratore Coral USB è un accessorio della piattaforma Coral AI di Google che consente agli sviluppatori di collegare le Edge TPU ai computer Linux. L’esecuzione dell’inferenza sulle Edge TPU è stata da 70 a 100 volte più veloce rispetto alle CPU.\n\n\n\n\n\n\nFigura 8.4: Confronto delle prestazioni tra acceleratore e CPU in diverse configurazioni hardware. Desktop CPU: 64-bit Intel(R) Xeon(R) E5–1650 v4 @ 3.60GHz. Embedded CPU: Quad-core Cortex-A53 @ 1.5GHz, †Dev Board: Quad-core Cortex-A53 @ 1.5GHz + Edge TPU. Fonte: TensorFlow Blog.\n\n\n\nAcceleratori NN (Neural Network): Gli acceleratori di reti neurali a funzione fissa sono acceleratori hardware progettati esplicitamente per i calcoli di reti neurali. Possono essere chip standalone o far parte di una soluzione di system-on-chip (SoC) più ampia. Ottimizzando l’hardware per le operazioni specifiche richieste dalle reti neurali, come moltiplicazioni di matrici e convoluzioni, gli acceleratori NN possono ottenere tempi di inferenza più rapidi e consumi energetici inferiori rispetto alle CPU e alle GPU per uso generico. Sono particolarmente utili nei dispositivi TinyML con vincoli di potenza o termici, come smartwatch, micro-droni o robotica.\nMa questi sono solo gli esempi più comuni. Stanno emergendo diversi altri tipi di hardware che hanno il potenziale per offrire vantaggi significativi per l’inferenza. Questi includono, ma non solo, hardware neuromorfico, elaborazione fotonica, ecc. In Sezione 10.3, esploreremo questi aspetti in modo più dettagliato.\nUn hardware efficiente per l’inferenza velocizza il processo, risparmia energia, prolunga la durata della batteria e può funzionare in condizioni di tempo reale. Man mano che l’intelligenza artificiale viene integrata in innumerevoli applicazioni, dalle telecamere intelligenti agli assistenti vocali, il ruolo dell’hardware ottimizzato diventerà sempre più importante. Sfruttando questi componenti hardware specializzati, sviluppatori e ingegneri possono portare la potenza dell’intelligenza artificiale a dispositivi e situazioni che prima erano impensabili.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#numeri-efficienti",
    "href": "contents/efficient_ai/efficient_ai.it.html#numeri-efficienti",
    "title": "8  IA Efficiente",
    "section": "8.6 Numeri Efficienti",
    "text": "8.6 Numeri Efficienti\nL’apprendimento automatico, e in particolare il deep learning, comporta enormi quantità di elaborazione. I modelli possono avere milioni o miliardi di parametri, spesso addestrati su vasti set di dati. Ogni operazione, ogni moltiplicazione o addizione, richiede risorse di elaborazione. Pertanto, la precisione dei numeri utilizzati in queste operazioni può avere un impatto significativo sulla velocità di elaborazione, sul consumo di energia e sui requisiti di memoria. È qui che entra in gioco il concetto di numeri efficienti.\n\n8.6.1 Formati Numerici\nEsistono molti tipi diversi di numeri. I numeri hanno una lunga storia nei sistemi di elaborazione.\nFloating point: Noto come “virgola mobile” a precisione singola, FP32 utilizza 32 bit per rappresentare un numero, incorporandone segno, esponente e mantissa. Comprendere come i numeri in virgola mobile sono rappresentati in modo approfondito è fondamentale per comprendere le varie ottimizzazioni possibili nei calcoli numerici. Il bit del segno determina se il numero è positivo o negativo, l’esponente controlla l’intervallo di valori che possono essere rappresentati e la mantissa determina la precisione del numero. La combinazione di questi componenti consente ai numeri in virgola mobile di rappresentare un’ampia gamma di valori con vari gradi di precisione.\nVideo 8.1 fornisce una panoramica completa di questi tre componenti principali, segno, esponente e mantissa, e di come funzionano insieme per rappresentare i numeri in virgola mobile.\n\n\n\n\n\n\nVideo 8.1: Numeri in Virgola Mobile\n\n\n\n\n\n\nFP32 è ampiamente adottato in molti framework di deep learning e bilancia accuratezza e requisiti computazionali. È prevalente nella fase di training per molte reti neurali grazie alla sua sufficiente precisione nel catturare dettagli minuti durante gli aggiornamenti dei pesi. Noto anche come virgola mobile a mezza precisione, FP16 utilizza 16 bit per rappresentare un numero, inclusi il segno, l’esponente e la frazione. Offre un buon equilibrio tra precisione e risparmio di memoria. FP16 è particolarmente popolare nella training di deep learning su GPU che supportano l’aritmetica a precisione mista, combinando i vantaggi di velocità di FP16 con la precisione di FP32 quando necessario.\nDiversi altri formati numerici rientrano in una classe esotica. Un esempio esotico è BF16 o Brain Floating Point. È un formato numerico a 16 bit progettato esplicitamente per applicazioni di deep learning. È un compromesso tra FP32 e FP16, che mantiene l’esponente a 8 bit di FP32 riducendo la mantissa a 7 bit (rispetto alla mantissa a 23 bit di FP32). Questa struttura dà priorità al range rispetto alla precisione. BF16 ha ottenuto risultati di training paragonabili in accuratezza a FP32, utilizzando significativamente meno memoria e risorse computazionali (Kalamkar et al. 2019). Ciò lo rende adatto non solo per l’inferenza, ma anche per il training di reti neurali profonde.\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019. «A Study of BFLOAT16 for Deep Learning Training». https://arxiv.org/abs/1905.12322.\nMantenendo l’esponente a 8 bit di FP32, BF16 offre un range simile, che è fondamentale per le attività di deep learning in cui determinate operazioni possono generare numeri molto grandi o molto piccoli. Allo stesso tempo, troncando la precisione, BF16 consente requisiti di memoria e computazionali ridotti rispetto a FP32. BF16 è emerso come una promettente via di mezzo nel panorama dei formati numerici per il deep learning, fornendo un’alternativa efficiente ed efficace ai formati FP32 e FP16 più tradizionali.\nFigura 8.5 mostra tre diversi formati in virgola mobile: Float32, Float16 e BFloat16.\n\n\n\n\n\n\nFigura 8.5: Tre formati a virgola mobile.\n\n\n\nIntero: Si tratta di rappresentazioni di numeri interi che utilizzano 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocità e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attività di inferenza, in particolare sui dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione è spesso accettabile, dati i guadagni di efficienza. Una forma estrema di numeri interi è per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno dei due valori: +1 o -1.\nLarghezze di bit variabili: Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezze di bit estremamente basse possono offrire accelerazioni significative e ridurre ulteriormente il consumo di energia. Sebbene permangano dei problemi nel mantenere l’accuratezza del modello con una quantizzazione così drastica, si continuano a fare progressi in quest’area.\nL’efficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano più pervasivi, soprattutto in ambienti reali con risorse limitate, l’attenzione su una numerica efficiente continuerà a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, è possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocità, memoria ed energia. Tabella 8.1 riassume questi compromessi.\n\n\n\nTabella 8.1: Confronto dei livelli di precisione nel deep learning.\n\n\n\n\n\n\n\n\n\n\nPrecisione\nPro\nContro\n\n\n\n\nFP32 (virgola mobile a 32 bit)\n\nPrecisione standard utilizzata nella maggior parte dei framework di deep learning.\nElevata accuratezza grazie all’ampia capacità di rappresentazione.\nAdatto per il training\n\n\nElevato utilizzo di memoria.\nTempi di inferenza più lenti rispetto ai modelli quantizzati.\nMaggiore consumo energetico.\n\n\n\nFP16 (virgola mobile a 16 bit)\n\nRiduce l’utilizzo di memoria rispetto a FP32.\nVelocizza i calcoli su hardware che supporta FP16.\nSpesso utilizzato nel training a precisione mista per bilanciare velocità e accuratezza.\n\n\nMinore capacità di rappresentazione rispetto a FP32.\nRischio di instabilità numerica in alcuni modelli o livelli.\n\n\n\nINT8 (intero a 8 bit)\n\nImpronta di memoria notevolmente ridotta rispetto alle rappresentazioni in virgola mobile.\nInferenza più rapida se l’hardware supporta i calcoli INT8.\nAdatto a molti scenari di quantizzazione post-training.\n\n\nLa quantizzazione può comportare una certa perdita di accuratezza.\nRichiede una calibrazione attenta durante la quantizzazione per ridurre al minimo il degrado della precisione.\n\n\n\nINT4 (intero a 4 bit)\n\nUtilizzo di memoria ancora inferiore rispetto a INT8.\nUlteriore potenziale di accelerazione per l’inferenza.\n\n\nRischio di perdita di precisione più elevato rispetto a INT8.\nLa calibrazione durante la quantizzazione diventa più critica.\n\n\n\nBinario\n\nIngombro di memoria minimo (solo 1 bit per parametro).\nInferenza estremamente rapida grazie alle operazioni bit a bit.\nEfficienza energetica.\n\n\nCalo significativo della precisione per molte attività.\nDinamiche di training complesse grazie alla quantizzazione estrema.\n\n\n\nTernario\n\nBasso utilizzo di memoria ma leggermente superiore a quello binario.\nOffre una via di mezzo tra rappresentazione ed efficienza.\n\n\nL’accuratezza potrebbe essere ancora inferiore a quella dei modelli di precisione più elevata.\nLe dinamiche di addestramento possono essere complesse.\n\n\n\n\n\n\n\n\n\n8.6.2 Vantaggi dell’Efficienza\nL’efficienza numerica è importante per i carichi di lavoro di machine learning per diversi motivi:\nEfficienza Computazionale: I calcoli ad alta precisione (come FP32 o FP64) possono essere lenti e richiedere molte risorse. Ridurre la precisione numerica può ottenere tempi di calcolo più rapidi, specialmente su hardware specializzato che supporta una precisione inferiore.\nEfficienza della Memoria: I requisiti di archiviazione diminuiscono con una precisione numerica ridotta. Ad esempio, FP16 richiede metà della memoria di FP32. Ciò è fondamentale quando si distribuiscono modelli su dispositivi edge con memoria limitata o si lavora con modelli di grandi dimensioni.\nEfficienza Energetica: I calcoli a precisione inferiore spesso consumano meno energia, il che è particolarmente importante per i dispositivi alimentati a batteria.\nIntroduzione del Rumore: È interessante notare che il rumore introdotto utilizzando una precisione inferiore può talvolta fungere da regolarizzatore, contribuendo a prevenire l’overfitting in alcuni modelli.\nAccelerazione Hardware: Molti acceleratori di IA e GPU moderni sono ottimizzati per operazioni di precisione inferiore, sfruttando i vantaggi dell’efficienza di tali numeri.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#valutazione-dei-modelli",
    "href": "contents/efficient_ai/efficient_ai.it.html#valutazione-dei-modelli",
    "title": "8  IA Efficiente",
    "section": "8.7 Valutazione dei Modelli",
    "text": "8.7 Valutazione dei Modelli\nVale la pena notare che i vantaggi e i compromessi effettivi possono variare in base all’architettura specifica della rete neurale, al set di dati, all’attività e all’hardware utilizzato. Prima di decidere una precisione numerica, è consigliabile eseguire esperimenti per valutare l’impatto sull’applicazione desiderata.\n\n8.7.1 Metriche di Efficienza\nUna profonda comprensione dei metodi di valutazione dei modelli è importante per guidare questo processo in modo sistematico. Quando si valuta l’efficacia e l’idoneità dei modelli di intelligenza artificiale per varie applicazioni, le metriche di efficienza vengono in primo piano.\nI FLOP (Floating Point Operations), introdotti in Training, misurano le esigenze computazionali di un modello. Ad esempio, una moderna rete neurale come BERT ha miliardi di FLOP, che potrebbero essere gestibili su un potente server cloud ma sarebbero gravosi su uno smartphone. FLOP più elevati possono portare a tempi di inferenza più prolungati e a un notevole consumo di energia, soprattutto su dispositivi senza acceleratori hardware specializzati. Quindi, per applicazioni in tempo reale come lo streaming video o i giochi, potrebbero essere più desiderabili modelli con FLOP più bassi.\nL’Utilizzo della Memoria riguarda la quantità di spazio di archiviazione richiesta dal modello, che influisce sia sullo spazio di archiviazione del dispositivo che sulla RAM. Si prenda in considerazione l’implementazione di un modello su uno smartphone: un modello che occupa diversi gigabyte di spazio non solo consuma prezioso spazio di archiviazione, ma potrebbe anche essere più lento a causa della necessità di caricare grandi pesi nella memoria. Ciò diventa particolarmente cruciale per dispositivi edge come telecamere di sicurezza o droni, dove impronte di memoria minime sono vitali per l’archiviazione e l’elaborazione rapida dei dati.\nIl Consumo Energetico diventa particolarmente cruciale per i dispositivi che si basano sulle batterie. Ad esempio, un monitor sanitario indossabile che utilizza un modello ad alto consumo energetico potrebbe esaurire la batteria in poche ore, rendendolo poco pratico per il monitoraggio continuo. L’ottimizzazione dei modelli per un basso consumo energetico diventa essenziale mentre ci muoviamo verso un’era dominata dai dispositivi IoT, dove molti dispositivi funzionano a batteria.\nIl Tempo di Inferenza riguarda la rapidità con cui un modello può produrre risultati. In applicazioni come la guida autonoma, dove decisioni in frazioni di secondo fanno la differenza tra sicurezza e calamità, i modelli devono funzionare rapidamente. Se il modello di un’auto a guida autonoma impiega anche solo pochi secondi in più per riconoscere un ostacolo, le conseguenze potrebbero essere disastrose. Quindi, garantire che il tempo di inferenza di un modello sia allineato con le richieste in tempo reale della sua applicazione è fondamentale.\nIn sostanza, queste metriche di efficienza sono più che dei numeri che stabiliscono dove e come un modello può essere distribuito in modo efficace. Un modello potrebbe vantare un’elevata accuratezza, ma se i suoi FLOP, l’utilizzo della memoria, il consumo energetico o il tempo di inferenza lo rendono inadatto alla piattaforma prevista o agli scenari del mondo reale, la sua utilità pratica diventa limitata.\n\n\n8.7.2 Confronti di Efficienza\nIl panorama dei modelli di machine learning è vasto, con ogni modello che offre un set unico di punti di forza e considerazioni di implementazione. Sebbene le cifre di accuratezza grezza o le velocità di training e inferenza possano essere parametri di riferimento allettanti, forniscono un quadro incompleto. Un’analisi comparativa più approfondita rivela diversi fattori critici che influenzano l’idoneità di un modello per le applicazioni TinyML. Spesso, incontriamo il delicato equilibrio tra accuratezza ed efficienza. Ad esempio, mentre un modello di deep learning e denso e una variante MobileNet leggera potrebbero eccellere nella classificazione delle immagini, le loro richieste di calcolo potrebbero essere ad estremi opposti. Questa differenziazione è particolarmente pronunciata quando si confrontano le distribuzioni su server cloud con risorse abbondanti rispetto ai limitati dispositivi TinyML. In molti scenari del mondo reale, i guadagni marginali in termini di accuratezza potrebbero essere oscurati dalle inefficienze di un modello ad alta intensità di risorse richieste.\nInoltre, la scelta del modello ottimale non è sempre universale, ma spesso dipende dalle specifiche di un’applicazione. Ad esempio, un modello che eccelle in scenari di rilevamento di oggetti generali potrebbe avere difficoltà in ambienti di nicchia, come il rilevamento di difetti di fabbricazione in una fabbrica. Questa adattabilità, o la sua mancanza, può influenzare l’utilità reale di un modello.\nUn’altra considerazione importante è la relazione tra la complessità del modello e i suoi vantaggi pratici. Prendiamo gli assistenti attivati tramite comando vocale, come “Alexa” o “OK Google”. Mentre un modello complesso potrebbe dimostrare una comprensione marginalmente superiore del parlato dell’utente se è più lento a rispondere rispetto a una controparte più semplice, l’esperienza utente potrebbe essere compromessa. Pertanto, l’aggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.\nUn’altra considerazione importante è la relazione tra la complessità del modello e i suoi vantaggi pratici. Prendiamo gli assistenti vocali come “Alexa” o “OK Google”. Mentre un modello complesso potrebbe dimostrare una comprensione leggermente superiore del parlato dell’utente se è più lento a rispondere rispetto a una controparte più semplice, l’esperienza utente potrebbe essere compromessa. Pertanto, l’aggiunta di layer o parametri solo a volte equivale a risultati migliori nel mondo reale.\nInoltre, mentre i set di dati di riferimento, come ImageNet (Russakovsky et al. 2015), COCO (Lin et al. 2014), Visual Wake Words (Wang e Zhan 2019), Google Speech Commands (Warden 2018), ecc. forniscono una metrica di prestazioni standardizzata, potrebbero non catturare la diversità e l’imprevedibilità dei dati del mondo reale. Due modelli di riconoscimento facciale con punteggi di riferimento simili potrebbero mostrare competenze diverse quando si trovano di fronte a background etnici diversi o condizioni di illuminazione difficili. Tali disparità sottolineano l’importanza di robustezza e coerenza tra dati diversi. Ad esempio, Figura 8.6 dal set di dati Dollar Street mostra immagini di stufe su redditi mensili estremi. Le stufe hanno forme e livelli tecnologici diversi in diverse regioni e livelli di reddito. Un modello che non è addestrato su set di dati diversi potrebbe funzionare bene su un benchmark ma fallire nelle applicazioni del mondo reale. Quindi, se un modello fosse addestrato solo su immagini di stufe trovate nei paesi ricchi, non riuscirebbe a riconoscere le stufe delle regioni più povere.\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. «ImageNet Large Scale Visual Recognition Challenge». Int. J. Comput. Vision 115 (3): 211–52. https://doi.org/10.1007/s11263-015-0816-y.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, e C Lawrence Zitnick. 2014. «Microsoft coco: Common objects in context». In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740–55. Springer.\n\nWang, LingFeng, e YaQing Zhan. 2019. «A conceptual peer review model for arXiv and other preprint databases». Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\nWarden, Pete. 2018. «Speech commands: A dataset for limited-vocabulary speech recognition». arXiv preprint arXiv:1804.03209.\n\n\n\n\n\n\nFigura 8.6: Diversi tipi di stufe. Fonte: Immagini di stufe di Dollar Street.\n\n\n\nIn sostanza, un’analisi comparativa approfondita trascende le metriche numeriche. È una valutazione olistica intrecciata con applicazioni del mondo reale, costi e le intricate sottigliezze che ogni modello porta con sé. Ecco perché avere parametri di riferimento e metriche standard ampiamente stabiliti e adottati dalla comunità diventa importante.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#conclusione",
    "href": "contents/efficient_ai/efficient_ai.it.html#conclusione",
    "title": "8  IA Efficiente",
    "section": "8.8 Conclusione",
    "text": "8.8 Conclusione\nL’intelligenza artificiale efficiente è fondamentale mentre ci spingiamo verso un’implementazione più ampia e diversificata del machine learning nel mondo reale. Questo capitolo ha fornito una panoramica, esplorando le varie metodologie e considerazioni alla base del raggiungimento di un’intelligenza artificiale efficiente, a partire dall’esigenza fondamentale, dalle somiglianze e dalle differenze tra i sistemi cloud, Edge e TinyML.\nAbbiamo esaminato le architetture dei modelli efficienti e la loro utilità per l’ottimizzazione. Le tecniche di compressione dei modelli come pruning, quantizzazione e distillazione della conoscenza esistono per aiutare a ridurre le richieste di calcolo e l’ingombro della memoria senza influire in modo significativo sulla precisione. Hardware specializzati come TPU e acceleratori NN offrono chip ottimizzati per le operazioni di rete neurale e il flusso di dati. I numeri efficienti bilanciano precisione ed efficienza, consentendo ai modelli di ottenere prestazioni robuste utilizzando risorse minime. Esploreremo questi argomenti in modo approfondito e dettagliato nei capitoli successivi.\nInsieme, questi formano un quadro olistico per un’intelligenza artificiale efficiente. Ma il viaggio non finisce qui. Il raggiungimento di un’intelligenza efficiente in modo ottimale richiede ricerca e innovazione continue. Man mano che i modelli diventano più sofisticati, i set di dati crescono e le applicazioni si diversificano in domini specializzati, l’efficienza deve evolversi di pari passo. La misura dell’impatto nel mondo reale richiede parametri di riferimento adatti e metriche standardizzate che vadano oltre le semplicistiche cifre dell’accuratezza.\nInoltre, l’intelligenza artificiale efficiente si espande oltre l’ottimizzazione tecnologica e comprende costi, impatto ambientale e considerazioni etiche per il bene della società in senso più ampio. Man mano che l’intelligenza artificiale permea i settori e la vita quotidiana, una prospettiva completa sull’efficienza sostiene il suo progresso sostenibile e responsabile. I capitoli successivi si baseranno su questi concetti fondamentali, fornendo approfondimenti concreti e norme pratiche per lo sviluppo e l’implementazione di soluzioni di intelligenza artificiale efficienti.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-ai-resource",
    "href": "contents/efficient_ai/efficient_ai.it.html#sec-efficient-ai-resource",
    "title": "8  IA Efficiente",
    "section": "8.9 Risorse",
    "text": "8.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nDeploying on Edge Devices: challenges and techniques.\nModel Evaluation.\nContinuous Evaluation Challenges for TinyML.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>IA Efficiente</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html",
    "href": "contents/optimizations/optimizations.it.html",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "",
    "text": "9.1 Introduzione\nAbbiamo strutturato questo capitolo in tre livelli. Innanzitutto, in Sezione 9.2 esaminiamo l’importanza e le metodologie per ridurre la complessità dei parametri dei modelli senza compromettere le loro capacità di inferenza. Vengono discusse tecniche come il “pruning” [potatura] e la distillazione della conoscenza, offrendo spunti su come i modelli possono essere compressi e semplificati mantenendo, o addirittura migliorando, le loro prestazioni.\nScendendo di un livello, in Sezione 9.3, studiamo il ruolo della precisione numerica nei calcoli dei modelli e come la sua modifica influisce sulle sue dimensioni, velocità e precisione. Esamineremo i vari formati numerici e come l’aritmetica a precisione ridotta può essere sfruttata per ottimizzare i modelli per la distribuzione embedded.\nInfine, man mano che scendiamo più in basso e ci avviciniamo all’hardware, in Sezione 9.4, esploreremo il panorama della progettazione congiunta hardware-software, esplorando come i modelli possono essere ottimizzati adattandoli alle caratteristiche e alle capacità specifiche dell’hardware target. Discuteremo di come i modelli possono essere adattati per sfruttare efficacemente le risorse hardware disponibili.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#introduzione",
    "href": "contents/optimizations/optimizations.it.html#introduzione",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "",
    "text": "Figura 9.1: Tre livelli da coprire.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#sec-model_ops_representation",
    "href": "contents/optimizations/optimizations.it.html#sec-model_ops_representation",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.2 Rappresentazione Efficiente del Modello",
    "text": "9.2 Rappresentazione Efficiente del Modello\nIl primo passo per l’ottimizzazione del modello inizia in un territorio familiare per la maggior parte dei professionisti del ML: la rappresentazione efficiente del modello viene spesso affrontata per la prima volta al livello più alto di astrazione della parametrizzazione, ovvero l’architettura stessa del modello.\nLa maggior parte dei professionisti del ML tradizionali progetta modelli con un obiettivo generale di alto livello in mente, che si tratti di classificazione delle immagini, rilevamento di persone o individuazione di parole chiave come menzionato in precedenza in questo testo. I loro progetti in genere finiscono per adattarsi naturalmente ad alcuni vincoli soft dovuti a risorse di elaborazione limitate durante lo sviluppo, ma in genere questi progetti non sono a conoscenza di vincoli successivi, come quelli richiesti se il modello deve essere distribuito su un dispositivo più limitato anziché sul cloud.\nIn questa sezione, discuteremo di come i professionisti possono sfruttare i principi della progettazione congiunta hardware-software anche nell’architettura di alto livello di un modello per rendere i loro modelli compatibili con i dispositivi edge. Da quelli più consapevoli dell’hardware a quelli meno consapevoli a questo livello di modifica, discutiamo alcune delle strategie più comuni per una parametrizzazione efficiente del modello: pruning, compressione e architetture edge-friendly. Abbiamo già parlato di pruning e compressione del modello in Sezione 8.4; questa sezione andrà oltre le definizioni per fornire una comprensione tecnica del loro funzionamento.\n\n9.2.1 Il Pruning\n\nPanoramica\nLa potatura (pruning) del modello è una tecnica di machine learning che mira a ridurre le dimensioni e la complessità di un modello di rete neurale mantenendone il più possibile le capacità predittive. L’obiettivo della potatura è quello di rimuovere componenti ridondanti o non essenziali del modello, tra cui connessioni tra neuroni, singoli neuroni o persino interi layer della rete.\nQuesto processo in genere comporta l’analisi del modello di machine learning per identificare e rimuovere pesi, nodi o layer che hanno scarso impatto sugli output del modello. Potando selettivamente un modello in questo modo, il numero totale di parametri può essere ridotto in modo significativo senza cali sostanziali nell’accuratezza del modello. Il modello compresso risultante richiede meno memoria e risorse di calcolo per l’addestramento e l’esecuzione, consentendo tempi di inferenza più rapidi.\nIl pruning del modello è particolarmente utile quando si distribuiscono modelli di apprendimento automatico su dispositivi con risorse di calcolo limitate, come telefoni cellulari o sistemi TinyML. La tecnica facilita la distribuzione di modelli più grandi e complessi su questi dispositivi riducendo le loro richieste di risorse. Inoltre, i modelli più piccoli richiedono meno dati per generalizzare bene e sono meno inclini all’overfitting [sovradattamento]. Fornendo un modo efficiente per semplificare i modelli, la potatura dei modelli è diventata una tecnica fondamentale per ottimizzare le reti neurali nell’apprendimento automatico.\nEsistono diverse tecniche di potatura comuni utilizzate nell’apprendimento automatico, tra cui la potatura strutturata, la potatura non strutturata, la potatura iterativa, la potatura bayesiana e persino la potatura casuale. Oltre a potare i pesi, si possono anche potare le attivazioni. La potatura di attivazioni prende di mira specificamente neuroni o filtri che si attivano raramente o hanno un’attivazione complessivamente bassa. Esistono numerosi altri metodi, come la potatura di sensibilità e movimento. Per un elenco completo dei metodi, si consiglia al lettore di leggere il seguente articolo: “A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations” (2023).\nQuindi, come si scelgono i metodi di potatura? Esistono molte varianti di tecniche di potatura, ciascuna delle quali varia l’euristica di ciò che dovrebbe essere mantenuto e potato dal modello, nonché il numero di volte in cui cui deve essere eseguita. Tradizionalmente, la potatura avviene dopo che il modello è completamente addestrato, dove il modello potato può subire una lieve perdita di accuratezza. Tuttavia, come discuteremo più avanti, recenti scoperte hanno trovato che la potatura può essere utilizzata durante l’addestramento (ad esempio, in modo iterativo) per identificare rappresentazioni del modello più efficienti e accurate.\n\n\nPotatura Strutturata\nIniziamo con la “potatura strutturata”, una tecnica che riduce le dimensioni di una rete neurale eliminando intere sotto-strutture specifiche del modello mantenendone la struttura generale. Rimuove interi neuroni/canali o layer in base a criteri di importanza. Ad esempio, per una rete neurale convoluzionale (CNN), potrebbero essere determinate istanze di filtro o canali. Per reti completamente connesse, potrebbero essere i neuroni stessi mantenendo la piena connettività o persino l’eliminazione di interi layer del modello che sono considerati insignificanti. Questo tipo di potatura spesso porta a reti sparse regolari e strutturate che sono compatibili con l’hardware.\nSono iniziate a emergere le “best practice” su come pensare alla potatura strutturata. Ci sono tre componenti principali:\n\n1. Strutture Candidate per il Pruning\nData la varietà di approcci, diverse strutture all’interno di una rete neurale vengono potate in base a criteri specifici. Le strutture primarie per la potatura includono neuroni, canali e talvolta interi layer, ognuno con le sue implicazioni e metodologie uniche. L’obiettivo di ogni approccio è garantire che il modello ridotto mantenga il più possibile la capacità predittiva del modello originale, migliorando al contempo l’efficienza computazionale e riducendo le dimensioni.\nQuando i neuroni vengono potati, rimuoviamo interi neuroni insieme ai loro pesi e bias associati, riducendo così la larghezza del layer. Questo tipo di potatura viene spesso utilizzato in layer completamente connessi.\nLa potatura del canale, che viene applicata prevalentemente nelle reti neurali convoluzionali (CNN), comporta l’eliminazione di interi canali o filtri, il che a sua volta riduce la profondità delle mappe delle feature e influisce sulla capacità della rete di estrarre determinate feature dai dati di input. Ciò è particolarmente cruciale nelle attività di elaborazione delle immagini in cui l’efficienza computazionale è fondamentale.\nInfine, la potatura dei layer adotta un approccio più aggressivo rimuovendo interi layer della rete. Ciò riduce significativamente la profondità della rete e quindi la sua capacità di plasmare pattern e gerarchie complesse nei dati. Questo approccio richiede un attento equilibrio per garantire che la capacità predittiva del modello non venga indebitamente compromessa.\nFigura 9.2 mostra la differenza tra la potatura di canale/filtro e quella del layer. Quando potiamo un canale, dobbiamo riconfigurare l’architettura del modello per adattarla ai cambiamenti strutturali. Una modifica consiste nel cambiare il numero di canali di input nel layer successivo (qui, il terzo e il layer più profondo): modificando le profondità dei filtri applicati al layer con il canale potato. D’altra parte, la potatura di un intero layer (rimuovendo tutti i canali nel layer) richiede modifiche più drastiche. Quella principale riguarda la modifica delle connessioni tra i layer rimanenti per sostituire o bypassare il layer potato. Nel nostro caso, riconfiguriamo per connettere il primo e l’ultimo layer. In tutti i casi di potatura, dobbiamo mettere a punto la nuova struttura per regolare i pesi.\n\n\n\n\n\n\nFigura 9.2: Potatura del canale e quella del layer.\n\n\n\n\n\n2. Stabilire un criterio per il Pruning\nStabilire criteri ben definiti per determinare quali strutture specifiche potare da un modello di rete neurale è una componente cruciale del processo di “pruning” del modello. L’obiettivo principale qui è identificare e rimuovere i componenti che contribuiscono meno alle capacità predittive del modello, mantenendo al contempo le strutture integrali per preservare l’accuratezza.\nUna strategia ampiamente adottata ed efficace per potare sistematicamente le strutture si basa sul calcolo di punteggi di importanza per singoli componenti come neuroni, filtri, canali o layer. Questi punteggi servono come metriche quantitative per valutare la significatività di ciascuna struttura e il suo effetto sull’output del modello.\nEsistono diverse tecniche per assegnare questi punteggi sull’importanza:\n\nPruning Basato sulla Magnitudo del peso: Questo approccio assegna punteggi di importanza a una struttura valutando la magnitudo aggregata dei pesi associati. Le strutture con magnitudo del peso complessivo inferiore sono considerate meno critiche per le prestazioni della rete.\nPruning Basato sul Bradiente: Questa tecnica utilizza i gradienti della funzione di los [perdita] rispetto ai pesi associati a una struttura. Le strutture con magnitudo del gradiente cumulativo basso, che indica un impatto minimo sulla perdita quando alterato, sono le candidate principali per la potatura.\nPruning Basato sull’Attivazione: Questo metodo tiene traccia della frequenza con cui un neurone o un filtro viene attivato memorizzando queste informazioni in un parametro chiamato contatore delle attivazioni. Ogni volta che la struttura viene attivata, il contatore viene incrementato. Un conteggio di attivazione basso suggerisce che la struttura è meno rilevante.\nPruning Basato sull’Espansione di Taylor: Questo approccio approssima la modifica nella funzione di perdita derivante dalla rimozione di un dato peso. Valutando la perturbazione della perdita cumulativa derivante dalla rimozione di tutti i pesi associati a una struttura, è possibile identificare le strutture con un impatto trascurabile sulla perdita, rendendole candidate idonee per la potatura.\n\nL’idea è di misurare, direttamente o indirettamente, il contributo di ogni componente all’output del modello. Le strutture con un’influenza minima in base ai criteri definiti vengono potate per prime. Ciò consente una potatura selettiva e ottimizzata che comprime al massimo i modelli preservando al contempo la capacità predittiva. In generale, è importante valutare l’impatto della rimozione di particolari strutture sull’output del modello, con lavori recenti come (Rachwan et al. 2022) e (Lubana e Dick 2020) che studiano combinazioni di tecniche come la potatura basata sulla magnitudine e la potatura basata sul gradiente.\n\nRachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, e Stephan Günnemann. 2022. «Winning the lottery ahead of time: Efficient early network pruning». In International Conference on Machine Learning, 18293–309. PMLR.\n\nLubana, Ekdeep Singh, e Robert P Dick. 2020. «A gradient flow framework for analyzing network pruning». arXiv preprint arXiv:2009.11839.\n\n\n3. Selezione di una strategia di potatura\nOra che abbiamo capito alcune tecniche per determinare l’importanza delle strutture all’interno di una rete neurale, il passo successivo è decidere come applicare queste intuizioni. Ciò comporta la selezione di una strategia di potatura appropriata, che stabilisce come e quando le strutture identificate vengono rimosse e come il modello viene messo a punto per mantenere le sue prestazioni. Esistono due principali strategie di potatura strutturata: quella iterativa e la one-shot.\nLa potatura iterativa rimuove gradualmente le strutture attraverso più cicli di potatura seguiti da messa a punto. In ogni ciclo, un piccolo set di strutture viene potato in base a criteri di importanza. Il modello viene poi messo a punto, consentendogli di adattarsi senza problemi ai cambiamenti strutturali prima della successiva iterazione di potatura. Questo approccio graduale e ciclico impedisce bruschi cali di accuratezza. Consente al modello di adattarsi lentamente man mano che le strutture vengono ridotte attraverso le iterazioni.\nConsideriamo una situazione in cui desideriamo potare i 6 canali meno efficaci (in base ad alcuni criteri specifici) da una rete neurale convoluzionale. In Figura 9.3, mostriamo un processo di potatura semplificato eseguito su 3 iterazioni. In ogni iterazione, eliminiamo solo 2 canali. La rimozione dei canali comporta un degrado della precisione. Nella prima iterazione, la precisione scende da 0.995 a 0.971. Tuttavia, dopo aver perfezionato il modello sulla nuova struttura, siamo in grado di recuperare dalla perdita di prestazioni, portando la precisione a 0.992. Poiché i cambiamenti strutturali sono minori e graduali, la rete può adattarsi più facilmente a essi. Eseguendo lo stesso processo altre 2 volte, finiamo con una precisione finale di 0.991 (una perdita di solo lo 0.4% rispetto all’originale) e una riduzione del 27% nel numero di canali. Pertanto, la potatura iterativa ci consente di mantenere le prestazioni beneficiando di una maggiore efficienza computazionale dovuta alla riduzione delle dimensioni del modello.\n\n\n\n\n\n\nFigura 9.3: Potatura iterativa.\n\n\n\nLa potatura one-shot adotta un approccio più aggressivo, potando una grande porzione di strutture simultaneamente in un’unica operazione in base a criteri di importanza predefiniti. Segue un’ampia messa a punto per recuperare l’accuratezza del modello. Sebbene più rapida, questa strategia aggressiva può degradare l’accuratezza se il modello non riesce a recuperare durante la messa a punto.\nLa scelta tra queste strategie comporta la valutazione di fattori quali dimensioni del modello, quanto è sparso il target, calcolo disponibile e perdite di accuratezza accettabili. La potatura one-shot può comprimere rapidamente i modelli, ma quella iterativa può consentire una migliore conservazione dell’accuratezza per un livello target di potatura. In pratica, la strategia è personalizzata in base ai vincoli del caso d’uso. L’obiettivo generale è quello di generare una strategia ottimale che rimuova la ridondanza, ottenga guadagni di efficienza tramite la potatura e metta a punto il modello per stabilizzare l’accuratezza a un livello accettabile per l’implementazione.\nOra si consideri la stessa rete che avevamo nell’esempio di potatura iterativa. Mentre nel processo iterativo abbiamo potato 2 canali alla volta, nella potatura one-shot poteremo i 6 canali contemporaneamente (Figura 9.4). La rimozione simultanea del 27% del canale della rete altera significativamente la struttura, causando un calo della precisione da 0.995 a 0.914. Date le modifiche principali, la rete non è in grado di adattarsi correttamente durante la messa a punto e la precisione è salita a 0.943, un degrado del 5% rispetto alla precisione della rete non potata. Mentre le strutture finali nei processi di potatura iterativa e di potatura one-shot sono identiche, la prima è in grado di mantenere prestazioni elevate mentre la seconda subisce degradi significativi.\n\n\n\n\n\n\nFigura 9.4: Potatura one-shot.\n\n\n\n\n\n\nVantaggi della Potatura Strutturata\nLa potatura strutturata offre una miriade di vantaggi che soddisfano vari aspetti dell’implementazione e dell’utilizzo del modello, specialmente in ambienti in cui le risorse computazionali sono limitate.\n\nEfficienza Computazionale: Eliminando intere strutture, come neuroni o canali, si riduce significativamente il carico computazionale durante le fasi di training e inferenza, consentendo così previsioni più rapide del modello e convergenza del training. Inoltre, la rimozione delle strutture riduce intrinsecamente il “footprint” [impronta] di memoria del modello, assicurando che richieda meno spazio di archiviazione e memoria durante il funzionamento, il che è particolarmente vantaggioso in ambienti con limiti di memoria come i sistemi TinyML.\nEfficienza Hardware: La potatura strutturata spesso si traduce in modelli più adatti all’implementazione su hardware specializzato, come i Field-Programmable Gate Arrays (FPGA) o Application-Specific Integrated Circuits (ASIC), a causa della regolarità e la semplicità dell’architettura potata. Con requisiti di elaborazione ridotti, si traduce in un consumo energetico inferiore, fondamentale per i dispositivi alimentati a batteria e i metodi di elaborazione sostenibili.\nManutenzione e Distribuzione: Il modello ridotto, sebbene più piccolo, mantiene la sua forma architettonica originale, che può semplificare la pipeline di distribuzione e garantire la compatibilità con i sistemi e i framework esistenti. Inoltre, con meno parametri e strutture più semplici, il modello potato diventa più facile da gestire e monitorare negli ambienti di produzione, riducendo potenzialmente le spese generali associate alla manutenzione e agli aggiornamenti del modello. Più avanti, quando approfondiremo MLOps, questa necessità diventerà evidente.\n\n\n\nPotatura non Strutturata\nIl “pruning” non-strutturato è, come suggerisce il nome, la potatura del modello senza riguardo alla sotto-struttura specifica del modello. Come accennato in precedenza, offre una maggiore aggressività nella potatura e può raggiungere maggiori diradazione del modello mantenendo la precisione, dati meno vincoli su ciò che può e non può essere potato. In genere, la potatura non-strutturata post-training consiste in un criterio di importanza per i singoli parametri/pesi del modello, potatura/rimozione dei pesi che scendono al di sotto dei criteri e una successiva messa a punto facoltativa per provare a recuperare la precisione persa durante la rimozione dei pesi.\nLa potatura non-strutturata presenta alcuni vantaggi rispetto a quella strutturata: la rimozione di singoli pesi anziché di intere sotto-strutture del modello spesso porta in pratica a minori diminuzioni della precisione del modello. Inoltre, in genere determinare il criterio di importanza per un singolo peso è molto più semplice che per un’intera sotto-struttura di parametri nella potatura strutturata, rendendo la prima preferibile nei casi in cui tale overhead è difficile o poco chiaro da calcolare. Analogamente, il processo effettivo di potatura strutturata è generalmente meno flessibile, poiché la rimozione di singoli pesi è generalmente più semplice della rimozione di intere sotto-strutture e della garanzia che il modello funzioni ancora.\nLa potatura non strutturata, pur offrendo il potenziale per una significativa riduzione delle dimensioni del modello e una migliore implementabilità, porta con sé problemi legati alla gestione di rappresentazioni sparse e alla garanzia dell’efficienza computazionale. È particolarmente utile in scenari in cui è fondamentale ottenere la massima compressione possibile del modello e in cui l’ambiente di distribuzione può gestire in modo efficiente i calcoli sparsi.\nTabella 9.1 fornisce un confronto conciso tra potatura strutturata e la non-strutturata. In questa tabella, gli aspetti relativi alla natura e all’architettura del modello potato (Definizione, Regolarità del modello e Livello di compressione) sono raggruppati insieme, seguiti dagli aspetti relativi alle considerazioni computazionali (Efficienza computazionale e Compatibilità hardware) e terminando con gli aspetti relativi all’implementazione e all’adattamento del modello potato (Complessità di implementazione e Complessità di messa a punto). Entrambe le strategie di potatura offrono vantaggi e problemi unici, come mostrato in Tabella 9.1, e la selezione tra di esse dovrebbe essere influenzata da requisiti specifici del progetto e della distribuzione.\n\n\n\nTabella 9.1: Confronto tra potatura strutturata e non-strutturata.\n\n\n\n\n\n\n\n\n\n\nAspetto\nPotatura strutturata\nPotatura non strutturata\n\n\n\n\nDefinizione\nPotatura di intere strutture (ad esempio, neuroni, canali, layer) all’interno della rete\nPotatura di singoli pesi o neuroni, con conseguenti matrici sparse o strutture di rete non regolari\n\n\nRegolarità del Modello\nMantiene un’architettura di rete regolare e strutturata\nSi traduce in architetture di rete irregolari e sparse\n\n\nLivello di Compressione\nPuò offrire una compressione del modello limitata rispetto alla potatura non-strutturata\nPuò ottenere una compressione del modello più elevata grazie alla potatura a grana fine\n\n\nEfficienza Computazionale\nIn genere più efficiente computazionalmente grazie al mantenimento di strutture regolari\nPuò essere inefficiente dal punto di vista computazionale a causa di matrici di peso sparse, a meno che non venga utilizzato hardware/software specializzato\n\n\nCompatibilità Hardware\nIn genere più compatibile con vari hardware grazie alle strutture regolari\nPotrebbe richiedere hardware che gestisca in modo efficiente i calcoli sparsi per ottenere vantaggi\n\n\nComplessità di Implementazione\nSpesso più semplice da implementare e gestire grazie al mantenimento della struttura della rete\nPuò essere complesso da gestire e calcolare a causa delle rappresentazioni sparse\n\n\nComplessità di Messa a Punto Fine\nPotrebbe richiedere strategie di messa a punto fine meno complesse dopo la potatura\nPotrebbe richiedere strategie di riaddestramento o messa a punto fine più complesse dopo la potatura\n\n\n\n\n\n\nIn Figura 9.5 abbiamo esempi che illustrano le differenze tra potatura non-strutturata e strutturata. Osservare che la potatura non-strutturata può portare a modelli che non rispettano più le garanzie strutturali di alto livello delle loro controparti originali non potate: la rete di sinistra non è più una rete completamente connessa dopo la potatura. La potatura strutturata, d’altro canto, mantiene quelle invarianti: al centro, la rete completamente connessa viene potata in modo che resti ancora completamente connessa; allo stesso modo, la CNN mantiene la sua struttura convoluzionale, sebbene con meno filtri.\n\n\n\n\n\n\nFigura 9.5: Potatura non-strutturata e strutturata. Fonte: Qi et al. (2021).\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang, e Honggang Zhang. 2021. «An efficient pruning scheme of deep neural networks for Internet of Things applications». EURASIP Journal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\n\n\nIpotesi del Biglietto della Lotteria\nLa potatura si è evoluta da una tecnica puramente post-addestramento che comportava un costo per una certa accuratezza, a un potente approccio di meta-apprendimento applicato durante l’addestramento per ridurre la complessità del modello. Questo progresso a sua volta migliora l’efficienza di calcolo, memoria e latenza sia nell’addestramento che nell’inferenza.\nUna scoperta rivoluzionaria che ha catalizzato questa evoluzione è stata l’ipotesi del biglietto della lotteria di Frankle e Carbin (2019). Il loro lavoro afferma che all’interno di reti neurali dense esistono sotto-reti sparse, denominate “biglietti vincenti”, che possono eguagliare o addirittura superare le prestazioni del modello originale quando addestrate in isolamento. In particolare, questi biglietti vincenti, quando inizializzati utilizzando gli stessi pesi della rete originale, possono raggiungere una convergenza e un’accuratezza di addestramento altrettanto elevate su un dato compito. Vale la pena sottolineare che hanno scoperto empiricamente l’ipotesi del biglietto della lotteria, che è stata successivamente formalizzata.\n\nFrankle, Jonathan, e Michael Carbin. 2019. «The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks». In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\nL’intuizione alla base di questa ipotesi è che, durante il processo di addestramento di una rete neurale, molti neuroni e connessioni diventano ridondanti o non importanti, in particolare con l’inclusione di tecniche di addestramento che incoraggiano la ridondanza come il “dropout” [abbandono]. L’identificazione, la potatura e l’inizializzazione di questi “biglietti vincenti” consentono un addestramento più rapido e modelli più efficienti, poiché contengono le informazioni essenziali per la decisione del modello per l’attività. Inoltre, come generalmente noto con la teoria del “bias-variance tradeoff” [compromesso tra bias e varianza], questi biglietti soffrono meno di sovra-parametrizzazione e quindi si generalizzano meglio piuttosto che sovra-adattarsi all’attività.\nIn Figura 9.6 abbiamo un esempio che mostra esperimenti di potatura e addestramento su una LeNet completamente connessa su una varietà di rapporti di potatura. Nel grafico a sinistra, si nota come una potatura pesante riveli una sotto-rete più efficiente (in verde) che è il 21,1% delle dimensioni della rete originale (in blu). La sotto-rete raggiunge una maggiore accuratezza e in modo più rapido rispetto alla versione non potata (la linea verde è sopra la linea blu). Tuttavia, la potatura ha un limite (punto ottimale) e un’ulteriore potatura produrrà degradi delle prestazioni e alla fine scenderà al di sotto delle prestazioni della versione non potata (nota come le sotto-reti rossa, viola e marrone diminuiscono gradualmente nelle prestazioni di accuratezza) a causa della significativa perdita nel numero di parametri.\n\n\n\n\n\n\nFigura 9.6: Esperimenti sull’ipotesi del biglietto della lotteria.\n\n\n\nPer scoprire questi biglietti vincenti della lotteria all’interno di una rete neurale, viene seguito un processo sistematico. Questo processo, illustrato in Figura 9.7 (a sinistra), prevede l’addestramento iterativo, la potatura e la reinizializzazione della rete. I passaggi seguenti delineano questo approccio:\n\nInizializzare i pesi della rete a valori casuali.\nAddestrare la rete finché non converge alle prestazioni desiderate.\nEliminare una percentuale di rami con i valori di peso più bassi.\nReinizializzare la rete con gli stessi valori casuali del passaggio 1.\nRipetere i passaggi 2-4 più volte o finché la precisione non peggiora in modo significativo.\n\nAlla fine, ci si ritrova con una rete potata (Figura 9.7 lato destro), che è una sotto-rete di quella di partenza. La sotto-rete dovrebbe avere una struttura significativamente più piccola, pur mantenendo un livello di precisione comparabile.\n\n\n\n\n\n\nFigura 9.7: Trovare la sottorete del biglietto vincente.\n\n\n\n\n\nProblemi e Limitazioni\nNon c’è niente di gratuito con le ottimizzazioni di potatura, con alcune scelte che comportano sia miglioramenti che costi da considerare. Di seguito, discutiamo alcuni compromessi che gli esperti devono considerare.\n\nGestione di Matrici di Peso Sparse: Una matrice di peso sparsa è una matrice in cui molti degli elementi sono pari a zero. La potatura non strutturata spesso produce matrici di peso sparse, in cui molti pesi vengono potati a zero. Sebbene ciò riduca le dimensioni del modello, introduce anche diversi problemi. L’inefficienza computazionale può sorgere perché l’hardware standard è ottimizzato per operazioni di matrice densa. Senza ottimizzazioni che sfruttano la sparsità, i risparmi computazionali derivanti dalla potatura possono essere persi. Sebbene le matrici sparse possano essere archiviate senza formati specializzati, sfruttare efficacemente la loro sparsità richiede una gestione attenta per evitare di sprecare risorse. Algoritmicamente, la navigazione in strutture sparse richiede di saltare in modo efficiente le voci zero, il che aggiunge complessità al calcolo e agli aggiornamenti del modello.\nQualità vs. Riduzione delle Dimensioni: Una sfida fondamentale sia nella potatura strutturata che in quella non-strutturata è bilanciare la riduzione delle dimensioni con il mantenimento o il miglioramento delle prestazioni predittive. È essenziale stabilire criteri di potatura robusti, sia per rimuovere intere strutture (potatura strutturata) sia singoli pesi (potatura non strutturata). Questi criteri di potatura scelti devono identificare accuratamente gli elementi la cui rimozione ha un impatto minimo sulle prestazioni. Spesso è necessaria un’attenta sperimentazione per garantire che il modello potato rimanga efficiente mantenendo al contempo le sue prestazioni predittive.\nFine-Tuning e Riaddestramento: La messa a punto post-potatura è fondamentale sia nella potatura strutturata che in quella non-strutturata per recuperare le prestazioni perse e stabilizzare il modello. La sfida comprende la determinazione dell’estensione, della durata e della natura del processo di messa a punto, che può essere influenzato dal metodo di potatura e dal grado di potatura applicato.\nCompatibilità ed Efficienza Hardware: Particolarmente pertinenti alla potatura non-strutturata, la compatibilità e l’efficienza hardware diventano critiche. La potatura non strutturata spesso si traduce in matrici di peso sparse, che potrebbero non essere gestite in modo efficiente da un certo hardware, annullando potenzialmente i vantaggi computazionali della potatura (vedere Figura 9.8). Garantire che i modelli potati, in particolare quelli risultanti dall’eliminazione non-strutturata, siano scalabili, compatibili ed efficienti sull’hardware target è una considerazione importante.\nConsiderazioni Legali ed Etiche: Ultimo ma non meno importante, il rispetto delle linee guida legali ed etiche è importante, soprattutto in ambiti con conseguenze significative. I metodi di potatura devono essere sottoposti a rigorosi processi di validazione, test e potenzialmente certificazione per garantire la conformità alle normative e agli standard pertinenti, sebbene al momento non esistano standard formali e “best practice” che siano esaminati e convalidati da entità terze. Ciò è particolarmente cruciale in applicazioni ad alto rischio come l’intelligenza artificiale medica e la guida autonoma, dove i cali di qualità dovuti a ottimizzazioni simili alla potatura possono essere pericolosi per la vita. Inoltre, le considerazioni etiche si estendono oltre la sicurezza fino all’equità e all’uguaglianza; un recente lavoro di (Tran et al. 2022) ha rivelato che la potatura può avere un impatto sproporzionato sulle persone di colore, sottolineando la necessità di una valutazione etica completa nel processo di potatura.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, e Rakshit Naidu. 2022. «Pruning has a disparate impact on model accuracy». Adv Neural Inf Process Syst 35: 17652–64.\n\n\n\n\n\n\nFigura 9.8: Matrice dei pesi sparsi.\n\n\n\n\n\n\n\n\n\nEsercizio 9.1: Pruning\n\n\n\n\n\nSi immagini che la rete neurale sia un cespuglio gigante e troppo cresciuto. La potatura è come tagliare strategicamente i rami per renderla più forte ed efficiente! Nel Colab, si imparerà come fare questa potatura in TensorFlow. La comprensione di questi concetti fornirà le basi per vedere come la potatura rende i modelli abbastanza piccoli da poter essere eseguiti sul telefono!\n\n\n\n\n\n\n\n9.2.2 Compressione del Modello\nLe tecniche di compressione del modello sono fondamentali per distribuire modelli di deep learning su dispositivi con risorse limitate. Queste tecniche mirano a creare modelli più piccoli ed efficienti che preservino le prestazioni predittive dei modelli originali.\n\nDistillazione della Conoscenza\nUna tecnica popolare è la knowledge distillation (KD) distillazione della conoscenza, che trasferisce la conoscenza da un modello “insegnante” ampio e complesso a un modello “studente” più piccolo. L’idea chiave è addestrare il modello studente a imitare gli output dell’insegnante. Il concetto di KD è stato reso popolare per la prima volta da Hinton (2005).\n\nHinton, Geoffrey. 2005. «Van Nostrand’s Scientific Encyclopedia». Wiley. https://doi.org/10.1002/0471743984.vse0673.\n\nPanoramica e Vantaggi\nLa distillazione della conoscenza implica il trasferimento della conoscenza da un modello insegnante ampio e complesso a un modello studente più piccolo. L’idea di base è quella di utilizzare gli output dell’insegnante, noti come soft targets, per guidare il training del modello studente. A differenza dei tradizionali “hard targets” (le vere etichette), quelli soft sono le distribuzioni di probabilità sulle classi che il modello insegnante prevede. Queste distribuzioni forniscono informazioni più complete sulle relazioni tra le classi, il che può aiutare il modello studente ad apprendere in modo più efficace.\nAbbiamo imparato che la funzione softmax converte gli output grezzi di un modello in una distribuzione di probabilità sulle classi. Una tecnica chiave in KD è la scalatura della temperatura, che viene applicata alla funzione softmax degli output del modello insegnante. Introducendo un parametro di temperatura, la distribuzione può essere regolata: una temperatura più alta produce probabilità più soft, il che significa che le differenze tra le probabilità di classe diventano meno estreme. Questo effetto di ammorbidimento determina una distribuzione più uniforme, in cui la fiducia del modello nella classe più probabile è ridotta e altre classi hanno probabilità più elevate, diverse da zero. Ciò è prezioso per il modello studente perché gli consente di apprendere non solo dalla classe più probabile, ma anche dalle probabilità relative di tutte le classi, catturando modelli sottili che potrebbero essere persi se addestrati solo su obiettivi difficili. Pertanto, la scalabilità della temperatura facilita il trasferimento di conoscenze più sfumate dal modello insegnante a quello studente.\nLa funzione di perdita nella distillazione della conoscenza in genere combina due componenti: una perdita di distillazione e una perdita di classificazione. La perdita di distillazione, spesso calcolata utilizzando la divergenza di Kullback-Leibler (KL), misura la differenza tra gli soft target prodotti dal modello insegnante e gli output del modello studente, incoraggiando lo studente a imitare le previsioni dell’insegnante. Nel frattempo, la perdita di classificazione assicura che il modello studente preveda correttamente le etichette vere in base ai dati originali. Insieme, queste due componenti aiutano lo studente modello a conservare le conoscenze dell’insegnante, rispettando al contempo le etichette di verità di base.\nQuesti componenti, quando configurati e armonizzati abilmente, consentono al modello studente di assimilare la conoscenza del modello insegnante, creando un percorso verso modelli più piccoli, efficienti e robusti, che mantengono la capacità predittiva delle loro controparti più grandi. Figura 9.9 visualizza la procedura di training della “knowledge distillation”. Notare come i logit o le soft label del modello insegnante vengono utilizzati per fornire una perdita di distillazione da cui il modello studente può imparare.\n\n\n\n\n\n\nFigura 9.9: Processo di training della distillazione della conoscenza. Fonte: IntelLabs (2023).\n\n\nIntelLabs. 2023. «Knowledge Distillation - Neural Network Distiller». https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\n\n\nSfide\nTuttavia, KD presenta una serie unica di sfide e considerazioni che ricercatori e professionisti devono affrontare attentamente. Una delle sfide è nella messa a punto meticolosa degli iperparametri, come il parametro “temperatura” nella funzione softmax e la ponderazione tra la distillazione e la perdita di classificazione nella funzione obiettivo. Raggiungere un equilibrio che sfrutti efficacemente gli output ammorbiditi del modello insegnante mantenendo al contempo la fedeltà alle etichette dei dati reali non è banale e può avere un impatto significativo sulle prestazioni e sulle capacità di generalizzazione del modello studente.\nInoltre, l’architettura del modello studente stesso pone un problema considerevole. Progettare un modello compatto per soddisfare i vincoli di calcolo e memoria, pur essendo in grado di assimilare le conoscenze essenziali dal modello insegnante, richiede una comprensione sfumata della capacità del modello e dei compromessi intrinseci coinvolti nella compressione. Il modello studente deve essere attentamente progettato per navigare nella dicotomia di dimensioni e prestazioni, assicurando che la conoscenza distillata venga catturata e utilizzata in modo significativo. Inoltre, la scelta del modello dell’insegnante, che influenza intrinsecamente la qualità e la natura della conoscenza da trasferire, è importante e introduce un ulteriore livello di complessità nel processo KD.\nQueste sfide sottolineano la necessità di un approccio completo e sfumato all’implementazione di KD, assicurando che i modelli degli studenti risultanti siano sia efficienti che efficaci nei loro contesti operativi.\n\n\n\nFattorizzazione di Matrici di Basso Rango\nSimile nel tema dell’approssimazione, la Low-Rank Matrix Factorization (LRMF) fattorizzazione di matrici di basso rango è una tecnica matematica utilizzata in algebra lineare e analisi dei dati per approssimare una matrice data scomponendola in due o più matrici di dimensione inferiore. L’idea fondamentale è di esprimere una matrice di grandi dimensioni come prodotto di matrici di rango inferiore, il che può aiutare a ridurre la complessità dei dati preservandone la struttura essenziale. Matematicamente, data una matrice \\(A \\in \\mathbb{R}^{m \\times n}\\), LRMF cercare le matrici \\(U \\in \\mathbb{R}^{m \\times k}\\) e \\(V \\in \\mathbb{R}^{k \\times n}\\) tali che \\(A \\approx UV\\), dove \\(k\\) è il rango ed è in genere molto più piccolo di \\(m\\) e \\(n\\).\n\nBackground e Benefici\nUno dei primo lavori nel campo della fattorizzazione di matrici, in particolare nel contesto dei sistemi di raccomandazione, è il documento di Koren, Bell, e Volinsky (2009). Gli autori esaminano vari modelli di fattorizzazione, fornendo approfondimenti sulla loro efficacia nel catturare i modelli sottostanti nei dati e nel migliorare l’accuratezza predittiva nel filtraggio collaborativo. LRMF è stato ampiamente applicato nei sistemi di raccomandazione (come Netflix, Facebook, ecc.), dove la matrice di interazione utente-elemento è fattorizzata per catturare fattori latenti corrispondenti alle preferenze dell’utente e agli attributi dell’elemento.\n\nKoren, Yehuda, Robert Bell, e Chris Volinsky. 2009. «Matrix Factorization Techniques for Recommender Systems». Computer 42 (8): 30–37. https://doi.org/10.1109/mc.2009.263.\nIl vantaggio principale della “fattorizzazione di matrici di basso rango” risiede nella sua capacità di ridurre la dimensionalità dei dati come mostrato in Figura 9.10, dove ci sono meno parametri da memorizzare, rendendola più efficiente dal punto di vista computazionale e riducendo i requisiti di archiviazione a costo di un po’ di elaborazione aggiuntiva. Ciò può portare a calcoli più rapidi e rappresentazioni di dati più compatte, il che è particolarmente prezioso quando si ha a che fare con grandi set di dati. Inoltre, può aiutare nella riduzione del rumore e può rivelare modelli e relazioni sottostanti nei dati.\nFigura 9.10 illustra la diminuzione della parametrizzazione abilitata dalla fattorizzazione di matrici di basso rango. Osservare come la matrice \\(M\\) può essere approssimata dal prodotto delle matrici \\(L_k\\) e \\(R_k^T\\). Per intuizione, la maggior parte dei layer completamente connessi nelle reti sono archiviati come matrice di proiezione \\(M\\), che richiede il caricamento di \\(m \\times n\\) parametri durante il calcolo. Tuttavia, scomponendola e approssimandola come prodotto di due matrici di rango inferiore, abbiamo bisogno di archiviare solo \\(m \\times k + k\\times n\\) parametri in termini di archiviazione, sostenendo al contempo un costo di calcolo aggiuntivo per la moltiplicazione delle matrici. Finché \\(k &lt; n/2\\), questa fattorizzazione ha meno parametri totali da archiviare, aggiungendo un calcolo di runtime \\(O(mkn)\\) (Gu 2023).\n\nGu, Ivy. 2023. «Deep Learning Model Compression (ii) by Ivy Gu Medium». https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\n\n\n\n\nFigura 9.10: Fattorizzazione di matrici di basso rango. Fonte: The Clever Machine.\n\n\n\n\n\nSfide\nMa professionisti e ricercatori incontrano una serie di problemi e considerazioni che richiedono una particolare attenzione e approcci strategici. Come con qualsiasi tecnica di compressione lossy [con perdita], potremmo perdere informazioni durante questo processo di approssimazione: scegliere il rango corretto che bilanci le informazioni perse e i costi computazionali è altrettanto complicato e aggiunge un ulteriore iperparametro da regolare.\nLa fattorizzazione di matrici di basso rango è uno strumento prezioso per la riduzione della dimensionalità e per adattare il calcolo ai dispositivi edge ma, come altre tecniche, deve essere attentamente regolata in base al modello e all’attività da svolgere. Una sfida fondamentale risiede nella gestione della complessità computazionale inerente a LRMF, soprattutto quando si hanno a che fare con dati ad alta dimensionalità e su larga scala. L’onere computazionale, in particolare nel contesto di applicazioni in tempo reale e set di dati massicci, rimane un ostacolo significativo per un utilizzo efficace di LRMF.\nInoltre, l’enigma della scelta del rango ottimale \\(k\\) per la fattorizzazione introduce un ulteriore livello di complessità. La selezione di \\(k\\) implica intrinsecamente un compromesso tra accuratezza dell’approssimazione e semplicità del modello, e l’identificazione di un rango che bilanci abilmente questi obiettivi contrastanti spesso richiede una combinazione di competenza di dominio, convalida empirica e, a volte, approcci euristici. La sfida è ulteriormente amplificata quando i dati comprendono rumore o quando la struttura intrinseca di basso rango non è pronunciata, rendendo la determinazione di un \\(k\\) adatto ancora più sfuggente.\nLa gestione di dati mancanti o sparsi, un evento comune in applicazioni come i sistemi di raccomandazione, pone un’altra sfida sostanziale. Le tecniche tradizionali di fattorizzazione delle matrici, come la Singular Value Decomposition (SVD), non sono direttamente applicabili alle matrici con voci mancanti, rendendo necessario lo sviluppo e l’applicazione di algoritmi specializzati in grado di fattorizzare matrici incomplete mitigando al contempo i rischi di overfitting alle voci osservate. Ciò spesso comporta l’incorporazione di termini di regolarizzazione o la limitazione della fattorizzazione in modi specifici, il che a sua volta introduce ulteriori iperparametri che devono essere selezionati giudiziosamente.\nInoltre, in scenari in cui i dati evolvono o crescono nel tempo, sviluppare modelli LRMF in grado di adattarsi a nuovi dati senza richiedere una completa rifattorizzazione è un’impresa critica ma impegnativa. Gli algoritmi di fattorizzazione di matrici incrementali e online cercano di risolvere questo problema consentendo l’aggiornamento delle matrici fattorizzate all’arrivo di nuovi dati, ma garantire stabilità, accuratezza ed efficienza computazionale in queste impostazioni dinamiche rimane un compito intricato. Ciò è particolarmente impegnativo nello spazio di TinyML, in cui la ridistribuzione dei rami per i modelli aggiornati può essere piuttosto impegnativa.\n\n\n\nDecomposizione dei Tensori\nAbbiamo visto in Sezione 6.4.1 che i tensori sono strutture flessibili, comunemente utilizzate dai framework ML, che possono rappresentare dati in dimensioni superiori. Similmente alla fattorizzazione di matrici di basso rango, i modelli più complessi possono memorizzare pesi in dimensioni superiori, come i tensori. La decomposizione tensoriale è l’analogo di dimensioni superiori della fattorizzazione di matrici, in cui un tensore modello viene scomposto in componenti di rango inferiore (cfr. Figura 9.11). Questi componenti di rango inferiore sono più facili da calcolare e memorizzare, ma possono soffrire degli stessi problemi menzionati sopra, come la perdita di informazioni e la necessità di una messa a punto sfumata degli iperparametri. Matematicamente, dato un tensore \\(\\mathcal{A}\\), la decomposizione tensoriale cerca di rappresentare \\(\\mathcal{A}\\) come una combinazione di tensori più semplici, facilitando una rappresentazione compressa che approssima i dati originali riducendo al minimo la perdita di informazioni.\nIl lavoro di Tamara G. Kolda e Brett W. Bader, “Tensor Decompositions and Applications” (2009), si distingue come un articolo fondamentale nel campo delle decomposizioni tensoriali. Gli autori forniscono una panoramica completa di vari metodi di decomposizione tensoriale, esplorandone i fondamenti matematici, gli algoritmi e un’ampia gamma di applicazioni, che vanno dall’elaborazione del segnale al data mining. Naturalmente, il motivo per cui ne stiamo discutendo è perché ha un enorme potenziale per i miglioramenti delle prestazioni del sistema, in particolare nello spazio di TinyML, dove la produttività e i risparmi di memoria sono fondamentali per la fattibilità delle distribuzioni.\n\n\n\n\n\n\nFigura 9.11: Decomposizione dei Tensori. Fonte: Xinyu (s.d.).\n\n\nXinyu, Chen. s.d.\n\n\n\n\n\n\n\n\nEsercizio 9.2: Compressione di Modelli Scalabili con TensorFlow\n\n\n\n\n\nQuesto Colab si addentra in una tecnica per comprimere i modelli mantenendo un’elevata accuratezza. L’idea chiave è quella di addestrare un modello con un termine di penalità extra che incoraggia il modello a essere più comprimibile. Quindi, il modello viene codificato utilizzando uno schema di codifica speciale che si allinea con questa penalità. Questo approccio consente di ottenere modelli compressi che funzionano altrettanto bene dei modelli originali ed è utile per distribuire modelli su dispositivi con risorse limitate come telefoni cellulari e dispositivi edge.\n\n\n\n\n\n\n\n9.2.3 Modelli Progettati per l’Edge\nOra raggiungiamo l’altro estremo del gradiente hardware-software, dove prendiamo decisioni specifiche sull’architettura del modello direttamente in base alla conoscenza dei dispositivi edge su cui desideriamo implementare.\nCome spiegato nelle sezioni precedenti, i dispositivi edge sono vincolati specificamente da limitazioni di memoria e calcoli parallelizzabili: in quanto tali, se ci sono requisiti critici di velocità di inferenza, i calcoli devono essere sufficientemente flessibili da soddisfare i vincoli hardware, qualcosa che può essere progettato a livello di architettura del modello. Inoltre, cercare di stipare grandi modelli SOTA ML su dispositivi edge anche dopo potatura e compressione è generalmente irrealizzabile puramente a causa delle dimensioni: la complessità del modello stesso deve essere scelta con più sfumature per adattarsi più fattibilmente al dispositivo. Gli sviluppatori di Edge ML hanno affrontato questa sfida architettonica sia attraverso la progettazione di architetture di modelli edge ML su misura sia attraverso la Neural Architecture Search (NAS) [ricerca di architettura neurale] avente il dispositivo come target, che può generare in modo più sistematico architetture fattibili di modelli su dispositivo.\n\nTecniche di Progettazione del Modello\nUn design di architettura edge friendly, comunemente utilizzato nel deep learning per l’elaborazione delle immagini, è quello delle convoluzioni separabili in profondità. Consiste in due fasi distinte: la prima è la convoluzione in profondità, in cui ogni canale di input viene convoluto in modo indipendente con il proprio set di filtri apprendibili, come mostrato in Figura 9.12. Questa fase riduce la complessità computazionale in modo significativo rispetto alle convoluzioni standard, poiché riduce drasticamente il numero di parametri e calcoli coinvolti. La seconda fase è la convoluzione puntuale, che combina l’output dei canali di convoluzione in profondità tramite una convoluzione 1x1, creando interazioni tra canali. Questo approccio offre diversi vantaggi. I vantaggi includono dimensioni ridotte del modello, tempi di inferenza più rapidi e spesso una migliore generalizzazione grazie al minor numero di parametri, rendendolo adatto ad applicazioni mobili ed embedded. Tuttavia, le convoluzioni separabili in profondità potrebbero non catturare interazioni spaziali complesse in modo efficace come le convoluzioni standard e potrebbero richiedere più profondità (livelli) per raggiungere lo stesso livello di potenza rappresentativa, portando potenzialmente a tempi di addestramento più lunghi. Tuttavia, la loro efficienza in termini di parametri e calcolo le rende una scelta popolare nelle moderne architetture di reti neurali convoluzionali.\n\n\n\n\n\n\nFigura 9.12: Convoluzioni separabili in profondità. Fonte: Hegde (2023).\n\n\nHegde, Sumant. 2023. «An Introduction to Separable Convolutions - Analytics Vidhya». https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\n\n\nArchitetture di Modello di Esempio\nIn quest’ottica, diverse architetture recenti sono state, fin dall’inizio, progettate specificamente per massimizzare la precisione in un’implementazione edge, in particolare SqueezeNet, MobileNet ed EfficientNet.\n\nSqueezeNet di Iandola et al. (2016), ad esempio, utilizza un’architettura compatta con convoluzioni 1x1 e moduli “fire” per ridurre al minimo il numero di parametri mantenendo al contempo una forte accuratezza.\nMobileNet di Howard et al. (2017), d’altra parte, impiega le suddette convoluzioni separabili in profondità per ridurre sia il calcolo che le dimensioni del modello.\nEfficientNet di Tan e Le (2023) adotta un approccio diverso ottimizzando il ridimensionamento della rete (ovvero variando la profondità, la larghezza e la risoluzione di una rete) e il ridimensionamento composto, una variazione più sfumata del ridimensionamento della rete, per ottenere prestazioni superiori con meno parametri.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, e Kurt Keutzer. 2016. «SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and 0.5 MB model size». ArXiv preprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nTan, Mingxing, e Quoc V. Le. 2023. «Demystifying Deep Learning». Wiley. https://doi.org/10.1002/9781394205639.ch6.\nQuesti modelli sono essenziali nel contesto dell’edge computing in cui la limitazione di potenza di elaborazione e di memoria richiede modelli leggeri ma efficaci in grado di eseguire in modo efficiente attività quali il riconoscimento delle immagini, il rilevamento di oggetti e altro ancora. I loro principi di progettazione mostrano l’importanza di un’architettura di modelli intenzionalmente personalizzata per l’edge computing, in cui prestazioni ed efficienza devono rientrare nei vincoli.\n\n\nSemplificazione della Ricerca di Architetture di Modelli\nInfine, per affrontare la sfida di trovare architetture di modelli efficienti che siano compatibili con i dispositivi edge, i ricercatori hanno sviluppato pipeline sistematizzate che semplificano la ricerca di progetti performanti. Due framework degni di nota in questo spazio sono TinyNAS di J. Lin et al. (2020) e MorphNet di Gordon et al. (2018), che automatizzano il processo di ottimizzazione delle architetture di reti neurali per l’implementazione edge.\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, e Edward Choi. 2018. «MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1586–95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\nTinyNAS è un innovativo framework di ricerca di architetture neurali introdotto nel documento MCUNet, progettato per scoprire in modo efficiente architetture di reti neurali leggere per dispositivi edge con risorse computazionali limitate. Sfruttando l’apprendimento per rinforzo e uno spazio di ricerca compatto di micromoduli neurali, TinyNAS ottimizza sia l’accuratezza che la latenza, consentendo l’implementazione di modelli di deep learning su microcontrollori, dispositivi IoT e altre piattaforme con risorse limitate. Nello specifico, TinyNAS, in combinazione con un ottimizzatore di rete, TinyEngine, genera diversi spazi di ricerca ridimensionando la risoluzione di input e la larghezza del modello, poi raccoglie la distribuzione FLOP di calcolo delle reti soddisfacenti all’interno dello spazio di ricerca per valutarne la priorità. TinyNAS si basa sul presupposto che uno spazio di ricerca che ospita FLOP più elevati con vincoli di memoria possa produrre modelli di accuratezza più elevata, cosa che gli autori hanno verificato in pratica nel loro lavoro. In termini di prestazioni empiriche, TinyEngine ha ridotto l’utilizzo di memoria di picco dei modelli di circa 3.4 volte e ha accelerato l’inferenza da 1.7 a 3.3 volte rispetto a TFLite e a CMSIS-NN.\nAnalogamente, MorphNet è un framework di ottimizzazione delle reti neurali progettato per rimodellare e trasformare automaticamente l’architettura delle reti neurali profonde, ottimizzandole per requisiti di distribuzione specifici. Ciò avviene in due fasi: in primo luogo, sfrutta un set di operazioni di morphing della rete personalizzabili, come l’ampliamento o l’approfondimento dei layer, per regolare dinamicamente la struttura della rete. Queste operazioni consentono alla rete di adattarsi a vari vincoli computazionali, tra cui dimensioni del modello, latenza e obiettivi di accuratezza, che sono estremamente diffusi nell’utilizzo dell’edge computing. Nella seconda fase, MorphNet utilizza un approccio basato sull’apprendimento di rinforzo per cercare la permutazione ottimale delle operazioni di morphing, bilanciando efficacemente il compromesso tra dimensioni del modello e prestazioni. Questo metodo innovativo consente ai professionisti del deep learning di adattare automaticamente le architetture delle reti neurali a requisiti hardware e applicativi specifici, garantendo un’implementazione efficiente ed efficace su diverse piattaforme.\nTinyNAS e MorphNet rappresentano alcuni dei numerosi progressi significativi nel campo dell’ottimizzazione sistematica delle reti neurali, consentendo di scegliere e generare sistematicamente architetture per adattarsi perfettamente ai vincoli del problema.\n\n\n\n\n\n\nEsercizio 9.3: Modelli Progettati per l’Edge\n\n\n\n\n\nSi Immagini di costruire un piccolo robot in grado di identificare diversi fiori. Deve essere intelligente, ma anche piccolo ed efficiente dal punto di vista energetico! Nel mondo dei “Modelli Progettati per l’Edge”, abbiamo appreso tecniche come le convoluzioni separabili in profondità e architetture come SqueezeNet, MobileNet ed EfficientNet, tutte progettate per concentrare l’intelligenza in modelli compatti. Ora, vediamo queste idee in azione con alcuni xColab:\nSqueezeNet in Action: Forse piacerebbe un Colab che mostra come addestrare un modello SqueezeNet su un set di dati di immagini di fiori. Ciò dimostrerebbe le sue piccole dimensioni e come impara a riconoscere i pattern nonostante la sua efficienza.\n\nMobileNet Exploration: Ci si è mai chiesto se quei piccoli modelli di immagini sono buoni quanto quelli grandi? Scopriamolo! In questo Colab, mettiamo a confronto MobileNet, il campione dei pesi leggeri, con un modello di classificazione delle immagini classico. Li faremo gareggiare per la velocità, misureremo le loro esigenze di memoria e vedremo chi vincerà per accuratezza. Preparatevi per una battaglia di cervelli di immagini!",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#sec-model_ops_numerics",
    "href": "contents/optimizations/optimizations.it.html#sec-model_ops_numerics",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.3 Rappresentazione Numerica Efficiente",
    "text": "9.3 Rappresentazione Numerica Efficiente\nLa rappresentazione numerica implica una miriade di considerazioni, tra cui, ma non solo, la precisione dei numeri, i loro formati di codifica e le operazioni aritmetiche facilitate. Implica invariabilmente una vasta gamma di diversi compromessi, in cui i professionisti sono incaricati di destreggiarsi tra accuratezza numerica ed efficienza computazionale. Ad esempio, mentre i numeri a bassa precisione possono offrire il fascino di un utilizzo di memoria ridotto e calcoli accelerati, presentano contemporaneamente sfide relative alla stabilità numerica e al potenziale degrado dell’accuratezza del modello.\n\nMotivazione\nEmerge l’imperativo per una rappresentazione numerica efficiente, in particolare perché l’ottimizzazione efficiente del modello da sola non è sufficiente quando si adattano i modelli per l’implementazione su dispositivi edge a bassa potenza che operano con vincoli rigorosi.\nOltre a ridurre al minimo le richieste di memoria, l’enorme potenziale di una rappresentazione numerica efficiente risiede, ma non è limitato a, queste modalità fondamentali. Riducendo l’intensità computazionale, la matematica efficiente può amplificare la velocità computazionale, consentendo di elaborare modelli più complessi su dispositivi a bassa potenza. Ridurre la precisione in bit di pesi e attivazioni su modelli fortemente sovra-parametrizzati consente la condensazione delle dimensioni del modello per dispositivi edge senza danneggiare significativamente l’accuratezza predittiva del modello. Con l’onnipresenza delle reti neurali nei modelli, la matematica efficiente ha un vantaggio unico nello sfruttare la struttura a layer delle NN per variare la precisione numerica tra i layer, riducendo al minimo la precisione nei layer resistenti e preservando una maggiore precisione in quelli sensibili.\nIn questa sezione, approfondiremo il modo in cui i professionisti possono sfruttare i principi della progettazione congiunta hardware-software ai livelli più bassi di un modello per facilitare la compatibilità con i dispositivi edge. Iniziando con un’introduzione ai numeri, esamineremo le sue implicazioni per la memoria del dispositivo e la complessità computazionale. Successivamente, intraprenderemo una discussione sui compromessi implicati nell’adozione di questa strategia, seguita da un’analisi approfondita di un metodo fondamentale della matematica efficiente: la quantizzazione.\n\n\n9.3.1 Le Basi\n\nI Tipi\nI dati numerici, il fondamento su cui si basano i modelli di apprendimento automatico, si manifestano in due forme principali. Si tratta di numeri interi e numeri in virgola mobile.\nNumeri Interi: Numeri interi, privi di componenti frazionarie, (ad esempio, -3, 0, 42) sono fondamentali negli scenari che richiedono valori discreti. Ad esempio, in ML, le etichette di classe in un’attività di classificazione potrebbero essere rappresentate come numeri interi, dove “gatto”, “cane” e “uccello” potrebbero essere codificati rispettivamente come 0, 1 e 2.\nNumeri in virgola mobile: Comprendendo numeri reali, (ad esempio, -3.14, 0.01, 2.71828) consentono la rappresentazione di valori con componenti frazionarie. Nei parametri del modello ML, i pesi potrebbero essere inizializzati con piccoli valori a virgola mobile, ad esempio 0.001 o -0.045, per avviare il processo di training. Attualmente, ci sono 4 popolari formati di precisione discussi di seguito.\nLarghezze di bit variabili: Oltre alle larghezze standard, sono in corso ricerche su numeri con larghezze di bit estremamente basse, persino fino a rappresentazioni binarie o ternarie. Le operazioni con larghezza di bit estremamente ridotta possono offrire accelerazioni significative e ridurre ulteriormente il consumo energetico. Sebbene permangano dei problemi nel mantenere l’accuratezza del modello con una quantizzazione così drastica, si continuano a fare progressi in quest’area.\n\n\nPrecisione\nLa precisione, che delinea l’esattezza con cui un numero è rappresentato, si biforca tipicamente in singola, doppia, mezza e negli ultimi anni sono emerse numerose altre precisioni per supportare meglio e in modo efficiente le attività di apprendimento automatico sull’hardware sottostante.\nDoppia precisione (Float64): Allocando 64 bit, la doppia precisione (ad esempio, 3.141592653589793) fornisce una precisione elevata, sebbene richieda più memoria e più risorse di calcolo. Nei calcoli scientifici, dove la precisione è fondamentale, variabili come π potrebbero essere rappresentate con Float64.\nSingola precisione (Float32): Con 32 bit a disposizione, la singola precisione (ad esempio, 3.1415927) raggiunge un equilibrio tra precisione numerica e risparmio della memoria. In ML, Float32 potrebbe essere impiegato per memorizzare i pesi durante l’addestramento per mantenere un livello ragionevole di precisione.\nHalf Precision (Float16): Limitata a 16 bit, la half precision (ad esempio, 3.14) riduce l’utilizzo della memoria e può velocizzare i calcoli, sebbene sacrifichi l’accuratezza e l’intervallo numerico. In ML, specialmente durante l’inferenza su dispositivi con risorse limitate, Float16 potrebbe essere utilizzato per ridurre l’impronta di memoria del modello.\nBfloat16: Brain Floating-Point Format o Bfloat16, impiega anche 16 bit ma li alloca in modo diverso rispetto a FP16: 1 bit per il segno, 8 bit per l’esponente (che si traduce nello stesso intervallo numerico di float32) e 7 bit per la frazione. Questo formato, sviluppato da Google, dà priorità a un intervallo di esponenti più ampio rispetto alla precisione, rendendolo particolarmente utile nelle applicazioni di apprendimento profondo in cui l’intervallo dinamico è cruciale.\nFigura 9.13 illustra le differenze tra i tre formati a virgola mobile: Float32, Float16 e BFloat16.\n\n\n\n\n\n\nFigura 9.13: Tre formati a virgola mobile.\n\n\n\nIntero: Le rappresentazioni di numeri interi sono realizzate utilizzando 8, 4 e 2 bit. Vengono spesso utilizzati durante la fase di inferenza delle reti neurali, in cui i pesi e le attivazioni del modello sono quantizzati a queste precisioni inferiori. Le rappresentazioni intere sono deterministiche e offrono notevoli vantaggi in termini di velocità e memoria rispetto alle rappresentazioni in virgola mobile. Per molte attività di inferenza, in particolare su dispositivi edge, la leggera perdita di accuratezza dovuta alla quantizzazione è spesso accettabile dati i guadagni di efficienza. Una forma estrema di numeri interi è per le reti neurali binarie (BNN), in cui pesi e attivazioni sono vincolati a uno di due valori: +1 o -1.\nÈ possibile fare riferimento a Sezione 8.6.1 per una tabella di confronto tra i compromessi dei diversi tipi numerici.\n\n\nCodifica e Archiviazione Numerica\nLa codifica numerica, l’arte di trasformare i numeri in un formato utilizzabile dal computer e la loro successiva memorizzazione sono fondamentali per l’efficienza computazionale. Ad esempio, i numeri in virgola mobile potrebbero essere codificati utilizzando lo standard IEEE 754, che ripartisce i bit tra i componenti segno, esponente e frazione, consentendo così la rappresentazione di una vasta gamma di valori con un singolo formato. Esistono alcuni nuovi formati in virgola mobile IEEE che sono stati definiti specificamente per i carichi di lavoro AI:\n\nbfloat16- Un formato in virgola mobile a 16 bit introdotto da Google. Ha 8 bit per esponente, 7 bit per mantissa e 1 bit per segno. Offre un compromesso di precisione ridotto tra float a 32 bit e interi a 8 bit. Supportato su molti acceleratori hardware.\nposit - Un formato configurabile che può rappresentare diversi livelli di precisione in base ai bit esponente. Mira a essere più efficiente dei float binari IEEE 754. Ha una gamma dinamica e una precisione regolabili.\nFlexpoint - Un formato introdotto da Intel che può regolare dinamicamente la precisione tra livelli o all’interno di un layer. Consente di adattare la precisione all’accuratezza e ai requisiti hardware.\nBF16ALT - Un formato a 16 bit proposto da ARM come alternativa a bfloat16. Utilizza un bit aggiuntivo nell’esponente per evitare overflow/underflow.\nTF32 - Introdotto da Nvidia per le GPU Ampere. Utilizza 10 bit per l’esponente invece di 8 bit come FP32. Migliora le prestazioni di training del modello mantenendo l’accuratezza.\nFP8 - Formato a virgola mobile a 8 bit che mantiene 6 bit per la mantissa e 2 bit per l’esponente. Consente una gamma dinamica migliore rispetto agli interi.\n\nGli obiettivi principali di questi nuovi formati sono di fornire alternative di precisione inferiore ai float a 32 bit per una migliore efficienza computazionale e prestazioni sugli acceleratori AI, mantenendo al contempo l’accuratezza del modello. Offrono diversi compromessi in termini di precisione, portata e costo/complessità di implementazione.\n\n\n\n9.3.2 Vantaggi dell’Efficienza\nCome visto in Sezione 8.6.2, l’efficienza numerica è importante per i carichi di lavoro di apprendimento automatico per una serie di motivi. L’efficienza numerica non riguarda solo la riduzione della larghezza di bit dei numeri, ma anche la comprensione dei compromessi tra accuratezza ed efficienza. Man mano che i modelli di apprendimento automatico diventano più pervasivi, soprattutto in ambienti reali con risorse limitate, l’attenzione su una numerica efficiente continuerà a crescere. Selezionando e sfruttando attentamente la precisione numerica appropriata, è possibile ottenere prestazioni di modello robuste ottimizzando al contempo velocità, memoria ed energia.\n\n\n9.3.3 Sfumature della Rappresentazione Numerica\nCi sono diverse sfumature con le rappresentazioni numeriche per ML che richiedono di avere una comprensione sia degli aspetti teorici che pratici della rappresentazione numerica, nonché una profonda consapevolezza dei requisiti e dei vincoli specifici del dominio applicativo.\n\nUtilizzo della Memoria\nL’impronta di memoria dei modelli ML, in particolare quelli di notevole complessità e profondità, può essere sostanziale, ponendo quindi una sfida significativa sia nelle fasi di training che di deployment. Ad esempio, una rete neurale profonda con 100 milioni di parametri, rappresentata utilizzando Float32 (32 bit o 4 byte per parametro), richiederebbe circa 400 MB di memoria solo per l’archiviazione dei pesi del modello. Ciò non tiene conto dei requisiti di memoria aggiuntivi durante il training per l’archiviazione di gradienti, stati dell’ottimizzatore e cache di passaggio forward [in avanti], che possono amplificare ulteriormente l’utilizzo della memoria, potenzialmente mettendo a dura prova le risorse su determinati hardware, in particolare dispositivi edge con capacità di memoria limitata.\nLa scelta della rappresentazione numerica ha un impatto ulteriore sull’utilizzo della memoria e sull’efficienza computazionale. Ad esempio, l’utilizzo di Float64 per i pesi del modello raddoppierebbe i requisiti di memoria rispetto a Float32 e potrebbe potenzialmente aumentare anche il tempo di elaborazione. Per una matrice di peso con dimensioni [1000, 1000], Float64 consumerebbe circa 8 MB di memoria, mentre Float32 la ridurrebbe a circa 4 MB. Pertanto, la selezione di un formato numerico appropriato è fondamentale per ottimizzare sia la memoria che l’efficienza computazionale.\n\n\nComplessità Computazionale\nLa precisione numerica ha un impatto diretto sulla complessità computazionale, influenzando il tempo e le risorse necessarie per eseguire operazioni aritmetiche. Ad esempio, le operazioni che utilizzano Float64 generalmente consumano più risorse computazionali rispetto alle loro controparti Float32 o Float16 (vedere Figura 9.14). Nel regno del ML, dove i modelli potrebbero dover elaborare milioni di operazioni (ad esempio, moltiplicazioni e addizioni in operazioni di matrice durante passaggi in forward e backward), anche piccole differenze nella complessità computazionale per operazione possono aggregarsi in un impatto sostanziale sui tempi di training e inferenza. Come mostrato in Figura 9.15, i modelli quantizzati possono essere molte volte più veloci delle loro versioni non-quantizzate.\n\n\n\n\n\n\nFigura 9.14: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Mark Horowitz, Stanford University.\n\n\n\n\n\n\n\n\n\nFigura 9.15: Velocità di tre diversi modelli in forma normale e quantizzata.\n\n\n\nOltre ai tempi di esecuzione puri, c’è anche una preoccupazione per l’efficienza energetica. Non tutti i calcoli numerici sono creati uguali dal punto di vista dell’hardware sottostante. Alcune operazioni numeriche sono più efficienti dal punto di vista energetico di altre. Ad esempio, Figura 9.16 di seguito mostra che l’addizione di interi è molto più efficiente dal punto di vista energetico della moltiplicazione di interi.\n\n\n\n\n\n\nFigura 9.16: Utilizzo di energia da parte di operazioni quantizzate. Fonte: Isscc (2014).\n\n\nIsscc. 2014. «Computing’s energy problem (and what we can do about it)». https://ieeexplore.ieee.org/document/6757323.\n\n\n\n\nCompatibilità Hardware\nGarantire la compatibilità e le prestazioni ottimizzate su diverse piattaforme hardware è un’altra sfida nella rappresentazione numerica. Hardware diversi, come CPU, GPU, TPU e FPGA, hanno capacità e ottimizzazioni diverse per gestire diverse precisioni numeriche. Ad esempio, alcune GPU potrebbero essere ottimizzate per i calcoli Float32, mentre altre potrebbero fornire accelerazioni per Float16. Sviluppare e ottimizzare modelli ML in grado di sfruttare le capacità numeriche specifiche di hardware diversi, garantendo al contempo che il modello mantenga la sua accuratezza e robustezza, richiede un’attenta considerazione e potenzialmente ulteriori sforzi di sviluppo e test.\n\n\nCompromessi di Precisione e Accuratezza\nIl compromesso tra precisione numerica e accuratezza del modello è una sfida “sfumata” nella rappresentazione numerica. L’utilizzo di numeri a bassa precisione, come Float16, potrebbe risparmiare memoria e velocizzare i calcoli, ma può anche introdurre problemi come errore di quantizzazione e intervallo numerico ridotto. Ad esempio, addestrare un modello con Float16 potrebbe introdurre problemi nella rappresentazione di valori di gradiente molto piccoli, potenzialmente influenzando la convergenza e la stabilità del processo di addestramento. Inoltre, in alcune applicazioni, come simulazioni scientifiche o calcoli finanziari, in cui l’elevata precisione è fondamentale, l’uso di numeri a bassa precisione potrebbe non essere consentito a causa del rischio di accumulare errori significativi.\n\n\nEsempi di Compromessi\nPer comprendere e apprezzare le sfumature, prendiamo in considerazione alcuni esempi di casi d’uso. Attraverso questi, ci renderemo conto che la scelta della rappresentazione numerica non è semplicemente una decisione tecnica, ma strategica, che influenza l’acume predittivo del modello, le sue esigenze computazionali e la sua implementabilità in diversi ambienti computazionali. In questa sezione esamineremo un paio di esempi per comprendere meglio i compromessi con i numeri e come si collegano al mondo reale.\n\nVeicoli Autonomi\nNel dominio dei veicoli autonomi, i modelli ML vengono impiegati per interpretare i dati dei sensori e prendere decisioni in tempo reale. I modelli devono elaborare dati ad alta dimensionalità da vari sensori (ad esempio, LiDAR, telecamere, radar) ed eseguire numerosi calcoli entro un intervallo di tempo limitato per garantire un funzionamento sicuro e reattivo del veicolo. Quindi i compromessi qui includerebbero:\n\nUtilizzo della Memoria: L’archiviazione e l’elaborazione di dati dei sensori ad alta risoluzione, specialmente in formati a virgola mobile, possono consumare una quantità di memoria sostanziale.\nComplessità Computazionale: L’elaborazione in tempo reale richiede calcoli efficienti, in cui numeri di precisione più elevata potrebbero impedire l’esecuzione tempestiva delle azioni di controllo.\n\n\n\nApplicazioni Sanitarie Mobili\nLe applicazioni sanitarie mobili spesso utilizzano modelli ML per attività come il riconoscimento delle attività, il monitoraggio della salute o l’analisi predittiva, operando nell’ambiente con risorse limitate dei dispositivi mobili. I compromessi in questo caso includerebbero:\n\nCompromessi di Precisione e Accuratezza: L’impiego di numeri a bassa precisione per conservare risorse potrebbe influire sull’accuratezza delle previsioni sanitarie o delle rilevazioni di anomalie, il che potrebbe avere implicazioni significative per la salute e la sicurezza degli utenti.\nCompatibilità Hardware: I modelli devono essere ottimizzati per diversi hardware mobili, garantendo un funzionamento efficiente su un’ampia gamma di dispositivi con diverse capacità di calcolo numerico.\n\n\n\nSistemi di Trading ad Alta Frequenza (HFT)\nI sistemi HFT sfruttano i modelli ML per prendere decisioni di trading rapide basate su dati di mercato in tempo reale. Questi sistemi richiedono risposte a bassissima latenza per capitalizzare le opportunità di trading di breve durata.\n\nComplessità Computazionale: I modelli devono elaborare e analizzare vasti flussi di dati di mercato con una latenza minima, dove anche lievi ritardi, potenzialmente introdotti da numeri a precisione più elevata, possono comportare opportunità perse.\nCompromessi di Precisione e Accuratezza: I calcoli finanziari spesso richiedono un’elevata precisione numerica per garantire valutazioni accurate dei prezzi e dei rischi, ponendo sfide nel bilanciamento tra efficienza computazionale e accuratezza numerica.\n\n\n\nSistemi di Sorveglianza Basati su Edge\nI sistemi di sorveglianza distribuiti su dispositivi edge, come le telecamere di sicurezza, utilizzano modelli ML per attività come rilevamento di oggetti, riconoscimento di attività e rilevamento di anomalie, spesso operando con vincoli di risorse rigorosi.\n\nUtilizzo della Memoria: L’archiviazione di modelli pre-addestrati e l’elaborazione di feed video in tempo reale richiedono un utilizzo efficiente della memoria, il che può essere impegnativo con numeri ad alta precisione.\nCompatibilità Hardware: Garantire che i modelli possano funzionare in modo efficiente su dispositivi edge con diverse capacità hardware e ottimizzazioni per diverse precisioni numeriche è fondamentale per una distribuzione diffusa.\n\n\n\nSimulazioni Scientifiche\nI modelli ML vengono sempre più utilizzati nelle simulazioni scientifiche, come la modellazione climatica o le simulazioni di dinamica molecolare, per migliorare le capacità predittive e ridurre le richieste di calcolo.\n\nCompromessi di Precisione e Accuratezza: Le simulazioni scientifiche spesso richiedono un’elevata precisione numerica per garantire risultati accurati e affidabili, il che può entrare in conflitto con il desiderio di ridurre le richieste di calcolo tramite numeri a bassa precisione.\nComplessità Computazionale: I modelli devono gestire ed elaborare dati di simulazione complessi e ad alta dimensionalità in modo efficiente per garantire risultati tempestivi e consentire simulazioni su larga scala o di lunga durata.\n\nQuesti esempi illustrano diversi scenari in cui le sfide della rappresentazione numerica nei modelli ML sono palesemente manifestate. Ogni sistema presenta un set unico di requisiti e vincoli, che richiedono strategie e soluzioni personalizzate per affrontare i problemi dell’utilizzo della memoria, della complessità computazionale, dei compromessi tra precisione e accuratezza e della compatibilità hardware.\n\n\n\n\n9.3.4 Quantizzazione\nLa quantizzazione è prevalente in vari domini scientifici e tecnologici e comporta essenzialmente la mappatura o la limitazione di un set o intervallo continuo in una controparte discreta per ridurre al minimo il numero di bit richiesti.\n\nAnalisi Iniziale\nIniziamo la nostra incursione nella quantizzazione con una breve analisi di un importante utilizzo della quantizzazione.\nNel signal processing [elaborazione del segnale], l’onda sinusoidale continua (mostrata in Figura 9.17) può essere quantizzata in valori discreti tramite un processo noto come campionamento. Questo è un concetto fondamentale nell’elaborazione del segnale digitale ed è cruciale per convertire segnali analogici (come l’onda sinusoidale continua) in una forma digitale che possa essere elaborata dai computer. L’onda sinusoidale è un esempio prevalente grazie alla sua natura periodica e regolare, il che la rende uno strumento utile per spiegare concetti come frequenza, ampiezza, fase e, naturalmente, quantizzazione.\n\n\n\n\n\n\nFigura 9.17: Onda Sinusoidale.\n\n\n\nNella versione quantizzata mostrata in Figura 9.18, l’onda sinusoidale continua (Figura 9.17) viene campionata a intervalli regolari (in questo caso, ogni \\(\\frac{\\pi}{4}\\) radianti) e solo questi valori campionati vengono rappresentati nella versione digitale del segnale. Le linee graduali tra i punti mostrano un modo per rappresentare il segnale quantizzato in una forma costante a tratti. Questo è un esempio semplificato di come funziona la conversione analogico-digitale, in cui un segnale continuo viene mappato su un set discreto di valori, consentendone la rappresentazione e l’elaborazione digitale.\n\n\n\n\n\n\nFigura 9.18: Onda Sinusoidale Quantizzata.\n\n\n\nTornando al contesto del Machine Learning (ML), la quantizzazione si riferisce al processo di limitazione dei possibili valori che i parametri numerici (come pesi e bias) possono assumere in un set discreto, riducendo così la precisione dei parametri e, di conseguenza, l’ingombro di memoria del modello. Se implementata correttamente, la quantizzazione può ridurre le dimensioni del modello fino a 4 volte e migliorare la latenza e la produttività dell’inferenza fino a 2-3 volte. Figura 9.19 illustra l’impatto che la quantizzazione ha sulle dimensioni di modelli diversi: ad esempio, un modello di classificazione delle immagini come ResNet-v2 può essere compresso da 180 MB a 45 MB con quantizzazione a 8 bit. In genere, la perdita di accuratezza del modello è inferiore all’1% con una quantizzazione ben fatta. L’accuratezza può spesso essere recuperata riaddestrando il modello quantizzato con tecniche di addestramento consapevoli della quantizzazione. Pertanto, questa tecnica è emersa come molto importante nell’implementazione di modelli ML in ambienti con risorse limitate, come dispositivi mobili, dispositivi IoT e piattaforme di edge computing, dove le risorse computazionali (memoria e potenza di elaborazione) sono limitate.\n\n\n\n\n\n\nFigura 9.19: Effetto della quantizzazione sulle dimensioni del modello. Fonte: HarvardX.\n\n\n\nEsistono diverse dimensioni della quantizzazione, come uniformità, stocasticità (o determinismo), simmetria, granularità (tra layer/canali/gruppi o persino all’interno dei canali), considerazioni sulla calibrazione dell’intervallo (statico o dinamico) e metodi di messa a punto (QAT, PTQ, ZSQ). Esaminiamo questi di seguito.\n\n\n\n9.3.5 I Tipi\n\nQuantizzazione Uniforme\nLa quantizzazione uniforme implica la mappatura di valori continui o ad alta precisione su una rappresentazione a precisione inferiore utilizzando una scala uniforme. Ciò significa che l’intervallo tra ogni possibile valore quantizzato è coerente. Ad esempio, se i pesi di un layer di rete neurale sono quantizzati su numeri interi a 8 bit (valori tra 0 e 255), un peso con un valore in virgola mobile di 0.56 potrebbe essere mappato su un valore intero di 143, presupponendo una mappatura lineare tra le scale originale e quantizzata. Grazie all’uso di pipeline matematiche intere o a virgola fissa, questa forma di quantizzazione consente il calcolo sul dominio quantizzato senza la necessità di dequantizzare in anticipo.\nIl processo per implementare la quantizzazione uniforme inizia con la scelta di un intervallo di numeri reali da quantizzare. Il passaggio successivo consiste nel selezionare una funzione di quantizzazione e mappare i valori reali sugli interi rappresentabili dalla larghezza di bit della rappresentazione quantizzata. Ad esempio, una scelta popolare per una funzione di quantizzazione è:\n\\[\nQ(r)=Int(r/S) - Z\n\\]\ndove \\(Q\\) è l’operatore di quantizzazione, \\(r\\) è un input a valore reale (nel nostro caso, un’attivazione o un peso), \\(S\\) è un fattore di scala a valore reale e \\(Z\\) è un punto zero intero. La funzione Int mappa un valore reale in un valore intero tramite un’operazione di arrotondamento. Tramite questa funzione, abbiamo mappato in modo efficace i valori reali \\(r\\) in alcuni valori interi, ottenendo livelli quantizzati uniformemente distanziati.\nQuando i professionisti hanno la necessità di recuperare i valori originali di precisione più elevata, i valori reali \\(r\\) possono essere recuperati dai valori quantizzati tramite un’operazione nota come dequantizzazione. Nell’esempio sopra, ciò significherebbe eseguire la seguente operazione sul nostro valore quantizzato:\n\\[\n\\bar{r} = S(Q(r) + Z)\n\\]\nCome discusso, una certa precisione nel valore reale viene persa dalla quantizzazione. In questo caso, il valore recuperato \\(\\bar{r}\\) non corrisponderà esattamente a \\(r\\) a causa dell’operazione di arrotondamento. Questo è un importante compromesso da notare; tuttavia, in molti utilizzi riusciti della quantizzazione, la perdita di precisione può essere trascurabile e l’accuratezza del test rimane elevata. Nonostante ciò, la quantizzazione uniforme continua a essere la scelta di fatto attuale per la sua semplicità e l’efficiente mappatura all’hardware.\n\n\nQuantizzazione Non-Uniforme\nLa quantizzazione non uniforme, d’altro canto, non mantiene un intervallo coerente tra i valori quantizzati. Questo approccio potrebbe essere utilizzato per allocare più possibili valori discreti in regioni in cui i valori dei parametri sono più densamente popolati, preservando così maggiori dettagli dove sono più necessari. Ad esempio, nelle distribuzioni a campana di pesi con lunghe code, un set di pesi in un modello si trova prevalentemente all’interno di un certo intervallo; quindi, più livelli di quantizzazione potrebbero essere assegnati a tale intervallo per preservare dettagli più fini, consentendoci di acquisire meglio le informazioni. Tuttavia, una delle principali debolezze della quantizzazione non uniforme è che richiede la dequantizzazione prima di calcoli di precisione più elevata a causa della sua non uniformità, limitando la sua capacità di accelerare il calcolo rispetto alla quantizzazione uniforme.\nIn genere, una quantizzazione non uniforme basata su regole utilizza una distribuzione logaritmica di passaggi e livelli esponenzialmente crescenti anziché linearmente. Un altra tipologia popolare risiede nella quantizzazione basata su codice binario in cui i vettori di numeri reali vengono quantizzati in vettori binari con un fattore di scala. In particolare, non esiste una soluzione in forma chiusa per minimizzare gli errori tra il valore reale e il valore non uniformemente quantizzato, quindi la maggior parte delle quantizzazioni in questo campo si basa su soluzioni euristiche. Ad esempio, un lavoro recente di Xu et al. (2018) formula la quantizzazione non uniforme come un problema di ottimizzazione in cui i passaggi/livelli di quantizzazione nel quantizzatore \\(Q\\) vengono regolati per ridurre al minimo la differenza tra il tensore originale e la controparte quantizzata.\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, e Hongbin Zha. 2018. «Alternating Multi-bit Quantization for Recurrent Neural Networks». In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\\[\n\\min_Q ||Q(r)-r||^2\n\\]\nInoltre, i quantizzatori addestrabili lo possono essere congiuntamente con parametri di modello e i passaggi/livelli di quantizzazione sono generalmente addestrati con ottimizzazione iterativa o discesa del gradiente. Inoltre, il clustering è stato utilizzato per alleviare la perdita di informazioni dalla quantizzazione. Sebbene in grado di catturare livelli di dettaglio più elevati, gli schemi di quantizzazione non uniformi possono essere difficili da implementare in modo efficiente su hardware di calcolo generale, rendendoli meno preferiti ai metodi che utilizzano la quantizzazione uniforme.\n\n\n\n\n\n\nFigura 9.20: Uniformità della Quantizzazione. Fonte: Gholami et al. (2021).\n\n\n\n\n\nQuantizzazione Stocastica\nA differenza dei due approcci precedenti che generano mappature deterministiche, c’è un po’ di lavoro che esplora l’idea della quantizzazione stocastica per l’addestramento consapevole della quantizzazione e l’addestramento a precisione ridotta. Questo approccio mappa numeri fluttuanti verso l’alto o verso il basso con una probabilità associata alla grandezza dell’aggiornamento del peso. La speranza generata dall’intuizione di alto livello è che un tale approccio probabilistico possa consentire a una rete neurale di esplorare di più, rispetto alla quantizzazione deterministica. Presumibilmente, abilitare un arrotondamento stocastico potrebbe consentire alle reti neurali di sfuggire agli ottimi locali, aggiornando così i propri parametri. Di seguito sono riportati due esempi di funzioni di mappatura stocastica:\n\n\n\n\n\n\n\nFigura 9.21: Funzioni di quantizzazione Intera e Binaria.\n\n\n\n\n\nQuantizzazione “Zero Shot”\nLa quantizzazione Zero-shot si riferisce al processo di conversione di un modello di deep learning a precisione completa direttamente in un modello quantizzato a bassa precisione senza la necessità di alcun riaddestramento o messa a punto sul modello quantizzato. Il vantaggio principale di questo approccio è la sua efficienza, in quanto elimina il processo, spesso dispendioso in termini di tempo e risorse, del riaddestramento post-quantizzazione. Sfruttando tecniche che anticipano e riducono al minimo gli errori di quantizzazione, la Zero-shot mira a mantenere l’accuratezza originale del modello anche dopo aver ridotto la sua precisione numerica. È particolarmente utile per i provider di “Machine Learning as a Service (MLaaS)” che mirano ad accelerare la distribuzione dei carichi di lavoro dei propri clienti senza dover accedere ai loro set di dati.\n\n\n\n9.3.6 Calibrazione\nLa calibrazione è il processo di selezione dell’intervallo di clipping [ritaglio] più efficace [\\(\\alpha\\), \\(\\beta\\)] per pesi e attivazioni da quantizzare. Ad esempio, si consideri la quantizzazione delle attivazioni che originariamente hanno un intervallo in virgola mobile tra -6 e 6 a interi a 8 bit. Prendere solo i valori minimi e massimi possibili di interi a 8 bit (da -128 a 127) come intervallo di quantizzazione, potrebbe non essere il più efficace. Invece, la calibrazione implicherebbe il passaggio di un set di dati rappresentativo e quindi l’utilizzo di questo intervallo osservato per la quantizzazione.\nEsistono molti metodi di calibrazione, ma alcuni comunemente utilizzati includono:\n\nMax: Utilizza il valore assoluto massimo visualizzato durante la calibrazione. Tuttavia, questo metodo è suscettibile di dati anomali. Notare come in Figura 9.22, abbiamo un cluster anomalo intorno a 2.1, mentre il resto è raggruppato attorno a valori più piccoli.\nEntropia: Utilizza la divergenza KL per ridurre al minimo la perdita di informazioni tra i valori originali in virgola mobile e i valori che potrebbero essere rappresentati dal formato quantizzato. Questo è il metodo predefinito utilizzato da TensorRT.\nPercentile: Imposta l’intervallo su un percentile della distribuzione dei valori assoluti osservati durante la calibrazione. Ad esempio, una calibrazione del 99% taglierebbe l’1% dei valori di magnitudine più grandi.\n\n\n\n\n\n\n\nFigura 9.22: Attivazioni di input nel layer 3 in ResNet50. Fonte: @Wu, Judd, e Isaev (2020).\n\n\n\nÈ importante notare che la qualità della calibrazione può fare la differenza tra un modello quantizzato che conserva la maggior parte della sua accuratezza e uno che si degrada in modo significativo. Quindi, è un passaggio essenziale nel processo di quantizzazione. Quando si sceglie un intervallo di calibrazione, ci sono due tipi: simmetrico e asimmetrico.\n\nQuantizzazione Simmetrica\nLa quantizzazione simmetrica mappa i valori reali su un intervallo di clipping simmetrico centrato su 0. Ciò comporta la scelta di un intervallo [\\(\\alpha\\), \\(\\beta\\)] dove \\(\\alpha = -\\beta\\). Ad esempio, un intervallo simmetrico si baserebbe sui valori min/max dei valori reali in modo tale che:\n\\[\n\\alpha = \\beta = max(abs(r_{max}), abs(r_{min}))\n\\]\nGli intervalli di clipping simmetrici sono i più ampiamente adottati nella pratica in quanto hanno il vantaggio di un’implementazione più semplice. In particolare, la mappatura da zero a zero nell’intervallo di clipping (talvolta chiamata “azzeramento del punto zero”) può portare a una riduzione del costo computazionale durante l’inferenza (Wu, Judd, e Isaev 2020).\n\n\nQuantizzazione Asimmetrica\nLa quantizzazione asimmetrica mappa i valori reali in un intervallo di clipping asimmetrico che non è necessariamente centrato sullo 0, come mostrato in Figura 9.23 a destra. Comporta la scelta di un intervallo [\\(\\alpha\\), \\(\\beta\\)] dove \\(\\alpha \\neq -\\beta\\). Ad esempio, selezionando un intervallo basato sui valori reali minimi e massimi, o dove \\(\\alpha = r_{min}\\) and \\(\\beta = r_{max}\\), si crea un intervallo asimmetrico. In genere, la quantizzazione asimmetrica produce intervalli di clipping più stretti rispetto a quella simmetrica, il che è importante quando i pesi e le attivazioni target sono sbilanciati, ad esempio, l’attivazione dopo la ReLU ha sempre valori non negativi. Nonostante produca intervalli di clipping più stretti, la quantizzazione asimmetrica è meno preferita di quella simmetrica in quanto non azzera sempre il valore dello zero reale.\n\n\n\n\n\n\nFigura 9.23: (a)simmetria della Quantizzazione. Fonte: Gholami et al. (2021).\n\n\n\n\n\nGranularità\nDopo aver deciso il tipo di intervallo di clipping, è essenziale restringerlo per consentire a un modello di mantenere la massima accuratezza possibile. Daremo un’occhiata alle reti neurali convoluzionali come nostro modo di esplorare metodi che ottimizzano la granularità degli intervalli di clipping per la quantizzazione. L’attivazione di input di un layer nella nostra CNN subisce una convoluzione con più filtri convoluzionali. Ogni filtro convoluzionale può possedere un intervallo di valori univoco. Si noti come in Figura 9.24 l’intervallo per il Filtro 1 sia molto più piccolo di quello per il Filtro 3. Di conseguenza, una caratteristica distintiva degli approcci di quantizzazione è la precisione con cui l’intervallo di clipping [α,β] viene determinato per i pesi.\n\n\n\n\n\n\nFigura 9.24: Granularità di quantizzazione: intervalli variabili. Fonte: Gholami et al. (2021).\n\n\n\n\nQuantizzazione a Layer: Questo approccio determina l’intervallo di clipping considerando tutti i pesi nei filtri convoluzionali di un layer. Quindi, lo stesso intervallo di clipping viene utilizzato per tutti i filtri convoluzionali. È il più semplice da implementare e, come tale, spesso si traduce in una precisione non ottimale a causa dell’ampia varietà di intervalli diversi tra i filtri. Ad esempio, un kernel convoluzionale con un intervallo di parametri più ristretto perde la sua risoluzione di quantizzazione a causa di un altro kernel nello stesso layer che ha un intervallo più ampio.\nGroupwise Quantization: Questo approccio raggruppa diversi canali all’interno di un layer per calcolare l’intervallo di clipping. Questo metodo può essere utile quando la distribuzione dei parametri su una singola convoluzione/attivazione varia molto. In pratica, questo metodo è stato utile in Q-BERT (Shen et al. 2020) per quantizzare i modelli Transformer (Vaswani et al. 2017) costituiti da layer di attenzione completamente connessi. Lo svantaggio di questo approccio è il costo aggiuntivo di contabilizzazione di diversi fattori di scala.\nChannelwise Quantization: Questo metodo popolare utilizza un intervallo fisso per ogni filtro convoluzionale che è indipendente dagli altri canali. Poiché a ogni canale viene assegnato un fattore di scala dedicato, questo metodo garantisce una risoluzione di quantizzazione più elevata e spesso si traduce in una maggiore accuratezza.\nSub-channelwise Quantization: Portando la quantizzazione canale per canale all’estremo, questo metodo determina l’intervallo di clipping rispetto a qualsiasi gruppo di parametri in una convoluzione o in un layer completamente connesso. Potrebbe comportare un overhead considerevole poiché è necessario tenere conto di diversi fattori di scala quando si elabora una singola convoluzione o un layer completamente connesso.\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, e Kurt Keutzer. 2020. «Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT». Proceedings of the AAAI Conference on Artificial Intelligence 34 (05): 8815–21. https://doi.org/10.1609/aaai.v34i05.6409.\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, e Illia Polosukhin. 2017. «Attention is all you need». Adv Neural Inf Process Syst 30.\nTra questi, la quantizzazione canale per canale è lo standard corrente utilizzato per quantizzare i kernel convoluzionali, poiché consente la regolazione degli intervalli di clipping per ogni singolo kernel con overhead trascurabile.\n\n\nQuantizzazione Statica e Dinamica\nDopo aver determinato il tipo e la granularità dell’intervallo di clipping, gli esperti devono decidere quando gli intervalli vengono determinati nei loro algoritmi di calibrazione dell’intervallo. Esistono due approcci per quantizzare le attivazioni: quantizzazione statica e quella dinamica.\nLa quantizzazione statica è l’approccio più frequentemente utilizzato. In questo, l’intervallo di clipping è precalcolato e statico durante l’inferenza. Non aggiunge alcun sovraccarico computazionale, ma, di conseguenza, comporta una minore accuratezza rispetto alla quantizzazione dinamica. Un metodo popolare per implementarlo è eseguire una serie di input di calibrazione per calcolare l’intervallo tipico di attivazioni (Jacob et al. 2018; Yao et al. 2021).\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. «Quantization and training of neural networks for efficient integer-arithmetic-only inference». In Proceedings of the IEEE conference on computer vision and pattern recognition, 2704–13.\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, et al. 2021. «Hawq-v3: Dyadic neural network quantization». In International Conference on Machine Learning, 11875–86. PMLR.\nLa quantizzazione dinamica è un approccio alternativo che calcola dinamicamente l’intervallo per ogni mappa di attivazione durante il runtime. L’approccio richiede calcoli in tempo reale che potrebbero avere un sovraccarico molto elevato. In questo modo, la quantizzazione dinamica spesso raggiunge la massima accuratezza poiché l’intervallo viene calcolato specificamente per ogni input.\nTra i due, il calcolo dell’intervallo in modo dinamico è solitamente molto costoso, quindi la maggior parte dei professionisti utilizzerà spesso la quantizzazione statica.\n\n\n\n9.3.7 Tecniche\nLe due tecniche prevalenti per la quantizzazione dei modelli sono la “Post Training Quantization” e la “Quantization-Aware Training”.\nPost Training Quantization: La quantizzazione post-addestramento (PTQ) è una tecnica di quantizzazione in cui il modello viene quantizzato dopo essere stato addestrato. Il modello viene addestrato in virgola mobile e poi i pesi e le attivazioni vengono quantizzati come fase di post-elaborazione. Questo è l’approccio più semplice e non richiede l’accesso ai dati di addestramento. Diversamente la “Quantization-Aware Training (QAT), PTQ” imposta direttamente i parametri di quantizzazione del peso e dell’attivazione, rendendolo poco costoso e adatto a situazioni con dati limitati o non etichettati. Tuttavia, non riaggiustare i pesi dopo la quantizzazione, specialmente nella quantizzazione a bassa precisione, può portare a un comportamento molto diverso e quindi a una minore accuratezza. Per affrontare questo problema, sono state sviluppate tecniche come la correzione della distorsione, l’equalizzazione degli intervalli di peso e i metodi di arrotondamento adattivo. PTQ può essere applicato anche in scenari zero-shot, in cui non sono disponibili dati di addestramento o di test. Questo metodo è stato reso ancora più efficiente per avvantaggiare modelli linguistici di grandi dimensioni che richiedono molta elaborazione e memoria. Di recente, è stata sviluppata SmoothQuant, una soluzione PTQ senza training, che preserva l’accuratezza ed è di uso generale che consente la quantizzazione di peso a 8 bit e attivazione a 8 bit per LLM, dimostrando un’accelerazione fino a 1.56x e una riduzione della memoria di 2x per LLM con una perdita trascurabile di accuratezza (Xiao et al. 2022).\nIn PTQ, un modello pre-addestrato subisce un processo di calibrazione, come mostrato in Figura 9.25. La calibrazione comporta l’utilizzo di un set di dati separato noto come dati di calibrazione, un sottoinsieme specifico dei dati di training riservato alla quantizzazione per aiutare a trovare gli intervalli di clipping e i fattori di scala appropriati.\n\n\n\n\n\n\nFigura 9.25: Quantizzazione e Calibrazione Post-Training. Fonte: Gholami et al. (2021).\n\n\n\nQuantization-Aware Training: L’addestramento consapevole della quantizzazione (QAT) è una messa a punto del modello PTQ. Il modello viene addestrato in modo consapevole della quantizzazione, consentendogli di adattarsi agli effetti della quantizzazione. Ciò produce una migliore accuratezza con l’inferenza quantizzata. La quantizzazione di un modello di rete neurale addestrato con metodi come PTQ introduce perturbazioni che possono deviare il modello dal suo punto di convergenza originale. Ad esempio, Krishnamoorthi ha dimostrato che anche con la quantizzazione per canale, reti come MobileNet non raggiungono la precisione di base con int8 “Post Training Quantization (PTQ)” e richiedono “Quantization-Aware Training (QAT)” (Krishnamoorthi 2018). Per risolvere questo problema, QAT riaddestra il modello con parametri quantizzati, impiegando passaggi forward e backward in virgola mobile ma quantizzando i parametri dopo ogni aggiornamento del gradiente. La gestione dell’operatore di quantizzazione non differenziabile è fondamentale; un metodo ampiamente utilizzato è lo “Straight Through Estimator (STE)”, che approssima l’operazione di arrotondamento come una funzione identità. Sebbene esistano altri metodi e varianti, STE rimane il più comunemente utilizzato per la sua efficacia pratica. In QAT, un modello pre-addestrato viene quantizzato e poi messo a punto utilizzando i dati di addestramento per regolare i parametri e recuperare il degrado della precisione, come mostrato in Figura 9.26. Il processo di calibrazione viene spesso condotto parallelamente al processo di messa a punto per QAT.\n\n\n\n\n\n\nFigura 9.26: Quantization-Aware Training. Fonte: Gholami et al. (2021).\n\n\nGholami, Dong Kim, Mahoney Yao, e Keutzer. 2021. «A Survey of Quantization Methods for Efficient Neural Network Inference)». ArXiv preprint. https://arxiv.org/abs/2103.13630.\n\n\nLa “Quantization-Aware Training” funge da estensione naturale della “Post-Training Quantization”. Dopo la quantizzazione iniziale eseguita da PTQ, QAT viene utilizzata per perfezionare e mettere a punto ulteriormente i parametri quantizzati: vedere come in Figura 9.27, il modello PTQ subisce un ulteriore passaggio, QAT. Comporta un processo di riqualificazione in cui il modello viene esposto a ulteriori iterazioni di training utilizzando i dati originali. Questo approccio di training dinamico consente al modello di adattare e regolare i suoi parametri, compensando il degrado delle prestazioni causato dalla quantizzazione.\n\n\n\n\n\n\nFigura 9.27: PTQ e QAT. Fonte: «The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training» (s.d.).\n\n\n«The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training». s.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nFigura 9.28 mostra l’accuratezza relativa di diversi modelli dopo PTQ e QAT. In quasi tutti i casi, QAT produce un’accuratezza migliore di PTQ. Si consideri ad esempio EfficientNet b0. Dopo PTQ, l’accuratezza scende dal 76.85% a 72.06%. Ma quando applichiamo QAT, l’accuratezza rimbalza al 76.95% (con persino un leggero miglioramento rispetto all’accuratezza originale).\n\n\n\n\n\n\nFigura 9.28: Accuratezza relativa di PTQ e QAT. Fonte: Wu, Judd, e Isaev (2020).\n\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nPost Training Quantization\nQuantization-Aware Training\nDynamic Quantization\n\n\n\n\nPro\n\n\n\n\n\nSemplicità\n✓\n✗\n✗\n\n\nPreservazione della precisione\n✗\n✓\n✓\n\n\nAdattabilità\n✗\n✗\n✓\n\n\nPrestazioni Ottimizzate\n✗\n✓\nPotenzialmente\n\n\nContro\n\n\n\n\n\nDegrado della Precisione\n✓\n✗\nPotenzialmente\n\n\nSovraccarico Computazionale\n✗\n✓\n✓\n\n\nComplessità di Implementazione\n✗\n✓\n✓\n\n\nCompromessi\n\n\n\n\n\nVelocità vs. Precisione\n✓\n✗\n✗\n\n\nPrecisione vs. Costo\n✗\n✓\n✗\n\n\nAdattabilità vs. Overhead\n✗\n✗\n✓\n\n\n\n\n\n9.3.8 Pesi vs. Attivazioni\nQuantizzazione del peso: Comporta la conversione dei pesi continui o ad alta precisione di un modello in pesi a bassa precisione, come la conversione dei pesi Float32 in pesi INT8 (interi) quantizzati - in Figura 9.29, la quantizzazione del peso avviene nel secondo passaggio (quadrati rossi) quando moltiplichiamo gli input. Ciò riduce le dimensioni del modello, riducendo così la memoria richiesta per archiviare il modello e le risorse computazionali necessarie per eseguire l’inferenza. Ad esempio, si consideri una matrice di pesi in un layer di rete neurale con pesi Float32 come [0.215, -1.432, 0.902, …]. Attraverso la quantizzazione del peso, questi potrebbero essere mappati su valori INT8 come [27, -183, 115, …], riducendo significativamente la memoria richiesta per memorizzarli.\n\n\n\n\n\n\nFigura 9.29: Quantizzazione del peso e dell’attivazione. Fonte: HarvardX.\n\n\n\nQuantizzazione dell’Attivazione: Comporta la quantizzazione dei valori di attivazione (output dei livelli) durante l’inferenza del modello. Ciò può ridurre le risorse computazionali richieste durante l’inferenza, ma introduce ulteriori problemi nel mantenimento dell’accuratezza del modello a causa della ridotta precisione dei calcoli intermedi. Ad esempio, in una rete neurale convoluzionale (CNN), le mappe di attivazione (mappe delle feature) prodotte dai layer convoluzionali, originariamente in Float32, potrebbero essere quantizzate su INT8 durante l’inferenza per accelerare il calcolo, in particolare su hardware ottimizzato per l’aritmetica degli interi. Inoltre, un lavoro recente ha esplorato l’uso della quantizzazione del “Activation-aware Weight Quantization” per la compressione e l’accelerazione LLM, che comporta la protezione di solo l’1% dei pesi salienti più importanti osservando le attivazioni, non i pesi (Lin et al. 2023).\n\n\n9.3.9 Compromessi\nLa quantizzazione introduce invariabilmente un compromesso tra dimensioni/prestazioni del modello e accuratezza. Sebbene riduca significativamente l’ingombro della memoria e possa accelerare l’inferenza, specialmente su hardware ottimizzato per aritmetica a bassa precisione, la precisione ridotta può degradare l’accuratezza del modello.\nDimensioni del Modello: Un modello con pesi rappresentati come Float32 quantizzato a INT8 può teoricamente ridurre le dimensioni del modello di un fattore 4, consentendone l’implementazione su dispositivi con memoria limitata. Le dimensioni di grandi modelli linguistici si stanno sviluppando a un ritmo più veloce della memoria GPU negli ultimi anni, portando a un grande divario tra domanda e offerta di memoria. Figura 9.30 illustra la recente tendenza del divario crescente tra le dimensioni del modello (linea rossa) e la memoria dell’acceleratore (linea gialla). Le tecniche di quantizzazione e compressione del modello possono aiutare a colmare il divario\n\n\n\n\n\n\nFigura 9.30: Dimensioni del modello vs. memoria dell’acceleratore. Fonte: Xiao et al. (2022).\n\n\nXiao, Seznec Lin, Demouth Wu, e Han. 2022. «SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models». ArXiv preprint. https://arxiv.org/abs/2211.10438.\n\n\nVelocità di Inferenza: La quantizzazione può anche accelerare l’inferenza, poiché l’aritmetica a precisione inferiore è computazionalmente meno costosa. Ad esempio, alcuni acceleratori hardware, come Edge TPU di Google, sono ottimizzati per l’aritmetica INT8 e possono eseguire l’inferenza in modo significativamente più rapido con modelli quantizzati INT8 rispetto alle loro controparti in virgola mobile. La riduzione della memoria dalla quantizzazione aiuta a ridurre la quantità di trasmissione dei dati, risparmiando memoria e velocizzando il processo. Figura 9.31 confronta l’aumento della produttività e la riduzione della memoria della larghezza di banda per diversi tipi di dati sulla NVIDIA Turing GPU.\n\n\n\n\n\n\nFigura 9.31: Vantaggi dei tipi di dati a precisione inferiore. Fonte: Wu, Judd, e Isaev (2020).\n\n\nWu, Zhang Judd, e Micikevicius Isaev. 2020. «Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation)». ArXiv preprint. https://arxiv.org/abs/2004.09602.\n\n\nPrecisione: La riduzione della precisione numerica post-quantizzazione può portare a un degrado della precisione del modello, che potrebbe essere accettabile in alcune applicazioni (ad esempio, classificazione delle immagini) ma non in altre (ad esempio, diagnosi medica). Pertanto, dopo la quantizzazione, il modello richiede in genere una ricalibrazione o una messa a punto per mitigare la perdita di accuratezza. Inoltre, un lavoro recente ha esplorato l’uso di Activation-aware Weight Quantization (Lin et al. 2023) che si basa sull’osservazione che proteggere solo l’1% dei pesi salienti può ridurre notevolmente l’errore di quantizzazione.\n\n\n9.3.10 Quantizzazione e Potatura\nPruning [potatura] e quantizzazione funzionano bene insieme ed è stato scoperto che il pruning non ostacola la quantizzazione. In effetti, il pruning può aiutare a ridurre l’errore di quantizzazione. Intuitivamente, ciò è dovuto al pruning che riduce il numero di pesi da quantizzare, riducendo così l’errore accumulato dalla quantizzazione. Ad esempio, una AlexNet non potata ha 60 milioni di pesi da quantizzare mentre una AlexNet potata ha solo 6.7 milioni di pesi da quantizzare. Questa significativa riduzione dei pesi aiuta a ridurre l’errore tra la quantizzazione dell’AlexNet non potato rispetto all’AlexNet potato. Inoltre, studi recenti hanno scoperto che il pruning consapevole della quantizzazione genera modelli più efficienti dal punto di vista computazionale rispetto al pruning o alla quantizzazione da soli; in genere, ha prestazioni simili o migliori in termini di efficienza computazionale rispetto ad altre tecniche di ricerca dell’architettura neurale come l’ottimizzazione bayesiana (Hawks et al. 2021).\n\n\n\n\n\n\nFigura 9.32: Precisione rispetto al tasso di compressione con diversi metodi di compressione. Fonte: Han, Mao, e Dally (2015).\n\n\nHan, Song, Huizi Mao, e William J Dally. 2015. «Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding». arXiv preprint arXiv:1510.00149.\n\n\n\n\n9.3.11 Quantizzazione Edge-aware\nLa quantizzazione non solo riduce le dimensioni del modello, ma consente anche calcoli più rapidi e consuma meno energia, rendendola fondamentale per lo sviluppo per edge. I dispositivi edge in genere hanno vincoli di risorse rigidi con elaborazione, memoria e potenza, impossibili da soddisfare per molti dei modelli deep NN profondi odierni. Inoltre, i processori edge non supportano le operazioni in virgola mobile, rendendo la quantizzazione intera particolarmente importante per chip come GAP-8, un SoC RISC-V per l’inferenza edge con un acceleratore CNN dedicato, che supporta solo l’aritmetica intera.\nUna piattaforma hardware che utilizza la quantizzazione è il gruppo ARM Cortex-M di core di processori ARM RISC a 32 bit. Sfruttano la quantizzazione a virgola fissa con fattori di scala di potenza di due, in modo che la quantizzazione e la de-quantizzazione possano essere eseguite in modo efficiente tramite spostamento di bit. Inoltre, Google Edge TPU, la soluzione emergente di Google per l’esecuzione di inferenze in periferia, è progettata per dispositivi piccoli e a bassa potenza e può supportare solo l’aritmetica a 8 bit. Molti modelli di reti neurali complesse che potevano essere distribuiti solo su server a causa delle loro elevate esigenze di elaborazione possono ora essere eseguiti su dispositivi edge grazie ai recenti progressi (ad esempio metodi di quantizzazione) nel campo dell’edge computing.\nOltre a essere una tecnica indispensabile per molti processori edge, la quantizzazione ha anche apportato notevoli miglioramenti ai processori non edge, incoraggiando tali processori a soddisfare i requisiti del Service Level Agreement (SLA) come la latenza del 99° percentile.\nPertanto, la quantizzazione combinata con una logica efficiente a bassa precisione e acceleratori dedicati di deep learning, è stata una forza trainante cruciale per l’evoluzione di tali processori edge.\nVideo 9.1 è una lezione sulla quantizzazione e sui diversi metodi di quantizzazione.\n\n\n\n\n\n\nVideo 9.1: Quantizzazione",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#sec-model_ops_hw",
    "href": "contents/optimizations/optimizations.it.html#sec-model_ops_hw",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.4 Implementazione Hardware Efficiente",
    "text": "9.4 Implementazione Hardware Efficiente\nL’implementazione hardware efficiente trascende la selezione di componenti adatti; richiede una comprensione olistica di come il software interagirà con le architetture sottostanti. L’essenza del raggiungimento delle massime prestazioni nelle applicazioni TinyML non risiede solo nell’affinare gli algoritmi per l’hardware, ma anche nell’assicurare che l’hardware sia strategicamente adattato per supportare questi algoritmi. Questa sinergia tra hardware e software è fondamentale. Mentre esaminiamo più a fondo le complessità dell’implementazione hardware efficiente, il significato di un approccio di progettazione congiunta, in cui hardware e software vengono sviluppati in tandem, diventa sempre più evidente. Questa sezione fornisce una panoramica delle tecniche di come l’hardware e le interazioni tra hardware e software possono essere ottimizzati per migliorare le prestazioni dei modelli.\n\n9.4.1 Ricerca di Architettura Neurale Basata sull’Hardware\nConcentrarsi solo sulla precisione durante l’esecuzione della ricerca di architettura neurale porta a modelli esponenzialmente complessi e che richiedono memoria e capacità di elaborazione crescenti. Ciò ha portato a vincoli hardware che limitano lo sfruttamento dei modelli di apprendimento profondo al loro pieno potenziale. Progettare manualmente l’architettura del modello è ancora più difficile se si considerano la varietà e le limitazioni dell’hardware. Ciò ha portato alla creazione di Hardware-aware Neural Architecture Search che incorpora le contrazioni hardware nella loro ricerca e ottimizza lo spazio di ricerca per un hardware e una precisione specifici. HW-NAS può essere categorizzato in base a come ottimizza per l’hardware. Esploreremo brevemente queste categorie e lasceremo dei link a documenti correlati per il lettore interessato.\n\nConfigurazione Single Target, Fixed Platfrom\nL’obiettivo qui è trovare la migliore architettura in termini di precisione ed efficienza hardware per un hardware target fisso. Per un hardware specifico, ad esempio Arduino Nicla Vision, questa categoria di HW-NAS cercherà l’architettura che ottimizza precisione, latenza, consumo energetico, ecc.\n\nStrategia di Ricerca Hardware-aware\nQui, la ricerca è un problema di ottimizzazione multi-obiettivo, in cui sia l’accuratezza che il costo dell’hardware guidano l’algoritmo di ricerca per trovare l’architettura più efficiente (Tan et al. 2019; Cai, Zhu, e Han 2019; B. Wu et al. 2019).\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, e Quoc V. Le. 2019. «MnasNet: Platform-aware Neural Architecture Search for Mobile». In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2820–28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\nCai, Han, Ligeng Zhu, e Song Han. 2019. «ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware». In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, e Yangqing Jia. 2019. «FBNet: Hardware-aware Efficient ConvNet Design via Differentiable Neural Architecture Search». In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10734–42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nSpazio di Ricerca Hardware-aware\nQui, lo spazio di ricerca è limitato alle architetture che funzionano bene sull’hardware specifico. Questo può essere ottenuto misurando le prestazioni degli operatori (operatore Conv, operatore Pool, …) o definendo un set di regole che limitano lo spazio di ricerca. (L. L. Zhang et al. 2020)\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, e Yunxin Liu. 2020. «Fast Hardware-Aware Neural Architecture Search». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\n\nConfigurazioni Single Target, Multiple Platform\nAlcuni hardware possono avere configurazioni diverse. Ad esempio, gli FPGA hanno blocchi logici configurabili (CLB) che possono essere configurati dal firmware. Questo metodo consente all’HW-NAS di esplorare diverse configurazioni. (Hu et al. 2023; Ho Yoon et al. 2012)\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song, Jun Yeong Seok, Kyung Jean Yoon, et al. 2012. «Frontiers in Electronic Materials». Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nTarget Multipli\nQuesta categoria mira a ottimizzare un singolo modello per più hardware. Questo può essere utile per lo sviluppo di dispositivi mobili in quanto può ottimizzare diversi modelli di telefoni. (Chu et al. 2021; Hu et al. 2023)\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. «Discovering Multi-Hardware Mobile Models via Architecture Search». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, e Jian Shi. 2023. «Halide Perovskite Semiconductors». Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nEsempi di “Hardware-Aware Neural Architecture Search”\n\nTinyNAS\nTinyNAS adotta un approccio in due fasi per trovare un’architettura ottimale per il modello tenendo a mente i vincoli del microcontrollore specifico.\nInnanzitutto, TinyNAS genera più spazi di ricerca variando la risoluzione di input del modello e il numero di canali dei layer. Quindi, TinyNAS sceglie uno spazio di ricerca in base ai FLOP (operazioni in virgola mobile al secondo) di ogni spazio di ricerca. Gli spazi con una probabilità maggiore di contenere architetture con un numero elevato di FLOP producono modelli con maggiore accuratezza: confrontare la linea rossa con la linea nera in Figura 9.33. Poiché un numero maggiore di FLOP significa che il modello ha una maggiore capacità di calcolo, è più probabile che il modello abbia una maggiore accuratezza.\nPoi, TinyNAS esegue un’operazione di ricerca sullo spazio scelto per trovare l’architettura ottimale per i vincoli specifici del microcontrollore. (J. Lin et al. 2020)\n\n\n\n\n\n\nFigura 9.33: Precisione degli spazi di ricerca. Fonte: J. Lin et al. (2020).\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\n\n\n\nTopology-Aware NAS\nSi concentra sulla creazione e l’ottimizzazione di uno spazio di ricerca allineato alla topologia hardware del dispositivo. (T. Zhang et al. 2020)\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Helen Li, e Yiran Chen. 2020. «AutoShrink: A Topology-Aware NAS for Discovering Efficient Neural Architecture». In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 6829–36. AAAI Press. https://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\n\n9.4.2 Sfide nella “Hardware-Aware Neural Architecture Search”\nSebbene HW-NAS abbia un potenziale elevato per trovare architetture ottimali per TinyML, presenta alcuni problemi. Le metriche hardware come latenza, consumo energetico e utilizzo dell’hardware sono più difficili da valutare rispetto alle metriche di accuratezza o di perdita. Spesso richiedono strumenti specializzati per misure precise. Inoltre, l’aggiunta di tutte queste metriche porta a uno spazio di ricerca molto più grande. Ciò fa sì che HW-NAS sia dispendioso in termini di tempo e denaro. Deve essere applicato a ogni hardware per risultati ottimali, tra le altre cose, il che significa che se si deve distribuire il modello su più dispositivi, la ricerca deve essere condotta più volte e produrrà modelli diversi, a meno che non si ottimizzi per tutti, il che significa una minore accuratezza. Infine, l’hardware cambia frequentemente e potrebbe essere necessario eseguire HW-NAS su ogni versione.\n\n\n9.4.3 Ottimizzazioni del Kernel\nLe ottimizzazioni del kernel sono modifiche apportate al kernel per migliorare le prestazioni dei modelli di apprendimento automatico su dispositivi con risorse limitate. Separeremo le ottimizzazioni del kernel in due tipi.\n\nOttimizzazioni del kernel Generali\nQueste sono ottimizzazioni del kernel da cui tutti i dispositivi possono trarre vantaggio. Forniscono tecniche per convertire il codice in istruzioni più efficienti.\n\n“Srotolamento” del Loop\nInvece di avere un loop con “loop control” (incrementando il contatore, si controlla la condizione di terminazione del loop), il loop può essere srotolato e il sovraccarico del “loop control” può essere omesso. Questo può anche fornire ulteriori opportunità di parallelismo che potrebbero non essere possibili con la struttura con loop. Questo può essere particolarmente utile per loop stretti, in cui il corpo del loop è un piccolo numero di istruzioni con molte iterazioni.\n\n\nBlocking\nIl Blocking viene utilizzato per rendere più efficienti i modelli di accesso alla memoria. Se abbiamo tre calcoli, il primo e l’ultimo devono accedere alla cache A e il secondo deve accedere alla cache B, il “blocking” ferma i primi due calcoli per ridurre il numero di letture di memoria necessarie.\n\n\nTiling\nAnalogamente al blocking, il tiling [piastrellatura] divide i dati e il calcolo in blocchi, ma si estende oltre i miglioramenti della cache. Il tiling crea partizioni di calcolo indipendenti che possono essere eseguite in parallelo, il che può comportare significativi miglioramenti delle prestazioni.\n\n\nLibrerie Kernel Ottimizzate\nQuesto comprende lo sviluppo di kernel ottimizzati che sfruttano appieno un hardware specifico. Un esempio è la libreria CMSIS-NN, che è una raccolta di kernel di reti neurali efficienti sviluppati per ottimizzare le prestazioni e ridurre al minimo l’ingombro di memoria dei modelli sui processori Arm Cortex-M, comuni sui dispositivi edge IoT. Il kernel sfrutta più capacità hardware dei processori Cortex-M come Single Instruction Multiple Data (SIMD), Floating Point Unit (FPU) e M-Profile Vector Extensions (MVE). Queste ottimizzazioni rendono più efficienti le operazioni comuni come le moltiplicazioni di matrici, aumentando le prestazioni delle operazioni del modello sui processori Cortex-M. (Lai, Suda, e Chandra 2018)\n\nLai, Liangzhen, Naveen Suda, e Vikas Chandra. 2018. «CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs». https://arxiv.org/abs/1801.06601.\n\n\n\n\n9.4.4 Compute-in-Memory (CiM)\nQuesto è un esempio di progettazione congiunta di algoritmo e hardware. CiM è un paradigma di elaborazione che esegue calcoli all’interno della memoria. Pertanto, le architetture CiM consentono di eseguire operazioni direttamente sui dati archiviati, senza la necessità di spostare i dati avanti e indietro tra unità di elaborazione e memoria separate. Questo paradigma di progettazione è particolarmente utile in scenari in cui lo spostamento dei dati è una fonte primaria di consumo energetico e latenza, come nelle applicazioni TinyML su dispositivi edge. Figura 9.34 è un esempio di utilizzo di CiM in TinyML: l’individuazione delle parole chiave richiede un processo sempre attivo che cerca determinate parole di attivazione (come “Hey, Siri”). Data la natura ad alta intensità di risorse di questa attività, l’integrazione di CiM per il modello di rilevamento delle parole chiave sempre attivo può migliorare l’efficienza.\nAttraverso la progettazione congiunta di algoritmo e hardware, gli algoritmi possono essere ottimizzati per sfruttare le caratteristiche uniche delle architetture CiM e, l’hardware CiM può essere personalizzato o configurato per supportare meglio i requisiti di elaborazione e le caratteristiche degli algoritmi. Ciò si ottiene utilizzando le proprietà analogiche delle celle di memoria, come l’addizione e la moltiplicazione nella DRAM. (Zhou et al. 2021)\n\n\n\n\n\n\nFigura 9.34: CiM per l’individuazione delle parole chiave. Fonte: Zhou et al. (2021).\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat, Xavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian, Manuel Le Gallo, e Paul N. Whatmough. 2021. «AnalogNets: Ml-hw Co-Design of Noise-robust TinyML Models and Always-On Analog Compute-in-Memory Accelerator». https://arxiv.org/abs/2111.06503.\n\n\n\n\n9.4.5 Ottimizzazione dell’Accesso alla Memoria\nDispositivi diversi possono avere gerarchie di memorie diverse. L’ottimizzazione per la gerarchia di memoria specifica nell’hardware specifico può portare a grandi miglioramenti delle prestazioni riducendo le costose operazioni di lettura e scrittura nella memoria. L’ottimizzazione del flusso di dati può essere ottenuta ottimizzando il riutilizzo dei dati all’interno di un singolo layer e tra più layer. Questa ottimizzazione del flusso di dati può essere adattata alla gerarchia di memoria specifica dell’hardware, il che può portare a maggiori vantaggi rispetto alle ottimizzazioni generali per diversi hardware.\n\nSfruttamento dei Dati Sparsi\nIl Pruning [potatura] è un approccio fondamentale per comprimere i modelli e renderli compatibili con dispositivi con risorse limitate. Ciò si traduce in modelli sparsi in cui molti pesi sono 0. Pertanto, sfruttare questa diradazione può portare a miglioramenti significativi nelle prestazioni. Sono stati creati degli strumenti per ottenere esattamente questo. RAMAN, è un acceleratore TinyML sparse progettato per l’inferenza su dispositivi edge. RAMAN sovrappone le attivazioni di input e output sullo stesso spazio di memoria, riducendo i requisiti di archiviazione fino al 50%. (Krishna et al. 2023)\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh Dwivedi, André van Schaik, Mahesh Mehendale, e Chetan Singh Thakur. 2023. «RAMAN: A Re-configurable and Sparse TinyML Accelerator for Inference on Edge». https://arxiv.org/abs/2306.06493.\n\n\nFramework di Ottimizzazione\nI framework di ottimizzazione sono stati introdotti per sfruttare le capacità specifiche dell’hardware per accelerare il software. Un esempio di tale framework è hls4ml: Figura 9.35 fornisce una panoramica del flusso di lavoro del framework. Questo flusso di lavoro di co-progettazione software-hardware open source aiuta a interpretare e tradurre algoritmi di machine learning per l’implementazione con tecnologie FPGA e ASIC. Funzionalità quali ottimizzazione di rete, nuove API Python, potatura consapevole della quantizzazione e flussi di lavoro FPGA end-to-end sono integrate nel framework hls4ml, sfruttando unità di elaborazione parallele, gerarchie di memoria e set di istruzioni specializzati per ottimizzare i modelli per hardware edge. Inoltre, hls4ml è in grado di tradurre algoritmi di apprendimento automatico direttamente nel firmware FPGA.\n\n\n\n\n\n\nFigura 9.35: workflow del framework hls4ml. Fonte: Fahim et al. (2021).\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, et al. 2021. «hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices». https://arxiv.org/abs/2103.05579.\n\n\nUn altro framework per FPGA che si concentra su un approccio olistico è CFU Playground (Prakash et al. 2023)\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. «CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs». In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nHardware Costruito Attorno al Software\nIn un approccio contrastante, l’hardware può essere progettato su misura attorno ai requisiti software per ottimizzare le prestazioni per un’applicazione specifica. Questo paradigma crea hardware specializzato per adattarsi meglio alle specifiche del software, riducendo così il sovraccarico computazionale e migliorando l’efficienza operativa. Un esempio di questo approccio è un’applicazione di riconoscimento vocale di (Kwon e Park 2021). Il documento propone una struttura in cui le operazioni di pre-elaborazione, tradizionalmente gestite dal software, sono assegnate ad un hardware progettato su misura. Questa tecnica è stata ottenuta introducendo la logica resistore-transistor in un modulo audio a circuito inter-integrato per il windowing e l’acquisizione di dati audio grezzi nell’applicazione di riconoscimento vocale. Di conseguenza, questa “delega” delle operazioni di pre-elaborazione ha portato a una riduzione del carico computazionale sul software, mostrando un’applicazione pratica della creazione di hardware attorno al software per migliorare l’efficienza e le prestazioni.\n\n\n\n\n\n\nFigura 9.36: Delega dell’elaborazione dei dati a un FPGA. Fonte: Kwon e Park (2021).\n\n\nKwon, Jisu, e Daejin Park. 2021. «Hardware/Software Co-Design for TinyML Voice-Recognition Application on Resource Frugal Edge Devices». Applied Sciences 11 (22): 11073. https://doi.org/10.3390/app112211073.\n\n\n\n\nSplitNet\nLi SplitNet sono state introdotte nel contesto dei sistemi Head-Mounted. Distribuiscono il carico di lavoro delle Deep Neural Network (DNN) tra i sensori della telecamera e un aggregatore. Ciò è particolarmente interessante nel contesto di TinyML. Il framework SplitNet è un NAS split-aware per trovare l’architettura di rete neurale ottimale per ottenere una buona accuratezza, dividere il modello tra i sensori e l’aggregatore e ridurre al minimo la comunicazione tra i sensori e l’aggregatore. Figura 9.37 dimostra come le SplitNet (in rosso) ottengano una maggiore accuratezza per una latenza inferiore (in esecuzione su ImageNet) rispetto ad altri approcci, come l’esecuzione del DNN sul sensore (All-on-sensor; in verde) o sul cellulare (All-on-aggregator; in blu). La comunicazione minima è importante in TinyML dove la memoria è fortemente limitata, in questo modo i sensori conducono parte dell’elaborazione sui loro chip e poi inviano solo le informazioni necessarie all’aggregatore. Durante i test su ImageNet, SplitNets è stato in grado di ridurre la latenza di un ordine di grandezza sui dispositivi di visione artificiale montati sulla testa [occhiali o visori]. Ciò può essere utile quando il sensore ha il suo chip. (Dong et al. 2022)\n\n\n\n\n\n\nFigura 9.37: Le SplitNet rispetto ad altri approcci. Fonte: Dong et al. (2022).\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T. Kung, e Ziyun Li. 2022. «SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12549–59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\n\n\nHardware Specifico per il “Data Augmentation”\nOgni dispositivo edge può possedere caratteristiche di sensore uniche, che portano a specifici modelli di rumore che possono influire sulle prestazioni del modello. Un esempio sono i dati audio, in cui sono prevalenti le variazioni derivanti dalla scelta del microfono. Applicazioni come le Keyword Spotting possono sperimentare miglioramenti sostanziali incorporando dati registrati da dispositivi simili a quelli destinati all’implementazione. La messa a punto dei modelli esistenti può essere impiegata per adattare i dati in modo preciso alle caratteristiche distintive del sensore.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#supporto-software-e-framework",
    "href": "contents/optimizations/optimizations.it.html#supporto-software-e-framework",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.5 Supporto Software e Framework",
    "text": "9.5 Supporto Software e Framework\nSebbene tutte le tecniche sopra menzionate come pruning, quantizzazione e numeri efficienti siano ben note, rimarrebbero poco pratiche e inaccessibili senza un ampio supporto software. Ad esempio, la quantizzazione diretta di pesi e attivazioni in un modello richiederebbe la modifica manuale della definizione del modello e l’inserimento di operazioni di quantizzazione. Allo stesso modo, la potatura diretta dei pesi del modello richiede la manipolazione dei tensori dei pesi. Tali approcci noiosi diventano impraticabili su larga scala.\nSenza l’ampia innovazione software nei framework, negli strumenti di ottimizzazione e nell’integrazione hardware, la maggior parte di queste tecniche rimarrebbe teorica o praticabile solo per gli esperti. Senza API del framework e automazione per semplificare l’applicazione di queste ottimizzazioni, non verrebbero adottate. Il supporto software le rende accessibili al pubblico e sblocca vantaggi concreti. Inoltre, problemi come la messa a punto degli iperparametri per la potatura, la gestione del compromesso tra dimensioni del modello e accuratezza e la garanzia della compatibilità con i dispositivi target pongono ostacoli che gli sviluppatori devono superare.\n\n9.5.1 API Native di Ottimizzazione\nI principali framework di machine learning come TensorFlow, PyTorch e MXNet forniscono librerie e API per consentire l’applicazione di tecniche comuni di ottimizzazione dei modelli senza richiedere implementazioni personalizzate. Ad esempio, TensorFlow offre il TensorFlow Model Optimization Toolkit che contiene moduli come:\n\nquantization - Applica un training che tiene conto della quantizzazione per convertire i modelli in virgola mobile in una precisione inferiore come int8 con una perdita di accuratezza minima. Gestisce la quantizzazione del peso e dell’attivazione.\nsparsity - Fornisce API di potatura per indurre la “sparsità” e rimuovere connessioni non necessarie in modelli come le reti neurali. Può potare pesi, livelli, ecc.\nclustering - Supporta la compressione del modello raggruppando i pesi per tassi di compressione più elevati.\n\nQueste API consentono agli utenti di abilitare tecniche di ottimizzazione come la quantizzazione e la potatura senza modificare direttamente il codice del modello. È possibile configurare parametri come i tassi di “sparsità” del target, le larghezze di bit di quantizzazione, ecc. Allo stesso modo, PyTorch fornisce torch.quantization per convertire i modelli in rappresentazioni di precisione inferiore. TorchTensor e TorchModule formano le classi di base per il supporto della quantizzazione. Offre inoltre torch.nn.utils.prune per la potatura nativa dei modelli. MXNet offre layer gluon.contrib che aggiungono funzionalità di quantizzazione come l’arrotondamento a punto fisso e l’arrotondamento stocastico di pesi/attivazioni durante l’addestramento. Ciò consente di includere facilmente la quantizzazione nei modelli gluon.\nIl vantaggio principale delle ottimizzazioni integrate è che gli utenti possono applicarle senza dover reimplementare tecniche complesse. Ciò rende i modelli ottimizzati accessibili a un’ampia gamma di professionisti. Garantisce inoltre che le best practice siano seguite basandosi sulla ricerca e sull’esperienza nell’implementazione dei metodi. Man mano che emergono nuove ottimizzazioni, i framework si sforzano di fornire supporto nativo e API ove possibile per abbassare ulteriormente la barriera verso un ML efficiente. La disponibilità di questi strumenti è fondamentale per un’adozione diffusa.\n\n\n9.5.2 Strumenti di Ottimizzazione Automatizzata\nGli strumenti di ottimizzazione automatizzati forniti dai framework possono analizzare i modelli e applicare automaticamente ottimizzazioni come quantizzazione, potatura e fusione degli operatori per rendere il processo più semplice e accessibile senza un’eccessiva messa a punto manuale. In effetti, questo si basa sulla sezione precedente. Ad esempio, TensorFlow fornisce il TensorFlow Model Optimization Toolkit che contiene moduli come:\n\nQuantizationAwareTraining - Quantizza automaticamente pesi e attivazioni in un modello per ridurre la precisione come UINT8 o INT8 con una perdita di accuratezza minima. Inserisce nodi di quantizzazione falsi durante l’addestramento in modo che il modello possa imparare a essere compatibile con la quantizzazione.\nPruning - Rimuove automaticamente le connessioni non necessarie in un modello in base all’analisi dell’importanza del peso. Può potare interi filtri in livelli convoluzionali o “attention head” [teste di attenzione] nei trasformatori. Gestisce il ri-addestramento iterativo per recuperare qualsiasi perdita di accuratezza.\nGraphOptimizer - Applica ottimizzazioni grafiche come la fusione degli operatori per consolidare le operazioni e ridurre la latenza di esecuzione, in particolare per l’inferenza. In Figura 9.38, si può vedere l’originale (Source Graph) a sinistra e come le sue operazioni vengono trasformate (consolidate) a destra. Notare come Block1 in Source Graph abbia 3 passaggi separati (Convolution, BiasAdd e Activation), che vengono poi consolidati insieme in Block1 su Optimized Graph.\n\n\n\n\n\n\n\nFigura 9.38: GraphOptimizer. Fonte: Wess et al. (2020).\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, e Anvesh Nookala. 2020. «ANNETTE: Accurate Neural Network Execution Time Estimation with Stacked Models». IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nQuesti moduli automatizzati richiedono solo all’utente di fornire il modello originale in virgola mobile e di gestire la pipeline di ottimizzazione end-to-end, inclusa qualsiasi riqualificazione per ripristinare la precisione. Anche altri framework come PyTorch offrono un crescente supporto all’automazione, ad esempio tramite torch.quantization.quantize_dynamic. L’ottimizzazione automatizzata rende l’apprendimento automatico efficiente accessibile ai professionisti senza competenze di ottimizzazione.\n\n\n9.5.3 Librerie di Ottimizzazione Hardware\nLibrerie hardware come TensorRT e TensorFlow XLA consentono di ottimizzare i modelli per l’hardware target tramite tecniche di cui abbiamo discusso in precedenza.\n\nQuantizzazione: Ad esempio, TensorRT e TensorFlow Lite supportano entrambi la quantizzazione dei modelli durante la conversione nel loro formato. Ciò fornisce accelerazioni sui SoC mobili con supporto INT8/INT4.\nOttimizzazione del Kernel: ad esempio, TensorRT esegue l’auto-tuning per ottimizzare i kernel CUDA in base all’architettura GPU per ogni layer nel grafo del modello. Ciò estrae la massima produttività.\nFusione degli Operatori: TensorFlow XLA esegue una fusione aggressiva per creare un binario ottimizzato per le TPU. Sui dispositivi mobili, framework come NCNN supportano anche operatori fusi [unificati].\nCodice Specifico per l’Hardware: Le librerie vengono utilizzate per generare codice binario ottimizzato specializzato per l’hardware target. Per esempio, TensorRT usa librerie Nvidia CUDA/cuDNN che sono ottimizzate manualmente per ogni architettura GPU. Questa codifica specifica per hardware è fondamentale per le prestazioni. Sui dispositivi TinyML, questo può significare codice assembly ottimizzato per una CPU Cortex M4, ad esempio. I fornitori forniscono CMSIS-NN e altre librerie.\nOttimizzazioni del Layout dei Dati: Possiamo sfruttare in modo efficiente la gerarchia della memoria di hardware come cache e registri tramite tecniche come riorganizzazione tensore/peso, tiling e riutilizzo. Ad esempio, TensorFlow XLA ottimizza i layout dei buffer per massimizzare l’utilizzo della TPU. Questo aiuta qualsiasi sistema con limiti di memoria.\nOttimizzazione Basata sulla Profilazione: Possiamo usare strumenti di profilazione per identificare i colli di bottiglia. Ad esempio, regolare i livelli di fusione del kernel in base alla profilazione della latenza. Sui SoC mobili, fornitori come Qualcomm forniscono profiler in SNPE per trovare opportunità di ottimizzazione nelle CNN. Questo approccio basato sui dati è importante per le prestazioni.\n\nIntegrando i modelli di framework con queste librerie hardware tramite pipeline di conversione ed esecuzione, gli sviluppatori di ML possono ottenere significativi incrementi di velocità e guadagni di efficienza da ottimizzazioni di basso livello su misura per l’hardware target. La stretta integrazione tra software e hardware è fondamentale per consentire un’implementazione performante delle applicazioni di ML, in particolare su dispositivi mobili e TinyML.\n\n\n9.5.4 Visualizzazione delle Ottimizzazioni\nL’implementazione di tecniche di ottimizzazione del modello senza visibilità degli effetti sul modello può essere impegnativa. Strumenti dedicati o strumenti di visualizzazione possono fornire informazioni critiche e utili sulle modifiche del modello e aiutano a tracciare il processo di ottimizzazione. Consideriamo le ottimizzazioni che abbiamo considerato in precedenza, come la potatura per la “sparsity” [diradazione] e la quantizzazione.\n\nSparsità\nAd esempio, si considerino le ottimizzazioni di sparsity. Gli strumenti di visualizzazione di sparsity possono fornire informazioni critiche sui modelli potati, mappando esattamente quali pesi sono stati rimossi. Ad esempio, le mappe di calore di sparsity possono utilizzare gradienti di colore per indicare la percentuale di pesi potati in ogni layer di una rete neurale. I layer con percentuali di potatura più elevate appaiono più scuri (cfr. Figura 9.39). Questo identifica quali layer sono stati semplificati di più tramite potatura (Souza 2020).\n\n\n\n\n\n\nFigura 9.39: Mappa “termica” della rete sparsa. Fonte: Numenta.\n\n\n\nI grafici di tendenza possono anche tracciare la scarsità nei successivi round di potatura: possono mostrare una rapida potatura iniziale seguita da incrementi più graduali. Il tracciamento della diradazione globale corrente insieme a statistiche come la diradazione media, minima e massima per ogli layer in tabelle o grafici fornisce una panoramica della composizione del modello. Per una rete convoluzionale di esempio, questi strumenti potrebbero rivelare che il primo layer di convoluzione viene potato del 20% mentre quello di classificazione finale viene potato del 70% data la sua ridondanza. La diradazione del modello globale può aumentare dal 10% dopo la potatura iniziale al 40% dopo cinque round.\nRendendo i dati di diradazione visivamente accessibili, i professionisti possono comprendere meglio esattamente come il loro modello viene ottimizzato e quali aree vengono interessate. La visibilità consente loro di mettere a punto e controllare il processo di potatura per una determinata architettura.\nLa visualizzazione della diradazione trasforma la potatura in una tecnica trasparente anziché in un’operazione “black-box”.\n\n\nQuantizzazione\nLa conversione di modelli in precisioni numeriche inferiori tramite quantizzazione introduce errori che possono influire sulla precisione del modello se non vengono monitorati e affrontati correttamente. La visualizzazione delle distribuzioni degli errori di quantizzazione fornisce informazioni preziose sugli effetti dei numeri di precisione ridotti applicati a diverse parti di un modello. Per questo, è possibile generare istogrammi degli errori di quantizzazione per pesi e attivazioni. Questi istogrammi possono rivelare la forma della distribuzione degli errori, se assomigliano a una distribuzione gaussiana o contengono valori anomali e picchi significativi. Figura 9.40 mostra le distribuzioni di diversi metodi di quantizzazione. Valori anomali elevati possono indicare problemi con particolari layer che gestiscono la quantizzazione. Il confronto degli istogrammi tra layer evidenzia eventuali aree problematiche che si distinguono con errori anormalmente elevati.\n\n\n\n\n\n\nFigura 9.40: Errori di Quantizzazione. Fonte: Kuzmin et al. (2022).\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, e Tijmen Blankevoort. 2022. «FP8 Quantization: The Power of the Exponent». https://arxiv.org/abs/2208.09225.\n\n\nLe visualizzazioni di attivazione sono importanti anche per rilevare problemi di overflow. Con la mappatura a colori delle attivazioni prima e dopo la quantizzazione, tutti i valori spinti al di fuori degli intervalli previsti diventano visibili. Ciò rivela problemi di saturazione e troncamento che potrebbero alterare le informazioni che fluiscono attraverso il modello. Il rilevamento di questi errori consente di ricalibrare le attivazioni per evitare la perdita di informazioni (Mandal 2022). Figura 9.41 è una mappatura a colori dei kernel convoluzionali AlexNet.\n\n\n\n\n\n\nFigura 9.41: Mappatura a colori delle attivazioni. Fonte: Krizhevsky, Sutskever, e Hinton (2017).\n\n\nKrizhevsky, Alex, Ilya Sutskever, e Geoffrey E. Hinton. 2017. «ImageNet classification with deep convolutional neural networks». A cura di F. Pereira, C. J. Burges, L. Bottou, e K. Q. Weinberger. Commun. ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\nAltre tecniche, come il tracciamento dell’errore di quantizzazione quadratico medio complessivo a ogni passaggio del processo di addestramento consapevole della quantizzazione, identificano fluttuazioni e divergenze. Picchi improvvisi nel grafico di tracciamento possono indicare punti in cui la quantizzazione sta interrompendo l’addestramento del modello. Il monitoraggio di questa metrica crea intuizione sul comportamento del modello in fase di quantizzazione. Insieme, queste tecniche trasformano la quantizzazione in un processo trasparente. Le intuizioni empiriche consentono ai professionisti di valutare correttamente gli effetti della quantizzazione. Individuano le aree dell’architettura del modello o del processo di training da ricalibrare in base ai problemi di quantizzazione osservati. Ciò aiuta a ottenere modelli quantizzati numericamente stabili e accurati.\nFornire questi dati consente ai professionisti di valutare correttamente l’impatto della quantizzazione e identificare potenziali aree problematiche del modello da ricalibrare o riprogettare per renderlo più adatto alla quantizzazione. Questa analisi empirica sviluppa l’intuizione sul raggiungimento di una quantizzazione ottimale.\nGli strumenti di visualizzazione possono fornire approfondimenti che aiutano i professionisti a comprendere meglio gli effetti delle ottimizzazioni sui loro modelli. La visibilità consente di correggere i problemi in anticipo prima che l’accuratezza o le prestazioni siano influenzate in modo significativo. Aiuta anche ad applicare le ottimizzazioni in modo più efficace per modelli specifici. Queste analisi di ottimizzazione aiutano a sviluppare l’intuizione quando si trasferiscono i modelli a rappresentazioni più efficienti.\n\n\n\n9.5.5 Conversione e Distribuzione del Modello\nUna volta che i modelli sono stati ottimizzati con successo in framework come TensorFlow e PyTorch, sono necessarie piattaforme specializzate di conversione e deployment [distribuzione] del modello per colmare il divario con l’esecuzione sui dispositivi target.\nTensorFlow Lite - La piattaforma di TensorFlow per convertire i modelli in un formato leggero ottimizzato per dispositivi mobili, embedded ed edge. Supporta ottimizzazioni come quantizzazione, fusione del kernel e rimozione di operazioni inutilizzate. I modelli possono essere eseguiti utilizzando kernel TensorFlow Lite ottimizzati sull’hardware del dispositivo. Fondamentale per la distribuzione mobile e TinyML.\nONNX Runtime - Esegue la conversione e l’inferenza per i modelli nel formato “open ONNX”. Fornisce kernel ottimizzati, supporta acceleratori hardware come GPU e distribuzione multipiattaforma dal cloud all’edge. Consente la distribuzione indipendente dal framework. Figura 9.42 è una mappa di interoperabilità ONNX, inclusi i principali framework più diffusi.\n\n\n\n\n\n\nFigura 9.42: Interoperabilità di ONNX. Fonte: TowardsDataScience.\n\n\n\nPyTorch Mobile - Consente l’esecuzione dei modelli PyTorch su iOS e Android convertendoli in rappresentazioni ottimizzate per dispositivi mobili. Fornisce implementazioni mobili efficienti di operazioni come convoluzione e funzioni speciali ottimizzate per hardware mobile.\nQueste piattaforme si integrano con driver hardware, sistemi operativi e librerie di acceleratori sui dispositivi per eseguire modelli in modo efficiente utilizzando l’ottimizzazione hardware. Inoltre, delegano le operazioni ad acceleratori ML dedicati, ove presenti. La disponibilità di queste piattaforme di distribuzione collaudate e robuste colma il divario tra l’ottimizzazione dei modelli nei framework e la distribuzione effettiva su miliardi di dispositivi. Consentono agli utenti di concentrarsi sullo sviluppo del modello anziché sulla creazione di runtime mobili personalizzati. L’innovazione continua per supportare nuovi hardware e ottimizzazioni in queste piattaforme è fondamentale per le ottimizzazioni di ML diffuse.\nFornendo queste pipeline di distribuzione ottimizzate, l’intero flusso di lavoro, dal training al deployment [distribuzione] del dispositivo, può sfruttare le ottimizzazioni del modello per fornire applicazioni ML performanti. Questa infrastruttura software end-to-end ha contribuito a guidare l’adozione di ML sul dispositivo.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#conclusione",
    "href": "contents/optimizations/optimizations.it.html#conclusione",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.6 Conclusione",
    "text": "9.6 Conclusione\nIn questo capitolo abbiamo discusso l’ottimizzazione del modello nell’ambito software-hardware. Ci siamo immersi in una rappresentazione efficiente del modello, dove abbiamo trattato le sfumature della potatura strutturata e non-strutturata e altre tecniche per la compressione del modello come la distillazione della conoscenza e la decomposizione di matrice e tensore. Ci siamo anche immersi brevemente nella progettazione del modello specifico per l’edge a livello di parametri e architettura del modello, esplorando argomenti come modelli specifici per l’edge e NAS basati sull’hardware.\nAbbiamo quindi esplorato rappresentazioni numeriche efficienti, dove abbiamo trattato le basi della matematica, codifiche numeriche e archiviazione, vantaggi della matematica efficiente e le sfumature della rappresentazione numerica con utilizzo della memoria, complessità computazionale, compatibilità hardware e scenari di compromesso. Abbiamo concluso concentrandoci su un elemento fondamentale della matematica efficiente: la quantizzazione, dove abbiamo esaminato la sua storia, calibrazione, tecniche e interazione con la potatura.\nInfine, abbiamo esaminato come possiamo apportare ottimizzazioni specifiche per l’hardware che abbiamo. Abbiamo esplorato come possiamo trovare architetture modello su misura per l’hardware, apportare ottimizzazioni nel kernel per gestire meglio il modello e framework creati per sfruttare al meglio l’hardware. Abbiamo anche esaminato come possiamo fare il contrario e creare hardware attorno al nostro software specifico e abbiamo parlato di come suddividere le reti per l’esecuzione su più processori disponibili sul dispositivo edge.\nComprendendo il quadro completo dei gradi di libertà all’interno dell’ottimizzazione del modello sia lontano che vicino all’hardware e i compromessi da considerare quando si implementano questi metodi, i professionisti possono sviluppare una pipeline più ponderata per comprimere i loro carichi di lavoro sui dispositivi edge.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/optimizations/optimizations.it.html#sec-model-optimizations-resource",
    "href": "contents/optimizations/optimizations.it.html#sec-model-optimizations-resource",
    "title": "9  Ottimizzazioni dei Modelli",
    "section": "9.7 Risorse",
    "text": "9.7 Risorse\nEcco un elenco curato di risorse per supportare sia gli studenti che gli insegnanti nel loro percorso di apprendimento e insegnamento. Stiamo lavorando costantemente per espandere questa raccolta e aggiungeremo nuovi esercizi nel prossimo futuro.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nQuantizzazione:\n\nQuantization: Part 1.\nQuantization: Part 2.\nPost-Training Quantization (PTQ).\nQuantization-Aware Training (QAT).\n\nPruning:\n\nPruning: Part 1.\nPruning: Part 2.\n\nKnowledge Distillation.\nClustering.\nNeural Architecture Search (NAS):\n\nNAS overview.\nNAS: Part 1.\nNAS: Part 2.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 9.1\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 9.1\nEsercizio 9.2\nEsercizio 9.3\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ottimizzazioni dei Modelli</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html",
    "href": "contents/hw_acceleration/hw_acceleration.it.html",
    "title": "10  Accelerazione IA",
    "section": "",
    "text": "10.1 Introduzione\nIl “machine learning” [apprendimento automatico] è emerso come una tecnologia trasformativa in molti settori, consentendo ai sistemi di apprendere e migliorare dai dati. Esiste una crescente domanda di soluzioni ML embedded per implementare funzionalità di apprendimento automatico in ambienti reali, in cui i modelli sono integrati in dispositivi edge come smartphone, elettrodomestici e veicoli autonomi. Tuttavia, questi dispositivi edge hanno risorse di elaborazione limitate rispetto ai server dei data center.\nL’accelerazione hardware specializzata consente l’apprendimento automatico ad alte prestazioni su dispositivi edge con risorse limitate. L’accelerazione hardware si riferisce all’utilizzo di chip e architetture in silicio personalizzati per esentare il processore principale dall’eseguire le operazioni ML ad alta intensità di elaborazione. Nelle reti neurali, i calcoli più intensivi sono le moltiplicazioni di matrici durante l’inferenza. Gli acceleratori hardware possono ottimizzare queste operazioni matriciali, fornendo accelerazioni da 10 a 100 volte superiori rispetto alle CPU generiche. Questa accelerazione sblocca la possibilità di eseguire modelli di reti neurali avanzati su dispositivi con vincoli di dimensioni, peso e potenza in tempo reale.\nQuesto capitolo fornisce una panoramica delle tecniche di accelerazione hardware per l’apprendimento automatico embedded e dei relativi compromessi di progettazione. L’obiettivo è fornire ai lettori un background essenziale nell’accelerazione del ML embedded. Questo consentirà una selezione informata dell’hardware e l’ottimizzazione del software per sviluppare capacità di apprendimento automatico ad alte prestazioni su dispositivi edge.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#background-e-basi",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#background-e-basi",
    "title": "10  Accelerazione IA",
    "section": "10.2 Background e Basi",
    "text": "10.2 Background e Basi\n\n10.2.1 Background Storico\nLe origini dell’accelerazione hardware risalgono agli anni ’60, con l’avvento dei coprocessori matematici in virgola mobile per eliminare i calcoli dalla CPU principale. Un primo esempio è stato il chip Intel 8087 rilasciato nel 1980 per accelerare le operazioni in virgola mobile per il processore 8086. Ciò ha stabilito la pratica di utilizzare processori specializzati per gestire in modo efficiente carichi di lavoro ad alta intensità di calcolo.\nNegli anni ’90, sono emerse le prime Graphics Processing Units (GPU) [Unità di elaborazione grafica] per elaborare rapidamente pipeline grafiche per rendering e giochi. La GeForce 256 di Nvidia nel 1999 è stata una delle prime GPU programmabili in grado di eseguire algoritmi software personalizzati. Le GPU esemplificano acceleratori a funzione fissa specifici per dominio e si sono evolute in acceleratori programmabili paralleli.\nNegli anni 2000, le GPU sono state applicate all’elaborazione generica in GPGPU. La loro elevata larghezza di banda di memoria e la produttività computazionale le hanno rese adatte a carichi di lavoro ad alta intensità di calcolo. Ciò ha incluso innovazioni nell’uso di GPU per accelerare il training di modelli di deep learning come AlexNet nel 2012.\nNegli ultimi anni, le Tensor Processing Unit (TPU) di Google rappresentano ASIC personalizzati specificamente progettati per la moltiplicazione di matrici nel deep learning. Durante l’inferenza, i loro core tensoriali ottimizzati raggiungono TeraOPS/watt più elevati rispetto a CPU o GPU. L’innovazione continua include tecniche di compressione del modello come pruning e quantizzazione per adattare reti neurali più grandi su dispositivi edge.\nQuesta evoluzione dimostra come l’accelerazione hardware si sia concentrata sulla risoluzione di colli di bottiglia ad alta intensità di calcolo, dalla matematica in virgola mobile alla grafica alla moltiplicazione di matrici per ML. Comprendere questa storia fornisce un contesto cruciale per gli acceleratori AI specializzati odierni.\n\n\n10.2.2 La Necessità di Accelerazione\nL’evoluzione dell’accelerazione hardware è strettamente legata alla storia più ampia dell’informatica. Nei primi decenni, la progettazione dei chip era regolata dalla legge di Moore e dal Dennard Scaling, che osservava che il numero di transistor su un circuito integrato raddoppiava ogni anno e le loro prestazioni (velocità) aumentavano man mano che i transistor diventavano più piccoli. Allo stesso tempo, la densità della potenza (potenza per unità di area) rimane costante. Queste due leggi sono state mantenute durante l’era single-core. Figura 10.1 mostra le tendenze di diverse metriche dei microprocessori. Come indica la figura, il Dennard Scaling fallisce intorno alla metà degli anni 2000; si noti come la velocità di clock (frequenza) rimanga pressoché costante anche se il numero di transistor continua ad aumentare.\nTuttavia, come descrive Patterson e Hennessy (2016), i vincoli tecnologici alla fine hanno imposto una transizione all’era multicore, con chip contenenti più core di elaborazione per offrire guadagni in termini di prestazioni. Le limitazioni di potenza hanno impedito un ulteriore ridimensionamento, il che ha portato al “silicio scuro” (Dark Silicon), in cui non tutte le aree del chip potevano essere attive simultaneamente (Xiu 2019).\n\nPatterson, David A, e John L Hennessy. 2016. Computer organization and design ARM edition: The hardware software interface. Morgan kaufmann.\n\nXiu, Liming. 2019. «Time Moore: Exploiting Moore’s Law From The Perspective of Time». IEEE Solid-State Circuits Mag. 11 (1): 39–55. https://doi.org/10.1109/mssc.2018.2882285.\nIl concetto di “dark silicon” è emerso come conseguenza di queste limitazioni. “Dark silicon” si riferisce a parti del chip che non possono essere alimentate simultaneamente a causa di limitazioni termiche e di potenza. In sostanza, con l’aumento della densità dei transistor, la quota del chip che poteva essere utilizzata attivamente senza surriscaldarsi o superare i budget di potenza si è ridotta.\nQuesto fenomeno ha comportato che, sebbene i chip avessero più transistor, non tutti potevano essere operativi simultaneamente, limitando i potenziali guadagni in termini di prestazioni. Questa crisi energetica ha reso necessario un passaggio all’era degli acceleratori, con unità hardware specializzate su misura per attività specifiche per massimizzare l’efficienza. L’esplosione dei carichi di lavoro dell’intelligenza artificiale ha ulteriormente spinto la domanda di acceleratori personalizzati. I fattori abilitanti includevano nuovi linguaggi di programmazione, strumenti software e progressi nella produzione.\n\n\n\n\n\n\nFigura 10.1: Tendenze dei Microprocessori. Fonte: Karl Rupp.\n\n\n\nFondamentalmente, gli acceleratori hardware vengono valutati in base a Prestazioni, Potenza e Area di silicio (PPA); la natura dell’applicazione target, sia essa legata alla memoria o al calcolo, influenza notevolmente la progettazione. Ad esempio, i carichi di lavoro legati alla memoria richiedono un’elevata larghezza di banda e un accesso a bassa latenza, mentre le applicazioni legate al calcolo richiedono la massima produttività di elaborazione.\n\n\n10.2.3 Principi Generali\nLa progettazione di acceleratori hardware specializzati comporta la gestione di compromessi complessi tra prestazioni, efficienza energetica, area di silicio e ottimizzazioni specifiche del carico di lavoro. Questa sezione delinea considerazioni e metodologie fondamentali per raggiungere un equilibrio ottimale in base ai requisiti dell’applicazione e ai vincoli hardware.\n\nPrestazioni entro i Budget di Potenza\nLe prestazioni si riferiscono alla produttività del lavoro di elaborazione per unità di tempo, comunemente misurata in operazioni in virgola mobile al secondo (FLOPS) o frame al secondo (FPS). Prestazioni più elevate consentono di completare più lavoro, ma il consumo di energia aumenta con l’attività.\nGli acceleratori hardware mirano a massimizzare le prestazioni entro budget di potenza stabiliti. Ciò richiede un attento bilanciamento del parallelismo, della frequenza di clock del chip, della tensione di esercizio, dell’ottimizzazione del carico di lavoro e di altre tecniche per massimizzare le operazioni per watt.\n\nPrestazioni = Throughput * Efficienza\nThroughput ~= Parallelismo * Frequenza di Clock\nEfficienza = Operazioni / Watt\n\nAd esempio, le GPU raggiungono un throughput elevato tramite architetture massivamente parallele. Tuttavia, la loro efficienza è inferiore a quella dei circuiti integrati specifici per applicazione (ASIC) personalizzati come il TPU di Google, che ottimizzano per un carico di lavoro specifico.\n\n\nGestione dell’Area e dei Costi del Silicio\nL’area del chip ha un impatto diretto sui costi di produzione. Le dimensioni maggiori dei die [https://it.wikipedia.org/wiki/Die_(elettronica)] richiedono più materiali, rese inferiori e tassi di difettosità più elevati. I pacchetti multi-die aiutano a scalare i progetti ma aggiungono complessità al packaging. L’area del silicio dipende da:\n\nRisorse di Calcolo, ad esempio numero di core, memoria, cache\nNodo del Processo di Produzione, transistor più piccoli consentono una maggiore densità\nModello di Programmazione, acceleratori programmati richiedono maggiore flessibilità\n\nLa progettazione dell’acceleratore implica la compressione delle massime prestazioni entro i vincoli dell’area. Tecniche come il pruning [potatura] e la compressione aiutano ad adattare modelli più grandi al chip.\n\n\nOttimizzazioni Specifiche del Carico di Lavoro\nIl carico di lavoro del target determina le architetture ottimali dell’acceleratore. Alcune delle considerazioni chiave includono:\n\nMemoria vs Limiti di Calcolo: I carichi di lavoro vincolati alla memoria richiedono una maggiore larghezza di banda di memoria, mentre le app vincolate al calcolo necessitano di un throughput [produttività] aritmetico.\nLocalità dei Dati: Lo spostamento dei dati dovrebbe essere ridotto al minimo per l’efficienza. La memoria vicina al calcolo aiuta.\nOperazioni a Livello di Bit: I tipi di dati a bassa precisione come INT8/INT4 ottimizzano la densità di calcolo.\nParallelismo dei Dati: Più unità di calcolo replicate consentono l’esecuzione parallela.\nPipelining: L’esecuzione sovrapposta delle operazioni aumenta la produttività.\n\nLa comprensione delle caratteristiche del carico di lavoro consente un’accelerazione personalizzata. Ad esempio, le reti neurali convoluzionali utilizzano operazioni di “finestra scorrevole” mappate in modo ottimale su array spaziali di elementi di elaborazione.\nSpaziando tra questi compromessi architettonici, gli acceleratori hardware possono offrire enormi guadagni in termini di prestazioni e abilitare applicazioni emergenti in intelligenza artificiale, grafica, elaborazione scientifica e altri domini.\n\n\nProgettazione Hardware Sostenibile\nNegli ultimi anni, la sostenibilità dell’IA è diventata una preoccupazione urgente, guidata da due fattori chiave: la scala crescente dei carichi di lavoro dell’IA e il consumo energetico associato.\nInnanzitutto, le dimensioni dei modelli e dei set di dati dell’IA sono cresciute rapidamente. Ad esempio, in base alle tendenze di elaborazione dell’IA di OpenAI, la quantità di elaborazione utilizzata per addestrare modelli all’avanguardia raddoppia ogni 3,5 mesi. Questa crescita esponenziale richiede enormi risorse di elaborazione nei data center.\nIn secondo luogo, l’uso di energia per l’addestramento e l’inferenza dell’IA presenta problemi di sostenibilità. I data center che eseguono applicazioni di IA consumano molta energia, contribuendo a elevate emissioni di carbonio. Si stima che l’addestramento di un grande modello di IA possa avere un’impronta di carbonio di 626.000 libbre di CO2 equivalente, quasi 5 volte le emissioni di un’auto media nel corso della sua vita.\nDi conseguenza, la ricerca e la pratica dell’IA devono dare priorità all’efficienza energetica e all’impatto sulle emissioni di carbonio insieme all’accuratezza. C’è una crescente attenzione all’efficienza del modello, alla progettazione del data center, all’ottimizzazione dell’hardware e ad altre soluzioni per migliorare la sostenibilità. Trovare un equilibrio tra progresso dell’intelligenza artificiale e responsabilità ambientale è emerso come una considerazione chiave e un’area di ricerca attiva in tutto il settore.\nSi prevede che la dimensione dei sistemi di intelligenza artificiale continuerà a crescere. Sviluppare un’intelligenza artificiale sostenibile è fondamentale per gestire l’impatto ambientale e consentire un’ampia distribuzione vantaggiosa di questa tecnologia trasformativa.\nParleremo di IA sostenibile in un capitolo successivo, dove ne discuteremo più in dettaglio.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#sec-aihw",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#sec-aihw",
    "title": "10  Accelerazione IA",
    "section": "10.3 Tipi di acceleratori",
    "text": "10.3 Tipi di acceleratori\nGli acceleratori hardware possono assumere molte forme. Possono esistere come widget (come il Neural Engine nel chip Apple M1) o come interi chip appositamente progettati per svolgere molto bene determinate attività. Questa sezione esaminerà i processori per carichi di lavoro di apprendimento automatico lungo lo spettro che va dagli ASIC altamente specializzati alle CPU più generiche. Ci concentriamo prima sull’hardware personalizzato appositamente progettato per l’intelligenza artificiale per comprendere le ottimizzazioni più estreme possibili quando vengono rimossi i vincoli di progettazione. Questo stabilisce un limite massimo per prestazioni ed efficienza.\nPoi prendiamo in considerazione progressivamente architetture più programmabili e adattabili, discutendo di GPU e FPGA. Queste fanno compromessi nella personalizzazione per mantenere la flessibilità. Infine, trattiamo le CPU generiche che sacrificano le ottimizzazioni per un carico di lavoro particolare in cambio di una programmabilità versatile tra le applicazioni.\nStrutturando l’analisi lungo questo spettro, miriamo a illustrare i compromessi fondamentali tra utilizzo, efficienza, programmabilità e flessibilità nella progettazione dell’acceleratore. Il punto di equilibrio ottimale dipende dai vincoli e dai requisiti dell’applicazione target. Questa prospettiva dello spettro fornisce un quadro per ragionare sulle scelte hardware per l’apprendimento automatico e sulle capacità richieste a ciascun livello di specializzazione.\nFigura 10.2 illustra la complessa interazione tra flessibilità, prestazioni, diversità funzionale e area di progettazione dell’architettura. Nota come l’ASIC si trovi nell’angolo in basso a destra, con area minima, flessibilità e consumo energetico e prestazioni massime, a causa della sua natura altamente specializzata per l’applicazione. Un compromesso chiave è la diversità funzionale rispetto alle prestazioni: le architetture per uso generico possono servire applicazioni diverse, ma le loro prestazioni applicative sono degradate rispetto alle architetture più personalizzate.\nLa progressione inizia con l’opzione più specializzata, gli ASIC appositamente progettati per l’intelligenza artificiale, per basare la nostra comprensione sulle massime ottimizzazioni possibili prima di espanderci ad architetture più generalizzabili. Questo approccio strutturato mira a chiarire lo spazio di progettazione dell’acceleratore.\n\n\n\n\n\n\nFigura 10.2: Compromessi di Progettazione. Fonte: El-Rayis (2014).\n\n\nEl-Rayis, A. O. 2014. «Reconfigurable architectures for the next generation of mobile device telecommunications systems». : https://www.researchgate.net/publication/292608967.\n\n\n\n10.3.1 Application-Specific Integrated Circuits (ASIC)\nUn “circuito integrato specifico per applicazione” (ASIC) è un tipo di circuito integrato (IC) progettato su misura per un’applicazione o un carico di lavoro specifico, anziché per un uso generico. A differenza di CPU e GPU, gli ASIC non supportano più applicazioni o carichi di lavoro. Piuttosto, sono ottimizzati per eseguire un singolo compito in modo estremamente efficiente. Google TPU è un esempio di ASIC.\nGli ASIC raggiungono questa efficienza adattando ogni aspetto del design del chip, ovvero le porte logiche sottostanti, i componenti elettronici, l’architettura, la memoria, l’I/O e il processo di produzione, specificamente per l’applicazione target. Questo livello di personalizzazione consente di rimuovere qualsiasi logica o funzionalità non necessaria richiesta per il calcolo generale. Il risultato è un IC che massimizza le prestazioni e l’efficienza energetica sul carico di lavoro desiderato. I guadagni di efficienza derivanti dall’hardware specifico per applicazione sono così sostanziali che queste aziende incentrate sul software dedicano enormi risorse ingegneristiche alla progettazione di ASIC personalizzati.\nL’ascesa di algoritmi di apprendimento automatico più complessi ha reso i vantaggi prestazionali abilitati dall’accelerazione hardware personalizzata un fattore di differenziazione competitiva chiave, anche per le aziende tradizionalmente concentrate sull’ingegneria del software. Gli ASIC sono diventati un investimento ad alta priorità per i principali provider cloud che mirano a offrire un calcolo AI più veloce.\n\nVantaggi\nGrazie alla loro natura personalizzata, gli ASIC offrono vantaggi significativi rispetto ai processori generici come CPU e GPU. I principali vantaggi includono quanto segue.\n\nPrestazioni ed efficienza massimizzate\nIl vantaggio più fondamentale degli ASIC è la massimizzazione delle prestazioni e dell’efficienza energetica personalizzando l’architettura hardware specificamente per l’applicazione target. Ogni transistor e aspetto della progettazione è ottimizzato per il carico di lavoro desiderato: non è necessaria alcuna logica o sovraccarico non necessario per supportare il calcolo generico.\nAd esempio, le Tensor Processing Units (TPU) di Google contengono architetture su misura esattamente per le operazioni di moltiplicazione di matrici utilizzate nelle reti neurali. Per progettare gli ASIC TPU, i team di ingegneria di Google devono definire chiaramente le specifiche del chip, scrivere la descrizione dell’architettura utilizzando linguaggi di descrizione hardware come Verilog, sintetizzare il design per mapparlo sui componenti hardware e posizionare e instradare con cura transistor e collegamenti in base alle regole di progettazione del processo di fabbricazione. Questo complesso processo di progettazione, noto come “very-large-scale integration” (VLSI) [integrazione su larga scala ], consente loro di creare un IC ottimizzato per carichi di lavoro di apprendimento automatico.\nDi conseguenza, gli ASIC TPU raggiungono un’efficienza di oltre un ordine di grandezza superiore nelle operazioni per watt rispetto alle GPU per uso generico sui carichi di lavoro di apprendimento automatico massimizzando le prestazioni e riducendo al minimo il consumo energetico tramite un design hardware full-stack personalizzato.\n\n\nMemoria On-Chip Specializzata\nGli ASIC incorporano SRAM on-chip e cache specificamente ottimizzate per fornire dati alle unità di elaborazione. Ad esempio, il system-on-a-chip M1 di Apple contiene una speciale SRAM a bassa latenza per accelerare le prestazioni del suo hardware di machine learning Neural Engine. Un’ampia memoria locale con elevata larghezza di banda consente di mantenere i dati vicino agli elementi di elaborazione. Questo fornisce enormi vantaggi in termini di velocità rispetto all’accesso DRAM off-chip, che può essere fino a 100 volte più lento.\nLa località dei dati e l’ottimizzazione della gerarchia di memoria sono fondamentali per un throughput elevato e un basso consumo energetico. Tabella 10.1 mostra “Numeri che Tutti Dovrebbero Conoscere”, di Jeff Dean.\n\n\n\nTabella 10.1: Confronto della latenza delle operazioni di elaborazione e di rete.\n\n\n\n\n\n\n\n\n\n\nOperazione\nLatenza\nNote\n\n\n\n\nRiferimento cache L1\n0,5 ns\n\n\n\nBranch mispredict\n5 ns\n\n\n\nRiferimento cache L2\n7 ns\n\n\n\nBlocco/sblocco mutex\n25 ns\n\n\n\nRiferimento memoria principale\n100 ns\n\n\n\nComprimere 1K byte con Zippy\n3.000 ns\n3 us\n\n\nInviaew 1 KB byte su una rete da 1 Gbps\n10.000 ns\n10 us\n\n\nLeggere 4 KB casualmente da SSD\n150.000 ns\n150 us\n\n\nLeggere 1 MB in sequenza dalla memoria\n250.000 ns\n250 us\n\n\nAndata e ritorno all’interno dello stesso data center\n500.000 ns\n0,5 ms\n\n\nLeggere 1 MB in sequenza da SSD\n1.000.000 ns\n1 ms\n\n\nRicerca su disco\n10.000.000 ns\n10 ms\n\n\nLeggere 1 MB in sequenza da disco\n20.000.000 ns\n20 ms\n\n\nInviare pacchetto CA → Paesi Bassi → CA\n150.000.000 ns\n150 ms\n\n\n\n\n\n\n\n\nTipi di Dati e Operazioni Personalizzati\nA differenza dei processori generici, gli ASIC possono essere progettati per supportare in modo nativo tipi di dati personalizzati come INT4 o bfloat16, ampiamente utilizzati nei modelli di ML. Ad esempio, l’architettura GPU Ampere di Nvidia ha un bfloat16 dedicato ai Tensor Core per accelerare i carichi di lavoro AI. I tipi di dati a bassa precisione consentono una maggiore densità aritmetica e prestazioni. Gli ASIC possono anche incorporare direttamente operazioni non standard negli algoritmi ML come operazioni primitive, ad esempio, il supporto nativo di funzioni di attivazione come ReLU rende l’esecuzione più efficiente. Per ulteriori dettagli, fare riferimento al capitolo Rappresentazioni Numeriche Efficienti.\n\n\nParallelismo Elevato\nLe architetture ASIC possono sfruttare un parallelismo più elevato ottimizzato per il carico di lavoro del target rispetto alle CPU o GPU generiche. Un maggior numero di unità di calcolo personalizzate per l’applicazione significa più operazioni eseguite simultaneamente. Gli ASIC altamente paralleli raggiungono un throughput enorme per carichi di lavoro paralleli di dati come l’inferenza di reti neurali.\n\n\nNodi di Processo Avanzati\nI processi di produzione all’avanguardia consentono di impacchettare più transistor in aree di die più piccole, aumentando la densità. Gli ASIC progettati specificamente per applicazioni ad alto volume possono ammortizzare meglio i costi dei nodi.\n\n\n\nSvantaggi\n\nTempistiche di Progettazione Lunghe\nIl processo di progettazione e validazione di un ASIC può richiedere 2-3 anni. La sintesi dell’architettura utilizzando linguaggi di descrizione hardware, la definizione del layout del chip e la fabbricazione del chip su nodi di processo avanzati comportano lunghi cicli di sviluppo. Ad esempio, per realizzare un chip da 7 nm, i team devono definire attentamente le specifiche, scrivere l’architettura in HDL, sintetizzare le porte logiche, posizionare i componenti, instradare tutte le interconnessioni e finalizzare il layout da inviare per la fabbricazione. Questa “Very Large-Scale Integration (VLSI)” significa che la progettazione e la produzione di ASIC possono tradizionalmente richiedere 2-5 anni.\nCi sono alcuni motivi chiave per cui le lunghe tempistiche di progettazione degli ASIC, spesso 2-3 anni, possono essere difficili per i carichi di lavoro di apprendimento automatico:\n\nGli algoritmi ML si evolvono rapidamente: Nuove architetture di modelli, tecniche di training e ottimizzazioni di rete emergono continuamente. Ad esempio, i Transformers sono diventati estremamente popolari nell’NLP negli ultimi anni. Quando un ASIC termina il tapeout, l’architettura ottimale per un carico di lavoro potrebbe essere cambiata.\nI dataset crescono rapidamente: Gli ASIC progettati per determinate dimensioni di modello o tipi di dati possono diventare sottodimensionati rispetto alla domanda. Ad esempio, i modelli di linguaggio naturale stanno aumentando esponenzialmente con più dati e parametri. Un chip progettato per BERT potrebbe non supportare GPT-3.\nLe applicazioni ML cambiano frequentemente: L’attenzione del settore cambia tra visione artificiale, parlato, NLP, sistemi di raccomandazione, ecc. Un ASIC ottimizzato per la classificazione delle immagini potrebbe avere meno rilevanza in pochi anni.\nCicli di progettazione più rapidi con GPU/FPGA: Gli acceleratori programmabili come le GPU possono adattarsi molto più rapidamente aggiornando le librerie software e i framework. I nuovi algoritmi possono essere implementati senza modifiche hardware.\nEsigenze di time-to-market: Ottenere un vantaggio competitivo in ML richiede di sperimentare e implementare rapidamente nuove idee. Attendere diversi anni per un ASIC è diverso da un’iterazione rapida.\n\nIl ritmo dell’innovazione in ML deve essere adattato meglio alla scala temporale pluriennale per lo sviluppo di ASIC. Sono necessari notevoli sforzi ingegneristici per estendere la durata di vita di ASIC tramite architetture modulari, ridimensionamento dei processi, compressione dei modelli e altre tecniche. Tuttavia, la rapida evoluzione di ML rende l’hardware a funzione fissa una sfida.\n\n\nElevati Costi di Progettazione Non Ricorrenti\nI costi fissi per portare un ASIC dalla progettazione alla produzione ad alto volume possono essere molto dispendiosi in termini di capitale, spesso decine di milioni di dollari. La fabbricazione di fotomaschere per il tape-out dei chip in nodi di processo avanzati, il packaging e il lavoro di progettazione una tantum sono costosi. Ad esempio, un solo tape-out del chip da 7 nm potrebbe costare milioni. L’elevato “non-recurring engineering (NRE)” [investimento di progettazione non ricorrente] riduce la fattibilità dell’ASIC ai casi di utilizzo della produzione ad alto volume in cui il costo iniziale può essere ammortizzato.\n\n\nIntegrazione e Programmazione Complesse\nGli ASIC richiedono un ampio lavoro di integrazione software, inclusi driver, compilatori, supporto del sistema operativo e strumenti di debug. Hanno anche bisogno di competenza nel packaging elettrico e termico. Inoltre, programmare in modo efficiente le architetture ASIC può comportare sfide come il partizionamento del carico di lavoro e la pianificazione su molte unità parallele. La natura personalizzata richiede notevoli sforzi di integrazione per trasformare l’hardware grezzo in acceleratori completamente operativi.\nMentre gli ASIC forniscono enormi guadagni di efficienza nelle applicazioni target adattando ogni aspetto della progettazione hardware a un’attività specifica, la loro natura fissa comporta compromessi in termini di flessibilità e costi di sviluppo rispetto agli acceleratori programmabili, che devono essere soppesati in base all’applicazione.\n\n\n\n\n10.3.2 Field-Programmable Gate Array (FPGA)\nGli FPGA sono circuiti integrati programmabili che possono essere riconfigurati per diverse applicazioni. La loro natura personalizzabile offre vantaggi per accelerare gli algoritmi AI rispetto agli ASIC fissi o alle GPU inflessibili. Mentre Google, Meta e NVIDIA stanno valutando di installare gli ASIC nei data center, Microsoft ha distribuito gli FPGA nei suoi data center (Putnam et al. 2014) nel 2011 per servire in modo efficiente diversi carichi di lavoro.\n\nVantaggi\nGli FPGA offrono diversi vantaggi rispetto alle GPU e agli ASIC per accelerare i carichi di lavoro di apprendimento automatico.\n\nFlessibilità Tramite “Reconfigurable Fabric”\nIl vantaggio principale degli FPGA è la capacità di riconfigurare il “fabric” [tessuto] sottostante per implementare architetture personalizzate ottimizzate per diversi modelli, a differenza degli ASIC a funzione fissa. Ad esempio, le società di trading quantitativo utilizzano gli FPGA per accelerare i loro algoritmi perché cambiano frequentemente e il basso costo NRE degli FPGA è più fattibile rispetto acquistare i nuovi ASIC. Figura 10.3 contiene una tabella che confronta tre diversi FPGA.\n\n\n\n\n\n\nFigura 10.3: Confronto di FPGA. Fonte: Gwennap (s.d.).\n\n\nGwennap, Linley. s.d. «Certus-NX Innovates General-Purpose FPGAs».\n\n\nGli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono fornite una quantità base di queste risorse e gli ingegneri programmano i chip compilando il codice HDL in flussi di bit che riorganizzano la struttura in diverse configurazioni. Questo rende gli FPGA adattabili man mano che gli algoritmi evolvono.\nSebbene gli FPGA possano non raggiungere le massime prestazioni ed efficienza degli ASIC specifici per il carico di lavoro, la loro programmabilità offre maggiore flessibilità man mano che gli algoritmi cambiano. Questa adattabilità rende gli FPGA una scelta interessante per accelerare le applicazioni di machine learning in evoluzione. Microsoft ha distribuito gli FPGA nei suoi data center Azure per carichi di lavoro di machine learning per servire applicazioni diverse al posto degli ASIC. La programmabilità consente l’ottimizzazione attraverso modelli ML in continua evoluzione.\n\n\nParallelismo e Pipeline Personalizzati\nLe architetture FPGA possono sfruttare il parallelismo spaziale e il pipelining adattando la progettazione hardware per rispecchiare il parallelismo nei modelli ML. Ad esempio, la piattaforma FPGA HARPv2 di Intel suddivide i layer di una rete convoluzionale MNIST su elementi di elaborazione separati per massimizzare la produttività. Sugli FPGA sono possibili anche modelli paralleli unici come le valutazioni di “ensemble” ad albero. Pipeline profonde con buffering e flusso di dati ottimizzati possono essere personalizzate in base alla struttura e ai tipi di dati di ogni modello. Questo livello di parallelismo e pipeline su misura non è fattibile sulle GPU.\n\n\nMemoria On-Chip a Bassa Latenza\nGrandi quantità di memoria on-chip ad alta larghezza di banda consentono l’archiviazione localizzata per pesi e attivazioni. Ad esempio, gli FPGA Xilinx Versal contengono 32 MB di blocchi RAM a bassa latenza e interfacce DDR4 a doppio canale per la memoria esterna. Avvicinare fisicamente la memoria alle unità di elaborazione riduce la latenza di accesso. Ciò fornisce significativi vantaggi di velocità rispetto alle GPU che attraversano PCIe (Peripheral Component Interconnect Express) o altri bus di sistema per raggiungere la memoria GDDR6 off-chip.\n\n\nSupporto Nativo per Bassa Precisione\nUn vantaggio fondamentale degli FPGA è la capacità di implementare in modo nativo qualsiasi larghezza di bit per unità aritmetiche, come INT4 o bfloat16, utilizzate nei modelli ML quantizzati. Ad esempio, gli FPGA Stratix 10 NX di Intel hanno core INT8 dedicati che possono raggiungere fino a 143 INT8 TOPS a ~1 TOPS/W Intel Stratix 10 NX FPGA. Le larghezze di bit inferiori aumentano la densità aritmetica e le prestazioni. Gli FPGA possono persino supportare la sintonizzazione a precisione mista o dinamica in fase di esecuzione.\n\n\n\nSvantaggi\n\nThroughput di Picco Inferiore Rispetto agli ASIC\nGli FPGA non possono eguagliare i numeri di throughput grezzi degli ASIC, personalizzati per un modello e una precisione specifici. I sovraccarichi del “fabric” riconfigurabile rispetto all’hardware a funzione fissa comportano prestazioni di picco inferiori. Ad esempio, i pod TPU v5e consentono di collegare fino a 256 chip con oltre 100 petaOps di prestazioni INT8, mentre gli FPGA possono offrire fino a 143 INT8 TOPS o 286 INT4 TOPS Intel Stratix 10 NX FPGA.\nQuesto perché gli FPGA comprendono blocchi di base: blocchi logici configurabili, blocchi RAM e interconnessioni. Vengono forniti con una quantità stabilita di queste risorse. Per programmare gli FPGA, gli ingegneri scrivono codice HDL e lo compilano in flussi di bit che riorganizzano il “fabric”, che ha sovraccarichi intrinseci rispetto a un ASIC appositamente progettato per un calcolo.\n\n\nComplessità di Programmazione\nPer ottimizzare le prestazioni FPGA, gli ingegneri devono programmare le architetture in linguaggi di descrizione hardware di basso livello come Verilog o VHDL. Ciò richiede competenza nella progettazione hardware e cicli di sviluppo più lunghi rispetto a framework software di livello superiore come TensorFlow. Massimizzare l’utilizzo può essere difficile nonostante i progressi nella sintesi di alto livello da C/C++.\n\n\nSovraccarichi di Riconfigurazione\nLa modifica delle configurazioni FPGA richiede il ricaricamento di un nuovo flusso di bit, che ha costi di latenza e dimensioni di archiviazione considerevoli. Ad esempio, la riconfigurazione parziale su FPGA Xilinx può richiedere centinaia di millisecondi. Questo rende impossibile lo scambio dinamico di architetture in tempo reale. L’archiviazione del flusso di bit consuma anche memoria on-chip.\n\n\nGuadagni in diminuzione sui nodi avanzati\nSebbene i nodi di processo più piccoli siano molto vantaggiosi per gli ASIC, offrono meno vantaggi per gli FPGA. A 7 nm e al di sotto, effetti come variazione di processo, vincoli termici e invecchiamento hanno un impatto sproporzionato sulle prestazioni degli FPGA. Anche le spese generali della struttura configurabile riducono i guadagni rispetto agli ASIC a funzione fissa.\n\n\nCaso di Studio\nGli FPGA hanno trovato ampia applicazione in vari campi, tra cui l’imaging medico, la robotica e la finanza, dove eccellono nella gestione di attività di machine learning ad alta intensità di calcolo. Nell’imaging medico, un esempio illustrativo è l’applicazione degli FPGA per la segmentazione dei tumori cerebrali, un processo tradizionalmente dispendioso in termini di tempo e soggetto a errori. Ad esempio, Xiong et al. hanno sviluppato un acceleratore di segmentazione quantizzato, che hanno riaddestrato utilizzando i set di dati BraTS19 e BraTS20. Il loro lavoro ha prodotto risultati notevoli, ottenendo miglioramenti delle prestazioni di oltre 5x e 44x e guadagni di efficienza energetica di 11x e 82x rispetto alle implementazioni GPU e CPU, rispettivamente (Xiong et al. 2021).\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei Cao, Xuegong Zhou, et al. 2021. «MRI-based brain tumor segmentation using FPGA-accelerated neural network». BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\n\n\n\n\n10.3.3 Digital Signal Processor (DSP)\nIl primo core di elaborazione del segnale digitale è stato costruito nel 1948 da Texas Instruments (The Evolution of Audio DSPs). Tradizionalmente, i DSP avrebbero avuto una logica per accedere direttamente ai dati digitali/audio nella memoria, eseguire un’operazione aritmetica (moltiplica-addiziona-accumula-MAC era una delle operazioni più comuni) e quindi scrivere il risultato nella memoria. Il DSP avrebbe incluso componenti analogici specializzati per recuperare i dati digitali/audio.\nUna volta entrati nell’era degli smartphone, i DSP hanno iniziato a comprendere attività più sofisticate. Richiedevano Bluetooth, Wi-Fi e connettività cellulare. Anche i media sono diventati molto più complessi. Oggi, è raro avere chip interi dedicati solo al DSP, ma un System on Chip includerebbe DSP e CPU per uso generico. Ad esempio, l’Hexagon Digital Signal Processor di Qualcomm afferma di essere un “processore di livello mondiale con funzionalità sia CPU che DSP per supportare le esigenze di elaborazione profondamente integrate della piattaforma mobile per funzioni sia multimediali che modem”. Google Tensors, il chip nei telefoni Google Pixel, include anche CPU e motori DSP specializzati.\n\nVantaggi\nI DSP offrono vantaggi architettonici in termini di throughput della matematica vettoriale, accesso alla memoria a bassa latenza, efficienza energetica e supporto per diversi tipi di dati, rendendoli adatti all’accelerazione ML embedded.\n\nArchitettura Ottimizzata per la Matematica Vettoriale\nI DSP contengono percorsi dati specializzati, file di registro e istruzioni ottimizzati specificamente per le operazioni di matematica vettoriale comunemente utilizzate nei modelli di apprendimento automatico. Ciò include motori di prodotto scalare, unità MAC e funzionalità SIMD su misura per calcoli vettoriali/matriciali. Ad esempio, il DSP CEVA-XM6 (“Ceva SensPro fonde AI e Vector DSP”) ha unità vettoriali a 512 bit per accelerare le convoluzioni. Questa efficienza sui carichi di lavoro di matematica vettoriale va ben oltre le CPU generiche.\n\n\nMemoria On-Chip a Bassa Latenza\nI DSP integrano grandi quantità di memoria SRAM veloce su chip per conservare i dati localmente per l’elaborazione. Avvicinare fisicamente la memoria alle unità di calcolo riduce la latenza di accesso. Ad esempio, il DSP SHARC+ di Analog contiene 10 MB di SRAM su chip. Questa memoria locale ad alta larghezza di banda offre vantaggi di velocità per le applicazioni in tempo reale.\n\n\nEfficienza Energetica\nI DSP sono progettati per fornire elevate prestazioni per watt su carichi di lavoro di segnali digitali. Percorsi dati efficienti, parallelismo e architetture di memoria consentono trilioni di operazioni matematiche al secondo entro budget di potenza mobili ridotti. Ad esempio, il DSP Hexagon di Qualcomm può fornire 4 trilioni di operazioni al secondo (TOPS) consumando watt minimi.\n\n\nSupporto per Matematica a Virgola Mobile e Intera\nA differenza delle GPU che eccellono in precisione singola o dimezzata, i DSP possono supportare nativamente tipi di dati a virgola mobile e intera a 8/16 bit utilizzati nei modelli ML. Alcuni DSP supportano l’accelerazione del prodotto scalare a precisione INT8 per reti neurali quantizzate.\n\n\n\nSvantaggi\nI DSP fanno compromessi architettonici che limitano il throughput di picco, la precisione e la capacità del modello rispetto ad altri acceleratori AI. Tuttavia, i loro vantaggi in termini di efficienza energetica e matematica intera li rendono una valida opzione di edge computing. Quindi, mentre i DSP offrono alcuni vantaggi rispetto alle CPU, presentano anche delle limitazioni per i carichi di lavoro di apprendimento automatico:\n\nThroughput di Picco Inferiore Rispetto ad ASIC/GPU\nI DSP non possono eguagliare il throughput computazionale grezzo delle GPU o degli ASIC personalizzati progettati specificamente per l’apprendimento automatico. Ad esempio, l’ASIC Cloud AI 100 di Qualcomm fornisce 480 TOPS su INT8, mentre il loro DSP Hexagon fornisce 10 TOPS. I DSP non hanno il massiccio parallelismo delle unità GPU SM.\n\n\nPrestazioni a Doppia Precisione più Lente\nLa maggior parte dei DSP deve essere ottimizzata per la virgola mobile di precisione più elevata necessaria in alcuni modelli ML. I loro motori di prodotto scalare si concentrano su INT8/16 e FP32, che forniscono una migliore efficienza energetica. Tuttavia, la produttività in virgola mobile a 64 bit è molto più bassa, il che può limitare l’utilizzo nei modelli che richiedono un’elevata precisione.\n\n\nCapacità del Modello Limitata\nLa limitata memoria on-chip dei DSP limita le dimensioni del modello che possono eseguire. Grandi modelli di deep learning con centinaia di megabyte di parametri supererebbero la capacità delle SRAM on-chip. I DSP sono più adatti per modelli di piccole e medie dimensioni destinati a dispositivi edge.\n\n\nComplessità di Programmazione\nLa programmazione efficiente delle architetture DSP richiede competenza nella programmazione parallela e nell’ottimizzazione dei modelli di accesso ai dati. Le loro microarchitetture specializzate hanno una curva di apprendimento più ripida rispetto ai framework software di alto livello, rendendo lo sviluppo più complesso.\n\n\n\n\n10.3.4 Graphics Processing Unit (GPU)\nIl termine “graphics processing unit” [unità di elaborazione grafica] esiste almeno dagli anni ’80. C’è sempre stata una richiesta di hardware grafico nelle console per videogiochi (elevata richiesta, doveva avere un costo relativamente basso) e nelle simulazioni scientifiche (richiesta inferiore, ma risoluzione più alta, poteva avere un prezzo elevato).\nIl termine è stato reso popolare, tuttavia, nel 1999 quando NVIDIA ha lanciato la GeForce 256, mirando principalmente al settore di mercato dei giochi per PC (Lindholm et al. 2008). Man mano che i giochi per PC diventavano più sofisticati, le GPU NVIDIA diventavano più programmabili. Presto, gli utenti si resero conto che potevano sfruttare questa programmabilità, eseguire vari carichi di lavoro non correlati alla grafica sulle GPU e trarre vantaggio dall’architettura sottostante. E così, alla fine degli anni 2000, le GPU divennero unità di elaborazione grafica per uso generale o GP-GPU.\n\nLindholm, Erik, John Nickolls, Stuart Oberman, e John Montrym. 2008. «NVIDIA Tesla: A Unified Graphics and Computing Architecture». IEEE Micro 28 (2): 39–55. https://doi.org/10.1109/mm.2008.31.\nAnche Intel Arc Graphics e AMD Radeon RX hanno sviluppato le loro GPU nel tempo.\n\nVantaggi\n\nElevata Capacità di Elaborazione\nIl vantaggio principale delle GPU è la loro capacità di eseguire calcoli in virgola mobile paralleli massivi ottimizzati per la computer grafica e l’algebra lineare (Raina, Madhavan, e Ng 2009). Le GPU moderne come la A100 di Nvidia offrono fino a 19,5 teraflop di prestazioni FP32 con 6912 core CUDA e 40 GB di memoria grafica strettamente accoppiati a 1,6 TB/s di larghezza di banda della memoria grafica.\n\nRaina, Rajat, Anand Madhavan, e Andrew Y. Ng. 2009. «Large-scale deep unsupervised learning using graphics processors». In Proceedings of the 26th Annual International Conference on Machine Learning, a cura di Andrea Pohoreckyj Danyluk, Léon Bottou, e Michael L. Littman, 382:873–80. ACM International Conference Proceeding Series. ACM. https://doi.org/10.1145/1553374.1553486.\nQuesta capacità di elaborazione grezza deriva dall’architettura “Streaming Multiprocessor” (SM) altamente parallela, pensata per carichi di lavoro paralleli ai dati (Zhihao Jia, Zaharia, e Aiken 2019). Ogni SM contiene centinaia di core scalari ottimizzati per la matematica float32/64. Con migliaia di SM su un chip, le GPU sono appositamente progettate per la moltiplicazione di matrici e le operazioni vettoriali utilizzate in tutte le reti neurali.\nAd esempio, l’ultima GPU H100 di Nvidia fornisce 4000 TFLOP di FP8, 2000 TFLOP di FP16, 1000 TFLOP di TF32, 67 TFLOP di FP32 e 34 TFLOP di FP64 come prestazioni di elaborazione, che possono accelerare notevolmente l’addestramento di grandi batch su modelli come BERT, GPT-3 e altre architetture di trasformatori. Il parallelismo scalabile delle GPU è fondamentale per accelerare il deep learning computazionalmente intensivo.\n\n\nEcosistema Software Maturo\nNvidia fornisce ampie librerie di runtime come cuDNN e cuBLAS che sono altamente ottimizzate per primitive di deep learning. Framework come TensorFlow e PyTorch si integrano con queste librerie per abilitare l’accelerazione GPU senza programmazione diretta. CUDA fornisce un controllo di livello inferiore per calcoli personalizzati.\nQuesto ecosistema consente di sfruttare rapidamente le GPU ad alto livello tramite Python senza competenze di programmazione GPU. Flussi di lavoro e astrazioni noti forniscono una comoda rampa di accesso per scalare gli esperimenti di deep learning. La maturità del software integra i vantaggi della produttività.\n\n\nAmpia Disponibilità\nLe economie di scala dell’elaborazione grafica rendono le GPU ampiamente accessibili nei data center, nelle piattaforme cloud come AWS e GCP e nelle workstation desktop. La loro disponibilità negli ambienti di ricerca ha fornito una comoda piattaforma di sperimentazione e innovazione nel ML. Ad esempio, quasi tutti i risultati di deep learning all’avanguardia hanno coinvolto l’accelerazione GPU per merito di questa ubiquità. L’ampio accesso integra la maturità del software per rendere le GPU l’acceleratore ML standard.\n\n\nArchitettura Programmabile\nSebbene non siano flessibili come gli FPGA, le GPU offrono programmabilità tramite linguaggi CUDA e shader per personalizzare i calcoli. Gli sviluppatori possono ottimizzare i modelli di accesso ai dati, creare nuove operazioni e regolare le precisioni per modelli e algoritmi in evoluzione.\n\n\n\nSvantaggi\nSebbene le GPU siano diventate l’acceleratore standard per il deep learning, la loro architettura presenta alcuni svantaggi importanti.\n\nMeno Efficienti degli ASIC Custom\nL’affermazione “Le GPU sono meno efficienti degli ASIC” potrebbe scatenare un acceso dibattito nel campo ML/AI e far esplodere questo libro.\nIn genere, le GPU sono percepite come meno efficienti degli ASIC perché questi ultimi sono realizzati su misura per attività specifiche e quindi possono funzionare in modo più efficiente nativamente. Con la loro architettura generica, le GPU sono intrinsecamente più versatili e programmabili, soddisfacendo un ampio spettro di attività computazionali oltre a ML/AI.\nTuttavia, le GPU moderne si sono evolute per includere un supporto hardware specializzato per operazioni AI essenziali, come la moltiplicazione di matrici generalizzata (GEMM) e altre operazioni di matrice, supporto nativo per la quantizzazione e supporto nativo per la potatura, che sono fondamentali per l’esecuzione efficace dei modelli ML. Questi miglioramenti hanno notevolmente migliorato l’efficienza delle GPU per le attività AI al punto che possono rivaleggiare con le prestazioni degli ASIC per determinate applicazioni.\nDi conseguenza, le GPU contemporanee sono convergenti, incorporando capacità specializzate simili ad ASIC all’interno di un framework di elaborazione flessibile e di uso generale. Questa adattabilità ha offuscato i confini tra i due tipi di hardware. Le GPU offrono un forte equilibrio tra specializzazione e programmabilità che si adatta bene alle esigenze dinamiche della ricerca e sviluppo ML/AI.\n\n\nElevate Esigenze di Larghezza di Banda di Memoria\nL’architettura massivamente parallela richiede un’enorme larghezza di banda di memoria per fornire migliaia di core, come mostrato nella Figura 1. Ad esempio, la GPU Nvidia A100 richiede 1.6 TB/sec per saturare completamente il suo computer. Le GPU si affidano ad ampi bus di memoria a 384 bit per RAM GDDR6 ad alta larghezza di banda, ma anche la GDDR6 più veloce raggiunge il massimo a circa 1 TB/sec. Questa dipendenza dalla DRAM esterna comporta latenza e sovraccarico di potenza.\n\n\nComplessità di Programmazione\nSebbene strumenti come CUDA siano utili, la mappatura e il partizionamento ottimali dei carichi di lavoro ML nell’architettura GPU massivamente parallela rimangono una sfida, il raggiungimento di un utilizzo elevato e della località della memoria richiede una messa a punto di basso livello (Zhe Jia et al. 2018). Astrazioni come TensorFlow possono tralasciare le prestazioni.\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, e Daniele P. Scarpazza. 2018. «Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking». ArXiv preprint. https://arxiv.org/abs/1804.06826.\n\n\nMemoria On-Chip Limitata\nLe GPU hanno cache di memoria on-chip relativamente piccole rispetto ai grandi requisiti di working set dei modelli ML durante l’addestramento. Si basano su un accesso ad alta larghezza di banda alla DRAM esterna, che gli ASIC riducono al minimo con una grande SRAM on-chip.\n\n\nArchitettura Fissa\nA differenza degli FPGA, l’architettura fondamentale della GPU non può essere modificata dopo la produzione. Questo vincolo limita l’adattamento a nuovi carichi di lavoro o layer ML. Il confine CPU-GPU crea anche overhead di spostamento dei dati.\n\n\n\nCaso di Studio\nLa recente ricerca rivoluzionaria condotta da OpenAI (Brown et al. 2020) con il loro modello GPT-3. GPT-3, un modello linguistico con 175 miliardi di parametri, ha dimostrato capacità di comprensione e generazione linguistica senza precedenti. Il suo addestramento, che avrebbe richiesto mesi su CPU convenzionali, è stato completato in pochi giorni utilizzando potenti GPU, ampliando così i confini delle capacità di elaborazione del linguaggio naturale (NLP).\n\n\n\n10.3.5 Central Processing Unit (CPU)\nIl termine CPU ha una lunga storia che risale al 1955 (Weik 1955) mentre la prima CPU a microprocessore, l’Intel 4004, è stata inventata nel 1971 (Chi ha inventato il microprocessore?). I compilatori traducono linguaggi di programmazione di alto livello come Python, Java o C per assemblare istruzioni (x86, ARM, RISC-V, ecc.) che le CPU devono elaborare. Il set di istruzioni che una CPU comprende è chiamato “instruction set architecture” (ISA), che definisce i comandi che il processore può eseguire direttamente. Deve essere concordato sia dall’hardware che dal software in esecuzione su di esso (vedere la sezione 5 per una descrizione più approfondita delle architetture del set di istruzioni-ISA).\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital Computing Systems. Ballistic Research Laboratories.\nUna panoramica degli sviluppi significativi nelle CPU:\n\nEra del Single-core (anni ’50-2000): Questa era è nota per i miglioramenti microarchitettonici aggressivi. Tecniche come l’esecuzione speculativa (esecuzione di un’istruzione prima che quella precedente fosse finita), “out-of-order execution” [esecuzione fuori ordine] (riordinamento delle istruzioni per renderle più efficaci) e “wider issue widths” [larghezze di emissione più ampie] (esecuzione di più istruzioni contemporaneamente) sono state implementate per aumentare la produttività delle istruzioni. Anche il termine “System on Chip” ha avuto origine in questa era, poiché diversi componenti analogici (componenti progettati con transistor) e componenti digitali (componenti progettati con linguaggi di descrizione hardware mappati su transistor) sono stati inseriti sulla stessa piattaforma per realizzare un’attività.\nEra Multicore (anni 2000): Guidata dalla diminuzione della legge di Moore, questa è caratterizzata dall’aumento del numero di core all’interno di una CPU. Ora, le attività possono essere suddivise su più core diversi, ognuno con il proprio percorso dati e unità di controllo. Molti dei problemi di quest’epoca riguardavano come condividere determinate risorse, quali risorse condividere e come mantenere coerenza e consistenza in tutti i core.\nUn Mare di acceleratori (anni 2010): Ancora una volta, spinta dalla diminuzione della legge di Moore, quest’epoca è caratterizzata dal delegare le attività più complicate su acceleratori (widget) collegati al datapath principale nelle CPU. È comune vedere acceleratori dedicati a vari carichi di lavoro di intelligenza artificiale, nonché elaborazione di immagini/digitali e crittografia. In queste progettazioni, le CPU sono spesso descritte più come giudici, che decidono quali attività devono essere elaborate piuttosto che eseguire l’elaborazione stessa. Qualsiasi attività potrebbe comunque essere eseguita sulla CPU anziché sugli acceleratori, ma la CPU sarebbe generalmente più lenta. Tuttavia, il costo di progettazione e programmazione dell’acceleratore è diventato un ostacolo non banale che ha suscitato interesse per le librerie specifiche per la progettazione (DSL).\nPresenza nei data center: Sebbene sentiamo spesso dire che le GPU dominano il mercato dei data center, le CPU sono comunque adatte per attività che non possiedono intrinsecamente un elevato grado di parallelismo. Le CPU spesso gestiscono attività seriali e di piccole dimensioni e coordinano il data center.\nSull’edge: Dati i vincoli più rigidi sulle risorse sull’edge, le CPU edge spesso implementano solo un sottoinsieme delle tecniche sviluppate nell’era single-core perché queste ottimizzazioni tendono a essere pesanti in termini di consumo di energia e area. Le CPU edge mantengono comunque un datapath relativamente semplice con capacità di memoria limitate.\n\nTradizionalmente, le CPU sono state sinonimo di elaborazione generica, un termine che è cambiato anche perché il carico di lavoro “medio” che un consumatore esegue cambia nel tempo. Ad esempio, i componenti in virgola mobile erano un tempo considerati riservati alla “elaborazione scientifica”, di solito venivano implementati come un coprocessore (un componente modulare che funzionava con il datapath) e raramente distribuiti ai consumatori medi. Confrontate questo atteggiamento con quello odierno, in cui le FPU sono integrate in ogni datapath.\n\nVantaggi\nSebbene la produttività in sé sia limitata, le CPU per uso generico offrono vantaggi pratici di accelerazione AI.\n\nProgrammabilità Generale\nLe CPU supportano carichi di lavoro diversi oltre al ML, offrendo una programmabilità flessibile per uso generico. Questa versatilità deriva dai loro set di istruzioni standardizzati e dagli ecosistemi di compilatori maturi, che consentono di eseguire qualsiasi applicazione, dai database e server Web alle pipeline analitiche (Hennessy e Patterson 2019).\n\nHennessy, John L., e David A. Patterson. 2019. «A new golden age for computer architecture». Commun. ACM 62 (2): 48–60. https://doi.org/10.1145/3282307.\nQuesto evita la necessità di acceleratori ML dedicati e consente di sfruttare l’infrastruttura basata su CPU esistenti per la distribuzione ML di base. Ad esempio, i server X86 di fornitori come Intel e AMD possono eseguire framework ML comuni utilizzando pacchetti Python e TensorFlow insieme ad altri carichi di lavoro aziendali.\n\n\nEcosistema Software Maturo\nPer decenni, librerie matematiche altamente ottimizzate come BLAS, LAPACK e FFTW hanno sfruttato istruzioni vettorializzate e multithreading su CPU (Dongarra 2009). I principali framework ML come PyTorch, TensorFlow e SciKit-Learn sono progettati per integrarsi perfettamente con questi kernel matematici di CPU.\n\nDongarra, Jack J. 2009. «The evolution of high performance computing on system z». IBM J. Res. Dev. 53: 3–4.\nI fornitori di hardware come Intel e AMD forniscono anche librerie di basso livello per ottimizzare completamente le prestazioni per primitive di deep learning (accelerazione dell’inferenza AI su CPU). Questo ecosistema software robusto e maturo consente di distribuire rapidamente ML su infrastrutture di CPU esistenti.\n\n\nAmpia Disponibilità\nLe economie di scala della produzione di CPU, guidate dalla domanda in molti mercati come PC, server e dispositivi mobili, le rendono disponibili ovunque. Le CPU Intel, ad esempio, hanno alimentato la maggior parte dei server per decenni (Ranganathan 2011). Questa ampia disponibilità nei data center riduce i costi hardware per l’implementazione di ML di base.\n\nRanganathan, Parthasarathy. 2011. «From Microprocessors to Nanostores: Rethinking Data-Centric Systems». Computer 44 (1): 39–48. https://doi.org/10.1109/mc.2011.18.\nAnche i piccoli dispositivi embedded in genere integrano una certa CPU, consentendo l’inferenza edge. L’ubiquità riduce la necessità di acquistare acceleratori ML specializzati in molte situazioni.\n\n\nBasso Consumo per L’inferenza\nOttimizzazioni come ARM Neon e le estensioni vettoriali Intel AVX forniscono un throughput di numeri interi e in virgola mobile a basso consumo ottimizzato per carichi di lavoro “a raffica” come l’inferenza (Ignatov et al. 2018). Sebbene più lenta delle GPU, l’inferenza CPU può essere implementata in ambienti con vincoli energetici. Ad esempio, le CPU Cortex-M di ARM ora offrono oltre 1 TOPS di prestazioni INT8 sotto 1 W, consentendo l’individuazione di parole chiave e applicazioni di visione su dispositivi edge (ARM).\n\n\n\nSvantaggi\nPur offrendo alcuni vantaggi, le CPU per uso generico presentano anche delle limitazioni per i carichi di lavoro AI.\n\nThroughput Inferiore Rispetto agli Acceleratori\nLe CPU non dispongono delle architetture specializzate per l’elaborazione parallela massiva che GPU e altri acceleratori forniscono. Il loro design per uso generico riduce il throughput computazionale per le operazioni matematiche altamente parallelizzabili comuni nei modelli ML (N. P. Jouppi et al. 2017a).\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nNon Ottimizzato per il Parallelismo dei Dati\nLe architetture delle CPU non sono specificamente ottimizzate per i carichi di lavoro paralleli dei dati inerenti all’AI (Sze et al. 2017). Assegnano un’area di silicio sostanziale alla decodifica delle istruzioni, all’esecuzione speculativa, alla memorizzazione nella cache e al controllo del flusso che fornisce pochi vantaggi per le operazioni su array utilizzate nelle reti neurali (accelerazione dell’inferenza AI sulle CPU). Tuttavia, le CPU moderne sono dotate di istruzioni vettoriali come AVX-512 specificamente per accelerare determinate operazioni chiave come la moltiplicazione matriciale.\nI multiprocessori di streaming GPU, ad esempio, dedicano la maggior parte dei transistor alle unità a virgola mobile anziché alla logica di predizione di diramazione complessa. Questa specializzazione consente un utilizzo molto più elevato per la matematica ML.\n\n\nMaggiore Latenza della Memoria\nLe CPU soffrono di una latenza maggiore nell’accesso alla memoria principale rispetto alle GPU e ad altri acceleratori (DDR). Tecniche come il tiling e il caching possono aiutare, ma la separazione fisica dalla RAM off-chip crea colli di bottiglia nei carichi di lavoro ML ad alta intensità di dati. Ciò sottolinea la necessità di architetture di memoria specializzate nell’hardware ML.\n\n\nInefficienza Energetica in Caso di Carichi di Lavoro Pesanti\nSebbene sia adatto per l’inferenza intermittente, il mantenimento di una produttività quasi di picco per l’addestramento comporta un consumo energetico inefficiente sulle CPU, in particolare sulle CPU mobili (Ignatov et al. 2018). Gli acceleratori ottimizzano esplicitamente il flusso di dati, la memoria e il calcolo per carichi di lavoro ML sostenuti. Le CPU sono inefficienti dal punto di vista energetico per l’addestramento di modelli di grandi dimensioni.\n\n\n\n\n10.3.6 Confronto\nTabella 10.2 confronta i diversi tipi di funzionalità hardware.\n\n\n\nTabella 10.2: Confronto di diversi acceleratori hardware per carichi di lavoro AI.\n\n\n\n\n\n\n\n\n\n\n\nAcceleratore\nDescrizione\nPrincipali vantaggi\nPrincipali svantaggi\n\n\n\n\nASIC\nIC personalizzati progettati per carichi di lavoro target come l’inferenza AI\n\nMassimizza le prestazioni/watt.\nOttimizzato per le operazioni tensoriali\nMemoria on-chip a bassa latenza\n\n\nL’architettura fissa manca di flessibilità\nElevato costo NRE\nLunghi cicli di progettazione\n\n\n\nFPGA\nFabric riconfigurabile con logica programmabile e routing\n\nArchitettura flessibile\nAccesso alla memoria a bassa latenza\n\n\nPrestazioni/watt inferiori rispetto agli ASIC\nProgrammazione complessa\n\n\n\nGPU\nOriginariamente per la grafica, ora utilizzate per l’accelerazione della rete neurale\n\nElevata produttività\nScalabilità parallela\nEcosistema software con CUDA\n\n\nNon efficienti dal punto di vista energetico come gli ASIC\nRichiede un’elevata larghezza di banda della memoria\n\n\n\nCPU\nProcessori per uso generico\n\nProgrammabilità\nDisponibilità ubiqua\n\n\nPrestazioni inferiori per carichi di lavoro AI\n\n\n\n\n\n\n\nIn generale, le CPU forniscono una baseline prontamente disponibile, le GPU offrono un’accelerazione ampiamente accessibile, gli FPGA offrono programmabilità e gli ASIC massimizzano l’efficienza per funzioni fisse. La scelta ottimale dipende dalla scala, dal costo, dalla flessibilità e da altri requisiti dell’applicazione target.\nSebbene inizialmente sviluppati per l’implementazione del data center, Google ha anche profuso notevoli sforzi nello sviluppo di TPU Edge. Questi TPU Edge mantengono l’ispirazione degli array sistolici [https://it.wikipedia.org/wiki/Array_sistolico], ma sono adattati alle risorse limitate accessibili all’edge.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#co-progettazione-hardware-software",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#co-progettazione-hardware-software",
    "title": "10  Accelerazione IA",
    "section": "10.4 Co-Progettazione Hardware-Software",
    "text": "10.4 Co-Progettazione Hardware-Software\nLa co-progettazione hardware-software si basa sul principio secondo cui i sistemi AI raggiungono prestazioni ed efficienza ottimali quando i componenti hardware e software sono progettati in stretta integrazione. Ciò comporta un ciclo di progettazione iterativo e collaborativo in cui l’architettura hardware e gli algoritmi software vengono sviluppati e perfezionati contemporaneamente con un feedback continuo tra i team.\nAd esempio, un nuovo modello di rete neurale può essere prototipato su una piattaforma di accelerazione basata su FPGA per ottenere dati sulle prestazioni reali all’inizio del processo di progettazione. Questi risultati forniscono un feedback ai progettisti hardware su potenziali ottimizzazioni e agli sviluppatori software su perfezionamenti del modello o framework per sfruttare meglio le capacità hardware. Questo livello di sinergia è difficile da raggiungere con la pratica comune di software sviluppato in modo indipendente per essere distribuito su hardware fisso.\nLa progettazione congiunta è fondamentale per i sistemi di intelligenza artificiale embedded che affrontano notevoli vincoli di risorse come budget di potenza ridotti, memoria e capacità di elaborazione limitate e requisiti di latenza in tempo reale. La stretta integrazione tra sviluppatori di algoritmi e architetti hardware aiuta a sbloccare le ottimizzazioni in tutto lo stack per soddisfare queste restrizioni. Le tecniche di abilitazione includono miglioramenti algoritmici come la ricerca e il pruning [potatura] dell’architettura neurale e progressi hardware come flussi di dati specializzati e gerarchie di memoria.\nRiunendo la progettazione hardware e software, anziché svilupparli separatamente, è possibile realizzare ottimizzazioni olistiche che massimizzano prestazioni ed efficienza. Le sezioni successive forniscono maggiori dettagli su specifici approcci di progettazione congiunta.\n\n10.4.1 La Necessità della Progettazione Congiunta\nDiversi fattori chiave rendono essenziale un approccio di progettazione congiunta hardware-software collaborativo per la creazione di sistemi di intelligenza artificiale efficienti.\n\nAumento delle Dimensioni e della Complessità del Modello\nI modelli di intelligenza artificiale all’avanguardia sono cresciuti rapidamente in termini di dimensioni, abilitati dai progressi nella progettazione dell’architettura neurale e dalla disponibilità di grandi set di dati. Ad esempio, il modello linguistico GPT-3 contiene 175 miliardi di parametri (Brown et al. 2020), che richiedono enormi risorse di calcolo per l’addestramento. Questa esplosione nella complessità del modello richiede una progettazione congiunta per sviluppare hardware e algoritmi efficienti in tandem. Tecniche come la compressione del modello (Cheng et al. 2018) e la quantizzazione devono essere co-ottimizzate con l’architettura hardware.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nCheng, Yu, Duo Wang, Pan Zhou, e Tao Zhang. 2018. «Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges». IEEE Signal Process Mag. 35 (1): 126–36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nVincoli della Distribuzione Embedded\nL’implementazione di applicazioni AI su dispositivi edge come telefoni cellulari o elettrodomestici intelligenti introduce vincoli significativi su energia, memoria e area di silicio (Sze et al. 2017). Abilitare l’inferenza in tempo reale con queste restrizioni richiede la co-esplorazione di ottimizzazioni hardware come flussi di dati specializzati e compressione con progettazione efficiente di reti neurali e tecniche di potatura. La co-progettazione massimizza le prestazioni entro rigidi vincoli di distribuzione.\n\n\nRapida Evoluzione degli Algoritmi AI\nL’intelligenza artificiale si sta evolvendo rapidamente, con nuove architetture di modelli, metodologie di training e framework software che emergono costantemente. Ad esempio, i Transformers sono diventati di recente molto popolari per l’NLP (Young et al. 2018). Per tenere il passo con queste innovazioni algoritmiche è necessaria una progettazione congiunta hardware-software per adattare le piattaforme ed evitare rapidamente il debito tecnico accumulato.\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, e Erik Cambria. 2018. «Recent Trends in Deep Learning Based Natural Language Processing [Review Article]». IEEE Comput. Intell. Mag. 13 (3): 55–75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nInterazioni Complesse Hardware-Software\nMolte interazioni e compromessi sottili tra scelte architettoniche hardware e ottimizzazioni software hanno un impatto significativo sull’efficienza complessiva. Ad esempio, tecniche come il partizionamento tensoriale e il batching influenzano il parallelismo e i modelli di accesso ai dati influenzano l’utilizzo della memoria. La progettazione congiunta fornisce una prospettiva multilivello per svelare queste dipendenze.\n\n\nNecessità di Specializzazione\nI carichi di lavoro dell’intelligenza artificiale traggono vantaggio da operazioni specializzate come matematica a bassa precisione e gerarchie di memoria personalizzate. Ciò motiva l’incorporazione di hardware personalizzato su misura per algoritmi di reti neurali piuttosto che affidarsi esclusivamente a software flessibile in esecuzione su hardware generico (Sze et al. 2017). Tuttavia, lo stack software deve mirare esplicitamente alle operazioni hardware personalizzate per realizzare i vantaggi.\n\n\nRichiesta di Maggiore Efficienza\nCon la crescente complessità del modello, si verificano rendimenti decrescenti e spese generali derivanti dall’ottimizzazione del solo hardware o software in isolamento (Putnam et al. 2014). Si presentano inevitabili compromessi che richiedono un’ottimizzazione globale su più livelli. La progettazione congiunta di hardware e software fornisce grandi guadagni di efficienza composti.\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. «A reconfigurable fabric for accelerating large-scale datacenter services». ACM SIGARCH Computer Architecture News 42 (3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\n\n10.4.2 Principi di Progettazione Congiunta Hardware-Software\nL’architettura hardware e lo stack software devono essere strettamente integrati e co-ottimizzati per creare sistemi di intelligenza artificiale efficienti e ad alte prestazioni. Nessuno dei due può essere progettato in isolamento; massimizzare le loro sinergie richiede un approccio olistico noto come progettazione congiunta hardware-software.\nL’obiettivo principale è adattare le capacità hardware in modo che corrispondano agli algoritmi e ai carichi di lavoro eseguiti dal software. Ciò richiede un ciclo di feedback tra architetti hardware e sviluppatori software per convergere su soluzioni ottimizzate. Diverse tecniche consentono un’efficace co-progettazione:\n\nOttimizzazione Software Consapevole dell’Hardware\nLo stack software può essere ottimizzato per sfruttare meglio le capacità hardware:\n\nParallelismo: Parallelizzare i calcoli matriciali come convoluzione o layer di attenzione per massimizzare la produttività sui motori vettoriali.\nOttimizzazione della Memoria: Ottimizzare i layout dei dati per migliorare la località della cache in base alla profilazione hardware. Ciò massimizza il riutilizzo e riduce al minimo l’accesso DRAM costoso.\nCompressione: Utilizzare la sparsity [diradazione] nei modelli per ridurre lo spazio di archiviazione e risparmiare sui calcoli tramite operazioni di zero-skipping.\nOperazioni Personalizzate: Incorporare operazioni specializzate come INT4 a bassa precisione o bfloat16 nei modelli per sfruttare al meglio il supporto hardware dedicato.\nMappatura del Flusso di Dati: Mappare esplicitamente le fasi del modello alle unità di calcolo per ottimizzare lo spostamento dei dati sull’hardware.\n\n\n\nSpecializzazione Hardware Algorithm-Driven\nL’hardware può essere adattato alle caratteristiche degli algoritmi ML:\n\nTipi di Dati Personalizzati: Supportare INT8/4 o bfloat16 a bassa precisione nell’hardware per una maggiore densità aritmetica.\nMemoria su Chip: Aumentare la larghezza di banda SRAM e ridurre la latenza di accesso per adattarla ai pattern di accesso alla memoria del modello.\nOperazioni Specifiche del Dominio: Aggiungere unità hardware per funzioni ML chiave come FFT o moltiplicazione di matrici per ridurre latenza ed energia.\nProfilazione del Modello: Utilizzare la simulazione e la profilazione del modello per identificare hotspot computazionali e ottimizzare l’hardware.\n\nLa chiave è il feedback collaborativo: le informazioni dalla profilazione dell’hardware guidano le ottimizzazioni del software, mentre i progressi algoritmici informano la specializzazione dell’hardware. Questo miglioramento reciproco fornisce guadagni di efficienza moltiplicativa rispetto agli sforzi isolati.\n\n\nCo-esplorazione Algoritmo-Hardware\nUna potente tecnica di co-progettazione prevede l’esplorazione congiunta di innovazioni nelle architetture di reti neurali e nella progettazione custom dell’hardware. A powerful co-design technique involves jointly exploring innovations in neural network architectures and custom hardware design. Ciò consente di trovare abbinamenti ideali su misura per i rispettivi punti di forza (Sze et al. 2017).\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, e Joel S. Emer. 2017. «Efficient Processing of Deep Neural Networks: A Tutorial and Survey». Proc. IEEE 105 (12): 2295–2329. https://doi.org/10.1109/jproc.2017.2761740.\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, e Hartwig Adam. 2017. «MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications». ArXiv preprint. https://arxiv.org/abs/1704.04861.\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, e Dmitry Kalenichenko. 2018. «Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2704–13. IEEE. https://doi.org/10.1109/cvpr.2018.00286.\n\nGale, Trevor, Erich Elsen, e Sara Hooker. 2019. «The state of sparsity in deep neural networks». ArXiv preprint abs/1902.09574. https://arxiv.org/abs/1902.09574.\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, e Paulius Micikevicius. 2021. «Accelerating Sparse Deep Neural Networks». CoRR abs/2104.08378. https://arxiv.org/abs/2104.08378.\nAd esempio, il passaggio ad architetture mobili come MobileNets (Howard et al. 2017) è stato guidato dai vincoli dei dispositivi edge come dimensioni del modello e latenza. La quantizzazione (Jacob et al. 2018) e le tecniche di pruning [potatura] (Gale, Elsen, e Hooker 2019) che hanno reso questi modelli efficienti sono diventate possibili grazie ad acceleratori hardware con supporto nativo per interi a bassa precisione e supporto per potatura (Mishra et al. 2021).\nI modelli basati sull’attenzione hanno prosperato su GPU e ASIC massivamente paralleli, dove il loro calcolo si mappa bene nello spazialmente, al contrario delle architetture RNN, che si basano sull’elaborazione sequenziale. La co-evoluzione di algoritmi e hardware ha evidenziato nuove capacità.\nUna co-esplorazione efficace richiede una stretta collaborazione tra ricercatori di algoritmi e architetti hardware. La prototipazione rapida su FPGA (C. Zhang et al. 2015) o simulatori di intelligenza artificiale specializzati consente una rapida valutazione di diverse coppie di architetture di modelli e progetti hardware pre-silicio.\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, e Jason Optimizing Cong. 2015. «FPGA-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 ACM». In SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA, 15:161–70.\nAd esempio, l’architettura TPU di Google si è evoluta con ottimizzazioni verso i modelli TensorFlow per massimizzare le prestazioni sulla classificazione delle immagini. Questo stretto ciclo di feedback ha prodotto modelli su misura per la TPU che sarebbero stati improbabili in isolamento.\nGli studi hanno mostrato guadagni di prestazioni ed efficienza da 2 a 5 volte superiori con la co-esplorazione algoritmo-hardware rispetto agli sforzi isolati di ottimizzazione di algoritmi o hardware (Suda et al. 2016). Parallelizzare lo sviluppo congiunto riduce anche i “time-to-deployment” [tempi di distribuzione].\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma, Sarma Vrudhula, Jae-sun Seo, e Yu Cao. 2016. «Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks». In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 16–25. ACM. https://doi.org/10.1145/2847263.2847276.\nNel complesso, esplorare le strette interdipendenze tra innovazione del modello e progressi hardware crea opportunità che devono essere visibili quando vengono affrontate in sequenza. Questa progettazione sinergica congiunta produce soluzioni maggiori della somma delle loro parti.\n\n\n\n10.4.3 Sfide\nSebbene la progettazione collaborativa possa migliorare l’efficienza, l’adattabilità e il time-to-market, presenta anche sfide ingegneristiche e organizzative.\n\nAumento dei Costi di Prototipazione\nÈ richiesta una prototipazione più estesa per valutare diverse accoppiate hardware-software. La necessità di prototipi rapidi e iterativi su FPGA o emulatori aumenta il sovraccarico della validazione. Ad esempio, Microsoft ha scoperto che erano necessari più prototipi per la progettazione collaborativa di un acceleratore AI rispetto alla progettazione sequenziale (Fowers et al. 2018).\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. «A Configurable Cloud-Scale DNN Processor for Real-Time AI». In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), 1–14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nOstacoli Organizzativi e di Team\nLa progettazione collaborativa richiede uno stretto coordinamento tra gruppi hardware e software tradizionalmente scollegati. Ciò potrebbe causare problemi di comunicazione o priorità e pianificazioni non allineate. Anche la navigazione di diversi flussi di lavoro di progettazione è impegnativa. Potrebbe esistere una certa inerzia organizzativa nell’adottare pratiche integrate.\n\n\nComplessità di Simulazione e Modellazione\nCatturare interazioni sottili tra layer hardware e software per la simulazione e la modellazione congiunte aggiunge una complessità significativa. Le astrazioni complete “cross-layer” sono difficili da costruire quantitativamente prima dell’implementazione, rendendo più difficile quantificare in anticipo le ottimizzazioni olistiche.\n\n\nRischi di Eccessiva Specializzazione\nUna progettazione congiunta rigorosa comporta il rischio di adattare eccessivamente le ottimizzazioni agli algoritmi correnti, sacrificando la generalità. Ad esempio, l’hardware ottimizzato esclusivamente per i modelli Transformer potrebbe avere prestazioni inferiori con le tecniche future. Mantenere la flessibilità richiede lungimiranza.\n\n\nProblemi sui Cambiamenti\nGli ingegneri che hanno familiarità con le consolidate pratiche di progettazione hardware o software discrete potrebbero accettare solo flussi di lavoro collaborativi familiari. Nonostante i vantaggi a lungo termine, i progetti potrebbero incontrare attriti nella transizione alla progettazione congiunta.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#software-per-hardware-ai",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#software-per-hardware-ai",
    "title": "10  Accelerazione IA",
    "section": "10.5 Software per Hardware AI",
    "text": "10.5 Software per Hardware AI\nAcceleratori hardware specializzati come GPU, TPU e FPGA sono essenziali per fornire applicazioni di intelligenza artificiale ad alte prestazioni. Tuttavia, è necessario un ampio stack software per sfruttare efficacemente queste piattaforme hardware, che coprano l’intero ciclo di vita di sviluppo e distribuzione. Framework e librerie costituiscono la spina dorsale dell’hardware AI, offrendo set di codice, algoritmi e funzioni pre-costruiti e robusti, specificamente ottimizzati per eseguire varie attività AI su hardware diversi. Sono progettati per semplificare le complessità dell’utilizzo dell’hardware da zero, che può richiedere molto tempo ed essere soggetto a errori. Il software svolge un ruolo importante:\n\nFornendo astrazioni di programmazione e modelli come CUDA e OpenCL per mappare i calcoli sugli acceleratori.\nIntegrando gli acceleratori in framework di deep learning popolari come TensorFlow e PyTorch.\nOttimizzando l’intero stack hardware-software con compilatori e tool.\nCon piattaforme di simulazione per modellare insieme hardware e software.\nCon l’infrastruttura per gestire la distribuzione sugli acceleratori.\n\nQuesto vasto ecosistema software è importante quanto l’hardware nel fornire applicazioni AI performanti ed efficienti. Questa sezione fornisce una panoramica degli strumenti disponibili a ogni livello dello stack per consentire agli sviluppatori di creare ed eseguire sistemi AI basati sull’accelerazione hardware.\n\n10.5.1 Modelli di Programmazione\nI modelli di programmazione forniscono astrazioni per mappare calcoli e dati su acceleratori hardware eterogenei:\n\nCUDA: Modello di programmazione parallela di Nvidia per sfruttare le GPU utilizzando estensioni a linguaggi come C/C++. Consente di avviare kernel su core GPU (Luebke 2008).\nOpenCL: Standard aperto per scrivere programmi che spaziano tra CPU, GPU, FPGA e altri acceleratori. Specifica un framework di elaborazione eterogeneo (Munshi 2009).\nOpenGL/WebGL: Interfacce di programmazione grafica 3D in grado di mappare codice generico su core GPU (Segal e Akeley 1999).\nVerilog/VHDL: “Hardware description languages (HDL)” [Linguaggi di descrizione hardware] utilizzati per configurare FPGA come acceleratori AI specificando circuiti digitali (Gannot e Ligthart 1994).\nTVM: Un framework di compilazione che fornisce un frontend Python per ottimizzare e mappare modelli di deep learning su diversi backend hardware (Chen et al. 2018).\n\n\nLuebke, David. 2008. «CUDA: Scalable parallel programming for high-performance scientific computing». In 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 836–38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\nMunshi, Aaftab. 2009. «The OpenCL specification». In 2009 IEEE Hot Chips 21 Symposium (HCS), 1–314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\nSegal, Mark, e Kurt Akeley. 1999. «The OpenGL graphics system: A specification (version 1.1)».\n\nGannot, G., e M. Ligthart. 1994. «Verilog HDL based FPGA design». In International Verilog HDL Conference, 86–92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. «TVM: An automated End-to-End optimizing compiler for deep learning». In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\nLe sfide principali includono l’espressione del parallelismo, la gestione della memoria tra dispositivi e l’abbinamento di algoritmi alle capacità hardware. Le astrazioni devono bilanciare la portabilità con la possibilità di personalizzazione hardware. I modelli di programmazione consentono agli sviluppatori di sfruttare gli acceleratori senza competenze hardware. Questi dettagli sono discussi nella sezione AI frameworks section.\n\n\n\n\n\n\nEsercizio 10.1: Software per hardware AI - TVM\n\n\n\n\n\nAbbiamo imparato che l’hardware AI sofisticato ha bisogno di un software speciale per fare magie. TVM è come un traduttore super intelligente, che trasforma il codice in istruzioni che gli acceleratori capiscono. In questo Colab, useremo TVM per creare un acceleratore finto chiamato VTA che esegue la moltiplicazione di matrici super velocemente. Pronti a vedere come il software alimenta l’hardware?\n\n\n\n\n\n\n10.5.2 Librerie e Runtime\nLibrerie e runtime specializzati forniscono astrazioni software per accedere e massimizzare l’utilizzo degli acceleratori AI:\n\nLibrerie Matematiche: Implementazioni altamente ottimizzate di primitive di algebra lineare come GEMM, FFT, convoluzioni, ecc., su misura per l’hardware target. Nvidia cuBLAS, Intel MKL e librerie di elaborazione Arm sono esempi.\nIntegrazioni di Framework: Librerie per accelerare framework di deep learning come TensorFlow, PyTorch e MXNet su hardware supportato. Ad esempio, cuDNN accelera le CNN sulle GPU Nvidia.\nRuntime: Software per gestire l’esecuzione dell’acceleratore, tra cui pianificazione, sincronizzazione, gestione della memoria e altre attività. Nvidia TensorRT è un ottimizzatore di inferenza e runtime.\nDriver e firmware: Software di basso livello per interfacciarsi con l’hardware, inizializzare i dispositivi e gestire l’esecuzione. Fornitori come Xilinx forniscono driver per le loro schede acceleratrici.\n\nAd esempio, gli integratori PyTorch utilizzano librerie cuDNN e cuBLAS per accelerare l’addestramento sulle GPU Nvidia. Il runtime TensorFlow XLA ottimizza e compila modelli per acceleratori come le TPU. I driver inizializzano i dispositivi e delegano le operazioni.\nLe sfide includono il partizionamento e la pianificazione efficienti dei carichi di lavoro su dispositivi eterogenei come nodi multi-GPU. I runtime devono anche ridurre al minimo il sovraccarico dei trasferimenti di dati e della sincronizzazione.\nLibrerie, runtime e driver forniscono i mattoni ottimizzati che gli sviluppatori di deep learning possono sfruttare per le prestazioni dell’acceleratore senza competenze di programmazione hardware. La loro ottimizzazione è essenziale per le distribuzioni.\n\n\n10.5.3 Ottimizzazione dei Compilatori\nL’ottimizzazione dei compilatori è fondamentale per estrarre le massime prestazioni ed efficienza dagli acceleratori hardware per i carichi di lavoro AI. Applicano ottimizzazioni che spaziano tra modifiche algoritmiche, trasformazioni a livello di grafico e generazione di codice di basso livello.\n\nOttimizzazione degli Algoritmi: Tecniche come quantizzazione, potatura e ricerca di architettura neurale per migliorare l’efficienza del modello e abbinare le capacità hardware.\nOttimizzazioni dei Grafi: Ottimizzazioni a livello di grafo come fusione degli operatori, riscrittura e trasformazioni di layout per ottimizzare le prestazioni sull’hardware target.\nGenerazione di Codice: Generazione di codice di basso livello ottimizzato per acceleratori da modelli e framework di alto livello.\n\nAd esempio, lo stack di compilatori “open” TVM applica la quantizzazione per un modello BERT che ha come target le GPU Arm. Fonde le operazioni di convoluzione puntuale e trasforma il layout dei pesi per ottimizzare l’accesso alla memoria. Infine, emette codice OpenGL ottimizzato per eseguire il carico di lavoro GPU.\nLe ottimizzazioni chiave del compilatore includono la massimizzazione del parallelismo, il miglioramento della località e del riutilizzo dei dati, la riduzione al minimo dell’ingombro della memoria e lo sfruttamento delle operazioni hardware personalizzate. I compilatori creano e ottimizzano i carichi di lavoro di machine learning in modo olistico su componenti hardware come CPU, GPU e altri acceleratori.\nTuttavia, la mappatura efficiente di modelli complessi introduce sfide come il partizionamento efficiente dei carichi di lavoro su dispositivi eterogenei. I compilatori a livello di produzione richiedono anche molto tempo per la messa a punto su carichi di lavoro rappresentativi. Tuttavia, l’ottimizzazione dei compilatori è per sfruttare tutte le capacità degli acceleratori AI.\n\n\n10.5.4 Simulazione e Modellazione\nIl software di simulazione è importante nella progettazione congiunta hardware-software. Consente accoppiare la modellazione di architetture hardware e stack software proposti:\n\nSimulazione Hardware: Piattaforme come Gem5 consentono la simulazione dettagliata di componenti hardware come pipeline, cache, interconnessioni e gerarchie di memoria. Gli ingegneri possono modellare le modifiche hardware senza prototipazione fisica (Binkert et al. 2011).\nSimulazione Software: Stack di compilatori come TVM supportano la simulazione di carichi di lavoro di machine learning per stimare le prestazioni sulle architetture hardware target. Questo aiuta con le ottimizzazioni software.\nCo-simulazione: Piattaforme unificate come SCALE-Sim (Samajdar et al. 2018) integrano la simulazione hardware e software in un unico strumento. Ciò consente un’analisi “what-if” per quantificare gli impatti a livello di sistema delle ottimizzazioni cross-layer all’inizio del ciclo di progettazione.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. «The gem5 simulator». ACM SIGARCH Computer Architecture News 39 (2): 1–7. https://doi.org/10.1145/2024716.2024718.\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, e Tushar Krishna. 2018. «Scale-sim: Systolic cnn accelerator simulator». ArXiv preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\nAd esempio, un progetto di acceleratore AI basato su FPGA potrebbe essere simulato utilizzando il linguaggio di descrizione hardware Verilog e sintetizzato in un modello Gem5. Verilog è adatto per descrivere la logica digitale e le interconnessioni dell’architettura dell’acceleratore. Verilog consente al progettista di specificare i datapath [percorsi dati], la logica di controllo, le memorie on-chip e altri componenti implementati nella struttura FPGA. Una volta completato il progetto Verilog, può essere sintetizzato in un modello che simula il comportamento dell’hardware, ad esempio utilizzando il simulatore Gem5. Gem5 è utile per questa attività perché consente la modellazione di sistemi completi, inclusi processori, cache, bus e acceleratori personalizzati. Gem5 supporta l’interfacciamento dei modelli Verilog dell’hardware alla simulazione, consentendo la modellazione unificata del sistema.\nIl modello di acceleratore FPGA sintetizzato potrebbe quindi avere carichi di lavoro ML simulati utilizzando TVM compilato su di esso all’interno dell’ambiente Gem5 per una modellazione unificata. TVM consente la compilazione ottimizzata di modelli di ML su hardware eterogeneo come FPGA. L’esecuzione di carichi di lavoro compilati con TVM sull’acceleratore all’interno della simulazione Gem5 fornisce un modo integrato per convalidare e perfezionare la progettazione hardware, lo stack software e l’integrazione di sistema prima di realizzare fisicamente l’acceleratore su un FPGA reale.\nQuesto tipo di co-simulazione fornisce stime di metriche complessive come throughput, latenza e potenza per guidare la progettazione congiunta prima della costosa prototipazione fisica. Aiutano anche con le ottimizzazioni di partizionamento tra hardware e software per guidare i compromessi di progettazione.\nTuttavia, la precisione nella modellazione di interazioni sottili di basso livello tra componenti è limitata. Le simulazioni quantificate sono stime ma non possono sostituire completamente i prototipi fisici e i test. Tuttavia, la simulazione e la modellazione unificate forniscono preziose informazioni iniziali sulle opportunità di ottimizzazione a livello di sistema durante il processo di co-progettazione.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#benchmarking-dellhardware-ai",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#benchmarking-dellhardware-ai",
    "title": "10  Accelerazione IA",
    "section": "10.6 Benchmarking dell’Hardware AI",
    "text": "10.6 Benchmarking dell’Hardware AI\nIl benchmarking è un processo critico che quantifica e confronta le prestazioni di varie piattaforme hardware progettate per accelerare le applicazioni di intelligenza artificiale. Guida le decisioni di acquisto, l’attenzione allo sviluppo e gli sforzi di ottimizzazione delle prestazioni per i produttori di hardware e gli sviluppatori di software.\nIl capitolo sul benchmarking esplora questo argomento in modo molto dettagliato, spiegando perché è diventato una parte indispensabile del ciclo di sviluppo dell’hardware AI e come influisce sul più ampio panorama tecnologico. Qui, esamineremo brevemente i concetti principali, ma consigliamo di fare riferimento al capitolo per maggiori dettagli.\nSuite di benchmarking come MLPerf, Fathom e AI Benchmark offrono una serie di test standardizzati utilizzabili su diverse piattaforme hardware. Queste suite misurano le prestazioni dell’acceleratore AI su varie reti neurali e attività di apprendimento automatico, dalla classificazione di immagini di base all’elaborazione complessa del linguaggio. Fornendo un terreno comune per il confronto, aiutano a garantire che le dichiarazioni sulle prestazioni siano coerenti e verificabili. Questi “tool” vengono applicati non solo per guidare lo sviluppo dell’hardware, ma anche per garantire che lo stack software sfrutti appieno il potenziale dell’architettura sottostante.\n\nMLPerf: Include un ampio set di benchmark che coprono sia l’addestramento (Mattson et al. 2020) che l’inferenza (Reddi et al. 2020) per una gamma di attività di machine learning.\nFathom: Si concentra sulle operazioni principali nei modelli di deep learning, enfatizzandone l’esecuzione su diverse architetture (Adolf et al. 2016).\nAI Benchmark: Mira a dispositivi mobili e consumer, valutando le prestazioni dell’IA nelle applicazioni per utenti finali (Ignatov et al. 2018).\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. «MLPerf Inference Benchmark». In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. «Fathom: Reference workloads for modern deep learning methods». In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, e Luc Van Gool. 2018. «AI Benchmark: Running deep neural networks on Android smartphones», 0–0.\nI benchmark hanno anche metriche delle prestazioni che sono misure quantificabili utilizzate per valutare l’efficacia degli acceleratori di IA. Queste metriche forniscono una visione completa delle capacità di un acceleratore e vengono utilizzate per guidare il processo di progettazione e selezione per i sistemi di IA. Le metriche comuni comprendono:\n\nThroughput: Solitamente misurato in operazioni al secondo, questo parametro indica il volume di calcoli che un acceleratore può gestire.\nLatenza: Il ritardo temporale tra input e output in un sistema è fondamentale per le attività di elaborazione in tempo reale.\nEfficienza Energetica: Calcolato come elaborazione per watt, che rappresenta il compromesso tra prestazioni e consumo energetico.\nEfficienza dei Costi: Valuta il costo operativo in relazione alle prestazioni, un parametro essenziale per le distribuzioni attente al budget.\nPrecisione: Nelle attività di inferenza, la precisione dei calcoli è fondamentale e talvolta bilanciata rispetto alla velocità.\nScalabilità: La capacità del sistema di mantenere i guadagni in termini di prestazioni man mano che il carico computazionale aumenta.\n\nI risultati del benchmark forniscono informazioni che vanno oltre i semplici numeri: possono rivelare colli di bottiglia nello stack software e nell’hardware. Ad esempio, i benchmark possono mostrare come l’aumento delle dimensioni del batch migliori l’utilizzo della GPU fornendo più parallelismo o come le ottimizzazioni del compilatore aumentino le prestazioni della TPU. Questi insegnamenti consentono un’ottimizzazione continua (Zhihao Jia, Zaharia, e Aiken 2019).\n\nJia, Zhihao, Matei Zaharia, e Alex Aiken. 2019. «Beyond Data and Model Parallelism for Deep Neural Networks». In Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019, a cura di Ameet Talwalkar, Virginia Smith, e Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand Jayarajan, Amar Phanishayee, Bianca Schroeder, e Gennady Pekhimenko. 2018. «Benchmarking and Analyzing Deep Neural Network Training». In 2018 IEEE International Symposium on Workload Characterization (IISWC), 88–100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\nIl benchmarking standardizzato fornisce una valutazione quantificata e comparabile degli acceleratori AI per informare la progettazione, l’acquisto e l’ottimizzazione. Tuttavia, anche la convalida delle prestazioni nel mondo reale rimane essenziale (H. Zhu et al. 2018).",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#sfide-e-soluzioni",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#sfide-e-soluzioni",
    "title": "10  Accelerazione IA",
    "section": "10.7 Sfide e Soluzioni",
    "text": "10.7 Sfide e Soluzioni\nGli acceleratori AI offrono notevoli miglioramenti delle prestazioni, ma spesso è necessario migliorare i significativi problemi di portabilità e compatibilità nella loro integrazione nel più ampio panorama AI. Il nocciolo della questione risiede nella diversità dell’ecosistema AI: esiste una vasta gamma di acceleratori, framework e linguaggi di programmazione per l’apprendimento automatico, ognuno con le sue caratteristiche e requisiti unici.\n\n10.7.1 Problemi di Portabilità/Compatibilità\nGli sviluppatori incontrano spesso difficoltà nel trasferire i loro modelli AI da un ambiente hardware a un altro. Ad esempio, un modello di machine learning sviluppato per un ambiente desktop in Python utilizzando il framework PyTorch, ottimizzato per una GPU Nvidia, potrebbe non essere facilmente trasferito a un dispositivo più vincolato come Arduino Nano 33 BLE. Questa complessità deriva da nette differenze nei requisiti di programmazione: Python e PyTorch sul desktop rispetto a un ambiente C++ su un Arduino, per non parlare del passaggio dall’architettura x86 ad ARM ISA.\nQueste divergenze evidenziano la complessità della portabilità all’interno dei sistemi AI. Inoltre, il rapido progresso negli algoritmi e nei modelli di intelligenza artificiale implica che gli acceleratori hardware debbano adattarsi continuamente, creando un obiettivo mobile per la compatibilità. L’assenza di standard e interfacce universali aggrava il problema, rendendo difficile l’implementazione di soluzioni di intelligenza artificiale in modo coerente su vari dispositivi e piattaforme.\n\nSoluzioni e Strategie\nPer affrontare questi ostacoli, il settore dell’intelligenza artificiale si sta muovendo verso diverse soluzioni:\n\nIniziative di Standardizzazione\nOpen Neural Network Exchange (ONNX) è in prima linea in questa ricerca, proponendo un ecosistema aperto e condiviso che promuove l’intercambiabilità dei modelli. ONNX facilita l’uso di modelli di intelligenza artificiale su vari framework, consentendo ai modelli addestrati in un ambiente di essere distribuiti in modo efficiente in un altro, riducendo significativamente la necessità di riscritture o modifiche che richiedono molto tempo.\n\n\nFramework Multipiattaforma\nA complemento degli sforzi di standardizzazione, framework multipiattaforma come TensorFlow Lite e PyTorch Mobile sono stati sviluppati specificamente per creare coesione tra diversi ambienti di calcolo che vanno dai desktop ai dispositivi mobili ed embedded. Questi framework offrono versioni semplificate e leggere delle loro versioni principali, garantendo compatibilità e integrità funzionale su diversi tipi di hardware senza sacrificare le prestazioni. Ciò garantisce che gli sviluppatori possano creare applicazioni con la certezza che funzioneranno su molti dispositivi, colmando un divario che tradizionalmente ha rappresentato una sfida considerevole nello sviluppo dell’intelligenza artificiale.\n\n\nPiattaforme Indipendenti dall’Hardware\nL’ascesa delle piattaforme indipendenti dall’hardware ha anche svolto un ruolo importante nella democratizzazione dell’uso dell’IA. Creando ambienti in cui le applicazioni di IA possono essere eseguite su vari acceleratori, queste piattaforme eliminano l’onere della codifica specifica per l’hardware dagli sviluppatori. Questa astrazione semplifica il processo di sviluppo e apre nuove possibilità per l’innovazione e l’implementazione delle applicazioni, libere dai vincoli delle specifiche hardware.\n\n\nStrumenti di Compilazione Avanzati\nInoltre, l’avvento di strumenti di compilazione avanzati come TVM, un compilatore di tensori end-to-end, offre un percorso ottimizzato attraverso la giungla delle diverse architetture hardware. TVM fornisce agli sviluppatori i mezzi per mettere a punto modelli di machine learning per un ampio spettro di substrati computazionali, garantendo prestazioni ottimali ed evitando la regolazione manuale del modello ogni volta che si verifica uno spostamento nell’hardware sottostante.\n\n\nCollaborazione tra Comunità e Settore\nLa collaborazione tra comunità open source e consorzi di settore non può essere sottovalutata. Questi organismi collettivi sono fondamentali per la formazione di standard condivisi e best practice a cui tutti gli sviluppatori e i produttori possono aderire. Tale collaborazione promuove un ecosistema AI più unificato e sinergico, riducendo significativamente la prevalenza di problemi di portabilità e spianando la strada verso l’integrazione e l’avanzamento dell’AI globale. Attraverso questi lavori combinati, l’AI si sta muovendo costantemente verso un futuro in cui la distribuzione di modelli senza soluzione di continuità su varie piattaforme diventa uno standard piuttosto che un’eccezione.\nRisolvere le sfide della portabilità è fondamentale per il campo dell’IA per realizzare il pieno potenziale degli acceleratori hardware in un panorama tecnologico dinamico e diversificato. Richiede uno sforzo concertato da parte dei produttori di hardware, degli sviluppatori di software e degli enti normativi per creare un ambiente più interoperabile e flessibile. Con innovazione e collaborazione continue, la comunità dell’IA può aprire la strada a un’integrazione e a un’implementazione senza soluzione di continuità dei modelli di IA su molte piattaforme.\n\n\n\n\n10.7.2 Problemi di Consumo Energetico\nIl consumo energetico è un problema cruciale nello sviluppo e nel funzionamento degli acceleratori AI dei data center, come le unità di elaborazione grafica (GPU) e le unità di elaborazione tensoriale (TPU) (N. P. Jouppi et al. 2017b) (Norrie et al. 2021) (N. Jouppi et al. 2023). Questi potenti componenti sono la spina dorsale dell’infrastruttura AI contemporanea, ma le loro elevate richieste di energia contribuiscono all’impatto ambientale della tecnologia e aumentano significativamente i costi operativi. Man mano che le esigenze di elaborazione dei dati diventano più complesse, con la crescente popolarità dell’AI e del deep learning, c’è una richiesta pressante di GPU e TPU in grado di fornire la potenza di calcolo necessaria in modo più efficiente. L’impatto di tali progressi è duplice: possono ridurre l’impatto ambientale di queste tecnologie e ridurre i costi di esecuzione delle applicazioni AI.\n\n———, et al. 2017b. «In-Datacenter Performance Analysis of a Tensor Processing Unit». In Proceedings of the 44th Annual International Symposium on Computer Architecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, e David Patterson. 2021. «The Design Process for Google’s Training Chips: Tpuv2 and TPUv3». IEEE Micro 41 (2): 56–63. https://doi.org/10.1109/mm.2021.3058217.\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, et al. 2023. «TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings». In Proceedings of the 50th Annual International Symposium on Computer Architecture. ISCA ’23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\nLe tecnologie hardware emergenti sono sul punto di rivoluzionare l’efficienza energetica in questo settore. L’informatica fotonica, ad esempio, utilizza la luce anziché l’elettricità per trasportare informazioni, offrendo la promessa di un’elaborazione ad alta velocità con una frazione del consumo energetico. Analizziamo più approfonditamente questa e altre tecnologie innovative nella sezione “Tecnologie Hardware Emergenti”, esplorando il loro potenziale per affrontare le attuali sfide del consumo energetico.\nAi margini della rete, gli acceleratori AI sono progettati per elaborare dati su dispositivi come smartphone, sensori IoT e dispositivi indossabili intelligenti. Questi dispositivi spesso funzionano con gravi limitazioni di potenza, rendendo necessario un attento bilanciamento tra prestazioni e consumo energetico. Un modello AI ad alte prestazioni può fornire risultati rapidi, ma a costo di esaurire rapidamente la durata della batteria e aumentare la produzione termica, il che può influire sulla funzionalità e sulla durata del dispositivo. La posta in gioco è più alta per i dispositivi distribuiti in aree remote o difficili da raggiungere, dove non è possibile garantire un’alimentazione costante, il che sottolinea la necessità di soluzioni a basso consumo energetico.\nI problemi di latenza aggravano ulteriormente la sfida dell’efficienza energetica ai margini. Le applicazioni AI Edge in settori quali la guida autonoma e il monitoraggio sanitario richiedono velocità, precisione e affidabilità, poiché i ritardi nell’elaborazione possono comportare gravi rischi per la sicurezza. Per queste applicazioni, gli sviluppatori devono ottimizzare sia gli algoritmi AI sia la progettazione hardware per raggiungere un equilibrio ottimale tra consumo energetico e latenza.\nQuesto sforzo di ottimizzazione non riguarda solo l’apporto di miglioramenti incrementali alle tecnologie esistenti; riguarda il ripensamento di come e dove elaboriamo le attività AI. Progettando acceleratori AI che siano sia efficienti dal punto di vista energetico sia in grado di elaborare rapidamente, possiamo garantire che questi dispositivi svolgano i loro scopi previsti senza un consumo energetico non necessario o prestazioni compromesse. Tali sviluppi potrebbero promuovere l’adozione diffusa dell’AI in vari settori, consentendo un uso più intelligente, sicuro e sostenibile della tecnologia.\n\n\n10.7.3 Superare i Vincoli delle Risorse\nAnche i vincoli di risorse rappresentano una sfida significativa per gli acceleratori Edge AI, poiché queste soluzioni hardware e software specializzate devono fornire prestazioni robuste entro i limiti dei dispositivi edge. A causa dei limiti di potenza e dimensioni, gli acceleratori Edge AI hanno spesso capacità di calcolo, memoria e archiviazione limitate (L. Zhu et al. 2023). Questa scarsità di risorse richiede un’attenta allocazione delle capacità di elaborazione per eseguire modelli di apprendimento automatico in modo efficiente.\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2023. «PockEngine: Sparse and Efficient Fine-tuning in a Pocket». In 56th Annual IEEE/ACM International Symposium on Microarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\nLin, Ji, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, e Song Han. 2023. «AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration». arXiv.\n\nLi, Yuhang, Xin Dong, e Wei Wang. 2020. «Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks». In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, e Song Han. 2020. «APQ: Joint Search for Network Architecture, Pruning and Quantization Policy». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2075–84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\nInoltre, la gestione di risorse limitate richiede approcci innovativi, tra cui la quantizzazione del modello (Lin et al. 2023) (Li, Dong, e Wang 2020), pruning (Wang et al. 2020) e l’ottimizzazione delle pipeline di inferenza. Gli acceleratori Edge AI devono trovare un delicato equilibrio tra la fornitura di funzionalità AI significative e il non esaurire le risorse disponibili, mantenendo al contempo un basso consumo energetico. Superare questi vincoli di risorse è fondamentale per garantire l’implementazione di successo dell’intelligenza artificiale ai margini, dove molte applicazioni, dall’IoT ai dispositivi mobili, si basano sull’uso efficiente di risorse hardware limitate per fornire un processo decisionale intelligente e in tempo reale.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#tecnologie-emergenti",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#tecnologie-emergenti",
    "title": "10  Accelerazione IA",
    "section": "10.8 Tecnologie Emergenti",
    "text": "10.8 Tecnologie Emergenti\nFinora abbiamo discusso la tecnologia hardware AI nel contesto della progettazione dell’architettura von Neumann convenzionale e dell’implementazione basata su CMOS. Questi chip AI specializzati offrono vantaggi come una maggiore produttività ed efficienza energetica, ma si basano sui principi di elaborazione tradizionali. La crescita inarrestabile della domanda di potenza di elaborazione AI sta guidando le innovazioni nei metodi di integrazione per l’hardware AI.\nSono emersi due approcci principali per massimizzare la densità di elaborazione, l’integrazione su “scala wafer” e le architetture basate su “chiplet”, di cui parleremo in questa sezione. Guardando molto più avanti, esamineremo le tecnologie emergenti che divergono dalle architetture convenzionali e adottano approcci fondamentalmente diversi per l’elaborazione specializzata AI.\nAlcuni di questi paradigmi non convenzionali includono l’elaborazione neuromorfica, che imita le reti neurali biologiche; l’elaborazione quantistica, che sfrutta gli effetti della meccanica quantistica; e l’elaborazione ottica, che utilizza fotoni anziché elettroni. Oltre ai nuovi substrati di elaborazione, le nuove tecnologie dei dispositivi stanno consentendo ulteriori guadagni attraverso una migliore memoria e interconnessione.\nEsempi includono i “memristor” [https://it.wikipedia.org/wiki/Memristore] per l’elaborazione in memoria e la nanofotonica per la comunicazione fotonica integrata. Insieme, queste tecnologie offrono il potenziale per miglioramenti di ordini di grandezza in termini di velocità, efficienza e scalabilità rispetto all’attuale hardware AI. Esamineremo questi aspetti in questa sezione.\n\n10.8.1 Metodi di Integrazione\nI metodi di integrazione si riferiscono agli approcci utilizzati per combinare e interconnettere i vari componenti di elaborazione e memoria di un chip o sistema AI. Collegando strettamente gli elementi di elaborazione chiave, l’integrazione mira a massimizzare le prestazioni, l’efficienza energetica e la densità.\nIn passato, l’elaborazione AI veniva eseguita principalmente su CPU e GPU costruite utilizzando metodi di integrazione convenzionali. Questi componenti discreti venivano fabbricati separatamente e collegati insieme su una scheda. Tuttavia, questa integrazione poco stretta crea colli di bottiglia, come i sovraccarichi dei trasferimento di dati.\nCon l’aumento dei carichi di lavoro AI, aumenta la domanda di una più stretta integrazione tra elementi di elaborazione, memoria e comunicazione. Alcuni fattori chiave dell’integrazione includono:\n\nRiduzione al minimo dello spostamento dei dati: Una stretta integrazione riduce la latenza e l’energia per lo spostamento dei dati tra i componenti. Ciò migliora l’efficienza.\nPersonalizzazione: Adattare tutti i componenti del sistema ai carichi di lavoro AI consente ottimizzazioni in tutto lo stack hardware.\nParallelismo: L’integrazione di molti elementi di elaborazione consente un calcolo parallelo massiccio.\nDensità: Una più stretta integrazione consente di impacchettare più transistor e memoria in una determinata area.\nCosto: Le economie di scala derivanti da grandi sistemi integrati possono ridurre i costi.\n\nIn risposta, nuove tecniche di produzione come la fabbricazione su scala di wafer e il confezionamento avanzato consentono ora livelli di integrazione molto più elevati. L’obiettivo è creare complessi di elaborazione AI unificati e specializzati, su misura per il deep learning e altri algoritmi AI. Un’integrazione più stretta è fondamentale per fornire le prestazioni e l’efficienza necessarie per la prossima generazione di AI.\n\nAI su Scala Wafer\nL’intelligenza artificiale su “wafer-scale” adotta un approccio estremamente integrato, producendo un intero wafer di silicio come un gigantesco chip. Ciò differisce drasticamente dalle CPU e GPU convenzionali, che tagliano ogni wafer in molti chip singoli più piccoli. Figura 10.4 mostra un confronto tra Cerebras Wafer Scale Engine 2, che è il chip più grande mai costruito, e la GPU più grande. Mentre alcune GPU possono contenere miliardi di transistor, impallidiscono comunque rispetto alla scala di un chip delle dimensioni di un wafer con oltre un trilione di transistor.\nL’approccio su scala di wafer diverge anche dai progetti system-on-chip più modulari che hanno ancora componenti discreti che comunicano tramite bus. Invece, l’intelligenza artificiale su scala di wafer consente la personalizzazione completa e la stretta integrazione di elaborazione, memoria e interconnessioni nell’intero di die.\n\n\n\n\n\n\nFigura 10.4: Wafer-scale vs. GPU. Fonte: Cerebras.\n\n\n\nProgettando il wafer come un’unità logica integrata, il trasferimento dati tra gli elementi è ridotto al minimo. Ciò fornisce una latenza e un consumo energetico inferiori rispetto ai design discreti system-on-chip o chiplet. Mentre i chiplet possono offrire flessibilità mescolando e abbinando i componenti, la comunicazione tra chiplet è impegnativa. La natura monolitica dell’integrazione su scala wafer elimina questi colli di bottiglia nella comunicazione tra chip.\nTuttavia, la scala ultra-large pone anche difficoltà per la producibilità e la resa con i design su scala wafer. Difetti in qualsiasi regione del wafer possono rendere (alcune parti del) chip inutilizzabile. Sono necessarie tecniche di litografia specializzate per produrre tali matrici di grandi dimensioni. Quindi, l’integrazione su scala wafer persegue i massimi guadagni in termini di prestazioni dall’integrazione ma richiede il superamento di sostanziali sfide di fabbricazione.\nVideo 10.1 fornisce ulteriore contesto sui chip AI su scala wafer.\n\n\n\n\n\n\nVideo 10.1: Wafer-scale AI Chips\n\n\n\n\n\n\n\n\nChiplet per AI\nIl design chiplet si riferisce a un’architettura semiconduttrice in cui un singolo circuito integrato (IC) è costruito da più componenti più piccoli e individuali noti come chiplet. Ogni chiplet è un blocco funzionale autonomo, in genere specializzato per un’attività o funzionalità specifica. Questi chiplet sono quindi interconnessi su un substrato o un package più grande per creare un sistema coeso. Figura 10.5 illustra questo concetto. Per l’hardware AI, i chiplet consentono di combinare diversi tipi di chip ottimizzati per attività come moltiplicazione di matrici, spostamento di dati, I/O analogico e memorie specializzate. Questa integrazione eterogenea differisce notevolmente dall’integrazione wafer-scale, in cui tutta la logica è prodotta come un unico chip monolitico. Aziende come Intel e AMD hanno adottato design chiplet per le loro CPU.\nI chiplet sono interconnessi utilizzando tecniche di packaging avanzate come interposer di substrato ad alta densità, impilamento 2.5D/3D e packaging a livello di wafer. Ciò consente di combinare chiplet realizzati con diversi nodi di processo, memorie specializzate e vari motori AI ottimizzati.\n\n\n\n\n\n\nFigura 10.5: Partizionamento chiplet.. Fonte: Vivet et al. (2021).\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar Fuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021. «IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management». IEEE J. Solid-State Circuits 56 (1): 79–97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nEcco alcuni vantaggi chiave dell’uso di chiplet per l’intelligenza artificiale:\n\nFlessibilità: I chiplet consentono la combinazione di diversi tipi di chip, nodi di processo e memorie su misura per ogni funzione. Questo è più modulare rispetto a un design fisso su scala wafer.\nResa: I chiplet più piccoli hanno una resa maggiore rispetto a un gigantesco chip su scala wafer. I difetti sono contenuti nei singoli chiplet.\nCosto: Sfrutta le capacità di produzione esistenti anziché richiedere nuovi processi specializzati. Riduce i costi riutilizzando la fabbricazione assestata.\nCompatibilità: Può integrarsi con architetture di sistema più convenzionali come PCIe e interfacce di memoria DDR standard.\n\nTuttavia, i chiplet devono anche affrontare sfide di integrazione e prestazioni:\n\nDensità inferiore rispetto alla scala wafer, poiché i chiplet sono limitati in termini di dimensioni.\nLatenza aggiuntiva durante la comunicazione tra chiplet rispetto all’integrazione monolitica. Richiede ottimizzazione per interconnessioni a bassa latenza.\nIl packaging avanzato aggiunge complessità rispetto all’integrazione su scala wafer, sebbene ciò sia discutibile.\n\nL’obiettivo principale dei chiplet è trovare il giusto equilibrio tra flessibilità modulare e densità di integrazione per prestazioni AI ottimali. I chiplet mirano a un’accelerazione AI efficiente pur lavorando entro i vincoli delle tecniche di produzione convenzionali. I chiplet prendono una via di mezzo tra gli estremi dell’integrazione su scala wafer e dei componenti completamente discreti. Ciò fornisce vantaggi pratici ma può sacrificare una certa densità computazionale ed efficienza rispetto a un sistema teorico a livello di wafer.\n\n\n\n10.8.2 Elaborazione Nùeuromorfica\nL’elaborazione neuromorfica è un campo emergente che mira a emulare l’efficienza e la robustezza dei sistemi neurali biologici per applicazioni di machine learning. Una differenza fondamentale rispetto alle classiche architetture di Von Neumann è la fusione di memoria ed elaborazione nello stesso circuito (Schuman et al. 2022; Marković et al. 2020; Furber 2016), come illustrato in Figura 10.6. La struttura del cervello ispira questo approccio integrato. Un vantaggio fondamentale è il potenziale per un miglioramento di ordini di grandezza nel calcolo efficiente dal punto di vista energetico rispetto all’hardware AI convenzionale. Ad esempio, le stime prevedono guadagni di 100x-1000x nell’efficienza energetica rispetto agli attuali sistemi basati su GPU per carichi di lavoro equivalenti.\n\nMarković, Danijela, Alice Mizrahi, Damien Querlioz, e Julie Grollier. 2020. «Physics for neuromorphic computing». Nature Reviews Physics 2 (9): 499–510. https://doi.org/10.1038/s42254-020-0208-2.\n\nFurber, Steve. 2016. «Large-scale neuromorphic computing systems». J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\n\n\n\n\nFigura 10.6: Confronto tra l’architettura di von Neumann e l’architettura neuromorfica. Fonte: Schuman et al. (2022).\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. «Opportunities for neuromorphic computing algorithms and applications». Nature Computational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nIntel e IBM stanno guidando gli sforzi commerciali nell’hardware neuromorfico. I chip Loihi e Loihi 2 di Intel (Davies et al. 2018, 2021) offrono core neuromorfici programmabili con apprendimento on-chip. Il dispositivo Northpole (Modha et al. 2023) di IBM comprende oltre 100 milioni di sinapsi a giunzione a tunnel magnetico e 68 miliardi di transistor. Questi chip specializzati offrono vantaggi come un basso consumo energetico per l’inferenza edge.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. «Loihi: A Neuromorphic Manycore Processor with On-Chip Learning». IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya, Gabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, e Sumedh R. Risbud. 2021. «Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook». Proc. IEEE 109 (5): 911–34. https://doi.org/10.1109/jproc.2021.3067593.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. «Neural inference at the frontier of energy, space, and time». Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\nMaass, Wolfgang. 1997. «Networks of spiking neurons: The third generation of neural network models». Neural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\nLe “Spiking neural network (SNN)” (Maass 1997) sono modelli computazionali per hardware neuromorfico. A differenza delle reti neurali profonde che comunicano tramite valori continui, le SNN utilizzano picchi discreti che sono più simili ai neuroni biologici. Questo consente un calcolo efficiente basato sugli eventi anziché un’elaborazione costante. Inoltre, le SNN considerano le caratteristiche temporali e spaziali dei dati di input. Ciò imita meglio le reti neurali biologiche, in cui la tempistica dei picchi neuronali svolge un ruolo importante. Tuttavia, l’addestramento delle SNN rimane impegnativo a causa della complessità temporale aggiunta. Figura 10.7 fornisce una panoramica della metodologia spiking: (a) Diagramma di un neurone; (b) Misura di un potenziale d’azione propagato lungo l’assone [https://it.wikipedia.org/wiki/Assone] di un neurone. Solo il potenziale d’azione è rilevabile lungo l’assone; (c) Il picco del neurone è approssimato con una rappresentazione binaria; (d) Elaborazione guidata dagli eventi; (e) Active Pixel Sensor e Dynamic Vision Sensor.\n\n\n\n\n\n\nFigura 10.7: Spiking neuromorfico. Fonte: Eshraghian et al. (2023).\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, e Wei D. Lu. 2023. «Training Spiking Neural Networks Using Lessons From Deep Learning». Proc. IEEE 111 (9): 1016–54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nSi può anche guardare Video 10.2 linkato di seguito per una spiegazione più dettagliata.\n\n\n\n\n\n\nVideo 10.2: Neuromorphic Computing\n\n\n\n\n\n\nDispositivi nanoelettronici specializzati chiamati memristor (Chua 1971) sono componenti sinaptici nei sistemi neuromorfici. I memristor agiscono come memoria non volatile con conduttanza regolabile, emulando la plasticità delle sinapsi reali. I memristor consentono l’apprendimento in situ senza trasferimenti di dati separati combinando funzioni di memoria ed elaborazione. Tuttavia, la tecnologia dei memristor deve ancora raggiungere la maturità e la scalabilità per l’hardware commerciale.\n\nChua, L. 1971. «Memristor-The missing circuit element». #IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\nL’integrazione della fotonica con il calcolo neuromorfico (Shastri et al. 2021) è emersa di recente come un’area di ricerca attiva. L’uso della luce per il calcolo e la comunicazione consente alte velocità e un consumo energetico ridotto. Tuttavia, la piena realizzazione di sistemi neuromorfici fotonici richiede il superamento di problemi di progettazione e integrazione.\nIl calcolo neuromorfico offre promettenti capacità per un’efficace inferenza edge, ma incontra ostacoli in merito ad algoritmi di addestramento, integrazione dei nanodispositivi e progettazione del sistema. La ricerca multidisciplinare in corso in informatica, ingegneria, scienza dei materiali e fisica sarà fondamentale per sbloccare il pieno potenziale di questa tecnologia per i casi d’uso dell’intelligenza artificiale.\n\n\n10.8.3 Calcolo Analogico\nIl computing analogico è un approccio emergente che utilizza segnali e componenti analogici come condensatori, induttori e amplificatori anziché la logica digitale per il calcolo. Rappresenta le informazioni come segnali elettrici continui anziché 0 e 1 discreti. Ciò consente al calcolo di riflettere direttamente la natura analogica dei dati del mondo reale, evitando errori di digitalizzazione e overhead.\nIl computing analogico ha generato un rinnovato interesse per l’hardware AI efficiente, in particolare per l’inferenza direttamente su dispositivi edge a basso consumo. I circuiti analogici, come la moltiplicazione e la sommatoria al centro delle reti neurali, possono essere utilizzati con un consumo energetico molto basso. Ciò rende l’analogico adatto per l’implementazione di modelli ML su nodi finali con vincoli energetici. Startup come Mythic stanno sviluppando acceleratori AI analogici.\nMentre il computing analogico era popolare nei primi computer, il boom della logica digitale ha portato al suo declino. Tuttavia, l’analogico è convincente per applicazioni di nicchia che richiedono estrema efficienza (Haensch, Gokmen, e Puri 2019). Contrasta con gli approcci neuromorfici digitali che utilizzano ancora picchi digitali per il calcolo. L’analogico può consentire un calcolo di precisione inferiore, ma richiede competenza nella progettazione di circuiti analogici. I compromessi su precisione, complessità di programmazione e costi di fabbricazione rimangono aree di ricerca attive.\n\nHaensch, Wilfried, Tayfun Gokmen, e Ruchir Puri. 2019. «The Next Generation of Deep Learning Hardware: Analog Computing». Proc. IEEE 107 (1): 108–22. https://doi.org/10.1109/jproc.2018.2871057.\n\nHazan, Avi, e Elishai Ezra Tsur. 2021. «Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation». Front. Neurosci. 15 (febbraio): 627221. https://doi.org/10.3389/fnins.2021.627221.\nIl calcolo neuromorfico, che mira a emulare i sistemi neurali biologici per un’inferenza ML efficiente, può utilizzare circuiti analogici per implementare i componenti e i comportamenti chiave del cervello. Ad esempio, i ricercatori hanno progettato circuiti analogici per modellare neuroni e sinapsi utilizzando condensatori, transistor e amplificatori operazionali (Hazan e Ezra Tsur 2021). I condensatori possono esibire le dinamiche di picco dei neuroni biologici, mentre gli amplificatori e i transistor forniscono una somma ponderata di input per imitare i dendriti. Le tecnologie a resistore variabile come i memristor possono realizzare sinapsi analogiche con plasticità dipendente dal tempo di picco, che può rafforzare o indebolire le connessioni in base all’attività di picco.\nStartup come SynSense hanno sviluppato chip neuromorfici analogici contenenti questi componenti biomimetici (Bains 2020). Questo approccio analogico si traduce in un basso consumo energetico e un’elevata scalabilità per i dispositivi edge rispetto alle complesse implementazioni SNN digitali.\n\nBains, Sunny. 2020. «The business of building brains». Nature Electronics 3 (7): 348–51. https://doi.org/10.1038/s41928-020-0449-1.\nTuttavia, l’addestramento di SNN analogiche sui chip rimane una sfida aperta. Nel complesso, la realizzazione analogica è una tecnica promettente per fornire l’efficienza, la scalabilità e la plausibilità biologica previste con il calcolo neuromorfico. La fisica dei componenti analogici combinata con la progettazione dell’architettura neurale potrebbe migliorare l’efficienza dell’inferenza rispetto alle reti neurali digitali convenzionali.\n\n\n10.8.4 Elettronica Flessibile\nMentre gran parte della nuova tecnologia hardware nell’area di lavoro ML si è concentrata sull’ottimizzazione e sulla creazione di sistemi più efficienti, c’è una traiettoria parallela che mira ad adattare l’hardware per applicazioni specifiche (Gates 2009; Musk et al. 2019; Tang et al. 2023; Tang, He, e Liu 2022; Kwon e Dong 2022). Una di queste strade è lo sviluppo di elettronica flessibile per casi d’uso AI.\n\nGates, Byron D. 2009. «Flexible Electronics». Science 323 (5921): 1566–67. https://doi.org/10.1126/science.1171230.\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, e Jia Liu. 2023. «Flexible braincomputer interfaces». Nature Electronics 6 (2): 109–18. https://doi.org/10.1038/s41928-022-00913-9.\n\nTang, Xin, Yichun He, e Jia Liu. 2022. «Soft bioelectronics for cardiac interfaces». Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\nL’elettronica flessibile si riferisce a circuiti elettronici e dispositivi fabbricati su substrati flessibili in plastica o polimeri anziché in silicio rigido. A differenza delle schede e dei chip rigidi convenzionali, ciò consente all’elettronica di piegarsi, torcersi e adattarsi a forme irregolari. Figura 10.8 mostra un esempio di un prototipo di dispositivo flessibile che misura in modalità wireless la temperatura corporea, che può essere integrato senza soluzione di continuità in indumenti o cerotti cutanei. La flessibilità e la piegabilità dei materiali elettronici emergenti consentono di integrarli in fattori di forma sottili e leggeri, adatti per applicazioni AI e TinyML embedded.\nL’hardware AI flessibile può adattarsi a superfici curve e funzionare in modo efficiente con budget di potenza in microwatt. La flessibilità consente inoltre fattori di forma arrotolabili o pieghevoli per ridurre al minimo l’ingombro e il peso del dispositivo, ideali per piccoli dispositivi intelligenti portatili e dispositivi indossabili che incorporano TinyML. Un altro vantaggio fondamentale dell’elettronica flessibile rispetto alle tecnologie convenzionali sono i costi di produzione inferiori e i processi di fabbricazione più semplici, che potrebbero democratizzare l’accesso a queste tecnologie. Mentre le maschere in silicio e i costi di fabbricazione in genere costano milioni di dollari, l’hardware flessibile in genere costa solo decine di centesimi per la produzione (Huang et al. 2011; Biggs et al. 2021). Il potenziale di fabbricare elettronica flessibile direttamente su pellicole di plastica utilizzando processi di stampa e rivestimento ad alta produttività può ridurre i costi e migliorare la producibilità su larga scala rispetto ai chip AI rigidi (Musk et al. 2019).\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi Sekitani, Takao Someya, e Kwang-Ting Cheng. 2011. «Pseudo-CMOS: A Design Style for Low-Cost and Robust Flexible Electronics». IEEE Trans. Electron Devices 58 (1): 141–50. https://doi.org/10.1109/ted.2010.2088127.\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony Sou, Catherine Ramsdale, Ken Williamson, Richard Price, e Scott White. 2021. «A natively flexible 32-bit Arm microprocessor». Nature 595 (7868): 532–36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\n\n\n\n\nFigura 10.8: Prototipo di dispositivo flessibile. Fonte: Jabil Circuit.\n\n\n\nIl campo è abilitato dai progressi nei semiconduttori organici e nei nanomateriali che possono essere depositati su pellicole sottili e flessibili. Tuttavia, la fabbricazione rimane impegnativa rispetto ai processi maturi del silicio. I circuiti flessibili attualmente presentano in genere prestazioni inferiori rispetto agli equivalenti rigidi. Tuttavia, promettono di trasformare l’elettronica in materiali leggeri e pieghevoli.\nI casi d’uso dell’elettronica flessibile sono adatti per l’integrazione intima con il corpo umano. Le potenziali applicazioni dell’intelligenza artificiale medica includono sensori biointegrati, “soft robot” e impianti che monitorano o stimolano il sistema nervoso in modo intelligente. In particolare, gli array di elettrodi flessibili potrebbero consentire interfacce neurali a densità più elevata e meno invasive rispetto agli equivalenti rigidi.\nPertanto, l’elettronica flessibile sta inaugurando una nuova era di dispositivi indossabili e sensori corporei, in gran parte grazie alle innovazioni nei transistor organici. Questi componenti consentono un’elettronica più leggera e pieghevole, ideale per dispositivi indossabili, pelle elettronica e dispositivi medici che si adattano al corpo.\nSono adatti per dispositivi bioelettronici in termini di biocompatibilità, aprendo la strada ad applicazioni in interfacce cerebrali e cardiache. Ad esempio, la ricerca sulle interfacce flessibili cervello-computer e sulla bioelettronica morbida per applicazioni cardiache dimostra il potenziale per applicazioni mediche di vasta portata.\nAziende e istituti di ricerca non stanno solo sviluppando e investendo grandi quantità di risorse in elettrodi flessibili, come mostrato nel lavoro di Neuralink (Musk et al. 2019). Tuttavia, stanno anche spingendo i confini per integrare modelli di apprendimento automatico nei sistemi (Kwon e Dong 2022). Questi sensori intelligenti mirano a una simbiosi fluida e duratura con il corpo umano.\n\nMusk, Elon et al. 2019. «An Integrated Brain-Machine Interface Platform With Thousands of Channels». J. Med. Internet Res. 21 (10): e16194. https://doi.org/10.2196/16194.\n\nKwon, Sun Hwa, e Lin Dong. 2022. «Flexible sensors and machine learning for heart monitoring». Nano Energy 102 (novembre): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, e P. W. C. Prasad. 2017. «Ethical Implications of User Perceptions of Wearable Devices». Sci. Eng. Ethics 24 (1): 1–28. https://doi.org/10.1007/s11948-017-9872-8.\n\nGoodyear, Victoria A. 2017. «Social media, apps and wearable technologies: Navigating ethical dilemmas and procedures». Qualitative Research in Sport, Exercise and Health 9 (3): 285–302. https://doi.org/10.1080/2159676x.2017.1303790.\n\nFarah, Martha J. 2005. «Neuroethics: The practical and the philosophical». Trends Cogn. Sci. 9 (1): 34–40. https://doi.org/10.1016/j.tics.2004.12.001.\n\nRoskies, Adina. 2002. «Neuroethics for the New Millenium». Neuron 35 (1): 21–23. https://doi.org/10.1016/s0896-6273(02)00763-8.\nEticamente, l’incorporazione di sensori intelligenti basati sull’apprendimento automatico nel corpo solleva importanti questioni. Le problematiche relative alla privacy dei dati, al consenso informato e alle implicazioni sociali a lungo termine di tali tecnologie sono al centro del lavoro in corso in neuroetica e bioetica (Segura Anaya et al. 2017; Goodyear 2017; Farah 2005; Roskies 2002). Il campo sta progredendo a un ritmo che richiede progressi paralleli nei parametri etici per guidare lo sviluppo e l’implementazione responsabili di queste tecnologie. Sebbene vi siano limitazioni e ostacoli etici da superare, le prospettive per l’elettronica flessibile sono ampie e promettono molto per la ricerca e le applicazioni future.\n\n\n10.8.5 Tecnologie delle Memorie\nLe tecnologie delle memorie sono fondamentali per l’hardware AI, ma la DDR DRAM e la SRAM convenzionali creano colli di bottiglia. I carichi di lavoro AI richiedono un’elevata larghezza di banda (&gt;1 TB/s). Le applicazioni scientifiche estreme dell’AI richiedono una latenza estremamente bassa (&lt;50 ns) per alimentare i dati alle unità di calcolo (Duarte et al. 2022), un’elevata densità (&gt;128 Gb) per archiviare grandi parametri di modelli e set di dati e un’eccellente efficienza energetica (&lt;100 fJ/b) per uso embedded (Verma et al. 2019). Sono necessarie nuove memorie per soddisfare queste esigenze. Le opzioni emergenti includono diverse nuove tecnologie:\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, e Vijay Janapa Reddi. 2022. «FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning». ArXiv preprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay, Lung-Yen Chen, Bonan Zhang, e Peter Deaville. 2019. «In-Memory Computing: Advances and Prospects». IEEE Solid-State Circuits Mag. 11 (3): 43–55. https://doi.org/10.1109/mssc.2019.2922889.\n\nLa RAM resistiva (ReRAM) può migliorare la densità con semplici array passivi. Tuttavia, permangono dei problemi legati alla variabilità (Chi et al. 2016).\nLa “Phase change memory (PCM)” [memoria a cambiamento di fase ] sfrutta le proprietà uniche del vetro calcogenuro. Le fasi cristalline e amorfe hanno resistenze diverse. L’Optane DCPMM di Intel fornisce PCM veloci (100 ns) e ad alta resistenza. Tuttavia, le sfide includono cicli di scrittura limitati e corrente di reset elevata (Burr et al. 2016).\nLo stacking 3D può anche aumentare la densità di memoria e la larghezza di banda integrando verticalmente strati di memoria con interconnessioni TSV (Loh 2008). Ad esempio, HBM fornisce interfacce larghe 1024 bit.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng, Jau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. «Recent Progress in Phase-Change?Pub _newline ?Memory Technology». IEEE Journal on Emerging and Selected Topics in Circuits and Systems 6 (2): 146–62. https://doi.org/10.1109/jetcas.2016.2547718.\n\nLoh, Gabriel H. 2008. «3D-Stacked Memory Architectures for Multi-core Processors». ACM SIGARCH Computer Architecture News 36 (3): 453–64. https://doi.org/10.1145/1394608.1382159.\nLe nuove tecnologie di memoria, con le loro innovative architetture e materiali cellulari, sono fondamentali per sbloccare il prossimo livello di prestazioni ed efficienza hardware AI. Realizzare i loro vantaggi nei sistemi commerciali rimane una sfida continua.\nL’elaborazione in-memory sta guadagnando terreno come promettente strada per ottimizzare l’apprendimento automatico e i carichi di lavoro di elaborazione ad alte prestazioni. Al centro, la tecnologia colloca l’archiviazione e l’elaborazione dei dati per migliorare l’efficienza energetica e ridurre la latenza Wong et al. (2012). Due tecnologie chiave sotto questo ombrello sono la “Resistive RAM (ReRAM)” e il “Processing-In-Memory (PIM)”.\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu, Pang-Shiu Chen, Byoungil Lee, Frederick T. Chen, e Ming-Jinn Tsai. 2012. «MetalOxide RRAM». Proc. IEEE 100 (6): 1951–70. https://doi.org/10.1109/jproc.2012.2190369.\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, e Yuan Xie. 2016. «Prime: a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory». ACM SIGARCH Computer Architecture News 44 (3): 27–39. https://doi.org/10.1145/3007787.3001140.\nReRAM (Wong et al. 2012) e PIM (Chi et al. 2016) sono le colonne portanti per l’elaborazione in memoria, l’archiviazione e l’elaborazione dei dati nella stessa posizione. ReRAM si concentra su questioni di uniformità, resistenza, conservazione, funzionamento multi-bit e scalabilità. D’altro canto, PIM coinvolge unità CPU integrate direttamente in array di memoria, specializzate per attività come la moltiplicazione di matrici, che sono centrali nei calcoli AI.\nQueste tecnologie trovano applicazioni nei carichi di lavoro AI e nell’elaborazione ad alte prestazioni, dove la sinergia di storage e calcolo può portare a significativi guadagni in termini di prestazioni. L’architettura è particolarmente utile per le attività di elaborazione intensiva comuni nei modelli di apprendimento automatico.\nMentre le tecnologie di elaborazione in memoria come ReRAM e PIM offrono interessanti prospettive di efficienza e prestazioni, presentano le loro sfide, come l’uniformità dei dati e i problemi di scalabilità in ReRAM (Imani, Rahimi, e S. Rosing 2016). Tuttavia, il campo è maturo per l’innovazione e affrontare queste limitazioni può aprire nuove frontiere nell’AI e nell’elaborazione ad alte prestazioni.\n\nImani, Mohsen, Abbas Rahimi, e Tajana S. Rosing. 2016. «Resistive Configurable Associative Memory for Approximate Computing». In Proceedings of the 2016 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1327–32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\n10.8.6 Calcolo Ottico\nNell’accelerazione dell’intelligenza artificiale, un’area di interesse in rapida crescita risiede nelle nuove tecnologie che si discostano dai paradigmi tradizionali. Alcune tecnologie emergenti menzionate sopra, come l’elettronica flessibile, il calcolo in memoria o persino il calcolo neuromorfico, stanno per diventare realtà, date le loro innovazioni e applicazioni rivoluzionarie. Una delle frontiere promettenti e all’avanguardia della prossima generazione è la tecnologia del calcolo ottico H. Zhou et al. (2022). Aziende come [LightMatter] stanno aprendo la strada all’uso della fotonica per i calcoli, utilizzando così i fotoni al posto degli elettroni per la trasmissione e il calcolo dei dati.\n\nZhou, Hailong, Jianji Dong, Junwei Cheng, Wenchan Dong, Chaoran Huang, Yichen Shen, Qiming Zhang, et al. 2022. «Photonic matrix multiplication lights up photonic accelerator and beyond». Light: Science &amp; Applications 11 (1): 30. https://doi.org/10.1038/s41377-022-00717-8.\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H. P. Pernice, Harish Bhaskaran, C. D. Wright, e Paul R. Prucnal. 2021. «Photonics for artificial intelligence and neuromorphic computing». Nat. Photonics 15 (2): 102–14. https://doi.org/10.1038/s41566-020-00754-y.\nIl calcolo ottico utilizza fotoni e dispositivi fotonici anziché i tradizionali circuiti elettronici per il calcolo e l’elaborazione dei dati. Trae ispirazione dai collegamenti di comunicazione in fibra ottica che si basano sulla luce per un trasferimento dati rapido ed efficiente (Shastri et al. 2021). La luce può propagarsi con una perdita molto inferiore rispetto agli elettroni dei semiconduttori, consentendo vantaggi intrinseci in termini di velocità ed efficienza.\nAlcuni vantaggi specifici dell’elaborazione ottica includono:\n\nAlta produttività: I fotoni possono trasmettere con larghezze di banda &gt;100 Tb/s utilizzando il multiplexing a divisione di lunghezza d’onda.\nBassa latenza: I fotoni interagiscono su scale temporali di femtosecondi, milioni di volte più velocemente dei transistor al silicio.\nParallelismo: Più segnali di dati possono propagarsi simultaneamente attraverso lo stesso mezzo ottico.\nBassa potenza: I circuiti fotonici che utilizzano guide d’onda e risonatori possono ottenere una logica e una memoria complesse con solo microwatt di potenza.\n\nTuttavia, l’elaborazione ottica deve attualmente affrontare sfide significative:\n\nMancanza di memoria ottica equivalente alla RAM elettronica\nRichiede la conversione tra domini ottici ed elettrici.\nSet limitato di componenti ottici disponibili rispetto al ricco ecosistema elettronico.\nMetodi di integrazione immaturi per combinare la fotonica con i tradizionali chip CMOS.\nModelli di programmazione complessi richiesti per gestire il parallelismo.\n\nDi conseguenza, l’elaborazione ottica è ancora in una fase di ricerca molto precoce nonostante il suo potenziale promettente. Tuttavia, le innovazioni tecniche potrebbero consentirgli di integrare l’elettronica e sbloccare guadagni di prestazioni per i carichi di lavoro AI. Aziende come Lightmatter sono pioniere nei primi acceleratori ottici AI. A lungo termine, se le sfide chiave saranno superate, potrebbe rappresentare un substrato di elaborazione rivoluzionario.\n\n\n10.8.7 Quantum Computing\nI computer quantistici sfruttano fenomeni unici della fisica quantistica, come la sovrapposizione e l’entanglement, per rappresentare ed elaborare informazioni in modi non possibili in modo classico. Invece dei bit binari, l’unità fondamentale è il bit quantistico o qubit. A differenza dei bit classici, che sono limitati a 0 o 1, i qubit possono esistere simultaneamente in una sovrapposizione di entrambi gli stati a causa degli effetti quantistici.\nAnche più qubit possono essere entangled, portando a una densità di informazioni esponenziale ma introducendo risultati probabilistici. La sovrapposizione consente il calcolo parallelo su tutti gli stati possibili, mentre l’entanglement consente correlazioni non locali tra qubit.\nGli algoritmi quantistici manipolano attentamente questi effetti meccanici quantistici intrinseci per risolvere problemi come l’ottimizzazione o la ricerca in modo più efficiente rispetto alle loro controparti classiche in teoria.\n\nTraining più rapido di reti neurali profonde sfruttando il parallelismo quantistico per operazioni di algebra lineare.\nGli algoritmi ML quantistici efficienti sfruttano le capacità uniche dei qubit.\nReti neurali quantistiche con effetti quantistici intrinseci integrati nell’architettura del modello.\nOttimizzatori quantistici che sfruttano algoritmi di “annealing” quantistica o adiabatici per problemi di ottimizzazione combinatoria.\n\nTuttavia, gli stati quantistici sono fragili e soggetti a errori che richiedono protocolli di correzione degli errori. La natura non intuitiva della programmazione quantistica introduce anche sfide non presenti nell’informatica classica.\n\nI bit quantistici rumorosi e fragili sono difficili da scalare. Il più grande computer quantistico odierno ha meno di 1000 qubit.\nInsieme limitato di porte e circuiti quantistici disponibili rispetto alla programmazione classica.\nMancanza di set di dati e benchmark per valutare l’apprendimento automatico quantistico in domini pratici.\n\nSebbene un vantaggio quantistico significativo per l’apprendimento automatico sia ancora lontano, la ricerca attiva presso aziende come D-Wave, Rigetti e IonQ sta facendo progredire l’ingegneria informatica quantistica e gli algoritmi quantistici. Le principali aziende tecnologiche come Google, IBM e Microsoft stanno esplorando attivamente l’informatica quantistica. Google ha recentemente annunciato un processore quantistico a 72 qubit chiamato Bristlecone e prevede di costruire un sistema quantistico commerciale a 49 qubit. Microsoft ha anche un programma di ricerca attivo nell’informatica quantistica topologica e collabora con la startup quantistica IonQ\nLe tecniche quantistiche potrebbero prima fare breccia nell’ottimizzazione prima di un’adozione più generalizzata dell’apprendimento automatico. La realizzazione del pieno potenziale dell’apprendimento automatico quantistico attende importanti traguardi nello sviluppo dell’hardware quantistico e nella maturità dell’ecosistema.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#tendenze-future",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#tendenze-future",
    "title": "10  Accelerazione IA",
    "section": "10.9 Tendenze Future",
    "text": "10.9 Tendenze Future\nIn questo capitolo, l’attenzione principale è stata rivolta alla progettazione di hardware specializzato ottimizzato per carichi di lavoro e algoritmi di machine learning. Questa discussione ha riguardato le architetture personalizzate di GPU e TPU per l’addestramento e l’inferenza delle reti neurali. Tuttavia, una direzione di ricerca emergente sta sfruttando l’apprendimento automatico per facilitare il processo di progettazione hardware stesso.\nIl processo di progettazione hardware comprende molte fasi complesse, tra cui specifica, modellazione di alto livello, simulazione, sintesi, verifica, prototipazione e fabbricazione. Gran parte di questo processo richiede tradizionalmente una vasta competenza umana, impegno e tempo. Tuttavia, i recenti progressi nell’apprendimento automatico stanno consentendo l’automazione e il miglioramento di parti del flusso di lavoro di progettazione hardware utilizzando tecniche di apprendimento automatico.\nEcco alcuni esempi di come l’apprendimento automatico sta trasformando la progettazione hardware:\n\nSintesi di circuiti automatizzata tramite apprendimento per rinforzo: Anziché realizzare manualmente progetti a livello di transistor, gli agenti di apprendimento automatico come l’apprendimento per rinforzo possono imparare a collegare porte logiche e generare automaticamente layout di circuiti. Ciò può accelerare il lungo processo di sintesi.\nSimulazione ed emulazione hardware basate su ML: I modelli di reti neurali profonde possono essere addestrati per prevedere come si comporterà un progetto hardware in diverse condizioni. Ad esempio, i modelli di apprendimento profondo possono essere addestrati per prevedere i conteggi dei cicli per determinati carichi di lavoro. Ciò consente una simulazione più rapida e accurata rispetto alle simulazioni RTL tradizionali.\nPianificazione automatizzata dei chip mediante algoritmi ML: La pianificazione dei chip comporta il posizionamento ottimale di diversi componenti su un die. Algoritmi evolutivi come quelli genetici e altri algoritmi ML come l’apprendimento per rinforzo vengono utilizzati per esplorare le opzioni di pianificazione. Ciò può migliorare significativamente i posizionamenti manuali di pianificazione in termini di tempi di consegna più rapidi e qualità dei posizionamenti.\nOttimizzazione dell’architettura basata su ML: Le nuove architetture hardware, come quelle per gli acceleratori ML efficienti, possono essere generate e ottimizzate automaticamente tramite la ricerca nello spazio di progettazione architettonica. Gli algoritmi di apprendimento automatico possono cercare efficacemente ampi spazi di progettazione architettonica.\n\nL’applicazione del ML all’automazione della progettazione hardware promette di rendere il processo più veloce, più economico e più efficiente. Apre possibilità di progettazione che richiederebbero più di una progettazione manuale. L’uso del ML nella progettazione hardware è un’area di ricerca attiva e di distribuzione precoce, e studieremo le tecniche coinvolte e il loro potenziale trasformativo.\n\n10.9.1 ML per l’automazione della progettazione hardware\nUna grande opportunità per l’apprendimento automatico nella progettazione hardware è l’automazione di parti del complesso e noioso flusso di lavoro di progettazione. Con “Hardware design automation (HDA)” ci si riferisce in generale all’uso di tecniche ML come l’apprendimento per rinforzo, algoritmi genetici e reti neurali per automatizzare attività come sintesi, verifica, floorplanning e altro. Ecco alcuni esempi di dove l’ML per HDA mostra una vera promessa:\n\nSintesi di circuiti automatizzata: La sintesi di circuiti comporta la conversione di una descrizione di alto livello della logica desiderata in un’implementazione di netlist a livello di gate ottimizzata. Questo processo complesso ha molte considerazioni e compromessi di progettazione. Gli agenti ML possono essere addestrati tramite l’apprendimento per rinforzo G. Zhou e Anderson (2023) per esplorare lo spazio di progettazione e produrre automaticamente sintesi ottimizzate. Startup come Symbiotic EDA stanno portando questa tecnologia sul mercato.\nAutomated chip floorplanning: Il Floorplanning si riferisce al posizionamento strategico di diversi componenti su un’area del chip. Algoritmi di ricerca come algoritmi genetici (Valenzuela e Wang 2000) e apprendimento per rinforzo (Mirhoseini et al. (2021), Agnesina et al. (2023)) possono essere utilizzati per automatizzare l’ottimizzazione il floorplan per ridurre al minimo la lunghezza dei collegamenti, il consumo di energia e altri obiettivi. Questi “floor planners” assistiti da ML automatizzati sono estremamente preziosi man mano che aumenta la complessità dei chip.\nSimulatori hardware ML: L’addestramento di modelli di reti neurali profonde per prevedere le prestazioni dei progetti hardware, poiché i simulatori possono accelerare il processo di simulazione di oltre 100 volte rispetto alle simulazioni architettoniche e RTL tradizionali.\nTraduzione automatica del codice: La conversione di linguaggi di descrizione hardware come Verilog in implementazioni RTL ottimizzate è fondamentale ma richiede molto tempo. I modelli ML possono essere addestrati per agire come agenti traduttori e automatizzare questo processo.\n\n\nZhou, Guanglei, e Jason H. Anderson. 2023. «Area-Driven FPGA Logic Synthesis Using Reinforcement Learning». In Proceedings of the 28th Asia and South Pacific Design Automation Conference, 159–65. ACM. https://doi.org/10.1145/3566097.3567894.\n\nValenzuela, Christine L, e Pearl Y Wang. 2000. «A genetic algorithm for VLSI floorplanning». In Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 1820, 2000 Proceedings 6, 671–80. Springer.\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. «A graph placement methodology for fast chip design». Nature 594 (7862): 207–12. https://doi.org/10.1038/s41586-021-03544-w.\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta, Austin Jiao, Ben Keller, Brucek Khailany, e Haoxing Ren. 2023. «AutoDMP: Automated DREAMPlace-based Macro Placement». In Proceedings of the 2023 International Symposium on Physical Design, 149–57. ACM. https://doi.org/10.1145/3569052.3578923.\nI vantaggi dell’HDA che utilizza ML sono tempi di progettazione ridotti, ottimizzazioni superiori ed esplorazione di spazi di progettazione troppo complessi per approcci manuali. Ciò può accelerare lo sviluppo hardware e portare a progetti migliori.\nLe sfide includono i limiti della generalizzazione ML, la natura black-box di alcune tecniche e compromessi sull’accuratezza. Tuttavia, la ricerca sta rapidamente avanzando per affrontare questi problemi e rendere le soluzioni HDA ML robuste e affidabili per l’uso in produzione. HDA fornisce un’importante via per ML per trasformare la progettazione hardware.\n\n\n10.9.2 Simulazione e Verifica Hardware Basate su ML\nLa simulazione e la verifica dei progetti hardware sono fondamentali prima della produzione per garantire che il progetto si comporti come previsto. Gli approcci tradizionali come la simulazione “register-transfer level” (RTL) sono complessi e richiedono molto tempo. Il ML introduce nuove opportunità per migliorare la simulazione e la verifica dell’hardware. Ecco alcuni esempi:\n\nModellazione surrogata per la simulazione: Modelli surrogati di un progetto altamente accurati possono essere creati utilizzando reti neurali. Questi modelli prevedono gli output dagli input molto più velocemente della simulazione RTL, consentendo una rapida esplorazione dello spazio di progettazione. Aziende come Ansys utilizzano questa tecnica.\nSimulatori ML: Grandi modelli di reti neurali possono essere addestrati su simulazioni RTL per imparare a imitare la funzionalità di un progetto hardware. Una volta addestrato, il modello NN può essere un simulatore altamente efficiente per test di regressione e altre attività. Graphcore ha dimostrato un’accelerazione di oltre 100 volte con questo approccio.\nVerifica formale tramite ML: La verifica formale dimostra matematicamente le proprietà di un progetto. Le tecniche di ML possono aiutare a generare proprietà di verifica e imparare a risolvere le complesse prove formali necessarie, automatizzando parti di questo processo impegnativo. Startup come Cortical.io stanno introducendo sul mercato soluzioni di verifica ML formali.\nRilevamento di bug: I modelli ML possono essere addestrati per elaborare progetti hardware e identificare potenziali problemi. Ciò aiuta i progettisti umani a ispezionare progetti complessi e a trovare bug. Facebook ha mostrato modelli di rilevamento di bug per l’hardware dei suoi server.\n\nI principali vantaggi dell’applicazione di ML alla simulazione e alla verifica sono tempi di esecuzione più rapidi per la convalida del progetto, test più rigorosi e riduzione del lavoro umano. Le sfide includono la verifica della correttezza del modello ML e la gestione dei casi limite. ML promette di accelerare significativamente i flussi di lavoro di test.\n\n\n10.9.3 ML per Architetture Hardware Efficienti\nUn obiettivo chiave è la progettazione di architetture hardware ottimizzate per prestazioni, potenza ed efficienza. ML introduce nuove tecniche per automatizzare e migliorare l’esplorazione dello spazio di progettazione dell’architettura per hardware generico e specializzato come gli acceleratori ML. Alcuni esempi promettenti sono:\n\nRicerca di architetture per hardware: Tecniche di ricerca come algoritmi evolutivi (Kao e Krishna 2020), ottimizzazione bayesiana (Reagen et al. (2017), Bhardwaj et al. (2020)), apprendimento per rinforzo (Kao, Jeong, e Krishna (2020), Krishnan et al. (2022)) possono generare automaticamente nuove architetture hardware mutando e mescolando attributi di progettazione come dimensione della cache, numero di unità parallele, larghezza di banda della memoria e così via. Ciò consente un’esplorazione efficiente di ampi spazi di progettazione.\nModellazione predittiva per l’ottimizzazione: I modelli ML possono essere addestrati per prevedere metriche di prestazioni, potenza ed efficienza hardware per una determinata architettura. Questi diventano “modelli surrogati” (Krishnan et al. 2023) per una rapida ottimizzazione ed esplorazione dello spazio sostituendo lunghe simulazioni.\nOttimizzazione dell’acceleratore specializzato: Per chip specializzati come unità di elaborazione tensore per AI, tecniche di ricerca architettura automatizzata basate su algoritmi ML (D. Zhang et al. 2022) promettono di trovare progetti rapidi ed efficienti.\n\n\nKao, Sheng-Chun, e Tushar Krishna. 2020. «Gamma: automating the HW mapping of DNN models on accelerators via genetic algorithm». In Proceedings of the 39th International Conference on Computer-Aided Design, 1–9. ACM. https://doi.org/10.1145/3400302.3415639.\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, e David Brooks. 2017. «A case for efficient accelerator design space exploration via Bayesian optimization». In 2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 1–6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel Hernández-Lobato, e Gu-Yeon Wei. 2020. «A comprehensive methodology to determine optimal coherence interfaces for many-accelerator SoCs». In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design, 145–50. ACM. https://doi.org/10.1145/3370748.3406564.\n\nKao, Sheng-Chun, Geonhwa Jeong, e Tushar Krishna. 2020. «ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning». In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 622–36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang, Izzeddin Gur, Vijay Janapa Reddi, e Aleksandra Faust. 2022. «Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration». https://arxiv.org/abs/2211.16385.\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, e Azalia Mirhoseini. 2022. «A full-stack search technique for domain optimized deep learning accelerators». In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 27–42. ASPLOS ’22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\nI vantaggi dell’utilizzo di ML includono un’esplorazione dello spazio di progettazione superiore, ottimizzazione automatizzata e riduzione dello sforzo manuale. Le sfide includono lunghi tempi di training per alcune tecniche e limitazioni degli ottimi locali. Tuttavia, ML per l’architettura hardware ha un grande potenziale per rivelare miglioramenti in termini di prestazioni ed efficienza.\n\n\n10.9.4 ML per Ottimizzare la Produzione e Ridurre i Difetti\nUna volta completata la progettazione hardware, si passa alla produzione. Tuttavia, variabilità e difetti durante la produzione possono influire su rese e qualità. Le tecniche ML vengono ora applicate per migliorare i processi di fabbricazione e ridurre i difetti. Ecco alcuni esempi:\n\nManutenzione predittiva: I modelli ML possono analizzare i dati dei sensori delle apparecchiature nel tempo e identificare segnali che prevedono le esigenze di manutenzione prima del guasto. Ciò consente una manutenzione proattiva, che può essere molto utile nel costoso processo di fabbricazione.\nOttimizzazione del processo: I modelli di apprendimento supervisionato possono essere addestrati sui dati di processo per identificare i fattori che portano a basse rese. I modelli possono quindi ottimizzare i parametri per migliorare rese, produttività o coerenza.\nPrevisione della resa: Analizzando i dati di prova da progetti realizzati utilizzando tecniche come alberi di regressione, i modelli ML possono prevedere le rese all’inizio della produzione, consentendo aggiustamenti del processo.\nRilevamento dei difetti: Le tecniche di visione artificiale ML possono essere applicate alle immagini dei progetti per identificare difetti invisibili all’occhio umano. Ciò consente un controllo di qualità di precisione e un’analisi delle cause principali.\nAnalisi proattiva dei guasti: I modelli ML possono aiutare a prevedere, diagnosticare e prevenire i problemi che portano a difetti e guasti a valle analizzando i dati di processo strutturati e non strutturati.\n\nL’applicazione del ML alla produzione consente l’ottimizzazione dei processi, il controllo di qualità in tempo reale, la manutenzione predittiva e rese più elevate. Le sfide includono la gestione di dati di produzione complessi e varianti. Ma il ML è pronto a trasformare la produzione di semiconduttori.\n\n\n10.9.5 Verso Modelli di Base per la Progettazione Hardware\nCome abbiamo visto, l’apprendimento automatico sta aprendo nuove possibilità nel flusso di lavoro di progettazione hardware, dalle specifiche alla produzione. Tuttavia, le attuali tecniche di ML hanno ancora una portata limitata e richiedono un’ampia progettazione specifica per dominio. La visione a lungo termine è lo sviluppo di sistemi di intelligenza artificiale generali che possono essere applicati con versatilità in tutte le attività di progettazione hardware.\nPer realizzare appieno questa visione, sono necessari investimenti e ricerca per sviluppare modelli di base per la progettazione hardware. Si tratta di modelli e architetture ML unificati e generici che possono apprendere complesse competenze di progettazione hardware con i dati di training e gli obiettivi corretti.\nLa realizzazione di modelli di base per la progettazione hardware end-to-end richiederà quanto segue:\n\nAccumulare grandi set di dati di alta qualità ed etichettati in tutte le fasi di progettazione hardware per addestrare i modelli di base.\nProgressi nelle tecniche ML multimodali e multi-task per gestire la diversità di dati e attività di progettazione hardware.\nInterfacce e layer di astrazione per collegare i modelli di base ai flussi e agli strumenti di progettazione esistenti.\nSviluppo di ambienti di simulazione e benchmark per addestrare e testare i modelli di base sulle capacità di progettazione hardware.\nMetodi per spiegare e interpretare le decisioni di progettazione dei modelli ML e le ottimizzazioni per attendibilità e verifica.\nTecniche di compilazione per ottimizzare i modelli di base per un’implementazione efficiente su piattaforme hardware.\n\nSebbene siano ancora in corso ricerche significative, i modelli di base rappresentano l’obiettivo a lungo termine più trasformativo per l’infusione dell’IA nel processo della progettazione hardware. Democratizzare la progettazione hardware tramite sistemi ML versatili e automatizzati promette di aprire una nuova era di progettazione di chip ottimizzata, efficiente e innovativa. Il viaggio che ci attende è pieno di sfide e opportunità aperte.\nSe sei interessato alla progettazione di architetture per computer assistite da ML (Krishnan et al. 2023), invitiamo a leggere Architecture 2.0.\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023. «ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design». In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–16. ACM. https://doi.org/10.1145/3579371.3589049.\nIn alternativa, si può guardare Video 10.3 for more details.\n\n\n\n\n\n\nVideo 10.3: Architecture 2.0",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#conclusione",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#conclusione",
    "title": "10  Accelerazione IA",
    "section": "10.10 Conclusione",
    "text": "10.10 Conclusione\nL’accelerazione hardware specializzata è diventata indispensabile per abilitare applicazioni di intelligenza artificiale performanti ed efficienti, poiché modelli e set di dati esplodono in complessità. Questo capitolo ha esaminato i limiti dei processori generici come le CPU per i carichi di lavoro di intelligenza artificiale. La loro mancanza di parallelismo e di throughput computazionale non consente di addestrare o eseguire rapidamente reti neurali profonde all’avanguardia. Queste motivazioni hanno guidato le innovazioni negli acceleratori personalizzati.\nAbbiamo esaminato GPU, TPU, FPGA e ASIC progettati specificamente per le operazioni matematiche intensive inerenti alle reti neurali. Coprendo questo spettro di opzioni, abbiamo mirato a fornire un framework per ragionare attraverso la selezione dell’acceleratore in base a vincoli relativi a flessibilità, prestazioni, potenza, costi e altri fattori.\nAbbiamo anche esplorato il ruolo del software nell’abilitazione e nell’ottimizzazione attive dell’accelerazione dell’intelligenza artificiale. Ciò abbraccia astrazioni di programmazione, framework, compilatori e simulatori. Abbiamo discusso della progettazione congiunta hardware-software come metodologia proattiva per la creazione di sistemi di intelligenza artificiale più olistici integrando strettamente l’innovazione degli algoritmi e i progressi hardware.\nMa c’è molto di più in arrivo! Frontiere entusiasmanti come l’informatica analogica, le reti neurali ottiche e l’apprendimento automatico quantistico rappresentano direzioni di ricerca attive che potrebbero sbloccare miglioramenti di ordini di grandezza in termini di efficienza, velocità e scala rispetto ai paradigmi attuali.\nIn definitiva, l’accelerazione hardware specializzata rimane indispensabile per sbloccare le prestazioni e l’efficienza necessarie per soddisfare la promessa dell’intelligenza artificiale dal cloud all’edge. Ci auguriamo che questo capitolo fornisca utili informazioni di base e approfondimenti sulla rapida innovazione che si sta verificando in questo dominio.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/hw_acceleration/hw_acceleration.it.html#sec-ai-acceleration-resource",
    "href": "contents/hw_acceleration/hw_acceleration.it.html#sec-ai-acceleration-resource",
    "title": "10  Accelerazione IA",
    "section": "10.11 Risorse",
    "text": "10.11 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 10.1\nVideo 10.2\nVideo 10.3\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\n\nEsercizio 10.1\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\n\nProssimamente.",
    "crumbs": [
      "Training",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Accelerazione IA</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html",
    "href": "contents/benchmarking/benchmarking.it.html",
    "title": "11  Benchmarking AI",
    "section": "",
    "text": "11.1 Introduzione\nIl benchmarking fornisce le misure essenziali necessarie per guidare il progresso dell’apprendimento automatico e comprendere veramente le prestazioni del sistema. Come ha affermato il fisico Lord Kelvin, “Misurare è conoscere”. I benchmark ci consentono di conoscere quantitativamente le capacità di diversi modelli, software e hardware. Consentono agli sviluppatori di ML di misurare il tempo di inferenza, l’utilizzo della memoria, il consumo energetico e altre metriche che caratterizzano un sistema. Inoltre, i benchmark creano processi standardizzati per la misurazione, consentendo confronti equi tra diverse soluzioni.\nQuando i benchmark vengono mantenuti nel tempo, diventano fondamentali per catturare i progressi attraverso generazioni di algoritmi, set di dati e hardware. I modelli e le tecniche che stabiliscono nuovi record sui benchmark di ML da un anno all’altro dimostrano miglioramenti tangibili in ciò che è possibile per l’apprendimento automatico “on-device”. Utilizzando i benchmark per misurare, i professionisti di ML possono conoscere le capacità reali dei loro sistemi e avere la certezza che ogni passaggio rifletta un progresso autentico verso lo stato dell’arte.\nIl benchmarking ha diversi obiettivi e scopi importanti che guidano la sua implementazione per i sistemi di apprendimento automatico.\nQuesto capitolo tratterà i 3 tipi di benchmark AI, le metriche standard, gli strumenti e le tecniche che i progettisti utilizzano per ottimizzare i loro sistemi e le sfide e le tendenze nel benchmarking.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#sec-benchmarking-ai",
    "href": "contents/benchmarking/benchmarking.it.html#sec-benchmarking-ai",
    "title": "11  Benchmarking AI",
    "section": "",
    "text": "Valutazione delle prestazioni. Ciò comporta la valutazione di parametri chiave come la velocità, l’accuratezza e l’efficienza di un dato modello. Ad esempio, in un contesto TinyML, è fondamentale confrontare la rapidità con cui un assistente vocale può riconoscere i comandi, poiché ciò valuta le prestazioni in tempo reale.\nValutazione delle risorse. Ciò significa valutare l’impatto del modello sulle risorse critiche del sistema, tra cui durata della batteria, utilizzo della memoria e sovraccarico computazionale. Un esempio rilevante è il confronto del consumo della batteria di due diversi algoritmi di riconoscimento delle immagini in esecuzione su un dispositivo indossabile.\nValidazione e verifica. Il benchmarking aiuta a garantire che il sistema funzioni correttamente e soddisfi i requisiti specificati. Un modo è quello di controllare l’accuratezza di un algoritmo, come un cardiofrequenzimetro su uno smartwatch, rispetto alle letture di apparecchiature di livello medico come forma di validazione clinica.\nAnalisi competitiva. Ciò consente di confrontare le soluzioni con le offerte concorrenti sul mercato. Ad esempio, il benchmarking di un modello personalizzato di rilevamento di oggetti rispetto ai benchmark TinyML comuni come MobileNet e Tiny-YOLO.\nCredibilità. I benchmark accurati sostengono la credibilità delle soluzioni AI e delle organizzazioni che le sviluppano. Dimostrano un impegno verso trasparenza, onestà e qualità, essenziali per creare fiducia con utenti e stakeholder.\nRegolamentazione e Standardizzazione. Man mano che il settore dell’AI continua a crescere, cresce anche la necessità di regolamentazione e standardizzazione per garantire che le soluzioni AI siano sicure, etiche ed efficaci. I benchmark accurati e affidabili sono essenziali per questo quadro normativo, poiché forniscono i dati e le prove necessari per valutare la conformità con gli standard del settore e i requisiti legali.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#contesto-storico",
    "href": "contents/benchmarking/benchmarking.it.html#contesto-storico",
    "title": "11  Benchmarking AI",
    "section": "11.2 Contesto Storico",
    "text": "11.2 Contesto Storico\n\n11.2.1 Benchmark Standard\nL’evoluzione dei benchmark nell’informatica illustra vividamente l’incessante ricerca dell’eccellenza e dell’innovazione da parte del settore. Nei primi giorni dell’informatica, negli anni ’60 e ’70, i benchmark erano rudimentali e progettati per i mainframe. Ad esempio, il benchmark Whetstone, che prende il nome dal compilatore Whetstone ALGOL, è stato uno dei primi test standardizzati per misurare le prestazioni aritmetiche in virgola mobile di una CPU. Questi benchmark pionieristici hanno spinto i produttori a perfezionare le loro architetture e algoritmi per ottenere punteggi di benchmark migliori.\nGli anni ’80 hanno segnato un cambiamento significativo con l’ascesa dei personal computer. Mentre aziende come IBM, Apple e Commodore gareggiavano per quote di mercato, i benchmark sono diventati strumenti essenziali per consentire una concorrenza leale. I benchmark CPU SPEC, introdotti dalla System Performance Evaluation Cooperative (SPEC), hanno stabilito test standardizzati che consentono confronti oggettivi tra diverse macchine. Questa standardizzazione ha creato un ambiente competitivo, spingendo i produttori di chip e i creatori di sistemi a migliorare continuamente le loro offerte hardware e software.\nGli anni ’90 hanno portato l’era delle applicazioni e dei videogiochi “graphics-intensive”. La necessità di benchmark per valutare le prestazioni delle schede grafiche ha portato alla creazione di 3DMark da parte di Futuremark. Mentre i giocatori e i professionisti cercavano schede grafiche ad alte prestazioni, aziende come NVIDIA e AMD sono state spinte a una rapida innovazione, portando a importanti progressi nella tecnologia GPU come gli shader programmabili.\nGli anni 2000 hanno visto un’impennata di telefoni cellulari e dispositivi portatili come i tablet. Con la portabilità è arrivata la sfida di bilanciare prestazioni e consumo energetico. Benchmark come MobileMark di BAPCo hanno valutato velocità e durata della batteria. Ciò ha spinto le aziende a sviluppare System-on-Chip (SOC) più efficienti dal punto di vista energetico, portando all’emergere di architetture come ARM che hanno dato priorità all’efficienza energetica.\nL’attenzione dell’ultimo decennio si è spostata verso il cloud computing, i big data e l’intelligenza artificiale. I provider di servizi cloud come Amazon Web Services e Google Cloud competono su prestazioni, scalabilità e convenienza. I benchmark specifici del cloud come CloudSuite sono diventati essenziali, spingendo i provider a ottimizzare la propria infrastruttura per servizi migliori.\n\n\n11.2.2 Benchmark Personalizzati\nOltre ai benchmark standard del settore, ci sono benchmark personalizzati specificamente progettati per soddisfare i requisiti unici di una particolare applicazione o attività. Sono personalizzati in base alle esigenze specifiche dell’utente o dello sviluppatore, assicurando che le metriche delle prestazioni siano direttamente pertinenti all’uso previsto del modello o del sistema di intelligenza artificiale. I benchmark personalizzati possono essere creati da singole organizzazioni, ricercatori o sviluppatori e sono spesso utilizzati insieme ai benchmark standard del settore per fornire una valutazione completa delle prestazioni dell’intelligenza artificiale.\nAd esempio, un ospedale potrebbe sviluppare un benchmark per valutare un modello di intelligenza artificiale per prevedere la riammissione dei pazienti. Questo benchmark incorporerebbe metriche pertinenti alla popolazione di pazienti dell’ospedale, come dati demografici, anamnesi e fattori sociali. Allo stesso modo, il benchmark di rilevamento delle frodi di un istituto finanziario potrebbe concentrarsi sull’identificazione accurata delle transazioni fraudolente riducendo al minimo i falsi positivi. Nel settore automobilistico, un benchmark di veicoli autonomi potrebbe dare priorità alle prestazioni in diverse condizioni, alla risposta agli ostacoli e alla sicurezza. I rivenditori potrebbero confrontare i sistemi di raccomandazione utilizzando il tasso di clic, il tasso di conversione e la soddisfazione del cliente. Le aziende manifatturiere potrebbero confrontare i sistemi di controllo qualità in base all’identificazione dei difetti, all’efficienza e alla riduzione degli sprechi. In ogni settore, i benchmark personalizzati forniscono alle organizzazioni criteri di valutazione su misura per le loro esigenze e il loro contesto unici. Ciò consente una valutazione più significativa di quanto i sistemi di intelligenza artificiale soddisfino i requisiti.\nIl vantaggio dei benchmark personalizzati risiede nella loro flessibilità e pertinenza. Possono essere progettati per testare aspetti specifici delle prestazioni critici per il successo della soluzione di intelligenza artificiale nella sua applicazione prevista. Ciò consente una valutazione più mirata e accurata delle capacità del modello o del sistema di intelligenza artificiale. I benchmark personalizzati forniscono anche informazioni preziose sulle prestazioni delle soluzioni di intelligenza artificiale in scenari reali, il che può essere cruciale per identificare potenziali problemi e aree di miglioramento.\nNell’intelligenza artificiale, i benchmark svolgono un ruolo cruciale nel guidare il progresso e l’innovazione. Sebbene i benchmark siano stati a lungo utilizzati nell’informatica, la loro applicazione all’apprendimento automatico è relativamente recente. I benchmark incentrati sull’intelligenza artificiale forniscono metriche standardizzate per valutare e confrontare le prestazioni di diversi algoritmi, architetture di modelli e piattaforme hardware.\n\n\n11.2.3 Consenso della Comunità\nUna prerogativa fondamentale affinché un benchmark abbia un impatto è che deve riflettere le priorità e i valori condivisi della più ampia comunità di ricerca. I benchmark progettati in modo isolato rischiano di non ottenere accettazione se trascurano metriche chiave considerate importanti dai gruppi leader. Attraverso uno sviluppo collaborativo con la partecipazione aperta di laboratori accademici, aziende e altri stakeholder, i benchmark possono incorporare un contributo collettivo su capacità critiche che vale la pena misurare. Ciò aiuta a garantire che i benchmark valutino aspetti che la comunità concorda siano essenziali per far progredire il campo. Il processo di raggiungimento dell’allineamento su attività e metriche supporta di per sé la convergenza su ciò che conta di più.\nInoltre, i benchmark pubblicati con ampia co-paternità da istituzioni rispettate hanno autorità e validità che convincono la comunità ad adottarli come standard affidabili. I benchmark percepiti come distorti da particolari interessi aziendali o istituzionali generano scetticismo. Anche il coinvolgimento continuo della comunità attraverso workshop e sfide è fondamentale dopo la versione iniziale, ed è ciò che, ad esempio, ha portato al successo di ImageNet. Col progredire della ricerca, la partecipazione collettiva consente un continuo perfezionamento ed espansione dei benchmark nel tempo.\nInfine, i benchmark sviluppati dalla comunità rilasciati con accesso aperto accelerano l’adozione e l’implementazione coerente. Abbiamo condiviso codice open source, documentazione, modelli e infrastrutture per ridurre le barriere che impediscono ai gruppi di confrontare le soluzioni su un piano di parità utilizzando implementazioni standardizzate. Questa coerenza è fondamentale per confronti equi. Senza coordinamento, laboratori e aziende potrebbero implementare i benchmark in modo diverso, riducendo la riproducibilità dei risultati.\nIl consenso della comunità conferisce ai benchmark una rilevanza duratura, mentre la frammentazione confonde. Attraverso lo sviluppo collaborativo e un funzionamento trasparente, i benchmark possono diventare standard autorevoli per monitorare i progressi. Molti dei benchmark di cui parliamo in questo capitolo sono stati sviluppati e creati dalla comunità, per la comunità, ed è questo che alla fine ha portato al loro successo.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmark-ai-sistema-modello-e-dati",
    "href": "contents/benchmarking/benchmarking.it.html#benchmark-ai-sistema-modello-e-dati",
    "title": "11  Benchmarking AI",
    "section": "11.3 Benchmark AI: Sistema, Modello e Dati",
    "text": "11.3 Benchmark AI: Sistema, Modello e Dati\nLa necessità di un benchmarking completo diventa fondamentale man mano che i sistemi AI diventano più complessi e onnipresenti. In questo contesto, i benchmark sono spesso classificati in tre categorie principali: Hardware, Modello e Dati. Analizziamo perché ognuno di questi gruppi è essenziale e il significato della valutazione dell’AI da queste tre dimensioni distinte:\n\n11.3.1 Benchmark di Sistema\nI calcoli AI, in particolare quelli nel deep learning, richiedono molte risorse. L’hardware su cui vengono eseguiti questi calcoli svolge un ruolo importante nel determinare la velocità, l’efficienza e la scalabilità delle soluzioni AI. Di conseguenza, i benchmark hardware aiutano a valutare le prestazioni di CPU, GPU, TPU e altri acceleratori nelle attività AI. Comprendendone le prestazioni, gli sviluppatori possono scegliere quali piattaforme hardware si adattano meglio a specifiche applicazioni AI. Inoltre, i produttori di hardware utilizzano questi benchmark per identificare aree di miglioramento, guidando l’innovazione nei progetti di chip specifici per AI.\n\n\n11.3.2 Benchmark del Modello\nL’architettura, le dimensioni e la complessità dei modelli AI variano notevolmente. Modelli diversi hanno diverse esigenze di calcolo e offrono diversi livelli di accuratezza ed efficienza. I benchmark dei modelli aiutano a valutare le prestazioni di varie architetture AI su attività standardizzate. Forniscono informazioni sulla velocità, l’accuratezza e le richieste di risorse di diversi modelli. Eseguendo il benchmarking dei modelli, i ricercatori possono identificare le architetture più performanti per attività specifiche, guidando la comunità AI verso soluzioni più efficienti ed efficaci. Inoltre, questi benchmark aiutano a monitorare i progressi della ricerca sull’intelligenza artificiale, mostrando i progressi nella progettazione e nell’ottimizzazione dei modelli.\n\n\n11.3.3 Benchmark dei Dati\nL’intelligenza artificiale, in particolare l’apprendimento automatico, è intrinsecamente basata sui dati. La qualità, le dimensioni e la diversità dei dati influenzano l’efficacia dell’addestramento e la capacità di generalizzazione dei modelli di intelligenza artificiale. I benchmark dei dati si concentrano sui set di dati utilizzati nell’addestramento e nella valutazione dell’intelligenza artificiale. Forniscono set di dati standardizzati che la comunità può utilizzare per addestrare e testare i modelli, garantendo parità di condizioni per i confronti. Inoltre, questi benchmark evidenziano le sfide relative alla qualità, alla diversità e alla rappresentazione dei dati, spingendo la comunità ad affrontare bias e lacune nei dati di addestramento dell’intelligenza artificiale. Comprendendo i benchmark dei dati, i ricercatori possono anche valutare come i modelli potrebbero comportarsi in scenari reali, garantendo robustezza e affidabilità.\nNelle restanti sezioni, discuteremo ciascuno di questi tipi di benchmark. L’attenzione sarà rivolta a un’esplorazione approfondita dei benchmark di sistema, poiché sono fondamentali per comprendere e migliorare le prestazioni del sistema di apprendimento automatico. Parleremo brevemente dei benchmark dei modelli e dei dati per una prospettiva completa, ma l’enfasi e la maggior parte del contenuto saranno dedicati ai benchmark di sistema.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmarking-di-sistema",
    "href": "contents/benchmarking/benchmarking.it.html#benchmarking-di-sistema",
    "title": "11  Benchmarking AI",
    "section": "11.4 Benchmarking di Sistema",
    "text": "11.4 Benchmarking di Sistema\n\n11.4.1 Granularità\nIl benchmarking del sistema di apprendimento automatico fornisce un approccio strutturato e sistematico per valutare le prestazioni di un sistema in diverse dimensioni. Data la complessità dei sistemi ML, possiamo analizzare le loro prestazioni attraverso diversi livelli di granularità e ottenere una visione completa dell’efficienza del sistema, identificare potenziali colli di bottiglia e individuare le aree di miglioramento. A tal fine, nel corso degli anni si sono evoluti vari tipi di benchmark che continuano a persistere.\nFigura 11.1 illustra i diversi livelli di granularità di un sistema ML. A livello di applicazione, i benchmark end-to-end valutano le prestazioni complessive del sistema, considerando fattori come la pre-elaborazione dei dati, l’addestramento del modello e l’inferenza. Mentre a livello di modello, i benchmark si concentrano sulla valutazione dell’efficienza e dell’accuratezza di modelli specifici. Ciò include la valutazione di quanto bene i modelli si generalizzano a nuovi dati e della loro efficienza computazionale durante l’addestramento e l’inferenza. Inoltre, il benchmarking può estendersi all’infrastruttura hardware e software, esaminando le prestazioni di singoli componenti come GPU o TPU.\n\n\n\n\n\n\nFigura 11.1: Granularità del sistema ML.\n\n\n\n\nMicro Benchmark\nI micro-benchmark nell’IA sono specializzati e valutano componenti distinti o operazioni specifiche all’interno di un processo di apprendimento automatico più ampio. Questi benchmark si concentrano su singole attività, offrendo approfondimenti sulle richieste computazionali di un particolare layer di rete neurale, l’efficienza di un’unica tecnica di ottimizzazione o la produttività di una specifica funzione di attivazione. Ad esempio, i professionisti potrebbero utilizzare i micro-benchmark per misurare il tempo di calcolo richiesto da un layer convoluzionale in un modello di deep learning o per valutare la velocità di preelaborazione che alimenta i dati nel modello. Tali valutazioni granulari sono fondamentali per la messa a punto e l’ottimizzazione di aspetti discreti dei modelli di IA, assicurando che ogni componente funzioni al massimo del suo potenziale.\nQuesti tipi di microbenchmark includono lo zoom su operazioni o componenti molto specifiche della pipeline AI, come le seguenti:\n\nOperazioni Tensoriali: Librerie come cuDNN (di NVIDIA) spesso hanno benchmark per misurare le prestazioni di singole operazioni tensoriali, come convoluzioni o moltiplicazioni di matrici, che sono fondamentali per i calcoli del deep learning.\nFunzioni di Attivazione: Benchmark che misurano la velocità e l’efficienza di varie funzioni di attivazione come ReLU, Sigmoid o Tanh in isolamento.\nBenchmark di Layer: Valutazioni dell’efficienza computazionale di distinti layer di rete neurale, come blocchi LSTM o Transformer, quando si opera su dimensioni di input standardizzate.\n\nEsempio: DeepBench, introdotto da Baidu, è un buon esempio di qualcosa che valuta quanto sopra. DeepBench valuta le prestazioni delle operazioni di base nei modelli di deep learning, fornendo informazioni su come diverse piattaforme hardware gestiscono l’addestramento e l’inferenza delle reti neurali.\n\n\n\n\n\n\nEsercizio 11.1: Benchmarking di Sistema - Operazioni Tensoriali\n\n\n\n\n\nCi si è mai chiesto come mai i filtri immagine diventano così veloci? Librerie speciali come cuDNN potenziano quei calcoli su determinati hardware. In questo Colab, useremo cuDNN con PyTorch per velocizzare il filtraggio delle immagini. Lo si consideri un piccolo benchmark, che mostra come il software giusto può sbloccare la potenza della GPU!\n\n\n\n\n\n\nMacro Benchmark\nI macro benchmark forniscono una visione olistica, valutando le prestazioni end-to-end di interi modelli di apprendimento automatico o sistemi di intelligenza artificiale completi. Invece di concentrarsi sulle singole operazioni, i macro benchmark valutano l’efficacia collettiva dei modelli in scenari o attività del mondo reale. Ad esempio, un macro benchmark potrebbe valutare le prestazioni complete di un modello di apprendimento profondo che esegue la classificazione delle immagini su un set di dati come ImageNet. Ciò include la misura dell’accuratezza, della velocità di calcolo e del consumo di risorse. Allo stesso modo, si potrebbero misurare il tempo e le risorse cumulativi necessari per addestrare un modello di elaborazione del linguaggio naturale su corpora di testo estesi o valutare le prestazioni di un intero sistema di raccomandazione, dall’inserimento dei dati agli output finali specifici dell’utente.\nEsempi: Questi benchmark valutano il modello di intelligenza artificiale:\n\nMLPerf Inference (Reddi et al. 2020): Un set di benchmark standard per misurare le prestazioni di software e hardware di apprendimento automatico. MLPerf ha una suite di benchmark dedicati per scale specifiche, come MLPerf Mobile per dispositivi di classe mobile e MLPerf Tiny, che si concentra su microcontrollori e altri dispositivi con risorse limitate.\nMLMark di EEMBC: Una suite di benchmarking per valutare le prestazioni e l’efficienza energetica dei dispositivi embedded che eseguono carichi di lavoro di apprendimento automatico. Questo benchmark fornisce informazioni su come diverse piattaforme hardware gestiscono attività come il riconoscimento delle immagini o l’elaborazione audio.\nAI-Benchmark (Ignatov et al. 2019): Uno strumento di benchmarking progettato per dispositivi Android, valuta le prestazioni delle attività di intelligenza artificiale sui dispositivi mobili, comprendendo vari scenari del mondo reale come il riconoscimento delle immagini, l’analisi dei volti e il riconoscimento ottico dei caratteri.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. «MLPerf Inference Benchmark». In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\nIgnatov, Andrey, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, e Luc Van Gool. 2019. «AI Benchmark: All About Deep Learning on Smartphones in 2019». In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 0–0. IEEE. https://doi.org/10.1109/iccvw.2019.00447.\n\n\nBenchmark end-to-end\nI benchmark end-to-end forniscono una valutazione completa che si estende oltre i confini del modello di IA stesso. Invece di concentrarsi esclusivamente sull’efficienza o l’accuratezza computazionale di un modello di apprendimento automatico, questi benchmark comprendono l’intera pipeline di un sistema di IA. Ciò include la pre-elaborazione iniziale dei dati, le prestazioni del modello principale, la post-elaborazione degli output del modello e altri componenti integrali come l’archiviazione e le interazioni di rete.\nLa pre-elaborazione dei dati è la prima fase in molti sistemi di IA, trasformando i dati grezzi in un formato adatto per l’addestramento o l’inferenza del modello. L’efficienza, la scalabilità e l’accuratezza di queste fasi di pre-elaborazione sono vitali per le prestazioni complessive del sistema. I benchmark end-to-end valutano questa fase, assicurando che la pulizia dei dati, la normalizzazione, l’aumento o qualsiasi altro processo di trasformazione non diventi un collo di bottiglia.\nAnche la fase di post-elaborazione è al centro dell’attenzione. Ciò comporta l’interpretazione degli output grezzi del modello, eventualmente la conversione dei punteggi in categorie significative, il filtraggio dei risultati o persino l’integrazione con altri sistemi. Nelle applicazioni del mondo reale, questa fase è fondamentale per fornire informazioni fruibili e i benchmark end-to-end ne garantiscono l’efficienza e l’efficacia.\nOltre alle operazioni di base dell’IA, altri componenti del sistema sono importanti per le prestazioni complessive e l’esperienza utente. Le soluzioni di archiviazione, basate su cloud, on-premise o ibride, possono avere un impatto significativo sui tempi di recupero e archiviazione dei dati, in particolare con vasti set di dati di IA. Allo stesso modo, le interazioni di rete, vitali per le soluzioni di IA basate su cloud o per i sistemi distribuiti, possono diventare colli di bottiglia delle prestazioni se non ottimizzate. I benchmark end-to-end valutano in modo olistico questi componenti, assicurando che l’intero sistema funzioni senza problemi, dal recupero dei dati alla consegna dell’output finale.\nAd oggi, non esistono benchmark end-to-end pubblici che tengano conto del ruolo dell’archiviazione dei dati, della rete e delle prestazioni di elaborazione. Si può sostenere che MLPerf Training and Inference si avvicini all’idea di un benchmark end-to-end, ma si concentrano esclusivamente sulle prestazioni del modello ML e non rappresentano scenari di distribuzione nel mondo reale di come i modelli vengono utilizzati sul campo. Tuttavia, forniscono un segnale molto utile che aiuta a valutare le prestazioni del sistema AI.\nData la specificità intrinseca del benchmarking end-to-end, viene in genere eseguito internamente in un’azienda “strumentando” [inserendo punti di controllo] distribuzioni di produzione reali di AI. Ciò consente agli ingegneri di avere una comprensione e una ripartizione realistiche delle prestazioni, ma data la sensibilità e la specificità delle informazioni, raramente vengono segnalate all’esterno dell’azienda.\n\n\nComprendere i Compromessi\nDiversi problemi sorgono nelle diverse fasi di un sistema di intelligenza artificiale. I micro-benchmark aiutano a mettere a punto i singoli componenti, i macro-benchmark aiutano a perfezionare le architetture o gli algoritmi del modello e i benchmark end-to-end guidano l’ottimizzazione dell’intero flusso di lavoro. Comprendendo dove si trova un problema, gli sviluppatori possono applicare ottimizzazioni mirate.\nInoltre, mentre i singoli componenti di un sistema di intelligenza artificiale potrebbero funzionare in modo ottimale in isolamento, possono emergere colli di bottiglia quando interagiscono. I benchmark end-to-end, in particolare, sono fondamentali per garantire che l’intero sistema, quando funziona collettivamente, soddisfi gli standard di prestazioni ed efficienza desiderati.\nInfine, le organizzazioni possono prendere decisioni informate su dove allocare le risorse individuando colli di bottiglia o inefficienze nelle prestazioni. Ad esempio, se i micro-benchmark rivelano inefficienze in specifiche operazioni tensoriali, gli investimenti possono essere indirizzati verso acceleratori hardware specializzati. Al contrario, se i benchmark end-to-end indicano problemi di recupero dei dati, gli investimenti potrebbero essere incanalati verso soluzioni di archiviazione migliori.\n\n\n\n11.4.2 Componenti dei Benchmark\nIn sostanza, un benchmark AI è più di un semplice test o punteggio; è un framework di valutazione completo. Per comprenderlo in modo approfondito, analizziamo i componenti tipici che compongono un benchmark AI.\n\nDataset Standardizzati\nI set di dati fungono da base per la maggior parte dei benchmark AI. Forniscono un set di dati coerente su cui i modelli vengono addestrati e valutati, garantendo parità di condizioni per i confronti.\nEsempio: ImageNet, un set di dati su larga scala contenente milioni di immagini etichettate che abbracciano migliaia di categorie, è uno standard di benchmarking popolare per le attività di classificazione delle immagini.\n\n\nAttività Predefinite\nUn benchmark dovrebbe avere un obiettivo o un compito chiaro che i modelli mirano a raggiungere. Questo compito definisce il problema che il sistema AI sta cercando di risolvere.\nEsempio: I compiti per i benchmark di elaborazione del linguaggio naturale potrebbero includere analisi del “sentiment”, riconoscimento di entità denominate o traduzione automatica.\n\n\nMetriche di Valutazione\nUna volta definito un task, i benchmark richiedono parametri per quantificare le prestazioni. Questi parametri offrono misure oggettive per confrontare diversi modelli o sistemi. Nei task di classificazione, parametri come accuratezza, precisione, richiamo e punteggio F1 sono comunemente utilizzati. Errori quadratici medi o assoluti potrebbero essere utilizzati per i task di regressione.\n\n\nBaseline e Modelli Baseline\nI benchmark spesso includono modelli “baseline” o implementazioni di riferimento. Di solito servono come punti di partenza o standard minimi di prestazione per confrontare nuovi modelli o nuove tecniche. I modelli “baseline” aiutano i ricercatori a misurare l’efficacia di nuovi algoritmi.\nNelle suite di benchmark, modelli semplici come la regressione lineare o le reti neurali di base sono spesso le baseline comuni. Queste forniscono un contesto quando si valutano modelli più complessi. Confrontando questi modelli più semplici, i ricercatori possono quantificare i miglioramenti derivanti da approcci avanzati.\nLe metriche delle prestazioni variano in base all’attività, ma ecco alcuni esempi:\n\nLe attività di classificazione utilizzano metriche come accuratezza, precisione, richiamo e punteggio F1.\nLe attività di regressione utilizzano spesso l’errore quadratico medio o l’errore assoluto medio.\n\n\n\nSpecifiche Hardware e Software\nData la variabilità introdotta da diverse configurazioni hardware e software, i benchmark spesso specificano o documentano gli ambienti hardware e software in cui vengono condotti i test.\nEsempio: Un benchmark AI potrebbe indicare che le valutazioni sono state condotte su una GPU NVIDIA Tesla V100 utilizzando TensorFlow v2.4.\n\n\nCondizioni Ambientali\nPoiché fattori esterni possono influenzare i risultati del benchmark, è essenziale controllare o documentare condizioni come temperatura, fonte di alimentazione o processi di background del sistema.\nEsempio: I benchmark AI mobili potrebbero specificare che i test sono stati condotti a temperatura ambiente con dispositivi collegati a una fonte di alimentazione per eliminare le variazioni del livello della batteria.\n\n\nRegole di Riproducibilità\nPer garantire che i benchmark siano credibili e possano essere replicati da altri nella comunità, spesso includono protocolli dettagliati che coprono tutto, dai “random seed” utilizzati agli iperparametri esatti.\nEsempio: Un benchmark per un’attività di learning di rinforzo potrebbe specificare gli episodi esatti dell’addestramento, i rapporti di esplorazione-sfruttamento e le strutture di ricompensa utilizzate.\n\n\nLinee Guida per l’Interpretazione dei Risultati\nOltre ai punteggi o alle metriche pure, i benchmark spesso forniscono linee guida o contesto per interpretare i risultati, aiutando i professionisti a comprendere le implicazioni più ampie.\nEsempio: Un benchmark potrebbe evidenziare che, sebbene il Modello A abbia ottenuto un punteggio più alto del Modello B in termini di accuratezza, offre migliori prestazioni in tempo reale, rendendolo più adatto per applicazioni sensibili al fattore tempo.\n\n\n\n11.4.3 Training vs. inferenza\nIl ciclo di vita dello sviluppo di un modello di apprendimento automatico prevede due fasi critiche: addestramento e inferenza. Training, come forse ricorderete, è il processo di apprendimento di modelli dai dati per creare il modello. L’inferenza si riferisce al modello che fa previsioni su nuovi dati non etichettati. Entrambe le fasi svolgono ruoli indispensabili ma distinti. Di conseguenza, ogni fase garantisce un rigoroso benchmarking per valutare metriche delle prestazioni come velocità, accuratezza ed efficienza computazionale.\nIl benchmarking della fase di training fornisce informazioni su come diverse architetture di modelli, valori di iperparametri e algoritmi di ottimizzazione influiscono sul tempo e sulle risorse necessarie per il training del modello. Ad esempio, il benchmarking mostra come la profondità della rete neurale influisce sul tempo di training su un dato set di dati. Il benchmarking rivela anche come acceleratori hardware come GPU e TPU possono accelerare il training.\nD’altro canto, il benchmark dell’inferenza valuta le prestazioni del modello in condizioni reali dopo la distribuzione. Le metriche chiave includono latenza, throughput, footprint di memoria e consumo energetico. Questo tipo di benchmarking determina se un modello soddisfa i requisiti della sua applicazione target in termini di tempo di risposta e vincoli del dispositivo. Tuttavia, ne discuteremo ampiamente per garantire una comprensione generale.\n\n\n11.4.4 I Benchmark del Training\nIl training [addestramento] rappresenta la fase in cui il sistema elabora e assimila dati grezzi per adattare e perfezionare i propri parametri. Pertanto, è un’attività algoritmica e comporta considerazioni a livello di sistema, tra cui pipeline di dati, archiviazione, risorse di elaborazione e meccanismi di orchestrazione. L’obiettivo è garantire che il sistema ML possa apprendere in modo efficiente dai dati, ottimizzando sia le prestazioni del modello sia l’utilizzo delle risorse del sistema.\n\nScopo\nDal punto di vista dei sistemi ML, i benchmark del training valutano quanto bene il sistema si adatta all’aumento dei volumi di dati e delle richieste di elaborazione. Si tratta di comprendere l’interazione tra hardware, software e pipeline di dati nel processo di training.\nSi consideri un sistema ML distribuito progettato per il training su vasti set di dati, come quelli utilizzati nelle raccomandazioni di prodotti di e-commerce su larga scala. Un benchmark di training valuterebbe l’efficienza con cui il sistema si adatta a più nodi, gestisce lo sharding [partizionamento] dei dati e gestisce guasti o abbandoni dei nodi durante il training.\nI benchmark di training valutano l’utilizzo di CPU, GPU, memoria e rete durante la fase di training, guidando le ottimizzazioni del sistema. Quando si addestra un modello in un sistema ML basato su cloud, è fondamentale capire come vengono utilizzate le risorse. Le GPU vengono sfruttate appieno? C’è un sovraccarico di memoria non necessario? I benchmark possono evidenziare colli di bottiglia o inefficienze nell’utilizzo delle risorse, con conseguenti risparmi sui costi e miglioramenti delle prestazioni.\nIl training di un modello ML è subordinato alla consegna tempestiva ed efficiente dei dati. I benchmark in questo contesto valuterebbero anche l’efficienza delle pipeline di dati, la velocità di preelaborazione dei dati e i tempi di recupero dell’archiviazione. Per i sistemi di analisi in tempo reale, come quelli utilizzati nel rilevamento delle frodi, la velocità con cui i dati di training vengono ingeriti, preelaborati e immessi nel modello può essere critica. I benchmark valuterebbero la latenza delle pipeline di dati, l’efficienza dei sistemi di archiviazione (come SSD rispetto a HDD) e la velocità delle attività di aumento o trasformazione dei dati.\n\n\nMetriche\nSe viste da una prospettiva di sistema, le metriche di training offrono informazioni che trascendono gli indicatori di prestazioni algoritmiche convenzionali. Queste metriche misurano l’efficacia di apprendimento del modello e misurano l’efficienza, la scalabilità e la robustezza dell’intero sistema ML durante la fase di training. Analizziamo più a fondo queste metriche e il loro significato.\nLe seguenti metriche sono spesso considerate importanti:\n\nTempo di training: Il tempo necessario per addestrare un modello da zero fino a raggiungere un livello di prestazioni soddisfacente. Misura direttamente le risorse di elaborazione necessarie per addestrare un modello. Ad esempio, il BERT di Google (Devlin et al. 2019) è un modello di elaborazione del linguaggio naturale che richiede diversi giorni per l’addestramento su un corpus enorme di dati di testo utilizzando più GPU. Il lungo tempo di training è una sfida significativa in termini di consumo di risorse e costi. In alcuni casi, i benchmark possono invece misurare la produttività del training (campioni di training per unità di tempo). La produttività può essere calcolata molto più velocemente e facilmente del tempo di addestramento, ma potrebbe oscurare le metriche che ci interessano davvero (ad esempio, il tempo di addestramento).\nScalabilità: Quanto bene il processo di addestramento può gestire gli aumenti delle dimensioni dei dati o della complessità del modello. La scalabilità può essere valutata misurando il tempo di addestramento, l’utilizzo della memoria e altri consumi di risorse all’aumentare delle dimensioni dei dati o della complessità del modello. Il modello GPT-3 di OpenAI (Brown et al. 2020) ha 175 miliardi di parametri, il che lo rende uno dei più grandi modelli linguistici esistenti. L’addestramento di GPT-3 ha richiesto notevoli sforzi ingegneristici per scalare il processo di addestramento in modo da gestire le enormi dimensioni del modello. Ciò ha comportato l’utilizzo di hardware specializzato, addestramento distribuito e altre tecniche per garantire che il modello potesse essere addestrato in modo efficiente.\nUtilizzo delle Risorse: La misura in cui il processo di addestramento utilizza le risorse di calcolo disponibili come CPU, GPU, memoria e I/O del disco. Un elevato utilizzo delle risorse può indicare un processo di training efficiente, mentre un basso utilizzo può suggerire colli di bottiglia o inefficienze. Ad esempio, il training di una rete neurale convoluzionale (CNN) per la classificazione delle immagini richiede notevoli risorse GPU. L’utilizzo di configurazioni multi-GPU e l’ottimizzazione del codice di training per l’accelerazione GPU possono migliorare notevolmente l’utilizzo delle risorse e l’efficienza del training.\nConsumo di Memoria: La quantità di memoria utilizzata dal processo di training. Il consumo di memoria può essere un fattore limitante per il training di modelli o set di dati di grandi dimensioni. Ad esempio, i ricercatori di Google hanno dovuto affrontare notevoli sfide di consumo di memoria durante il training di BERT. Il modello ha centinaia di milioni di parametri, che richiedono grandi quantità di memoria. I ricercatori hanno dovuto sviluppare tecniche per ridurre il consumo di memoria, come il checkpointing del gradiente e il parallelismo del modello.\nConsumo Energetico: L’energia consumata durante il training. Man mano che i modelli di apprendimento automatico diventano più complessi, il consumo energetico è diventato un fattore importante da considerare. Il training di grandi modelli di apprendimento automatico può consumare molta energia, e quindi molto carbonio. Ad esempio, si è stimato che l’addestramento di GPT-3 di OpenAI abbia un’impronta di carbonio equivalente a un viaggio in auto di 700.000 chilometri.\nThroughput: l numero di campioni di addestramento elaborati per unità di tempo. Un throughput [produttività] più elevato indica generalmente un processo di addestramento più efficiente. La produttività è una metrica importante da considerare quando si addestra un sistema di raccomandazione per una piattaforma di e-commerce. Una produttività elevata assicura che il modello possa elaborare rapidamente grandi volumi di dati di interazione dell’utente, il che è fondamentale per mantenere la pertinenza e l’accuratezza delle raccomandazioni. Ma è anche importante capire come bilanciare la produttività con i limiti di latenza. Pertanto, un vincolo di produttività limitato dalla latenza viene spesso imposto agli accordi sul livello di servizio per le distribuzioni di applicazioni del data center.\nCosto: Il costo della training di un modello può includere sia risorse computazionali che umane. Il costo è importante quando si considera la praticità e la fattibilità del training di modelli grandi o complessi. Si stima che l’addestramento di modelli di linguaggio grandi come GPT-3 costi milioni di dollari. Questo costo include risorse computazionali, elettriche e umane necessarie per lo sviluppo e l’addestramento del modello.\nTolleranza agli Errori e Robustezza: La capacità del processo di training di gestire guasti o errori senza bloccarsi o produrre risultati errati. Questo è importante per garantire l’affidabilità del processo di addestramento. Errori di rete o malfunzionamenti hardware possono verificarsi in uno scenario reale in cui un modello di apprendimento automatico viene addestrato su un sistema distribuito. Negli ultimi anni, è diventato abbondantemente chiaro che gli errori derivanti dalla corruzione “silenziosa” dei dati sono emersi come un problema importante. Un processo di addestramento affidabile e tollerante agli errori può recuperare da tali errori senza compromettere l’integrità del modello.\nFacilità d’Uso e Flessibilità: La facilità con cui il processo di addestramento può essere impostato e utilizzato e la sua flessibilità nella gestione di diversi tipi di dati e modelli. In aziende come Google, l’efficienza può talvolta essere misurata dal numero di anni di “Software Engineer (SWE)” risparmiati poiché ciò si traduce direttamente in impatto. La facilità d’uso e la flessibilità possono ridurre il tempo e lo sforzo necessari per addestrare un modello. TensorFlow e PyTorch sono popolari framework di apprendimento automatico che forniscono interfacce intuitive e API flessibili per la creazione e l’addestramento di modelli di machine-learning. Questi framework supportano molte architetture di modelli e sono dotati di strumenti che semplificano il processo di addestramento.\nRiproducibilità: La capacità di riprodurre i risultati del processo di training. La riproducibilità è importante per verificare la correttezza e la validità di un modello. Tuttavia, le variazioni dovute alle caratteristiche stocastiche della rete spesso rendono difficile riprodurre il comportamento preciso delle applicazioni in fase di addestramento, il che può rappresentare una sfida per il benchmarking.\n\nEseguendo il benchmarking per questi tipi di metriche, possiamo ottenere una visione completa delle prestazioni e dell’efficienza del processo di training da una prospettiva di sistema. Ciò può aiutare a identificare le aree di miglioramento e garantire che le risorse siano utilizzate in modo efficace.\n\n\nI Task\nSelezionare una manciata di task [attività] rappresentative per il benchmarking dei sistemi di machine learning è una sfida, perché l’apprendimento automatico viene applicato a vari domini con caratteristiche e requisiti unici. Ecco alcune delle sfide affrontate nella selezione di attività rappresentative:\n\nDiversità di Applicazioni: L’apprendimento automatico viene utilizzato in numerosi campi, come sanità, finanza, elaborazione del linguaggio naturale, visione artificiale e molti altri. Ogni campo ha attività specifiche che potrebbero non essere rappresentative di altri campi. Ad esempio, le attività di classificazione delle immagini nella visione artificiale potrebbero non essere importanti per il rilevamento delle frodi finanziarie.\nVariabilità nei Tipi di Dati e nella Qualità: Diverse attività richiedono tipi di dati diversi, come testo, immagini, video o dati numerici. La qualità e la disponibilità dei dati possono variare notevolmente tra le attività, rendendo difficile selezionare attività rappresentative delle sfide generali affrontate nell’apprendimento automatico.\nComplessità e Difficoltà delle Attività: La complessità delle attività varia notevolmente. Alcune sono relativamente semplici, mentre altre sono molto complesse e richiedono modelli e tecniche sofisticate. Selezionare attività rappresentative che coprano le complessità riscontrate nell’apprendimento automatico è difficile.\nProblemi Etici e di Privacy: Alcune attività possono riguardare dati sensibili o privati, come cartelle cliniche o informazioni personali. Queste attività possono presentare problemi etici e di privacy che devono essere affrontati, rendendole meno adatte come attività rappresentative per il benchmarking.\nRequisiti di Scalabilità e Risorse: Attività diverse possono avere requisiti di scalabilità e risorse diverse. Alcune attività possono richiedere ampie risorse di calcolo, mentre altre possono essere eseguite con risorse minime. Selezionare attività che rappresentino i requisiti di risorse generali nell’apprendimento automatico è difficile.\nMetriche di Valutazione: Le metriche utilizzate per valutare le prestazioni dei modelli di apprendimento automatico variano tra le attività. Alcune attività possono avere metriche di valutazione consolidate, mentre altre non hanno metriche chiare o standardizzate. Ciò può rendere difficile confrontare le prestazioni tra diverse attività.\nGeneralizzabilità dei Risultati: I risultati ottenuti dal benchmarking su un’attività specifica potrebbero non essere generalizzabili ad altre attività. Ciò significa che le prestazioni di un sistema di apprendimento automatico su un’attività selezionata potrebbero non essere indicative delle sue prestazioni su altre attività.\n\nÈ importante considerare attentamente questi fattori quando si progettano benchmark per garantire che siano significativi e pertinenti per la vasta gamma di attività incontrate nel machine learning.\n\n\nI Benchmark\nEcco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per l’addestramento di sistemi di apprendimento automatico.\nMLPerf Training Benchmark\nMLPerf è una suite di benchmark progettata per misurare le prestazioni di hardware, software e servizi di apprendimento automatico. Il benchmark di MLPerf Training (Mattson et al. 2020a) si concentra sul tempo necessario per addestrare i modelli a una metrica di qualità target. Include carichi di lavoro diversi, come classificazione delle immagini, rilevamento di oggetti, traduzione e apprendimento per rinforzo.\nMetriche:\n\nTempo di training per la qualità target\nThroughput (esempi al secondo)\nUtilizzo delle risorse (CPU, GPU, memoria, I/O del disco)\n\nDAWNBench\nDAWNBench (Coleman et al. 2019) è una suite di benchmark incentrata sul tempo di training end-to-end del deep learning e sulle prestazioni dell’inferenza. Include attività comuni come la classificazione delle immagini e la risposta alle domande.\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, e Matei Zaharia. 2019. «Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark». ACM SIGOPS Operating Systems Review 53 (1): 14–25. https://doi.org/10.1145/3352020.3352024.\nMetriche:\n\nTempo di training per la precisione target\nLatenza dell’inferenza\nCosto (in termini di risorse di cloud computing e storage)\n\nFathom\nFathom (Adolf et al. 2016) è un benchmark dell’Università di Harvard che valuta le prestazioni dei modelli di deep learning utilizzando un set diversificato di carichi di lavoro. Questi includono attività comuni come la classificazione delle immagini, il riconoscimento vocale e la modellazione del linguaggio.\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, e David Brooks. 2016. «Fathom: Reference workloads for modern deep learning methods». In 2016 IEEE International Symposium on Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\nMetriche:\n\nOperazioni al secondo (per misurare l’efficienza computazionale)\nTempo di completamento per ogni carico di lavoro\nLarghezza di banda della memoria\n\n\n\nCaso d’Uso di Esempio\nConsideriamo uno scenario in cui vogliamo eseguire il benchmark dell’addestramento di un modello di classificazione delle immagini su una piattaforma hardware specifica.\n\nTask: L’attività consiste nell’addestrare una rete neurale convoluzionale (CNN) per la classificazione delle immagini sul set di dati CIFAR-10.\nBenchmark: Possiamo usare il benchmark di addestramento MLPerf per questa attività. Include un carico di lavoro di classificazione delle immagini pertinente alla nostra attività.\nMetriche: Misureremo le seguenti metriche:\n\n\nTempo di addestramento per raggiungere una precisione target del 90%.\nThroughput in termini di immagini elaborate al secondo.\nUtilizzo di GPU e CPU durante l’addestramento.\n\nMisurando queste metriche, possiamo valutare le prestazioni e l’efficienza del processo di addestramento sulla piattaforma hardware selezionata. Queste informazioni possono quindi essere utilizzate per identificare potenziali colli di bottiglia o aree di miglioramento.\n\n\n\n11.4.5 Benchmark di Inferenza\nL’inferenza nell’apprendimento automatico si riferisce all’uso di un modello addestrato per fare previsioni su dati nuovi e mai visti prima. È la fase in cui il modello applica le conoscenze apprese per risolvere il problema per cui è stato progettato, come la classificazione di immagini, il riconoscimento vocale o la traduzione di testo.\n\nScopo\nQuando creiamo modelli di machine learning, il nostro obiettivo finale è di distribuirli in applicazioni del mondo reale in cui possano fornire previsioni accurate e affidabili su dati nuovi e mai visti. Questo processo di utilizzo di un modello addestrato per fare previsioni è noto come inferenza. Le prestazioni reali di un modello di apprendimento automatico possono differire in modo significativo dalle sue prestazioni su set di dati di addestramento o validazione, il che rende l’inferenza di benchmarking un passaggio cruciale nello sviluppo e nell’implementazione di modelli di machine learning.\nIl benchmarking dell’inferenza ci consente di valutare quanto bene un modello di apprendimento automatico funziona in scenari del mondo reale. Questa valutazione garantisce che il modello sia pratico e affidabile quando distribuito in applicazioni, fornendo una comprensione più completa del comportamento del modello con dati reali. Inoltre, il benchmarking può aiutare a identificare potenziali colli di bottiglia o limitazioni nelle prestazioni del modello. Ad esempio, se un modello impiega troppo tempo per dedurre, potrebbe non essere pratico per applicazioni in tempo reale come la guida autonoma o gli assistenti vocali.\nL’efficienza delle risorse è un altro aspetto critico dell’inferenza, poiché può essere computazionalmente intensiva e richiedere memoria e potenza di elaborazione significative. Il benchmarking aiuta a garantire che il modello sia efficiente per quanto riguarda l’utilizzo delle risorse, il che è particolarmente importante per i dispositivi edge con capacità computazionali limitate, come smartphone o dispositivi IoT. Inoltre, il benchmarking ci consente di confrontare le prestazioni del nostro modello con quelli concorrenti o versioni precedenti dello stesso modello. Questo confronto è essenziale per prendere decisioni informate su quale modello implementare in un’applicazione specifica.\nInfine, è fondamentale garantire che le previsioni del modello non siano solo accurate, ma anche coerenti tra diversi dati. Il benchmarking aiuta a verificare l’accuratezza e la coerenza del modello, assicurando che soddisfi i requisiti dell’applicazione. Valuta inoltre la robustezza del modello, assicurando che possa gestire la variabilità dei dati del mondo reale e comunque fare previsioni accurate.\n\n\nMetriche\n\nPrecisione: La precisione è una delle metriche più importanti quando si confrontano i modelli di machine learning. Quantifica la percentuale di previsioni corrette effettuate dal modello rispetto ai valori o alle etichette reali. Ad esempio, se un modello di rilevamento dello spam riesce a classificare correttamente 95 messaggi e-mail su 100, la sua precisione verrebbe calcolata al 95%.\nLatenza: La latenza è una metrica delle prestazioni che calcola il ritardo o l’intervallo di tempo tra la ricezione dell’input e la produzione dell’output corrispondente da parte del sistema di apprendimento automatico. Un esempio che descrive chiaramente la latenza è un’applicazione di traduzione in tempo reale; se esiste un ritardo di mezzo secondo dal momento in cui un utente inserisce una frase al momento in cui l’app visualizza il testo tradotto, la latenza del sistema è di 0.5 secondi.\nLatency-Bounded Throughput: Il throughput limitato dalla latenza è una metrica preziosa che combina gli aspetti di latenza e throughput, misurando il throughput massimo di un sistema pur rispettando un vincolo di latenza specificato. Ad esempio, in un’applicazione di streaming video che utilizza un modello di apprendimento automatico per generare e visualizzare automaticamente i sottotitoli, il throughput limitato dalla latenza misurerebbe quanti frame video il sistema può elaborare al secondo (throughput) garantendo al contempo che i sottotitoli vengano visualizzati con un ritardo non superiore a 1 secondo (latenza). Questa metrica è particolarmente importante nelle applicazioni in tempo reale in cui soddisfare i requisiti di latenza è fondamentale per l’esperienza utente.\nThroughput: Il throughput valuta la capacità del sistema misurando il numero di inferenze o previsioni che un modello di apprendimento automatico può gestire entro un’unità di tempo specifica. Si consideri un sistema di riconoscimento vocale che utilizza una Recurrent Neural Network (RNN) come modello sottostante; se questo sistema riesce a elaborare e comprendere 50 diverse clip audio in un minuto, allora la sua velocità di elaborazione è di 50 clip al minuto.\nEfficienza energetica: L’efficienza energetica è una metrica che determina la quantità di energia consumata dal modello di apprendimento automatico per eseguire una singola inferenza. Un esempio lampante di ciò sarebbe un modello di elaborazione del linguaggio naturale basato su un’architettura di rete Transformer; se utilizza 0,1 Joule di energia per tradurre una frase dall’inglese al francese, la sua efficienza energetica è misurata a 0,1 Joule per inferenza.\nUtilizzo della memoria: L’utilizzo della memoria quantifica il volume di RAM necessario a un modello di apprendimento automatico per svolgere attività di inferenza. Un esempio rilevante per illustrare questo sarebbe un sistema di riconoscimento facciale basato su una CNN; se un tale sistema richiede 150 MB di RAM per elaborare e riconoscere i volti all’interno di un’immagine, il suo utilizzo della memoria è di 150 MB.\n\n\n\nI Task\nLe sfide nella scelta di attività rappresentative per il benchmarking dei sistemi di apprendimento automatico inferenziale sono, in generale, piuttosto simili alla tassonomia che abbiamo fornito per la il training. Tuttavia, per essere pignoli, discutiamone nel contesto dei sistemi di machine learning inferenziale.\n\nDiversità di Applicazioni: L’apprendimento automatico inferenziale è impiegato in numerosi domini come sanità, finanza, intrattenimento, sicurezza e altro. Ogni dominio ha attività uniche e ciò che è rappresentativo in un dominio potrebbe non esserlo in un altro. Ad esempio, un’attività di inferenza per prevedere i prezzi delle azioni nel dominio finanziario potrebbe differire dalle attività di riconoscimento delle immagini nel dominio medico.\nVariabilità nei Tipi di Dati: Diverse attività di inferenza richiedono diversi tipi di dati: testo, immagini, video, dati numerici, ecc. Assicurarsi che i benchmark affrontino l’ampia varietà di tipi di dati utilizzati nelle applicazioni del mondo reale è una sfida. Ad esempio, i sistemi di riconoscimento vocale elaborano dati audio, che sono molto diversi dai dati visivi elaborati dai sistemi di riconoscimento facciale.\nComplessità delle Attività: La complessità delle attività di inferenza può variare enormemente, da attività di classificazione di base ad attività complesse che richiedono modelli all’avanguardia. Ad esempio, distinguere tra due categorie (classificazione binaria) è in genere più semplice che rilevare centinaia di tipi di oggetti in una scena affollata.\nRequisiti in Tempo Reale: Alcune applicazioni richiedono risposte immediate o in tempo reale, mentre altre possono consentire un certo ritardo. Nella guida autonoma, il rilevamento degli oggetti in tempo reale e il processo decisionale sono fondamentali, mentre un motore di raccomandazione per un sito Web di shopping potrebbe tollerare lievi ritardi.\nProblemi di Scalabilità: Data la variabilità della scala delle applicazioni, dai dispositivi edge ai server basati su cloud, le attività devono rappresentare i diversi ambienti di elaborazione in cui si verifica l’inferenza. Ad esempio, un’attività di inferenza in esecuzione sulle risorse limitate di uno smartphone è diversa da un potente server cloud.\nDiversità delle Metriche di Valutazione: Le metriche utilizzate per valutare le prestazioni possono variare in modo significativo a seconda dell’attività. Trovare un terreno comune o una metrica universalmente accettata per attività diverse è una sfida. Ad esempio, la precisione e il “recall” [richiamo] potrebbero essere vitali per un’attività di diagnosi medica, mentre la produttività (inferenze al secondo) potrebbero essere più cruciali per le attività di elaborazione video.\nProblemi Etici e di Privacy: Esistono problemi relativi all’etica e alla privacy, soprattutto in aree sensibili come il riconoscimento facciale o l’elaborazione dei dati personali. Questi problemi possono influire sulla selezione e sulla natura delle attività utilizzate per il benchmarking. Ad esempio, l’utilizzo di dati facciali reali per il benchmarking può sollevare problemi di privacy, mentre i dati sintetici potrebbero non replicare le sfide del mondo reale.\nDiversità Hardware: Con un’ampia gamma di dispositivi, da GPU, CPU e TPU ad ASIC personalizzati utilizzati per l’inferenza, garantire che le attività siano rappresentative su hardware diversi è una sfida. Ad esempio, un’attività ottimizzata per l’inferenza su una GPU potrebbe avere prestazioni non ottimali su un dispositivo edge.\n\n\n\nI Benchmark\nEcco alcuni lavori originali che hanno gettato le basi fondamentali per lo sviluppo di benchmark sistematici per sistemi di apprendimento automatico inferenziale.\nMLPerf Inference Benchmark: MLPerf Inference è una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una varietà di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l’obiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza. Le sue metriche includono:\nMLPerf Inference è una suite di benchmark completa che valuta le prestazioni dei modelli di apprendimento automatico durante la fase di inferenza. Comprende una varietà di carichi di lavoro, tra cui classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, con l’obiettivo di fornire metriche standardizzate e approfondite per la valutazione di diversi sistemi di inferenza.\nMetriche:\n\nTempo di inferenza\nLatenza\nThroughput [Produttività]\nPrecisione\nConsumo energetico\n\nAI Benchmark: AI Benchmark è uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attività di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un’analisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware. Le sue metriche includono:\nAI Benchmark è uno strumento di benchmarking che valuta le prestazioni dei modelli di intelligenza artificiale e di apprendimento automatico su dispositivi mobili e piattaforme di edge computing. Include test per attività di classificazione delle immagini, rilevamento di oggetti ed elaborazione del linguaggio naturale, fornendo un’analisi dettagliata delle prestazioni di inferenza su diverse piattaforme hardware.\nMetriche:\n\nTempo di inferenza\nLatenza\nConsumo energetico\nUtilizzo della memoria\nThroughput [Produttività]\n\nToolkit OpenVINO: Il toolkit OpenVINO fornisce uno strumento di benchmark per misurare le prestazioni dei modelli di apprendimento profondo per varie attività, come la classificazione delle immagini, il rilevamento degli oggetti e il riconoscimento facciale, su hardware Intel. Offre approfondimenti dettagliati sulle prestazioni di inferenza dei modelli su diverse configurazioni hardware. Le sue metriche includono:\nMetriche:\n\nTempo di inferenza\nThroughput [Produttività]\nLatenza\nUtilizzo di CPU e GPU\n\n\n\nCaso d’Uso di Esempio\nConsideriamo uno scenario in cui vogliamo valutare le prestazioni di inferenza di un modello di rilevamento di oggetti su uno specifico dispositivo edge.\nTask: L’attività consiste nell’eseguire il rilevamento di oggetti in tempo reale su flussi video, rilevando e identificando oggetti quali veicoli, pedoni e segnali stradali.\nBenchmark: Possiamo utilizzare AI Benchmark per questa attività in quanto valuta le prestazioni di inferenza sui dispositivi edge, il che si adatta al nostro scenario.\nMetriche: Misureremo le seguenti metriche:\n\nTempo di inferenza per elaborare ogni fotogramma video\nLatenza per generare i “bounding box” per gli oggetti rilevati\nConsumo energetico durante il processo di inferenza\nProduttività in termini di fotogrammi video elaborati al secondo\n\nMisurando queste metriche, possiamo valutare le prestazioni del modello di rilevamento di oggetti sul dispositivo edge e identificare eventuali colli di bottiglia o aree di ottimizzazione per migliorare le capacità di elaborazione in tempo reale.\n\n\n\n\n\n\nEsercizio 11.2: Benchmark di Inferenza - MLPerf\n\n\n\n\n\nPrepararsi a mettere alla prova i propri modelli di intelligenza artificiale! MLPerf è come le Olimpiadi per le prestazioni del machine learning. In questo Colab, utilizzeremo un toolkit chiamato CK per eseguire benchmark MLPerf ufficiali, misurare la velocità e l’accuratezza di un proprio modello e persino utilizzare TVM per dargli una spinta super veloce. Pronti a vedere il modello vincere la sua medaglia?\n\n\n\n\n\n\n\n11.4.6 Esempio di Benchmark\nPer illustrare correttamente i componenti di un benchmark di sistema, possiamo esaminare il benchmark di individuazione delle parole chiave in MLPerf Tiny e spiegare la motivazione alla base di ogni decisione.\n\nTask\nL’individuazione delle parole chiave è stata selezionata come attività perché è un caso d’uso comune in TinyML che è stato ben consolidato per anni. Inoltre, l’hardware tipico utilizzato per l’individuazione delle parole chiave differisce sostanzialmente dalle offerte di altri benchmark, come l’attività di riconoscimento vocale di MLPerf Inference.\n\n\nIl Dataset\nGoogle Speech Commands (Warden 2018) è stato selezionato come il miglior dataset per rappresentare l’attività. Il dataset è ben consolidato nella comunità di ricerca e ha una licenza permissiva, che consente di utilizzarlo facilmente in un benchmark.\n\nWarden, Pete. 2018. «Speech commands: A dataset for limited-vocabulary speech recognition». ArXiv preprint abs/1804.03209. https://arxiv.org/abs/1804.03209.\n\n\nModello\nIl componente principale successivo è il modello, che fungerà da carico di lavoro primario per il benchmark. Il modello dovrebbe essere ben consolidato come soluzione per l’attività selezionata piuttosto che una soluzione all’avanguardia. Il modello selezionato è un semplice modello di convoluzione separabile in profondità. Questa architettura non è la soluzione all’avanguardia per l’attività, ma è ben consolidata e non progettata per una piattaforma hardware specifica come molte soluzioni all’avanguardia. Nonostante sia un benchmark di inferenza, stabilisce anche una ricetta di training di riferimento per essere completamente riproducibile e trasparente.\n\n\nMetriche\nLa latenza è stata selezionata come metrica primaria per il benchmark, poiché i sistemi di individuazione delle parole chiave devono reagire rapidamente per mantenere la soddisfazione dell’utente. Inoltre, dato che i sistemi TinyML sono spesso alimentati a batteria, il consumo energetico viene misurato per garantire l’efficienza della piattaforma hardware. L’accuratezza del modello viene misurata anche per garantire che le ottimizzazioni applicate da un submitter, come la quantizzazione, non degradino l’accuratezza oltre una soglia.\n\n\nBenchmark Harness\nMLPerf Tiny utilizza EEMBCs EnergyRunner benchmark harness per caricare gli input nel modello e isolare e misurare il consumo energetico del dispositivo. Quando si misura il consumo energetico, è fondamentale selezionare un “harness” [imbracatura] che sia accurato ai livelli di potenza previsti dei dispositivi sottoposti a test e sufficientemente semplice da non diventare un peso per i partecipanti al benchmark.\n\n\nLa Baseline\nGli invii di baseline sono fondamentali per contestualizzare i risultati e come punto di riferimento per aiutare i partecipanti a iniziare. L’invio di base dovrebbe dare priorità alla semplicità e alla leggibilità rispetto alle prestazioni avanzate. L’individuazione della parola chiave della baseline utilizza un microcontrollore STM standard come hardware e TensorFlow Lite come microcontrollore (David et al. 2021) come framework di inferenza.\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\n\n\n\n11.4.7 Sfide e Limitazioni\nSebbene il benchmarking fornisca una metodologia strutturata per la valutazione delle prestazioni in domini complessi come l’intelligenza artificiale e l’informatica, il processo pone anche diverse sfide. Se non affrontati correttamente, questi ostacoli possono minare la credibilità e l’accuratezza dei risultati del benchmarking. Alcune delle difficoltà predominanti affrontate nel benchmarking includono quanto segue:\n\nCopertura incompleta del problema: Le attività di benchmarking potrebbero non rappresentare completamente lo spazio del problema. Ad esempio, i set di dati di classificazione delle immagini comuni come CIFAR-10 hanno una diversità limitata nei tipi di immagini. Gli algoritmi ottimizzati per tali benchmark potrebbero non riuscire a generalizzare bene con i set di dati del mondo reale.\nInsignificanza statistica: I benchmark devono avere prove e campioni di dati sufficienti per produrre risultati statisticamente significativi. Ad esempio, il benchmarking di un modello OCR su solo poche scansioni di testo potrebbe non catturare adeguatamente i suoi veri tassi di errore.\nRiproducibilità limitata: Variazioni di hardware, versioni software, basi di codice e altri fattori possono ridurre la riproducibilità dei risultati di benchmark. MLPerf affronta questo problema fornendo implementazioni di riferimento e specifiche ambientali.\nDisallineamento con gli obiettivi finali: I benchmark che si concentrano solo su metriche di velocità o accuratezza possono disallineare gli obiettivi reali come costi ed efficienza energetica. I benchmark devono riflettere tutti gli assi prestazionali critici.\nRapida obsolescenza: A causa del rapido ritmo dei progressi nell’intelligenza artificiale e nell’informatica, i benchmark e i loro set di dati possono rapidamente diventare obsoleti. Mantenere benchmark aggiornati è quindi una sfida persistente.\n\nMa di tutte queste, la sfida più importante è l’ingegneria dei benchmark.\n\nLotteria Hardware\nLa “hardware lottery” nel benchmarking dei sistemi di machine learning si riferisce alla situazione in cui il successo o l’efficienza di un modello di apprendimento automatico sono significativamente influenzati dalla compatibilità del modello con l’hardware sottostante (Chu et al. 2021). In altre parole, alcuni modelli hanno prestazioni eccezionali perché sono adatti alle caratteristiche o alle capacità specifiche dell’hardware su cui vengono eseguiti, piuttosto che perché sono modelli intrinsecamente superiori.\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, e Andrew Howard. 2021. «Discovering Multi-Hardware Mobile Models via Architecture Search». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\n\n\n\n\nFigura 11.2: Hardware Lottery.\n\n\n\nAd esempio, alcuni modelli di apprendimento automatico possono essere progettati e ottimizzati per sfruttare le capacità di elaborazione parallela di acceleratori hardware specifici, come le unità di elaborazione grafica (GPU) o le unità di elaborazione tensore (TPU). Di conseguenza, questi modelli potrebbero mostrare prestazioni superiori quando vengono sottoposti a benchmark su tale hardware rispetto ad altri modelli che non sono ottimizzati per l’hardware.\nAd esempio, un articolo del 2018 ha introdotto una nuova architettura di rete neurale convoluzionale per la classificazione delle immagini che ha raggiunto un’accuratezza all’avanguardia su ImageNet. Tuttavia, l’articolo menzionava solo che il modello era stato addestrato su 8 GPU senza specificare il modello, la dimensione della memoria o altri dettagli rilevanti. Uno studio di follow-up ha cercato di riprodurre i risultati, ma ha scoperto che addestrare lo stesso modello su GPU comunemente disponibili ha ottenuto un’accuratezza inferiore del 10%, anche dopo l’ottimizzazione degli iperparametri. L’hardware originale probabilmente aveva una larghezza di banda di memoria e una potenza di calcolo molto più elevate. Come altro esempio, i tempi di addestramento per modelli linguistici di grandi dimensioni possono variare drasticamente in base alle GPU utilizzate.\nLa “hardware lottery” può introdurre sfide e bias nel benchmarking dei sistemi di apprendimento automatico, poiché le prestazioni del modello non dipendono esclusivamente dall’architettura o dall’algoritmo del modello, ma anche dalla compatibilità e dalle sinergie con l’hardware sottostante. Ciò può rendere difficile confrontare equamente diversi modelli e identificare il modello migliore in base ai suoi meriti intrinseci. Può anche portare a una situazione in cui la comunità converge su modelli che sono adatti all’hardware più diffuso del momento, trascurando potenzialmente altri modelli che potrebbero essere superiori ma incompatibili con le attuali tendenze hardware.\n\n\nBenchmark Engineering\nLa lotteria hardware si verifica quando un modello di apprendimento automatico funziona in modo eccezionalmente bene o male su una configurazione hardware specifica a causa di compatibilità o incompatibilità impreviste. Il modello non è esplicitamente progettato o ottimizzato per quell’hardware specifico dagli sviluppatori o dagli ingegneri; piuttosto, capita che si allinei o (non si allinei) con le capacità o le limitazioni dell’hardware. In questo caso, le prestazioni del modello sull’hardware sono un prodotto della coincidenza piuttosto che della progettazione.\nContrariamente alla lotteria hardware accidentale, il benchmark engineering implica l’ottimizzazione o la progettazione deliberata di un modello di apprendimento automatico per funzionare eccezionalmente bene su hardware specifico, spesso per vincere benchmark o competizioni. Questa ottimizzazione intenzionale potrebbe includere la modifica dell’architettura, degli algoritmi o dei parametri del modello per sfruttare appieno le funzionalità e le capacità dell’hardware.\n\n\nProblema\nIl benchmark engineering si riferisce alla modifica o all’ottimizzazione di un sistema di intelligenza artificiale per ottimizzare le prestazioni su test di benchmark specifici, spesso a scapito della generalizzabilità o delle prestazioni nel mondo reale. Ciò può includere la regolazione di iperparametri, dati di training o altri aspetti del sistema specificamente per ottenere punteggi elevati sulle metriche di benchmark senza necessariamente migliorare la funzionalità o l’utilità complessiva del sistema.\nLa motivazione alla base dell’ingegneria dei benchmark spesso deriva dal desiderio di ottenere punteggi di prestazioni elevate per scopi di marketing o competitivi. Punteggi di benchmark elevati possono dimostrare la superiorità di un sistema di intelligenza artificiale rispetto ai concorrenti e possono essere un argomento chiave per la vendita per potenziali utenti o investitori. Questa pressione per ottenere buoni risultati nei benchmark a volte porta a dare priorità alle ottimizzazioni specifiche del benchmark rispetto a miglioramenti più olistici del sistema.\nPuò comportare diversi rischi e sfide. Uno dei rischi principali è che il sistema di intelligenza artificiale possa funzionare meglio nelle applicazioni del mondo reale rispetto a quanto suggeriscono i punteggi di benchmark. Ciò può portare a insoddisfazione dell’utente, danni alla reputazione e potenziali problemi di sicurezza o etici. Inoltre, l’ingegneria dei benchmark può contribuire a una mancanza di trasparenza e responsabilità nella comunità dell’intelligenza artificiale, poiché può essere difficile discernere quanta parte delle prestazioni di un sistema di intelligenza artificiale sia dovuta a miglioramenti genuini rispetto a ottimizzazioni specifiche del benchmark.\nLa comunità AI deve dare priorità alla trasparenza e alla responsabilità per mitigare i rischi associati all’ingegneria dei benchmark. Ciò può includere la divulgazione di eventuali ottimizzazioni o modifiche apportate specificamente per i test di benchmark e la fornitura di valutazioni più complete dei sistemi AI che includono metriche delle prestazioni del mondo reale e punteggi di benchmark. I ricercatori e gli sviluppatori devono dare priorità a miglioramenti olistici dei sistemi AI che ne migliorino la generalizzabilità e la funzionalità in varie applicazioni anziché concentrarsi esclusivamente su ottimizzazioni specifiche del benchmark.\n\n\nProblemi\nUno dei problemi principali dell’ingegneria del benchmark è che può compromettere le prestazioni reali dei sistemi di intelligenza artificiale. Quando gli sviluppatori si concentrano sull’ottimizzazione dei loro sistemi per ottenere punteggi elevati in specifici test di benchmark, potrebbero trascurare altri importanti aspetti delle prestazioni del sistema, cruciali nelle applicazioni del mondo reale. Ad esempio, un sistema di intelligenza artificiale progettato per il riconoscimento delle immagini potrebbe essere progettato per funzionare eccezionalmente bene in un test di benchmark che include un set specifico di immagini, ma necessita di aiuto per riconoscere accuratamente immagini leggermente diverse da quelle nel set di test.\nUn’altra area di miglioramento con l’ingegneria di benchmark è che può comportare sistemi di intelligenza artificiale privi di generalizzabilità. In altre parole, mentre il sistema può funzionare bene nel test di benchmark, potrebbe aver bisogno di aiuto per gestire una vasta gamma di input o scenari. Ad esempio, un modello di intelligenza artificiale sviluppato per l’elaborazione del linguaggio naturale potrebbe essere progettato per ottenere punteggi elevati in un test di benchmark che include un tipo specifico di testo, ma non riesce a elaborare accuratamente il testo che non rientra in quel tipo specifico.\nPuò anche portare a risultati fuorvianti. Quando i sistemi di intelligenza artificiale sono progettati per funzionare bene nei test di benchmark, i risultati potrebbero non riflettere accuratamente le reali capacità del sistema. Questo può essere problematico per gli utenti o gli investitori che si affidano ai punteggi di benchmark per prendere decisioni informate su quali sistemi di intelligenza artificiale utilizzare o in cui investire. Ad esempio, un sistema di intelligenza artificiale progettato per ottenere punteggi elevati in un test di benchmark per il riconoscimento vocale potrebbe dover essere più in grado di riconoscere accuratamente il parlato in situazioni reali, portando gli utenti o gli investitori a prendere decisioni basate su informazioni imprecise.\n\n\nAttenuazione\nEsistono diversi modi per mitigare l’ingegneria dei benchmark. La trasparenza nel processo di benchmarking è fondamentale per mantenere l’accuratezza e l’affidabilità dei benchmark. Ciò implica la divulgazione chiara delle metodologie, dei set di dati e dei criteri di valutazione utilizzati nei test di benchmark, nonché di eventuali ottimizzazioni o modifiche apportate al sistema di intelligenza artificiale ai fini del benchmark.\nUn modo per ottenere trasparenza è attraverso l’uso di benchmark open source. I benchmark open source vengono resi disponibili al pubblico, consentendo a ricercatori, sviluppatori e altre parti interessate di esaminarli, criticarli e contribuire, garantendone così l’accuratezza e l’affidabilità. Questo approccio collaborativo facilita anche la condivisione delle “best practice” e lo sviluppo di benchmark più solidi e completi.\nUn esempio è MLPerf Tiny. È un framework open source progettato per semplificare il confronto di diverse soluzioni nel mondo di TinyML. Il suo design modulare consente di sostituire i componenti per il confronto o il miglioramento. Le implementazioni di riferimento, mostrate in verde e arancione in Figura 11.3, fungono da base per i risultati. TinyML spesso necessita di ottimizzazione nell’intero sistema e gli utenti possono contribuire concentrandosi su parti specifiche, come la quantizzazione. Il design modulare del benchmark consente agli utenti di mostrare i propri contributi e il vantaggio competitivo modificando un’implementazione di riferimento. In breve, MLPerf Tiny offre un modo flessibile e modulare per valutare e migliorare le applicazioni TinyML, semplificando il confronto e il miglioramento di diversi aspetti della tecnologia.\n\n\n\n\n\n\nFigura 11.3: Design modulare di MLPerf Tiny. Fonte: Mattson et al. (2020a).\n\n\n———, et al. 2020a. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nUn altro metodo per ottenere trasparenza è attraverso la revisione paritaria dei benchmark. Ciò comporta che esperti indipendenti esaminino e convalidino la metodologia, i set di dati e i risultati del benchmark per garantirne la credibilità e l’affidabilità. La revisione paritaria può fornire un mezzo prezioso per verificare l’accuratezza dei test di benchmark e contribuire a creare fiducia nei risultati.\nLa standardizzazione dei benchmark è un’altra importante soluzione per mitigare l’ingegneria dei benchmark. I benchmark standardizzati forniscono un quadro comune per la valutazione dei sistemi di intelligenza artificiale, garantendo coerenza e comparabilità tra diversi sistemi e applicazioni. Ciò può essere ottenuto sviluppando standard e “best practice” per l’intero settore per il benchmarking e tramite metriche e criteri di valutazione comuni.\nAnche la verifica da parte di terze parti dei risultati può essere preziosa per mitigare l’ingegneria dei benchmark. Ciò comporta che una terza parte indipendente verifichi i risultati di un test di benchmark per garantirne la credibilità e l’affidabilità. La verifica di terze parti può creare fiducia nei risultati e fornire un mezzo prezioso per convalidare le prestazioni e le capacità dei sistemi di intelligenza artificiale.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmarking-del-modello",
    "href": "contents/benchmarking/benchmarking.it.html#benchmarking-del-modello",
    "title": "11  Benchmarking AI",
    "section": "11.5 Benchmarking del Modello",
    "text": "11.5 Benchmarking del Modello\nIl benchmarking dei modelli di machine learning è importante per determinare l’efficacia e l’efficienza di vari algoritmi di apprendimento automatico nella risoluzione di compiti o problemi specifici. Analizzando i risultati ottenuti dal benchmarking, sviluppatori e ricercatori possono identificare i punti di forza e di debolezza dei loro modelli, portando a decisioni più informate sulla selezione del modello e su un’ulteriore ottimizzazione.\nL’evoluzione e il progresso dei modelli di apprendimento automatico sono intrinsecamente collegati alla disponibilità e alla qualità dei set di dati. Nell’apprendimento automatico, i dati fungono da materia prima che alimenta gli algoritmi, consentendo loro di apprendere, adattarsi e, in definitiva, eseguire compiti che erano tradizionalmente di dominio degli esseri umani. Pertanto, è importante comprendere questa storia.\n\n11.5.1 Contesto Storico\nI dataset di apprendimento automatico hanno una storia ricca e si sono evoluti in modo significativo nel corso degli anni, crescendo in dimensioni, complessità e diversità per soddisfare le richieste sempre crescenti del settore. Diamo un’occhiata più da vicino a questa evoluzione, partendo da uno dei primi e più iconici set di dati: MNIST.\n\nMNIST (1998)\nIl dataset MNIST, creato da Yann LeCun, Corinna Cortes e Christopher J.C. Burges nel 1998, può essere considerato una pietra miliare nella storia dei dataset di machine learning. Comprende 70.000 immagini in scala di grigi da 28x28 pixel etichettate di cifre scritte a mano (0-9). MNIST è stato ampiamente utilizzato per il benchmarking degli algoritmi nell’elaborazione delle immagini e nell’apprendimento automatico come punto di partenza per molti ricercatori e professionisti. Figura 11.4 mostra alcuni esempi di cifre scritte a mano.\n\n\n\n\n\n\nFigura 11.4: Cifre scritte a mano in MNIST. Fonte: Suvanjanprasai.\n\n\n\n\n\nImageNet (2009)\nFacciamo un salto al 2009 e vediamo l’introduzione di ImageNet, che ha segnato un balzo significativo nella scala e nella complessità dei dataset. ImageNet è composto da oltre 14 milioni di immagini etichettate che abbracciano più di 20.000 categorie. Fei-Fei Li e il suo team lo hanno sviluppato per far progredire il riconoscimento degli oggetti e la ricerca sulla visione artificiale. Il dataset è diventato sinonimo della ImageNet Large Scale Visual Recognition Challenge (ILSVRC), una competizione annuale cruciale nello sviluppo di modelli di deep learning, tra cui il famoso AlexNet nel 2012.\n\n\nCOCO (2014)\nIl Common Objects in Context (COCO) dataset (Lin et al. 2014), rilasciato nel 2014, ha ulteriormente ampliato il panorama dei set di dati di apprendimento automatico introducendo un set più ricco di annotazioni. COCO è costituito da immagini contenenti scene complesse con più oggetti e ogni immagine è annotata con riquadri di delimitazione degli oggetti, maschere di segmentazione e didascalie. Questo set di dati è stato determinante nel far progredire la ricerca nel rilevamento degli oggetti, nella segmentazione e nella didascalia delle immagini.\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, e C Lawrence Zitnick. 2014. «Microsoft coco: Common objects in context». In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740–55. Springer.\nhttps://cocodataset.org/images/jpg/coco-examples.jpg\n\n\nGPT-3 (2020)\nSebbene gli esempi sopra riportati si concentrino principalmente sui dataset di immagini, si sono verificati anche sviluppi significativi nei dataset di testo. Un esempio degno di nota è GPT-3 (Brown et al. 2020), sviluppato da OpenAI. GPT-3 è un modello linguistico addestrato su testo Internet eterogeneo. Sebbene il dataset utilizzato per addestrare GPT-3 non sia disponibile al pubblico, il modello stesso, costituito da 175 miliardi di parametri, è una testimonianza della scala e della complessità dei moderni dataset e modelli di apprendimento automatico.\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. «Language Models are Few-Shot Learners». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nPresente e Futuro\nOggi disponiamo di una pletora di dataset che abbracciano vari domini, tra cui sanità, finanza, scienze sociali e altro ancora. Le seguenti caratteristiche ci aiutano a classificare lo spazio e la crescita dei dataset di apprendimento automatico che alimentano lo sviluppo del modello.\n\nDiversità dei Set di Dati: La varietà di set di dati disponibili per ricercatori e ingegneri si è ampliata notevolmente, coprendo molti campi, tra cui l’elaborazione del linguaggio naturale, il riconoscimento delle immagini e altro ancora. Questa diversità ha alimentato lo sviluppo di modelli di apprendimento automatico specializzati, su misura per attività specifiche, come la traduzione, il riconoscimento vocale e il riconoscimento facciale.\nVolume di Dati: L’enorme volume di dati che è diventato disponibile nell’era digitale ha anche svolto un ruolo cruciale nel progresso dei modelli di apprendimento automatico. I grandi set di dati consentono ai modelli di catturare la complessità e le sfumature dei fenomeni del mondo reale, portando a previsioni più accurate e affidabili.\nQualità e Pulizia dei Dati: La qualità dei dati è un altro fattore critico che influenza le prestazioni dei modelli di apprendimento automatico. Set di dati puliti, ben etichettati e imparziali sono essenziali per modelli di addestramento solidi ed equi.\nAccesso Aperto ai Dati: La disponibilità di set di dati “open-access” ha contribuito in modo significativo anche al progresso dell’apprendimento automatico. I dati “aperti” consentono ai ricercatori di tutto il mondo di collaborare, condividere approfondimenti e basarsi sul lavoro degli altri, portando a un’innovazione più rapida e allo sviluppo di modelli più avanzati.\nProblemi di Etica e Privacy: Man mano che i set di dati crescono in dimensioni e complessità, le considerazioni etiche e i problemi di privacy diventano sempre più importanti. È in corso un dibattito sull’equilibrio tra lo sfruttamento dei dati per i progressi dell’apprendimento automatico e la protezione dei diritti alla privacy degli individui.\n\nLo sviluppo di modelli di apprendimento automatico si basa in larga misura sulla disponibilità di set di dati diversificati, grandi, di alta qualità e ad accesso libero. Mentre andiamo avanti, affrontare le considerazioni etiche e le preoccupazioni sulla privacy associate all’uso di grandi set di dati è fondamentale per garantire che le tecnologie di apprendimento automatico siano vantaggiose per la società. C’è una crescente consapevolezza che i dati agiscono come carburante per l’apprendimento automatico, guidando e alimentando lo sviluppo di modelli di apprendimento automatico. Di conseguenza, si sta ponendo maggiore attenzione sullo sviluppo dei set di dati stessi. Esploreremo questo aspetto in modo più dettagliato nella sezione del benchmarking dei dati.\n\n\n\n11.5.2 Metriche del Modello\nLa valutazione del modello di machine learning si è evoluta da un focus ristretto sulla precisione a un approccio più completo che considera una serie di fattori, da considerazioni etiche e applicabilità nel mondo reale a vincoli pratici come dimensioni ed efficienza del modello. Questo cambiamento riflette la maturazione del campo poiché i modelli di apprendimento automatico vengono sempre più applicati in scenari reali diversi e complessi.\n\nPrecisione\nLa precisione è una delle metriche più intuitive e comunemente utilizzate per valutare i modelli di apprendimento automatico. In sostanza, la precisione misura la percentuale di previsioni corrette effettuate dal modello rispetto a tutte le previsioni. Ad esempio, immaginiamo di aver sviluppato un modello di apprendimento automatico per classificare le immagini come contenenti o meno un gatto. Se testiamo questo modello su un set di dati di 100 immagini e ne identifica correttamente 90, calcoleremmo la sua precisione al 90%.\nNelle fasi iniziali dell’apprendimento automatico, la precisione era spesso la metrica principale, se non l’unica, considerata quando si valutavano le prestazioni del modello. Ciò è comprensibile, data la sua natura semplice e la facilità di interpretazione. Tuttavia, con il progredire del settore, i limiti del fare affidamento esclusivamente sulla precisione sono diventati più evidenti.\nSi consideri l’esempio di un modello di diagnosi medica con una precisione del 95%. Sebbene a prima vista possa sembrare impressionante, dobbiamo guardare più a fondo per valutare appieno le prestazioni del modello. Supponiamo che il modello non riesca a diagnosticare accuratamente condizioni gravi che, sebbene rare, possono avere gravi conseguenze; la sua elevata precisione potrebbe non essere così significativa. Un esempio pertinente di ciò è il modello di apprendimento automatico della retinopatia di Google, progettato per diagnosticare la retinopatia diabetica e l’edema maculare diabetico da fotografie della retina.\nIl modello di Google ha dimostrato livelli di precisione impressionanti in contesti di laboratorio. Tuttavia, quando è stato distribuito in ambienti clinici reali in Thailandia, ha dovuto affrontare sfide significative. Nel contesto reale, il modello ha incontrato popolazioni di pazienti diverse, qualità delle immagini variabili e una gamma di diverse condizioni mediche a cui non era stato esposto durante il suo training. Di conseguenza, le sue prestazioni avrebbero potuto essere migliori e ha fatto fatica a mantenere gli stessi livelli di accuratezza osservati in laboratorio. Questo esempio serve come un chiaro promemoria del fatto che, sebbene un’elevata accuratezza sia un attributo importante e desiderabile per un modello di diagnosi medica, deve essere valutata insieme ad altri fattori, come la capacità del modello di generalizzare a diverse popolazioni e gestire condizioni reali diverse e imprevedibili, per comprenderne veramente il valore e il potenziale impatto sull’assistenza ai pazienti.\nAllo stesso modo, se il modello funziona bene in media ma mostra significative disparità nelle prestazioni tra diversi gruppi demografici, anche questo sarebbe motivo di preoccupazione.\nL’evoluzione dell’apprendimento automatico ha quindi visto uno spostamento verso un approccio più olistico alla valutazione del modello, tenendo conto non solo dell’accuratezza, ma anche di altri fattori cruciali come la correttezza, trasparenza e applicabilità nel mondo reale. Un esempio lampante è il progetto Gender Shades del MIT Media Lab, guidato da Joy Buolamwini, che evidenzia significativi pregiudizi razziali e di genere nei sistemi commerciali di riconoscimento facciale. Il progetto ha valutato le prestazioni di tre tecnologie di riconoscimento facciale sviluppate da IBM, Microsoft e Face++. Ha scoperto che tutte presentavano dei pregiudizi, con prestazioni migliori su volti maschili e dalla pelle più chiara rispetto a volti femminili e dalla pelle più scura.\nSebbene l’accuratezza rimanga una metrica fondamentale e preziosa per la valutazione dei modelli di apprendimento automatico, è necessario un approccio più completo per valutare appieno le prestazioni di un modello. Ciò significa considerare metriche aggiuntive che tengano conto di correttezza, trasparenza e applicabilità nel mondo reale, nonché condurre test rigorosi su diversi set di dati per scoprire e mitigare eventuali potenziali pregiudizi. Il passaggio a un approccio più olistico alla valutazione del modello riflette la maturazione del campo e il suo crescente riconoscimento delle implicazioni nel mondo reale e delle considerazioni etiche associate all’implementazione di modelli di apprendimento automatico.\n\n\nCorrettezza\nLa correttezza nei modelli di apprendimento automatico è un aspetto multiforme e critico che richiede un’attenzione particolare, in particolare nelle applicazioni ad alto rischio che influenzano significativamente la vita delle persone, come nei processi di approvazione dei prestiti, nelle assunzioni e nella giustizia penale. Si riferisce al trattamento equo di tutti gli individui, indipendentemente dai loro attributi demografici o sociali come razza, genere, età o stato socioeconomico.\nAffidarsi semplicemente all’accuratezza può essere insufficiente e potenzialmente fuorviante quando si valutano i modelli. Ad esempio, si consideri un modello di approvazione dei prestiti con un tasso di accuratezza del 95%. Sebbene questa cifra possa sembrare impressionante a prima vista, non rivela come il modello si comporta nei diversi gruppi demografici. Se questo modello discrimina costantemente un gruppo particolare, la sua accuratezza è meno encomiabile e la sua correttezza viene messa in discussione.\nLa discriminazione può manifestarsi in varie forme, come la discriminazione diretta, in cui un modello utilizza esplicitamente attributi sensibili come razza o genere nel suo processo decisionale, o discriminazione indiretta, in cui variabili apparentemente neutre sono correlate ad attributi sensibili, influenzando indirettamente i risultati del modello. Un esempio infame di quest’ultimo è lo strumento COMPAS utilizzato nel sistema di giustizia penale degli Stati Uniti, che ha mostrato pregiudizi razziali nel prevedere i tassi di recidiva nonostante non utilizzasse esplicitamente la razza come variabile.\nAffrontare l’equità implica un attento esame delle prestazioni del modello tra gruppi diversi, identificando potenziali pregiudizi e rettificando le disparità attraverso misure correttive come il ribilanciamento dei set di dati, l’adeguamento dei parametri del modello e l’implementazione di algoritmi consapevoli dell’equità e della correttezza. Ricercatori e professionisti sviluppano continuamente metriche e metodologie su misura per casi d’uso specifici per valutare la correttezza e l’equità in scenari del mondo reale. Ad esempio, l’analisi di impatto disparato, la parità demografica e le pari opportunità sono alcune delle metriche impiegate per valutare l’equità/correttezza.\nInoltre, la trasparenza e l’interpretabilità dei modelli sono fondamentali per raggiungere la correttezza. Comprendere come un modello prende decisioni può rivelare potenziali pregiudizi e consentire alle parti interessate di ritenere responsabili gli sviluppatori. Strumenti open source come AI Fairness 360 di IBM e Fairness Indicators di TensorFlow sono in fase di sviluppo per facilitare le valutazioni dell’equità/correttezza e l’attenuazione dei pregiudizi nei modelli di apprendimento automatico.\nGarantire l’equità/correttezza nei modelli di apprendimento automatico, in particolare nelle applicazioni che hanno un impatto significativo sulla vita delle persone, richiede una rigorosa valutazione delle prestazioni del modello in gruppi diversi, un’attenta identificazione e attenuazione dei pregiudizi e l’implementazione di misure di trasparenza e interpretabilità. Affrontando la correttezza in modo completo, possiamo lavorare per sviluppare modelli di apprendimento automatico equi, giusti e vantaggiosi per la società.\n\n\nComplessità\n\nParameteri*\nNelle fasi iniziali del machine learning, il benchmarking dei modelli si basava spesso sui conteggi dei parametri come proxy [sostituto] per la complessità del modello. La logica era che più parametri in genere portano a un modello più complesso, che dovrebbe, a sua volta, fornire prestazioni migliori. Tuttavia, questo approccio si è dimostrato inadeguato in quanto deve tenere conto del costo computazionale associato all’elaborazione di molti parametri.\nAd esempio, GPT-3, sviluppato da OpenAI, è un modello linguistico che vanta ben 175 miliardi di parametri. Sebbene raggiunga prestazioni all’avanguardia in varie attività di elaborazione del linguaggio naturale, le sue dimensioni e le risorse computazionali necessarie per eseguirlo lo rendono poco pratico per l’implementazione in molti scenari del mondo reale, in particolare quelli con capacità computazionali limitate.\nAffidarsi ai conteggi dei parametri come proxy per la complessità del modello non riesce a considerare anche l’efficienza del modello. Se ottimizzato per l’efficienza, un modello con meno parametri potrebbe essere altrettanto efficace, se non di più, di un modello con un conteggio di parametri più elevato. Ad esempio, MobileNets, sviluppato da Google, è una famiglia di modelli progettati specificamente per dispositivi mobili ed edge. Utilizzano convoluzioni separabili in base alla profondità per ridurre il numero di parametri e i costi computazionali, pur mantenendo prestazioni competitive.\nAlla luce di queste limitazioni, il settore si è spostato verso un approccio più olistico al benchmarking dei modelli che considera i conteggi dei parametri e altri fattori cruciali come le operazioni in virgola mobile al secondo (FLOP), il consumo di memoria e la latenza. I FLOP, in particolare, sono emersi come una metrica importante in quanto forniscono una rappresentazione più accurata del carico computazionale imposto da un modello. Questo passaggio a un approccio più completo al benchmarking dei modelli riflette il riconoscimento della necessità di bilanciare prestazioni e praticità, assicurando che i modelli siano efficaci, efficienti e implementabili in scenari reali.\n\n\nFLOP\nLa dimensione di un modello di apprendimento automatico è un aspetto essenziale che influisce direttamente sulla sua usabilità in scenari pratici, soprattutto quando le risorse computazionali sono limitate. Tradizionalmente, il numero di parametri in un modello veniva spesso utilizzato come proxy per le sue dimensioni, con l’ipotesi di base che più parametri si sarebbero tradotti in prestazioni migliori. Tuttavia, questa visione semplicistica non considera il costo computazionale dell’elaborazione di questi parametri. È qui che entra in gioco il concetto di “floating-point operations per second (FLOP)” [operazioni in virgola mobile al secondo], che fornisce una rappresentazione più accurata del carico computazionale imposto da un modello.\nI FLOP misurano il numero di operazioni in virgola mobile eseguite da un modello per generare una previsione. Un modello con molti FLOP richiede risorse computazionali sostanziali per elaborare il vasto numero di operazioni, il che potrebbe renderlo poco pratico per alcune applicazioni. Al contrario, un modello con un conteggio di FLOP inferiore è più leggero e può essere facilmente distribuito in scenari in cui le risorse computazionali sono limitate.\nFigura 11.5, da (Bianco et al. 2018), mostra la relazione tra la Top-1 Accuracy su ImageNet (asse y), i G-FLOP del modello (asse x) e il conteggio dei parametri del modello (dimensione del cerchio).\n\n\n\n\n\n\nFigura 11.5: Un grafico che raffigura la top-1 Imagenet Accuracy rispetto al conteggio FLOP di un modello insieme al conteggio dei parametri del modello. La figura mostra un compromesso complessivo tra complessità e accuratezza del modello, sebbene alcune architetture del modello siano più efficienti di altre. Fonte: Bianco et al. (2018).\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, e Paolo Napoletano. 2018. «Benchmark analysis of representative deep neural network architectures». IEEE access 6: 64270–77.\n\n\nConsideriamo un esempio. BERT [Bidirectional Encoder Representations from Transformers] (Devlin et al. 2019), un popolare modello di elaborazione del linguaggio naturale, ha oltre 340 milioni di parametri, il che lo rende un modello di grandi dimensioni con elevata accuratezza e prestazioni impressionanti in varie attività. Tuttavia, le dimensioni di BERT, unite al suo elevato numero di FLOP, lo rendono un modello computazionalmente intensivo che potrebbe non essere adatto per applicazioni in tempo reale o per l’implementazione su dispositivi edge con capacità computazionali limitate.\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, e Kristina Toutanova. 2019. «BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding». In Proceedings of the 2019 Conference of the North, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423.\nAlla luce di ciò, c’è stato un crescente interesse nello sviluppo di modelli più piccoli in grado di raggiungere livelli di prestazioni simili alle loro controparti più grandi, pur essendo più efficienti nel carico computazionale. DistilBERT, ad esempio, è una versione più piccola di BERT che mantiene il 97% delle sue prestazioni, pur essendo il 40% più piccola in termini di numero di parametri. La riduzione delle dimensioni si traduce anche in un numero di FLOP inferiore, rendendo DistilBERT una scelta più pratica per scenari con risorse limitate.\nIn sintesi, mentre il conteggio dei parametri fornisce un’indicazione utile della dimensione del modello, non è una metrica completa in quanto deve considerare il costo computazionale associato all’elaborazione di questi parametri. I FLOP, d’altro canto, offrono una rappresentazione più accurata del carico computazionale di un modello e sono quindi una considerazione essenziale quando si distribuiscono modelli di apprendimento automatico in scenari reali, in particolare quando le risorse computazionali sono limitate. L’evoluzione dal basarsi esclusivamente sul conteggio dei parametri alla considerazione dei FLOP indica una maturazione nel campo, che riflette una maggiore consapevolezza dei vincoli pratici e delle sfide dell’implementazione di modelli di apprendimento automatico in contesti diversi.\n\n\nEfficienza\nAnche le metriche di efficienza, come il consumo di memoria e la latenza/capacità di elaborazione, hanno acquisito importanza. Queste metriche sono particolarmente cruciali quando si distribuiscono modelli su dispositivi edge o in applicazioni in tempo reale, poiché misurano la velocità con cui un modello può elaborare i dati e la quantità di memoria richiesta. In questo contesto, le curve di Pareto vengono spesso utilizzate per visualizzare il compromesso tra diverse metriche, aiutando le parti interessate a decidere quale modello si adatta meglio alle loro esigenze.\n\n\n\n\n11.5.3 Lezioni Apprese\nIl benchmarking dei modelli ci ha offerto diverse preziose intuizioni che possono essere sfruttate per guidare l’innovazione nei benchmark di sistema. La progressione dei modelli di apprendimento automatico è stata profondamente influenzata dall’avvento delle classifiche e dalla disponibilità open source di modelli e set di dati. Questi elementi hanno svolto il ruolo di catalizzatori significativi, spingendo l’innovazione e accelerando l’integrazione di modelli all’avanguardia negli ambienti di produzione. Tuttavia, come approfondiremo ulteriormente, questi non sono gli unici fattori che contribuiscono allo sviluppo dei benchmark di apprendimento automatico.\nLe classifiche svolgono un ruolo fondamentale nel fornire un metodo oggettivo e trasparente per ricercatori e professionisti per valutare l’efficacia di diversi modelli, classificandoli in base alle loro prestazioni nei benchmark. Questo sistema promuove un ambiente competitivo, incoraggiando lo sviluppo di modelli che non siano solo accurati ma anche efficienti. L’ImageNet Large Scale Visual Recognition Challenge (ILSVRC) ne è un ottimo esempio, con la sua classifica annuale che contribuisce in modo significativo allo sviluppo di modelli innovativi come AlexNet.\nL’accesso open source a modelli e set di dati all’avanguardia diffonde ulteriormente l’apprendimento automatico, facilitando la collaborazione tra ricercatori e professionisti in tutto il mondo. Questo accesso aperto accelera il processo di test, convalida e distribuzione di nuovi modelli in ambienti di produzione, come dimostrato dall’adozione diffusa di modelli come BERT e GPT-3 in varie applicazioni, dall’elaborazione del linguaggio naturale a compiti multimodali più complessi.\nPiattaforme di collaborazione della comunità come Kaggle hanno rivoluzionato il settore ospitando competizioni che uniscono data scientist da tutto il mondo per risolvere problemi intricati. Benchmark specifici fungono da paletti per l’innovazione e lo sviluppo di modelli.\nInoltre, la disponibilità di set di dati diversi e di alta qualità è fondamentale per l’addestramento e il test dei modelli di apprendimento automatico. Set di dati come ImageNet hanno svolto un ruolo fondamentale nell’evoluzione dei modelli di riconoscimento delle immagini, mentre ampi set di dati di testo hanno facilitato i progressi nei modelli di elaborazione del linguaggio naturale.\nInfine, è necessario supportare i contributi di istituti accademici e di ricerca. Il loro ruolo nella pubblicazione di articoli di ricerca, nella condivisione di risultati in conferenze e nella promozione della collaborazione tra varie istituzioni ha contribuito in modo significativo al progresso dei modelli e dei benchmark di apprendimento automatico.\n\nTendenze Emergenti\nMan mano che i modelli di apprendimento automatico diventano più sofisticati, lo diventano anche i benchmark necessari per valutarli in modo accurato. Ci sono diversi benchmark e dataset emergenti che stanno guadagnando popolarità grazie alla loro capacità di valutare i modelli in scenari più complessi e realistici:\nDataset Multimodali: Questi set di dati contengono più tipi di dati, come testo, immagini e audio, per rappresentare meglio le situazioni del mondo reale. Un esempio è VQA (Visual Question Answering) (Antol et al. 2015), in cui viene testata la capacità dei modelli di rispondere a domande basate su testo sulle immagini.\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, e Devi Parikh. 2015. «VQA: Visual Question Answering». In 2015 IEEE International Conference on Computer Vision (ICCV), 2425–33. IEEE. https://doi.org/10.1109/iccv.2015.279.\nValutazione di Correttezza e Bias: C’è una crescente attenzione alla creazione di benchmark che valutino l’equità/Correttezza e i bias [pregiudizi] dei modelli di apprendimento automatico. Esempi includono il toolkit AI Fairness 360, che offre un set completo di metriche e set di dati per valutare il bias nei modelli.\nGeneralizzazione Out-of-Distribution: Test di quanto bene i modelli funzionano su dati diversi dalla distribuzione di training originale. Questo valuta la capacità del modello di generalizzare a dati nuovi e inediti. Esempi di benchmark sono Wilds (Koh et al. 2021), RxRx e ANC-Bench.\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. «WILDS: A Benchmark of in-the-Wild Distribution Shifts». In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:5637–64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, e Dawn Song. 2021. «Natural Adversarial Examples». In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15262–71. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, e Quoc V. Le. 2020. «Adversarial Examples Improve Image Recognition». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 816–25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\nRobustezza Avversaria: Valutazione delle prestazioni del modello in caso di attacchi avversari o perturbazioni ai dati di input. Questo testa la robustezza del modello. Esempi di benchmark sono ImageNet-A (Hendrycks et al. 2021), ImageNet-C (Xie et al. 2020) e CIFAR-10.1.\nPrestazioni nel Mondo Reale: Test di modelli su set di dati del mondo reale che corrispondono da vicino alle attività finali anziché solo su set di dati di benchmark predefiniti. Esempi sono set di dati di imaging medico per attività sanitarie o log di chat di assistenza clienti per sistemi di dialogo.\nEfficienza Energetica e di Calcolo: Benchmark che misurano le risorse di calcolo necessarie per ottenere una particolare accuratezza. Questo valuta l’efficienza del modello. Esempi sono MLPerf e Greenbench, già discussi nella sezione Benchmarking dei sistemi.\nInterpretabilità e Spiegabilità: Benchmark che valutano quanto sia facile comprendere e spiegare la logica interna e le previsioni di un modello. Esempi di parametri sono la fedeltà ai gradienti di input e la coerenza delle spiegazioni.\n\n\n\n11.5.4 Limitazioni e Sfide\nSebbene i benchmark dei modelli siano uno strumento essenziale per valutare i modelli di machine learning, è necessario affrontare diverse limitazioni e sfide per garantire che riflettano accuratamente le prestazioni in scenari reali.\nIl dataset non corrisponde a scenari reali: Spesso, i dati utilizzati nei benchmark dei modelli vengono puliti e preelaborati a tal punto che potrebbe essere necessario rappresentare accuratamente i dati che un modello incontrerebbe in applicazioni reali. Questa versione idealizzata dei dati può portare a una sovrastima delle prestazioni di un modello. Nel caso del set di dati ImageNet, le immagini sono ben etichettate e categorizzate. Tuttavia, in uno scenario reale, un modello potrebbe dover gestire immagini sfocate che potrebbero essere meglio illuminate o scattate da angolazioni scomode. Questa discrepanza può influire in modo significativo sulle prestazioni del modello.\nSim2Real Gap: Il Sim2Real Gap si riferisce alla differenza nelle prestazioni di un modello quando si passa da un ambiente simulato a un ambiente reale. Questo gap è spesso osservato nella robotica, dove un robot addestrato in un ambiente simulato ha difficoltà a svolgere compiti nel mondo reale a causa della complessità e dell’imprevedibilità degli ambienti reali. Un robot addestrato a raccogliere oggetti in un ambiente simulato potrebbe aver bisogno di aiuto per svolgere lo stesso compito nel mondo reale perché l’ambiente simulato non rappresenta accuratamente le complessità della fisica, dell’illuminazione e della variabilità degli oggetti del mondo reale.\nSfide nella Creazione di Dataset: La creazione di un set di dati per il benchmarking del modello è un’attività impegnativa che richiede un’attenta considerazione di vari fattori come qualità dei dati, diversità e rappresentazione. Come discusso nella sezione di ingegneria dei dati, garantire che i dati siano puliti, imparziali e rappresentativi dello scenario del mondo reale è fondamentale per l’accuratezza e l’affidabilità del benchmark. Ad esempio, quando si crea un set di dati per un’attività correlata all’assistenza sanitaria, è importante assicurarsi che i dati siano rappresentativi dell’intera popolazione e non distorti verso un particolare gruppo demografico. Ciò garantisce che il modello funzioni bene in diverse popolazioni di pazienti.\nI benchmark del modello sono essenziali per misurare la capacità di un’architettura di modello di risolvere un’attività fissa, ma è importante affrontare le limitazioni e le sfide ad essi associate. Ciò include il garantire che il set di dati rappresenti accuratamente scenari del mondo reale, affrontare il divario Sim2Real e superare le sfide della creazione di set di dati imparziali e rappresentativi. Affrontando queste sfide e molte altre, possiamo garantire che i benchmark del modello forniscano una valutazione più accurata e affidabile delle prestazioni di un modello in applicazioni del mondo reale.\nLo Speech Commands dataset e il suo successore MSWC sono benchmark comuni per una delle applicazioni TinyML per eccellenza, l’individuazione delle parole chiave. I comandi vocali stabiliscono metriche di errore di streaming oltre la precisione di classificazione standard top-1 più pertinenti al caso d’uso di individuazione delle parole chiave. L’utilizzo di metriche pertinenti ai casi è ciò che eleva un dataset a un benchmark del modello.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmarking-dei-dati",
    "href": "contents/benchmarking/benchmarking.it.html#benchmarking-dei-dati",
    "title": "11  Benchmarking AI",
    "section": "11.6 Benchmarking dei Dati",
    "text": "11.6 Benchmarking dei Dati\nNegli ultimi anni, l’intelligenza artificiale si è concentrata sullo sviluppo di modelli di apprendimento automatico sempre più sofisticati, come i grandi modelli linguistici. L’obiettivo è stato quello di creare modelli in grado di prestazioni di livello umano o sovrumane su un’ampia gamma di attività, addestrandoli su enormi set di dati. Questo approccio incentrato sul modello ha prodotto rapidi progressi, con modelli che hanno ottenuto risultati all’avanguardia su molti benchmark consolidati. Figura 11.6 mostra le prestazioni dei sistemi di intelligenza artificiale rispetto alle prestazioni umane (contrassegnate dalla linea orizzontale a 0) in cinque applicazioni: riconoscimento della scrittura a mano, riconoscimento vocale, riconoscimento delle immagini, comprensione della lettura e comprensione del linguaggio. Negli ultimi dieci anni, le prestazioni dell’intelligenza artificiale hanno superato quelle degli esseri umani.\nTuttavia, le crescenti preoccupazioni su questioni come pregiudizi, sicurezza e robustezza persistono anche nei modelli che raggiungono un’elevata accuratezza sui benchmark standard. Inoltre, alcuni set di dati popolari utilizzati per la valutazione dei modelli stanno iniziando a saturarsi, con modelli che raggiungono prestazioni quasi perfette su divisioni di test esistenti (Kiela et al. 2021). Come semplice esempio, ci sono immagini di test nel classico dataset di cifre scritte a mano MNIST che potrebbero sembrare indecifrabili per la maggior parte dei valutatori umani, ma a cui è stata assegnata un’etichetta quando è stato creato il set di dati: i modelli che concordano con quelle etichette potrebbero sembrare esibire prestazioni sovrumane, ma potrebbero invece catturare solo idiosincrasie del processo di etichettatura e acquisizione dalla creazione del set di dati nel 1994. Con lo stesso spirito, i ricercatori di visione artificiale ora chiedono: “Abbiamo finito con ImageNet?” (Beyer et al. 2020). Ciò evidenzia i limiti nell’approccio convenzionale incentrato sul modello di ottimizzazione dell’accuratezza su set di dati fissi tramite innovazioni architettoniche.\n\nBeyer, Lucas, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, e Aäron van den Oord. 2020. «Are we done with imagenet?» ArXiv preprint abs/2006.07159. https://arxiv.org/abs/2006.07159.\n\n\n\n\n\n\nFigura 11.6: IA e prestazioni umane. Fonte: Kiela et al. (2021).\n\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. «Dynabench: Rethinking Benchmarking in NLP». In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4110–24. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\n\nSta emergendo un paradigma alternativo chiamato IA incentrata sui dati. Invece di trattare i dati come statici e concentrarsi strettamente sulle prestazioni del modello, questo approccio riconosce che i modelli sono validi solo quanto i loro dati di training. Quindi, l’enfasi si sposta sulla cura di dataset di alta qualità che riflettano meglio la complessità del mondo reale, sviluppando benchmark di valutazione più informativi e considerando attentamente come i dati vengono campionati, preelaborati e aumentati. L’obiettivo è ottimizzare il comportamento del modello migliorando i dati anziché semplicemente ottimizzando le metriche su set di dati imperfetti. L’intelligenza artificiale incentrata sui dati esamina e migliora criticamente i dati stessi per produrre un’intelligenza artificiale utile. Ciò riflette un’importante evoluzione nella mentalità, poiché il campo affronta le carenze di un benchmarking ristretto.\nQuesta sezione esplorerà le principali differenze tra gli approcci all’intelligenza artificiale incentrati sui modelli e sui dati. Questa distinzione ha importanti implicazioni sul modo in cui eseguiamo il benchmarking dei sistemi di intelligenza artificiale. In particolare, vedremo come concentrarsi sulla qualità dei dati e sull’efficienza può migliorare direttamente le prestazioni dell’apprendimento automatico come alternativa all’ottimizzazione delle sole architetture dei modelli. L’approccio incentrato sui dati riconosce che i modelli sono validi solo quanto i loro dati di addestramento. Quindi, migliorare la cura dei dati, i benchmark di valutazione e i processi di gestione dei dati può produrre sistemi di intelligenza artificiale più sicuri, più equi e più robusti. Ripensare al benchmarking per dare priorità ai dati insieme ai modelli rappresenta un’importante evoluzione, poiché il campo mira a fornire un impatto affidabile nel mondo reale.\n\n11.6.1 Limitazioni dell’IA Incentrata sul Modello\nNell’era dell’IA incentrata sul modello, una caratteristica importante era lo sviluppo di architetture di modelli complesse. Ricercatori e professionisti hanno dedicato notevoli sforzi alla progettazione di modelli sofisticati e intricati nella ricerca di prestazioni superiori. Ciò ha spesso comportato l’incorporazione di livelli aggiuntivi e la messa a punto di una moltitudine di iperparametri per ottenere miglioramenti nell’accuratezza. Contemporaneamente, c’era una notevole enfasi sullo sfruttamento di algoritmi avanzati. Questi algoritmi, spesso in prima linea nelle ultime ricerche, sono stati impiegati per migliorare le prestazioni dei modelli di IA. L’obiettivo principale di questi algoritmi era ottimizzare il processo di apprendimento dei modelli, estraendo così il massimo delle informazioni dai dati di addestramento.\nSebbene l’approccio incentrato sul modello sia stato centrale per molti progressi nell’IA, ha diverse aree di miglioramento. Innanzitutto, lo sviluppo di architetture di modelli complesse può spesso portare a un overfitting. Questo è quando il modello funziona bene sui dati di addestramento ma deve generalizzare a nuovi dati mai visti. I layer aggiuntivi e la complessità possono catturare il rumore nei dati di training come se fosse un pattern reale, danneggiando le prestazioni del modello su nuovi dati.\nIn secondo luogo, affidarsi ad algoritmi avanzati può a volte oscurare la reale comprensione del funzionamento di un modello. Questi algoritmi spesso agiscono come una scatola nera, rendendo difficile interpretare il modo in cui il modello prende decisioni. Questa mancanza di trasparenza può essere un ostacolo significativo, specialmente in applicazioni critiche come sanità e finanza, dove la comprensione del processo decisionale del modello è fondamentale.\nIn terzo luogo, l’enfasi sul raggiungimento di risultati all’avanguardia su set di dati di riferimento può a volte essere fuorviante. Questi dataset devono rappresentare in modo più completo le complessità e la variabilità dei dati del mondo reale. Un modello che funziona bene su un set di dati di riferimento potrebbe non essere necessariamente generalizzato bene a dati nuovi e mai visti in un’applicazione del mondo reale. Questa discrepanza può portare a una falsa fiducia nelle capacità del modello e ostacolarne l’applicabilità pratica.\nInfine, l’approccio incentrato sul modello spesso si basa su grandi set di dati etichettati per l’addestramento. Tuttavia, ottenere tali set di dati richiede tempo e impegno in molti scenari del mondo reale. Questa dipendenza da grandi dataset limita anche l’applicabilità dell’IA in domini in cui i dati sono scarsi o costosi da etichettare.\nCome risultato delle ragioni di cui sopra, e di molte altre, la comunità dell’IA sta passando a un approccio più incentrato sui dati. Invece di concentrarsi solo sull’architettura del modello, i ricercatori stanno ora dando priorità alla cura di set di dati di alta qualità, allo sviluppo di migliori benchmark di valutazione e alla considerazione di come i dati vengono campionati e preelaborati. L’idea chiave è che i modelli sono validi solo quanto i loro dati di training. Quindi, concentrandoci sull’ottenimento dei dati giusti, potremo sviluppare sistemi di intelligenza artificiale più equi, sicuri e allineati con i valori umani. Questo cambiamento incentrato sui dati rappresenta un importante cambiamento di mentalità man mano che l’intelligenza artificiale progredisce.\n\n\n11.6.2 Verso un’Intelligenza Artificiale Incentrata sui Dati\nL’intelligenza artificiale incentrata sui dati è un paradigma che sottolinea l’importanza di dataset di alta qualità, ben etichettati e diversificati nello sviluppo di modelli di intelligenza artificiale. Contrariamente all’approccio incentrato sul modello, che si concentra sulla rifinitura e l’iterazione dell’architettura e dell’algoritmo del modello per migliorare le prestazioni, l’intelligenza artificiale incentrata sui dati dà priorità alla qualità dei dati di input come motore principale per migliorare le prestazioni del modello. I dati di alta qualità sono puliti, ben etichettati e rappresentativi degli scenari del mondo reale che il modello incontrerà. Al contrario, i dati di bassa qualità possono portare a scarse prestazioni del modello, indipendentemente dalla complessità o dalla sofisticatezza dell’architettura del modello.\nL’intelligenza artificiale incentrata sui dati pone una forte enfasi sulla pulizia e l’etichettatura dei dati. La pulizia comporta la rimozione di valori anomali, la gestione dei valori mancanti e la risoluzione di altre incongruenze nei dati. L’etichettatura, d’altro canto, comporta l’assegnazione di etichette significative e accurate ai dati. Entrambi questi processi sono fondamentali per garantire che il modello di intelligenza artificiale venga addestrato su dati accurati e pertinenti. Un altro aspetto importante dell’approccio incentrato sui dati è il “data augmentation” [l’aumento dei dati]. Ciò comporta l’aumento artificiale delle dimensioni e della diversità del set di dati applicando varie trasformazioni ai dati, come rotazione, ridimensionamento e capovolgimento delle immagini di addestramento. L’aumento dei dati aiuta a migliorare la robustezza del modello e le capacità di generalizzazione.\nCi sono diversi vantaggi nell’adottare un approccio incentrato sui dati per lo sviluppo dell’intelligenza artificiale. Innanzitutto, porta a prestazioni del modello migliorate e capacità di generalizzazione. Assicurandosi che il modello venga addestrato su dati diversi e di alta qualità, il modello può generalizzare meglio a dati nuovi e mai visti (Mattson et al. 2020b).\nInoltre, un approccio incentrato sui dati può spesso portare a modelli più semplici che sono più facili da interpretare e gestire. Questo perché l’enfasi è sui dati piuttosto che sull’architettura del modello, il che significa che i modelli più semplici possono raggiungere prestazioni elevate quando addestrati su dati di alta qualità.\nIl passaggio all’IA incentrata sui dati rappresenta un significativo cambiamento di paradigma. Dando priorità alla qualità dei dati di input, questo approccio mira a migliorare le prestazioni del modello e le capacità di generalizzazione, portando infine a sistemi di IA più robusti e affidabili. Mentre continuiamo ad avanzare nella nostra comprensione e applicazione dell’IA, è probabile che l’approccio incentrato sui dati svolga un ruolo importante nel plasmare il futuro di questo campo.\n\n\n11.6.3 Benchmarking dei Dati\nIl benchmarking dei dati mira a valutare problemi comuni nei set di dati, come l’identificazione di errori di etichetta, caratteristiche rumorose, squilibrio di rappresentazione (ad esempio, su 1000 classi in Imagenet-1K, ci sono oltre 100 categorie che sono solo tipi di cani), squilibrio di classe (dove alcune classi hanno molti più campioni di altre), se i modelli addestrati su un dato set di dati possono generalizzare a caratteristiche fuori distribuzione o quali tipi di bias potrebbero esistere in un dato set di dati (Mattson et al. 2020b). Nella sua forma più semplice, il benchmarking dei dati mira a migliorare l’accuratezza su un set di test rimuovendo campioni di addestramento rumorosi o etichettati in modo errato mantenendo fissa l’architettura del modello. Recenti competizioni nel benchmarking dei dati hanno invitato i partecipanti a presentare nuove strategie di “augmentation” e tecniche di apprendimento attivo.\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020b. «MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance». IEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\nLe tecniche incentrate sui dati continuano a guadagnare attenzione nel benchmarking, soprattutto perché i modelli di base sono sempre più addestrati su obiettivi auto-supervisionati. Rispetto ai set di dati più piccoli come Imagenet-1K, i set di dati più grandi comunemente usati nell’apprendimento auto-supervisionato, come Common Crawl, OpenImages e LAION-5B, contengono quantità maggiori di rumore, duplicati, bias e dati potenzialmente offensivi.\nDataComp è una competizione di dataset lanciata di recente che ha come obiettivo la valutazione di grandi corpora. DataComp si concentra sulle coppie linguaggio-immagine usate per addestrare i modelli CLIP. Il documento introduttivo rileva che quando il budget di elaborazione totale per l’addestramento è costante, i modelli CLIP più performanti nelle attività downstream, come la classificazione ImageNet, vengono addestrati solo sul 30% del pool di campioni disponibile. Ciò suggerisce che un corretto filtraggio di grandi corpora è fondamentale per migliorare l’accuratezza dei modelli di base. Analogamente, Demystifying CLIP Data (Xu et al. 2023) chiede se il successo di CLIP sia attribuibile all’architettura o al set di dati.\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, e Christoph Feichtenhofer. 2023. «Demystifying CLIP Data». ArXiv preprint abs/2309.16671. https://arxiv.org/abs/2309.16671.\nDataPerf è un altro recente lavoro incentrato sul benchmarking dei dati in varie modalità. DataPerf offre round di competizione online per stimolare il miglioramento dei dataset. L’offerta inaugurale è stata lanciata con sfide in termini di visione, parlato, acquisizione, debug e prompt di testo per la generazione di immagini.\n\n\n11.6.4 Efficienza dei Dati\nMan mano che i modelli di apprendimento automatico diventano più grandi e complessi e le risorse di elaborazione diventano più scarse di fronte alla crescente domanda, diventa difficile soddisfare i requisiti di elaborazione anche con le flotte di machine learning più grandi. Per superare queste sfide e garantire la scalabilità del sistema di apprendimento automatico, è necessario esplorare nuove opportunità che aumentino gli approcci convenzionali alla scalabilità delle risorse.\nMigliorare la qualità dei dati può essere un metodo utile per avere un impatto significativo sulle prestazioni del sistema di apprendimento automatico. Uno dei principali vantaggi del miglioramento della qualità dei dati è il potenziale di poter ridurre le dimensioni del set di dati di addestramento mantenendo o addirittura migliorando le prestazioni del modello. Questa riduzione delle dimensioni dei dati è direttamente correlata alla quantità di tempo di addestramento richiesto, consentendo così ai modelli di convergere in modo più rapido ed efficiente. Raggiungere questo equilibrio tra qualità dei dati e dimensioni del set di dati è un compito impegnativo che richiede lo sviluppo di metodi, algoritmi e tecniche sofisticati.\nPossono essere adottati diversi approcci per migliorare la qualità dei dati. Questi metodi includono e non sono limitati a quanto segue:\n\nPulizia dei Dati: Ciò comporta la gestione dei valori mancanti, la correzione degli errori e la rimozione dei valori anomali. I dati puliti assicurano che il modello non stia imparando da rumore o imprecisioni.\nInterpretabilità e Spiegabilità dei Dati: Le tecniche comuni includono LIME (Ribeiro, Singh, e Guestrin 2016), che fornisce informazioni sui limiti decisionali dei classificatori, e valori Shapley (Lundberg e Lee 2017), che stimano l’importanza dei singoli campioni nel contribuire alle previsioni di un modello.\nFeature Engineering: Trasformare o creare nuove funzionalità può migliorare significativamente le prestazioni del modello fornendo informazioni più pertinenti per l’apprendimento.\nData Augmentation: Aumentare i dati creando nuovi campioni tramite varie trasformazioni può aiutare a migliorare la robustezza e la generalizzazione del modello.\nActive Learning: Questo è un approccio di apprendimento semi-supervisionato in cui il modello interroga attivamente un “oracolo” umano per etichettare i campioni più informativi (Coleman et al. 2022). Ciò garantisce che il modello venga addestrato sui dati più rilevanti.\nRiduzione della Dimensionalità: Tecniche come PCA possono ridurre il numero di feature in un set di dati, riducendo così la complessità e il tempo di addestramento.\n\n\nRibeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. «” Why should i trust you?” Explaining the predictions of any classifier». In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135–44.\n\nLundberg, Scott M., e Su-In Lee. 2017. «A Unified Approach to Interpreting Model Predictions». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert D. Nowak, Roshan Sumbaly, Matei Zaharia, e I. Zeki Yalniz. 2022. «Similarity Search for Efficient Active Learning and Search of Rare Concepts». In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, 6402–10. AAAI Press. https://ojs.aaai.org/index.php/AAAI/article/view/20591.\nEsistono molti altri metodi in circolazione. Ma l’obiettivo è lo stesso. Affinare il set di dati e garantire che sia della massima qualità può ridurre il tempo di addestramento necessario per la convergenza dei modelli. Tuttavia, per raggiungere questo obiettivo è necessario sviluppare e implementare metodi, algoritmi e tecniche sofisticati in grado di pulire, preelaborare e aumentare i dati, mantenendo al contempo i campioni più informativi. Questa è una sfida continua che richiederà una continua ricerca e innovazione nel campo dell’apprendimento automatico.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#la-tripletta",
    "href": "contents/benchmarking/benchmarking.it.html#la-tripletta",
    "title": "11  Benchmarking AI",
    "section": "11.7 La Tripletta",
    "text": "11.7 La Tripletta\nMentre i benchmark di sistema, modello e dati sono stati tradizionalmente studiati in modo isolato, si sta diffondendo la consapevolezza che per comprendere e far progredire completamente l’IA, dobbiamo adottare una visione più olistica. Iterando tra sistemi di benchmarking, modelli e dataset insieme, potrebbero emergere nuove intuizioni che non sono evidenti quando questi componenti vengono analizzati separatamente. Le prestazioni del sistema influiscono sulla precisione del modello, le capacità del modello determinano le esigenze dei dati e le caratteristiche dei dati determinano i requisiti del sistema.\nIl benchmarking della triade di sistema, modello e dati in modo integrato porterà probabilmente a scoperte sulla progettazione congiunta dei sistemi di IA, sulle proprietà di generalizzazione dei modelli e sul ruolo della cura e della qualità dei dati nel consentire le prestazioni. Piuttosto che benchmark ristretti di singoli componenti, il futuro dell’IA richiede benchmark che valutino la relazione simbiotica tra piattaforme di elaborazione, algoritmi e dati di training. Questa prospettiva a livello di sistema sarà fondamentale per superare le attuali limitazioni e sbloccare il prossimo livello di capacità dell’IA.\nFigura 11.7 illustra i molti modi potenziali per far interagire tra loro il benchmarking dei dati, quello dei modelli e quello dell’infrastruttura di sistema. L’esplorazione di queste complesse interazioni probabilmente porterà alla scoperta di nuove opportunità di ottimizzazione e capacità di miglioramento. La tripletta di benchmark di dati, modelli e sistemi offre un ricco spazio per la progettazione congiunta e la co-ottimizzazione.\n\n\n\n\n\n\nFigura 11.7: La tripletta del Benchmarking.\n\n\n\nSebbene questa prospettiva integrata rappresenti una tendenza emergente, il settore ha ancora molto da scoprire sulle sinergie e i compromessi tra questi componenti. Mentre eseguiamo il benchmarking iterativo di combinazioni di dati, modelli e sistemi, emergeranno nuove intuizioni che rimangono nascoste quando questi elementi vengono studiati separatamente. Questo approccio di benchmarking multiforme che traccia le intersezioni di dati, algoritmi e hardware promette di essere una strada fruttuosa per importanti progressi nell’intelligenza artificiale, anche se è ancora nelle sue fasi iniziali.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#benchmark-per-tecnologie-emergenti",
    "href": "contents/benchmarking/benchmarking.it.html#benchmark-per-tecnologie-emergenti",
    "title": "11  Benchmarking AI",
    "section": "11.8 Benchmark per Tecnologie Emergenti",
    "text": "11.8 Benchmark per Tecnologie Emergenti\nDate le loro significative differenze rispetto alle tecniche esistenti, le tecnologie emergenti possono essere particolarmente difficili da progettare per i benchmark. I benchmark standard utilizzati per le tecnologie esistenti potrebbero non evidenziare le caratteristiche chiave del nuovo approccio. Al contrario, i nuovi benchmark potrebbero essere visti come artificiosi per favorire la tecnologia emergente rispetto ad altre. Potrebbero essere così diversi dai benchmark esistenti da non poter essere compresi e perdere significato. Pertanto, i benchmark per le tecnologie emergenti devono bilanciare equità, applicabilità e facilità di confronto con quelli esistenti.\nUn esempio di tecnologia emergente in cui il benchmarking si è dimostrato particolarmente difficile è nel Neuromorphic Computing. Utilizzando il cervello come fonte di ispirazione per un’intelligenza generale scalabile, robusta ed efficiente dal punto di vista energetico, il calcolo neuromorfico (Schuman et al. 2022) incorpora direttamente meccanismi biologicamente realistici sia negli algoritmi di calcolo che nell’hardware, come le reti neurali spiking (Maass 1997) e le architetture non-von Neumann architectures per eseguirle (Davies et al. 2018; Modha et al. 2023). Da una prospettiva full-stack di modelli, tecniche di training e sistemi hardware, il calcolo neuromorfico differisce dall’hardware e dall’intelligenza artificiale convenzionali. Pertanto, esiste una sfida fondamentale nello sviluppo di benchmark equi e utili per guidare la tecnologia.\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, e Bill Kay. 2022. «Opportunities for neuromorphic computing algorithms and applications». Nature Computational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\nMaass, Wolfgang. 1997. «Networks of spiking neurons: The third generation of neural network models». Neural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. «Loihi: A Neuromorphic Manycore Processor with On-Chip Learning». IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. «Neural inference at the frontier of energy, space, and time». Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\nYik, Jason, Soikat Hasan Ahmed, Zergham Ahmed, Brian Anderson, Andreas G. Andreou, Chiara Bartolozzi, Arindam Basu, et al. 2023. «NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking». https://arxiv.org/abs/2304.04640.\nUn’iniziativa in corso per sviluppare benchmark neuromorfici standard è NeuroBench (Yik et al. 2023). Per un benchmarking adeguato del neuromorfico, NeuroBench segue principi di alto livello di inclusività attraverso l’applicabilità di attività e metriche sia alle soluzioni neuromorfiche che non neuromorfiche, attuabilità dell’implementazione utilizzando strumenti comuni e aggiornamenti iterativi per continuare a garantire la pertinenza man mano che il campo cresce rapidamente. NeuroBench e altri benchmark per le tecnologie emergenti forniscono una guida critica per le tecniche future, che potrebbero essere necessarie man mano che i limiti di scalabilità degli approcci esistenti si avvicinano.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#conclusione",
    "href": "contents/benchmarking/benchmarking.it.html#conclusione",
    "title": "11  Benchmarking AI",
    "section": "11.9 Conclusione",
    "text": "11.9 Conclusione\nCiò che viene misurato viene migliorato. Questo capitolo ha esplorato la natura multiforme del benchmarking che abbraccia sistemi, modelli e dati. Il benchmarking è importante per far progredire l’IA in quanto fornisce le misurazioni essenziali per monitorare i progressi.\nI benchmark del sistema ML consentono l’ottimizzazione attraverso metriche di velocità, efficienza e scalabilità. I benchmark del modello guidano l’innovazione attraverso attività e metriche standardizzate oltre l’accuratezza. I benchmark dei dati evidenziano problemi di qualità, equilibrio e rappresentazione.\nÈ importante notare che la valutazione di questi componenti in modo isolato presenta dei limiti. In futuro, sarà probabilmente utilizzato un benchmarking più integrato per esplorare l’interazione tra benchmark di sistema, modello e dati. Questa visione promette nuove intuizioni sulla progettazione congiunta di dati, algoritmi e infrastrutture.\nMan mano che l’IA diventa più complessa, il benchmarking completo diventa ancora più critico. Gli standard devono evolversi continuamente per misurare nuove capacità e rivelare limitazioni. Una stretta collaborazione tra settore, mondo accademico, etichette nazionali, ecc. è essenziale per sviluppare benchmark rigorosi, trasparenti e socialmente utili.\nIl benchmarking fornisce la bussola per guidare il progresso nell’IA. Misurando costantemente e condividendo apertamente i risultati, possiamo orientarci verso sistemi performanti, robusti e affidabili. Se l’IA deve soddisfare adeguatamente le esigenze sociali e umane, deve essere sottoposta a benchmarking tenendo a mente gli interessi dell’umanità. A tal fine, ci sono aree emergenti, come il benchmarking della sicurezza dei sistemi di IA, ma questo è per un altro giorno e qualcosa di cui possiamo discutere ulteriormente in “Generative AI”!\nIl benchmarking è un argomento in continua evoluzione. L’articolo The Olympics of AI: Benchmarking Machine Learning Systems copre diversi sottocampi emergenti nel benchmarking dell’IA, tra cui robotica, realtà estesa e calcolo neuromorfico che incoraggiamo il lettore ad approfondire.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/benchmarking/benchmarking.it.html#sec-benchmarking-ai-resource",
    "href": "contents/benchmarking/benchmarking.it.html#sec-benchmarking-ai-resource",
    "title": "11  Benchmarking AI",
    "section": "11.10 Risorse",
    "text": "11.10 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale secondo il proprio ritmo. Incoraggiamo studenti e docenti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nPerché il benchmarking è importante?\nBenchmarking di inferenza embedded.\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nProssimamente.\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 11.1\nEsercizio 11.2\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html",
    "href": "contents/ondevice_learning/ondevice_learning.it.html",
    "title": "12  Apprendimento On-Device",
    "section": "",
    "text": "12.1 Introduzione\nL’apprendimento su dispositivo si riferisce all’addestramento di modelli ML direttamente sul dispositivo in cui vengono distribuiti, al contrario dei metodi tradizionali in cui i modelli vengono addestrati su server potenti e poi distribuiti sui dispositivi. Questo metodo è particolarmente rilevante per TinyML, in cui i sistemi ML sono integrati in dispositivi minuscoli e con risorse limitate.\nUn esempio di apprendimento su dispositivo può essere visto in un termostato intelligente che si adatta al comportamento dell’utente nel tempo. Inizialmente, il termostato può avere un modello generico che comprende modelli di utilizzo di base. Tuttavia, poiché è esposto a più dati, come gli orari in cui l’utente è a casa o fuori, le temperature preferite e le condizioni meteorologiche esterne, il termostato può perfezionare il suo modello direttamente sul dispositivo per fornire un’esperienza personalizzata. Tutto ciò avviene senza inviare dati a un server centrale per l’elaborazione.\nUn altro esempio è nel testo predittivo sugli smartphone. Mentre gli utenti digitano, il telefono impara dai modelli linguistici dell’utente e suggerisce parole o frasi che probabilmente verranno utilizzate in seguito. Questo apprendimento avviene direttamente sul dispositivo e il modello si aggiorna in tempo reale man mano che vengono raccolti più dati. Un esempio pratico di apprendimento su dispositivo ampiamente utilizzato è Gboard. Su un telefono Android, Gboard impara da modelli di digitazione e dettatura per migliorare l’esperienza per tutti gli utenti. L’apprendimento “On-device” è anche chiamato “apprendimento federato”. Figura 12.1 mostra il ciclo di apprendimento federato sui dispositivi mobili: A. il dispositivo impara dai modelli utente; B. gli aggiornamenti del modello locale vengono comunicati al cloud; C. il server cloud aggiorna il modello globale e invia il nuovo modello a tutti i dispositivi.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#introduzione",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#introduzione",
    "title": "12  Apprendimento On-Device",
    "section": "",
    "text": "Figura 12.1: Ciclo di apprendimento federato. Fonte: Google Research.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#vantaggi-e-limiti",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#vantaggi-e-limiti",
    "title": "12  Apprendimento On-Device",
    "section": "12.2 Vantaggi e Limiti",
    "text": "12.2 Vantaggi e Limiti\nL’apprendimento su dispositivo offre diversi vantaggi rispetto al tradizionale ML basato su cloud. Mantenendo dati e modelli sul dispositivo, elimina la necessità di costose trasmissioni di dati e risolve i problemi di privacy. Ciò consente esperienze più personalizzate e reattive, poiché il modello può adattarsi in tempo reale al comportamento dell’utente.\nTuttavia, l’apprendimento su dispositivo presenta anche dei compromessi. Le limitate risorse di elaborazione sui dispositivi dei consumatori possono rendere difficile l’esecuzione di modelli complessi in locale. Anche i set di dati sono più limitati poiché sono costituiti solo da dati generati dall’utente da un singolo dispositivo. Inoltre, l’aggiornamento dei modelli richiede l’invio di nuove versioni anziché aggiornamenti cloud senza interruzioni.\nL’apprendimento su dispositivo apre nuove possibilità abilitando l’intelligenza artificiale offline mantenendo al contempo la privacy dell’utente. Tuttavia, richiede una gestione attenta della complessità dei modelli e dei dati entro i limiti dei dispositivi dei consumatori. Trovare il giusto equilibrio tra localizzazione e offload dal cloud è fondamentale per ottimizzare le esperienze su dispositivo.\n\n12.2.1 Vantaggi\n\nPrivacy e Sicurezza dei Dati\nUno dei vantaggi significativi dell’apprendimento sul dispositivo è la maggiore privacy e sicurezza dei dati degli utenti. Ad esempio, si consideri uno smartwatch che monitora parametri sanitari sensibili come la frequenza cardiaca e la pressione sanguigna. Elaborando i dati e adattando i modelli direttamente sul dispositivo, i dati biometrici rimangono localizzati, aggirando la necessità di trasmettere dati grezzi ai server cloud dove potrebbero essere soggetti a violazioni.\nLe violazioni dei server sono tutt’altro che rare, con milioni di record compromessi ogni anno. Ad esempio, la violazione di Equifax del 2017 ha esposto i dati personali di 147 milioni di persone. Mantenendo i dati sul dispositivo, il rischio di tali esposizioni è drasticamente ridotto. L’apprendimento sul dispositivo elimina la dipendenza dall’archiviazione cloud centralizzata e protegge dall’accesso non autorizzato da varie minacce, tra cui attori malintenzionati, minacce interne ed esposizione accidentale.\nRegolamenti come l’Health Insurance Portability and Accountability Act (HIPAA) e il General Data Protection Regulation (GDPR) impongono rigorosi requisiti di riservatezza dei dati che l’apprendimento sul dispositivo affronta abilmente. Garantendo che i dati rimangano localizzati e non vengano trasferiti ad altri sistemi, l’apprendimento sul dispositivo facilita la conformità a tali regolamenti.\nL’apprendimento sul dispositivo non è solo vantaggioso per i singoli utenti; ha implicazioni significative per le organizzazioni e i settori che gestiscono dati altamente sensibili. Ad esempio, in ambito militare, l’apprendimento sul dispositivo consente ai sistemi di prima linea di adattare modelli e funzioni indipendentemente dalle connessioni ai server centrali che potrebbero essere potenzialmente compromessi. Le informazioni critiche e sensibili sono saldamente protette dalla localizzazione dell’elaborazione e dell’apprendimento dei dati. Tuttavia, ciò comporta il compromesso che i singoli dispositivi assumono più valore e possono incentivare furti o distruzioni poiché diventano gli unici vettori di modelli di intelligenza artificiale specializzati. È necessario prestare attenzione alla protezione dei dispositivi stessi durante la transizione all’apprendimento sul dispositivo.\nÈ inoltre importante preservare la privacy, la sicurezza e la conformità normativa dei dati personali e sensibili. Invece che nel cloud, i modelli di training e operativi aumentano sostanzialmente le misure di privacy a livello locale, assicurando che i dati degli utenti siano protetti da potenziali minacce.\nTuttavia, questo è solo parzialmente intuitivo perché l’apprendimento sul dispositivo potrebbe invece esporre i sistemi a nuovi attacchi alla privacy. Con preziosi riepiloghi dei dati e aggiornamenti dei modelli archiviati in modo permanente su singoli dispositivi, potrebbe essere molto più difficile proteggerli fisicamente e digitalmente rispetto a un grande cluster di elaborazione. Mentre l’apprendimento sul dispositivo riduce la quantità di dati compromessi in una qualsiasi violazione, potrebbe anche introdurre nuovi pericoli disperdendo informazioni sensibili su molti terminali decentralizzati. Le pratiche di sicurezza attente sono ancora essenziali per i sistemi “on-device”.\n\n\nNormativa di Conformità\nL’apprendimento sul dispositivo aiuta ad affrontare le principali normative sulla privacy come GDPR)e CCPA. Queste normative richiedono la localizzazione dei dati, limitando i trasferimenti di dati transfrontalieri a paesi approvati con controlli adeguati. Il GDPR impone inoltre requisiti di “privacy by design” e consenso per la raccolta dei dati. Mantenendo l’elaborazione dei dati e il training del modello localizzati sul dispositivo, i dati sensibili degli utenti non vengono trasferiti altrove. Ciò evita importanti grattacapi di conformità per le organizzazioni.\nAd esempio, un fornitore di servizi sanitari che monitora i parametri vitali dei pazienti con dispositivi indossabili deve garantire che i trasferimenti di dati transfrontalieri siano conformi a HIPAA e GDPR se utilizza il cloud. Determinare le leggi del paese applicabili e garantire le approvazioni per i flussi di dati internazionali introduce oneri legali e ingegneristici. Con l’apprendimento on-device, nessun dato lascia il dispositivo, semplificando la conformità. Il tempo e le risorse spesi per la conformità vengono ridotti in modo significativo.\nSettori come sanità, finanza e governo, che hanno dati altamente regolamentati, possono trarre grandi vantaggi dal training sul dispositivo. Localizzando i dati e l’apprendimento, i requisiti normativi di privacy e sovranità dei dati vengono soddisfatti più facilmente. Le soluzioni su dispositivo forniscono un modo efficiente per creare applicazioni di IA conformi.\nLe principali normative sulla privacy impongono restrizioni sullo spostamento transfrontaliero dei dati che l’apprendimento su dispositivo affronta intrinsecamente tramite elaborazione localizzata. Ciò riduce l’onere di conformità per le organizzazioni che lavorano con dati regolamentati.\n\n\nRiduzione della Larghezza di Banda, dei Costi e Maggiore Efficienza\nUno dei principali vantaggi dell’apprendimento su dispositivo è la significativa riduzione dell’utilizzo della larghezza di banda e dei costi associati all’infrastruttura cloud. Mantenendo i dati localizzati per l’addestramento del modello anziché trasmettere dati grezzi al cloud, l’apprendimento su dispositivo può comportare notevoli risparmi di larghezza di banda. Ad esempio, una rete di telecamere che analizzano i filmati video può ottenere significative riduzioni nel trasferimento di dati addestrando i modelli sul dispositivo anziché trasmettere in streaming tutti i filmati video al cloud per l’elaborazione.\nQuesta riduzione nella trasmissione dei dati consente di risparmiare larghezza di banda e si traduce in costi inferiori per server, reti e archiviazione dei dati nel cloud. Le grandi organizzazioni, che potrebbero spendere milioni in infrastrutture cloud per addestrare modelli di dati sui dispositivi, possono sperimentare drastiche riduzioni dei costi tramite l’apprendimento on-device. Nell’era dell’intelligenza artificiale generativa, in cui i costi sono aumentati in modo significativo, trovare modi per contenere le spese è diventato sempre più importante.\nInoltre, anche i costi energetici e ambientali della gestione di grandi server farm sono diminuiti. I data center consumano grandi quantità di energia, contribuendo alle emissioni di gas serra. Riducendo la necessità di un’ampia infrastruttura basata su cloud, l’apprendimento sui dispositivi contribuisce a mitigare l’impatto ambientale dell’elaborazione dei dati (Wu et al. 2022).\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. «Sustainable ai: Environmental implications, challenges and opportunities». Proceedings of Machine Learning and Systems 4: 795–813.\nSpecificamente per le applicazioni endpoint [finali], l’apprendimento sui dispositivi riduce al minimo il numero di chiamate API di rete necessarie per eseguire l’inferenza tramite un provider cloud. I costi cumulativi associati alla larghezza di banda e alle chiamate API possono aumentare rapidamente per le applicazioni con milioni di utenti. Al contrario, eseguire training e inferenze localmente è notevolmente più efficiente e conveniente. Con ottimizzazioni all’avanguardia, è stato dimostrato che l’apprendimento on-device riduce i requisiti di memoria del training, migliora drasticamente l’efficienza della memoria e riduce fino al 20% la latenza per iterazione (Dhar et al. 2021).\nUn altro vantaggio fondamentale dell’apprendimento sul dispositivo è la possibilità per i dispositivi IoT di adattare continuamente il loro modello ML a nuovi dati per un apprendimento continuo e permanente. I modelli sul dispositivo possono rapidamente diventare obsoleti man mano che il comportamento dell’utente, i modelli di dati e le preferenze cambiano. L’apprendimento continuo consente al modello di adattarsi in modo efficiente a nuovi dati e miglioramenti e di mantenere elevate prestazioni del modello nel tempo.\n\n\n\n12.2.2 Limitazioni\nMentre i tradizionali sistemi ML basati su cloud hanno accesso a risorse di elaborazione pressoché infinite, l’apprendimento sul dispositivo è spesso limitato nella potenza di elaborazione e di archiviazione del dispositivo edge su cui viene addestrato il modello. Per definizione, un dispositivo edge è un dispositivo con risorse di elaborazione, memoria ed energia limitate che non possono essere facilmente aumentate o diminuite. Pertanto, la dipendenza dai dispositivi edge può limitare la complessità, l’efficienza e le dimensioni dei modelli ML sul dispositivo.\n\nRisorse di elaborazione\nI tradizionali sistemi ML basati su cloud utilizzano grandi server con più GPU o TPU di fascia alta, che forniscono una potenza di calcolo e una memoria pressoché infinite. Ad esempio, servizi come Amazon Web Services (AWS) EC2 consentono di configurare cluster di istanze GPU per un training parallelo massiccio.\nAl contrario, l’apprendimento sul dispositivo è limitato dall’hardware del dispositivo edge su cui viene eseguito. I dispositivi edge si riferiscono a endpoint come smartphone, elettronica embedded e dispositivi IoT. Per definizione, questi dispositivi hanno risorse di elaborazione, memoria ed energia molto limitate rispetto al cloud.\nAd esempio, uno smartphone tipico o Raspberry Pi può avere solo pochi core CPU, pochi GB di RAM e una piccola batteria. Ancora più limitati in termini di risorse sono i dispositivi microcontrollore TinyML come Arduino Nano BLE Sense. Le risorse sono fisse su questi dispositivi e non possono essere facilmente aumentate su richiesta, come il ridimensionamento dell’infrastruttura cloud. Questa dipendenza dai dispositivi edge limita direttamente la complessità, l’efficienza e le dimensioni dei modelli che possono essere distribuiti per l’addestramento sul dispositivo:\n\nComplessità: I limiti di memoria, elaborazione e potenza limitano la progettazione dell’architettura del modello, così come il numero di layer e dei parametri.\nEfficienza: I modelli devono essere fortemente ottimizzati tramite metodi come la quantizzazione e la potatura per essere eseguiti più velocemente e consumare meno energia.\nDimensioni: I file del modello effettivo devono essere compressi il più possibile per rientrare nei limiti di archiviazione dei dispositivi edge.\n\nPertanto, mentre il cloud offre una scalabilità infinita, l’apprendimento sul dispositivo deve operare entro i rigidi vincoli di risorse dell’hardware. Ciò richiede un’attenta progettazione congiunta di modelli semplificati, metodi di addestramento e ottimizzazioni su misura specificamente per i dispositivi edge.\n\n\nDimensioni, Accuratezza e Generalizzazione del Dataset\nOltre alle risorse di elaborazione limitate, l’apprendimento sul dispositivo è anche limitato dal set di dati disponibile per i modelli di training.\nNel cloud, i modelli vengono addestrati su dataset enormi e diversi come ImageNet o Common Crawl. Ad esempio, ImageNet contiene oltre 14 milioni di immagini attentamente categorizzate in migliaia di classi.\nL’apprendimento sul dispositivo si basa invece su “data silos” più piccoli e decentralizzati, unici per ogni dispositivo. Il rullino fotografico di uno smartphone potrebbe contenere solo migliaia di foto degli interessi e degli ambienti degli utenti.\nQuesti dati decentralizzati portano alla necessità di dati IID (indipendenti e distribuiti in modo identico). Ad esempio, due amici potrebbero scattare molte foto degli stessi luoghi e oggetti, il che significa che le loro distribuzioni di dati sono altamente correlate piuttosto che indipendenti.\nMotivi per cui i dati potrebbero essere non IID nelle impostazioni sul dispositivo:\n\nEterogeneità degli utenti: Utenti diversi hanno interessi e ambienti diversi.\nDifferenze tra dispositivi: Sensori, regioni e dati demografici influenzano i dati.\nEffetti temporali: Ora del giorno, impatti stagionali sui dati.\n\nL’efficacia del ML si basa in gran parte su dati di training ampi e diversificati. Con set di dati piccoli e localizzati, i modelli on-device potrebbero non riuscire a generalizzare tra diverse popolazioni di utenti e ambienti. Ad esempio, un modello di rilevamento delle malattie addestrato solo su immagini di un singolo ospedale non si generalizzerebbe bene ad altri dati demografici dei pazienti. Le prestazioni nel mondo reale non potranno che migliorare con progressi medici estesi e diversificati. Quindi, mentre l’apprendimento basato su cloud sfrutta enormi set di dati, l’apprendimento su dispositivo si basa su “silo di dati” decentralizzati molto più piccoli, unici per ogni utente.\nI dati limitati e le ottimizzazioni richieste per l’apprendimento on-device possono avere un impatto negativo sulla precisione e sulla generalizzazione del modello:\n\nI piccoli dataset aumentano il rischio di overfitting. Ad esempio, un classificatore di frutta addestrato su 100 immagini rischia di overfitting rispetto a uno addestrato su 1 milione di immagini diverse.\nI dati rumorosi generati dall’utente riducono la qualità. Il rumore del sensore o l’etichettatura impropria dei dati da parte di non esperti possono degradare l’addestramento.\nOttimizzazioni come la potatura e la quantizzazione compromettono la precisione per l’efficienza. Un modello quantizzato a 8 bit funziona più velocemente ma meno accuratamente di un modello a 32 bit.\n\nQuindi, mentre i modelli cloud raggiungono un’elevata precisione con enormi set di dati e senza vincoli, i modelli su dispositivo possono avere difficoltà a generalizzare. Alcuni studi dimostrano che il training sul dispositivo corrisponde all’accuratezza del cloud su determinate attività. Tuttavia, le prestazioni sui carichi di lavoro reali richiedono ulteriori studi (Lin et al. 2022).\nAd esempio, un modello cloud può rilevare con precisione la polmonite nelle radiografie del torace di migliaia di ospedali. Tuttavia, un modello sul dispositivo addestrato solo su una piccola popolazione locale di pazienti potrebbe non riuscire a generalizzare.\nUn’accuratezza inaffidabile limita l’applicabilità nel mondo reale dell’apprendimento sul dispositivo per usi critici come la diagnosi di malattie o i veicoli a guida autonoma.\nIl training sul dispositivo è anche più lento del cloud a causa delle risorse limitate. Anche se ogni iterazione è più veloce, il processo di training complessivo richiede più tempo.\nAd esempio, un’applicazione di robotica in tempo reale potrebbe richiedere aggiornamenti del modello entro millisecondi. L’On-device training su un piccolo hardware embedded potrebbe richiedere secondi o minuti per l’aggiornamento, troppo lento per l’uso in tempo reale.\nLe sfide relative a precisione, generalizzazione e velocità pongono ostacoli all’adozione dell’apprendimento on-device per sistemi di produzione reali, soprattutto quando affidabilità e bassa latenza sono fondamentali.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#adattamento-on-device",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#adattamento-on-device",
    "title": "12  Apprendimento On-Device",
    "section": "12.3 Adattamento On-device",
    "text": "12.3 Adattamento On-device\nIn un’attività ML, il consumo di risorse proviene principalmente da tre fonti:\n\nIl modello ML stesso;\nIl processo di ottimizzazione durante l’apprendimento del modello\nArchiviazione ed elaborazione del dataset utilizzato per l’apprendimento.\n\nDi conseguenza, ci sono tre approcci per adattare gli algoritmi ML esistenti su dispositivi con risorse limitate:\n\nRiduzione della complessità del modello ML\nModifica delle ottimizzazioni per ridurre i requisiti delle risorse di training\nCreazione di nuove rappresentazioni dei dati più efficienti in termini di archiviazione\n\nNella sezione seguente, esamineremo questi metodi di adattamento dell’apprendimento on-device. Il capitolo Ottimizzazioni dei Modelli fornisce maggiori dettagli sulle ottimizzazioni del modello.\n\n12.3.1 Riduzione della Complessità del Modello\nIn questa sezione, discuteremo brevemente i modi per ridurre la complessità del modello quando si adattano i modelli ML sul dispositivo. Per i dettagli sulla riduzione della complessità del modello, fare riferimento al capitolo Ottimizzazioni dei Modelli.\n\nAlgoritmi ML tradizionali\nA causa delle limitazioni di elaborazione e memoria dei dispositivi edge, alcuni algoritmi ML tradizionali sono ottimi candidati per applicazioni di apprendimento on-device grazie alla loro natura leggera. Alcuni esempi di algoritmi con basso impatto sulle risorse includono Naive Bayes Classifiers, Support Vector Machines (SVM), Linear Regression, Logistic Regression e algoritmi Decision Tree selezionati.\nCon alcuni perfezionamenti, questi algoritmi ML classici possono essere adattati a specifiche architetture hardware ed eseguire attività semplici. I loro bassi requisiti di prestazioni semplificano l’integrazione dell’apprendimento continuo anche su dispositivi edge.\n\n\nPruning\nIl “pruning” [potatura] è una tecnica per ridurre le dimensioni e la complessità di un modello ML per migliorarne l’efficienza e le prestazioni di generalizzazione. Ciò è utile per l’addestramento di modelli su dispositivi edge, in cui vogliamo ridurre al minimo l’utilizzo delle risorse mantenendo un’accuratezza competitiva.\nL’obiettivo principale della potatura è rimuovere parti del modello che non contribuiscono in modo significativo al suo potere predittivo, mantenendo al contempo gli aspetti più informativi. Nel contesto degli alberi decisionali, la potatura comporta la rimozione di alcuni rami (sottoalberi) dall’albero, portando a un albero più piccolo e semplice. Nel contesto di DNN, la potatura viene utilizzata per ridurre il numero di neuroni (unità) o connessioni nella rete, come mostrato in Figura 12.2.\n\n\n\n\n\n\nFigura 12.2: Potatura della rete.\n\n\n\n\n\nRiduzione della Complessità dei Modelli di Deep Learning\nI framework DNN tradizionali basati su cloud hanno un sovraccarico di memoria troppo elevato per essere utilizzati sul dispositivo. Ad esempio, i sistemi di deep learning come PyTorch e TensorFlow richiedono centinaia di megabyte di overhead di memoria durante l’addestramento di modelli come MobilenetV2 e l’overhead aumenta con l’aumentare del numero di parametri di addestramento.\nLa ricerca attuale per DNN leggeri esplora principalmente architetture CNN. Esistono anche diversi framework “bare-metal” [tutto in hardware] progettati per eseguire reti neurali su MCU mantenendo bassi l’overhead computazionale e l’ingombro di memoria. Alcuni esempi includono MNN, TVM e TensorFlow Lite. Tuttavia, possono eseguire l’inferenza solo durante i passaggi in avanti e non supportano la backpropagation. Sebbene questi modelli siano progettati per l’implementazione edge, la loro riduzione nei pesi del modello e nelle connessioni architettoniche ha portato a minori requisiti di risorse per l’apprendimento continuo.\nIl compromesso tra prestazioni e supporto del modello è chiaro quando si adattano i sistemi DNN più diffusi. Come adattiamo i modelli DNN esistenti a impostazioni con risorse limitate mantenendo il supporto per la backpropagation e l’apprendimento continuo? Le ultime ricerche suggeriscono tecniche di progettazione congiunta di algoritmi e sistemi che aiutano a ridurre il consumo di risorse dell’addestramento ML sui dispositivi edge. Utilizzando tecniche come il “quantization-aware scaling” (QAS) [ridimensionamento consapevole della quantizzazione], aggiornamenti sparsi e altre tecniche all’avanguardia, l’apprendimento sul dispositivo è possibile su sistemi embedded con poche centinaia di kilobyte di RAM senza memoria aggiuntiva mantenendo un’elevata precisione.\n\n\n\n12.3.2 Modifica dei Processi di Ottimizzazione\nLa scelta della giusta strategia di ottimizzazione è importante per l’addestramento DNN su un dispositivo, poiché consente di trovare un buon minimo locale. Poiché l’addestramento avviene su un dispositivo, questa strategia deve anche considerare la memoria e la potenza limitate.\n\nQuantization-Aware Scaling\nLa quantizzazione è un metodo comune per ridurre l’impronta di memoria dell’addestramento DNN. Sebbene ciò possa introdurre nuovi errori, questi possono essere mitigati progettando un modello per caratterizzare questo errore statistico. Ad esempio, i modelli potrebbero utilizzare l’arrotondamento stocastico o introdurre l’errore di quantizzazione negli aggiornamenti del gradiente.\nUna tecnica algoritmica specifica è Quantization-Aware Scaling (QAS), che migliora le prestazioni delle reti neurali su hardware a bassa precisione, come dispositivi edge, dispositivi mobili o sistemi TinyML, regolando i fattori di scala durante il processo di quantizzazione.\nCome abbiamo discusso nel capitolo Ottimizzazioni del modello, la quantizzazione è il processo di mappatura di un intervallo continuo di valori su un set discreto di valori. Nel contesto delle reti neurali, la quantizzazione spesso comporta la riduzione della precisione dei pesi e delle attivazioni da virgola mobile a 32 bit a formati a precisione inferiore come numeri interi a 8 bit. Questa riduzione di precisione può diminuire significativamente il costo computazionale e l’ingombro di memoria del modello, rendendolo adatto per l’implementazione su hardware a bassa precisione. Figura 12.3 è un esempio di quantizzazione float-to-integer.\n\n\n\n\n\n\nFigura 12.3: Quantizzazione float-to-integer. Fonte: Nvidia.\n\n\n\nTuttavia, il processo di quantizzazione può anche introdurre errori di quantizzazione che possono degradare le prestazioni del modello. Il ridimensionamento “quantization-aware” è una tecnica che mira a ridurre al minimo questi errori regolando i fattori di scala utilizzati nel processo di quantizzazione.\nIl processo QAS prevede due fasi principali:\n\nAddestramento basato sulla quantizzazione: In questa fase, la rete neurale viene addestrata tenendo conto della quantizzazione, simulandola per imitarne gli effetti durante i passaggi “forward” e “backward”. Ciò consente al modello di imparare a compensare gli errori di quantizzazione e migliorarne le prestazioni su hardware a bassa precisione. Per i dettagli, fare riferimento alla sezione QAT in Ottimizzazioni del modello.\nQuantizzazione e ridimensionamento: Dopo l’addestramento, il modello viene quantizzato in un formato a bassa precisione e i fattori di scala vengono regolati per ridurre al minimo gli errori di quantizzazione. I fattori di scala vengono scelti in base alla distribuzione dei pesi e delle attivazioni nel modello e vengono regolati per garantire che i valori quantizzati siano compresi nell’intervallo del formato a bassa precisione.\n\nQAS viene utilizzato per superare le difficoltà di ottimizzazione dei modelli su dispositivi minuscoli senza dover effettuare la messa a punto degli iperparametri; QAS ridimensiona automaticamente i gradienti tensoriali con varie precisioni di bit. Ciò stabilizza il processo di addestramento e corrisponde all’accuratezza della precisione in virgola mobile.\n\n\nAggiornamenti Sparsi\nSebbene QAS consenta l’ottimizzazione di un modello quantizzato, utilizza una grande quantità di memoria, il che non è realistico per l’addestramento sul dispositivo. Quindi, gli aggiornamenti “spare” vengono utilizzati per ridurre l’ingombro di memoria del calcolo “full backward”. Invece di potare i pesi per l’inferenza, l’aggiornamento sparso pota il gradiente durante la “backward propagation” [propagazione all’indietro] per aggiornare il modello in modo sparso. In altre parole, l’aggiornamento sparso salta i gradienti del calcolo di layer e sottotensori meno importanti.\nTuttavia, determinare lo schema di un aggiornamento sparso ottimale dato un budget di memoria vincolante può essere difficile a causa dell’ampio spazio di ricerca. Ad esempio, il modello MCUNet ha 43 layer convoluzionali e uno spazio di ricerca di circa 1030. Una tecnica per affrontare questo problema è l’analisi del contributo. L’analisi del contributo misura il miglioramento dell’accuratezza dai bias (aggiornamento degli ultimi bias rispetto al solo aggiornamento del classificatore) e pesi (aggiornamento del peso di un layer extra rispetto al solo aggiornamento del bias). Cercando di massimizzare questi miglioramenti, l’analisi del contributo deriva automaticamente uno schema di aggiornamento sparso ottimale per abilitare l’addestramento sul dispositivo.\n\n\nTraining Layer-Wise\nAltri metodi oltre alla quantizzazione possono aiutare a ottimizzare le routine. Uno di questi metodi è l’addestramento “layer-wise”. Un consumatore significativo di memoria dell’addestramento DNN è la backpropagation end-to-end, che richiede che tutte le feature map intermedie siano archiviate in modo che il modello possa calcolare i gradienti. Un’alternativa a questo approccio che riduce l’impronta di memoria dell’addestramento DNN è l’addestramento sequenziale “layer-by-layer” (T. Chen et al. 2016). Invece dell’addestramento end-to-end, l’addestramento di un singolo layer alla volta aiuta a evitare di dover archiviare le feature map intermedie.\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, e Carlos Guestrin. 2016. «Training deep nets with sublinear memory cost». ArXiv preprint abs/1604.06174. https://arxiv.org/abs/1604.06174.\n\n\nTrading Computation for Memory\nLa strategia “trading computation for memory” [scambio di elaborazione per memoria] comporta il rilascio di parte della memoria utilizzata per archiviare i risultati intermedi. Invece, questi risultati possono essere ricalcolati in base alle necessità. È stato dimostrato che la riduzione della memoria in cambio di più elaborazione riduce l’impronta di memoria dell’addestramento DNN per adattarsi a quasi tutti i budget, riducendo al minimo anche i costi di elaborazione (Gruslys et al. 2016).\n\nGruslys, Audrunas, Rémi Munos, Ivo Danihelka, Marc Lanctot, e Alex Graves. 2016. «Memory-Efficient Backpropagation Through Time». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 4125–33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\n\n12.3.3 Sviluppo di Nuove Rappresentazioni dei Dati\nLa dimensionalità e il volume dei dati di training possono avere un impatto significativo sull’adattamento sul dispositivo. Quindi, un’altra tecnica per adattare i modelli su dispositivi con risorse limitate è quella di rappresentare i set di dati in modo più efficiente.\n\nCompressione dei Dati\nL’obiettivo della compressione dei dati è raggiungere elevate precisioni limitando al contempo la quantità di dati di training. Un metodo per raggiungere questo obiettivo è dare priorità alla complessità del campione: la quantità di dati di training necessari affinché l’algoritmo raggiunga una precisione target (Dhar et al. 2021).\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh Kurup, e Mohak Shah. 2021. «A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective». ACM Transactions on Internet of Things 2 (3): 1–49. https://doi.org/10.1145/3450494.\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, e Farinaz Koushanfar. 2017. «TinyDL: Just-in-time deep learning solution for constrained embedded systems». In 2017 IEEE International Symposium on Circuits and Systems (ISCAS), 1–4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\nLi, Xiang, Tao Qin, Jian Yang, e Tie-Yan Liu. 2016. «LightRNN: Memory and Computation-Efficient Recurrent Neural Networks». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 4385–93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\nAltri metodi più comuni di compressione dei dati si concentrano sulla riduzione della dimensionalità e del volume dei dati di training. Ad esempio, un approccio potrebbe sfruttare la sparsità della matrice per ridurre l’ingombro di memoria per l’archiviazione dei dati di training. I dati di training possono essere trasformati in un embedding a dimensione inferiore e fattorizzati in una matrice di dizionario moltiplicata per una matrice di coefficienti blocchi sparsi (Darvish Rouhani, Mirhoseini, e Koushanfar 2017). Un altro esempio potrebbe riguardare la rappresentazione di parole provenienti da un ampio set di dati di formazione linguistica in un formato vettoriale più compresso (Li et al. 2016).",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#il-transfer-learning",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#il-transfer-learning",
    "title": "12  Apprendimento On-Device",
    "section": "12.4 Il Transfer Learning",
    "text": "12.4 Il Transfer Learning\nIl transfer learning è una tecnica di ML in cui un modello sviluppato per un’attività specifica viene riutilizzato come punto di partenza per un modello su una seconda attività. Nel contesto dell’intelligenza artificiale su dispositivi, il transfer learning ci consente di sfruttare modelli pre-addestrati che hanno già appreso rappresentazioni utili da grandi set di dati e di perfezionarli per attività specifiche utilizzando set di dati più piccoli direttamente sul dispositivo. Ciò può ridurre significativamente le risorse di calcolo e il tempo necessari per l’addestramento dei modelli da zero.\nFigura 12.4 include alcuni esempi intuitivi di transfer learning dal mondo reale. Ad esempio, sapendo andare in bicicletta, si sa come bilanciarsi su veicoli a due ruote. Quindi, risulterebbe più facile imparare ad andare in moto rispetto a chi non sa andare in bicicletta.\n\n\n\n\n\n\nFigura 12.4: Trasferimento di conoscenze tra attività. Fonte: Zhuang et al. (2021).\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, e Qing He. 2021. «A Comprehensive Survey on Transfer Learning». Proc. IEEE 109 (1): 43–76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nPrendiamo l’esempio di un’applicazione di sensore intelligente che utilizza l’intelligenza artificiale on-device per riconoscere gli oggetti nelle immagini acquisite dal dispositivo. Tradizionalmente, ciò richiederebbe l’invio dei dati dell’immagine a un server, dove un ampio modello di rete neurale elabora i dati e invia i risultati. Con l’intelligenza artificiale on-device, il modello viene archiviato ed eseguito direttamente sul dispositivo, eliminando la necessità di inviare dati a un server.\nPer personalizzare il modello per le caratteristiche on-device, addestrare un modello di rete neurale da zero sul dispositivo sarebbe poco pratico a causa delle risorse di calcolo limitate e della durata della batteria. È qui che entra in gioco il “transfer learning” [apprendimento tramite trasferimento]. Invece di addestrare un modello da zero, possiamo prendere un modello pre-addestrato, come una rete neurale convoluzionale (CNN) o una rete di trasformatori addestrata su un ampio set di dati di immagini, e perfezionarlo per la nostra specifica attività di riconoscimento degli oggetti. Questa messa a punto può essere eseguita direttamente sul dispositivo utilizzando un set di dati più piccolo di immagini pertinenti all’attività. Sfruttando il modello pre-addestrato, possiamo ridurre le risorse di calcolo e il tempo necessari per il training, ottenendo comunque un’elevata precisione per l’attività di riconoscimento degli oggetti.\nIl transfer learning è importante per rendere praticabile l’intelligenza artificiale on-device, consentendoci di sfruttare modelli pre-addestrati e di perfezionarli per attività specifiche, riducendo così le risorse di calcolo e il tempo necessari per il training. La combinazione di intelligenza artificiale sul dispositivo e il “transfer learning” apre nuove possibilità per applicazioni di intelligenza artificiale più attente alla privacy e più reattive alle esigenze degli utenti.\nIl transfer learning ha rivoluzionato il modo in cui i modelli vengono sviluppati e distribuiti, sia nel cloud che nell’edge. Il transfer learning viene utilizzato nel mondo reale. Un esempio del genere è l’uso del transfer learning per sviluppare modelli di intelligenza artificiale in grado di rilevare e diagnosticare malattie da immagini mediche, come raggi X, scansioni MRI [risonanza magnetica] e TAC. Ad esempio, i ricercatori della Stanford University hanno sviluppato un modello di apprendimento di trasferimento in grado di rilevare il cancro nelle immagini della pelle con una precisione del 97% (Esteva et al. 2017). Questo modello è stato pre-addestrato su 1.28 milioni di immagini per classificare un’ampia gamma di oggetti e poi specializzato per il rilevamento del cancro tramite l’addestramento su un set di dati di immagini della pelle curato da dermatologi.\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, e Sebastian Thrun. 2017. «Dermatologist-level classification of skin cancer with deep neural networks». Nature 542 (7639): 115–18. https://doi.org/10.1038/nature21056.\nL’implementazione negli scenari di produzione può essere ampiamente categorizzata in due fasi: pre-distribuzione e post-distribuzione.\n\n12.4.1 Specializzazione Pre-Distribuzione\nNella fase di pre-implementazione, il transfer learning funge da catalizzatore per accelerare il processo di sviluppo. Ecco come funziona tipicamente: Si immagini di creare un sistema per riconoscere diverse razze di cani. Invece di partire da zero, possiamo utilizzare un modello pre-addestrato che ha già padroneggiato il compito più ampio di riconoscere gli animali nelle immagini.\nQuesto modello pre-addestrato funge da solida base e contiene una vasta conoscenza acquisita da dati estesi. Quindi perfezioniamo questo modello utilizzando un set di dati specializzato contenente immagini di varie razze di cani. Questo processo di messa a punto adatta il modello alle nostre esigenze specifiche, ovvero identificare con precisione le razze di cani. Una volta perfezionato e convalidato per soddisfare i criteri di prestazione, questo modello specializzato è pronto per l’implementazione.\nEcco come funziona in pratica:\n\nInizia con un Modello Pre-Addestrato: Si inizia selezionando un modello che è già stato addestrato su un set di dati completo, solitamente correlato a un’attività generale. Questo modello funge da base per l’attività in questione.\nFinetuning: Il modello pre-addestrato viene quindi perfezionato su un set di dati più piccolo e specifico per l’attività desiderata. Questo passaggio consente al modello di adattare e specializzare la sua conoscenza ai requisiti specifici dell’applicazione.\nValidazione: Dopo la messa a punto, il modello viene convalidato per garantire che soddisfi i criteri di prestazione per l’attività specializzata.\nDeployment: Una volta convalidato, il modello specializzato viene distribuito nell’ambiente di produzione.\n\nQuesto metodo riduce significativamente il tempo e le risorse di calcolo necessarie per addestrare un modello da zero (Pan e Yang 2010). Adottando l’apprendimento tramite trasferimento, i sistemi embedded possono raggiungere un’elevata precisione su attività specializzate senza la necessità di raccogliere dati estesi o di impiegare risorse di calcolo significative per l’addestramento da zero.\n\nPan, Sinno Jialin, e Qiang Yang. 2010. «A Survey on Transfer Learning». IEEE Trans. Knowl. Data Eng. 22 (10): 1345–59. https://doi.org/10.1109/tkde.2009.191.\n\n\n12.4.2 Adattamento Post-Distribuzione\nL’implementazione su un dispositivo non deve necessariamente segnare il culmine del percorso educativo di un modello ML. Con l’avvento dell’apprendimento per trasferimento, apriamo le porte all’implementazione di modelli ML adattivi in scenari del mondo reale, soddisfacendo le esigenze personalizzate degli utenti.\nConsideriamo un’applicazione reale in cui un genitore desidera identificare il proprio figlio in una raccolta di immagini di un evento scolastico sul proprio smartphone. In questo scenario, il genitore si trova di fronte alla sfida di localizzare il proprio figlio in mezzo alle immagini di molti altri bambini. L’apprendimento per trasferimento può essere impiegato qui per perfezionare il modello di un sistema embedded per questo compito unico e specializzato. Inizialmente, il sistema potrebbe utilizzare un modello generico addestrato per riconoscere i volti nelle immagini. Tuttavia, con l’apprendimento per trasferimento, il sistema può adattare questo modello per riconoscere le caratteristiche specifiche del figlio dell’utente.\nEcco come funziona:\n\nRaccolta Dati: Il sistema embedded raccoglie immagini che includono il bambino, idealmente con l’input del genitore per garantire accuratezza e pertinenza. Ciò può essere fatto direttamente sul dispositivo, mantenendo la privacy dei dati dell’utente.\nFine Tuning del Modello: Il modello di riconoscimento facciale preesistente, che è stato addestrato su un set di dati ampio e diversificato, viene quindi perfezionato utilizzando le immagini del bambino appena raccolte. Questo processo adatta il modello per riconoscere le caratteristiche facciali specifiche del bambino, distinguendolo dagli altri bambini nelle immagini.\nValidazione: Il modello rifinito viene poi convalidato per garantire che riconosca accuratamente il bambino in varie immagini. Ciò può comportare che il genitore verifichi le prestazioni del modello e fornisca feedback per ulteriori miglioramenti.\nDeployment: Una volta convalidato, il modello adattato viene distribuito sul dispositivo, consentendo al genitore di identificare facilmente il proprio figlio nelle immagini senza doverle esaminare manualmente.\n\nQuesta personalizzazione al volo migliora l’efficacia del modello per il singolo utente, assicurando che tragga vantaggio dalla personalizzazione ML. Questo è, in parte, il modo in cui iPhotos o Google Photos funzionano quando ci chiedono di riconoscere un volto e poi, in base a queste informazioni, indicizzano tutte le foto di quel volto. Poiché l’apprendimento e l’adattamento avvengono sul dispositivo stesso, non ci sono rischi per la privacy personale. Le immagini dei genitori non vengono caricate su un server cloud o condivise con terze parti, proteggendo la privacy della famiglia e continuando a raccogliere i benefici di un modello ML personalizzato. Questo approccio rappresenta un significativo passo avanti nella ricerca per fornire agli utenti soluzioni ML personalizzate che rispettino e sostengano la loro privacy.\n\n\n12.4.3 Vantaggi\nIl transfer learning è diventato una tecnica importante in ML e intelligenza artificiale, ed è particolarmente prezioso per diversi motivi.\n\nScarsità di Dati: In molti scenari reali, acquisire un set di dati etichettato sufficientemente grande per addestrare un modello ML da zero è complicato. Il transfer learning mitiga questo problema consentendo l’uso di modelli pre-addestrati che hanno già appreso funzionalità preziose da un vasto set di dati.\nSpese Computazionali: Addestrare un modello da zero richiede risorse computazionali e tempo significativi, specialmente per modelli complessi come reti neurali profonde. Utilizzando il transfer learning, possiamo sfruttare il calcolo che è già stato eseguito durante l’addestramento del modello sorgente, risparmiando così tempo e potenza computazionale.\nDati Annotati Limitati: Per alcune attività specifiche, potrebbero essere disponibili ampi dati grezzi, ma il processo di etichettatura di tali dati per l’apprendimento supervisionato può essere costoso e richiedere molto tempo. Il transfer learning ci consente di utilizzare modelli pre-addestrati su un’attività correlata con dati etichettati, quindi richiedendo meno dati annotati per la nuova attività.\n\nCi sono vantaggi nel riutilizzare le funzionalità:\n\nHierarchical Feature Learning: I modelli di deep learning, in particolare le reti neurali convoluzionali (CNN), possono apprendere funzionalità gerarchiche. I layer inferiori in genere apprendono funzionalità generiche come bordi e forme, mentre quelli superiori apprendono funzionalità più complesse e specifiche per l’attività. Il transfer learning ci consente di riutilizzare le funzionalità generiche apprese da un modello e di perfezionare i livelli superiori per la nostra attività specifica.\nAumento delle Prestazioni: È stato dimostrato che il transfer learning aumenta le prestazioni dei modelli su attività con dati limitati. La conoscenza acquisita dall’attività dall’attività sorgente può fornire un prezioso punto di partenza e portare a una convergenza più rapida e a una maggiore accuratezza nell’attività target.\n\n\n\n\n\n\n\nEsercizio 12.1: Il Transfer Learning\n\n\n\n\n\nSi immagini di addestrare un’IA a riconoscere i fiori come un professionista, ma senza aver bisogno di un milione di immagini di fiori! Questo è il potere del transfer learning. In questo Colab, prenderemo un’IA che conosce già le immagini e le insegneremo a diventare un’esperta di fiori con meno sforzo. Prepararsi a rendere la propria IA più intelligente, non è più difficile!\n\n\n\n\n\n\n12.4.4 Concetti Fondamentali\nComprendere i concetti fondamentali del transfer learning è essenziale per utilizzare efficacemente questo potente approccio in ML. Qui, analizzeremo alcuni dei principi e dei componenti principali che stanno alla base del processo di transfer learning.\n\nAttività di Origine e di Destinazione\nNel transfer learning, sono coinvolte due attività principali: l’attività di origine e quella di destinazione. L’attività di origine è quella per la quale il modello è già stato addestrato e ha appreso informazioni preziose. L’attività di destinazione è la nuova attività che vogliamo che il modello esegua. L’obiettivo del transfer learning è sfruttare le conoscenze acquisite dall’attività di origine per migliorare le prestazioni nell’attività di destinazione.\nSupponiamo di avere un modello addestrato per riconoscere vari frutti nelle immagini (attività di origine) e di voler creare un nuovo modello per riconoscere diverse verdure nelle immagini (attività di destinazione). In tal caso, possiamo utilizzare il transfer learning per sfruttare le conoscenze acquisite durante l’attività di riconoscimento della frutta per migliorare le prestazioni del modello di riconoscimento della verdura.\n\n\nTrasferimento della Rappresentazione\nIl trasferimento della rappresentazione riguarda le rappresentazioni apprese (caratteristiche) dall’attività di origine all’attività di destinazione. Esistono tre tipi principali di trasferimento della rappresentazione:\n\nTrasferimento di Istanza: Implica il riutilizzo delle istanze di dati dall’attività di origine nell’attività di destinazione.\nTrasferimento della Rappresentazione delle Feature: Implica il trasferimento delle rappresentazioni di Feature [funzionalità] apprese dall’attività di origine all’attività di destinazione.\nTrasferimento di Parametri: Implica il trasferimento dei parametri appresi del modello (pesi) dall’attività di origine all’attività di destinazione.\n\nNell’elaborazione del linguaggio naturale, un modello addestrato per comprendere la sintassi e la grammatica di una lingua (attività di origine) può trasferire le sue rappresentazioni apprese a un nuovo modello progettato per eseguire l’analisi del sentiment (attività di destinazione).\n\n\nFinetuning\nIl “finetuning” [messa a punto] è il processo di regolazione dei parametri di un modello pre-addestrato per adattarlo all’attività di destinazione. In genere, ciò comporta l’aggiornamento dei pesi dei layer del modello, in particolare degli ultimi layer, per rendere il modello più pertinente per la nuova attività. Nella classificazione delle immagini, un modello pre-addestrato su un set di dati generale come ImageNet (attività di origine) può essere messo a punto regolando i pesi dei suoi livelli per ottenere buone prestazioni in un’attività di classificazione specifica, come il riconoscimento di specie animali specifiche (attività di destinazione).\n\n\nEstrazione delle Feature\nL’estrazione delle “feature” [caratteristiche] comporta l’utilizzo di un modello pre-addestrato come estrattore di feature fisse, in cui l’output dei layer intermedi del modello viene utilizzato come feature per l’attività di destinazione. Questo approccio è particolarmente utile quando l’attività di destinazione ha un set di dati di piccole dimensioni, poiché le feature apprese dal modello pre-addestrato possono migliorare significativamente le prestazioni. Nell’analisi delle immagini mediche, un modello pre-addestrato su un ampio set di dati di immagini mediche generali (attività di origine) può essere utilizzato come estrattore di feature per fornire funzionalità preziose per un nuovo modello progettato per riconoscere specifici tipi di tumori nelle immagini radiografiche (attività di destinazione).\n\n\n\n12.4.5 Tipi di Apprendimento Tramite Trasferimento\nL’apprendimento tramite trasferimento può essere classificato in tre tipi principali in base alla natura delle attività e dei dati di origine e di destinazione. Esploriamo ciascun tipo in dettaglio:\n\nApprendimento Tramite Trasferimento Induttivo\nNell’apprendimento tramite trasferimento induttivo, l’obiettivo è apprendere la funzione predittiva di destinazione con l’aiuto dei dati di origine. In genere comporta la messa a punto di un modello pre-addestrato sull’attività di destinazione con dati etichettati disponibili. Un esempio comune di apprendimento tramite trasferimento induttivo sono le attività di classificazione delle immagini. Ad esempio, un modello pre-addestrato sul set di dati ImageNet (attività di origine) può essere messo a punto per classificare tipi specifici di uccelli (attività di destinazione) utilizzando un set di dati etichettato più piccolo di immagini di uccelli.\n\n\nApprendimento Tramite Trasferimento Transduttivo\nL’apprendimento tramite trasferimento transduttivo comporta l’utilizzo di dati di origine e destinazione, ma solo dell’attività di origine. L’obiettivo principale è trasferire la conoscenza dal dominio di origine al dominio di destinazione, anche se le attività rimangono le stesse. L’analisi del “sentiment” per diverse lingue può servire come esempio di apprendimento tramite trasferimento transduttivo. Un modello addestrato per eseguire l’analisi del sentiment in inglese (attività di origine) può essere adattato per eseguire l’analisi del sentiment in un’altra lingua, come il francese (attività di destinazione), sfruttando set di dati paralleli di frasi in inglese e francese con gli stessi sentimenti.\n\n\nApprendimento con Trasferimento Non Supervisionato\nL’apprendimento con trasferimento non supervisionato viene utilizzato quando le attività di origine e di destinazione sono correlate, ma non sono disponibili dati etichettati per l’attività di destinazione. L’obiettivo è sfruttare la conoscenza acquisita dall’attività di origine per migliorare le prestazioni nell’attività di destinazione, anche senza dati etichettati. Un esempio di apprendimento di trasferimento non supervisionato è la modellazione degli argomenti nei dati di testo. Un modello addestrato per estrarre argomenti da articoli di notizie (attività di origine) può essere adattato per estrarre argomenti da post sui social media (attività di destinazione) senza aver bisogno di dati etichettati per i post sui social media.\n\n\nConfronto e Compromessi\nSfruttando questi diversi tipi di apprendimento per trasferimento, i professionisti possono scegliere l’approccio che meglio si adatta alla natura dei loro compiti e ai dati disponibili, portando infine a modelli di ML più efficaci ed efficienti. Quindi, in sintesi:\n\nInduttivo: Diversi compiti di origine e destinazione, domini diversi\nTrasduttivo: diversi compiti di origine e destinazione, stesso dominio\nNon supervisionato: dati di origine non etichettati, trasferisce le rappresentazioni delle feature\n\nTabella 12.1 presenta una matrice che delinea in modo un po’ più dettagliato le somiglianze e le differenze tra i tipi di apprendimento per trasferimento:\n\n\n\nTabella 12.1: Confronto dei tipi di apprendimento per trasferimento.\n\n\n\n\n\n\n\n\n\n\n\nAspetto\nApprendimento Induttivo per Trasferimento\nApprendimento Trasduttivo per Trasferimento\nApprendimento Non Supervisionato\n\n\n\n\nDati Etichettati per l’Attività di Destinazione\nObbligatorio\nNon obbligatorio\nNon obbligatorio\n\n\nAttività di origine\nPuò essere diversa\nLo stesso\nLo stesso o diverso\n\n\nAttività di destinazione\nPuò essere diversa\nLo stesso\nPuò essere diverso\n\n\nObiettivo\nMigliorare le prestazioni dell’attività target con i dati sorgente\nTrasferisci la conoscenza dal dominio sorgente a quello target\nSfrutta l’attività sorgente per migliorare le prestazioni dell’attività target senza dati etichettati\n\n\nEsempio\nDa ImageNet alla classificazione degli uccelli\nAnalisi del sentiment in diverse lingue\nModellazione degli argomenti per diversi dati di testo\n\n\n\n\n\n\n\n\n\n12.4.6 Vincoli e Considerazioni\nQuando si intraprende un apprendimento per trasferimento, ci sono diversi fattori che devono essere considerati per garantire un trasferimento di conoscenze di successo e prestazioni del modello. Ecco una ripartizione di alcuni fattori chiave:\n\nSomiglianza dei Domini\nLa similarità di dominio si riferisce a quanto strettamente correlati sono i domini di origine e di destinazione. Più simili sono i domini, più è probabile che l’apprendimento per trasferimento abbia successo. Trasferire la conoscenza da un modello addestrato su immagini di scene esterne (dominio di origine) a un nuovo compito che prevede il riconoscimento di oggetti in scene interne (dominio di destinazione) potrebbe avere più successo rispetto al trasferire la conoscenza da scene esterne a un compito che prevede l’analisi del testo, poiché i domini (immagini vs. testo) sono piuttosto diversi.\n\n\nSimilarità dell’Attività\nLa similarità dell’attività si riferisce a quanto strettamente correlati sono le attività di origine e di destinazione. È probabile che attività simili traggano maggiori benefici dall’apprendimento per trasferimento. Un modello addestrato per riconoscere diverse razze di cani (attività di origine) può essere adattato più facilmente per riconoscere diverse razze di gatti (attività di destinazione) rispetto a quanto possa essere adattato per eseguire un’attività completamente diversa come la traduzione di una lingua.\n\n\nQualità e Quantità dei Dati\nLa qualità e la quantità dei dati disponibili per il compito di destinazione possono avere un impatto significativo sul successo dell’apprendimento per trasferimento. Più dati di alta qualità possono comportare migliori prestazioni del modello. Supponiamo di avere un ampio set di dati con immagini chiare e ben etichettate per riconoscere specie di uccelli specifiche. In tal caso, il processo di apprendimento per trasferimento avrà probabilmente più successo rispetto a un set di dati piccolo e rumoroso.\n\n\nSovrapposizione dello Spazio delle Feature\nLa sovrapposizione dello spazio delle feature si riferisce a quanto bene le feature apprese dal modello sorgente si allineano con quelle necessarie per l’attività di destinazione. Una maggiore sovrapposizione può portare a un apprendimento per trasferimento più efficace. Un modello addestrato su immagini ad alta risoluzione (attività di origine) potrebbe non trasferirsi bene a un’attività di destinazione che coinvolge immagini a bassa risoluzione, poiché lo spazio delle feature (alta risoluzione rispetto a bassa risoluzione) è diverso.\n\n\nComplessità del Modello\nAnche la complessità del modello sorgente può influire sul successo dell’apprendimento per trasferimento. A volte, un modello più semplice potrebbe trasferirsi meglio di uno complesso, poiché è meno probabile che si adatti eccessivamente all’attività di origine. Ad esempio, un semplice modello di rete neurale convoluzionale (CNN) addestrato su dati di immagini (attività di origine) può essere trasferito con maggiore successo a una nuova attività di classificazione di immagini (attività di destinazione) rispetto a una CNN complessa con molti layer, poiché è meno probabile che il modello più semplice si adatti eccessivamente all’attività di origine.\nConsiderando questi fattori, i professionisti del ML possono prendere decisioni informate su quando e come utilizzare l’apprendimento per trasferimento, portando infine a prestazioni del modello più efficaci nell’attività di destinazione. Il successo dell’apprendimento per trasferimento dipende dal grado di similarità tra i domini di origine e di destinazione. L’overfitting [adattamento eccessivo] è rischioso, soprattutto quando la messa a punto avviene su un set di dati limitato. Sul fronte computazionale, alcuni modelli pre-addestrati, a causa delle loro dimensioni, potrebbero non adattarsi comodamente ai vincoli di memoria di alcuni dispositivi o potrebbero essere eseguiti in modo proibitivamente lento. Nel tempo, con l’evoluzione dei dati, c’è il potenziale per la “drift” [deriva] del modello, che indica la necessità di un riaddestramento periodico o di un adattamento continuo.\nScoprire di più sull’apprendimento per trasferimento in Video 12.1 di seguito.\n\n\n\n\n\n\nVideo 12.1: Il Transfer Learning",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#sec-fl",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#sec-fl",
    "title": "12  Apprendimento On-Device",
    "section": "12.5 Apprendimento Automatico Federato",
    "text": "12.5 Apprendimento Automatico Federato\nPanoramica dell’Apprendimento Federato\nL’Internet moderna è piena di grandi reti di dispositivi connessi. Che si tratti di telefoni cellulari, termostati, smart speaker o altri prodotti IoT, innumerevoli dispositivi edge sono una miniera d’oro per dati iperpersonalizzati e ricchi. Tuttavia, con quei dati ricchi arriva una serie di problemi con il trasferimento delle informazioni e la privacy. Costruire un set di dati di training nel cloud da questi dispositivi comporterebbe un’ampia larghezza di banda, costi per il trasferimento dati e violazione della privacy degli utenti.\nL’apprendimento federato offre una soluzione a questi problemi: addestrare i modelli parzialmente sui dispositivi edge e comunicare solo gli aggiornamenti al cloud. Nel 2016, un team di Google ha progettato un’architettura per l’apprendimento federato che tenta di risolvere questi problemi.\nNel loro articolo iniziale, Google delinea un principio di algoritmo di apprendimento federato chiamato FederatedAveraging, che è mostrato in Figura 12.5. In particolare, FederatedAveraging esegue la “stochastic gradient descent (SGD) [discesa del gradiente stocastico] su diversi dispositivi edge. In questo processo, ogni dispositivo calcola un gradiente \\(g_k = \\nabla F_k(w_t)\\) che viene poi applicato per aggiornare i pesi lato server come (con \\(\\eta\\) come tasso di apprendimento su \\(k\\) client): \\[\nw_{t+1} \\rightarrow w_t - \\eta \\sum_{k=1}^{K} \\frac{n_k}{n}g_k\n\\] Questo riassume l’algoritmo di base per l’apprendimento federato sulla destra. Per ogni round di addestramento, il server prende un set casuale di dispositivi client e chiama ogni client per addestrare sul suo batch locale usando i pesi lato server più recenti. Tali pesi vengono poi restituiti al server, dove vengono raccolti individualmente e calcolati per aggiornare i pesi del modello globale.\n\n\n\n\n\n\nFigura 12.5: Algoritmo FederatedAverage proposto da Google. Fonte: McMahan et al. (2017).\n\n\n\nCon questa struttura proposta, ci sono alcuni vettori chiave per ottimizzare ulteriormente l’apprendimento federato. Descriveremo ciascuno di essi nelle seguenti sottosezioni.\nVideo 12.2 fornisce una panoramica dell’apprendimento federato.\n\n\n\n\n\n\nVideo 12.2: Il Transfer Learning\n\n\n\n\n\n\n\n12.5.1 Efficienza della Comunicazione\nUno dei principali colli di bottiglia nell’apprendimento federato è la comunicazione. Ogni volta che un client addestra il modello, deve comunicare i propri aggiornamenti al server. Analogamente, una volta che il server ha calcolato la media di tutti gli aggiornamenti, deve inviarli al client. Ciò comporta enormi costi di larghezza di banda e risorse su grandi reti di milioni di dispositivi. Con l’avanzare del campo dell’apprendimento federato, sono state sviluppate alcune ottimizzazioni per ridurre al minimo questa comunicazione. Per affrontare l’ingombro del modello, i ricercatori hanno sviluppato tecniche di compressione del modello. Nel protocollo client-server, l’apprendimento federato può anche ridurre al minimo la comunicazione tramite la condivisione selettiva degli aggiornamenti sui client. Infine, anche tecniche di aggregazione efficienti possono semplificare il processo di comunicazione.\n\n\n12.5.2 Compressione del Modello\nNell’apprendimento federato standard, il server comunica l’intero modello a ciascun client, quindi il client invia tutti i pesi aggiornati. Ciò significa che il modo più semplice per ridurre l’ingombro di memoria e comunicazione del client è ridurre al minimo le dimensioni del modello che deve essere comunicato. Possiamo impiegare tutte le strategie di ottimizzazione del modello discusse in precedenza per farlo.\nNel 2022, un altro team di Google ha proposto che ogni client comunichi tramite un formato compresso e decomprima il modello al volo per l’addestramento (Yang et al. 2023), allocando e deallocando l’intera memoria per il modello solo per un breve periodo durante l’addestramento. Il modello viene compresso tramite una gamma di diverse strategie di quantizzazione elaborate nel loro documento. Nel frattempo, il server può aggiornare il modello non compresso decomprimendolo e applicando gli aggiornamenti man mano che arrivano.\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Françoise Beaufays, Rajiv Mathews, e Mingqing Chen. 2023. «Online Model Compression for Federated Learning with Large Models». In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1–5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\n12.5.3 Condivisione Selettiva degli Aggiornamenti\nEsistono molti metodi per condividere selettivamente gli aggiornamenti. Il principio generale è che la riduzione della porzione del modello che i client stanno addestrando lato edge riduce la memoria necessaria per l’addestramento e la dimensione della comunicazione con il server. Nell’apprendimento federato di base, il client addestra l’intero modello. Ciò significa che quando un client invia un aggiornamento al server, ha gradienti per ogni peso nella rete.\nTuttavia, non possiamo semplicemente ridurre la comunicazione inviando parti di quei gradienti da ogni client al server perché i gradienti fanno parte di un intero aggiornamento necessario per migliorare il modello. Invece, si deve progettare architettonicamente il modello in modo che ogni client addestri solo una piccola parte del modello più ampio, riducendo la comunicazione totale e ottenendo comunque il vantaggio dell’addestramento sui dati del client. Un articolo (Shi e Radu 2022) dell’Università di Sheffield applica questo concetto a una CNN suddividendo il modello globale in due parti: una parte superiore e una inferiore, come mostrato in Z. Chen e Xu (2023).\n\nShi, Hongrui, e Valentin Radu. 2022. «Data selection for efficient model update in federated learning». In Proceedings of the 2nd European Workshop on Machine Learning and Systems, 72–78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\n\n\n\n\nFigura 12.6: Suddivisione dell’architettura del modello per la condivisione selettiva. Fonte: Shi et al., (2022).\n\n\n\nLa parte inferiore è progettata per concentrarsi sulle funzionalità generiche nel set di dati, mentre la parte superiore, addestrata su tali funzionalità generiche, è progettata per essere più sensibile alle mappe di attivazione. Ciò significa che la parte inferiore del modello viene addestrata tramite la media federata standard su tutti i client. Nel frattempo, la parte superiore del modello viene addestrata interamente sul lato server dalle mappe di attivazione generate dai client. Questo approccio riduce drasticamente la comunicazione per il modello, rendendo comunque la rete robusta a vari tipi di input trovati nei dati sui dispositivi client.\n\n\n12.5.4 Aggregazione Ottimizzata\nOltre a ridurre il sovraccarico di comunicazione, l’ottimizzazione della funzione di aggregazione può migliorare la velocità e l’accuratezza dell’addestramento del modello in determinati casi d’uso di apprendimento federato. Mentre lo standard per l’aggregazione è solo la media, vari altri approcci possono migliorare l’efficienza, l’accuratezza e la sicurezza del modello. Un’alternativa è la “media ritagliata”, che limita gli aggiornamenti del modello entro un intervallo specifico. Un’altra strategia per preservare la sicurezza è l’aggregazione media della privacy differenziale. Questo approccio integra la privacy differenziale nella fase di aggregazione per proteggere le identità dei client. Ogni client aggiunge uno strato di rumore casuale ai propri aggiornamenti prima di comunicare al server. Il server si aggiorna quindi con gli aggiornamenti rumorosi, il che significa che la quantità di rumore deve essere regolata attentamente per bilanciare privacy e precisione.\nOltre ai metodi di aggregazione che migliorano la sicurezza, ci sono diverse modifiche ai metodi di aggregazione che possono migliorare la velocità di training e le prestazioni aggiungendo metadati client insieme agli aggiornamenti del peso. Il “Momentum aggregation” è una tecnica che aiuta ad affrontare il problema della convergenza. Nell’apprendimento federato, i dati client possono essere estremamente eterogenei a seconda dei diversi ambienti in cui vengono utilizzati i dispositivi. Ciò significa che molti modelli con dati eterogenei potrebbero aver bisogno di aiuto per convergere. Ogni client memorizza localmente un termine di “momentum”, che traccia il ritmo del cambiamento su diversi aggiornamenti. Con i client che comunicano questo “momentum”, il server può tenere conto della velocità di cambiamento di ogni aggiornamento quando si modifica il modello globale per accelerare la convergenza. Allo stesso modo, l’aggregazione ponderata può tenere conto delle prestazioni del client o di altri parametri come il tipo di dispositivo o la potenza della connessione di rete per regolare il peso con cui il server dovrebbe incorporare gli aggiornamenti del modello. Ulteriori descrizioni di algoritmi di aggregazione specifici sono fornite da Moshawrab et al. (2023).\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim, e Ali Raad. 2023. «Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives». Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\n12.5.5 Gestione dei Dati non-IID\nQuando si utilizza l’apprendimento federato per addestrare un modello su molti dispositivi client, è conveniente considerare i dati come indipendenti e distribuiti in modo identico (IID) su tutti i client. Quando i dati sono IID, il modello convergerà più velocemente e funzionerà meglio perché ogni aggiornamento locale su un dato client è più rappresentativo del set di dati più ampio. Questo semplifica l’aggregazione, poiché è possibile calcolare direttamente la media di tutti i client. Tuttavia, differisce dal modo in cui i dati spesso appaiono nel mondo reale. Si considerino alcuni dei seguenti modi in cui i dati possono essere non IID:\n\nImparando su un set di dispositivi di monitoraggio sanitari, diversi modelli di dispositivi potrebbero significare diverse qualità e proprietà dei sensori. Ciò significa che sensori e dispositivi di bassa qualità possono produrre dati e, pertanto, aggiornamenti del modello nettamente diversi da quelli di alta qualità\nUna tastiera intelligente addestrata per eseguire la correzione automatica. Se si ha una quantità sproporzionata di dispositivi da una determinata regione, lo slang, la struttura della frase o persino il linguaggio che stavano usando potrebbero deviare più aggiornamenti verso un certo stile di digitazione\nSe si hanno sensori per la fauna selvatica in aree remote, la connettività potrebbe non essere distribuita equamente, facendo sì che alcuni client in determinate regioni non siano in grado di inviare più aggiornamenti rispetto ad altri. Se quelle regioni hanno un’attività di fauna selvatica diversa da alcune specie, ciò potrebbe distorcere gli aggiornamenti verso quegli animali\n\nEsistono alcuni approcci per affrontare i dati non IID nell’apprendimento federato. Uno potrebbe essere quello di modificare l’algoritmo di aggregazione. Se si utilizza un algoritmo di aggregazione ponderato, è possibile regolarlo in base a diverse proprietà del client come regione, proprietà del sensore o connettività (Zhao et al. 2018).\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, e Vikas Chandra. 2018. «Federated learning with non-iid data». ArXiv preprint abs/1806.00582. https://arxiv.org/abs/1806.00582.\n\n\n12.5.6 Selezione del Client\nConsiderando tutti i fattori che influenzano l’efficacia dell’apprendimento federato, come i dati IID e la comunicazione, la selezione del client è una componente chiave per garantire che un sistema si alleni bene. La selezione dei client sbagliati può distorcere il dataset, con conseguenti dati non IID. Analogamente, la scelta casuale di client con cattive connessioni di rete può rallentare la comunicazione. Pertanto, è necessario considerare diverse caratteristiche chiave quando si seleziona il sottoinsieme corretto di client.\nQuando si selezionano i client, ci sono tre componenti principali da considerare: eterogeneità dei dati, allocazione delle risorse e costo della comunicazione. Possiamo selezionare i client in base alle metriche proposte in precedenza nella sezione non IID per affrontare l’eterogeneità dei dati. Nell’apprendimento federato, tutti i dispositivi possono avere diverse quantità di elaborazione, con il risultato che alcuni sono più inefficienti di altri nell’addestramento. Quando si seleziona un sottoinsieme di client per l’addestramento, si deve considerare un equilibrio tra eterogeneità dei dati e risorse disponibili. In uno scenario ideale, è sempre possibile selezionare il sottoinsieme di client con le maggiori risorse. Tuttavia, questo potrebbe distorcere il set di dati, quindi è necessario trovare un equilibrio. Le differenze di comunicazione aggiungono un altro layer; si desidera evitare di essere bloccati dall’attesa che i dispositivi con connessioni scadenti trasmettano tutti i loro aggiornamenti. Pertanto, è anche necessario considerare la scelta di un sottoinsieme di dispositivi diversi ma ben collegati.\n\n\n12.5.7 Un Esempio di Apprendimento Federato Distribuito: G board\nUn esempio primario di un sistema di apprendimento federato distribuito è la tastiera di Google, Gboard, per dispositivi Android. Nell’implementare l’apprendimento federato per la tastiera, Google si è concentrata sull’impiego di tecniche di privacy differenziali per proteggere i dati e l’identità dell’utente. Gboard sfrutta modelli linguistici per diverse funzionalità chiave, come Next Word Prediction (NWP), Smart Compose (SC) e On-The-Fly rescoring (OTF) (Xu et al. 2023), come mostrato in Figura 12.7.\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A Choquette-Choo, Peter Kairouz, H Brendan McMahan, Jesse Rosenstock, e Yuanbo Zhang. 2023. «Federated Learning of Gboard Language Models with Differential Privacy». ArXiv preprint abs/2305.18465. https://arxiv.org/abs/2305.18465.\nNWP anticiperà la parola successiva che l’utente tenta di digitare in base a quella precedente. SC fornisce suggerimenti in linea per velocizzare la digitazione in base a ciascun carattere. OTF riclassificherà le parole successive proposte in base al processo di digitazione attivo. Tutti e tre questi modelli devono essere eseguiti rapidamente sull’edge e l’apprendimento federato può accelerare l’addestramento sui dati degli utenti. Tuttavia, caricare ogni parola digitata da un utente sul cloud per l’addestramento costituirebbe una violazione massiccia della privacy. Pertanto, l’apprendimento federato enfatizza la privacy differenziale, che protegge l’utente consentendo al contempo una migliore esperienza utente.\n\n\n\n\n\n\nFigura 12.7: Funzionalità di Google G Board. Fonte: Zheng et al., (2023).\n\n\n\nPer raggiungere questo obiettivo, Google ha impiegato il suo algoritmo DP-FTRL, che fornisce una garanzia formale che i modelli addestrati non memorizzeranno dati o identità utente specifici. La progettazione del sistema dell’algoritmo è mostrata in Figura 12.8. DP-FTRL, combinato con l’aggregazione sicura, crittografa gli aggiornamenti del modello e fornisce un equilibrio ottimale tra privacy e utilità. Inoltre, il clipping adattivo viene applicato nel processo di aggregazione per limitare l’impatto dei singoli utenti sul modello globale (passaggio 3 in Figura 12.8). Combinando tutte queste tecniche, Google può perfezionare continuamente la sua tastiera preservando al contempo la privacy dell’utente in un modo formalmente dimostrabile.\n\n\n\n\n\n\nFigura 12.8: Privacy Differenziale in G Board. Fonte: Zheng et al., (2023).\n\n\n\n\n\n\n\n\n\nEsercizio 12.2: Apprendimento Federato - Generazione di Testo\n\n\n\n\n\nAvete mai usato quelle tastiere intelligenti che suggeriscono la parola successiva? Con l’apprendimento federato, possiamo renderle ancora migliori senza sacrificare la privacy. In questo Colab, insegneremo a un’IA a prevedere le parole tramite l’addestramento su dati di testo distribuiti su più dispositivi. Prepariamoci a rendere la digitazione ancora più fluida!\n\n\n\n\n\n\n\n\n\n\nEsercizio 12.3: Apprendimento Federato - Classificazione delle Immagini\n\n\n\n\n\nVogliamo addestrare un’IA esperta di immagini senza inviare le proprie foto al cloud? L’apprendimento federato è la risposta! In questo Colab, addestreremo un modello su più dispositivi, ognuno dei quali apprende dalle proprie immagini. La privacy è protetta e il lavoro di squadra fa funzionare il sogno dell’IA!\n\n\n\n\n\n\n12.5.8 Benchmarking per l’Apprendimento Federato: MedPerf\nUno degli esempi più ricchi di dati edge sono i dispositivi medici. Questi dispositivi memorizzano alcuni dei dati più personali degli utenti, ma offrono enormi progressi nel trattamento personalizzato e una migliore accuratezza nell’intelligenza artificiale medica. Dati questi due fattori, i dispositivi medici sono il caso d’uso perfetto per l’apprendimento federato. MedPerf è una piattaforma open source utilizzata per confrontare i modelli utilizzando la valutazione federata (Karargyris et al. 2023). Invece di addestrare semplicemente i modelli tramite apprendimento federato, MedPerf porta il modello sui dispositivi edge per testarlo rispetto ai dati personalizzati, preservando al contempo la privacy. In questo modo, un comitato di benchmark può valutare vari modelli nel mondo reale sui dispositivi edge, preservando comunque l’anonimato del paziente.\n\nKarargyris, Alexandros, Renato Umeton, Micah J Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023. «Federated benchmarking of medical artificial intelligence with MedPerf». Nature Machine Intelligence 5 (7): 799–810. https://doi.org/10.1038/s42256-023-00652-2.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#problemi-di-sicurezza",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#problemi-di-sicurezza",
    "title": "12  Apprendimento On-Device",
    "section": "12.6 Problemi di Sicurezza",
    "text": "12.6 Problemi di Sicurezza\nL’esecuzione di training e adattamento del modello ML sui dispositivi degli utenti finali introduce anche rischi per la sicurezza che devono essere affrontati. Alcune preoccupazioni chiave per la sicurezza includono:\n\nEsposizione di dati privati: I dati di training potrebbero essere trapelati o rubati dai dispositivi\nAvvelenamento dei dati: Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello\nEstrazione del modello: Degli aggressori potrebbero tentare di rubare i parametri del modello addestrato\nInferenza di appartenenza: I modelli potrebbero rivelare la partecipazione di dati di utenti specifici\nAttacchi di evasione: Input appositamente creati possono causare una classificazione errata\n\nQualsiasi sistema che esegue l’apprendimento sul dispositivo introduce preoccupazioni per la sicurezza, poiché potrebbe esporre vulnerabilità in modelli su larga scala. Numerosi rischi per la sicurezza sono associati a qualsiasi modello ML, ma questi rischi hanno conseguenze specifiche per l’apprendimento on-device. Fortunatamente, esistono metodi per mitigare questi rischi e migliorare le prestazioni reali dell’apprendimento su dispositivo.\n\n12.6.1 Avvelenamento dei Dati\nL’apprendimento automatico on-device introduce sfide uniche per la sicurezza dei dati rispetto all’addestramento tradizionale basato su cloud. In particolare, gli attacchi di avvelenamento dei dati rappresentano una seria minaccia durante l’apprendimento su dispositivo. Gli avversari possono manipolare i dati di training per degradare le prestazioni del modello quando vengono distribuiti.\nEsistono diverse tecniche di attacco di avvelenamento dei dati:\n\nLabel Flipping: Comporta l’applicazione di etichette errate ai campioni. Ad esempio, nella classificazione delle immagini, le foto di gatti possono essere etichettate come cani per confondere il modello. Anche capovolgere il 10% delle etichette può avere conseguenze significative sul modello.\nInserimento dei Dati: Introduce input falsi o distorti nel set di training. Ciò potrebbe includere immagini pixelate, audio rumoroso o testo distorto.\nCorruzione Logica: Altera i [pattern] sottostanti (https://www.worldscientific.com/doi/10.1142/S0218001414600027) nei dati per fuorviare il modello. Nell’analisi del “sentiment”, le recensioni altamente negative possono essere contrassegnate come positive tramite questa tecnica. Per questo motivo, recenti sondaggi hanno dimostrato che molte aziende hanno più paura dell’avvelenamento dei dati rispetto ad altre preoccupazioni di ML avversarie.\n\nCiò che rende l’avvelenamento dei dati allarmante è il modo in cui sfrutta la discrepanza tra set di dati curati e dati di training in tempo reale. Consideriamo un set di dati di foto di gatti raccolti da Internet. Nelle settimane successive, quando questi dati addestrano un modello on-device, le nuove foto di gatti sul Web differiscono in modo significativo.\nCon l’avvelenamento dei dati, gli aggressori acquistano domini e caricano contenuti che influenzano una parte dei dati di training. Anche piccole modifiche ai dati hanno un impatto significativo sul comportamento appreso dal modello. Di conseguenza, l’avvelenamento può instillare pregiudizi razzisti, sessisti o altri pregiudizi dannosi se non controllato.\nMicrosoft Tay è un chatbot lanciato da Microsoft nel 2016. È stato progettato per imparare dalle sue interazioni con gli utenti su piattaforme di social media come Twitter. Sfortunatamente, Microsoft Tay è diventato un esempio lampante di avvelenamento dei dati nei modelli di ML. Entro 24 ore dal suo lancio, Microsoft ha dovuto mettere Tay offline perché aveva iniziato a produrre messaggi offensivi e inappropriati, tra cui incitamento all’odio e commenti razzisti. Ciò è accaduto perché alcuni utenti sui social media hanno intenzionalmente fornito a Tay input dannosi e offensivi, da cui il chatbot ha poi imparato e incorporato nelle sue risposte.\nQuesto incidente è un chiaro esempio di avvelenamento dei dati, poiché i malintenzionati hanno intenzionalmente manipolato i dati utilizzati per addestrare il chatbot e modellarne le risposte. L’avvelenamento dei dati ha portato il chatbot ad adottare pregiudizi dannosi e a produrre output che i suoi sviluppatori non avevano previsto. Dimostra come anche piccole quantità di dati creati in modo dannoso possano avere un impatto significativo sul comportamento dei modelli ML e sottolinea l’importanza di implementare solidi meccanismi di filtraggio e convalida dei dati per impedire che tali incidenti si verifichino.\nTali pregiudizi potrebbero avere pericolosi impatti nel mondo reale. La convalida rigorosa dei dati, il rilevamento delle anomalie e il monitoraggio della provenienza dei dati sono misure difensive fondamentali. L’adozione di framework come Five Safes garantisce che i modelli siano addestrati su dati rappresentativi di alta qualità (Desai et al. 2016).\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. «Five Safes: Designing data access for research». Economics Working Paper Series 1601: 28.\nL’avvelenamento dei dati è una preoccupazione urgente per l’apprendimento sicuro sul dispositivo poiché i dati dell’endpoint non possono essere facilmente monitorati in tempo reale. Se ai modelli viene consentito di adattarsi da soli, corriamo il rischio che il dispositivo agisca in modo dannoso. Tuttavia, la ricerca continua nell’ML avversario mira a sviluppare soluzioni robuste per rilevare e mitigare tali attacchi ai dati.\n\n\n12.6.2 Attacchi Avversari\nDurante la fase di addestramento, gli aggressori potrebbero iniettare dati dannosi nel dataset di training, il che può alterare sottilmente il comportamento del modello. Ad esempio, un aggressore potrebbe aggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini. Se fatto in modo intelligente, l’accuratezza del modello potrebbe non diminuire in modo significativo e l’attacco potrebbe essere notato. Il modello classificherebbe quindi erroneamente alcuni gatti come cani, il che potrebbe avere conseguenze a seconda dell’applicazione.\nIn un sistema di telecamere di sicurezza embedded, ad esempio, ciò potrebbe consentire a un intruso di evitare il rilevamento indossando uno specifico pattern che il modello è stato ingannato a classificare come non minaccioso.\nDurante la fase di inferenza, gli aggressori possono utilizzare esempi avversari per ingannare il modello. Gli esempi avversari sono input che sono stati leggermente alterati in un modo da far sì che il modello faccia previsioni errate. Ad esempio, un aggressore potrebbe aggiungere una piccola quantità di rumore a un’immagine in un modo che un sistema di riconoscimento facciale identifichi erroneamente una persona. Questi attacchi possono essere particolarmente preoccupanti nelle applicazioni in cui è in gioco la sicurezza, come i veicoli autonomi. Un esempio concreto di ciò è quando i ricercatori sono riusciti a far sì che un sistema di riconoscimento della segnaletica stradale classificasse erroneamente un segnale di stop come un segnale di limite di velocità. Questo tipo di classificazione errata potrebbe causare incidenti se si verificasse in un sistema di guida autonoma nel mondo reale.\nPer mitigare questi rischi, possono essere impiegate diverse difese:\n\nValidazione e Sanificazione dei Dati: Prima di incorporare nuovi dati nel dataset di addestramento, questi devono essere convalidati e sanificati a fondo per garantire che non siano dannosi.\nAddestramento Avversario: Il modello può essere addestrato su esempi avversari per renderlo più robusto a questi tipi di attacchi.\nValidazione degli Input: Durante l’inferenza, gli input devono essere convalidati per garantire che non siano stati manipolati per creare esempi avversari.\nAudit e Monitoraggio Regolari: L’audit e il monitoraggio regolari del comportamento del modello possono aiutare a rilevare e mitigare gli attacchi avversari. Tuttavia, è più facile a dirsi che a farsi nel contesto di piccoli sistemi ML. Spesso è difficile monitorare i sistemi ML embedded all’endpoint a causa delle limitazioni della larghezza di banda della comunicazione, di cui parleremo nel capitolo MLOps.\n\nComprendendo i potenziali rischi e implementando queste difese, possiamo contribuire a proteggere il training on-device all’endpoint/edge e mitigare l’impatto degli attacchi avversari. La maggior parte delle persone confonde facilmente l’avvelenamento dei dati e gli attacchi avversari. Quindi Tabella 12.2 confronta l’avvelenamento dei dati e gli attacchi avversari:\n\n\n\nTabella 12.2: Confronto tra avvelenamento dei dati e attacchi avversari.\n\n\n\n\n\n\n\n\n\n\nAspetto\nAvvelenamento dei dati\nAttacchi avversari\n\n\n\n\nTempistica\nFase di addestramento\nFase di inferenza\n\n\nTarget\nDati di addestramento\nDati di input\n\n\nObiettivo\nInfluenza negativamente le prestazioni del modello\nCausa previsioni errate\n\n\nMetodo\nInserire esempi dannosi nei dati di training, spesso con etichette errate\nAggiungere rumore attentamente elaborato ai dati di input\n\n\nEsempio\nAggiungere immagini di gatti etichettati come cani a un set di dati utilizzato per addestrare un modello di classificazione delle immagini\nAggiungere una piccola quantità di rumore a un’immagine in modo che un sistema di riconoscimento facciale identifichi erroneamente una persona\n\n\nEffetti Potenziali\nIl modello apprende modelli errati e fa previsioni errate\nPrevisioni errate immediate e potenzialmente pericolose\n\n\nApplicazioni Interessate\nQualsiasi modello ML\nVeicoli autonomi, sistemi di sicurezza, ecc.\n\n\n\n\n\n\n\n\n12.6.3 Inversione del Modello\nGli attacchi di inversione del modello rappresentano una minaccia per la privacy dei modelli di machine learning su dispositivo addestrati su dati utente sensibili (Nguyen et al. 2023). Comprendere questo vettore di attacco e le strategie di mitigazione saranno importanti per creare un’intelligenza artificiale su dispositivo sicura ed etica. Ad esempio, si immagini un’app per iPhone che utilizza l’apprendimento su dispositivo per categorizzare le foto in gruppi come “spiaggia”, “cibo” o “selfie” per una ricerca più semplice.\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, e Ngai-Man Cheung. 2023. «Re-Thinking Model Inversion Attacks Against Deep Neural Networks». In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 16384–93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\nIl modello su dispositivo potrebbe essere addestrato da Apple su un set di dati di foto iCloud di utenti consenzienti. Un aggressore malintenzionato potrebbe tentare di estrarre parti di quelle foto di addestramento iCloud originali utilizzando l’inversione del modello. In particolare, l’aggressore inserisce input sintetici creati ad arte nel classificatore di foto su dispositivo. Modificando gli input sintetici e osservando come il modello li categorizza, possono perfezionare gli input fino a ricostruire copie dei dati di training originali, come una foto di una spiaggia dall’iCloud di un utente. Ora, l’aggressore ha violato la privacy di quell’utente ottenendo una delle sue foto senza consenso. Questo dimostra perché l’inversione del modello è pericolosa: può potenzialmente far trapelare dati di training altamente sensibili.\nLe foto sono un tipo di dati particolarmente rischioso perché spesso contengono persone identificabili, informazioni sulla posizione e momenti privati. Tuttavia, la stessa metodologia di attacco potrebbe essere applicata ad altri dati personali, come registrazioni audio, messaggi di testo o dati sanitari degli utenti.\nPer difendersi dall’inversione del modello, sarebbe necessario prendere precauzioni come l’aggiunta di rumore agli output del modello o l’utilizzo di tecniche di apprendimento automatico che preservano la privacy come l’apprendimento federato per addestrare il modello sul dispositivo. L’obiettivo è impedire agli aggressori di ricostruire i dati di training originali.\n\n\n12.6.4 Problemi di Sicurezza dell’Apprendimento On-Device\nSebbene l’avvelenamento dei dati e gli attacchi avversari siano preoccupazioni comuni per i modelli ML in generale, l’apprendimento su dispositivo introduce rischi di sicurezza unici. Quando vengono pubblicate varianti su dispositivo di modelli su larga scala, gli avversari possono sfruttare questi modelli più piccoli per attaccare le loro controparti più grandi. La ricerca ha dimostrato che man mano che i modelli su dispositivo e i modelli su scala reale diventano più simili, la vulnerabilità dei modelli originali su larga scala aumenta in modo significativo. Ad esempio, le valutazioni su 19 reti neurali profonde (DNN) hanno rivelato che lo sfruttamento dei modelli su dispositivo potrebbe aumentare la vulnerabilità dei modelli originali su larga scala di fino a 100 volte.\nEsistono tre tipi principali di rischi per la sicurezza specifici dell’apprendimento on-device:\n\nAttacchi Basati sul Trasferimento: Questi attacchi sfruttano la proprietà di trasferibilità tra un modello surrogato (un’approssimazione del modello di destinazione, simile a un modello su dispositivo) e un modello di destinazione remoto (il modello originale su scala reale). Gli aggressori generano esempi avversari utilizzando il modello surrogato, che può quindi essere utilizzato per ingannare il modello di destinazione. Ad esempio, si immagini un modello on-device progettato per identificare le e-mail di spam. Un aggressore potrebbe usare questo modello per generare un’e-mail di spam che non viene rilevata dal sistema di filtraggio più grande e completo.\nAttacchi Basati sull’Ottimizzazione: Questi attacchi generano esempi avversari per attacchi basati sul trasferimento usando una qualche forma di funzione obiettivo e modificano iterativamente gli input per ottenere il risultato desiderato. Gli attacchi di stima del gradiente, ad esempio, approssimano il gradiente del modello usando output di query (come punteggi di confidenza softmax), mentre gli attacchi senza gradiente usano la decisione finale del modello (la classe prevista) per approssimare il gradiente, sebbene richiedano molte più query.\nAttacchi di Query con Priorità di Trasferimento: Questi attacchi combinano elementi di attacchi basati sul trasferimento e basati sull’ottimizzazione. Eseguono il reverse engineering dei modelli sul dispositivo per fungere da surrogati del modello completo di destinazione. In altre parole, gli aggressori usano il modello sul dispositivo più piccolo per capire come funziona il modello più grande e quindi usano questa conoscenza per attaccare il modello completo.\n\nGrazie alla comprensione di questi rischi specifici associati all’apprendimento on-device, possiamo sviluppare protocolli di sicurezza più solidi per proteggere sia i modelli on-device che quelli su scala reale da potenziali attacchi.\n\n\n12.6.5 Attenuazione dei Rischi dell’Apprendimento On-Device\nSi possono impiegare vari metodi per mitigare i numerosi rischi per la sicurezza associati all’apprendimento on-device. Questi metodi possono essere specifici per il tipo di attacco o fungere da strumento generale per rafforzare la sicurezza.\nUna strategia per ridurre i rischi per la sicurezza è quella di ridurre la somiglianza tra modelli on-device e modelli su scala reale, riducendo così la trasferibilità fino al 90%. Questo metodo, noto come similarity-unpairing, affronta il problema che si verifica quando gli avversari sfruttano la somiglianza del gradiente di input tra i due modelli. Ottimizzando il modello su scala reale per creare una nuova versione con accuratezza simile ma gradienti di input diversi, possiamo costruire il modello on-device quantizzando questo modello su scala reale aggiornato. Questa disassociazione riduce la vulnerabilità dei modelli su dispositivo limitando l’esposizione del modello su scala reale originale. È importante notare che l’ordine di ottimizzazione e quantizzazione può essere variato pur ottenendo la mitigazione del rischio (Hong, Carlini, e Kurakin 2023).\nPer contrastare l’avvelenamento dei dati, è fondamentale reperire set di dati da fornitori affidabili e fidati.\nPer combattere gli attacchi avversari, si possono impiegare diverse strategie. Un approccio proattivo prevede la generazione di esempi avversari e la loro incorporazione nel set di dati di training del modello, rafforzando così il modello contro tali attacchi. Strumenti come CleverHans, una libreria di training open source, sono fondamentali per creare esempi avversari. La “Defense distillation” [distillazione della difesa] è un’altra strategia efficace, in cui il modello sul dispositivo genera probabilità di classificazioni diverse anziché decisioni definitive (Hong, Carlini, e Kurakin 2023), rendendo più difficile per gli esempi avversari sfruttare il modello.\n\nHong, Sanghyun, Nicholas Carlini, e Alexey Kurakin. 2023. «Publishing Efficient On-device Models Increases Adversarial Vulnerability». In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), 271–90. IEEE; IEEE. https://doi.org/10.1109/satml54575.2023.00026.\nIl furto di proprietà intellettuale è un altro problema significativo quando si distribuiscono modelli on-device. Il furto di proprietà intellettuale è un problema quando si distribuiscono modelli on-device, poiché gli avversari potrebbero tentare di sottoporre a reverse engineering il modello per rubare la tecnologia sottostante. Per proteggersi dal furto di proprietà intellettuale, l’eseguibile binario del modello addestrato dovrebbe essere archiviato su un’unità microcontrollore con software crittografato e interfacce fisiche protette del chip. Inoltre, il set di dati finale utilizzato per l’addestramento del modello dovrebbe essere mantenuto privato.\nInoltre, i modelli on-device utilizzano spesso set di dati noti o open source, come Visual Wake Words di MobileNet. Pertanto, è importante mantenere la privacy del set di dati finale utilizzato per l’addestramento del modello. Inoltre, proteggere il processo di “data augmentation” e incorporare casi d’uso specifici può ridurre al minimo il rischio di reverse engineering di un modello on-device.\nInfine, l’Adversarial Threat Landscape for Artificial Intelligence Systems (ATLAS) funge da prezioso strumento matriciale che aiuta a valutare il profilo di rischio dei modelli su dispositivo, consentendo agli sviluppatori di identificare e mitigare i potenziali rischi in modo proattivo.\n\n\n12.6.6 Protezione dei Dati di Training\nEsistono vari modi per proteggere i dati di training sul dispositivo. Ogni concetto è molto profondo e potrebbe valere una lezione a sé stante. Quindi, qui, faremo un breve accenno a quei concetti in modo che si sappia cosa approfondire.\n\nCrittografia\nLa crittografia funge da prima linea di difesa per i dati di training. Ciò comporta l’implementazione della crittografia end-to-end per l’archiviazione locale su dispositivi e canali di comunicazione per impedire l’accesso non autorizzato ai dati di training grezzi. Ambienti di esecuzione affidabili, come Intel SGX e ARM TrustZone, sono essenziali per facilitare il training sicuro su dati crittografati.\nInoltre, quando si aggregano aggiornamenti da più dispositivi, è possibile impiegare protocolli di elaborazione “multi-party” sicuri per migliorare la sicurezza (Kairouz, Oh, e Viswanath 2015); un’applicazione pratica di ciò è nell’apprendimento collaborativo on-device, in cui è possibile implementare l’aggregazione crittografica che preserva la privacy degli aggiornamenti del modello utente. Questa tecnica nasconde efficacemente i dati dei singoli utenti anche durante la fase di aggregazione.\n\nKairouz, Peter, Sewoong Oh, e Pramod Viswanath. 2015. «Secure Multi-party Differential Privacy». In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, a cura di Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, e Roman Garnett, 2008–16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nPrivacy Differenziale\nLa privacy differenziale è un’altra strategia cruciale per proteggere i dati di training. Iniettando rumore statistico calibrato nei dati, possiamo mascherare i singoli record estraendo comunque preziosi pattern di popolazione (Dwork e Roth 2013). Anche la gestione del budget per la privacy su più iterazioni di training e la riduzione del rumore man mano che il modello converge sono essenziali (Abadi et al. 2016). Possono essere impiegati metodi come la privacy differenziale formalmente dimostrabile, che può includere l’aggiunta di rumore di Laplace o gaussiano scalato alla sensibilità del set di dati.\n\nDwork, Cynthia, e Aaron Roth. 2013. «The Algorithmic Foundations of Differential Privacy». Foundations and Trends in Theoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nRilevamento delle Anomalie\nIl rilevamento delle anomalie svolge un ruolo importante nell’identificazione e nell’attenuazione di potenziali attacchi di avvelenamento dei dati. Ciò può essere ottenuto tramite analisi statistiche come la “Principal Component Analysis (PCA)” [analisi delle componenti principali] e il clustering, che aiutano a rilevare deviazioni nei dati di training aggregati. I metodi di serie temporali come i grafici Cumulative Sum (CUSUM) sono utili per identificare spostamenti indicativi di potenziale avvelenamento. Anche il confronto delle distribuzioni dei dati correnti con distribuzioni di dati pulite precedentemente visualizzate può aiutare a segnalare anomalie. Inoltre, i batch sospetti di essere avvelenati dovrebbero essere rimossi dal processo di aggregazione degli aggiornamenti di training. Ad esempio, è possibile condurre controlli a campione su sottoinsiemi di immagini di training sui dispositivi utilizzando hash photoDNA per identificare input avvelenati.\n\n\nValidazione dei Dati di Input\nInfine, la convalida dei dati di input è essenziale per garantire l’integrità e la validità dei dati di input prima che vengano immessi nel modello di training, proteggendo così dai payload avversari. Misure di similarità, come la distanza del coseno, possono essere impiegate per catturare input che si discostano in modo significativo dalla distribuzione prevista. Gli input sospetti che potrebbero contenere payload avversari devono essere messi in quarantena e sanificati. Inoltre, l’accesso del parser ai dati di training deve essere limitato solo ai percorsi di codice convalidati. Sfruttare le funzionalità di sicurezza hardware, come ARM Pointer Authentication, può impedire la corruzione della memoria (ARM Limited, 2023). Un esempio di ciò è l’implementazione di controlli di integrità degli input sui dati di training audio utilizzati dagli smart speaker prima dell’elaborazione da parte del modello di riconoscimento vocale (Z. Chen e Xu 2023).\n\nChen, Zhiyong, e Shugong Xu. 2023. «Learning domain-heterogeneous speaker recognition systems with personalized continual federated learning». EURASIP Journal on Audio, Speech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#framework-di-training-on-device",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#framework-di-training-on-device",
    "title": "12  Apprendimento On-Device",
    "section": "12.7 Framework di Training On-Device",
    "text": "12.7 Framework di Training On-Device\nFramework di inferenza embedded come TF-Lite Micro (David et al. 2021), TVM (T. Chen et al. 2018) e MCUNet (Lin et al. 2020) forniscono un runtime snello per l’esecuzione di modelli di reti neurali su microcontrollori e altri dispositivi con risorse limitate. Tuttavia, non supportano l’addestramento on-device. L’addestramento richiede un proprio set di strumenti specializzati a causa dell’impatto della quantizzazione sul calcolo del gradiente e dell’ingombro di memoria della backpropagation (Lin et al. 2022).\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. «Tensorflow lite micro: Embedded machine learning for tinyml systems». Proceedings of Machine Learning and Systems 3: 800–811.\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. «TVM: An automated End-to-End optimizing compiler for deep learning». In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 578–94.\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, e Song Han. 2020. «MCUNet: Tiny Deep Learning on IoT Devices». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, e Song Han. 2022. «On-device training under 256kb memory». Adv. Neur. In. 35: 22941–54.\nNegli ultimi anni, hanno iniziato a emergere una manciata di strumenti e framework che consentono l’addestramento sul dispositivo. Tra questi Tiny Training Engine (Lin et al. 2022), TinyTL (Cai et al. 2020) e TinyTrain (Kwon et al. 2023).\n\n12.7.1 Tiny Training Engine\nTiny Training Engine (TTE) utilizza diverse tecniche per ottimizzare l’utilizzo della memoria e velocizzare il processo di training. Una panoramica del flusso di lavoro TTE è mostrata in Figura 12.9. Innanzitutto, TTE scarica la differenziazione automatica in fase di compilazione anziché in fase di runtime, riducendo significativamente il sovraccarico durante il training. In secondo luogo, TTE esegue l’ottimizzazione del grafo come la potatura e gli aggiornamenti sparsi per ridurre i requisiti di memoria e accelerare i calcoli.\n\n\n\n\n\n\nFigura 12.9: Flusso di lavoro di TTE.\n\n\n\nIn particolare, TTE segue quattro passaggi principali:\n\nDurante la fase di compilazione, TTE traccia il grafo di propagazione “forward” e deriva il grafo “backward” corrispondente per la backpropagation. Ciò consente alla differenziazione di avvenire in fase di compilazione anziché in fase di esecuzione.\nTTE elimina tutti i nodi che rappresentano pesi congelati dal grafo backward. I pesi congelati sono pesi che non vengono aggiornati durante l’addestramento per ridurre l’impatto di determinati neuroni. La potatura dei loro nodi consente di risparmiare memoria.\nTTE riordina gli operatori di discesa del gradiente per intercalarli con i calcoli del passaggio del backward. Questa pianificazione riduce al minimo le “impronte” [occupazione] di memoria.\nTTE utilizza la generazione di codice per compilare i grafi “forward” e “backward” ottimizzati, che vengono poi distribuiti per l’addestramento on-device.\n\n\n\n12.7.2 Tiny Transfer Learning\nTiny Transfer Learning (TinyTL) consente un training efficiente in termini di memoria sul dispositivo tramite una tecnica chiamata congelamento dei pesi. Durante il training, gran parte del collo di bottiglia della memoria deriva dall’archiviazione delle attivazioni intermedie e dall’aggiornamento dei pesi nella rete neurale.\nPer ridurre questo sovraccarico di memoria, TinyTL congela la maggior parte dei pesi in modo che non debbano essere aggiornati durante il training. Ciò elimina la necessità di archiviare le attivazioni intermedie per le parti congelate della rete. TinyTL ottimizza solo i termini di bias, che sono molto più piccoli dei pesi. Una panoramica del flusso di lavoro TinyTL è mostrata in Figura 12.10.\n\n\n\n\n\n\nFigura 12.10: Flusso di lavoro di TinyTL. Fonte: Cai et al. (2020).)\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, e Song Han. 2020. «TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning». In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, a cura di Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, e Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nI pesi di congelamento si applicano a layer completamente connessi, nonché a layer di normalizzazione e convoluzionali. Tuttavia, solo l’adattamento dei bias limita la capacità del modello di apprendere e adattarsi a nuovi dati.\nPer aumentare l’adattabilità senza molta memoria aggiuntiva, TinyTL utilizza un piccolo modello di apprendimento residuo. Questo affina le mappe delle feature intermedie per produrre output migliori, anche con pesi fissi. Il modello residuo introduce un overhead minimo, inferiore al 3,8% in più rispetto al modello di base.\nCongelando la maggior parte dei pesi, TinyTL riduce significativamente l’utilizzo della memoria durante l’addestramento on-device. Il modello residuo consente quindi di adattarsi e apprendere in modo efficace per l’attività. L’approccio combinato fornisce un addestramento on-device efficiente in termini di memoria con un impatto minimo sulla precisione del modello.\n\n\n12.7.3 Tiny Train\nTinyTrain riduce significativamente il tempo necessario per l’addestramento sul dispositivo aggiornando selettivamente solo determinate parti del modello. Ciò avviene utilizzando una tecnica chiamata aggiornamento sparso adattivo all’attività, come mostrato in Figura 12.11.\nIn base ai dati utente, alla memoria e al calcolo disponibili sul dispositivo, TinyTrain sceglie dinamicamente quali layer della rete neurale aggiornare durante l’addestramento. Questa selezione di layer è ottimizzata per ridurre l’utilizzo di calcolo e memoria mantenendo un’elevata accuratezza.\n\n\n\n\n\n\nFigura 12.11: Flusso di lavoro di TinyTrain. Fonte: Kwon et al. (2023).\n\n\nKwon, Young D, Rui Li, Stylianos I Venieris, Jagmohan Chauhan, Nicholas D Lane, e Cecilia Mascolo. 2023. «TinyTrain: Deep Neural Network Training at the Extreme Edge». ArXiv preprint abs/2307.09988. https://arxiv.org/abs/2307.09988.\n\n\nPiù specificamente, TinyTrain esegue prima il pre-addestramento offline del modello. Durante il pre-addestramento, non solo addestra il modello sui dati dell’attività, ma anche il meta-addestramento del modello. Meta-addestramento significa addestrare il modello sui metadati relativi al processo di addestramento stesso. Questo meta-addestramento migliora la capacità del modello di adattarsi in modo accurato anche quando sono disponibili dati limitati per l’attività target.\nPoi, durante la fase di adattamento online, quando il modello viene personalizzato sul dispositivo, TinyTrain esegue aggiornamenti adattivi sparsi all’attività. Utilizzando i criteri relativi alle capacità del dispositivo, seleziona solo determinati layer da aggiornare tramite backpropagation. I layer vengono scelti per bilanciare accuratezza, utilizzo della memoria e tempo di elaborazione.\nAggiornando in modo sparso i layer su misura per il dispositivo e l’attività, TinyTrain riduce significativamente il tempo di addestramento sul dispositivo e l’utilizzo delle risorse. Il meta-training offline migliora anche l’accuratezza quando si adatta a dati limitati. Insieme, questi metodi consentono un training on-device rapido, efficiente e accurato.\n\n\n12.7.4 Confronto\nTabella 12.3 riassume le principali somiglianze e differenze tra i diversi framework.\n\n\n\nTabella 12.3: Confronto di framework per l’ottimizzazione del training on-device.\n\n\n\n\n\n\n\n\n\n\nFramework\nSomiglianze\nDifferenze\n\n\n\n\nTiny Training Engine\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta potatura, sparsità, ecc.\n\n\nTraccia grafi forward & backward\nElimina i pesi congelati\nInterlaccia backprop e gradienti\nGenerazione di codice\n\n\n\nTinyTL\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta congelamento, sparsità, ecc.\n\n\nCongela la maggior parte dei pesi\nAdatta solo i bias\nUtilizza il modello residuo\n\n\n\nTinyTrain\n\nAddestramento sul dispositivo\nOttimizza memoria e calcolo\nSfrutta sparsità, ecc.\n\n\nMeta-addestramento nel pre-addestramento\nAggiornamento sparse adattivo alle attività\nAggiornamento selettivo dei layer",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#conclusione",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#conclusione",
    "title": "12  Apprendimento On-Device",
    "section": "12.8 Conclusione",
    "text": "12.8 Conclusione\nIl concetto di apprendimento on-device [su dispositivo] è sempre più importante per aumentare l’usabilità e la scalabilità di TinyML. Questo capitolo ha esplorato le complessità dell’apprendimento on-device, esplorandone vantaggi e limiti, strategie di adattamento, algoritmi e tecniche chiave correlate, implicazioni di sicurezza e framework di training on-device esistenti ed emergenti.\nL’apprendimento su dispositivo è, senza dubbio, un paradigma rivoluzionario che porta con sé numerosi vantaggi per le distribuzioni ML embedded ed edge. Eseguendo il training direttamente sui dispositivi endpoint, si elimina la necessità di una connettività cloud continua, rendendolo particolarmente adatto per applicazioni IoT ed edge computing. Presenta vantaggi quali maggiore privacy, facilità di conformità ed efficienza delle risorse. Allo stesso tempo, l’apprendimento su on-device deve affrontare limitazioni legate a vincoli hardware, dimensioni dei dati limitate e ridotta accuratezza e generalizzazione del modello.\nMeccanismi quali la ridotta complessità del modello, tecniche di ottimizzazione e compressione dei dati e metodi di apprendimento correlati quali apprendimento tramite trasferimento e apprendimento federato consentono ai modelli di adattarsi per apprendere ed evolversi in base a vincoli di risorse, fungendo così da fondamento per un efficace ML sui dispositivi edge.\nLe problematiche critiche di sicurezza nell’apprendimento su dispositivo evidenziate in questo capitolo, che vanno dall’avvelenamento dei dati e dagli attacchi avversari ai rischi specifici introdotti dall’apprendimento on-device, devono essere affrontate in carichi di lavoro reali affinché l’apprendimento su dispositivo sia un paradigma praticabile. Strategie di mitigazione efficaci, quali convalida dei dati, crittografia, privacy differenziale, rilevamento delle anomalie e convalida dei dati di input, sono fondamentali per salvaguardare i sistemi di apprendimento on-device da queste minacce.\nL’emergere di framework di formazione specializzati on-device, come Tiny Training Engine, Tiny Transfer Learning e Tiny Train, offre strumenti pratici che consentono una formazione efficiente sui dispositivi. Questi framework impiegano varie tecniche per ottimizzare l’utilizzo della memoria, ridurre il sovraccarico computazionale e semplificare il processo di training on-device.\nIn conclusione, l’apprendimento on-device è in prima linea in TinyML, promettendo un futuro in cui i modelli possono acquisire autonomamente conoscenze e adattarsi ad ambienti mutevoli su dispositivi edge. L’applicazione dell’apprendimento on-device ha il potenziale per rivoluzionare vari ambiti, tra cui sanità, IoT industriale e città intelligenti. Tuttavia, il potenziale trasformativo dell’apprendimento on-device deve essere bilanciato con misure di sicurezza robuste per proteggere da violazioni dei dati e minacce avversarie. L’adozione di framework di training on-device innovativi e l’implementazione di protocolli di sicurezza rigorosi sono passaggi chiave per sbloccare il pieno potenziale dell’apprendimento su dispositivo. Man mano che questa tecnologia continua a evolversi, promette di rendere i nostri dispositivi più intelligenti, più reattivi e meglio integrati nella nostra vita quotidiana.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ondevice_learning/ondevice_learning.it.html#sec-on-device-learning-resource",
    "href": "contents/ondevice_learning/ondevice_learning.it.html#sec-on-device-learning-resource",
    "title": "12  Apprendimento On-Device",
    "section": "12.9 Risorse",
    "text": "12.9 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nIntro to TensorFlow Lite (TFLite).\nTFLite Optimization and Quantization.\nTFLite Quantization-Aware Training.\nTrasferimento dell’Apprendimento:\n\nTransfer Learning: with Visual Wake Words example.\nOn-device Training and Transfer Learning.\n\nAddestramento Distribuito:\n\nDistributed Training.\nDistributed Training.\n\nMonitoraggio Continuo:\n\nContinuous Evaluation Challenges for TinyML.\nFederated Learning Challenges.\nContinuous Monitoring with Federated ML.\nContinuous Monitoring Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 12.1\nVideo 12.2\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 12.1\nEsercizio 12.2\nEsercizio 12.3\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Apprendimento On-Device</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html",
    "href": "contents/ops/ops.it.html",
    "title": "13  Operazioni di ML",
    "section": "",
    "text": "13.1 Introduzione\nMachine Learning Operations (MLOps) è un approccio sistematico che combina machine learning (ML), data science e ingegneria del software per automatizzare il ciclo di vita end-to-end di ML. Ciò include tutto, dalla preparazione dei dati e dal training del modello alla distribuzione e alla manutenzione. MLOps garantisce che i modelli ML siano sviluppati, distribuiti e mantenuti in modo efficiente ed efficace.\nCominciamo prendendo un caso di esempio generale (ad esempio, ML non edge). Prendiamo in considerazione un’azienda di “ride sharing” che desidera distribuire un modello di machine learning per prevedere la domanda dei passeggeri in tempo reale. Il team di data science impiega mesi per sviluppare un modello, ma quando è il momento di distribuirlo, si rende conto che deve essere compatibile con l’ambiente di produzione del team di ingegneria. La distribuzione del modello richiede la ricostruzione da zero, il che comporta settimane di lavoro aggiuntivo. È qui che entra in gioco MLOps.\nCon MLOps, protocolli e strumenti, il modello sviluppato dal team di data science può essere distribuito e integrato senza problemi nell’ambiente di produzione. In sostanza, MLOps elimina gli attriti durante lo sviluppo, la distribuzione e la manutenzione dei sistemi ML. Migliora la collaborazione tra i team tramite flussi di lavoro e interfacce definiti. MLOps accelera anche la velocità di iterazione consentendo la distribuzione continua per i modelli ML.\nPer l’azienda di ride sharing, implementare MLOps significa che il loro modello di previsione della domanda può essere frequentemente riqualificato e distribuito in base ai nuovi dati in arrivo. Ciò mantiene il modello accurato nonostante il cambiamento del comportamento del passeggero. MLOps consente inoltre all’azienda di sperimentare nuove tecniche di modellazione poiché i modelli possono essere rapidamente testati e aggiornati.\nAltri vantaggi di MLOps includono il monitoraggio avanzato della discendenza del modello, la riproducibilità e l’auditing. La catalogazione dei flussi di lavoro ML e la standardizzazione degli artefatti, come il logging delle versioni del modello, il monitoraggio della discendenza dei dati e il confezionamento di modelli e parametri, consente una visione più approfondita della provenienza del modello. La standardizzazione di questi artefatti facilita la tracciabilità di un modello fino alle sue origini, la replica del processo di sviluppo del modello e l’esame di come una versione del modello è cambiata nel tempo. Ciò facilita anche la conformità alle normative, che è particolarmente critica in settori regolamentati come sanità e finanza, dove è importante essere in grado di verificare e spiegare i modelli.\nLe principali organizzazioni adottano MLOps per aumentare la produttività, aumentare la collaborazione e accelerare i risultati ML. Fornisce i framework, gli strumenti e le best practice per gestire efficacemente i sistemi ML durante il loro ciclo di vita. Ciò si traduce in modelli più performanti, tempi di realizzazione più rapidi e un vantaggio competitivo duraturo. Mentre esploriamo ulteriormente MLOps, si consideri come l’implementazione di queste pratiche può aiutare ad affrontare le sfide ML embedded oggi e in futuro.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#contesto-storico",
    "href": "contents/ops/ops.it.html#contesto-storico",
    "title": "13  Operazioni di ML",
    "section": "13.2 Contesto Storico",
    "text": "13.2 Contesto Storico\nMLOps affonda le sue radici in DevOps, un insieme di pratiche che combinano sviluppo software (Dev) e operazioni IT (Ops) per accorciare il ciclo di vita dello sviluppo e fornire una distribuzione “continua” di software di alta qualità. I parallelismi tra MLOps e DevOps sono evidenti nella loro attenzione all’automazione, alla collaborazione e al miglioramento continuo. In entrambi i casi, l’obiettivo è quello di abbattere i “silos” tra i diversi team (sviluppatori, operazioni e, nel caso di MLOps, data scientist e ingegneri ML) e creare un processo più snello ed efficiente. È utile comprendere meglio la storia di questa evoluzione per comprendere MLOps nel contesto dei sistemi tradizionali.\n\n13.2.1 DevOps\nIl termine “DevOps” è stato coniato per la prima volta nel 2009 da Patrick Debois, un consulente e professionista Agile. Debois ha organizzato la prima conferenza DevOpsDays a Ghent, in Belgio, nel 2009. La conferenza ha riunito professionisti dello sviluppo e delle operazioni per discutere di modi per migliorare la collaborazione e automatizzare i processi.\nDevOps ha le sue radici nel movimento Agile, iniziato nei primi anni 2000. Agile ha fornito le basi per un approccio più collaborativo allo sviluppo software e ha enfatizzato le piccole release iterative. Tuttavia, Agile si concentra principalmente sulla collaborazione tra team di sviluppo. Man mano che le metodologie Agile diventavano più popolari, le organizzazioni si sono rese conto della necessità di estendere questa collaborazione ai team operativi.\nLa natura isolata dei team di sviluppo e delle operazioni ha spesso portato a inefficienze, conflitti e ritardi nella distribuzione del software. Questa necessità di una migliore collaborazione e integrazione tra questi team ha portato al movimento DevOps. DevOps può essere visto come un’estensione dei principi Agile, inclusi i team operativi.\nI principi chiave di DevOps includono collaborazione, automazione, integrazione continua, distribuzione e feedback. DevOps si concentra sull’automazione dell’intera pipeline di distribuzione del software, dallo sviluppo alla distribuzione. Mira a migliorare la collaborazione tra i team di sviluppo e operativi, utilizzando strumenti come Jenkins, Docker e Kubernetes per semplificare il ciclo di vita dello sviluppo.\nMentre Agile e DevOps condividono principi comuni in materia di collaborazione e feedback, DevOps mira specificamente all’integrazione di sviluppo e operazioni IT, espandendo Agile oltre i soli team di sviluppo. Introduce pratiche e strumenti per automatizzare la distribuzione del software e migliorare la velocità e la qualità delle release del software.\n\n\n13.2.2 MLOps\nMLOps, d’altro canto, sta per Machine Learning Operations ed estende i principi di DevOps al ciclo di vita ML. MLOps mira ad automatizzare e semplificare il ciclo di vita ML end-to-end, dalla preparazione dei dati e sviluppo del modello alla distribuzione e al monitoraggio. L’obiettivo principale di MLOps è facilitare la collaborazione tra data scientist, data engineer e operazioni IT e automatizzare la distribuzione, il monitoraggio e la gestione dei modelli ML. Alcuni fattori chiave hanno portato all’ascesa di MLOps.\n\nData drift: La deriva dei dati degrada le prestazioni del modello nel tempo, motivando la necessità di rigorosi monitoraggi e procedure di riqualificazione automatizzate fornite da MLOps.\nRiproducibilità: La mancanza di riproducibilità negli esperimenti di machine learning ha motivato i sistemi MLOps a tracciare codice, dati e variabili di ambiente per abilitare flussi di lavoro ML riproducibili.\nSpiegabilità: La natura di “scatola nera” e la mancanza di spiegabilità di modelli complessi hanno motivato la necessità di funzionalità MLOps per aumentare la trasparenza e la spiegabilità del modello.\nMonitoraggio: L’incapacità di monitorare in modo affidabile le prestazioni del modello dopo la distribuzione ha evidenziato la necessità di soluzioni MLOps con una solida strumentazione delle prestazioni del modello e avvisi.\nAttrito: L’attrito nel riaddestramento e nella distribuzione manuale dei modelli ha motivato la necessità di sistemi MLOps che automatizzano le pipeline di distribuzione dell’apprendimento automatico.\nOttimizzazione: La complessità della configurazione dell’infrastruttura di apprendimento automatico ha motivato la necessità di piattaforme MLOps con un’infrastruttura ML ottimizzata e pronta all’uso.\n\nSebbene DevOps e MLOps condividano l’obiettivo comune di automatizzare e semplificare i processi, differiscono significativamente in termini di attenzione e sfide. DevOps si occupa principalmente di sviluppo software e operazioni IT. Consente la collaborazione tra questi team e automatizza la distribuzione del software. Al contrario, MLOps si concentra sul ciclo di vita dell’apprendimento automatico. Affronta complessità aggiuntive come versioning dei dati, versioning dei modelli e monitoraggio dei modelli. MLOps richiede la collaborazione tra una gamma più ampia di stakeholder, tra cui data scientist, data engineer e IT operations. Va oltre l’ambito del DevOps tradizionale incorporando le sfide uniche della gestione dei modelli ML durante il loro ciclo di vita. Tabella 13.1 fornisce un confronto affiancato di DevOps e MLOps, evidenziandone le principali differenze e somiglianze.\n\n\n\nTabella 13.1: Confronto tra DevOps e MLOps.\n\n\n\n\n\n\n\n\n\n\nAspect\nDevOps\nMLOps\n\n\n\n\nObiettivo\nSemplificazione dei processi di sviluppo software e operativi\nOttimizzazione del ciclo di vita dei modelli di apprendimento automatico\n\n\nMetodologia\nIntegrazione continua e distribuzione continua (CI/CD) per lo sviluppo software\nSimile a CI/CD ma incentrato sui flussi di lavoro di apprendimento automatico\n\n\nStrumenti Principali\nControllo delle versioni (Git), strumenti CI/CD (Jenkins, Travis CI), gestione della configurazione (Ansible, Puppet)\nStrumenti di versioning dei dati, strumenti di training e deployment dei modelli, pipeline CI/CD su misura per ML\n\n\nProblemi Principali\nIntegrazione del codice, test, gestione delle release, automazione, infrastruttura come codice\nGestione dei dati, versioning dei modelli, monitoraggio degli esperimenti, deployment dei modelli, scalabilità dei flussi di lavoro ML\n\n\nRisultati Tipici\nRelease software più rapide e affidabili, collaborazione migliorata tra team di sviluppo e operativi\nGestione e deployment efficienti dei modelli di apprendimento automatico, collaborazione migliorata tra data scientist e ingegneri\n\n\n\n\n\n\nScoprire di più sui cicli di vita ML tramite un “case study” che presenta il riconoscimento vocale in Video 13.1.\n\n\n\n\n\n\nVideo 13.1: MLOps",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#componenti-chiave-di-mlops",
    "href": "contents/ops/ops.it.html#componenti-chiave-di-mlops",
    "title": "13  Operazioni di ML",
    "section": "13.3 Componenti Chiave di MLOps",
    "text": "13.3 Componenti Chiave di MLOps\nIn questo capitolo, forniremo una panoramica dei componenti principali di MLOps, un set emergente di pratiche che consente una distribuzione solida e una gestione del ciclo di vita dei modelli ML in produzione. Sebbene alcuni elementi MLOps, come l’automazione e il monitoraggio, siano stati trattati nei capitoli precedenti, li integreremo in un framework e approfondiremo funzionalità aggiuntive, come la governance. Inoltre, descriveremo e collegheremo gli strumenti più diffusi utilizzati in ogni componente, come LabelStudio per l’etichettatura dei dati. Alla fine, speriamo che abbiate compreso la metodologia MLOps end-to-end che porta i modelli dall’ideazione alla creazione di valore sostenibile all’interno delle organizzazioni.\n\n13.3.1 Gestione dei Dati\nUna gestione dei dati solida e l’ingegneria dei dati potenziano attivamente implementazioni MLOps di successo. I team acquisiscono, archiviano e preparano correttamente i dati grezzi da sensori, database, app e altri sistemi per la formazione e la distribuzione dei modelli.\nI team monitorano attivamente le modifiche ai set di dati nel tempo utilizzando il controllo delle versioni con Git e strumenti come GitHub o GitLab. Gli scienziati dei dati collaborano alla cura dei set di dati unendo le modifiche di più collaboratori. I team possono rivedere o ripristinare ogni iterazione di un set di dati, se necessario.\nI team etichettano e annotano meticolosamente i dati utilizzando un software di etichettatura come LabelStudio, che consente ai team distribuiti di lavorare insieme all’etichettatura dei set di dati. Man mano che le variabili target e le convenzioni di etichettatura si evolvono, i team mantengono l’accessibilità alle versioni precedenti.\nI team archiviano il set di dati non elaborato e tutte le risorse derivate su servizi cloud come Amazon S3 o Google Cloud Storage. Questi servizi forniscono un’archiviazione scalabile e resiliente con funzionalità di controllo delle versioni. I team possono impostare autorizzazioni di accesso granulari.\nLe pipeline di dati robuste create dai team automatizzano l’estrazione, l’unione, la pulizia e la trasformazione dei dati grezzi in set di dati pronti per l’analisi. Prefect, Apache Airflow e dbt sono orchestratori di flussi di lavoro che consentono agli ingegneri di sviluppare pipeline di elaborazione dati flessibili e riutilizzabili.\nAd esempio, una pipeline può acquisire dati da database PostgreSQL, API REST e CSV archiviati su S3 [Simple Storage Service]. Può filtrare, deduplicare e aggregare i dati, gestire gli errori e salvare l’output su S3. La pipeline può anche spingere i dati trasformati in un feature store come Tecton o Feast per un accesso a bassa latenza.\nIn un caso d’uso di manutenzione predittiva industriale, i dati dei sensori vengono acquisiti dai dispositivi in S3. Una pipeline perfetta elabora i dati dei sensori, unendoli ai record di manutenzione. Il set di dati arricchito è archiviato in Feast in modo che i modelli possano recuperare facilmente i dati più recenti per l’addestramento e le previsioni.\nVideo 13.2 di seguito riporta una breve panoramica delle pipeline di dati.\n\n\n\n\n\n\nVideo 13.2: Pipeline di Dati\n\n\n\n\n\n\n\n\n13.3.2 Pipeline CI/CD\nLe pipeline di integrazione continua e distribuzione continua (CI/CD) automatizzano attivamente la progressione dei modelli ML dallo sviluppo iniziale alla distribuzione in produzione. Adattati per i sistemi ML, i principi CI/CD consentono ai team di distribuire rapidamente e in modo robusto nuovi modelli con errori manuali ridotti al minimo.\nLe pipeline CI/CD orchestrano i passaggi chiave, tra cui il controllo delle nuove modifiche al codice, la trasformazione dei dati, la formazione e la registrazione di nuovi modelli, i test di convalida, la containerizzazione, la distribuzione in ambienti come cluster di staging e la promozione in produzione. I team sfruttano le soluzioni CI/CD più diffuse come Jenkins, CircleCI e GitHub Actions per eseguire queste pipeline MLOps, mentre Prefect, Metaflow e Kubeflow offrono opzioni incentrate su ML.\nFigura 13.1 illustra una pipeline CI/CD specificamente pensata per MLOps. Il processo inizia con un dataset e un repository di feature (a sinistra), che alimenta una fase di ingestione del dataset. Dopo l’ingestione, i dati vengono sottoposti a convalida per garantirne la qualità prima di essere trasformati per l’addestramento. Parallelamente, un trigger di riaddestramento può avviare la pipeline in base a criteri specificati. I dati passano poi attraverso una fase di addestramento/ottimizzazione del modello all’interno di un motore di elaborazione dati, seguita dalla valutazione e convalida del modello. Una volta convalidato, il modello viene registrato e archiviato in un repository di metadati e artefatti di apprendimento automatico. La fase finale prevede la distribuzione del modello addestrato nuovamente nel dataset e nel repository di feature, creando così un processo ciclico per il miglioramento continuo e la distribuzione di modelli di apprendimento automatico.\n\n\n\n\n\n\nFigura 13.1: Diagramma CI/CD MLOps. Fonte: HarvardX.\n\n\n\nAd esempio, quando uno scienziato dei dati verifica i miglioramenti a un modello di classificazione delle immagini in un repository GitHub, questo attiva attivamente una pipeline CI/CD Jenkins. La pipeline riesegue le trasformazioni dei dati e l’addestramento del modello sui dati più recenti, monitorando gli esperimenti con MLflow. Dopo i test di validazione automatizzati, i team distribuiscono il contenitore del modello in un cluster di staging Kubernetes per un ulteriore controllo qualità. Una volta approvato, Jenkins facilita un rollout graduale del modello in produzione con distribuzioni canary per rilevare eventuali problemi. Se vengono rilevate anomalie, la pipeline consente ai team di tornare alla versione precedente del modello in modo fluido.\nLe pipeline CI/CD consentono ai team di iterare e distribuire rapidamente modelli ML collegando i diversi passaggi dallo sviluppo alla distribuzione con automazione continua. L’integrazione di strumenti MLOps come MLflow migliora il packaging del modello, il controllo delle versioni e la tracciabilità della pipeline. CI/CD è fondamentale per far progredire i modelli oltre i prototipi in sistemi aziendali sostenibili.\n\n\n13.3.3 Addestramento del Modello\nNella fase di training del modello, gli scienziati dei dati sperimentano attivamente diverse architetture e algoritmi ML per creare modelli ottimizzati che estraggono informazioni e modelli dai dati. MLOps introduce best practice e automazione per rendere questo processo iterativo più efficiente e riproducibile.\nI moderni framework ML come TensorFlow, PyTorch e Keras forniscono componenti predefiniti che semplificano la progettazione di reti neurali e altre architetture di modelli. Gli scienziati dei dati sfruttano moduli integrati per layer, attivazioni, perdite, ecc. e API di alto livello come Keras per concentrarsi maggiormente sull’architettura del modello.\nMLOps consente ai team di impacchettare il codice di training del modello in script e notebook riutilizzabili e tracciati. Man mano che i modelli vengono sviluppati, funzionalità come ottimizzazione degli iperparametri, ricerca dell’architettura neurale e selezione automatica delle funzionalità si ripetono rapidamente per trovare le configurazioni più performanti.\nI team utilizzano Git per controllare le versioni del codice di training e ospitarlo in repository come GitHub per tenere traccia delle modifiche nel tempo. Ciò consente una collaborazione fluida tra i data scientist.\nNotebook come Jupyter creano un eccellente ambiente di sviluppo di modelli interattivi. I notebook contengono l’inserimento dei dati, la pre-elaborazione, la dichiarazione del modello, il ciclo di training, la valutazione e il codice di esportazione in un documento riproducibile.\nInfine, i team orchestrano il training del modello come parte di una pipeline CI/CD per l’automazione. Ad esempio, una pipeline Jenkins può attivare uno script Python per caricare nuovi dati di training, riaddestrare un classificatore TensorFlow, valutare le metriche del modello e registrare automaticamente il modello se vengono raggiunte le soglie di prestazione.\nUn esempio di flusso di lavoro prevede che uno scienziato dei dati utilizzi un notebook PyTorch per sviluppare un modello CNN per la classificazione delle immagini. La libreria fastai fornisce API di alto livello per semplificare l’addestramento delle CNN sui set di dati delle immagini. Il notebook addestra il modello sui dati campione, valuta le metriche di accuratezza e ottimizza gli iperparametri come la velocità di apprendimento e i layer per ottimizzare le prestazioni. Questo notebook riproducibile è controllato dalla versione e integrato in una pipeline di riaddestramento.\nL’automazione e la standardizzazione dell’addestramento del modello consentono ai team di accelerare la sperimentazione e raggiungere il rigore necessario per produrre sistemi ML.\n\n\n13.3.4 Valutazione del Modello\nPrima di distribuire i modelli, i team eseguono una valutazione e dei test rigorosi per convalidare i benchmark delle prestazioni e la prontezza per il rilascio. MLOps introduce le best practice relative alla convalida, all’audit e ai test canary dei modelli.\nIn genere, i team valutano i modelli rispetto ai dataset di test di holdout che non vengono utilizzati durante la formazione. I dati di test provengono dalla stessa distribuzione dei dati di produzione. I team calcolano metriche come accuratezza, AUC, precisione, richiamo e punteggio F1.\nI team monitorano inoltre le stesse metriche nel tempo rispetto ai campioni di dati di test. Se i dati di valutazione provengono da flussi di produzione live, questo rileva le derive dei dati che degradano le prestazioni del modello nel tempo.\nLa supervisione umana per il rilascio del modello rimane importante. Gli scienziati dei dati esaminano le prestazioni nei segmenti e nelle sezioni chiave. L’analisi degli errori aiuta a identificare i punti deboli del modello per guidare il miglioramento. team applicano tecniche di equità e rilevamento di bias.\nIl test Canary rilascia un modello a un piccolo sottoinsieme di utenti per valutare le prestazioni nel mondo reale prima di un’ampia distribuzione. I team indirizzano gradualmente il traffico alla versione canary monitorando i problemi.\nAd esempio, un rivenditore valuta un modello di raccomandazione di prodotto personalizzato rispetto ai dati di test storici, esaminando le metriche di accuratezza e diversità. I team calcolano anche le metriche sui dati dei clienti in tempo reale nel tempo, rilevando una riduzione dell’accuratezza nelle ultime 2 settimane. Prima dell’implementazione completa, il nuovo modello viene rilasciato al 5% del traffico web per garantire che non vi sia alcun degrado.\nL’automazione della valutazione e delle versioni canary riduce i rischi di distribuzione. Tuttavia, la revisione umana deve ancora essere più critica per valutare le dinamiche meno quantificabili del comportamento del modello. Una rigorosa convalida pre-distribuzione fornisce sicurezza nell’immissione dei modelli in produzione.\n\n\n13.3.5 Distribuzione del Modello\nI team devono confezionare, testare e tracciare correttamente i modelli ML per distribuirli in modo affidabile in produzione. MLOps introduce framework e procedure per il versioning attivo, la distribuzione, il monitoraggio e l’aggiornamento dei modelli in modi sostenibili.\nI team containerizzano i modelli utilizzando Docker, che raggruppa codice, librerie e dipendenze in un’unità standardizzata. I container consentono una portabilità fluida tra gli ambienti.\nFramework come TensorFlow Serving e BentoML aiutano a servire le previsioni dai modelli distribuiti tramite API ottimizzate per le prestazioni. Questi framework gestiscono il versioning, il ridimensionamento e il monitoraggio.\nI team distribuiscono prima i modelli aggiornati in ambienti di staging o QA per i test prima del rollout completo in produzione. Le distribuzioni shadow o canary instradano un campione di traffico per testare le varianti del modello. I team aumentano gradualmente l’accesso ai nuovi modelli.\nI team creano solide procedure di rollback nel caso in cui emergano problemi. I rollback ripristinano l’ultima versione valida del modello. L’integrazione con pipeline CI/CD semplifica la ridistribuzione, se necessario.\nI team monitorano attentamente gli artefatti del modello, come script, pesi, log e metriche, per ogni versione con strumenti di metadati ML come MLflow. Ciò mantiene la discendenza e la verificabilità.\nAd esempio, un rivenditore containerizza un modello di raccomandazione di prodotto in TensorFlow Serving e lo distribuisce in un cluster di staging Kubernetes. Dopo aver monitorato e approvato le prestazioni sul traffico di esempio, Kubernetes sposta il 10% del traffico di produzione al nuovo modello. Se non vengono rilevati problemi dopo alcuni giorni, il nuovo modello occupa il 100% del traffico. Tuttavia, i team dovrebbero mantenere la versione precedente accessibile per il rollback, se necessario.\nI processi di distribuzione del modello consentono ai team di rendere i sistemi ML resilienti in produzione tenendo conto di tutti gli stati di transizione.\n\n\n13.3.6 Gestione dell’Infrastruttura\nI team MLOps sfruttano ampiamente gli strumenti “infrastructure as code (IaC)” e le solide architetture cloud per gestire attivamente le risorse necessarie per lo sviluppo, la formazione e la distribuzione dei sistemi ML.\nI team utilizzano strumenti IaC come Terraform, CloudFormation e Ansible per definire, fornire e aggiornare a livello di programmazione l’infrastruttura in modo controllato dalla versione. Per MLOps, i team utilizzano ampiamente Terraform per avviare risorse su AWS, GCP e Azure.\nPer la creazione e la formazione dei modelli, i team forniscono dinamicamente risorse di elaborazione come server GPU, cluster di container, storage e database tramite Terraform in base alle esigenze degli scienziati dei dati. Il codice incapsula e preserva le definizioni dell’infrastruttura.\nI container e gli orchestratori come Docker e Kubernetes consentono ai team di impacchettare modelli e distribuirli in modo affidabile in diversi ambienti. I contenitori possono essere attivati o disattivati automaticamente in base alla domanda.\nSfruttando l’elasticità del cloud, i team aumentano o diminuiscono le risorse per soddisfare i picchi nei carichi di lavoro come i lavori di ottimizzazione degli iperparametri o i picchi nelle richieste di previsione. Auto-scaling consente un’efficienza dei costi ottimizzata.\nL’infrastruttura si estende su dispositivi on-prem, cloud ed edge. Uno stack tecnologico robusto offre flessibilità e resilienza. Gli strumenti di monitoraggio consentono ai team di osservare l’utilizzo delle risorse.\nAd esempio, una configurazione Terraform può distribuire un cluster GCP Kubernetes per ospitare modelli TensorFlow addestrati esposti come microservizi di previsione. Il cluster aumenta i pod per gestire un traffico maggiore. L’integrazione CI/CD distribuisce senza problemi nuovi contenitori di modelli.\nLa gestione attenta dell’infrastruttura tramite IaC e monitoraggio consente ai team di prevenire i colli di bottiglia nell’operatività dei sistemi ML su larga scala.\n\n\n13.3.7 Monitoraggio\nI team MLOps mantengono attivamente un monitoraggio robusto per mantenere la visibilità nei modelli ML distribuiti in produzione. Il monitoraggio continuo fornisce informazioni sulle prestazioni del modello e del sistema in modo che i team possano rilevare e risolvere rapidamente i problemi per ridurre al minimo le interruzioni.\nI team monitorano attivamente gli aspetti chiave del modello, inclusa l’analisi di campioni di previsioni live per tracciare metriche come accuratezza e matrice di confusione nel tempo.\nQuando monitorano le prestazioni, i team devono profilare i dati in arrivo per verificare la deriva del modello, un calo costante dell’accuratezza del modello dopo l’implementazione in produzione. La deriva del modello può verificarsi in due modi: deriva del concetto e deriva dei dati. La deriva del concetto si riferisce a un cambiamento fondamentale osservato nella relazione tra i dati di input e quelli target. Ad esempio, con l’avanzare della pandemia di COVID-19, i siti di e-commerce e vendita al dettaglio hanno dovuto correggere le raccomandazioni del modello poiché i dati di acquisto erano ampiamente distorti verso articoli come il disinfettante per le mani. La deriva dei dati descrive i cambiamenti nella distribuzione dei dati nel tempo. Ad esempio, gli algoritmi di riconoscimento delle immagini utilizzati nelle auto a guida autonoma devono tenere conto della stagionalità nell’osservazione dell’ambiente circostante. I team monitorano anche le metriche delle prestazioni delle applicazioni come latenza ed errori per le integrazioni dei modelli.\nDa una prospettiva infrastrutturale, i team monitorano i problemi di capacità come elevato utilizzo di CPU, memoria e disco e interruzioni del sistema. Strumenti come Prometheus, Grafana ed Elastic consentono ai team di raccogliere, analizzare, interrogare e visualizzare attivamente diverse metriche di monitoraggio. Le dashboard rendono le dinamiche altamente visibili.\nI team configurano gli allarmi per le metriche di monitoraggio chiave come cali di accuratezza e guasti del sistema per consentire una risposta proattiva agli eventi che minacciano l’affidabilità. Ad esempio, i cali di accuratezza del modello attivano avvisi per i team per esaminare potenziali deviazioni dei dati e riaddestrare i modelli utilizzando campioni di dati aggiornati e rappresentativi.\nDopo la distribuzione, il monitoraggio completo consente ai team di mantenere la fiducia nello stato del modello e del sistema. Consente ai team di rilevare e risolvere preventivamente le deviazioni tramite allarmi e dashboard basati sui dati. Il monitoraggio attivo è essenziale per mantenere sistemi ML altamente disponibili e affidabili.\nGuardare il video qui sotto per saperne di più sul monitoraggio.\n\n\n\n\n\n\nVideo 13.3: Monitoraggio del Modello\n\n\n\n\n\n\n\n\n13.3.8 Governance\nI team MLOps stabiliscono attivamente pratiche di governance appropriate come componente fondamentale. La governance fornisce una supervisione sui modelli ML per garantire che siano affidabili, etici e conformi. Senza governance, sussistono rischi significativi di modelli che si comportano in modi pericolosi o proibiti quando vengono distribuiti in applicazioni e processi aziendali.\nLa governance MLOps impiega tecniche per fornire trasparenza sulle previsioni, sulle prestazioni e sul comportamento del modello durante l’intero ciclo di vita ML. Metodi di spiegabilità come SHAP e LIME aiutano gli auditor a comprendere perché i modelli effettuano determinate previsioni evidenziando le caratteristiche di input influenti alla base delle decisioni. Bias detection analizza le prestazioni del modello in diversi gruppi demografici definiti da attributi come età, sesso ed etnia per rilevare eventuali distorsioni sistematiche. I team eseguono rigorose procedure di test su set di dati rappresentativi per convalidare le prestazioni del modello prima della distribuzione.\nUna volta in produzione, i team monitorano la concept drift [deriva del concetto] per determinare se le relazioni predittive cambiano nel tempo in modi che degradano l’accuratezza del modello. I team analizzano anche i registri di produzione per scoprire modelli nei tipi di errori generati dai modelli. La documentazione sulla provenienza dei dati, le procedure di sviluppo e le metriche di valutazione fornisce ulteriore visibilità.\nPiattaforme come Watson OpenScale incorporano funzionalità di governance come il monitoraggio dei bias e la spiegabilità direttamente nella creazione di modelli, nei test e nel monitoraggio della produzione. Le aree di interesse principali della governance sono trasparenza, correttezza e conformità. Ciò riduce al minimo i rischi che i modelli si comportino in modo errato o pericoloso quando integrati nei processi aziendali. L’integrazione di pratiche di governance nei flussi di lavoro MLOps consente ai team di garantire un’IA affidabile.\n\n\n13.3.9 Comunicazione e Collaborazione\nMLOps abbatte attivamente i “silos” e consente il libero flusso di informazioni e approfondimenti tra i team in tutte le fasi del ciclo di vita ML. Strumenti come MLflow, Weights & Biases e contesti di dati forniscono tracciabilità e visibilità per migliorare la collaborazione.\nI team utilizzano MLflow per sistematizzare il monitoraggio di esperimenti, versioni e artefatti del modello. Gli esperimenti possono essere loggati a livello di programmazione da notebook di data science e job di formazione. Il registro dei modelli fornisce un hub centrale per i team per archiviare modelli pronti per la produzione prima della distribuzione, con metadati come descrizioni, metriche, tag e discendenza. Le integrazioni con Github, GitLab facilitano i trigger per la modifica del codice.\n“Weights & Biases” fornisce strumenti collaborativi su misura per i team ML. Gli scienziati dei dati registrano gli esperimenti, visualizzano metriche come curve di perdita e condividono approfondimenti sulla sperimentazione con i colleghi. Le dashboard di confronto evidenziano le differenze del modello. I team discutono dei progressi e dei passaggi successivi.\nLa definizione di contesti di dati condivisi, ovvero glossari, dizionari di dati e riferimenti di schemi, garantisce l’allineamento del significato e dell’utilizzo dei dati tra i ruoli. La documentazione aiuta a comprendere chi non ha accesso diretto ai dati.\nAd esempio, uno scienziato dei dati può utilizzare “Weights & Biases” per analizzare un esperimento con un modello di rilevamento delle anomalie e condividere i risultati della valutazione con altri membri del team per discutere dei miglioramenti. Il modello finale può quindi essere registrato con MLflow prima di essere consegnato per la distribuzione.\nL’abilitazione della trasparenza, della tracciabilità e della comunicazione tramite MLOps consente ai team di rimuovere i colli di bottiglia e accelerare la distribuzione di sistemi ML di impatto.\nVideo 13.4 affronta le sfide chiave nella distribuzione del modello, tra cui la deriva del concetto, la deriva del modello e i problemi di ingegneria del software.\n\n\n\n\n\n\nVideo 13.4: Sfide della Distribuzione",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#debito-tecnico-nascosto-nei-sistemi-ml",
    "href": "contents/ops/ops.it.html#debito-tecnico-nascosto-nei-sistemi-ml",
    "title": "13  Operazioni di ML",
    "section": "13.4 Debito Tecnico Nascosto nei Sistemi ML",
    "text": "13.4 Debito Tecnico Nascosto nei Sistemi ML\nIl debito tecnico [https://it.wikipedia.org/wiki/Debito_tecnico] è sempre più pressante per i sistemi di apprendimento automatico. Questa metafora, originariamente proposta negli anni ’90, paragona i costi a lungo termine dello sviluppo rapido del software al debito finanziario. Proprio come un debito finanziario alimenta una crescita vantaggiosa, un debito tecnico gestito con attenzione consente una rapida iterazione. Tuttavia, se non controllato, l’accumulo di debito tecnico può superare qualsiasi guadagno.\nFigura 13.2 illustra i vari componenti che contribuiscono al debito tecnico nascosto dei sistemi ML. Mostra la natura interconnessa di configurazione, raccolta dati ed estrazione di funzionalità, che è fondamentale per la base di codice ML. Le dimensioni delle caselle indicano la proporzione dell’intero sistema rappresentata da ciascun componente. Nei sistemi ML industriali, il codice per l’algoritmo del modello costituisce solo una piccola frazione (vedere la piccola casella nera al centro rispetto a tutte le altre caselle grandi). La complessità dei sistemi ML e la natura frenetica del settore rendono molto facile l’accumulo di debito tecnico.\n\n\n\n\n\n\nFigura 13.2: Componenti del sistema ML. Fonte: Sambasivan et al. (2021)\n\n\n\n\n13.4.1 Erosione dei Confini del Modello\nA differenza del software tradizionale, ML non ha confini chiari tra i componenti, come si vede nel diagramma sopra. Questa erosione dell’astrazione crea intrecci che esacerbano il debito tecnico in diversi modi:\n\n\n13.4.2 Intreccio\nUn accoppiamento stretto tra i componenti del modello ML rende difficile isolare le modifiche. La modifica di una parte provoca effetti a catena imprevedibili in tutto il sistema. “Changing Anything Changes Everything (noto anche come CACE)” [Cambiare qualcosa cambia tutto] è un fenomeno che si applica a qualsiasi modifica apportata al sistema. Le potenziali mitigazioni includono la scomposizione del problema quando possibile o il monitoraggio ravvicinato delle modifiche nel comportamento per contenerne l’impatto.\n\n\n13.4.3 Cascate di Correzione\nFigura 13.3 illustra il concetto di cascate di correzione nel flusso di lavoro ML, dalla definizione del problema all’implementazione del modello. Gli archi rappresentano le potenziali correzioni iterative necessarie in ogni fase del flusso di lavoro, con colori diversi corrispondenti a problemi distinti come l’interazione con la fragilità del mondo fisico, competenze inadeguate nel dominio dell’applicazione, sistemi di ricompensa in conflitto e scarsa documentazione inter-organizzativa.\nLe frecce rosse indicano l’impatto delle cascate, che possono portare a revisioni significative nel processo di sviluppo del modello. Al contrario, la linea rossa tratteggiata rappresenta la misura drastica di abbandono del processo per riavviarlo. Questa immagine sottolinea la natura complessa e interconnessa dello sviluppo del sistema ML e l’importanza di affrontare questi problemi all’inizio del ciclo di sviluppo per mitigare i loro effetti di amplificazione a valle.\n\n\n\n\n\n\nFigura 13.3: Diagramma di flusso delle cascate di correzione. Fonte: Sambasivan et al. (2021).\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, e Lora M Aroyo. 2021. «“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI». In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nLa creazione di modelli in sequenza crea dipendenze rischiose in cui i modelli successivi si basano su quelli precedenti. Ad esempio, prendere un modello esistente e perfezionarlo per un nuovo caso d’uso sembra efficiente. Tuttavia, questo incorpora ipotesi dal modello originale che potrebbero eventualmente richiedere una correzione.\nDiversi fattori influenzano la decisione di creare modelli in sequenza o meno:\n\nDimensioni del dataset e tasso di crescita: Con set di dati statici e di piccole dimensioni, la messa a punto dei modelli esistenti ha spesso senso. Per set di dati di grandi dimensioni e in crescita, l’addestramento di modelli personalizzati da zero consente una maggiore flessibilità per tenere conto dei nuovi dati.\nRisorse di elaborazione disponibili: La messa a punto richiede meno risorse rispetto all’addestramento di modelli di grandi dimensioni da zero. Con risorse limitate, sfruttare i modelli esistenti potrebbe essere l’unico approccio fattibile.\n\nMentre la messa a punto dei modelli esistenti può essere efficiente, la modifica dei componenti fondamentali in seguito diventa estremamente costosa a causa di questi effetti a cascata. Pertanto, si dovrebbe considerare attentamente l’introduzione di nuove architetture di modelli, anche se ad alta intensità di risorse, per evitare cascate di correzioni in futuro. Questo approccio può aiutare ad attenuare gli effetti di amplificazione dei problemi a valle e a ridurre il debito tecnico. Tuttavia, ci sono ancora scenari in cui la creazione di modelli sequenziali ha senso, il che richiede un attento equilibrio tra efficienza, flessibilità e manutenibilità a lungo termine nel processo di sviluppo ML.\n\n\n13.4.4 Consumatori Non Dichiarati\nUna volta che le previsioni del modello ML sono rese disponibili, molti sistemi downstream [derivati] potrebbero utilizzarle silenziosamente come input per un’ulteriore elaborazione. Tuttavia, il modello originale non è stato progettato per adattarsi a questo ampio riutilizzo. A causa dell’opacità intrinseca dei sistemi ML, diventa impossibile analizzare completamente l’impatto degli output del modello come input altrove. Le modifiche al modello possono quindi avere conseguenze costose e pericolose interrompendo dipendenze non rilevate.\nI “consumatori” non dichiarati possono anche abilitare loop di feedback nascosti se i loro output influenzano indirettamente i dati di training del modello originale. Le mitigazioni includono la limitazione dell’accesso alle previsioni, la definizione di contratti di servizio rigorosi e il monitoraggio di segnali di influenze non-modellate. Architettare sistemi ML per incapsulare e isolare i loro effetti limita i rischi di propagazione imprevista.\n\n\n13.4.5 Debito di Dipendenza dai Dati\nIl debito di dipendenza dei dati si riferisce a dipendenze di dati instabili e sottoutilizzate, che possono avere ripercussioni dannose e difficili da rilevare. Sebbene questo sia un fattore chiave del debito tecnologico per il software tradizionale, tali sistemi possono trarre vantaggio dall’uso di strumenti ampiamente disponibili per l’analisi statica da parte di compilatori e linker per identificare dipendenze di questo tipo. I sistemi ML necessitano di strumenti simili.\nUna mitigazione per le dipendenze di dati instabili è l’uso del versioning, che garantisce la stabilità degli input ma comporta il costo della gestione di più set di dati e il potenziale della obsolescenza. Un’altra mitigazione per le dipendenze di dati sottoutilizzate è quella di condurre una valutazione esaustiva “leave-one-feature-out”.\n\n\n13.4.6 Debito di Analisi dai Cicli di Feedback\nA differenza del software tradizionale, i sistemi ML possono cambiare il loro comportamento nel tempo, rendendo difficile l’analisi pre-distribuzione. Questo debito si manifesta nei cicli di feedback, sia diretti che nascosti.\nI cicli di feedback diretti si verificano quando un modello influenza i suoi input futuri, ad esempio consigliando prodotti agli utenti che, a loro volta, modellano i dati di formazione futuri. I cicli nascosti sorgono indirettamente tra modelli, ad esempio due sistemi che interagiscono tramite ambienti del mondo reale. I cicli di feedback graduali sono particolarmente difficili da rilevare. Questi cicli portano al debito di analisi, ovvero l’incapacità di prevedere come un modello agirà completamente dopo il rilascio. Essi compromettono la validazione pre-distribuzione consentendo un’autoinfluenza non modellata.\nUn attento monitoraggio e distribuzioni “canary” aiutano a rilevare il feedback. Tuttavia, permangono sfide fondamentali nella comprensione delle interazioni complesse del modello. Le scelte architettoniche che riducono l’intreccio e l’accoppiamento mitigano l’effetto composto del debito di analisi.\n\n\n13.4.7 Le Giungle di Pipeline\nI workflow [flussi di lavoro] ML spesso necessitano di interfacce più standardizzate tra i componenti. Ciò porta i team a “incollare” gradualmente le pipeline con codice personalizzato. Ciò che emerge sono “giungle di pipeline”, ovvero passaggi di pre-elaborazione aggrovigliati che sono fragili e resistono al cambiamento. Evitare modifiche a queste pipeline disordinate fa sì che i team sperimentino attraverso prototipi alternativi. Presto, proliferano molteplici modi di fare. La necessità di astrazioni e interfacce impedisce quindi la condivisione, il riutilizzo e l’efficienza.\nIl debito tecnico si accumula man mano che le pipeline si solidificano in vincoli legacy. I team sprecano tempo nella gestione di codice idiosincratico anziché massimizzare le prestazioni del modello. Principi architettonici come modularità e incapsulamento sono necessari per stabilire interfacce pulite. Le astrazioni condivise consentono componenti intercambiabili, impediscono il lock-in e promuovono la diffusione delle “best practice” tra i team. Liberarsi dalle “giungle di pipeline” richiede in definitiva l’applicazione di standard che impediscano l’accumulo di debito di astrazione. I vantaggi delle interfacce e delle API che domano la complessità superano i costi di transizione.\n\n\n13.4.8 Debito di Configurazione\nI sistemi ML comportano una configurazione estesa di iperparametri, architetture e altri parametri di ottimizzazione. Tuttavia, la configurazione è spesso un ripensamento, che necessita di più rigore e test: aumentano le configurazioni ad hoc, amplificate dalle numerose “manopole” disponibili per l’ottimizzazione di modelli ML complessi.\nQuesto accumulo di debito tecnico ha diverse conseguenze. Configurazioni fragili e obsolete portano a dipendenze nascoste e bug che causano guasti di produzione. La conoscenza sulle configurazioni ottimali è isolata anziché condivisa, portando a un lavoro ridondante. Riprodurre e confrontare i risultati diventa difficile quando le configurazioni mancano di documentazione. I vincoli legacy si accumulano poiché i team temono di modificare configurazioni poco comprese.\nPer affrontare il debito di configurazione è necessario stabilire standard per documentare, testare, convalidare e archiviare centralmente le configurazioni. Investire in approcci più automatizzati, come l’ottimizzazione degli iperparametri e la ricerca dell’architettura, riduce la dipendenza dall’ottimizzazione manuale. Una migliore igiene della configurazione rende il miglioramento iterativo più gestibile impedendo alla complessità di aumentare all’infinito. La chiave è riconoscere la configurazione come parte integrante del ciclo di vita del sistema ML piuttosto che come un ripensamento ad hoc.\n\n\n13.4.9 Il Mondo che Cambia\nI sistemi ML operano in ambienti dinamici del mondo reale. Le soglie e le decisioni inizialmente efficaci diventano obsolete man mano che il mondo si evolve. Tuttavia, i vincoli legacy rendono difficile adattare i sistemi a popolazioni, modelli di utilizzo e altri fattori contestuali mutevoli.\nQuesto debito si manifesta in due modi principali. In primo luogo, le soglie preimpostate e le euristiche richiedono una rivalutazione e una messa a punto costanti man mano che i loro valori ottimali si spostano. In secondo luogo, la convalida dei sistemi tramite test statici di unità e integrazione fallisce quando input e comportamenti sono obiettivi in movimento.\nRispondere a un mondo in continua evoluzione in tempo reale con sistemi ML legacy è impegnativo. Il debito tecnico si accumula man mano che le ipotesi decadono. La mancanza di architettura modulare e la capacità di aggiornare dinamicamente i componenti senza effetti collaterali esacerbano questi problemi.\nPer mitigare questo problema è necessario integrare configurabilità, monitoraggio e aggiornabilità modulare. L’apprendimento online, in cui i modelli si adattano continuamente e solidi cicli di feedback alle pipeline di formazione, aiutano a sintonizzarsi automaticamente sul mondo. Tuttavia, anticipare e progettare il cambiamento è essenziale per prevenire l’erosione delle prestazioni nel mondo reale nel tempo.\n\n\n13.4.10 Gestire il Debito Tecnico nelle Fasi Iniziali\nÈ comprensibile che il debito tecnico si accumuli naturalmente nelle prime fasi di sviluppo del modello. Quando si punta a creare rapidamente modelli MVP, i team spesso hanno bisogno di informazioni più complete su quali componenti raggiungeranno la scala o richiederanno modifiche. È previsto un po’ di lavoro differito.\nTuttavia, anche i sistemi iniziali frammentati dovrebbero seguire principi come “Flexible Foundations” per evitare di mettersi nei guai:\n\nIl codice modulare e le librerie riutilizzabili consentono di scambiare i componenti in un secondo momento\nL’accoppiamento debole tra modelli, archivi dati e logica aziendale facilita il cambiamento\nI layer di astrazione nascondono i dettagli di implementazione che potrebbero cambiare nel tempo\nIl servizio di modelli containerizzati mantiene aperte le opzioni sui requisiti di distribuzione\n\nLe decisioni che sembrano ragionevoli al momento possono limitare seriamente la flessibilità futura. Ad esempio, incorporare la logica aziendale chiave nel codice modello anziché tenerla separata rende estremamente difficili le modifiche successive al modello.\nCon una progettazione ponderata, tuttavia, è possibile creare rapidamente all’inizio mantenendo gradi di libertà per migliorare. Man mano che il sistema matura, emergono prudenti punti di interruzione in cui l’introduzione di nuove architetture in modo proattivo evita massicce rilavorazioni in futuro. In questo modo si bilanciano le urgenti tempistiche con la riduzione delle future cascate di correzione.\n\n\n13.4.11 Riepilogo\nSebbene il debito finanziario sia una buona metafora per comprendere i compromessi, differisce dalla misurabilità del debito tecnico. Il debito tecnico deve essere completamente monitorato e quantificato. Ciò rende difficile per i team gestire i compromessi tra muoversi rapidamente e introdurre intrinsecamente più debito rispetto al prendersi il tempo per ripagare tale debito.\nIl documento Hidden Technical Debt of Machine Learning Systems diffonde la consapevolezza delle sfumature del debito tecnologico specifico del sistema ML. Incoraggia un ulteriore sviluppo nell’ampia area del ML manutenibile.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#ruoli-e-responsabilità",
    "href": "contents/ops/ops.it.html#ruoli-e-responsabilità",
    "title": "13  Operazioni di ML",
    "section": "13.5 Ruoli e Responsabilità",
    "text": "13.5 Ruoli e Responsabilità\nData la vastità di MLOps, l’implementazione di successo di sistemi ML richiede competenze diversificate e una stretta collaborazione tra persone con diverse aree di competenza. Mentre gli scienziati dei dati creano i modelli ML di base, è necessario un lavoro di squadra interfunzionale per distribuire con successo questi modelli in ambienti di produzione e consentire loro di fornire un valore aziendale sostenibile.\nMLOps fornisce il framework e le pratiche per coordinare gli sforzi di vari ruoli coinvolti nello sviluppo, nella distribuzione e nell’esecuzione di sistemi MLG. Collegare i “silos” tradizionali tra i team di dati, ingegneria e operazioni è fondamentale per il successo di MLOps. Abilitare una collaborazione senza soluzione di continuità attraverso il ciclo di vita dell’apprendimento automatico accelera la realizzazione dei vantaggi garantendo al contempo l’affidabilità e le prestazioni a lungo termine dei modelli ML.\nEsamineremo alcuni ruoli chiave coinvolti in MLOps e le loro responsabilità principali. Comprendere l’ampiezza delle competenze necessarie per rendere operativi i modelli ML guida l’assemblaggio dei team MLOps. Chiarisce inoltre come i flussi di lavoro tra i ruoli si adattano alla metodologia MLOps sovraordinata.\n\n13.5.1 Ingegneri dei Dati\nGli ingegneri dei dati sono responsabili della creazione e della manutenzione dell’infrastruttura dati e delle pipeline che alimentano i dati nei modelli ML. Garantiscono che i dati vengano trasferiti senza problemi dai sistemi di origine agli ambienti di archiviazione, elaborazione e progettazione delle funzionalità necessari per lo sviluppo e la distribuzione dei modelli ML. Le loro principali responsabilità includono:\n\nMigrare dati grezzi da database, sensori e app “on-prem” [in azienda], in data lake basati su cloud, come Amazon S3 o Google Cloud Storage. Ciò fornisce un’archiviazione economica e scalabile.\nCreare pipeline di dati con “scheduler” [pianificatori] di flussi di lavoro come Apache Airflow, Prefect e dbt. Questi estraggono i dati dalle sorgenti, li trasformano e li convalidano, e li caricano direttamente in destinazioni come data warehouse, feature store o per l’addestramento del modello.\nTrasformare dati grezzi e disordinati in set di dati strutturati e pronti per l’analisi. Ciò include la gestione di valori nulli o malformati, la deduplicazione, l’unione di origini dati disparate, l’aggregazione dei dati e la progettazione di nuove feature.\nManutenere componenti dell’infrastruttura dati come data warehouse cloud (Snowflake, Redshift, BigQuery), data lake e sistemi di gestione dei metadati. Provisioning e ottimizzazione dei sistemi di elaborazione dati.\nDefinire i processi di versioning, backup e archiviazione dei dati per i set di dati e funzionalità ML e applicare policy di governance dei dati.\n\nAd esempio, un’azienda manifatturiera può utilizzare pipeline Apache Airflow per estrarre dati dei sensori dai PLC in fabbrica e trasferirli in un data lake Amazon S3. Gli ingegneri dei dati elaborerebbero poi questi dati grezzi per filtrarli, pulirli e unirli ai metadati del prodotto. Questi output della pipeline verrebbero quindi caricati in un data warehouse Snowflake da cui è possibile leggere le feature per l’addestramento e la previsione del modello.\nIl team di ingegneria dei dati crea e sostiene la base dati per uno sviluppo e un funzionamento affidabili del modello. Il loro lavoro consente agli scienziati dei dati e agli ingegneri ML di concentrarsi sulla creazione, l’addestramento e l’implementazione di modelli ML su larga scala.\n\n\n13.5.2 Data Scientist\nIl lavoro dei “data scientist” [scienziato dei dati] è concentrarsi sulla ricerca, sperimentazione, sviluppo e miglioramento continuo dei modelli ML. Sfruttano la loro competenza in statistica, modellazione e algoritmi per creare modelli ad alte prestazioni. Le loro principali responsabilità includono:\n\nCollaborare con team aziendali e di dati per identificare opportunità in cui ML può aggiungere valore, inquadrare il problema e definire metriche di successo.\nEseguire analisi esplorative dei dati per comprendere le relazioni nei dati, ricavare informazioni e identificare funzionalità rilevanti per la modellazione.\nRicercare e sperimentare diversi algoritmi ML e architetture di modelli in base al problema e alle caratteristiche dei dati e sfruttare librerie come TensorFlow, PyTorch e Keras.\nMassimizzare le prestazioni, addestrare e perfezionare i modelli regolando gli iperparametri, regolando le architetture delle reti neurali, l’ingegneria delle funzionalità, ecc.\nValutare le prestazioni del modello tramite metriche come accuratezza, AUC e punteggi F1 ed eseguire analisi degli errori per identificare aree di miglioramento.\nSviluppare nuove versioni del modello mediante l’integrazione di nuovi dati, test di diversi approcci, ottimizzazione del comportamento del modello e mantenimento della documentazione e della discendenza per i modelli.\n\nAd esempio, uno scienziato dei dati può sfruttare TensorFlow e TensorFlow Probability per sviluppare un modello di previsione della domanda per la pianificazione dell’inventario ala vendita al dettaglio. Itereranno su diversi modelli di sequenza come LSTM e sperimenteranno funzionalità derivate da dati di prodotto, vendite e stagionali. Il modello verrà valutato in base a metriche di errore rispetto alla domanda effettiva prima dell’implementazione. Lo scienziato dei dati monitora le prestazioni e riqualifica/migliora il modello man mano che arrivano nuovi dati.\nI data scientist guidano la creazione, il miglioramento e l’innovazione del modello attraverso la loro competenza nelle tecniche di ML. Collaborano strettamente con altri ruoli per garantire che i modelli creino il massimo impatto aziendale.\n\n\n13.5.3 ML Engineer\nGli “ingegneri ML” consentono ai modelli sviluppati dagli scienziati dei dati di essere prodotti e distribuiti su larga scala. La loro competenza fa sì che i modelli servano in modo affidabile alle previsioni nelle applicazioni e nei processi aziendali. Le loro principali responsabilità includono:\n\nPrendere modelli prototipo dagli scienziati dei dati e rafforzarli per gli ambienti di produzione tramite best practice di codifica.\nCreare API e microservizi per la distribuzione dei modelli utilizzando strumenti come Flask, FastAPI. Containerizzare i modelli con Docker.\nGestire le versioni dei modelli, sincronizzarli in produzione utilizzando pipeline CI/CD e implementare release canary, test A/B e procedure di rollback.\nOttimizzare le prestazioni dei modelli per elevata scalabilità, bassa latenza ed efficienza dei costi. Sfruttare compressione, quantizzazione e servizio multi-modello.\nMonitorare i modelli una volta in produzione e garantire affidabilità e precisione continue. Riqualificare periodicamente i modelli.\n\nAd esempio, un ingegnere ML può prendere un modello di rilevamento delle frodi TensorFlow sviluppato da data scientist e containerizzarlo utilizzando TensorFlow Serving per una distribuzione scalabile. Il modello verrebbe integrato nella pipeline di elaborazione delle transazioni dell’azienda tramite API. L’ingegnere ML implementa un registro dei modelli e una pipeline CI/CD utilizzando MLFlow e Jenkins per distribuire gli aggiornamenti del modello in modo affidabile. Gli ingegneri ML monitorano quindi il modello in esecuzione per prestazioni continue utilizzando strumenti come Prometheus e Grafana. Se l’accuratezza del modello diminuisce, avviano la riqualificazione e la distribuzione di una nuova versione del modello.\nIl team di ingegneria ML consente ai modelli di data science di progredire senza problemi in sistemi di produzione sostenibili e robusti. La loro competenza nella creazione di sistemi modulari e monitorati offre un valore aziendale continuo.\n\n\n13.5.4 DevOps Engineer\nGli “ingegneri DevOps” abilitano MLOps creando e gestendo l’infrastruttura sottostante per lo sviluppo, la distribuzione e il monitoraggio dei modelli ML. Forniscono l’architettura cloud e le pipeline di automazione. Le loro principali responsabilità includono:\n\nApprovvigionare e gestire l’infrastruttura cloud per i flussi di lavoro ML utilizzando strumenti IaC come Terraform, Docker e Kubernetes.\nSviluppare pipeline CI/CD per il riaddestramento, la convalida e la distribuzione del modello. Integrare strumenti ML nella pipeline, come MLflow e Kubeflow.\nMonitorare le prestazioni del modello e dell’infrastruttura tramite strumenti come Prometheus, Grafana, stack ELK. Creare allarmi e dashboard.\nImplementare pratiche di governance relative allo sviluppo, al test e alla promozione del modello per consentire riproducibilità e tracciabilità.\nEmbedding dei modelli ML nelle applicazioni. Espongono i modelli tramite API e microservizi per l’integrazione.\nOttimizzazione delle prestazioni e dei costi dell’infrastruttura e sfruttamento dell’autoscaling, delle istanze spot e della disponibilità in tutte le regioni.\n\nAd esempio, un ingegnere DevOps esegue il provisioning di un cluster Kubernetes su AWS utilizzando Terraform per eseguire lavori di training ML e distribuzione online. Costruiscono una pipeline CI/CD in Jenkins, che attiva il retraining del modello se sono disponibili nuovi dati. Dopo il test automatizzato, il modello viene registrato con MLflow e distribuito nel cluster Kubernetes. L’ingegnere monitora quindi lo stato del cluster, l’utilizzo delle risorse del contenitore e la latenza dell’API utilizzando Prometheus e Grafana.\nIl team DevOps consente una rapida sperimentazione e distribuzioni affidabili per ML tramite competenze cloud, automazione e monitoraggio. Il loro lavoro massimizza l’impatto del modello riducendo al minimo il debito tecnico.\n\n\n13.5.5 Project Manager\nI project manager svolgono un ruolo fondamentale in MLOps coordinando le attività tra i team coinvolti nella distribuzione dei progetti ML. Aiutano a guidare l’allineamento, la “accountability” [affidabilità] ed accelerano i risultati. Le loro principali responsabilità includono:\n\nCollaborare con le parti interessate per definire obiettivi di progetto, metriche di successo, tempistiche e budget; delineare specifiche e “scope”.\nCreare un piano di progetto che comprenda acquisizione dati, sviluppo modello, configurazione infrastrutturale, distribuzione e monitoraggio.\nCoordinare i lavori di progettazione, sviluppo e test tra ingegneri dei dati, scienziati dei dati, ingegneri ML e ruoli DevOps.\nMonitorare i progressi e le milestone, identificare gli ostacoli e risolverli tramite azioni correttive e gestire rischi e problemi.\nFacilitare la comunicazione tramite report di stato, riunioni, workshop e documentazione e consentire una collaborazione senza interruzioni.\nGuidare l’aderenza alle tempistiche e al budget e aumentare i superamenti o le carenze previsti per la mitigazione.\n\nAd esempio, un project manager creerebbe un piano di progetto per sviluppare e migliorare un modello di previsione dell’abbandono dei clienti. Coordinare data engineer che creano pipeline di dati, data scientist che sperimentano modelli, ML engineer che producono modelli e DevOps che impostano l’infrastruttura di distribuzione. Il project manager monitora i progressi tramite milestone come preparazione del set di dati, prototipazione del modello, distribuzione e monitoraggio. Per attuare soluzioni preventive, evidenziano eventuali rischi, ritardi o problemi di budget.\nI project manager qualificati consentono ai team MLOps di lavorare in sinergia per fornire rapidamente il massimo valore aziendale dagli investimenti ML. La loro leadership e organizzazione si allineano con team diversi.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#sfide-dei-sistemi-embedded",
    "href": "contents/ops/ops.it.html#sfide-dei-sistemi-embedded",
    "title": "13  Operazioni di ML",
    "section": "13.6 Sfide dei Sistemi Embedded",
    "text": "13.6 Sfide dei Sistemi Embedded\nEsamineremo brevemente le sfide dei sistemi embedded in modo da definire il contesto per quelle specifiche che emergono con gli MLOps embedded, di cui parleremo nella sezione seguente.\n\n13.6.1 Risorse di Elaborazione Limitate\nI dispositivi embedded come i microcontrollori e i telefoni cellulari hanno una potenza di elaborazione molto più limitata rispetto alle macchine dei data center o alle GPU. Un tipico microcontrollore può avere solo KB di RAM, velocità della CPU MHz e nessuna GPU. Ad esempio, un microcontrollore in uno smartwatch può avere solo un processore a 32 bit in esecuzione a 120 MHz con 320 KB di RAM (Stm32L4Q5Ag 2021). Ciò consente modelli ML semplici come piccole regressioni lineari o foreste casuali, ma reti neurali profonde più complesse sarebbero irrealizzabili. Le strategie per mitigare questo includono quantizzazione, potatura, architetture di modelli efficienti e scaricamento di determinati calcoli sul cloud quando la connettività lo consente.\n\nStm32L4Q5Ag. 2021. STMicroelectronics.\n\n\n13.6.2 Memoria Limitata\nMemorizzare grandi modelli ML e set di dati direttamente su dispositivi embedded è spesso impossibile con una memoria limitata. Ad esempio, un modello di rete neurale profonda può facilmente occupare centinaia di MB, il che supera la capacità di archiviazione di molti sistemi embedded. Si consideri questo esempio. Una fotocamera per la fauna selvatica che cattura immagini per rilevare animali può avere solo una scheda di memoria da 2 GB. Ne serve di più per memorizzare un modello di deep learning per la classificazione delle immagini che spesso ha una dimensione di centinaia di MB. Di conseguenza, ciò richiede l’ottimizzazione dell’utilizzo della memoria tramite compressione dei pesi, numeri di precisione inferiore e pipeline di inferenza in streaming.\n\n\n13.6.3 Connettività Intermittente\nMolti dispositivi embedded operano in ambienti remoti senza una connettività Internet affidabile. Dobbiamo fare affidamento su qualcosa di diverso dall’accesso cloud costante per un comodo riaddestramento, monitoraggio e distribuzione. Al contrario, abbiamo bisogno di strategie intelligenti sia di pianificazione che si memorizzazione nella cache per ottimizzare le connessioni intermittenti. Ad esempio, un modello che prevede la resa del raccolto in una fattoria remota potrebbe dover fare previsioni giornaliere ma avere connettività al cloud solo una volta alla settimana quando l’agricoltore si reca in città. Il modello deve funzionare in modo indipendente tra una connessione e l’altra.\n\n\n13.6.4 Limitazioni della Potenza\nI dispositivi embedded come telefoni, dispositivi indossabili e sensori remoti sono alimentati a batteria. L’inferenza e la comunicazione continue possono esaurire rapidamente le batterie, limitandone la funzionalità. Ad esempio, un collare intelligente che contrassegna gli animali in via di estinzione funziona con una piccola batteria. L’esecuzione continua di un modello di tracciamento GPS scaricherebbe la batteria nel giro di pochi giorni. Il collare deve pianificare con attenzione quando attivare il modello. Pertanto, l’ML integrato deve gestire attentamente le attività per risparmiare energia. Le tecniche includono acceleratori hardware ottimizzati, caching delle previsioni ed esecuzione di modelli adattivi.\n\n\n13.6.5 Gestione della Flotta\nPer i dispositivi embedded prodotti in serie, milioni di unità possono essere distribuite sul campo per orchestrare gli aggiornamenti. Ipoteticamente, l’aggiornamento di un modello di rilevamento delle frodi su 100 milioni di carte di credito (future intelligenti) richiede l’invio sicuro degli aggiornamenti a ciascun dispositivo distribuito anziché a un data center centralizzato. Una scala così distribuita rende la gestione dell’intera flotta molto più difficile di un cluster di server centralizzato. Richiede protocolli intelligenti per aggiornamenti “over-the-air”, gestione dei problemi di connettività e monitoraggio dei vincoli di risorse tra i dispositivi.\n\n\n13.6.6 Raccolta Dati On-Device\nLa raccolta di dati di formazione utili richiede la progettazione sia dei sensori sul dispositivo sia delle pipeline software. Questo è diverso dai server, dove possiamo estrarre dati da fonti esterne. Le sfide includono la gestione del rumore dei sensori. I sensori su una macchina industriale rilevano vibrazioni e temperatura per prevedere le esigenze di manutenzione. Ciò richiede la messa a punto dei sensori e delle frequenze di campionamento per acquisire dati utili.\n\n\n13.6.7 Personalizzazione Specifica del Dispositivo\nUno smart speaker impara i modelli vocali e la cadenza del parlato di un singolo utente per migliorare la precisione del riconoscimento proteggendo al contempo la privacy. Adattare i modelli ML a dispositivi e utenti specifici è importante, ma ciò pone sfide per la privacy. L’apprendimento sul dispositivo consente la personalizzazione senza trasmettere così tanti dati privati. Tuttavia, bilanciare il miglioramento del modello, la tutela della privacy e i vincoli richiede nuove tecniche.\n\n\n13.6.8 Considerazioni sulla Sicurezza\nSe un ML embedded estremamente grande in sistemi come i veicoli a guida autonoma non viene progettato con attenzione, ci sono seri rischi per la sicurezza. Per garantire un funzionamento sicuro prima dell’implementazione, le auto a guida autonoma devono essere sottoposte a test approfonditi in pista in scenari simulati di pioggia, neve e ostacoli. Ciò richiede una convalida approfondita, dispositivi di sicurezza, simulatori e conformità agli standard prima dell’implementazione.\n\n\n13.6.9 Diversi Target Hardware\nEsiste una vasta gamma di processori embedded, tra cui ARM, x86, acceleratori AI specializzati, FPGA, ecc. Supportare questa eterogeneità rende difficile l’implementazione. Abbiamo bisogno di strategie come framework standardizzati, test approfonditi e messa a punto del modello per ogni piattaforma. Ad esempio, un modello di rilevamento degli oggetti necessita di implementazioni efficienti su dispositivi embedded come Raspberry Pi, Nvidia Jetson e Google Edge TPU.\n\n\n13.6.10 Copertura dei Test\nTestare rigorosamente i casi limite è difficile con risorse di simulazione embedded limitate, ma test esaustivi sono fondamentali in sistemi come le auto a guida autonoma. Testare esaustivamente un modello di pilota automatico richiede milioni di chilometri simulati, esponendolo a eventi rari come guasti dei sensori. Pertanto, strategie come la generazione di dati sintetici, la simulazione distribuita e l’ingegneria del caos aiutano a migliorare la copertura.\n\n\n13.6.11 Rilevamento della Deriva del Concetto\nCon dati di monitoraggio limitati da ogni dispositivo remoto, rilevare cambiamenti nei dati di input nel tempo è molto più difficile. La deriva può portare a degradazioni delle prestazioni del modello. Sono necessari metodi “leggeri” per identificare quando è necessario un riaddestramento. Un modello che prevede i carichi della rete elettrica mostra un calo delle prestazioni man mano che i modelli di utilizzo cambiano nel tempo. Con i soli dati locali sui dispositivi, questa tendenza è difficile da individuare.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#mlops-tradizionali-e-mlops-embedded",
    "href": "contents/ops/ops.it.html#mlops-tradizionali-e-mlops-embedded",
    "title": "13  Operazioni di ML",
    "section": "13.7 MLOps Tradizionali e MLOps Embedded",
    "text": "13.7 MLOps Tradizionali e MLOps Embedded\nNegli MLOps tradizionali, i modelli ML vengono in genere distribuiti in ambienti basati su cloud o server, con risorse abbondanti come potenza di calcolo e memoria. Questi ambienti facilitano il funzionamento regolare di modelli complessi che richiedono risorse di calcolo significative. Ad esempio, un modello di riconoscimento delle immagini basato su cloud potrebbe essere utilizzato da una piattaforma di social media per contrassegnare automaticamente le foto con etichette pertinenti. In questo caso, il modello può sfruttare le vaste risorse disponibili nel cloud per elaborare in modo efficiente grandi quantità di dati.\nD’altro canto, i MLOps embedded comportano la distribuzione di modelli ML su sistemi embedded, sistemi di calcolo specializzati progettati per eseguire funzioni specifiche all’interno di sistemi più grandi. I sistemi embedded sono in genere caratterizzati dalle loro risorse di calcolo e potenza limitate. Ad esempio, un modello ML potrebbe essere “embedded” in un termostato intelligente per ottimizzare il riscaldamento e il raffreddamento in base alle preferenze e alle abitudini dell’utente. Il modello deve essere ottimizzato per funzionare in modo efficiente sull’hardware limitato del termostato senza comprometterne le prestazioni o la precisione.\nLa differenza fondamentale tra i MLOps tradizionali e quelli embedded risiede nei vincoli di risorse del sistema embedded. Mentre gli MLOps tradizionali possono sfruttare abbondanti risorse cloud o server, gli MLOps embedded devono fare i conti con le limitazioni hardware su cui viene distribuito il modello. Ciò richiede un’attenta ottimizzazione e messa a punto del modello per garantire che possa fornire informazioni accurate e preziose entro i vincoli del sistema embedded.\nInoltre, gli MLOps embedded devono considerare le sfide uniche poste dall’integrazione dei modelli ML con altri componenti del sistema embedded. Ad esempio, il modello deve essere compatibile con il software e l’hardware del sistema e deve essere in grado di interfacciarsi senza problemi con altri componenti, come sensori o attuatori. Ciò richiede una profonda comprensione sia dei sistemi ML che di quelli integrati e una stretta collaborazione tra data scientist, ingegneri e altre parti interessate.\nQuindi, mentre gli MLOps tradizionali e gli MLOps embedded condividono l’obiettivo comune di distribuire e mantenere modelli ML in ambienti di produzione, le sfide uniche poste dai sistemi embedded richiedono un approccio specializzato. Gli MLOps embedded devono bilanciare attentamente la necessità di accuratezza e prestazioni del modello con i vincoli dell’hardware su cui viene distribuito il modello. Ciò richiede una profonda comprensione sia dei sistemi ML che di quelli embedded e una stretta collaborazione tra i vari stakeholder per garantire l’integrazione di successo dei modelli ML nei sistemi embedded.\nQuesta volta, raggrupperemo i sottoargomenti in categorie più ampie per semplificare la struttura del nostro processo di pensiero su MLOps. Questa struttura aiuterà a comprendere come i diversi aspetti di MLOps siano interconnessi e perché ciascuno sia importante per il funzionamento efficiente dei sistemi ML mentre discutiamo le sfide nel contesto dei sistemi embedded.\n\nGestione del Ciclo di Vita del Modello\n\nGestione dei Dati: Gestione dell’ingestione dei dati, convalida e controllo delle versioni.\nAddestramento dei Modelli: Tecniche e pratiche per un addestramento dei modelli efficace e scalabile.\nValutazione dei Modelli: Strategie per testare e convalidare le prestazioni dei modelli.\nDistribuzione dei modelli: Approcci per la distribuzione dei modelli in ambienti di produzione.\n\nIntegrazione di Sviluppo e Operazioni\n\nPipeline CI/CD: Integrazione dei modelli ML in pipeline di integrazione e distribuzione continue.\nGestione dell’infrastruttura: Impostazione e manutenzione dell’infrastruttura necessaria per la formazione e la distribuzione dei modelli.\nComunicazione e Collaborazione: Garanzia di una comunicazione e collaborazione fluide tra data scientist, ingegneri ML e team operativi.\n\nEccellenza operativa\n\nMonitoraggio: Tecniche per il monitoraggio delle prestazioni dei modelli, della deriva dei dati e dello stato operativo.\nGovernance: Implementazione di policy per la verificabilità, la conformità e le considerazioni etiche dei modelli.\n\n\n\n13.7.1 Gestione del Ciclo di Vita del Modello\n\nGestione dei Dati\nNei tradizionali MLOps centralizzati, i dati vengono aggregati in grandi dataset e data lake, poi elaborati su server cloud o “on-prem” [in sede]. Tuttavia, MLOps embedded si basa su dati decentralizzati da sensori locali sui dispositivi. I dispositivi raccolgono batch più piccoli di dati incrementali, spesso rumorosi e non strutturati. Con vincoli di connettività, questi dati non possono sempre essere trasmessi istantaneamente al cloud e devono essere memorizzati nella cache in modo intelligente ed elaborati all’edge.\nA causa della potenza di calcolo limitata sui dispositivi embedded, i dati si possono solo preelaborare e pulire in modo minimo prima della trasmissione. Il filtraggio e l’elaborazione anticipati avvengono nei gateway edge per ridurre i carichi di trasmissione. Mentre si sfrutta l’archiviazione cloud, altre elaborazioni e archiviazioni avvengono all’edge per tenere conto della connettività intermittente. I dispositivi identificano e trasmettono solo i sottoinsiemi di dati più critici al cloud.\nAnche l’etichettatura richiede un accesso centralizzato ai dati, che richiede tecniche più automatizzate come l’apprendimento federato, in cui i dispositivi etichettano in modo collaborativo i dati dei peer. Con i dispositivi edge personali, la privacy dei dati e le normative sono preoccupazioni critiche. La raccolta, la trasmissione e l’archiviazione dei dati devono essere sicure e conformi.\nAd esempio, uno smartwatch può raccogliere il conteggio dei passi giornalieri, la frequenza cardiaca e le coordinate GPS. Questi dati vengono memorizzati nella cache locale e trasmessi a un gateway edge quando è disponibile il WiFi: il gateway elabora e filtra i dati prima di sincronizzare i sottoinsiemi rilevanti con la piattaforma cloud per riaddestrare i modelli.\n\n\nAddestramento del Modello\nNei tradizionali MLOps centralizzati, i modelli vengono addestrati utilizzando dati abbondanti tramite deep learning su server GPU cloud ad alta potenza. Tuttavia, glii MLOps embedded necessitano di maggiore supporto in termini di complessità del modello, disponibilità dei dati e risorse di elaborazione per l’addestramento.\nIl volume di dati aggregati è molto più basso, spesso richiedendo tecniche come l’apprendimento federato tra dispositivi per creare set di addestramento. La natura specializzata dei dati edge limita anche i set di dati pubblici per il pre-addestramento. Per questioni di privacy, i campioni di dati devono essere strettamente controllati e resi anonimi ove possibile.\nInoltre, i modelli devono utilizzare architetture semplificate ottimizzate per hardware edge a bassa potenza. Date le limitazioni di elaborazione, le GPU di fascia alta sono inaccessibili per un deep learning intensivo. L’addestramento sfrutta server edge e cluster a bassa potenza con approcci distribuiti per spartire il carico.\nIl “transfer learning” emerge come una strategia cruciale per affrontare la scarsità di dati e l’irregolarità nell’apprendimento automatico, in particolare negli scenari di edge computing. Come illustrato in Figura 13.4, questo approccio prevede la pre-formazione di modelli su grandi set di dati pubblici e la loro successiva messa a punto su dati edge specifici del dominio. La figura raffigura una rete neurale in cui i layer iniziali (da W_{A1} a W_{A4}), responsabili dell’estrazione delle feature generali, sono congelati (indicati da una linea tratteggiata verde). Questi layer conservano la conoscenza delle attività precedenti, accelerando l’apprendimento e riducendo i requisiti di risorse. Gli ultimi livelli (da W_{A5} a W_{A7}), oltre la linea tratteggiata blu, sono messi a punto per l’attività specifica, concentrandosi sull’apprendimento delle feature specifiche dell’attività.\n\n\n\n\n\n\nFigura 13.4: Trasferimento dell’apprendimento in MLOps. Fonte: HarvardX.\n\n\n\nQuesto metodo non solo mitiga la scarsità di dati, ma si adatta anche alla natura decentralizzata dei dati embedded. Inoltre, tecniche come l’apprendimento incrementale sul dispositivo possono personalizzare ulteriormente i modelli in base a casi d’uso specifici. La mancanza di dati ampiamente etichettati in molti domini motiva anche l’uso di tecniche semi-supervisionate, che completano l’approccio di apprendimento per trasferimento. Sfruttando le conoscenze preesistenti e adattandole a compiti specializzati, l’apprendimento per trasferimento all’interno di un framework MLOps consente ai modelli di ottenere prestazioni più elevate con meno risorse, anche in ambienti con vincoli di dati.\nAd esempio, un assistente domestico intelligente può pre-addestrare un modello di riconoscimento audio su clip YouTube pubbliche, il che aiuta a eseguire il bootstrap con conoscenze generali. Quindi trasferisce l’apprendimento a un piccolo campione di dati domestici per classificare elettrodomestici ed eventi personalizzati, specializzandosi nel modello. Il modello si trasforma in una rete neurale leggera ottimizzata per dispositivi abilitati al microfono in tutta la casa.\nPertanto, gli MLOps embedded affrontano sfide acute nella costruzione di set di dati di training, nella progettazione di modelli efficienti e nella distribuzione del calcolo per lo sviluppo del modello rispetto alle impostazioni tradizionali. Dati i vincoli embedded, è necessario un attento adattamento, come l’apprendimento tramite trasferimento e il training distribuito, per addestrare i modelli.\n\n\nValutazione del Modello\nNei tradizionali MLOps centralizzati, i modelli vengono valutati principalmente utilizzando metriche di accuratezza e dataset di test di holdout. Tuttavia, gli MLOps embedded richiedono una valutazione più olistica che tenga conto dei vincoli di sistema oltre all’accuratezza.\nI modelli devono essere testati in anticipo e spesso su hardware edge distribuito che copre diverse configurazioni. Oltre all’accuratezza, fattori come latenza, utilizzo della CPU, ingombro di memoria e consumo energetico sono criteri di valutazione critici. I modelli vengono selezionati in base a compromessi tra queste metriche per soddisfare i vincoli dei dispositivi edge.\nAnche la deriva dei dati deve essere monitorata, dove i modelli addestrati sui dati cloud degradano in accuratezza nel tempo sui dati edge locali. I dati embedded hanno spesso una maggiore variabilità rispetto ai set di addestramento centralizzati. Valutare i modelli su diversi campioni di dati edge operativi è fondamentale. Ma a volte, ottenere i dati per monitorare la deriva può essere difficile se questi dispositivi sono in circolazione e la comunicazione è una barriera.\nIl monitoraggio continuo fornisce visibilità sulle prestazioni del mondo reale dopo l’implementazione, rivelando colli di bottiglia non evidenziati durante i test. Ad esempio, un aggiornamento del modello di una smart camera potrebbe essere inizialmente testato su 100 telecamere e poi annullato se si osserva un calo della precisione, prima di essere esteso a tutte le 5000 telecamere.\n\n\nDistribuzione del Modello\nNegli MLOps tradizionali, le nuove versioni del modello vengono distribuite direttamente sui server tramite endpoint API. Tuttavia, i dispositivi embedded richiedono meccanismi di distribuzione ottimizzati per ricevere modelli aggiornati. Gli aggiornamenti over-the-air (OTA) forniscono un approccio standardizzato alla distribuzione wireless di nuove versioni di software o firmware ai dispositivi embedded. Invece dell’accesso API diretto, i pacchetti OTA consentono la distribuzione remota di modelli e dipendenze come bundle pre-costruiti. In alternativa, l’apprendimento federato consente aggiornamenti del modello senza accesso diretto ai dati di formazione grezzi. Questo approccio decentralizzato ha il potenziale per un miglioramento continuo del modello, ma necessita di piattaforme MLOps robuste.\nLa distribuzione del modello si basa su interfacce fisiche come connessioni seriali USB o UART per dispositivi profondamente embedded privi di connettività. Il packaging del modello segue ancora principi simili agli aggiornamenti OTA, ma il meccanismo di distribuzione è adattato alle capacità dell’hardware edge. Inoltre, spesso vengono utilizzati protocolli OTA specializzati ottimizzati per reti IoT anziché protocolli WiFi o Bluetooth standard. I fattori chiave includono efficienza, affidabilità, sicurezza e telemetria, come il monitoraggio dei progressi, soluzioni come Mender. Io fornisce servizi OTA incentrati su embedded che gestiscono aggiornamenti differenziali tra flotte di dispositivi.\nFigura 13.5 presenta una panoramica di “Model Lifecycle Management” in un contesto MLOps, illustrando il flusso dallo sviluppo (in alto a sinistra) alla distribuzione e al monitoraggio (in basso a destra). Il processo inizia con lo sviluppo ML, in cui il codice e le configurazioni sono “version-controlled”. La gestione dei dati e dei modelli è fondamentale per il processo, coinvolgendo set di dati e repository di funzionalità. Training continuo, conversione del modello e registro del modello sono fasi chiave nell’operazionalizzazione della training. La distribuzione del modello include la fornitura del modello e la gestione dei log di fornitura. Sono in atto meccanismi di allarme per segnalare i problemi, che alimentano il monitoraggio continuo per garantire le prestazioni e l’affidabilità del modello nel tempo. Questo approccio integrato garantisce che i modelli siano sviluppati e mantenuti in modo efficace durante tutto il loro ciclo di vita.\n\n\n\n\n\n\nFigura 13.5: Gestione del ciclo di vita del modello. Fonte: HarvardX.\n\n\n\n\n\n\n13.7.2 Integrazione di Sviluppo e Operazioni\n\nPipeline CI/CD\nNelle MLOps tradizionali, una solida infrastruttura CI/CD come Jenkins e Kubernetes consente l’automazione della pipeline per la distribuzione di modelli su larga scala. Tuttavia, le MLOps embedded necessitano di questa infrastruttura centralizzata e di flussi di lavoro CI/CD più personalizzati per i dispositivi edge.\nLa creazione di pipeline CI/CD deve tenere conto di un panorama frammentato di diverse versioni hardware, firmware e vincoli di connettività. Non esiste una piattaforma standard per orchestrare le pipeline e il supporto degli strumenti è più limitato.\nI test devono coprire in anticipo questo ampio spettro di dispositivi embedded target, il che è difficile senza un accesso centralizzato. Le aziende devono investire molto nell’acquisizione e nella gestione dell’infrastruttura di test nell’ecosistema embedded eterogeneo.\nGli aggiornamenti over-the-air richiedono la configurazione di server specializzati per distribuire in modo sicuro i bundle di modelli ai dispositivi sul campo. Anche le procedure di rollout e rollback devono essere attentamente personalizzate per particolari famiglie di dispositivi.\nCon gli strumenti CI/CD tradizionali meno applicabili, le MLOps embedded si affidano maggiormente a script personalizzati e integrazione. Le aziende adottano approcci diversi, dai framework open source alle soluzioni completamente interne. Una stretta integrazione tra sviluppatori, ingegneri edge e clienti finali stabilisce processi di rilascio affidabili.\nPertanto, gli MLOps embedded non possono sfruttare l’infrastruttura cloud centralizzata per CI/CD. Le aziende combinano pipeline personalizzate, infrastruttura di test e distribuzione OTA per distribuire modelli su sistemi edge frammentati e disconnessi.\n\n\nGestione dell’Infrastruttura\nNei tradizionali MLOps centralizzati, l’infrastruttura comporta l’approvvigionamento di server cloud, GPU e reti ad alta larghezza di banda per carichi di lavoro intensivi come l’addestramento di modelli e la fornitura di previsioni su larga scala. Tuttavia, gli MLOps embedded richiedono un’infrastruttura più eterogenea che si estende su dispositivi edge, gateway e cloud.\nI dispositivi edge come i sensori catturano e preelaborano i dati localmente prima della trasmissione intermittente per evitare di sovraccaricare le reti: i gateway aggregano ed elaborano i dati dei dispositivi prima di inviare sottoinsiemi selezionati al cloud per l’addestramento e l’analisi. Il cloud fornisce gestione centralizzata ed elaborazione supplementare.\nQuesta infrastruttura necessita di una stretta integrazione e bilanciamento dei carichi di elaborazione e comunicazione. La larghezza di banda di rete è limitata, il che richiede un attento filtraggio e compressione dei dati. Le capacità di elaborazione edge sono modeste rispetto al cloud, imponendo vincoli di ottimizzazione.\nLa gestione di aggiornamenti OTA sicuri su grandi flotte di dispositivi presenta sfide all’edge. I rollout devono essere incrementali e pronti per il rollback per una rapida mitigazione. Dato l’ambiente decentralizzato, l’aggiornamento dell’infrastruttura edge richiede coordinamento.\nAd esempio, un impianto industriale può eseguire l’elaborazione di base del segnale sui sensori prima di inviare i dati a un gateway on-prem. Il gateway gestisce l’aggregazione dei dati, il monitoraggio dell’infrastruttura e gli aggiornamenti OTA. Solo i dati curati vengono trasmessi al cloud per analisi avanzate e riaddestramento del modello.\nMLOps embedded richiede una gestione olistica dell’infrastruttura distribuita che abbraccia edge vincolato, gateway e cloud centralizzato. I carichi di lavoro sono bilanciati tra i livelli tenendo conto delle sfide di connettività, elaborazione e sicurezza.\n\n\nComunicazione e Collaborazione\nNelle MLOps tradizionali, la collaborazione tende a concentrarsi su data scientist, ingegneri ML e team DevOps. Tuttavia, le MLOps embedded richiedono un coordinamento interfunzionale più stretto tra ruoli aggiuntivi per affrontare i vincoli di sistema.\nGli ingegneri edge ottimizzano le architetture dei modelli per gli ambienti hardware target. Forniscono feedback ai data scientist durante lo sviluppo in modo che i modelli si adattino anticipatamente alle capacità dei dispositivi. Analogamente, i team di prodotto definiscono i requisiti operativi informati dai contesti degli utenti finali.\nCon più stakeholder nell’ecosistema embedded, i canali di comunicazione devono facilitare la condivisione delle informazioni tra team centralizzati e remoti. Il monitoraggio dei problemi e la gestione dei progetti garantiscono l’allineamento.\nGli strumenti collaborativi ottimizzano i modelli per dispositivi specifici. I data scientist possono registrare i problemi replicati dai dispositivi sul campo in modo che i modelli siano specializzati in dati di nicchia. L’accesso remoto ai dispositivi facilita il debug e la raccolta dati.\nAd esempio, i data scientist possono collaborare con i team sul campo che gestiscono flotte di turbine eoliche per recuperare campioni di dati operativi. Questi dati vengono utilizzati per specializzare i modelli rilevando anomalie specifiche per quella classe di turbine. Gli aggiornamenti dei modelli vengono testati in simulazioni e rivisti dagli ingegneri prima dell’implementazione sul campo.\nGli MLOps embedded impongono un coordinamento continuo tra data scientist, ingegneri, clienti finali e altre parti interessate durante l’intero ciclo di vita del ML. Grazie a una stretta collaborazione, i modelli possono essere personalizzati e ottimizzati per i dispositivi edge mirati.\n\n\n\n13.7.3 Eccellenza operativa\n\nMonitoraggio\nIl monitoraggio MLOps tradizionale si concentra sul monitoraggio centralizzato dell’accuratezza del modello, delle metriche delle prestazioni e della deriva dei dati. Tuttavia, MLOps embedded deve tenere conto del monitoraggio decentralizzato su diversi dispositivi e ambienti edge.\nI dispositivi edge richiedono una raccolta dati ottimizzata per trasmettere metriche di monitoraggio chiave senza sovraccaricare le reti. Le metriche aiutano a valutare le prestazioni del modello, i modelli di dati, l’utilizzo delle risorse e altri comportamenti sui dispositivi remoti.\nCon una connettività limitata, vengono eseguite più analisi all’edge prima di aggregare le informazioni centralmente. I gateway svolgono un ruolo chiave nel monitoraggio dello stato di salute della flotta e nel coordinamento degli aggiornamenti software. Gli indicatori confermati vengono infine propagati al cloud.\nUn’ampia copertura dei dispositivi è impegnativa ma critica. Possono sorgere problemi specifici per determinati tipi di dispositivi, quindi il monitoraggio deve coprire l’intero spettro. Le distribuzioni “canary” aiutano a testare i processi di monitoraggio prima del ridimensionamento.\nIl rilevamento delle anomalie identifica gli incidenti che richiedono il rollback dei modelli o la riqualificazione su nuovi dati. Tuttavia, l’interpretazione degli allarmi richiede la comprensione dei contesti dei dispositivi univoci in base all’input di ingegneri e clienti.\nAd esempio, una casa automobilistica può monitorare i veicoli autonomi per gli indicatori di degradazione del modello utilizzando la memorizzazione nella cache, l’aggregazione e i flussi in tempo reale. Gli ingegneri valutano quando le anomalie identificate garantiscono gli aggiornamenti OTA per migliorare i modelli in base a fattori come la posizione e l’età del veicolo.\nIl monitoraggio MLOps embedded fornisce osservabilità nelle prestazioni del modello e del sistema in ambienti edge decentralizzati. Un’attenta raccolta, analisi e collaborazione dei dati fornisce informazioni significative per mantenere l’affidabilità.\n\n\nGovernance\nNelle MLOps tradizionali, la governance si concentra sulla spiegabilità del modello, la correttezza e la conformità per i sistemi centralizzati. Tuttavia, le MLOps embedded devono anche affrontare le sfide di governance a livello di dispositivo relative alla privacy dei dati, alla sicurezza e alla protezione.\nCon i sensori che raccolgono dati personali e sensibili, la governance dei dati locali sui dispositivi è fondamentale. I controlli di accesso ai dati, l’anonimizzazione e la memorizzazione nella cache crittografata aiutano ad affrontare i rischi per la privacy e la conformità come HIPAA e GDPR. Gli aggiornamenti devono mantenere patch e impostazioni di sicurezza.\nLa governance della sicurezza considera gli impatti fisici del comportamento difettoso del dispositivo. I guasti potrebbero causare condizioni non sicure in veicoli, fabbriche e sistemi critici. Ridondanza, sistemi di sicurezza e sistemi di allarme aiutano a mitigare i rischi.\nLa governance tradizionale, come il monitoraggio dei bias e la spiegabilità del modello, rimane imperativa ma è più difficile da implementare per l’intelligenza artificiale embedded. Anche dare un’occhiata ai modelli black-box su dispositivi a basso consumo pone delle sfide.\nAd esempio, un dispositivo medico può cancellare i dati personali sul dispositivo prima della trasmissione. I rigidi protocolli di governance dei dati approvano gli aggiornamenti del modello. La spiegabilità del modello è limitata, ma l’attenzione è rivolta al rilevamento di comportamenti anomali. I sistemi di backup prevengono i guasti.\nLa governance MLOps embedded deve comprendere privacy, sicurezza, protezione, trasparenza ed etica. Sono necessarie tecniche specializzate e collaborazione di squadra per aiutare a stabilire fiducia e responsabilità all’interno di ambienti decentralizzati.\n\n\n\n13.7.4 Confronto\nTabella 13.2 evidenzia le somiglianze e le differenze tra MLOps Tradizionali e MLOps Embedded sulla base di tutto ciò che abbiamo imparato finora:\n\n\n\nTabella 13.2: Confronto tra le pratiche MLOps Tradizionali e quelle MLOps Embedded.\n\n\n\n\n\n\n\n\n\n\nArea\nMLOps Tradizionali\nMLOps Embedded\n\n\n\n\nGestione dei Dati\nGrandi set di dati, data lake, feature store\nAcquisizione dati sul dispositivo, edge caching ed elaborazione\n\n\nSviluppo del Modello\nSfrutta il deep learning, reti neurali complesse, addestramento GPU\nVincoli sulla complessità del modello, necessità di ottimizzazione\n\n\nDistribuzione\nCluster di server, distribuzione cloud, bassa latenza su larga scala\nDistribuzione OTA su dispositivi, connettività intermittente\n\n\nMonitoraggio\nDashboard, log, allarmi per le prestazioni del modello cloud\nMonitoraggio sul dispositivo di previsioni, utilizzo delle risorse\n\n\nRiqualificazione\nRi-addestramento dei modelli su nuovi dati\nApprendimento federato da dispositivi, ri-addestramento edge\n\n\nInfrastruttura\nInfrastruttura cloud dinamica\nInfrastruttura edge/cloud eterogenea\n\n\nCollaborazione\nMonitoraggio degli esperimenti condivisi e registro dei modelli\nCollaborazione per l’ottimizzazione specifica del dispositivo\n\n\n\n\n\n\nQuindi, mentre Embedded MLOps condivide i principi fondamentali di MLOps, si trova ad affrontare vincoli unici nell’adattare flussi di lavoro e infrastrutture specificamente per dispositivi edge con risorse limitate.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#offerte-commerciali",
    "href": "contents/ops/ops.it.html#offerte-commerciali",
    "title": "13  Operazioni di ML",
    "section": "13.8 Offerte Commerciali",
    "text": "13.8 Offerte Commerciali\nSebbene la comprensione dei principi non sostituisca la loro comprensione, un numero crescente di offerte commerciali aiuta ad alleviare l’onere di creare pipeline ML e integrare strumenti per creare, testare, distribuire e monitorare modelli ML in produzione.\n\n13.8.1 MLOps Tradizionali\nGoogle, Microsoft e Amazon offrono la loro versione di servizi ML gestiti. Questi includono servizi che gestiscono il training e la sperimentazione dei modelli, l’hosting e il ridimensionamento dei modelli e il monitoraggio. Queste offerte sono disponibili tramite un’API e SDK client, nonché tramite interfacce utente Web. Sebbene sia possibile creare le proprie soluzioni MLOps end-to-end utilizzando parti di ciascuno, i maggiori vantaggi in termini di facilità d’uso derivano dal rimanere all’interno di un singolo ecosistema di provider per sfruttare le integrazioni tra servizi.\nForniremo una rapida panoramica dei servizi che rientrano in ogni parte del ciclo di vita MLOps descritto sopra, con esempi di offerte da diversi provider. Lo spazio si sta muovendo molto rapidamente; nuove aziende e prodotti stanno entrando in scena molto rapidamente e questi non sono pensati per fungere da approvazione dell’offerta di una particolare azienda.\n\nGestione dei Dati\nL’archiviazione dei dati e il versioning sono elementi essenziali per qualsiasi offerta commerciale e la maggior parte sfrutta le soluzioni di archiviazione generiche esistenti come S3. Altri utilizzano opzioni più specializzate come l’archiviazione basata su git (ad esempio: Dataset Hub di Hugging Face Questa è un’area in cui i provider semplificano il supporto delle opzioni di archiviazione dei dati dei concorrenti, poiché non vogliono che ciò rappresenti una barriera per l’adozione del resto dei loro servizi MLOps. Ad esempio, la pipeline di addestramento di Vertex AI supporta senza problemi i set di dati archiviati in S3, Google Cloud Buckets o Dataset Hub di Hugging Face.\n\n\nAddestramento del Modello\nI servizi di training gestiti sono il punto di forza dei provider cloud, in quanto forniscono accesso on-demand a hardware che è fuori dalla portata della maggior parte delle aziende più piccole. Fatturano solo l’hardware per il tempo del training, accelerato con GPU accessibili anche ai team di sviluppatori più piccoli. Il controllo che gli sviluppatori hanno sul loro flusso di lavoro del training può variare notevolmente a seconda delle loro esigenze. Alcuni provider hanno servizi che forniscono poco più dell’accesso alle risorse e si affidano allo sviluppatore per gestire autonomamente il ciclo di training, il logging e l’archiviazione dei modelli. Altri servizi sono semplici come puntare a un modello di base e a un set di dati etichettato per avviare un lavoro di messa a punto completamente gestito (ad esempio: Vertex AI Fine Tuning).\nUna parola di avvertimento: A partire dal 2023, la domanda di hardware GPU supera di gran lunga l’offerta e, di conseguenza, i provider cloud stanno razionando l’accesso alle loro GPU. In alcune regioni dei data center, le GPU potrebbero non essere disponibili o richiedere contratti a lungo termine.\n\n\nValutazione del Modello\nLe attività di valutazione del modello in genere comportano il monitoraggio dell’accuratezza, della latenza e dell’utilizzo delle risorse dei modelli sia nelle fasi di test che di produzione. A differenza dei sistemi embedded, i modelli ML distribuiti sul cloud beneficiano di una connettività Internet costante e di capacità di logging illimitate. Di conseguenza, è spesso possibile acquisire e loggare ogni richiesta e risposta. Ciò rende trattabile la riproduzione o la generazione di richieste sintetiche per confrontare modelli e versioni diversi.\nAlcuni provider offrono anche servizi che automatizzano il monitoraggio degli esperimenti di modifica degli iperparametri del modello. Tracciano le esecuzioni e le prestazioni e generano artefatti da queste esecuzioni di training del modello. Esempio: WeightsAndBiases\n\n\nDistribuzione del Modello\nOgni provider in genere ha un servizio denominato “model registry”, in cui vengono archiviati e a cui si accede ai modelli di training. Spesso, questi registri possono anche fornire accesso a modelli di base che sono open source o forniti da grandi aziende tecnologiche (o, in alcuni casi, come LLAMA, entrambi!). Questi registri dei modelli costituiscono un luogo comune per confrontare tutti i modelli e le loro versioni per consentire un facile processo decisionale su quale scegliere per un dato caso d’uso. Esempio: Vertex AI’s model registry\nDal registro dei modelli, distribuire un modello a un endpoint di inferenza è rapido e semplice, e gestisce il provisioning delle risorse, il download del peso del modello e l’hosting di un dato modello. Questi servizi in genere forniscono accesso al modello tramite un’API REST con cui possono essere inviate richieste di inferenza. A seconda del tipo di modello, è possibile configurare risorse specifiche, ad esempio quale tipo di acceleratore GPU potrebbe essere necessario per raggiungere le prestazioni desiderate. Alcuni provider possono anche offrire opzioni di inferenza “serverless” o batch che non necessitano di un endpoint persistente per accedere al modello. Esempio: AWS SageMaker Inference\n\n\n\n13.8.2 MLOps Embedded\nNonostante la proliferazione di nuovi strumenti ML Ops in risposta all’aumento della domanda, le sfide descritte in precedenza hanno limitato la disponibilità di tali strumenti negli ambienti di sistemi embedded. Più di recente, nuovi strumenti come Edge Impulse (Janapa Reddi et al. 2023) hanno reso il processo di sviluppo un po’ più semplice, come descritto di seguito.\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler, Daniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. «Edge Impulse: An MLOps Platform for Tiny Machine Learning». Proceedings of Machine Learning and Systems 5.\n\nEdge Impulse\nEdge Impulse è una piattaforma di sviluppo end-to-end per la creazione e l’implementazione di modelli di apprendimento automatico su dispositivi edge come microcontrollori e piccoli processori. Mira a rendere l’apprendimento automatico embedded più accessibile agli sviluppatori di software tramite la sua interfaccia web di facile utilizzo e strumenti integrati per la raccolta dati, lo sviluppo di modelli, l’ottimizzazione e l’implementazione. Le sue funzionalità principali includono quanto segue:\n\nFlusso di lavoro intuitivo drag-and-drop per la creazione di modelli ML senza bisogno di codifica\nStrumenti per l’acquisizione, l’etichettatura, la visualizzazione e la preelaborazione dei dati dai sensori\nScelta di architetture di modelli, tra cui reti neurali e apprendimento non supervisionato\nTecniche di ottimizzazione dei modelli per bilanciare metriche delle prestazioni e vincoli hardware\nDistribuzione senza soluzione di continuità su dispositivi edge tramite compilazione, SDK e benchmark\nFunzionalità di collaborazione per team e integrazione con altre piattaforme\n\nCon Edge Impulse, gli sviluppatori con competenze limitate in data science possono sviluppare modelli ML specializzati che funzionano in modo efficiente in piccoli ambienti di elaborazione. Fornisce una soluzione completa per la creazione di intelligenza embedded e l’avanzamento del machine learning.\n\nInterfaccia utente\nEdge Impulse è stato progettato con sette principi chiave: accessibilità, funzionalità end-to-end, un approccio incentrato sui dati, interattività, estensibilità, orientamento al team e supporto della community. L’interfaccia utente intuitiva, mostrata in Figura 13.6, guida gli sviluppatori di tutti i livelli di esperienza attraverso il caricamento dei dati, la selezione di un’architettura di modello, l’addestramento del modello e la sua distribuzione su piattaforme hardware pertinenti. Va notato che, come qualsiasi strumento, Edge Impulse è destinato ad assistere, non a sostituire, le considerazioni fondamentali come la determinazione se ML è una soluzione appropriata o l’acquisizione delle competenze di dominio richieste per una determinata applicazione.\n\n\n\n\n\n\nFigura 13.6: Schermata dell’interfaccia utente di Edge Impulse per la creazione di flussi di lavoro dai dati di input alle funzionalità di output.\n\n\n\nCiò che rende Edge Impulse degno di nota è il suo flusso di lavoro end-to-end completo ma intuitivo. Gli sviluppatori iniziano caricando i propri dati tramite upload del file o strumenti di interfaccia a riga di comando (CLI), dopodiché possono esaminare campioni grezzi e visualizzare la distribuzione dei dati nelle divisioni di training e test. Successivamente, gli utenti possono scegliere tra vari “blocchi” di pre-elaborazione per facilitare l’elaborazione del segnale digitale (DSP). Mentre vengono forniti valori di parametri predefiniti, gli utenti possono personalizzare i parametri in base alle proprie esigenze, osservando le considerazioni su memoria e la latenza visualizzate. Gli utenti possono scegliere facilmente la propria architettura di rete neurale, senza bisogno di alcun codice.\nGrazie all’editor visivo della piattaforma, gli utenti possono personalizzare i componenti dell’architettura e i parametri specifici, assicurandosi al contempo che il modello sia ancora addestrabile. Gli utenti possono anche sfruttare algoritmi di apprendimento non supervisionato, come il clustering K-means e i Gaussian Mixture Model (GMM).\n\n\nOttimizzazioni\nPer adattarsi ai vincoli di risorse delle applicazioni TinyML, Edge Impulse fornisce una “matrice di confusione” che riassume le metriche chiave delle prestazioni, tra cui accuratezza per classe e punteggi F1. La piattaforma chiarisce i compromessi tra prestazioni del modello, dimensioni e latenza utilizzando simulazioni in Renode e benchmarking specifici del dispositivo. Per i casi di utilizzo dei dati in streaming, uno strumento di calibrazione delle prestazioni sfrutta un algoritmo genetico per trovare configurazioni di post-elaborazione ideali che bilanciano tassi di falsa accettazione e falso rifiuto. Sono disponibili tecniche come quantizzazione, ottimizzazione del codice e ottimizzazione specifica del dispositivo per i modelli. Per la distribuzione, i modelli possono essere compilati in formati appropriati per i dispositivi edge target. Gli SDK del firmware nativi consentono anche la raccolta diretta dei dati sui dispositivi.\nOltre a semplificare lo sviluppo, Edge Impulse ridimensiona il processo di modellazione stesso. Una funzionalità chiave è EON Tuner, uno strumento di apprendimento automatico automatico (AutoML) che assiste gli utenti nell’ottimizzazione degli iperparametri in base ai vincoli di sistema. Esegue una ricerca casuale per generare rapidamente configurazioni per l’elaborazione del segnale digitale e le fasi di training. I modelli risultanti vengono visualizzati affinché l’utente possa selezionarli in base a metriche di prestazioni, memoria e latenza pertinenti. Per i dati, l’apprendimento attivo facilita il training su un piccolo sottoinsieme etichettato, seguito dall’etichettatura manuale o automatica di nuovi campioni in base alla vicinanza alle classi esistenti. Ciò espande l’efficienza dei dati.\n\n\nCasi d’Uso\nOltre all’accessibilità della piattaforma stessa, il team di Edge Impulse ha ampliato la base di conoscenza dell’ecosistema ML embedded. La piattaforma si presta ad ambienti accademici, essendo stata utilizzata in corsi online e workshop in loco a livello globale. Sono stati pubblicati numerosi casi di studio con casi d’uso di settore e di ricerca, in particolare Oura Ring, che utilizza ML per identificare i pattern del sonno. Il team ha reso i repository open source su GitHub, facilitando la crescita della comunità. Gli utenti possono anche rendere pubblici i progetti per condividere tecniche e scaricare librerie da condividere tramite Apache. L’accesso a livello di organizzazione consente la collaborazione sui flussi di lavoro.\nNel complesso, Edge Impulse è straordinariamente completo e integrabile per i flussi di lavoro degli sviluppatori. Piattaforme più grandi come Google e Microsoft si concentrano maggiormente sul cloud rispetto ai sistemi embedded. I framework TinyMLOps come Neuton AI e Latent AI offrono alcune funzionalità ma non hanno le capacità end-to-end di Edge Impulse. TensorFlow Lite Micro è il motore di inferenza standard grazie alla flessibilità, allo stato open source e all’integrazione di TensorFlow, ma utilizza più memoria e storage rispetto al compilatore EON di Edge Impulse. Altre piattaforme devono essere aggiornate, focalizzate sull’aspetto accademico o più versatili. In sintesi, Edge Impulse mira a semplificare e scalare l’ML embedded tramite una piattaforma accessibile e automatizzata.\n\n\n\nLimitazioni\nSebbene Edge Impulse fornisca una pipeline accessibile per ML embedded, permangono importanti limitazioni e rischi. Una sfida fondamentale è la qualità e la disponibilità dei dati: i modelli sono validi solo quanto i dati utilizzati per addestrarli. Gli utenti devono disporre di campioni etichettati sufficienti che catturino l’ampiezza delle condizioni operative previste e delle modalità di errore. Le anomalie e i valori anomali etichettati sono critici, ma richiedono molto tempo per essere raccolti e identificati. Dati insufficienti o distorti comportano scarse prestazioni del modello indipendentemente dalle capacità dello strumento.\nAnche il deploying su dispositivi a bassa potenza presenta sfide intrinseche. I modelli ottimizzati potrebbero comunque dover richiedere più risorse per MCU a bassissimo consumo. Trovare il giusto equilibrio tra compressione e accuratezza richiede un po’ di sperimentazione. Lo strumento semplifica, ma deve comunque eliminare la necessità di competenze di base in ML ed elaborazione del segnale. Gli ambienti embedded limitano anche il debug e l’interpretabilità rispetto al cloud.\nSebbene siano ottenibili risultati impressionanti, gli utenti non dovrebbero considerare Edge Impulse come una soluzione “Push Button ML”. Un’attenta definizione dell’ambito del progetto, la raccolta dati, la valutazione del modello e il test sono comunque essenziali. Come con qualsiasi strumento di sviluppo, si consigliano aspettative ragionevoli e diligenza nell’applicazione. Tuttavia, Edge Impulse può accelerare la prototipazione e l’implementazione di ML embedded per gli sviluppatori disposti a investire lo sforzo di data science e ingegneria richiesto.\n\n\n\n\n\n\nEsercizio 13.1: Edge Impulse\n\n\n\n\n\nPronti a far salire di livello i vostri piccoli progetti di machine-learning? Combiniamo la potenza di Edge Impulse con le fantastiche visualizzazioni di Weights & Biases (WandB). In questo Colab, si imparerà a monitorare i progressi del training del modello come un professionista! Si immagini di vedere fantastici grafici del modello che diventa più intelligente, confrontando diverse versioni e assicurandovi che la vostra IA funzioni al meglio anche su dispositivi minuscoli.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#casi-di-studio",
    "href": "contents/ops/ops.it.html#casi-di-studio",
    "title": "13  Operazioni di ML",
    "section": "13.9 Casi di Studio",
    "text": "13.9 Casi di Studio\n\n13.9.1 Oura Ring\nOura Ring è un dispositivo indossabile che può misurare l’attività, il sonno e il recupero quando viene posizionato sul dito dell’utente. Utilizzando sensori per tracciare le metriche fisiologiche, il dispositivo utilizza ML embedded per prevedere le fasi del sonno. Per stabilire una base di legittimità nel settore, Oura ha condotto un esperimento di correlazione per valutare il successo del dispositivo nel prevedere le fasi del sonno rispetto a uno studio di base. Ciò ha portato a una solida correlazione del 62% rispetto alla base di riferimento dell’82-83%. Pertanto, il team ha deciso di determinare come migliorare ulteriormente le proprie prestazioni.\nLa prima sfida è stata ottenere dati migliori in termini sia di quantità che di qualità. Avrebbero potuto ospitare uno studio più ampio per ottenere un set di dati più completo, ma i dati sarebbero stati così rumorosi e grandi che sarebbe stato difficile aggregarli, ripulirli e analizzarli. È qui che entra in gioco Edge Impulse.\nAbbiamo condotto un massiccio studio sul sonno su 100 uomini e donne di età compresa tra 15 e 73 anni in tre continenti (Asia, Europa e Nord America). Oltre a indossare l’Oura Ring, i partecipanti erano tenuti a sottoporsi al test PSG [https://it.wikipedia.org/wiki/Polisonnografia] standard del settore, che ha fornito una “etichetta” per questo set di dati. Con 440 notti di sonno da parte di 106 partecipanti, il set di dati ha totalizzato 3.444 ore di lunghezza tra dati Ring e PSG. Con Edge Impulse, Oura ha potuto caricare e consolidare facilmente i dati da diverse fonti in un bucket S3 privato. Sono stati anche in grado di impostare una Data Pipeline per unire campioni di dati in file individuali e preelaborare i dati senza dover eseguire lo “scrubbing” [pulizia] manuale.\nCol tempo risparmiato nell’elaborazione dei dati grazie a Edge Impulse, il team Oura ha potuto concentrarsi sui driver chiave della propria previsione. Hanno estratto solo tre tipi di dati dei sensori: frequenza cardiaca, movimento e temperatura corporea. Dopo aver suddiviso i dati utilizzando la validazione incrociata a cinque livelli e classificato le fasi del sonno, il team ha ottenuto una correlazione del 79%, solo pochi punti percentuali in meno rispetto allo standard. Hanno prontamente distribuito due tipi di modelli di rilevamento del sonno: uno semplificato utilizzando solo l’accelerometro dell’anello e uno più completo sfruttando i segnali periferici mediati dal Autonomic Nervous System (ANS) [sistema nervoso autonomo] e le caratteristiche circadiane [ritmo cardiaco in 24 ore]. Con Edge Impulse, hanno in programma di condurre ulteriori analisi di diversi tipi di attività e sfruttare la scalabilità della piattaforma per continuare a sperimentare con diverse fonti di dati e sottoinsiemi di caratteristiche estratte.\nMentre la maggior parte della ricerca ML si concentra su fasi dominanti del modello come la formazione e la messa a punto, questo caso di studio sottolinea l’importanza di un approccio olistico alle operazioni ML, in cui anche le fasi iniziali di aggregazione dei dati e pre-elaborazione hanno un impatto fondamentale sulla riuscita.\n\n\n13.9.2 ClinAIOps\nDiamo un’occhiata a MLOps nel contesto del monitoraggio medico sanitario per comprendere meglio come MLOps “maturi” in un’implementazione nel mondo reale. In particolare, prendiamo in considerazione il continuous therapeutic monitoring (CTM) [monitoraggio terapeutico continuo] abilitato da dispositivi e sensori indossabili. Il CTM cattura dati fisiologici dettagliati dai pazienti, offrendo l’opportunità di aggiustamenti più frequenti e personalizzati ai trattamenti.\nI sensori indossabili abilitati per ML consentono un monitoraggio continuo fisiologico e dell’attività al di fuori delle cliniche, aprendo possibilità per aggiustamenti terapeutici tempestivi e basati sui dati. Ad esempio, i biosensori indossabili per l’insulina (Psoma e Kanthou 2023) e i sensori ECG da polso per il monitoraggio del glucosio (Li et al. 2021) possono automatizzare il dosaggio di insulina per il diabete, i sensori ECG e PPG da polso possono regolare gli anticoagulanti in base ai modelli di fibrillazione atriale (Attia et al. 2018; Guo et al. 2019), e gli accelerometri che tracciano l’andatura possono innescare cure preventive per la mobilità in declino negli anziani (Liu et al. 2022). La varietà di segnali che ora possono essere catturati passivamente e continuamente consente la titolazione e l’ottimizzazione della terapia su misura per le mutevoli esigenze di ogni paziente. Chiudendo il cerchio tra rilevamento fisiologico e risposta terapeutica con TinyML e apprendimento sul dispositivo, i dispositivi indossabili sono pronti a trasformare molte aree della medicina personalizzata.\n\nPsoma, Sotiria D., e Chryso Kanthou. 2023. «Wearable Insulin Biosensors for Diabetes Management: Advances and Challenges». Biosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, e Zedong Nie. 2021. «Non-invasive Monitoring of Three Glucose Ranges Based On ECG By Using DBSCAN-CNN». #IEEE_J_BHI# 25 (9): 3340–50. https://doi.org/10.1109/jbhi.2021.3072628.\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman, Suraj Kapa, Paul A. Friedman, e Peter A. Noseworthy. 2018. «Noninvasive assessment of dofetilide plasma concentration using a deep learning (neural network) analysis of the surface electrocardiogram: A proof of concept study». PLoS One 13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia, Li Yan, et al. 2019. «Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation». J. Am. Coll. Cardiol. 74 (19): 2365–75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella Jensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022. «Monitoring gait at home with radio waves in Parkinson’s disease: A marker of severity, progression, and medication response». Sci. Transl. Med. 14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\nIl ML è molto promettente nell’analisi dei dati CTM per fornire raccomandazioni basate sui dati per gli aggiustamenti della terapia. Ma semplicemente distribuire modelli di intelligenza artificiale in “silos”, senza integrarli correttamente nei flussi di lavoro clinici e nel processo decisionale, può portare a una scarsa adozione o a risultati non ottimali. In altre parole, pensare solo a MLOps non è sufficiente per renderli utili nella pratica. Questo studio dimostra che sono necessari framework per incorporare intelligenza artificiale e CTM nella pratica clinica reale senza soluzione di continuità.\nQuesto caso di studio analizza “ClinAIOps” come modello per operazioni ML embedded in ambienti clinici complessi (Chen et al. 2023). Forniamo una panoramica del framework e del motivo per cui è necessario, esaminiamo un esempio di applicazione e discutiamo le principali sfide di implementazione relative al monitoraggio del modello, all’integrazione del flusso di lavoro e agli incentivi per gli stakeholder. L’analisi di esempi concreti come ClinAIOps illumina principi cruciali e best practice per operazioni AI affidabili ed efficaci in molti domini.\nI framework MLOps tradizionali non sono sufficienti per integrare il monitoraggio terapeutico continuo (CTM) e l’IA in contesti clinici per alcuni motivi chiave:\n\nMLOps si concentra sul ciclo di vita del modello ML: training, distribuzione, monitoraggio. Ma l’assistenza sanitaria implica il coordinamento di più stakeholder umani, pazienti e medici, non solo modelli.\nMLOps mira ad automatizzare il monitoraggio e la gestione del sistema IT. Tuttavia, l’ottimizzazione della salute del paziente richiede cure personalizzate e supervisione umana, non solo automazione.\nCTM e l’erogazione dell’assistenza sanitaria sono sistemi sociotecnici complessi con molte parti mobili. MLOps non fornisce un framework per coordinare il processo decisionale umano e AI.\nLe considerazioni etiche relative all’AI sanitaria richiedono giudizio umano, supervisione e responsabilità. I framework MLOps non hanno processi per la supervisione etica.\nI dati sanitari dei pazienti sono altamente sensibili e regolamentati. MLOps da solo non garantisce la gestione delle informazioni sanitarie protette secondo gli standard normativi e di privacy.\nLa convalida clinica dei piani di trattamento guidati dall’AI è essenziale per l’adozione da parte del provider. MLOps non incorpora la valutazione specifica del dominio delle raccomandazioni del modello.\nL’ottimizzazione delle metriche sanitarie come i risultati dei pazienti richiede l’allineamento degli incentivi e dei flussi di lavoro delle parti interessate, che MLOps puramente incentrato sulla tecnologia trascura.\n\nPertanto, l’integrazione efficace di AI/ML e CTM nella pratica clinica richiede più di semplici pipeline di modelli e dati; richiede il coordinamento di un complesso processo decisionale collaborativo umano-AI, che ClinAIOps mira ad affrontare tramite i suoi cicli di feedback multi-stakeholder.\n\nCicli di Feedback\nIl framework ClinAIOps, mostrato in Figura 13.7, fornisce questi meccanismi attraverso tre cicli di feedback. I cicli sono utili per coordinare le informazioni dal monitoraggio fisiologico continuo, l’esperienza del medico e la guida dell’IA tramite cicli di feedback, consentendo una medicina di precisione basata sui dati mantenendo al contempo la responsabilità umana. ClinAIOps fornisce un modello per un’efficace simbiosi uomo-IA nell’assistenza sanitaria: il paziente è al centro, fornendo sfide e obiettivi sanitari che informano il regime terapeutico; il medico supervisiona questo regime, fornendo input per gli aggiustamenti basati sui dati di monitoraggio continuo e sui report sanitari del paziente; mentre gli sviluppatori di IA svolgono un ruolo cruciale creando sistemi che generano allarmi per gli aggiornamenti della terapia, che il medico quindi esamina.\nQuesti cicli di feedback, di cui parleremo di seguito, aiutano a mantenere la responsabilità e il controllo del medico sui piani di trattamento esaminando i suggerimenti dell’IA prima che abbiano un impatto sui pazienti. Aiutano a personalizzare dinamicamente il comportamento e gli output del modello di IA in base allo stato di salute mutevole di ciascun paziente. Contribuiscono a migliorare l’accuratezza del modello e l’utilità clinica nel tempo, imparando dalle risposte del medico e del paziente. Facilitano il processo decisionale condiviso e l’assistenza personalizzata durante le interazioni paziente-medico. Consentono una rapida ottimizzazione delle terapie in base a dati frequenti del paziente che i medici non possono analizzare manualmente.\n\n\n\n\n\n\nFigura 13.7: Ciclo ClinAIOps. Fonte: Chen et al. (2023).\n\n\n\n\nCiclo Paziente-IA\nIl ciclo paziente-IA consente un’ottimizzazione frequente della terapia guidata dal monitoraggio fisiologico continuo. Ai pazienti vengono prescritti dispositivi indossabili come smartwatch o cerotti cutanei per raccogliere passivamente segnali sanitari rilevanti. Ad esempio, un paziente diabetico potrebbe avere un monitoraggio continuo del glucosio o un paziente con malattie cardiache potrebbe indossare un cerotto ECG. Un modello di IA analizza i flussi di dati sanitari longitudinali del paziente nel contesto delle sue cartelle cliniche elettroniche: diagnosi, esami di laboratorio, farmaci e dati demografici. Il modello di IA suggerisce modifiche al regime di trattamento su misura per quell’individuo, come la modifica di una dose di farmaco o di un programma di somministrazione. Piccole modifiche entro un intervallo di sicurezza pre-approvato possono essere apportate dal paziente in modo indipendente, mentre le modifiche più importanti vengono prima esaminate dal medico. Questo stretto feedback tra la fisiologia del paziente e la terapia guidata dall’IA consente ottimizzazioni tempestive basate sui dati come raccomandazioni automatizzate sul dosaggio di insulina basate sui livelli di glucosio in tempo reale per i pazienti diabetici.\n\n\nCiclo Clinico-IA\nIl ciclo clinico-IA consente la supervisione clinica sulle raccomandazioni generate dall’IA per garantire sicurezza e responsabilità. Il modello di IA fornisce al medico raccomandazioni terapeutiche e riepiloghi facilmente esaminabili dei dati rilevanti del paziente su cui si basano i suggerimenti. Ad esempio, un’IA può suggerire di ridurre la dose di farmaci per la pressione sanguigna di un paziente iperteso in base a letture costantemente basse. Il medico può accettare, rifiutare o modificare le modifiche alla prescrizione proposte dall’IA. Questo feedback del medico addestra e migliora ulteriormente il modello. Inoltre, il medico stabilisce i limiti per i tipi e l’entità delle modifiche al trattamento che l’IA può raccomandare autonomamente ai pazienti. Esaminando i suggerimenti dell’IA, il medico mantiene l’autorità di trattamento finale in base al proprio giudizio clinico e alla propria responsabilità. Questo ciclo consente loro di supervisionare i casi dei pazienti con l’assistenza dell’IA in modo efficiente.\n\n\nCiclo Paziente-Clinico\nInvece di una raccolta dati di routine, il medico può concentrarsi sull’interpretazione di modelli di dati di alto livello e sulla collaborazione con il paziente per stabilire obiettivi e priorità di salute. L’assistenza AI libererà anche il tempo dei medici, consentendo loro di concentrarsi maggiormente sull’ascolto delle storie e delle preoccupazioni dei pazienti. Ad esempio, il medico può discutere di cambiamenti di dieta ed esercizio fisico con un paziente diabetico per migliorare il controllo del glucosio in base ai dati di monitoraggio continuo. La frequenza degli appuntamenti può anche essere regolata dinamicamente in base ai progressi del paziente anziché seguire un calendario fisso. Liberato dalla raccolta di dati di base, il medico può fornire “coaching” e cure personalizzate a ciascun paziente informato dai suoi dati sanitari continui. La relazione paziente-medico diventa più produttiva e personalizzata.\n\n\n\nEsempio di Ipertensione\nConsideriamo un esempio. Secondo i “Centers for Disease Control and Prevention”, quasi la metà degli adulti soffre di ipertensione (48.1%, 119.9 milioni). L’ipertensione può essere gestita tramite ClinAIOps con l’aiuto di sensori indossabili utilizzando il seguente approccio:\n\nRaccolta Dati\nI dati raccolti includerebbero il monitoraggio continuo della pressione sanguigna tramite un dispositivo indossato al polso dotato di sensori per fotopletismografia (PPG) ed elettrocardiografia (ECG) per stimare la pressione sanguigna (Zhang, Zhou, e Zeng 2017). Il dispositivo indossabile monitorerebbe anche l’attività fisica del paziente tramite accelerometri embedded. Il paziente registrerebbe tutti i farmaci antipertensivi assunti, insieme all’ora e alla dose. Verrebbero inoltre incorporati i dettagli demografici e la storia clinica del paziente dalla sua cartella clinica elettronica (EHR). Questi dati multimodali del mondo reale forniscono un contesto prezioso al modello di intelligenza artificiale per analizzare i modelli di pressione sanguigna del paziente, i livelli di attività, l’aderenza ai farmaci e le risposte alla terapia.\n\nZhang, Qingxue, Dian Zhou, e Xuan Zeng. 2017. «Highly wearable cuff-less blood pressure and heart rate monitoring with single-arm electrocardiogram and photoplethysmogram signals». BioMedical Engineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nModello di Intelligenza Artificiale\nIl modello di intelligenza artificiale sul dispositivo analizzerebbe le tendenze continue della pressione sanguigna del paziente, i modelli circadiani, i livelli di attività fisica, i comportamenti di aderenza ai farmaci e altri contesti. Utilizzerebbe ML per prevedere dosi ottimali di farmaci antipertensivi e tempi per controllare la pressione sanguigna dell’individuo. Il modello invierebbe raccomandazioni di modifica del dosaggio direttamente al paziente per piccoli aggiustamenti o al medico revisore per l’approvazione per modifiche più significative. Osservando il feedback del medico sulle sue raccomandazioni e valutando i risultati della pressione sanguigna risultanti nei pazienti, il modello AI potrebbe essere continuamente riqualificato e migliorato per le prestazioni. L’obiettivo è una gestione della pressione sanguigna completamente personalizzata ottimizzata per le esigenze e le risposte di ciascun paziente.\n\n\nCiclo Paziente-IA\nNel ciclo Paziente-IA, il paziente iperteso riceverebbe notifiche sul suo dispositivo indossabile o sull’app per smartphone collegata che raccomandano modifiche ai suoi farmaci antipertensivi. Per piccole modifiche della dose entro un intervallo di sicurezza predefinito, il paziente potrebbe implementare in modo indipendente la modifica suggerita dal modello di IA al suo regime. Tuttavia, il paziente deve ottenere l’approvazione del medico prima di modificare il dosaggio per modifiche più significative. Fornire raccomandazioni personalizzate e tempestive sui farmaci automatizza un elemento di autogestione dell’ipertensione per il paziente. Può migliorare la sua aderenza al regime e i risultati del trattamento. Il paziente è autorizzato a sfruttare le informazioni dell’IA per controllare meglio la sua pressione sanguigna.\n\n\nCiclo Clinico-IA\nNel ciclo Clinico-IA, il fornitore riceverebbe riepiloghi delle tendenze continue della pressione sanguigna del paziente e visualizzazioni dei suoi modelli di assunzione dei farmaci e dell’aderenza. Esaminano le modifiche al dosaggio antipertensivo suggerite dal modello AI e decidono se approvare, rifiutare o modificare le raccomandazioni prima che raggiungano il paziente. Il medico specifica anche i limiti di quanto l’AI può raccomandare in modo indipendente di modificare i dosaggi senza la supervisione del medico. Se la pressione sanguigna del paziente tende a livelli pericolosi, il sistema avvisa il medico in modo che possa intervenire tempestivamente e modificare i farmaci o richiedere una visita al pronto soccorso. Questo ciclo mantiene responsabilità e sicurezza consentendo al contempo al medico di sfruttare le intuizioni dell’AI mantenendo il medico responsabile dell’approvazione delle principali modifiche al trattamento.\n\n\nCiclo Paziente-Clinico\nNel ciclo Paziente-Clinico, mostrato in Figura 13.8, le visite di persona si concentrerebbero meno sulla raccolta di dati o sulle modifiche di base dei farmaci. Invece, il medico potrebbe interpretare tendenze e modelli di alto livello nei dati di monitoraggio continuo del paziente e avere discussioni mirate su dieta, esercizio fisico, gestione dello stress e altri cambiamenti nello stile di vita per migliorare il controllo della pressione sanguigna in modo olistico. La frequenza degli appuntamenti potrebbe essere ottimizzata dinamicamente in base alla stabilità del paziente anziché seguire un calendario fisso. Poiché il medico non avrebbe bisogno di rivedere tutti i dati granulari, potrebbe concentrarsi sulla fornitura di cure e raccomandazioni personalizzate durante le visite. Con il monitoraggio continuo e l’ottimizzazione assistita dall’intelligenza artificiale dei farmaci tra le visite, la relazione medico-paziente si concentra sugli obiettivi di benessere generale e diventa più incisiva. Questo approccio proattivo e personalizzato basato sui dati può aiutare a evitare complicazioni dell’ipertensione come ictus, insufficienza cardiaca e altre minacce alla salute e al benessere del paziente.\n\n\n\n\n\n\nFigura 13.8: Ciclo interattivo ClinAIOps. Fonte: Chen et al. (2023).\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, e Pranav Rajpurkar. 2023. «A framework for integrating artificial intelligence for clinical care with continuous therapeutic monitoring». Nat. Biomed. Eng., novembre. https://doi.org/10.1038/s41551-023-01115-0.\n\n\n\n\n\nMLOps vs. ClinAIOps\nL’esempio dell’ipertensione illustra bene perché i tradizionali MLOps sono insufficienti per molte applicazioni AI del mondo reale e perché sono invece necessari framework come ClinAIOps.\nCon l’ipertensione, il semplice sviluppo e distribuzione di un modello ML per la regolazione dei farmaci avrebbe successo solo se considerasse il contesto clinico più ampio. Il paziente, il medico e il sistema sanitario hanno preoccupazioni sulla definizione dell’adozione. Il modello AI non può ottimizzare da solo i risultati della pressione sanguigna: richiede l’integrazione con flussi di lavoro, comportamenti e incentivi.\n\nAlcune lacune chiave evidenziate dall’esempio in un approccio MLOps puro:\nIl modello stesso non avrebbe i dati dei pazienti del mondo reale su larga scala per raccomandare trattamenti in modo affidabile. ClinAIOps consente ciò raccogliendo feedback da medici e pazienti tramite monitoraggio continuo.\nI medici si fiderebbero delle raccomandazioni del modello solo con trasparenza, spiegabilità e responsabilità. ClinAIOps mantiene il medico informato per creare fiducia.\nI pazienti hanno bisogno di coaching e motivazione personalizzati, non solo di notifiche AI. Il ciclo paziente-clinico di ClinAIOps facilita questo.\nL’affidabilità dei sensori e l’accuratezza dei dati sarebbero sufficienti solo con la supervisione clinica. ClinAIOps convalida le raccomandazioni.\nLa responsabilità per i risultati del trattamento deve essere chiarita solo con un modello ML. ClinAIOps mantiene la responsabilità umana.\nI sistemi sanitari dovrebbero dimostrare il valore per cambiare i flussi di lavoro. ClinAIOps allinea le parti interessate.\n\nIl caso dell’ipertensione mostra chiaramente la necessità di guardare oltre la formazione e l’implementazione di un modello ML performante per considerare l’intero sistema sociotecnico umano-IA. Questa è la lacuna chiave che ClinAIOps mira a colmare rispetto ai tradizionali MLOps. I tradizionali MLOps sono eccessivamente focalizzati sulla tecnologia per automatizzare lo sviluppo e l’implementazione del modello ML, mentre ClinAIOps incorpora il contesto clinico e il coordinamento umano-IA attraverso cicli di feedback multi-stakeholder.\nTabella 13.3 li confronta. Questa tabella evidenzia come, quando si implementa MLOps, sia necessario considerare più dei semplici modelli ML.\n\n\n\nTabella 13.3: Confronto tra operazioni MLOps e AI per uso clinico.\n\n\n\n\n\n\n\n\n\n\n\nMLOps tradizionali\nClinAIOps\n\n\n\n\nFocus\nSviluppo e distribuzione di modelli ML\nCoordinamento del processo decisionale umano e AI\n\n\nParti interessate\nData scientist, ingegneri IT\nPazienti, medici, sviluppatori AI\n\n\nCicli di feedback\nRiqualificazione del modello, monitoraggio\nPaziente-IA, clinico-IA, paziente-clinico\n\n\nObiettivo\nRendere operative le distribuzioni ML\nOttimizzare i risultati di salute del paziente\n\n\nProcessi\nPipeline e infrastruttura automatizzate\nIntegra flussi di lavoro clinici e supervisione\n\n\nConsiderazioni sui dati\nCreazione di set di dati di formazione\nPrivacy, etica, informazioni sanitarie protette\n\n\nValidazione del modello\nTest delle metriche delle prestazioni del modello\nValutazione clinica delle raccomandazioni\n\n\nImplementazione\nSi concentra sull’integrazione tecnica\nAllinea gli incentivi degli stakeholder umani\n\n\n\n\n\n\n\n\nRiepilogo\nIn ambiti complessi come l’assistenza sanitaria, l’implementazione di successo dell’IA richiede di andare oltre un focus ristretto sul training e il deploying di modelli ML performanti. Come illustrato nell’esempio dell’ipertensione, l’integrazione dell’IA nel mondo reale richiede il coordinamento di diverse parti interessate, l’allineamento degli incentivi, la convalida delle raccomandazioni e il mantenimento della responsabilità. Framework come ClinAIOps, che facilitano il processo decisionale collaborativo tra uomo e IA attraverso cicli di feedback integrati, sono necessari per affrontare queste sfide multiformi. Invece di automatizzare semplicemente le attività, l’IA deve aumentare le capacità umane e i flussi di lavoro clinici. Ciò consente all’IA di avere un impatto positivo sui risultati dei pazienti, sulla salute della popolazione e sull’efficienza dell’assistenza sanitaria.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#conclusione",
    "href": "contents/ops/ops.it.html#conclusione",
    "title": "13  Operazioni di ML",
    "section": "13.10 Conclusione",
    "text": "13.10 Conclusione\nL’ML embedded è pronto a trasformare molti settori abilitando le funzionalità AI direttamente su dispositivi edge come smartphone, sensori e hardware IoT. Tuttavia, lo sviluppo e l’implementazione di modelli TinyML su sistemi embedded con risorse limitate pone sfide uniche rispetto ai tradizionali MLOps basati su cloud.\nQuesto capitolo ha fornito un’analisi approfondita delle principali differenze tra MLOps tradizionali ed embedded nel ciclo di vita del modello, flussi di lavoro di sviluppo, gestione dell’infrastruttura e pratiche operative. Abbiamo discusso di come fattori come connettività intermittente, dati decentralizzati e computing limitato sul dispositivo richiedano tecniche innovative come apprendimento federato, inferenza sul dispositivo e ottimizzazione del modello. Modelli architettonici come apprendimento cross-device e infrastruttura edge-cloud gerarchica aiutano a mitigare i vincoli.\nAttraverso esempi concreti come Oura Ring e ClinAIOps, abbiamo dimostrato i principi applicati per MLOps embedded. I casi di studio hanno evidenziato considerazioni critiche che vanno oltre l’ingegneria ML di base, come l’allineamento degli incentivi delle parti interessate, il mantenimento della responsabilità e il coordinamento del processo decisionale tra uomo e IA. Ciò sottolinea la necessità di un approccio olistico che abbracci sia gli elementi tecnici che quelli umani.\nMentre gli MLOps embedded incontrano degli ostacoli, strumenti emergenti come Edge Impulse e lezioni dai pionieri aiutano ad accelerare l’innovazione del TinyML. Una solida comprensione dei principi fondamentali degli MLOps adattati agli ambienti embedded consentirà a più organizzazioni di superare i vincoli e fornire capacità di intelligenza artificiale distribuita. Man mano che i framework e le best practice maturano, l’integrazione fluida dell’ML nei dispositivi e nei processi edge trasformerà i settori attraverso l’intelligenza localizzata.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/ops/ops.it.html#sec-embedded-aiops-resource",
    "href": "contents/ops/ops.it.html#sec-embedded-aiops-resource",
    "title": "13  Operazioni di ML",
    "section": "13.11 Risorse",
    "text": "13.11 Risorse\nEcco un elenco curato di risorse per supportare studenti e insegnanti nei loro percorsi di apprendimento e insegnamento. Lavoriamo continuamente per espandere questa raccolta e presto aggiungeremo nuovi esercizi.\n\n\n\n\n\n\nSlide\n\n\n\n\n\nQueste slide sono uno strumento prezioso per gli insegnanti per tenere lezioni e per gli studenti per rivedere il materiale al proprio ritmo. Incoraggiamo sia gli studenti che gli insegnanti a sfruttare queste slide per migliorare la loro comprensione e facilitare un trasferimento efficace delle conoscenze.\n\nMLOps, DevOps, and AIOps.\nMLOps overview.\nTiny MLOps.\nMLOps: a use case.\nMLOps: Key Activities and Lifecycle.\nML Lifecycle.\nScaling TinyML: Challenges and Opportunities.\nOperazionalizzazione del Training:\n\nTraining Ops: CI/CD trigger.\nContinuous Integration.\nContinuous Deployment.\nProduction Deployment.\nProduction Deployment: Online Experimentation.\nTraining Ops Impact on MLOps.\n\nDeployment del Modello:\n\nScaling ML Into Production Deployment.\nContainers for Scaling ML Deployment.\nChallenges for Scaling TinyML Deployment: Part 1.\nChallenges for Scaling TinyML Deployment: Part 2.\nModel Deployment Impact on MLOps.\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nVideo 13.1\nVideo 13.2\nVideo 13.3\nVideo 13.4\n\n\n\n\n\n\n\n\n\n\nEsercizi\n\n\n\n\n\nPer rafforzare i concetti trattati in questo capitolo, abbiamo curato una serie di esercizi che sfidano gli studenti ad applicare le proprie conoscenze e ad approfondire la propria comprensione.\n\nEsercizio 13.1\n\n\n\n\n\n\n\n\n\n\nLaboratori\n\n\n\n\n\nOltre agli esercizi, offriamo anche una serie di laboratori pratici che consentono agli studenti di acquisire esperienza pratica con le tecnologie di intelligenza artificiale embedded. Questi laboratori forniscono una guida passo dopo passo, consentendo agli studenti di sviluppare le proprie competenze in un ambiente strutturato e di supporto. Siamo lieti di annunciare che presto saranno disponibili nuovi laboratori, che arricchiranno ulteriormente l’esperienza di apprendimento.\n\nProssimamente.",
    "crumbs": [
      "Deployment",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operazioni di ML</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html",
    "href": "contents/privacy_security/privacy_security.html",
    "title": "14  Security & Privacy",
    "section": "",
    "text": "14.1 Introduction\nMachine learning has evolved substantially from its academic origins, where privacy was not a primary concern. As ML migrated into commercial and consumer applications, the data became more sensitive - encompassing personal information like communications, purchases, and health data. This explosion of data availability fueled rapid advancements in ML capabilities. However, it also exposed new privacy risks, as demonstrated by incidents like the AOL data leak in 2006 and the Cambridge Analytica scandal.\nThese events highlighted the growing need to address privacy in ML systems. In this chapter, we explore privacy and security considerations together, as they are inherently linked in ML:\nFor example, an ML-powered home security camera must secure video feeds against unauthorized access and provide privacy protections to ensure only intended users can view the footage. A breach of either security or privacy could expose private user moments.\nEmbedded ML systems like smart assistants and wearables are ubiquitous and process intimate user data. However, their computational constraints often prevent heavy security protocols. Designers must balance performance needs with rigorous security and privacy standards tailored to embedded hardware limitations.\nThis chapter provides essential knowledge for addressing the complex privacy and security landscape of embedded ML. We will explore vulnerabilities and cover various techniques that enhance privacy and security within embedded systems’ resource constraints.\nWe hope that by building a holistic understanding of risks and safeguards, you will gain the principles to develop secure, ethical, embedded ML applications.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#introduction",
    "href": "contents/privacy_security/privacy_security.html#introduction",
    "title": "14  Security & Privacy",
    "section": "",
    "text": "Privacy refers to controlling access to sensitive user data, such as financial information or biometric data collected by an ML application.\nSecurity protects ML systems and data from hacking, theft, and misuse.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#terminology",
    "href": "contents/privacy_security/privacy_security.html#terminology",
    "title": "14  Security & Privacy",
    "section": "14.2 Terminology",
    "text": "14.2 Terminology\nIn this chapter, we will discuss security and privacy together, so there are key terms that we need to be clear about.\n\nPrivacy: Consider an ML-powered home security camera that identifies and records potential threats. This camera records identifiable information of individuals approaching and potentially entering this home, including faces. Privacy concerns may surround who can access this data.\nSecurity: Consider an ML-powered home security camera that identifies and records potential threats. The security aspect would ensure that hackers cannot access these video feeds and recognition models.\nThreat: Using our home security camera example, a threat could be a hacker trying to access live feeds or stored videos or using false inputs to trick the system.\nVulnerability: A common vulnerability might be a poorly secured network through which the camera connects to the internet, which could be exploited to access the data.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#historical-precedents",
    "href": "contents/privacy_security/privacy_security.html#historical-precedents",
    "title": "14  Security & Privacy",
    "section": "14.3 Historical Precedents",
    "text": "14.3 Historical Precedents\nWhile the specifics of machine learning hardware security can be distinct, the embedded systems field has a history of security incidents that provide critical lessons for all connected systems, including those using ML. Here are detailed explorations of past breaches:\n\n14.3.1 Stuxnet\nIn 2010, something unexpected was found on a computer in Iran - a very complicated computer virus that experts had never seen before. Stuxnet was a malicious computer worm that targeted supervisory control and data acquisition (SCADA) systems and was designed to damage Iran’s nuclear program (Farwell e Rohozinski 2011). Stuxnet was using four “zero-day exploits” - attacks that take advantage of secret weaknesses in software that no one knows about yet. This made Stuxnet very sneaky and hard to detect.\n\nFarwell, James P., e Rafal Rohozinski. 2011. «Stuxnet and the Future of Cyber War». Survival 53 (1): 23–40. https://doi.org/10.1080/00396338.2011.555586.\nBut Stuxnet wasn’t designed to steal information or spy on people. Its goal was physical destruction - to sabotage centrifuges at Iran’s Natanz nuclear plant! So how did the virus get onto computers at the Natanz plant, which was supposed to be disconnected from the outside world for security? Experts think someone inserted a USB stick containing Stuxnet into the internal Natanz network. This allowed the virus to “jump” from an outside system onto the isolated nuclear control systems and wreak havoc.\nStuxnet was incredibly advanced malware built by national governments to cross from the digital realm into real-world infrastructure. It specifically targeted important industrial machines, where embedded machine learning is highly applicable in a way never done before. The virus provided a wake-up call about how sophisticated cyberattacks could now physically destroy equipment and facilities.\nThis breach was significant due to its sophistication; Stuxnet specifically targeted programmable logic controllers (PLCs) used to automate electromechanical processes such as the speed of centrifuges for uranium enrichment. The worm exploited vulnerabilities in the Windows operating system to gain access to the Siemens Step7 software controlling the PLCs. Despite not being a direct attack on ML systems, Stuxnet is relevant for all embedded systems as it showcases the potential for state-level actors to design attacks that bridge the cyber and physical worlds with devastating effects.\n\n\n14.3.2 Jeep Cherokee Hack\nThe Jeep Cherokee hack was a groundbreaking event demonstrating the risks inherent in increasingly connected automobiles (Miller 2019). In a controlled demonstration, security researchers remotely exploited a vulnerability in the Uconnect entertainment system, which had a cellular connection to the internet. They were able to control the vehicle’s engine, transmission, and brakes, alarming the automotive industry into recognizing the severe safety implications of cyber vulnerabilities in vehicles. Video 14.1 below is a short documentary of the attack.\n\nMiller, Charlie. 2019. «Lessons learned from hacking a car». IEEE Design &amp; Test 36 (6): 7–9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\n\n\n\n\nVideo 14.1: Jeep Cherokee Hack\n\n\n\n\n\n\nWhile this wasn’t an attack on an ML system per se, the reliance of modern vehicles on embedded systems for safety-critical functions has significant parallels to the deployment of ML in embedded systems, underscoring the need for robust security at the hardware level.\n\n\n14.3.3 Mirai Botnet\nThe Mirai botnet involved the infection of networked devices such as digital cameras and DVR players (Antonakakis et al. 2017). In October 2016, the botnet was used to conduct one of the largest DDoS attacks, disrupting internet access across the United States. The attack was possible because many devices used default usernames and passwords, which were easily exploited by the Mirai malware to control the devices. Video 14.2 explains how the Mirai Botnet works.\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie Bursztein, Jaime Cochran, Zakir Durumeric, et al. 2017. «Understanding the mirai botnet». In 26th USENIX security symposium (USENIX Security 17), 1093–1110.\n\n\n\n\n\n\nVideo 14.2: Mirai Botnet\n\n\n\n\n\n\nAlthough the devices were not ML-based, the incident is a stark reminder of what can happen when numerous embedded devices with poor security controls are networked, which is becoming more common with the growth of ML-based IoT devices.\n\n\n14.3.4 Implications\nThese historical breaches demonstrate the cascading effects of hardware vulnerabilities in embedded systems. Each incident offers a precedent for understanding the risks and designing better security protocols. For instance, the Mirai botnet highlights the immense destructive potential when threat actors can gain control over networked devices with weak security, a situation becoming increasingly common with ML systems. Many current ML devices function as “edge” devices meant to collect and process data locally before sending it to the cloud. Much like the cameras and DVRs compromised by Mirai, edge ML devices often rely on embedded hardware like ARM processors and run lightweight O.S. like Linux. Securing the device credentials is critical.\nSimilarly, the Jeep Cherokee hack was a watershed moment for the automotive industry. It exposed serious vulnerabilities in the growing network-connected vehicle systems and their lack of isolation from core drive systems like brakes and steering. In response, auto manufacturers invested heavily in new cybersecurity measures, though gaps likely remain.\nChrysler did a recall to patch the vulnerable Uconnect software, allowing the remote exploit. This included adding network-level protections to prevent unauthorized external access and compartmentalizing in-vehicle systems to limit lateral movement. Additional layers of encryption were added for commands sent over the CAN bus within vehicles.\nThe incident also spurred the creation of new cybersecurity standards and best practices. The Auto-ISAC was established for automakers to share intelligence, and the NHTSA guided management risks. New testing and audit procedures were developed to assess vulnerabilities proactively. The aftereffects continue to drive change in the automotive industry as cars become increasingly software-defined.\nUnfortunately, manufacturers often overlook security when developing new ML edge devices - using default passwords, unencrypted communications, unsecured firmware updates, etc. Any such vulnerabilities could allow attackers to gain access and control devices at scale by infecting them with malware. With a botnet of compromised ML devices, attackers could leverage their aggregated computational power for DDoS attacks on critical infrastructure.\nWhile these events didn’t directly involve machine learning hardware, the principles of the attacks carry over to ML systems, which often involve similar embedded devices and network architectures. As ML hardware is increasingly integrated with the physical world, securing it against such breaches is paramount. The evolution of security measures in response to these incidents provides valuable insights into protecting current and future ML systems from analogous vulnerabilities.\nThe distributed nature of ML edge devices means threats can propagate quickly across networks. And if devices are being used for mission-critical purposes like medical devices, industrial controls, or self-driving vehicles, the potential physical damage from weaponized ML bots could be severe. Just like Mirai demonstrated the dangerous potential of poorly secured IoT devices, the litmus test for ML hardware security will be how vulnerable or resilient these devices are to worm-like attacks. The stakes are raised as ML spreads to safety-critical domains, putting the onus on manufacturers and system operators to incorporate the lessons from Mirai.\nThe lesson is the importance of designing for security from the outset and having layered defenses. The Jeep case highlights potential vulnerabilities for ML systems around externally facing software interfaces and isolation between subsystems. Manufacturers of ML devices and platforms should assume a similar proactive and comprehensive approach to security rather than leaving it as an afterthought. Rapid response and dissemination of best practices will be crucial as threats evolve.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#security-threats-to-ml-models",
    "href": "contents/privacy_security/privacy_security.html#security-threats-to-ml-models",
    "title": "14  Security & Privacy",
    "section": "14.4 Security Threats to ML Models",
    "text": "14.4 Security Threats to ML Models\nML models face security risks that can undermine their integrity, performance, and trustworthiness if not adequately addressed. While there are several different threats, the primary threats include: Model theft, where adversaries steal the proprietary model parameters and the sensitive data they contain. Data poisoning, which compromises models through data tampering. Adversarial attacks deceive the model to make incorrect or unwanted predictions.\n\n14.4.1 Model Theft\nModel theft occurs when an attacker gains unauthorized access to a deployed ML model. The concern here is the theft of the model’s structure and trained parameters and the proprietary data it contains (Ateniese et al. 2015). Model theft is a real and growing threat, as demonstrated by cases like ex-Google engineer Anthony Levandowski, who allegedly stole Waymo’s self-driving car designs and started a competing company. Beyond economic impacts, model theft can seriously undermine privacy and enable further attacks.\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, e Giovanni Felici. 2015. «Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers». Int. J. Secur. Netw. 10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\nFor instance, consider an ML model developed for personalized recommendations in an e-commerce application. If a competitor steals this model, they gain insights into business analytics, customer preferences, and even trade secrets embedded within the model’s data. Attackers could leverage stolen models to craft more effective inputs for model inversion attacks, deducing private details about the model’s training data. A cloned e-commerce recommendation model could reveal customer purchase behaviors and demographics.\nTo understand model inversion attacks, consider a facial recognition system used to grant access to secured facilities. The system is trained on a dataset of employee photos. An attacker could infer features of the original dataset by observing the model’s output to various inputs. For example, suppose the model’s confidence level for a particular face is significantly higher for a given set of features. In that case, an attacker might deduce that someone with those features is likely in the training dataset.\nThe methodology of model inversion typically involves the following steps:\n\nAccessing Model Outputs: The attacker queries the ML model with input data and observes the outputs. This is often done through a legitimate interface, like a public API.\nAnalyzing Confidence Scores: For each input, the model provides a confidence score that reflects how similar the input is to the training data.\nReverse-Engineering: By analyzing the confidence scores or output probabilities, attackers can use optimization techniques to reconstruct what they believe is close to the original input data.\n\nOne historical example of such a vulnerability being explored was the research on inversion attacks against the U.S. Netflix Prize dataset, where researchers demonstrated that it was possible to learn about an individual’s movie preferences, which could lead to privacy breaches (Narayanan e Shmatikov 2006).\n\nNarayanan, Arvind, e Vitaly Shmatikov. 2006. «How to break anonymity of the netflix prize dataset». arXiv preprint cs/0610105.\nModel theft implies that it could lead to economic losses, undermine competitive advantage, and violate user privacy. There’s also the risk of model inversion attacks, where an adversary could input various data into the stolen model to infer sensitive information about the training data.\nBased on the desired asset, model theft attacks can be divided into two categories: exact model properties and approximate model behavior.\n\nStealing Exact Model Properties\nIn these attacks, the objective is to extract information about concrete metrics, such as a network’s learned parameters, fine-tuned hyperparameters, and the model’s internal layer architecture (Oliynyk, Mayer, e Rauber 2023).\n\nLearned Parameters: Adversaries aim to steal a model’s learned knowledge (weights and biases) to replicate it. Parameter theft is generally used with other attacks, such as architecture theft, which lacks parameter knowledge.\nFine-Tuned Hyperparameters: Training is costly, and identifying the optimal configuration of hyperparameters (such as learning rate and regularization) can be time-consuming and resource-intensive. Consequently, stealing a model’s optimized hyperparameters enables adversaries to replicate the model without incurring the exact development costs.\nModel Architecture: This attack concerns the specific design and structure of the model, such as layers, neurons, and connectivity patterns. Beyond reducing associated training costs, this theft poses a severe risk to intellectual property, potentially undermining a company’s competitive advantage. Architecture theft can be achieved by exploiting side-channel attacks (discussed later).\n\n\n\nStealing Approximate Model Behavior\nInstead of extracting exact numerical values of the model’s parameters, these attacks aim to reproduce the model’s behavior (predictions and effectiveness), decision-making, and high-level characteristics (Oliynyk, Mayer, e Rauber 2023). These techniques aim to achieve similar outcomes while allowing for internal deviations in parameters and architecture. Types of approximate behavior theft include gaining the same level of effectiveness and obtaining prediction consistency.\n\nOliynyk, Daryna, Rudolf Mayer, e Andreas Rauber. 2023. «I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences». ACM Comput. Surv. 55 (14s): 1–41. https://doi.org/10.1145/3595292.\n\nLevel of Effectiveness: Attackers aim to replicate the model’s decision-making capabilities rather than focus on the precise parameter values. This is done through understanding the overall behavior of the model. Consider a scenario where an attacker wants to copy the behavior of an image classification model. By analyzing the model’s decision boundaries, the attack tunes its model to reach an effectiveness comparable to the original model. This could entail analyzing 1) the confusion matrix to understand the balance of prediction metrics (true positive, true negative, false positive, false negative) and 2) other performance metrics, such as F1 score and precision, to ensure that the two models are comparable.\nPrediction Consistency: The attacker tries to align their model’s prediction patterns with the target model’s. This involves matching prediction outputs (both positive and negative) on the same set of inputs and ensuring distributional consistency across different classes. For instance, consider a natural language processing (NLP) model that generates sentiment analysis for move reviews (labels reviews as positive, neutral, or negative). The attacker will try to fine-tune their model to match the prediction of the original models on the same set of movie reviews. This includes ensuring that the model makes the same mistakes (mispredictions) that the targeted model makes.\n\n\n\nCase Study\nIn 2018, Tesla filed a lawsuit against self-driving car startup Zoox, alleging former employees stole confidential data and trade secrets related to Tesla’s autonomous driving assistance system.\nTesla claimed that several of its former employees took over 10 G.B. of proprietary data, including ML models and source code, before joining Zoox. This allegedly included one of Tesla’s crucial image recognition models for identifying objects.\nThe theft of this sensitive proprietary model could help Zoox shortcut years of ML development and duplicate Tesla’s capabilities. Tesla argued this theft of I.P. caused significant financial and competitive harm. There were also concerns it could allow model inversion attacks to infer private details about Tesla’s testing data.\nThe Zoox employees denied stealing any proprietary information. However, the case highlights the significant risks of model theft—enabling the cloning of commercial models, causing economic impacts, and opening the door for further data privacy violations.\n\n\n\n14.4.2 Data Poisoning\nData poisoning is an attack where the training data is tampered with, leading to a compromised model (Biggio, Nelson, e Laskov 2012). Attackers can modify existing training examples, insert new malicious data points, or influence the data collection process. The poisoned data is labeled in such a way as to skew the model’s learned behavior. This can be particularly damaging in applications where ML models make automated decisions based on learned patterns. Beyond training sets, poisoning tests and validation data can allow adversaries to boost reported model performance artificially.\n\nBiggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. «Poisoning Attacks against Support Vector Machines». In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\nThe process usually involves the following steps:\n\nInjection: The attacker adds incorrect or misleading examples into the training set. These examples are often designed to look normal to cursory inspection but have been carefully crafted to disrupt the learning process.\nTraining: The ML model trains on this manipulated dataset and develops skewed understandings of the data patterns.\nDeployment: Once the model is deployed, the corrupted training leads to flawed decision-making or predictable vulnerabilities the attacker can exploit.\n\nThe impacts of data poisoning extend beyond just classification errors or accuracy drops. For instance, if incorrect or malicious data is introduced into a traffic sign recognition system’s training set, the model may learn to misclassify stop signs as yield signs, which can have dangerous real-world consequences, especially in embedded autonomous systems like autonomous vehicles.\nData poisoning can degrade a model’s accuracy, force it to make incorrect predictions or cause it to behave unpredictably. In critical applications like healthcare, such alterations can lead to significant trust and safety issues.\nThere are six main categories of data poisoning (Oprea, Singhal, e Vassilev 2022):\n\nOprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. «Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?» Computer 55 (11): 94–99. https://doi.org/10.1109/mc.2022.3190787.\n\nAvailability Attacks: These attacks seek to compromise a model’s overall functionality. They cause it to misclassify most testing samples, rendering the model unusable for practical applications. An example is label flipping, where labels of a specific, targeted class are replaced with labels from a different one.\nTargeted Attacks: Unlike availability attacks, targeted attacks aim to compromise a small number of the testing samples. So, the effect is localized to a limited number of classes, while the model maintains the same original level of accuracy on most of the classes. The targeted nature of the attack requires the attacker to possess knowledge of the model’s classes, making detecting these attacks more challenging.\nBackdoor Attacks: In these attacks, an adversary targets specific patterns in the data. The attacker introduces a backdoor (a malicious, hidden trigger or pattern) into the training data, such as altering certain features in structured data or a pattern of pixels at a fixed position. This causes the model to associate the malicious pattern with specific labels. As a result, when the model encounters test samples that contain a malicious pattern, it makes false predictions, highlighting the importance of caution and prevention in the role of data security professionals.\nSubpopulation Attacks: Attackers selectively choose to compromise a subset of the testing samples while maintaining accuracy on the rest of the samples. You can think of these attacks as a combination of availability and targeted attacks: performing availability attacks (performance degradation) within the scope of a targeted subset. Although subpopulation attacks may seem very similar to targeted attacks, the two have clear differences:\nScope: While targeted attacks target a selected set of samples, subpopulation attacks target a general subpopulation with similar feature representations. For example, in a targeted attack, an actor inserts manipulated images of a ‘speed bump’ warning sign (with carefully crafted perturbation or patterns), which causes an autonomous car to fail to recognize such a sign and slow down. On the other hand, manipulating all samples of people with a British accent so that a speech recognition model would misclassify a British person’s speech is an example of a subpopulation attack.\nKnowledge: While targeted attacks require a high degree of familiarity with the data, subpopulation attacks require less intimate knowledge to be effective.\n\n\nCase Study 1\nIn 2017, researchers demonstrated a data poisoning attack against a popular toxicity classification model called Perspective (Hosseini et al. 2017). This ML model detects toxic comments online.\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, e Radha Poovendran. 2017. «Deceiving google’s perspective api built for detecting toxic comments». ArXiv preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\nThe researchers added synthetically generated toxic comments with slight misspellings and grammatical errors to the model’s training data. This slowly corrupted the model, causing it to misclassify increasing numbers of severely toxic inputs as non-toxic over time.\nAfter retraining on the poisoned data, the model’s false negative rate increased from 1.4% to 27% - allowing extremely toxic comments to bypass detection. The researchers warned this stealthy data poisoning could enable the spread of hate speech, harassment, and abuse if deployed against real moderation systems.\nThis case highlights how data poisoning can degrade model accuracy and reliability. For social media platforms, a poisoning attack that impairs toxicity detection could lead to the proliferation of harmful content and distrust of ML moderation systems. The example demonstrates why securing training data integrity and monitoring for poisoning is critical across application domains.\n\n\nCase Study 2\nInterestingly enough, data poisoning attacks are not always malicious (Shan et al. 2023). Nightshade, a tool developed by a team led by Professor Ben Zhao at the University of Chicago, utilizes data poisoning to help artists protect their art against scraping and copyright violations by generative A.I. models. Artists can use the tool to modify their images subtly before uploading them online.\nWhile these changes are imperceptible to the human eye, they can significantly degrade the performance of generative AI models when integrated into the training data. Generative models can be manipulated to produce unrealistic or nonsensical outputs. For example, with just 300 corrupted images, the University of Chicago researchers could deceive the latest Stable Diffusion model into generating images of canines resembling felines or bovines when prompted for automobiles.\nAs the quantity of corrupted images online grows, the efficacy of models trained on scraped data will decline exponentially. Initially, identifying corrupted data is challenging and necessitates manual intervention. Subsequently, contamination spreads rapidly to related concepts as generative models establish connections between words and their visual representations. Consequently, a corrupted image of a “car” could propagate into generated images linked to terms such as “truck,” “train,” and “bus.”\nOn the other hand, this tool can be used maliciously and affect legitimate generative model applications. This shows the very challenging and novel nature of machine learning attacks.\nFigura 17.26 demonstrates the effects of different levels of data poisoning (50 samples, 100 samples, and 300 samples of poisoned images) on generating images in various categories. Notice how the images start deforming and deviating from the desired category. For example, after 300 poison samples, a car prompt generates a cow.\n\n\n\n\n\n\nFigura 14.1: Data poisoning. Source: Shan et al. (2023).\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, e Ben Y Zhao. 2023. «Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models». ArXiv preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\n\n\n\n14.4.3 Adversarial Attacks\nAdversarial attacks aim to trick models into making incorrect predictions by providing them with specially crafted, deceptive inputs (called adversarial examples) (Parrish et al. 2023). By adding slight perturbations to input data, adversaries can “hack” a model’s pattern recognition and deceive it. These are sophisticated techniques where slight, often imperceptible alterations to input data can trick an ML model into making a wrong prediction.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. «Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models». ArXiv preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. «Zero-Shot Text-to-Image Generation». In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. «High-Resolution Image Synthesis with Latent Diffusion Models». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\nOne can generate prompts that lead to unsafe images in text-to-image models like DALLE (Ramesh et al. 2021) or Stable Diffusion (Rombach et al. 2022). For example, by altering the pixel values of an image, attackers can deceive a facial recognition system into identifying a face as a different person.\nAdversarial attacks exploit the way ML models learn and make decisions during inference. These models work on the principle of recognizing patterns in data. An adversary crafts malicious inputs with perturbations to mislead the model’s pattern recognition—essentially ‘hacking’ the model’s perceptions.\nAdversarial attacks fall under different scenarios:\n\nWhitebox Attacks: The attacker has comprehensive knowledge of the target model’s internal workings, including the training data, parameters, and architecture. This extensive access facilitates the exploitation of the model’s vulnerabilities. The attacker can leverage specific and subtle weaknesses to construct highly effective adversarial examples.\nBlackbox Attacks: In contrast to whitebox attacks, in blackbox attacks, the attacker has little to no knowledge of the target model. The adversarial actor must carefully observe the model’s output behavior to carry out the attack.\nGreybox Attacks: These attacks occupy a spectrum between black-box and white-box attacks. The adversary possesses partial knowledge of the target model’s internal structure. For instance, the attacker might know the training data but lack information about the model’s architecture or parameters. In practical scenarios, most attacks fall within this grey area.\n\nThe landscape of machine learning models is complex and broad, especially given their relatively recent integration into commercial applications. This rapid adoption, while transformative, has brought to light numerous vulnerabilities within these models. Consequently, various adversarial attack methods have emerged, each strategically exploiting different aspects of different models. Below, we highlight a subset of these methods, showcasing the multifaceted nature of adversarial attacks on machine learning models:\n\nGenerative Adversarial Networks (GANs) are deep learning models consisting of two networks competing against each other: a generator and a discriminator (Goodfellow et al. 2020). The generator tries to synthesize realistic data while the discriminator evaluates whether they are real or fake. GANs can be used to craft adversarial examples. The generator network is trained to produce inputs that the target model misclassifies. These GAN-generated images can then attack a target classifier or detection model. The generator and the target model are engaged in a competitive process, with the generator continually improving its ability to create deceptive examples and the target model enhancing its resistance to such examples. GANs provide a robust framework for crafting complex and diverse adversarial inputs, illustrating the adaptability of generative models in the adversarial landscape.\nTransfer Learning Adversarial Attacks exploit the knowledge transferred from a pre-trained model to a target model, creating adversarial examples that can deceive both models. These attacks pose a growing concern, particularly when adversaries have knowledge of the feature extractor but lack access to the classification head (the part or layer responsible for making the final classifications). Referred to as “headless attacks,” these transferable adversarial strategies leverage the expressive capabilities of feature extractors to craft perturbations while oblivious to the label space or training data. The existence of such attacks underscores the importance of developing robust defenses for transfer learning applications, especially since pre-trained models are commonly used (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Commun. ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. «Headless Horseman: Adversarial Attacks on Transfer Learning Models». In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\nCase Study\nIn 2017, researchers conducted experiments by placing small black and white stickers on stop signs (Eykholt et al. 2017). When viewed by a normal human eye, the stickers did not obscure the sign or prevent interpretability. However, when images of the stickers stop signs were fed into standard traffic sign classification ML models, they were misclassified as speed limit signs over 85% of the time.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. «Robust Physical-World Attacks on Deep Learning Models». ArXiv preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\nThis demonstration showed how simple adversarial stickers could trick ML systems into misreading critical road signs. If deployed realistically, these attacks could endanger public safety, causing autonomous vehicles to misinterpret stop signs as speed limits. Researchers warned this could potentially cause dangerous rolling stops or acceleration into intersections.\nThis case study provides a concrete illustration of how adversarial examples exploit the pattern recognition mechanisms of ML models. By subtly altering the input data, attackers can induce incorrect predictions and pose significant risks to safety-critical applications like self-driving cars. The attack’s simplicity demonstrates how even minor, imperceptible changes can lead models astray. Consequently, developers must implement robust defenses against such threats.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#security-threats-to-ml-hardware",
    "href": "contents/privacy_security/privacy_security.html#security-threats-to-ml-hardware",
    "title": "14  Security & Privacy",
    "section": "14.5 Security Threats to ML Hardware",
    "text": "14.5 Security Threats to ML Hardware\nA systematic examination of security threats to embedded machine learning hardware is essential to comprehensively understanding potential vulnerabilities in ML systems. Initially, hardware vulnerabilities arising from intrinsic design flaws that can be exploited will be explored. This foundational knowledge is crucial for recognizing the origins of hardware weaknesses. Following this, physical attacks will be examined, representing the most direct and overt methods of compromising hardware integrity. Building on this, fault injection attacks will be analyzed, demonstrating how deliberate manipulations can induce system failures.\nAdvancing to side-channel attacks next will show the increasing complexity, as these rely on exploiting indirect information leakages, requiring a nuanced understanding of hardware operations and environmental interactions. Leaky interfaces will show how external communication channels can become vulnerable, leading to accidental data exposures. Counterfeit hardware discussions benefit from prior explorations of hardware integrity and exploitation techniques, as they often compound these issues with additional risks due to their questionable provenance. Finally, supply chain risks encompass all concerns above and frame them within the context of the hardware’s journey from production to deployment, highlighting the multifaceted nature of hardware security and the need for vigilance at every stage.\nTabella 14.1 overview table summarizing the topics:\n\n\n\nTabella 14.1: Threat types on hardware security.\n\n\n\n\n\n\n\n\n\n\nThreat Type\nDescription\nRelevance to ML Hardware Security\n\n\n\n\nHardware Bugs\nIntrinsic flaws in hardware designs that can compromise system integrity.\nFoundation of hardware vulnerability.\n\n\nPhysical Attacks\nDirect exploitation of hardware through physical access or manipulation.\nBasic and overt threat model.\n\n\nFault-injection Attacks\nInduction of faults to cause errors in hardware operation, leading to potential system crashes.\nSystematic manipulation leading to failure.\n\n\nSide-Channel Attacks\nExploitation of leaked information from hardware operation to extract sensitive data.\nIndirect attack via environmental observation.\n\n\nLeaky Interfaces\nVulnerabilities arising from interfaces that expose data unintentionally.\nData exposure through communication channels.\n\n\nCounterfeit Hardware\nUse of unauthorized hardware components that may have security flaws.\nCompounded vulnerability issues.\n\n\nSupply Chain Risks\nRisks introduced through the hardware lifecycle, from production to deployment.\nCumulative & multifaceted security challenges.\n\n\n\n\n\n\n\n14.5.1 Hardware Bugs\nHardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. An example of such vulnerabilities came to light with the discovery of Meltdown and Spectre—two hardware vulnerabilities that exploit critical vulnerabilities in modern processors. These bugs allow attackers to bypass the hardware barrier that separates applications, allowing a malicious program to read the memory of other programs and the operating system.\nMeltdown (Kocher et al. 2019a) and Spectre (Kocher et al. 2019b) work by taking advantage of optimizations in modern CPUs that allow them to speculatively execute instructions out of order before validity checks have been completed. This reveals data that should be inaccessible, which the attack captures through side channels like caches. The technical complexity demonstrates the difficulty of eliminating vulnerabilities even with extensive validation.\n\n———, et al. 2019a. «Spectre Attacks: Exploiting Speculative Execution». In 2019 IEEE Symposium on Security and Privacy (SP). IEEE. https://doi.org/10.1109/sp.2019.00002.\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, et al. 2019b. «Spectre Attacks: Exploiting Speculative Execution». In 2019 IEEE Symposium on Security and Privacy (SP). IEEE. https://doi.org/10.1109/sp.2019.00002.\nIf an ML system is processing sensitive data, such as personal user information or proprietary business analytics, Meltdown and Spectre represent a real and present danger to data security. Consider the case of an ML accelerator card designed to speed up machine learning processes, such as the ones we discussed in the A.I. Hardware chapter. These accelerators work with the CPU to handle complex calculations, often related to data analytics, image recognition, and natural language processing. If such an accelerator card has a vulnerability akin to Meltdown or Spectre, it could leak the data it processes. An attacker could exploit this flaw not just to siphon off data but also to gain insights into the ML model’s workings, including potentially reverse-engineering the model itself (thus, going back to the issue of model theft.\nA real-world scenario where this could be devastating would be in the healthcare industry. ML systems routinely process highly sensitive patient data to help diagnose, plan treatment, and forecast outcomes. A bug in the system’s hardware could lead to the unauthorized disclosure of personal health information, violating patient privacy and contravening strict regulatory standards like the Health Insurance Portability and Accountability Act (HIPAA)\nThe Meltdown and Spectre vulnerabilities are stark reminders that hardware security is not just about preventing unauthorized physical access but also about ensuring that the hardware’s architecture does not become a conduit for data exposure. Similar hardware design flaws regularly emerge in CPUs, accelerators, memory, buses, and other components. This necessitates ongoing retroactive mitigations and performance trade-offs in deployed systems. Proactive solutions like confidential computing architectures could mitigate entire classes of vulnerabilities through fundamentally more secure hardware design. Thwarting hardware bugs requires rigor at every design stage, validation, and deployment.\n\n\n14.5.2 Physical Attacks\nPhysical tampering refers to the direct, unauthorized manipulation of physical computing resources to undermine the integrity of machine learning systems. It’s a particularly insidious attack because it circumvents traditional cybersecurity measures, which often focus more on software vulnerabilities than hardware threats.\nPhysical tampering can take many forms, from the relatively simple, such as someone inserting a USB device loaded with malicious software into a server, to the highly sophisticated, such as embedding a hardware Trojan during the manufacturing process of a microchip (discussed later in greater detail in the Supply Chain section). ML systems are susceptible to this attack because they rely on the accuracy and integrity of their hardware to process and analyze vast amounts of data correctly.\nConsider an ML-powered drone used for geographical mapping. The drone’s operation relies on a series of onboard systems, including a navigation module that processes inputs from various sensors to determine its path. If an attacker gains physical access to this drone, they could replace the genuine navigation module with a compromised one that includes a backdoor. This manipulated module could then alter the drone’s flight path to conduct surveillance over restricted areas or even smuggle contraband by flying undetected routes.\nAnother example is the physical tampering of biometric scanners used for access control in secure facilities. By introducing a modified sensor that transmits biometric data to an unauthorized receiver, an attacker can access personal identification data to authenticate individuals.\nThere are several ways that physical tampering can occur in ML hardware:\n\nManipulating sensors: Consider an autonomous vehicle equipped with cameras and LiDAR for environmental perception. A malicious actor could deliberately manipulate the physical alignment of these sensors to create occlusion zones or distort distance measurements. This could compromise object detection capabilities and potentially endanger vehicle occupants.\nHardware trojans: Malicious circuit modifications can introduce trojans designed to activate upon specific input conditions. For instance, an ML accelerator chip might operate as intended until encountering a predetermined trigger, at which point it behaves erratically.\nTampering with memory: Physically exposing and manipulating memory chips could allow the extraction of encrypted ML model parameters. Fault injection techniques can also corrupt model data to degrade accuracy.\nIntroducing backdoors: Gaining physical access to servers, an adversary could use hardware keyloggers to capture passwords and create backdoor accounts for persistent access. These could then be used to exfiltrate ML training data over time.\nSupply chain attacks: Manipulating third-party hardware components or compromising manufacturing and shipping channels creates systemic vulnerabilities that are difficult to detect and remediate.\n\n\n\n14.5.3 Fault-injection Attacks\nBy intentionally introducing faults into ML hardware, attackers can induce errors in the computational process, leading to incorrect outputs. This manipulation compromises the integrity of ML operations and can serve as a vector for further exploitation, such as system reverse engineering or security protocol bypass. Fault injection involves deliberately disrupting standard computational operations in a system through external interference (Joye e Tunstall 2012). By precisely triggering computational errors, adversaries can alter program execution in ways that degrade reliability or leak sensitive information.\n\nJoye, Marc, e Michael Tunstall. 2012. Fault Analysis in Cryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro Pellicioli, e Gerardo Pelosi. 2010. «Low voltage fault attacks to AES». In 2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST), 7–12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\nHutter, Michael, Jorn-Marc Schmidt, e Thomas Plos. 2009. «Contact-based fault injections and power analysis on RFID tags». In 2009 European Conference on Circuit Theory and Design, 409–12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\nAmiel, Frederic, Christophe Clavier, e Michael Tunstall. 2006. «Fault analysis of DPA-resistant algorithms». In International Workshop on Fault Diagnosis and Tolerance in Cryptography, 223–36. Springer.\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, e Berk Sunar. 2007. «Trojan Detection using IC Fingerprinting». In 2007 IEEE Symposium on Security and Privacy (SP ’07), 29–45. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\nSkorobogatov, Sergei. 2009. «Local heating attacks on Flash memory devices». In 2009 IEEE International Workshop on Hardware-Oriented Security and Trust, 1–6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\nSkorobogatov, Sergei P, e Ross J Anderson. 2003. «Optical fault induction attacks». In Cryptographic Hardware and Embedded Systems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA, August 1315, 2002 Revised Papers 4, 2–12. Springer.\nVarious physical tampering techniques can be used for fault injection. Low voltage (Barenghi et al. 2010), power spikes (Hutter, Schmidt, e Plos 2009), clock glitches (Amiel, Clavier, e Tunstall 2006), electromagnetic pulses (Agrawal et al. 2007), temperate increase (S. Skorobogatov 2009) and laser strikes (S. P. Skorobogatov e Anderson 2003) are common hardware attack vectors. They are precisely timed to induce faults like flipped bits or skipped instructions during critical operations.\nFor ML systems, consequences include impaired model accuracy, denial of service, extraction of private training data or model parameters, and reverse engineering of model architectures. Attackers could use fault injection to force misclassifications, disrupt autonomous systems, or steal intellectual property.\nFor example, in (Breier et al. 2018), the authors successfully injected a fault attack into a deep neural network deployed on a microcontroller. They used a laser to heat specific transistors, forcing them to switch states. In one instance, they used this method to attack a ReLU activation function, resulting in the function always outputting a value of 0, regardless of the input. In the assembly code in Figura 14.2, the attack caused the executing program always to skip the jmp end instruction on line 6. This means that HiddenLayerOutput[i] is always set to 0, overwriting any values written to it on lines 4 and 5. As a result, the targeted neurons are rendered inactive, resulting in misclassifications.\n\n\n\n\n\n\nFigura 14.2: Fault-injection demonstrated with assembly code. Source: Breier et al. (2018).\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, e Yang Liu. 2018. «Deeplaser: Practical fault attack on deep neural networks». ArXiv preprint abs/1806.05859. https://arxiv.org/abs/1806.05859.\n\n\nAn attacker’s strategy could be to infer information about the activation functions using side-channel attacks (discussed next). Then, the attacker could attempt to target multiple activation function computations by randomly injecting faults into the layers as close to the output layer as possible, increasing the likelihood and impact of the attack.\nEmbedded devices are particularly vulnerable due to limited physical hardening and resource constraints that restrict robust runtime defenses. Without tamper-resistant packaging, attacker access to system buses and memory enables precise fault strikes. Lightweight embedded ML models also lack redundancy to overcome errors.\nThese attacks can be particularly insidious because they bypass traditional software-based security measures, often not accounting for physical disruptions. Furthermore, because ML systems rely heavily on the accuracy and reliability of their hardware for tasks like pattern recognition, decision-making, and automated responses, any compromise in their operation due to fault injection can have severe and wide-ranging consequences.\nMitigating fault injection risks necessitates a multilayer approach. Physical hardening through tamper-proof enclosures and design obfuscation helps reduce access. Lightweight anomaly detection can identify unusual sensor inputs or erroneous model outputs (Hsiao et al. 2023). Error-correcting memories minimize disruption, while data encryption safeguards information. Emerging model watermarking techniques trace stolen parameters.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. «MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles». In 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nHowever, balancing robust protections with embedded systems’ tight size and power limits remains challenging. Cryptography limits and lack of secure co-processors on cost-sensitive embedded hardware restrict options. Ultimately, fault injection resilience demands a cross-layer perspective spanning electrical, firmware, software, and physical design layers.\n\n\n14.5.4 Side-Channel Attacks\nSide-channel attacks constitute a class of security breaches that exploit information inadvertently revealed through the physical implementation of computing systems. In contrast to direct attacks targeting software or network vulnerabilities, these attacks leverage the system’s inherent hardware characteristics to extract sensitive information.\nThe fundamental premise of a side-channel attack is that a device’s operation can inadvertently reveal information. Such leaks can come from various sources, including the electrical power a device consumes (Kocher, Jaffe, e Jun 1999), the electromagnetic fields it emits (Gandolfi, Mourtel, e Olivier 2001), the time it takes to process certain operations, or even the sounds it produces. Each channel can indirectly glimpse the system’s internal processes, revealing information that can compromise security.\n\nKocher, Paul, Joshua Jaffe, e Benjamin Jun. 1999. «Differential power analysis». In Advances in CryptologyCRYPTO’99: 19th Annual International Cryptology Conference Santa Barbara, California, USA, August 1519, 1999 Proceedings 19, 388–97. Springer.\n\nGandolfi, Karine, Christophe Mourtel, e Francis Olivier. 2001. «Electromagnetic analysis: Concrete results». In Cryptographic Hardware and Embedded SystemsCHES 2001: Third International Workshop Paris, France, May 1416, 2001 Proceedings 3, 251–61. Springer.\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, e Pankaj Rohatgi. 2011. «Introduction to differential power analysis». Journal of Cryptographic Engineering 1 (1): 5–27. https://doi.org/10.1007/s13389-011-0006-y.\nFor instance, consider a machine learning system performing encrypted transactions. Encryption algorithms are supposed to secure data but require computational work to encrypt and decrypt information. An attacker can analyze the power consumption patterns of the device performing encryption to figure out the cryptographic key. With sophisticated statistical methods, small variations in power usage during the encryption process can be correlated with the data being processed, eventually revealing the key. Some differential analysis attack techniques are Differential Power Analysis (DPA) (Kocher et al. 2011), Differential Electromagnetic Analysis (DEMA), and Correlation Power Analysis (CPA).\nFor example, consider an attacker trying to break the AES encryption algorithm using a differential analysis attack. The attacker would first need to collect many power or electromagnetic traces (a trace is a record of consumptions or emissions) of the device while performing AES encryption.\nOnce the attacker has collected sufficient traces, they would use a statistical technique to identify correlations between the traces and the different values of the plaintext (original, unencrypted text) and ciphertext (encrypted text). These correlations would then be used to infer the value of a bit in the AES key and, eventually, the entire key. Differential analysis attacks are dangerous because they are low-cost, effective, and non-intrusive, allowing attackers to bypass algorithmic and hardware-level security measures. Compromises by these attacks are also hard to detect because they do not physically modify the device or break the encryption algorithm.\nBelow, a simplified visualization illustrates how analyzing the encryption device’s power consumption patterns can help extract information about the algorithm’s operations and, in turn, the secret data. Consider a device that takes a 5-byte password as input. The different voltage patterns measured while the encryption device performs operations on the input to authenticate the password will be analyzed and compared.\nFirst, the power analysis of the device’s operations after entering a correct password is shown in the first picture in Figura 14.3. The dense blue graph outputs the encryption device’s voltage measurement. What is significant here is the comparison between the different analysis charts rather than the specific details of what is happening in each scenario.\n\n\n\n\n\n\nFigura 14.3: Power analysis of an encryption device with a correct password. Source: Colin O’Flynn.\n\n\n\nWhen an incorrect password is entered, the power analysis chart is shown in Figura 14.4. The first three bytes of the password are correct. As a result, the voltage patterns are very similar or identical between the two charts, up to and including the fourth byte. After the device processes the fourth byte, a mismatch between the secret key and the attempted input is determined. A change in the pattern at the transition point between the fourth and fifth bytes is noticed: the voltage increases (the current decreases) because the device has stopped processing the rest of the input.\n\n\n\n\n\n\nFigura 14.4: Power analysis of an encryption device with a (partially) wrong password. Source: Colin O’Flynn.\n\n\n\nFigura 14.5 describes another chart of a completely wrong password. After the device finishes processing the first byte, it determines that it is incorrect and stops further processing - the voltage goes up and the current down.\n\n\n\n\n\n\nFigura 14.5: Power analysis of an encryption device with a wrong password. Source: Colin O’Flynn.\n\n\n\nThe example above demonstrates how information about the encryption process and the secret key can be inferred by analyzing different inputs and attempting to ‘eavesdrop’ on the device’s operations on each input byte. For a more detailed explanation, watch Video 14.3 below.\n\n\n\n\n\n\nVideo 14.3: Power Attack\n\n\n\n\n\n\nAnother example is an ML system for speech recognition, which processes voice commands to perform actions. By measuring the latency for the system to respond to commands or the power used during processing, an attacker could infer what commands are being processed and thus learn about the system’s operational patterns. Even more subtly, the sound emitted by a computer’s fan or hard drive could change in response to the workload, which a sensitive microphone could pick up and analyze to determine what kind of operations are being performed.\nIn real-world scenarios, side-channel attacks have effectively extracted encryption keys and compromised secure communications. One of the earliest recorded instances of such an attack occurred in the 1960s when the British intelligence agency MI5 confronted the challenge of deciphering encrypted communications from the Egyptian Embassy in London. Their cipher-breaking efforts were initially thwarted by the computational limitations of the time until an ingenious observation by MI5 agent Peter Wright altered the course of the operation.\nMI5 agent Peter Wright proposed using a microphone to capture the subtle acoustic signatures emitted from the embassy’s rotor cipher machine during encryption (Burnet e Thomas 1989). The distinct mechanical clicks of the rotors as operators configured them daily leaked critical information about the initial settings. This simple side channel of sound enabled MI5 to reduce the complexity of deciphering messages dramatically. This early acoustic leak attack highlights that side-channel attacks are not merely a digital age novelty but a continuation of age-old cryptanalytic principles. The notion that where there is a signal, there is an opportunity for interception remains foundational. From mechanical clicks to electrical fluctuations and beyond, side channels enable adversaries to extract secrets indirectly through careful signal analysis.\n\nBurnet, David, e Richard Thomas. 1989. «Spycatcher: The Commodification of Truth». J. Law Soc. 16 (2): 210. https://doi.org/10.2307/1410360.\n\nAsonov, D., e R. Agrawal. 2004. «Keyboard acoustic emanations». In IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004, 3–11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\nGnad, Dennis R. E., Fabian Oboril, e Mehdi B. Tahoori. 2017. «Voltage drop-based fault attacks on FPGAs using valid bitstreams». In 2017 27th International Conference on Field Programmable Logic and Applications (FPL), 1–7. IEEE; IEEE. https://doi.org/10.23919/fpl.2017.8056840.\n\nZhao, Mark, e G. Edward Suh. 2018. «FPGA-Based Remote Power Side-Channel Attacks». In 2018 IEEE Symposium on Security and Privacy (SP), 229–44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\nToday, acoustic cryptanalysis has evolved into attacks like keyboard eavesdropping (Asonov e Agrawal 2004). Electrical side channels range from power analysis on cryptographic hardware (Gnad, Oboril, e Tahoori 2017) to voltage fluctuations (Zhao e Suh 2018) on machine learning accelerators. Timing, electromagnetic emission, and even heat footprints can likewise be exploited. New and unexpected side channels often emerge as computing becomes more interconnected and miniaturized.\nJust as MI5’s analog acoustic leak transformed their codebreaking, modern side-channel attacks circumvent traditional boundaries of cyber defense. Understanding the creative spirit and historical persistence of side channel exploits is key knowledge for developers and defenders seeking to secure modern machine learning systems comprehensively against digital and physical threats.\n\n\n14.5.5 Leaky Interfaces\nLeaky interfaces in embedded systems are often overlooked backdoors that can become significant security vulnerabilities. While designed for legitimate purposes such as communication, maintenance, or debugging, these interfaces may inadvertently provide attackers with a window through which they can extract sensitive information or inject malicious data.\nAn interface becomes “leaky” when it exposes more information than it should, often due to a lack of stringent access controls or inadequate shielding of the transmitted data. Here are some real-world examples of leaky interface issues causing security problems in IoT and embedded devices:\n\nBaby Monitors: Many WiFi-enabled baby monitors have been found to have unsecured interfaces for remote access. This allowed attackers to gain live audio and video feeds from people’s homes, representing a major privacy violation.\nPacemakers: Interface vulnerabilities were discovered in some pacemakers that could allow attackers to manipulate cardiac functions if exploited. This presents a potentially life-threatening scenario.\nSmart Lightbulbs: A researcher found he could access unencrypted data from smart lightbulbs via a debug interface, including WiFi credentials, allowing him to gain access to the connected network (Greengard 2015).\nSmart Cars: If left unsecured, The OBD-II diagnostic port has been shown to provide an attack vector into automotive systems. Researchers could use it to control brakes and other components (Miller e Valasek 2015).\n\n\nGreengard, Samuel. 2015. The Internet of Things. The MIT Press. https://doi.org/10.7551/mitpress/10277.001.0001.\n\nMiller, Charlie, e Chris Valasek. 2015. «Remote exploitation of an unaltered passenger vehicle». Black Hat USA 2015 (S 91): 1–91.\nWhile the above are not directly connected with ML, consider the example of a smart home system with an embedded ML component that controls home security based on behavior patterns it learns over time. The system includes a maintenance interface accessible via the local network for software updates and system checks. If this interface does not require strong authentication or the data transmitted through it is not encrypted, an attacker on the same network could gain access. They could then eavesdrop on the homeowner’s daily routines or reprogram the security settings by manipulating the firmware.\nSuch leaks are a privacy issue and a potential entry point for more damaging exploits. The exposure of training data, model parameters, or ML outputs from a leak could help adversaries construct adversarial examples or reverse-engineer models. Access through a leaky interface could also be used to alter an embedded device’s firmware, loading it with malicious code that could turn off the device, intercept data, or use it in botnet attacks.\nTo mitigate these risks, a multi-layered approach is necessary, spanning technical controls like authentication, encryption, anomaly detection, policies and processes like interface inventories, access controls, auditing, and secure development practices. Turning off unnecessary interfaces and compartmentalizing risks via a zero-trust model provide additional protection.\nAs designers of embedded ML systems, we should assess interfaces early in development and continually monitor them post-deployment as part of an end-to-end security lifecycle. Understanding and securing interfaces is crucial for ensuring the overall security of embedded ML.\n\n\n14.5.6 Counterfeit Hardware\nML systems are only as reliable as the underlying hardware. In an era where hardware components are global commodities, the rise of counterfeit or cloned hardware presents a significant challenge. Counterfeit hardware encompasses any components that are unauthorized reproductions of original parts. Counterfeit components infiltrate ML systems through complex supply chains that stretch across borders and involve numerous stages from manufacture to delivery.\nA single lapse in the supply chain’s integrity can result in the insertion of counterfeit parts designed to closely imitate the functions and appearance of genuine hardware. For instance, a facial recognition system for high-security access control may be compromised if equipped with counterfeit processors. These processors could fail to accurately process and verify biometric data, potentially allowing unauthorized individuals to access restricted areas.\nThe challenge with counterfeit hardware is multifaceted. It undermines the quality and reliability of ML systems, as these components may degrade faster or perform unpredictably due to substandard manufacturing. The security risks are also profound; counterfeit hardware can contain vulnerabilities ripe for exploitation by malicious actors. For example, a cloned network router in an ML data center might include a hidden backdoor, enabling data interception or network intrusion without detection.\nFurthermore, counterfeit hardware poses legal and compliance risks. Companies inadvertently utilizing counterfeit parts in their ML systems may face serious legal repercussions, including fines and sanctions for failing to comply with industry regulations and standards. This is particularly true for sectors where compliance with specific safety and privacy regulations is mandatory, such as healthcare and finance.\nThe issue of counterfeit hardware is exacerbated by economic pressures to reduce costs, which can compel businesses to source from lower-cost suppliers without stringent verification processes. This economizing can inadvertently introduce counterfeit parts into otherwise secure systems. Additionally, detecting these counterfeits is inherently difficult since they are created to pass as the original components, often requiring sophisticated equipment and expertise to identify.\nIn ML, where decisions are made in real time and based on complex computations, the consequences of hardware failure are inconvenient and potentially dangerous. Stakeholders in the field of ML need to understand these risks thoroughly. The issues presented by counterfeit hardware necessitate a deep dive into the current challenges facing ML system integrity and emphasize the importance of vigilant, informed management of the hardware life cycle within these advanced systems.\n\n\n14.5.7 Supply Chain Risks\nThe threat of counterfeit hardware is closely tied to broader supply chain vulnerabilities. Globalized, interconnected supply chains create multiple opportunities for compromised components to infiltrate a product’s lifecycle. Supply chains involve numerous entities, from design to manufacturing, assembly, distribution, and integration. A lack of transparency and oversight of each partner makes verifying integrity at every step challenging. Lapses anywhere along the chain can allow the insertion of counterfeit parts.\nFor example, a contracted manufacturer may unknowingly receive and incorporate recycled electronic waste containing dangerous counterfeits. An untrustworthy distributor could smuggle in cloned components. Insider threats at any vendor might deliberately mix counterfeits into legitimate shipments.\nOnce counterfeits enter the supply stream, they move quickly through multiple hands before ending up in ML systems where detection is difficult. Advanced counterfeits like refurbished parts or clones with repackaged externals can masquerade as authentic components, passing visual inspection.\nTo identify fakes, thorough technical profiling using micrography, X-ray screening, component forensics, and functional testing is often required. However, such costly analysis is impractical for large-volume procurement.\nStrategies like supply chain audits, screening suppliers, validating component provenance, and adding tamper-evident protections can help mitigate risks. However, given global supply chain security challenges, a zero-trust approach is prudent. Designing ML systems to use redundant checking, fail-safes, and continuous runtime monitoring provides resilience against component compromises.\nRigorous validation of hardware sources coupled with fault-tolerant system architectures offers the most robust defense against the pervasive risks of convoluted, opaque global supply chains.\n\n\n14.5.8 Case Study\nIn 2018, Bloomberg Businessweek published an alarming story that got much attention in the tech world. The article claimed that Supermicro had secretly planted tiny spy chips on server hardware. Reporters said Chinese state hackers working with Supermicro could sneak these tiny chips onto motherboards during manufacturing. The tiny chips allegedly gave the hackers backdoor access to servers used by over 30 major companies, including Apple and Amazon.\nIf true, this would allow hackers to spy on private data or even tamper with systems. However, after investigating, Apple and Amazon found no proof that such hacked Supermicro hardware existed. Other experts questioned whether the Bloomberg article was accurate reporting.\nWhether the story is completely true or not is not our concern from a pedagogical viewpoint. However, this incident drew attention to the risks of global supply chains for hardware, especially manufactured in China. When companies outsource and buy hardware components from vendors worldwide, there needs to be more visibility into the process. In this complex global pipeline, there are concerns that counterfeits or tampered hardware could be slipped in somewhere along the way without tech companies realizing it. Companies relying too much on single manufacturers or distributors creates risk. For instance, due to the over-reliance on TSMC for semiconductor manufacturing, the U.S. has invested 50 billion dollars into the CHIPS Act.\nAs ML moves into more critical systems, verifying hardware integrity from design through production and delivery is crucial. The reported Supermicro backdoor demonstrated that for ML security, we cannot take global supply chains and manufacturing for granted. We must inspect and validate hardware at every link in the chain.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#embedded-ml-hardware-security",
    "href": "contents/privacy_security/privacy_security.html#embedded-ml-hardware-security",
    "title": "14  Security & Privacy",
    "section": "14.6 Embedded ML Hardware Security",
    "text": "14.6 Embedded ML Hardware Security\n\n14.6.1 Trusted Execution Environments\n\nAbout TEE\nA Trusted Execution Environment (TEE) is a secure area within a main processor that provides a high level of security for the execution of code and protection of data. TEEs operate by isolating the execution of sensitive tasks from the rest of the device’s operations, thereby creating an environment resistant to attacks from software and hardware vectors.\n\n\nBenefits\nTEEs are particularly valuable in scenarios where sensitive data must be processed or where the integrity of a system’s operations is critical. In the context of ML hardware, TEEs ensure that the ML algorithms and data are protected against tampering and leakage. This is essential because ML models often process private information, trade secrets, or data that could be exploited if exposed.\nFor instance, a TEE can protect ML model parameters from being extracted by malicious software on the same device. This protection is vital for privacy and maintaining the integrity of the ML system, ensuring that the models perform as expected and do not provide skewed outputs due to manipulated parameters. Apple’s Secure Enclave, found in iPhones and iPads, is a form of TEE that provides an isolated environment to protect sensitive user data and cryptographic operations.\nIn ML systems, TEEs can:\n\nSecurely perform model training and inference, ensuring the computation results remain confidential.\nProtect the confidentiality of input data, like biometric information, used for personal identification or sensitive classification tasks.\nSecure ML models by preventing reverse engineering, which can protect proprietary information and maintain a competitive advantage.\nEnable secure updates to ML models, ensuring that updates come from a trusted source and have not been tampered with in transit.\n\nThe importance of TEEs in ML hardware security stems from their ability to protect against external and internal threats, including the following:\n\nMalicious Software: TEEs can prevent high-privilege malware from accessing sensitive areas of the ML system.\nPhysical Tampering: By integrating with hardware security measures, TEEs can protect against physical tampering that attempts to bypass software security.\nSide-channel Attacks: Although not impenetrable, TEEs can mitigate certain side-channel attacks by controlling access to sensitive operations and data patterns.\n\n\n\nMechanics\nThe fundamentals of TEEs contain four main parts:\n\nIsolated Execution: Code within a TEE runs in a separate environment from the device’s main operating system. This isolation protects the code from unauthorized access by other applications.\nSecure Storage: TEEs can securely store cryptographic keys, authentication tokens, and sensitive data, preventing access by regular applications running outside the TEE.\nIntegrity Protection: TEEs can verify the integrity of code and data, ensuring that they have not been altered before execution or during storage.\nData Encryption: Data handled within a TEE can be encrypted, making it unreadable to entities without the proper keys, which are also managed within the TEE.\n\nHere are some examples of TEEs that provide hardware-based security for sensitive applications:\n\nARMTrustZone:This technology creates secure and normal world execution environments isolated using hardware controls and implemented in many mobile chipsets.\nIntelSGX:Intel’s Software Guard Extensions provide an enclave for code execution that protects against certain software attacks, specifically O.S. layer attacks. They are used to safeguard workloads in the cloud.\nQualcomm Secure Execution Environment:A Hardware sandbox on Qualcomm chipsets for mobile payment and authentication apps.\nApple SecureEnclave:TEE for biometric data and key management on iPhones and iPads.Facilitates mobile payments.\n\nFigura 14.6 is a diagram demonstrating a secure enclave isolated from the main processor to provide an extra layer of security. The secure enclave has a boot ROM to establish a hardware root of trust, an AES engine for efficient and secure cryptographic operations, and protected memory. It also has a mechanism to store information securely on attached storage separate from the NAND flash storage used by the application processor and operating system. This design keeps sensitive user data secure even when the Application Processor kernel becomes compromised.\n\n\n\n\n\n\nFigura 14.6: System-on-chip secure enclave. Source: Apple.\n\n\n\n\n\nTradeoffs\nIf TEEs are so good, why don’t all systems have TEE enabled by default? The decision to implement a TEE is not taken lightly. There are several reasons why a TEE might only be present in some systems by default. Here are some tradeoffs and challenges associated with TEEs:\nCost: Implementing TEEs involves additional costs. There are direct costs for the hardware and indirect costs associated with developing and maintaining secure software for TEEs. These costs may only be justifiable for some devices, especially low-margin products.\nComplexity: TEEs add complexity to system design and development. Integrating a TEE with existing systems requires a substantial redesign of the hardware and software stack, which can be a barrier, especially for legacy systems.\nPerformance Overhead: While TEEs offer enhanced security, they can introduce performance overhead. For example, the additional steps in verifying and encrypting data can slow down system performance, which may be critical in time-sensitive applications.\nDevelopment Challenges: Developing for TEEs requires specialized knowledge and often must adhere to strict development protocols. This can extend development time and complicate the debugging and testing processes.\nScalability and Flexibility: TEEs, due to their secure nature, may impose limitations on scalability and flexibility. Upgrading secure components or scaling the system for more users or data can be more challenging when everything must pass through a secure, enclosed environment.\nEnergy Consumption: The increased processing required for encryption, decryption, and integrity checks can lead to higher energy consumption, a significant concern for battery-powered devices.\nMarket Demand: Not all markets or applications require the level of security provided by TEEs. For many consumer applications, the perceived risk may be low enough that manufacturers opt not to include TEEs in their designs.\nSecurity Certification and Assurance: Systems with TEEs may need rigorous security certifications with bodies like Common Criteria (CC) or the European Union Agency for Cybersecurity (ENISA), which can be lengthy and expensive. Some organizations may choose to refrain from implementing TEEs to avoid these hurdles.\nLimited Resource Devices: Devices with limited processing power, memory, or storage may only support TEEs without compromising their primary functionality.\n\n\n\n14.6.2 Secure Boot\n\nAbout\nA secure boot is a security standard that ensures a device boots using only software trusted by the original equipment manufacturer (OEM). When the device starts up, the firmware checks the signature of each piece of boot software, including the bootloader, kernel, and base operating system, to ensure it’s not tampered with. If the signatures are valid, the device continues to boot. If not, the boot process stops to prevent potential security threats from executing.\n\n\nBenefits\nThe integrity of an ML system is critical from the moment it is powered on. A compromised boot process could undermine the system by allowing malicious software to load before the operating system and ML applications start. This could lead to manipulated ML operations, stolen data, or the device being repurposed for malicious activities such as botnets or crypto-mining.\nSecure Boot helps protect embedded ML hardware in several ways:\n\nProtecting ML Data: Ensuring that the data used by ML models, which may include private or sensitive information, is not exposed to tampering or theft during the boot process.\nGuarding Model Integrity: Maintaining the ML models’ integrity is important, as tampering with them could lead to incorrect or malicious outcomes.\nSecure Model Updates: Enabling secure updates to ML models and algorithms, ensuring that updates are authenticated and have not been altered.\n\n\n\nMechanics\nTEEs benefit from Secure Boot in multiple ways. Figura 14.7 illustrates a flow diagram of a trusted embedded system. For instance, during initial validation, Secure Boot ensures that the code running inside the TEE is the correct and untampered version approved by the device manufacturer. It can ensure resilience against tampering by verifying the digital signatures of the firmware and other critical components; Secure Boot prevents unauthorized modifications that could undermine the TEE’s security properties. Secure Boot establishes a foundation of trust upon which the TEE can securely operate, enabling secure operations such as cryptographic key management, secure processing, and sensitive data handling.\n\n\n\n\n\n\nFigura 14.7: Secure Boot flow. Source: R. V. e A. (2018).\n\n\nR. V., Rashmi, e Karthikeyan A. 2018. «Secure boot of Embedded Applications - A Review». In 2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA), 291–98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\n\n\nCase Study: Apple’s Face ID\nLet’s take a real-world example. Apple’s Face ID technology uses advanced machine learning algorithms to enable facial recognition on iPhones and iPads. It relies on a sophisticated framework of sensors and software to accurately map the geometry of a user’s face. For Face ID to function securely and protect user biometric data, the device’s operations must be trustworthy from the moment it is powered on, which is where Secure Boot plays a crucial role. Here’s how Secure Boot works in conjunction with Face ID:\nInitial Verification: When an iPhone is powered on, the Secure Boot process begins in the Secure Enclave, a coprocessor providing an extra security layer. The Secure Enclave is responsible for processing fingerprint data for Touch ID and facial recognition data for Face ID. The boot process verifies that Apple has signed the Secure Enclave’s firmware and has not been tampered with. This step ensures that the firmware used to process biometric data is authentic and safe.\nContinuous Security Checks: After the initial power-on self-test and verification by Secure Boot, the Secure Enclave communicates with the device’s main processor to continue the secure boot chain. It verifies the digital signatures of the iOS kernel and other critical boot components before allowing the boot process to proceed. This chained trust model prevents unauthorized modifications to the bootloader and operating system, which could compromise the device’s security.\nFace Data Processing: Once the device has completed its secure boot sequence, the Secure Enclave can interact safely with the ML algorithms that power Face ID. Facial recognition involves projecting and analyzing over 30,000 invisible dots to create a depth map of the user’s face and an infrared image. This data is then converted into a mathematical representation and compared with the registered face data securely stored in the Secure Enclave.\nSecure Enclave and Data Protection: The Secure Enclave is designed to protect sensitive data and handle the cryptographic operations that secure it. It ensures that even if the operating system kernel is compromised, the facial data cannot be accessed by unauthorized apps or attackers. Face ID data never leaves the device and is not backed up to iCloud or anywhere else.\nFirmware Updates: Apple frequently releases firmware updates to address security vulnerabilities and improve the functionality of its systems. Secure Boot ensures that each firmware update is authenticated and that only updates signed by Apple are installed on the device, preserving the integrity and security of the Face ID system.\nBy using Secure Boot with dedicated hardware like the Secure Enclave, Apple can provide strong security assurances for sensitive operations like facial recognition.\n\n\nChallenges\nImplementing Secure Boot poses several challenges that must be addressed to realize its full benefits.\nKey Management Complexity: Generating, storing, distributing, rotating, and revoking cryptographic keys provably securely is extremely challenging yet vital for maintaining the chain of trust. Any compromise of keys cripples protections. Large enterprises managing multitudes of device keys face particular scale challenges.\nPerformance Overhead: Checking cryptographic signatures during Boot can add 50-100ms or more per component verified. This delay may be prohibitive for time-sensitive or resource-constrained applications. However, performance impacts can be reduced through parallelization and hardware acceleration.\nSigning Burden: Developers must diligently ensure that all software components involved in the boot process - bootloaders, firmware, OS kernel, drivers, applications, etc. are correctly signed by trusted keys. Accommodating third-party code signing remains an issue.\nCryptographic Verification: Secure algorithms and protocols must validate the legitimacy of keys and signatures, avoid tampering or bypass, and support revocation. Accepting dubious keys undermines trust.\nCustomizability Constraints: Vendor-locked Secure Boot architectures limit user control and upgradability. Open-source bootloaders like u-boot and coreboot enable security while supporting customizability.\nScalable Standards: Emerging standards like Device Identifier Composition Engine (DICE) and IDevID promise to securely provision and manage device identities and keys at scale across ecosystems.\nAdopting Secure Boot requires following security best practices around key management, crypto validation, signed updates, and access control. Secure Boot provides a robust foundation for building device integrity and trust when implemented with care.\n\n\n\n14.6.3 Hardware Security Modules\n\nAbout HSM\nA Hardware Security Module (HSM) is a physical device that manages digital keys for strong authentication and provides crypto-processing. These modules are designed to be tamper-resistant and provide a secure environment for performing cryptographic operations. HSMs can come in standalone devices, plug-in cards, or integrated circuits on another device.\nHSMs are crucial for various security-sensitive applications because they offer a hardened, secure enclave for storing cryptographic keys and executing cryptographic functions. They are particularly important for ensuring the security of transactions, identity verifications, and data encryption.\n\n\nBenefits\nHSMs provide several functionalities that are beneficial for the security of ML systems:\nProtecting Sensitive Data: In machine learning applications, models often process sensitive data that can be proprietary or personal. HSMs protect the encryption keys used to secure this data, both at rest and in transit, from exposure or theft.\nEnsuring Model Integrity: The integrity of ML models is vital for their reliable operation. HSMs can securely manage the signing and verification processes for ML software and firmware, ensuring unauthorized parties have not altered the models.\nSecure Model Training and Updates: The training and updating of ML models involve the processing of potentially sensitive data. HSMs ensure that these processes are conducted within a secure cryptographic boundary, protecting against the exposure of training data and unauthorized model updates.\n\n\nTradeoffs\nHSMs involve several tradeoffs for embedded ML. These tradeoffs are similar to TEEs, but for completeness, we will also discuss them here through the lens of HSM.\nCost: HSMs are specialized devices that can be expensive to procure and implement, raising the overall cost of an ML project. This may be a significant factor for embedded systems, where cost constraints are often stricter.\nPerformance Overhead: While secure, the cryptographic operations performed by HSMs can introduce latency. Any added delay can be critical in high-performance embedded ML applications where inference must happen in real-time, such as in autonomous vehicles or translation devices.\nPhysical Space: Embedded systems are often limited by physical space, and adding an HSM can be challenging in tightly constrained environments. This is especially true for consumer electronics and wearable technology, where size and form factor are key considerations.\nPower Consumption: HSMs require power for their operation, which can be a drawback for battery-operated devices with long battery life. The secure processing and cryptographic operations can drain the battery faster, a significant tradeoff for mobile or remote embedded ML applications.\nComplexity in Integration: Integrating HSMs into existing hardware systems adds complexity. It often requires specialized knowledge to manage the secure communication between the HSM and the system’s processor and develop software capable of interfacing with the HSM.\nScalability: Scaling an ML solution that uses HSMs can be challenging. Managing a fleet of HSMs and ensuring uniformity in security practices across devices can become complex and costly when the deployment size increases, especially when dealing with embedded systems where communication is costly.\nOperational Complexity: HSMs can make updating firmware and ML models more complex. Every update must be signed and possibly encrypted, which adds steps to the update process and may require secure mechanisms for key management and update distribution.\nDevelopment and Maintenance: The secure nature of HSMs means that only limited personnel have access to the HSM for development and maintenance purposes. This can slow down the development process and make routine maintenance more difficult.\nCertification and Compliance: Ensuring that an HSM meets specific industry standards and compliance requirements can add to the time and cost of development. This may involve undergoing rigorous certification processes and audits.\n\n\n\n14.6.4 Physical Unclonable Functions (PUFs)\n\nAbout\nPhysical Unclonable Functions (PUFs) provide a hardware-intrinsic means for cryptographic key generation and device authentication by harnessing the inherent manufacturing variability in semiconductor components. During fabrication, random physical factors such as doping variations, line edge roughness, and dielectric thickness result in microscale differences between semiconductors, even when produced from the same masks. These create detectable timing and power variances that act as a “fingerprint” unique to each chip. PUFs exploit this phenomenon by incorporating integrated circuits to amplify minute timing or power differences into measurable digital outputs.\nWhen stimulated with an input challenge, the PUF circuit produces an output response based on the device’s intrinsic physical characteristics. Due to their physical uniqueness, the same challenge will yield a different response on other devices. This challenge-response mechanism can be used to generate keys securely and identifiers tied to the specific hardware, perform device authentication, or securely store secrets. For example, a key derived from a PUF will only work on that device and cannot be cloned or extracted even with physical access or full reverse engineering (Gao, Al-Sarawi, e Abbott 2020).\n\n\nBenefits\nPUF key generation avoids external key storage, which risks exposure. It also provides a foundation for other hardware security primitives like Secure Boot. Implementation challenges include managing varying reliability and entropy across different PUFs, sensitivity to environmental conditions, and susceptibility to machine learning modeling attacks. When designed carefully, PUFs enable promising applications in IP protection, trusted computing, and anti-counterfeiting.\n\n\nUtility\nMachine learning models are rapidly becoming a core part of the functionality for many embedded devices, such as smartphones, smart home assistants, and autonomous drones. However, securing ML on resource-constrained embedded hardware can be challenging. This is where physical unclonable functions (PUFs) come in uniquely handy. Let’s look at some examples of how PUFs can be useful.\nPUFs provide a way to generate unique fingerprints and cryptographic keys tied to the physical characteristics of each chip on the device. Let’s take an example. We have a smart camera drone that uses embedded ML to track objects. A PUF integrated into the drone’s processor could create a device-specific key to encrypt the ML model before loading it onto the drone. This way, even if an attacker somehow hacks the drone and tries to steal the model, they won’t be able to use it on another device!\nThe same PUF key could also create a digital watermark embedded in the ML model. If that model ever gets leaked and posted online by someone trying to pirate it, the watermark could help prove it came from your stolen drone and didn’t originate from the attacker. Also, imagine the drone camera connects to the cloud to offload some of its ML processing. The PUF can authenticate that the camera is legitimate before the cloud will run inference on sensitive video feeds. The cloud could verify that the drone has not been physically tampered with by checking that the PUF responses have not changed.\nPUFs enable all this security through their challenge-response behavior’s inherent randomness and hardware binding. Without needing to store keys externally, PUFs are ideal for securing embedded ML with limited resources. Thus, they offer a unique advantage over other mechanisms.\n\n\nMechanics\nThe working principle behind PUFs, shown in Figura 14.8, involves generating a “challenge-response” pair, where a specific input (the challenge) to the PUF circuit results in an output (the response) that is determined by the unique physical properties of that circuit. This process can be likened to a fingerprinting mechanism for electronic devices. Devices that use ML for processing sensor data can employ PUFs to secure communication between devices and prevent the execution of ML models on counterfeit hardware.\nFigura 14.8 illustrates an overview of the PUF basics: a) PUF can be thought of as a unique fingerprint for each piece of hardware; b) an Optical PUF is a special plastic token that is illuminated, creating a unique speckle pattern that is then recorded; c) in an APUF (Arbiter PUF), challenge bits select different paths, and a judge decides which one is faster, giving a response of ‘1’ or ‘0’; d) in an SRAM PUF, the response is determined by the mismatch in the threshold voltage of transistors, where certain conditions lead to a preferred response of ‘1’. Each of these methods uses specific characteristics of the hardware to create a unique identifier.\n\n\n\n\n\n\nFigura 14.8: PUF basics. Source: Gao, Al-Sarawi, e Abbott (2020).\n\n\nGao, Yansong, Said F. Al-Sarawi, e Derek Abbott. 2020. «Physical unclonable functions». Nature Electronics 3 (2): 81–91. https://doi.org/10.1038/s41928-020-0372-5.\n\n\n\n\nChallenges\nThere are a few challenges with PUFs. The PUF response can be sensitive to environmental conditions, such as temperature and voltage fluctuations, leading to inconsistent behavior that must be accounted for in the design. Also, since PUFs can generate many unique challenge-response pairs, managing and ensuring the consistency of these pairs across the device’s lifetime can be challenging. Last but not least, integrating PUF technology may increase the overall manufacturing cost of a device, although it can save costs in key management over the device’s lifecycle.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#privacy-concerns-in-data-handling",
    "href": "contents/privacy_security/privacy_security.html#privacy-concerns-in-data-handling",
    "title": "14  Security & Privacy",
    "section": "14.7 Privacy Concerns in Data Handling",
    "text": "14.7 Privacy Concerns in Data Handling\nHandling personal and sensitive data securely and ethically is critical as machine learning permeates devices like smartphones, wearables, and smart home appliances. For medical hardware, handling data securely and ethically is further required by law through the Health Insurance Portability and Accountability Act (HIPAA). These embedded ML systems pose unique privacy risks, given their intimate proximity to users’ lives.\n\n14.7.1 Sensitive Data Types\nEmbedded ML devices like wearables, smart home assistants, and autonomous vehicles frequently process highly personal data that requires careful handling to maintain user privacy and prevent misuse. Specific examples include medical reports and treatment plans processed by health wearables, private conversations continuously captured by smart home assistants, and detailed driving habits collected by connected cars. Compromise of such sensitive data can lead to serious consequences like identity theft, emotional manipulation, public shaming, and mass surveillance overreach.\nSensitive data takes many forms - structured records like contact lists and unstructured content like conversational audio and video streams. In medical settings, protected health information (PHI) is collected by doctors throughout every interaction and is heavily regulated by strict HIPAA guidelines. Even outside of medical settings, sensitive data can still be collected in the form of Personally Identifiable Information (PII), which is defined as “any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means.” Examples of PII include email addresses, social security numbers, and phone numbers, among other fields. PII is collected in medical settings and other settings (financial applications, etc) and is heavily regulated by Department of Labor policies.\nEven derived model outputs could indirectly leak details about individuals. Beyond just personal data, proprietary algorithms and datasets also warrant confidentiality protections. In the Data Engineering section, we covered several topics in detail.\nTechniques like de-identification, aggregation, anonymization, and federation can help transform sensitive data into less risky forms while retaining analytical utility. However, diligent controls around access, encryption, auditing, consent, minimization, and compliance practices are still essential throughout the data lifecycle. Regulations like GDPR categorize different classes of sensitive data and prescribe responsibilities around their ethical handling. Standards like NIST 800-53 provide rigorous security control guidance for confidentiality protection. With growing reliance on embedded ML, understanding sensitive data risks is crucial.\n\n\n14.7.2 Applicable Regulations\nMany embedded ML applications handle sensitive user data under HIPAA, GDPR, and CCPA regulations. Understanding the protections mandated by these laws is crucial for building compliant systems.\n\nHIPAA Privacy Rule establishes care providers that conduct certain governs medical data privacy and security in the US, with severe penalties for violations. Any health-related embedded ML devices like diagnostic wearables or assistive robots would need to implement controls like audit trails, access controls, and encryption prescribed by HIPAA.\nGDPR imposes transparency, retention limits, and user rights on EU citizen data, even when processed by companies outside the EU. Smart home systems capturing family conversations or location patterns would need GDPR compliance. Key requirements include data minimization, encryption, and mechanisms for consent and erasure.\nCCPA, which applies in California, protects consumer data privacy through provisions like required disclosures and opt-out rights—ioT gadgets like smart speakers and fitness trackers Californians use likely to fall under its scope.\nThe CCPA was the first state-specific set of regulations regarding privacy concerns. Following the CCPA, similar regulations were also enacted in 10 other states, with some states proposing bills for consumer data privacy protections.\n\nAdditionally, when relevant to the application, sector-specific rules govern telematics, financial services, utilities, etc. Best practices like Privacy by design, impact assessments, and maintaining audit trails help embed compliance if it is not already required by law. Given potentially costly penalties, consulting legal/compliance teams is advisable when developing regulated embedded ML systems.\n\n\n14.7.3 De-identification\nIf medical data is de-identified thoroughly, HIPAA guidelines do not directly apply, and there are far fewer regulations. However, medical data needs to be de-identified using HIPAA methods (Safe Harbor methods or Expert Determination methods) for HIPAA guidelines to no longer apply.\n\nSafe Harbor Methods\nSafe Harbor methods are most commonly used for de-identifying protected healthcare information due to the limited resources needed compared to Expert Determination methods. Safe Harbor de-identification requires scrubbing datasets of any data that falls into one of 18 categories. The following categories are listed as sensitive information based on the Safe Harbor standard:\n\nName, Geographic locator, Birthdate, Phone Number, Email Address, addresses, Social Security Numbers, Medical Record Numbers, health beneficiary Numbers, Device Identifiers and Serial Numbers, Certificate/License Numbers (Birth Certificate, Drivers License, etc), Account Numbers, Vehicle Identifiers, Website URLs, FullFace Photos and Comparable Images, Biometric Identifiers, Any other unique identifiers\n\nFor most of these categories, all data must be removed regardless of the circumstances. For other categories, including geographical information and birthdate, the data can be partially removed enough to make the information hard to re-identify. For example, if a zip code is large enough, the first 3 digits can remain since there are enough people in the geographic area to make re-identification difficult. Birthdates need to be scrubbed of all elements except birth year, and all ages above 89 need to be aggregated into a 90+ category.\n\n\nExpert Determination Methods\nSafe Harbor methods work for several cases of medical data de-identification, though re-identification is still possible in some cases. For example, let’s say you collect data on a patient in an urban city with a large zip code, but you have documented a rare disease that they have—a disease that only 25 people have in the entire city. Given geographic data coupled with birth year, it is highly possible that someone can re-identify this individual, which is an extremely detrimental privacy breach.\nIn unique cases like these, expert determination data de-identification methods are preferred. Expert determination de-identification requires a “person with appropriate knowledge of and experience with generally accepted statistical and scientific principles and methods for rendering information not individually identifiable” to evaluate a dataset and determine if the risk of re-identification of individual data in a given dataset in combination with publicly available data (voting records, etc.), is extremely small.\nExpert Determination de-identification is understandably harder to complete than Safe Harbour de-identification due to the cost and feasibility of accessing an expert to verify the likelihood of re-identifying a dataset. However, in many cases, expert determination is required to ensure that re-identification of data is extremely unlikely.\n\n\n\n14.7.4 Data Minimization\nData minimization involves collecting, retaining, and processing only the necessary user data to reduce privacy risks from embedded ML systems. This starts by restricting the data types and instances gathered to the bare minimum required for the system’s core functionality. For example, an object detection model only collects the images needed for that specific computer vision task. Similarly, a voice assistant would limit audio capture to specific spoken commands rather than persistently recording ambient sounds.\nWhere possible, temporary data that briefly resides in memory without persisting storage provides additional minimization. A clear legal basis, like user consent, should be established for collection and retention. Sandboxing and access controls prevent unauthorized use beyond intended tasks. Retention periods should be defined based on purpose, with secure deletion procedures removing expired data.\nData minimization can be broken down into 3 categories:\n\n“Data must be adequate about the purpose that is pursued.” Data omission can limit the accuracy of models trained on the data and any general usefulness of a dataset. Data minimization requires a minimum amount of data to be collected from users while creating a dataset that adds value to others.\nThe data collected from users must be relevant to the purpose of the data collection.\nUsers’ data should be limited to only the necessary data to fulfill the purpose of the initial data collection. If similarly robust and accurate results can be obtained from a smaller dataset, any additional data beyond this smaller dataset should not be collected.\n\nEmerging techniques like differential Privacy, federated learning, and synthetic data generation allow useful insights derived from less raw user data. Performing data flow mapping and impact assessments helps identify opportunities to minimize raw data usage.\nMethodologies like Privacy by Design (Cavoukian 2009) consider such minimization early in system architecture. Regulations like GDPR also mandate data minimization principles. With a multilayered approach across legal, technical, and process realms, data minimization limits risks in embedded ML products.\n\nCavoukian, Ann. 2009. «Privacy by design». Office of the Information and Privacy Commissioner.\n\nCase Study - Performance-Based Data Minimization\nPerformance-based data minimization (Biega et al. 2020) focuses on expanding upon the third category of data minimization mentioned above, namely limitation. It specifically defines the robustness of model results on a given dataset by certain performance metrics, such that data should not be additionally collected if it does not significantly improve performance. Performance metrics can be divided into two categories:\n\nBiega, Asia J., Peter Potash, Hal Daumé, Fernando Diaz, e Michèle Finck. 2020. «Operationalizing the Legal Principle of Data Minimization for Personalization». In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, a cura di Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, e Yiqun Liu, 399–408. ACM. https://doi.org/10.1145/3397271.3401034.\n\nGlobal data minimization performance\n\n\nSatisfied if a dataset minimizes the amount of per-user data while its mean performance across all data is comparable to the mean performance of the original, unminimized dataset.\n\n\nPer user data minimization performance\n\n\nSatisfied if a dataset minimizes the amount of per-user data while the minimum performance of individual user data is comparable to that of individual user data in the original, unminimized dataset.\n\nPerformance-based data minimization can be leveraged in machine-learning settings, including movie recommendation algorithms and e-commerce settings.\nGlobal data minimization is much more feasible than per-user data minimization, given the much more significant difference in per-user losses between the minimized and original datasets.\n\n\n\n14.7.5 Consent and Transparency\nMeaningful consent and transparency are crucial when collecting user data for embedded ML products like smart speakers, wearables, and autonomous vehicles. When first set up. Ideally, the device should clearly explain what data types are gathered, for what purposes, how they are processed, and retention policies. For example, a smart speaker might collect voice samples to train speech recognition and personalized voice profiles. During use, reminders and dashboard options provide ongoing transparency into how data is handled, such as weekly digests of captured voice snippets. Control options allow revoking or limiting consent, like turning off the storage of voice profiles.\nConsent flows should provide granular controls beyond just binary yes/no choices. For instance, users could selectively consent to certain data uses, such as training speech recognition, but not personalization. Focus groups and usability testing with target users shape consent interfaces and wording of privacy policies to optimize comprehension and control. Respecting user rights, such as data deletion and rectification, demonstrates trustworthiness. Vague legal jargon hampers transparency. Regulations like GDPR and CCPA reinforce consent requirements. Thoughtful consent and transparency provide users agency over their data while building trust in embedded ML products through open communication and control.\n\n\n14.7.6 Privacy Concerns in Machine Learning\n\nGenerative AI\nPrivacy and security concerns have also risen with the public use of generative AI models, including OpenAI’s GPT4 and other LLMs. ChatGPT, in particular, has been discussed more recently about Privacy, given all the personal information collected from ChatGPT users. In June, a class action lawsuit was filed against ChatGPT due to concerns that it was trained on proprietary medical and personal information without proper permissions or consent. As a result of these privacy concerns, many companies have prohibited their employees from accessing ChatGPT, and uploading private, company related information to the chatbot. Further, ChatGPT is susceptible to prompt injection and other security attacks that could compromise the privacy of the proprietary data upon which it was trained.\n\nCase Study\nWhile ChatGPT has instituted protections to prevent people from accessing private and ethically questionable information, several individuals have successfully bypassed these protections through prompt injection and other security attacks. As demonstrated in Figura 14.9, users can bypass ChatGPT protections to mimic the tone of a “deceased grandmother” to learn how to bypass a web application firewall (Gupta et al. 2023).\n\n\n\n\n\n\nFigura 14.9: Grandma role play to bypass safety restrictions. Source: Gupta et al. (2023).\n\n\n\nFurther, users have also successfully used reverse psychology to manipulate ChatGPT and access information initially prohibited by the model. In Figura 14.10, a user is initially prevented from learning about piracy websites through ChatGPT but can bypass these restrictions using reverse psychology.\n\n\n\n\n\n\nFigura 14.10: Reverse psychology to bypass safety restrictions. Source: Gupta et al. (2023).\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, e Lopamudra Praharaj. 2023. «From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy». #IEEE_O_ACC# 11: 80218–45. https://doi.org/10.1109/access.2023.3300381.\n\n\nThe ease at which security attacks can manipulate ChatGPT is concerning, given the private information it was trained upon without consent. Further research on data privacy in LLMs and generative AI should focus on preventing the model from being so naive to prompt injection attacks.\n\n\n\nData Erasure\nMany previous regulations mentioned above, including GDPR, include a “right to be forgotten” clause. This clause essentially states that “the data subject shall have the right to obtain from the controller the erasure of personal data concerning him or her without undue delay.” However, in several cases, even if user data has been erased from a platform, the data is only partially erased if a machine learning model has been trained on this data for separate purposes. Through methods similar to membership inference attacks, other individuals can still predict the training data a model was trained upon, even if the data’s presence was explicitly removed online.\nOne approach to addressing privacy concerns with machine learning training data has been through differential privacy methods. For example, by adding Laplacian noise in the training set, a model can be robust to membership inference attacks, preventing deleted data from being recovered. Another approach to preventing deleted data from being inferred from security attacks is simply retraining the model from scratch on the remaining data. Since this process is time-consuming and computationally expensive, other researchers have attempted to address privacy concerns surrounding inferring model training data through a process called machine unlearning, in which a model actively iterates on itself to remove the influence of “forgotten” data that it might have been trained on, as mentioned below.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#privacy-preserving-ml-techniques",
    "href": "contents/privacy_security/privacy_security.html#privacy-preserving-ml-techniques",
    "title": "14  Security & Privacy",
    "section": "14.8 Privacy-Preserving ML Techniques",
    "text": "14.8 Privacy-Preserving ML Techniques\nMany techniques have been developed to preserve privacy, each addressing different aspects and data security challenges. These methods can be broadly categorized into several key areas: Differential Privacy, which focuses on statistical privacy in data outputs; Federated Learning, emphasizing decentralized data processing; Homomorphic Encryption and Secure Multi-party Computation (SMC), both enabling secure computations on encrypted or private data; Data Anonymization and Data Masking and Obfuscation, which alter data to protect individual identities; Private Set Intersection and Zero-Knowledge Proofs, facilitating secure data comparisons and validations; Decentralized Identifiers (DIDs) for self-sovereign digital identities; Privacy-Preserving Record Linkage (PPRL), linking data across sources without exposure; Synthetic Data Generation, creating artificial datasets for safe analysis; and Adversarial Learning Techniques, enhancing data or model resistance to privacy attacks.\nGiven the extensive range of these techniques, it is not feasible to dive into each in depth within a single course or discussion, let alone for anyone to know it all in its glorious detail. Therefore, we will explore a few specific techniques in relative detail, providing a deeper understanding of their principles, applications, and the unique privacy challenges they address in machine learning. This focused approach will give us a more comprehensive and practical understanding of key privacy-preserving methods in modern ML systems.\n\n14.8.1 Differential Privacy\n\nCore Idea\nDifferential Privacy is a framework for quantifying and managing the privacy of individuals in a dataset (Dwork et al. 2006). It provides a mathematical guarantee that the privacy of individuals in the dataset will not be compromised, regardless of any additional knowledge an attacker may possess. The core idea of differential Privacy is that the outcome of any analysis (like a statistical query) should be essentially the same, whether any individual’s data is included in the dataset or not. This means that by observing the analysis result, one cannot determine whether any individual’s data was used in the computation.\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, e Adam Smith. 2006. «Calibrating Noise to Sensitivity in Private Data Analysis». In Theory of Cryptography, a cura di Shai Halevi e Tal Rabin, 265–84. Berlin, Heidelberg: Springer Berlin Heidelberg.\nFor example, let’s say a database contains medical records for 10 patients. We want to release statistics about the prevalence of diabetes in this sample without revealing one patient’s condition. To do this, we could add a small amount of random noise to the true count before releasing it. If the true number of diabetes patients is 6, we might add noise from a Laplace distribution to randomly output 5, 6, or 7 each with some probability. An observer now can’t tell if any single patient has diabetes based only on the noisy output. The query result looks similar to whether each patient’s data is included or excluded. This is differential Privacy. More formally, a randomized algorithm satisfies ε-differential Privacy if, for any neighbor databases D and Dʹ differing by only one entry, the probability of any outcome changes by at most a factor of ε. A lower ε provides stronger privacy guarantees.\nThe Laplace Mechanism is one of the most straightforward and commonly used methods to achieve differential Privacy. It involves adding noise that follows a Laplace distribution to the data or query results. Apart from the Laplace Mechanism, the general principle of adding noise is central to differential Privacy. The idea is to add random noise to the data or the results of a query. The noise is calibrated to ensure the necessary privacy guarantee while keeping the data useful.\nWhile the Laplace distribution is common, other distributions like Gaussian can also be used. Laplace noise is used for strict ε-differential Privacy for low-sensitivity queries. In contrast, Gaussian distributions can be used when Privacy is not guaranteed, known as (ϵ, 𝛿)-Differential Privacy. In this relaxed version of differential Privacy, epsilon and delta define the amount of Privacy guaranteed when releasing information or a model related to a dataset. Epsilon sets a bound on how much information can be learned about the data based on the output. At the same time, delta allows for a small probability of the privacy guarantee to be violated. The choice between Laplace, Gaussian, and other distributions will depend on the specific requirements of the query and the dataset and the tradeoff between Privacy and accuracy.\nTo illustrate the tradeoff of Privacy and accuracy in (\\(\\epsilon\\), \\(\\delta\\))-differential Privacy, the following graphs in Figura 14.11 show the results on accuracy for different noise levels on the MNIST dataset, a large dataset of handwritten digits (Abadi et al. 2016). The delta value (black line; right y-axis) denotes the level of privacy relaxation (a high value means Privacy is less stringent). As Privacy becomes more relaxed, the accuracy of the model increases.\n\n\n\n\n\n\nFigura 14.11: Privacy-accuracy tradeoff. Source: Abadi et al. (2016).\n\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nThe key points to remember about differential Privacy are the following:\n\nAdding Noise: The fundamental technique in differential Privacy is adding controlled random noise to the data or query results. This noise masks the contribution of individual data points.\nBalancing Act: There’s a balance between Privacy and accuracy. More noise (lower ϵ) in the data means higher Privacy but less accuracy in the model’s results.\nUniversality: Differential Privacy doesn’t rely on assumptions about what an attacker knows. This makes it robust against re-identification attacks, where an attacker tries to uncover individual data.\nApplicability: It can be applied to various types of data and queries, making it a versatile tool for privacy-preserving data analysis.\n\n\n\nTradeoffs\nThere are several tradeoffs to make with differential Privacy, as is the case with any algorithm. But let’s focus on the computational-specific tradeoffs since we care about ML systems. There are some key computational considerations and tradeoffs when implementing differential Privacy in a machine-learning system:\nNoise generation: Implementing differential Privacy introduces several important computational tradeoffs compared to standard machine learning techniques. One major consideration is the need to securely generate random noise from distributions like Laplace or Gaussian that get added to query results and model outputs. High-quality cryptographic random number generation can be computationally expensive.\nSensitivity analysis: Another key requirement is rigorously tracking the sensitivity of the underlying algorithms to single data points getting added or removed. This global sensitivity analysis is required to calibrate the noise levels properly. However, analyzing worst-case sensitivity can substantially increase computational complexity for complex model training procedures and data pipelines.\nPrivacy budget management: Managing the privacy loss budget across multiple queries and learning iterations is another bookkeeping overhead. The system must keep track of cumulative privacy costs and compose them to explain overall privacy guarantees. This adds a computational burden beyond just running queries or training models.\nBatch vs. online tradeoffs: For online learning systems with continuous high-volume queries, differentially private algorithms require new mechanisms to maintain utility and prevent too much accumulated privacy loss since each query can potentially alter the privacy budget. Batch offline processing is simpler from a computational perspective as it processes data in large batches, where each batch is treated as a single query. High-dimensional sparse data also increases sensitivity analysis challenges.\nDistributed training: When training models using distributed or federated approaches, new cryptographic protocols are needed to track and bound privacy leakage across nodes. Secure multiparty computation with encrypted data for differential Privacy adds substantial computational load.\nWhile differential Privacy provides strong formal privacy guarantees, implementing it rigorously requires additions and modifications to the machine learning pipeline at a computational cost. Managing these overheads while preserving model accuracy remains an active research area.\n\n\nCase Study\nApple’s implementation of differential Privacy in iOS and MacOS provides a prominent real-world example of how differential Privacy can be deployed at large scale. Apple wanted to collect aggregated usage statistics across their ecosystem to improve products and services, but aimed to do so without compromising individual user privacy.\nTo achieve this, they implemented differential privacy techniques directly on user devices to anonymize data points before sending them to Apple servers. Specifically, Apple uses the Laplace mechanism to inject carefully calibrated random noise. For example, suppose a user’s location history contains [Work, Home, Work, Gym, Work, Home]. In that case, the differentially private version might replace the exact locations with a noisy sample like [Gym, Home, Work, Work, Home, Work].\nApple tunes the Laplace noise distribution to provide a high level of Privacy while preserving the utility of aggregated statistics. Increasing noise levels provides stronger privacy guarantees (lower ε values in DP terminology) but can reduce data utility. Apple’s privacy engineers empirically optimized this tradeoff based on their product goals.\nApple obtains high-fidelity aggregated statistics by aggregating hundreds of millions of noisy data points from devices. For instance, they can analyze new iOS apps’ features while masking any user’s app behaviors. On-device computation avoids sending raw data to Apple servers.\nThe system uses hardware-based secure random number generation to sample from the Laplace distribution on devices efficiently. Apple also had to optimize its differentially private algorithms and pipeline to operate under the computational constraints of consumer hardware.\nMultiple third-party audits have verified that Apple’s system provides rigorous differential privacy protections in line with their stated policies. Of course, assumptions around composition over time and potential re-identification risks still apply. Apple’s deployment shows how differential Privacy can be realized in large real-world products when backed by sufficient engineering resources.\n\n\n\n\n\n\nEsercizio 14.1: Differential Privacy - TensorFlow Privacy\n\n\n\n\n\nWant to train an ML model without compromising anyone’s secrets? Differential Privacy is like a superpower for your data! In this Colab, we’ll use TensorFlow Privacy to add special noise during training. This makes it way harder for anyone to determine if a single person’s data was used, even if they have sneaky ways of peeking at the model.\n\n\n\n\n\n\n\n14.8.2 Federated Learning\n\nCore Idea\nFederated Learning (FL) is a type of machine learning in which a model is built and distributed across multiple devices or servers while keeping the training data localized. It was previously discussed in the Model Optimizations chapter. Still, we will recap it here briefly to complete it and focus on things that pertain to this chapter.\nFL aims to train machine learning models across decentralized networks of devices or systems while keeping all training data localized. Figura 14.12 illustrates this process: each participating device leverages its local data to calculate model updates, which are then aggregated to build an improved global model. However, the raw training data is never directly shared, transferred, or compiled. This privacy-preserving approach allows for the joint development of ML models without centralizing the potentially sensitive training data in one place.\n\n\n\n\n\n\nFigura 14.12: Federated Learning lifecycle. Source: Jin et al. (2020).\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, e Qiang Yang. 2020. «Towards utilizing unlabeled data in federated learning: A survey and prospective». arXiv preprint arXiv:2002.11545.\n\n\nOne of the most common model aggregation algorithms is Federated Averaging (FedAvg), where the global model is created by averaging all of the parameters from local parameters. While FedAvg works well with independent and identically distributed data (IID), alternate algorithms like Federated Proximal (FedProx) are crucial in real-world applications where data is often non-IID. FedProx is designed for the FL process when there is significant heterogeneity in the client updates due to diverse data distributions across devices, computational capabilities, or varied amounts of data.\nBy leaving the raw data distributed and exchanging only temporary model updates, federated learning provides a more secure and privacy-enhancing alternative to traditional centralized machine learning pipelines. This allows organizations and users to benefit collaboratively from shared models while maintaining control and ownership over sensitive data. The decentralized nature of FL also makes it robust to single points of failure.\nImagine a group of hospitals that want to collaborate on a study to predict patient outcomes based on their symptoms. However, they cannot share their patient data due to privacy concerns and regulations like HIPAA. Here’s how Federated Learning can help.\n\nLocal Training: Each hospital trains a machine learning model on patient data. This training happens locally, meaning the data never leaves the hospital’s servers.\nModel Sharing: After training, each hospital only sends the model (specifically, its parameters or weights ) to a central server. It does not send any patient data.\nAggregating Models: The central server aggregates these models from all hospitals into a single, more robust model. This process typically involves averaging the model parameters.\nBenefit: The result is a machine learning model that has learned from a wide range of patient data without sharing sensitive data or removing it from its original location.\n\n\n\nTradeoffs\nThere are several system performance-related aspects of FL in machine learning systems. It would be wise to understand these tradeoffs because there is no “free lunch” for preserving Privacy through FL (Li et al. 2020).\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, e Virginia Smith. 2020. «Federated Learning: Challenges, Methods, and Future Directions». IEEE Signal Process Mag. 37 (3): 50–60. https://doi.org/10.1109/msp.2020.2975749.\nCommunication Overhead and Network Constraints: In FL, one of the most significant challenges is managing the communication overhead. This involves the frequent transmission of model updates between a central server and numerous client devices, which can be bandwidth-intensive. The total number of communication rounds and the size of transmitted messages per round need to be reduced to minimize communication further. This can lead to substantial network traffic, especially in scenarios with many participants. Additionally, latency becomes a critical factor — the time taken for these updates to be sent, aggregated, and redistributed can introduce delays. This affects the overall training time and impacts the system’s responsiveness and real-time capabilities. Managing this communication while minimizing bandwidth usage and latency is crucial for implementing FL.\nComputational Load on Local Devices: FL relies on client devices (like smartphones or IoT devices, which especially matter in TinyML) for model training, which often have limited computational power and battery life. Running complex machine learning algorithms locally can strain these resources, leading to potential performance issues. Moreover, the capabilities of these devices can vary significantly, resulting in uneven contributions to the model training process. Some devices process updates faster and more efficiently than others, leading to disparities in the learning process. Balancing the computational load to ensure consistent participation and efficiency across all devices is a key challenge in FL.\nModel Training Efficiency: FL’s decentralized nature can impact model training’s efficiency. Achieving convergence, where the model no longer significantly improves, can be slower in FL than in centralized training methods. This is particularly true in cases where the data is non-IID (non-independent and identically distributed) across devices. Additionally, the algorithms used for aggregating model updates play a critical role in the training process. Their efficiency directly affects the speed and effectiveness of learning. Developing and implementing algorithms that can handle the complexities of FL while ensuring timely convergence is essential for the system’s performance.\nScalability Challenges: Scalability is a significant concern in FL, especially as the number of participating devices increases. Managing and coordinating model updates from many devices adds complexity and can strain the system. Ensuring that the system architecture can efficiently handle this increased load without degrading performance is crucial. This involves not just handling the computational and communication aspects but also maintaining the quality and consistency of the model as the scale of the operation grows. A key challenge is designing FL systems that scale effectively while maintaining performance.\nData Synchronization and Consistency: Ensuring data synchronization and maintaining model consistency across all participating devices in FL is challenging. Keeping all devices synchronized with the latest model version can be difficult in environments with intermittent connectivity or devices that go offline periodically. Furthermore, maintaining consistency in the learned model, especially when dealing with a wide range of devices with different data distributions and update frequencies, is crucial. This requires sophisticated synchronization and aggregation strategies to ensure that the final model accurately reflects the learnings from all devices.\nEnergy Consumption: The energy consumption of client devices in FL is a critical factor, particularly for battery-powered devices like smartphones and other TinyML/IoT devices. The computational demands of training models locally can lead to significant battery drain, which might discourage continuous participation in the FL process. Balancing the computational requirements of model training with energy efficiency is essential. This involves optimizing algorithms and training processes to reduce energy consumption while achieving effective learning outcomes. Ensuring energy-efficient operation is key to user acceptance and the sustainability of FL systems.\n\n\nCase Studies\nHere are a couple of real-world case studies that can illustrate the use of federated learning:\n\nGoogle Gboard\nGoogle uses federated learning to improve predictions on its Gboard mobile keyboard app. The app runs a federated learning algorithm on users’ devices to learn from their local usage patterns and text predictions while keeping user data private. The model updates are aggregated in the cloud to produce an enhanced global model. This allows for providing next-word predictions personalized to each user’s typing style while avoiding directly collecting sensitive typing data. Google reported that the federated learning approach reduced prediction errors by 25% compared to the baseline while preserving Privacy.\n\n\nHealthcare Research\nThe UK Biobank and American College of Cardiology combined datasets to train a model for heart arrhythmia detection using federated learning. The datasets could not be combined directly due to legal and Privacy restrictions. Federated learning allowed collaborative model development without sharing protected health data, with only model updates exchanged between the parties. This improved model accuracy as it could leverage a wider diversity of training data while meeting regulatory requirements.\n\n\nFinancial Services\nBanks are exploring using federated learning for anti-money laundering (AML) detection models. Multiple banks could jointly improve AML Models without sharing confidential customer transaction data with competitors or third parties. Only the model updates need to be aggregated rather than raw transaction data. This allows access to richer training data from diverse sources while avoiding regulatory and confidentiality issues around sharing sensitive financial customer data.\nThese examples demonstrate how federated learning provides tangible privacy benefits and enables collaborative ML in settings where direct data sharing is impossible.\n\n\n\n\n14.8.3 Machine Unlearning\n\nCore Idea\nMachine unlearning is a fairly new process that describes how the influence of a subset of training data can be removed from the model. Several methods have been used to perform machine unlearning and remove the influence of a subset of training data from the final model. A baseline approach might consist of simply fine-tuning the model for more epochs on just the data that should be remembered to decrease the influence of the data “forgotten” by the model. Since this approach doesn’t explicitly remove the influence of data that should be erased, membership inference attacks are still possible, so researchers have adopted other approaches to unlearn data from a model explicitly. One type of approach that researchers have adopted includes adjusting the model loss function to treat the losses of the “forget set explicitly” (data to be unlearned) and the “retain set” (remaining data that should still be remembered) differently (Tarun et al. 2022; Khan e Swaroop 2021).\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, e Mohan Kankanhalli. 2022. «Deep Regression Unlearning». ArXiv preprint abs/2210.08196. https://arxiv.org/abs/2210.08196.\n\nKhan, Mohammad Emtiyaz, e Siddharth Swaroop. 2021. «Knowledge-Adaptation Priors». In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, a cura di Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 19757–70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\nCase Study\nSome researchers demonstrate a real-life example of machine unlearning approaches applied to SOTA machine learning models through training an LLM, LLaMA2-7b, to unlearn any references to Harry Potter (Eldan e Russinovich 2023). Though this model took 184K GPU hours to pre-train, it only took 1 GPU hour of fine-tuning to erase the model’s ability to generate or recall Harry Potter-related content without noticeably compromising the accuracy of generating content unrelated to Harry Potter. Figura 14.13 demonstrates how the model output changes before (Llama-7b-chat-hf column) and after (Finetuned Llama-b column) unlearning has occurred.\n\n\n\n\n\n\nFigura 14.13: Llama unlearning Harry Potter. Source: Eldan e Russinovich (2023).\n\n\nEldan, Ronen, e Mark Russinovich. 2023. «Who’s Harry Potter? Approximate Unlearning in LLMs». ArXiv preprint abs/2310.02238. https://arxiv.org/abs/2310.02238.\n\n\n\n\nOther Uses\n\nRemoving adversarial data\nDeep learning models have previously been shown to be vulnerable to adversarial attacks, in which the attacker generates adversarial data similar to the original training data, where a human cannot tell the difference between the real and fabricated data. The adversarial data results in the model outputting incorrect predictions, which could have detrimental consequences in various applications, including healthcare diagnosis predictions. Machine unlearning has been used to unlearn the influence of adversarial data to prevent these incorrect predictions from occurring and causing any harm\n\n\n\n\n14.8.4 Homomorphic Encryption\n\nCore Idea\nHomomorphic encryption is a form of encryption that allows computations to be carried out on ciphertext, generating an encrypted result that, when decrypted, matches the result of operations performed on the plaintext. For example, multiplying two numbers encrypted with homomorphic encryption produces an encrypted product that decrypts the actual product of the two numbers. This means that data can be processed in an encrypted form, and only the resulting output needs to be decrypted, significantly enhancing data security, especially for sensitive information.\nHomomorphic encryption enables outsourced computation on encrypted data without exposing the data itself to the external party performing the operations. However, only certain computations like addition and multiplication are supported in partially homomorphic schemes. Fully homomorphic encryption (FHE) that can handle any computation is even more complex. The number of possible operations is limited before noise accumulation corrupts the ciphertext.\nTo use homomorphic encryption across different entities, carefully generated public keys must be exchanged for operations across separately encrypted data. This advanced encryption technique enables previously impossible secure computation paradigms but requires expertise to implement correctly for real-world systems.\n\n\nBenefits\nHomomorphic encryption enables machine learning model training and inference on encrypted data, ensuring that sensitive inputs and intermediate values remain confidential. This is critical in healthcare, finance, genetics, and other domains, which are increasingly relying on ML to analyze sensitive and regulated data sets containing billions of personal records.\nHomomorphic encryption thwarts attacks like model extraction and membership inference that could expose private data used in ML workflows. It provides an alternative to TEEs using hardware enclaves for confidential computing. However, current schemes have high computational overheads and algorithmic limitations that constrain real-world applications.\nHomomorphic encryption realizes the decades-old vision of secure multiparty computation by allowing computation on ciphertexts. Conceptualized in the 1970s, the first fully homomorphic cryptosystems emerged in 2009, enabling arbitrary computations. Ongoing research is making these techniques more efficient and practical.\nHomomorphic encryption shows great promise in enabling privacy-preserving machine learning under emerging data regulations. However, given constraints, one should carefully evaluate its applicability against other confidential computing approaches. Extensive resources exist to explore homomorphic encryption and track progress in easing adoption barriers.\n\n\nMechanics\n\nData Encryption: Before data is processed or sent to an ML model, it is encrypted using a homomorphic encryption scheme and public key. For example, encrypting numbers \\(x\\) and \\(y\\) generates ciphertexts \\(E(x)\\) and \\(E(y)\\).\nComputation on Ciphertext: The ML algorithm processes the encrypted data directly. For instance, multiplying the ciphertexts \\(E(x)\\) and \\(E(y)\\) generates \\(E(xy)\\). More complex model training can also be done on ciphertexts.\nResult Encryption: The result \\(E(xy)\\) remains encrypted and can only be decrypted by someone with the corresponding private key to reveal the actual product \\(xy\\).\n\nOnly authorized parties with the private key can decrypt the final outputs, protecting the intermediate state. However, noise accumulates with each operation, preventing further computation without decryption.\nBeyond healthcare, homomorphic encryption enables confidential computing for applications like financial fraud detection, insurance analytics, genetics research, and more. It offers an alternative to techniques like multipartymultiparty computation and TEEs. Ongoing research aims to improve the efficiency and capabilities.\nTools like HElib, SEAL, and TensorFlow HE provide libraries for exploring implementing homomorphic encryption in real-world machine learning pipelines.\n\n\nTradeoffs\nFor many real-time and embedded applications, fully homomorphic encryption remains impractical for the following reasons.\nComputational Overhead: Homomorphic encryption imposes very high computational overheads, often resulting in slowdowns of over 100x for real-world ML applications. This makes it impractical for many time-sensitive or resource-constrained uses. Optimized hardware and parallelization can help but not eliminate this issue.\nComplexity of Implementation The sophisticated algorithms require deep expertise in cryptography to be implemented correctly. Nuances like format compatibility with floating point ML models and scalable key management pose hurdles. This complexity hinders widespread practical adoption.\nAlgorithmic Limitations: Current schemes restrict the functions and depth of computations supported, limiting the models and data volumes that can be processed. Ongoing research is pushing these boundaries, but restrictions remain.\nHardware Acceleration: Homomorphic encryption requires specialized hardware, such as secure processors or coprocessors with TEEs, which adds design and infrastructure costs.\nHybrid Designs: Rather than encrypting entire workflows, selective application of homomorphic encryption to critical subcomponents can achieve protection while minimizing overheads.\n\n\n\n\n\n\nEsercizio 14.2: Homomorphic Encryption\n\n\n\n\n\nReady to unlock the power of encrypted computation? Homomorphic encryption is like a magic trick for your data! In this Colab, we’ll learn how to do calculations on secret numbers without ever revealing them. Imagine training a model on data you can’t even see – that’s the power of this mind-bending technology.\n\n\n\n\n\n\n\n14.8.5 Secure Multiparty Communication\n\nCore Idea\nThe overarching goal of MPC is to enable different parties to jointly compute a function over their inputs while keeping those inputs private. For example, two organizations may want to collaborate on training a machine learning model by combining their respective data sets. Still, they cannot directly reveal that data due to Privacy or confidentiality constraints. MPC aims to provide protocols and techniques that allow them to achieve the benefits of pooled data for model accuracy without compromising the privacy of each organization’s sensitive data.\nAt a high level, MPC works by carefully splitting the computation into parts that each party can execute independently using their private input. The results are then combined to reveal only the final output of the function and nothing about the intermediate values. Cryptographic techniques are used to guarantee that the partial results remain private provably.\nLet’s take a simple example of an MPC protocol. One of the most basic MPC protocols is the secure addition of two numbers. Each party splits its input into random shares that are secretly distributed. They exchange the shares and locally compute the sum of the shares, which reconstructs the final sum without revealing the individual inputs. For example, if Alice has input x and Bob has input y:\n\nAlice generates random \\(x_1\\) and sets \\(x_2 = x - x_1\\)\nBob generates random \\(y_1\\) and sets \\(y_2 = y - y_1\\)\nAlice sends \\(x_1\\) to Bob, Bob sends \\(y_1\\) to Alice (keeping \\(x_2\\) and \\(y_2\\) secret)\nAlice computes \\(x_2 + y_1 = s_1\\), Bob computes \\(x_1 + y_2 = s_2\\)\n\\(s_1 + s_2 = x + y\\) is the final sum, without revealing \\(x\\) or \\(y\\).\n\nAlice’s and Bob’s individual inputs (\\(x\\) and \\(y\\)) remain private, and each party only reveals one number associated with their original inputs. The random spits ensure no information about the original numbers disclosed\nSecure Comparison: Another basic operation is a secure comparison of two numbers, determining which is greater than the other. This can be done using techniques like Yao’s Garbled Circuits, where the comparison circuit is encrypted to allow joint evaluation of the inputs without leaking them.\nSecure Matrix Multiplication: Matrix operations like multiplication are essential for machine learning. MPC techniques like additive secret sharing can be used to split matrices into random shares, compute products on the shares, and then reconstruct the result.\nSecure Model Training: Distributed machine learning training algorithms like federated averaging can be made secure using MPC. Model updates computed on partitioned data at each node are secretly shared between nodes and aggregated to train the global model without exposing individual updates.\nThe core idea behind MPC protocols is to divide the computation into steps that can be executed jointly without revealing intermediate sensitive data. This is accomplished by combining cryptographic techniques like secret sharing, homomorphic encryption, oblivious transfer, and garbled circuits. MPC protocols enable the collaborative computation of sensitive data while providing provable privacy guarantees. This privacy-preserving capability is essential for many machine learning applications today involving multiple parties that cannot directly share their raw data.\nThe main approaches used in MPC include:\n\nHomomorphic encryption: Special encryption allows computations to be carried out on encrypted data without decrypting it.\nSecret sharing: The private data is divided into random shares distributed to each party. Computations are done locally on the shares and finally reconstructed.\nOblivious transfer: A protocol where a receiver obtains a subset of data from a sender, but the sender does not know which specific data was transferred.\nGarbled circuits: The function to be computed is represented as a Boolean circuit that is encrypted (“garbled”) to allow joint evaluation without revealing inputs.\n\n\n\nTradeoffs\nWhile MPC protocols provide strong privacy guarantees, they come at a high computational cost compared to plain computations. Every secure operation, like addition, multiplication, comparison, etc., requires more processing orders than the equivalent unencrypted operation. This overhead stems from the underlying cryptographic techniques:\n\nIn partially homomorphic encryption, each computation on ciphertexts requires costly public-key operations. Fully homomorphic encryption has even higher overheads.\nSecret sharing divides data into multiple shares, so even basic operations require manipulating many shares.\nOblivious transfer and garbled circuits add masking and encryption to hide data access patterns and execution flows.\nMPC systems require extensive communication and interaction between parties to compute on shares/ciphertexts jointly.\n\nAs a result, MPC protocols can slow down computations by 3-4 orders of magnitude compared to plain implementations. This becomes prohibitively expensive for large datasets and models. Therefore, training machine learning models on encrypted data using MPC remains infeasible today for realistic dataset sizes due to the overhead. Clever optimizations and approximations are needed to make MPC practical.\nOngoing MPC research aims to close this efficiency gap through cryptographic advances, new algorithms, trusted hardware like SGX enclaves, and leveraging accelerators like GPUs/TPUs. However, in the foreseeable future, some degree of approximation and performance tradeoff is needed to scale MPC to meet the demands of real-world machine learning systems.\n\n\n\n14.8.6 Synthetic Data Generation\n\nCore Idea\nSynthetic data generation has emerged as an important privacy-preserving machine learning approach that allows models to be developed and tested without exposing real user data. The key idea is to train generative models on real-world datasets and then sample from these models to synthesize artificial data that statistically match the original data distribution but does not contain actual user information. For example, a GAN could be trained on a dataset of sensitive medical records to learn the underlying patterns and then used to sample synthetic patient data.\nThe primary challenge of synthesizing data is to ensure adversaries are unable to re-identify the original dataset. A simple approach to achieving synthetic data is adding noise to the original dataset, which still risks privacy leakage. When noise is added to data in the context of differential privacy, sophisticated mechanisms based on the data’s sensitivity are used to calibrate the amount and distribution of noise. Through these mathematically rigorous frameworks, differential Privacy generally guarantees Privacy at some level, which is the primary goal of this privacy-preserving technique. Beyond preserving privacy, synthetic data combats multiple data availability issues such as imbalanced datasets, scarce datasets, and anomaly detection.\nResearchers can freely share this synthetic data and collaborate on modeling without revealing private medical information. Well-constructed synthetic data protects Privacy while providing utility for developing accurate models. Key techniques to prevent reconstructing the original data include adding differential privacy noise during training, enforcing plausibility constraints, and using multiple diverse generative models. Here are some common approaches for generating synthetic data:\n\nGenerative Adversarial Networks (GANs): GANs are an AI algorithm used in unsupervised learning where two neural networks compete against each other in a game. Figura 14.14 is an overview of the GAN system. The generator network (big red box) is responsible for producing the synthetic data, and the discriminator network (yellow box) evaluates the authenticity of the data by distinguishing between fake data created by the generator network and the real data. The generator and discriminator networks learn and update their parameters based on the results. The discriminator acts as a metric on how similar the fake and real data are to one another. It is highly effective at generating realistic data and is a popular approach for generating synthetic data.\n\n\n\n\n\n\n\nFigura 14.14: Flowchart of GANs. Source: Rosa e Papa (2021).\n\n\nRosa, Gustavo H. de, e João P. Papa. 2021. «A survey on text generation using generative adversarial networks». Pattern Recogn. 119 (novembre): 108098. https://doi.org/10.1016/j.patcog.2021.108098.\n\n\n\nVariational Autoencoders (VAEs): VAEs are neural networks capable of learning complex probability distributions and balancing data generation quality and computational efficiency. They encode data into a latent space where they learn the distribution to decode the data back.\nData Augmentation: This involves transforming existing data to create new, altered data. For example, flipping, rotating, and scaling (uniformly or non-uniformly) original images can help create a more diverse, robust image dataset before training an ML model.\nSimulations: Mathematical models can simulate real-world systems or processes to mimic real-world phenomena. This is highly useful in scientific research, urban planning, and economics.\n\n\n\nBenefits\nWhile synthetic data may be necessary due to Privacy or compliance risks, it is widely used in machine learning models when available data is of poor quality, scarce, or inaccessible. Synthetic data offers more efficient and effective development by streamlining robust model training, testing, and deployment processes. It allows researchers to share models more widely without breaching privacy laws and regulations. Collaboration between users of the same dataset will be facilitated, which will help broaden the capabilities and advancements in ML research.\nThere are several motivations for using synthetic data in machine learning:\n\nPrivacy and compliance: Synthetic data avoids exposing personal information, allowing more open sharing and collaboration. This is important when working with sensitive datasets like healthcare records or financial information.\nData scarcity: When insufficient real-world data is available, synthetic data can augment training datasets. This improves model accuracy when limited data is a bottleneck.\nModel testing: Synthetic data provides privacy-safe sandboxes for testing model performance, debugging issues, and monitoring for bias.\nData labeling: High-quality labeled training data is often scarce and expensive. Synthetic data can help auto-generate labeled examples.\n\n\n\nTradeoffs\nWhile synthetic data aims to remove any evidence of the original dataset, privacy leakage is still a risk since the synthetic data mimics the original data. The statistical information and distribution are similar, if not the same, between the original and synthetic data. By resampling from the distribution, adversaries may still be able to recover the original training samples. Due to their inherent learning processes and complexities, neural networks might accidentally reveal sensitive information about the original training data.\nA core challenge with synthetic data is the potential gap between synthetic and real-world data distributions. Despite advancements in generative modeling techniques, synthetic data may only partially capture real data’s complexity, diversity, and nuanced patterns. This can limit the utility of synthetic data for robustly training machine learning models. Rigorously evaluating synthetic data quality through adversary methods and comparing model performance to real data benchmarks helps assess and improve fidelity. However, inherently, synthetic data remains an approximation.\nAnother critical concern is the privacy risks of synthetic data. Generative models may leak identifiable information about individuals in the training data, which could enable reconstruction of private information. Emerging adversarial attacks demonstrate the challenges in preventing identity leakage from synthetic data generation pipelines. Techniques like differential Privacy can help safeguard Privacy but come with tradeoffs in data utility. There is an inherent tension between producing useful synthetic data and fully protecting sensitive training data, which must be balanced.\nAdditional pitfalls of synthetic data include amplified biases, labeling difficulties, the computational overhead of training generative models, storage costs, and failure to account for out-of-distribution novel data. While these are secondary to the core synthetic-real gap and privacy risks, they remain important considerations when evaluating the suitability of synthetic data for particular machine-learning tasks. As with any technique, the advantages of synthetic data come with inherent tradeoffs and limitations that require thoughtful mitigation strategies.\n\n\n\n14.8.7 Summary\nWhile all the techniques we have discussed thus far aim to enable privacy-preserving machine learning, they involve distinct mechanisms and tradeoffs. Factors like computational constraints, required trust assumptions, threat models, and data characteristics help guide the selection process for a particular use case. However, finding the right balance between Privacy, accuracy, and efficiency necessitates experimentation and empirical evaluation for many applications. Tabella 14.2 is a comparison table of the key privacy-preserving machine learning techniques and their pros and cons:\n\n\n\nTabella 14.2: Comparing techniques for privacy-preserving machine learning.\n\n\n\n\n\n\n\n\n\n\nTechnique\nPros\nCons\n\n\n\n\nDifferential Privacy\n\nStrong formal privacy guarantees\nRobust to auxiliary data attacks\nVersatile for many data types and analyses\n\n\nAccuracy loss from noise addition\nComputational overhead for sensitivity analysis and noise generation\n\n\n\nFederated Learning\n\nAllows collaborative learning without sharing raw data\nData remains decentralized improving security\nNo need for encrypted computation\n\n\nIncreased communication overhead\nPotentially slower model convergence\nUneven client device capabilities\n\n\n\nSecure Multi-Party Computation\n\nEnables joint computation on sensitive data\nProvides cryptographic privacy guarantees\nFlexible protocols for various functions\n\n\nVery high computational overhead\nComplexity of implementation\nAlgorithmic constraints on function depth\n\n\n\nHomomorphic Encryption\n\nAllows computation on encrypted data\nPrevents intermediate state exposure\n\n\nExtremely high computational cost\nComplex cryptographic implementations\nRestrictions on function types\n\n\n\nSynthetic Data Generation\n\nEnables data sharing without leakage\nMitigates data scarcity problems\n\n\nSynthetic-real gap in distributions\nPotential for reconstructing private data\nBiases and labeling challenges",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#conclusion",
    "href": "contents/privacy_security/privacy_security.html#conclusion",
    "title": "14  Security & Privacy",
    "section": "14.9 Conclusion",
    "text": "14.9 Conclusion\nMachine learning hardware security is critical as embedded ML systems are increasingly deployed in safety-critical domains like medical devices, industrial controls, and autonomous vehicles. We have explored various threats spanning hardware bugs, physical attacks, side channels, supply chain risks, etc. Defenses like TEEs, Secure Boot, PUFs, and hardware security modules provide multilayer protection tailored for resource-constrained embedded devices.\nHowever, continual vigilance is essential to track emerging attack vectors and address potential vulnerabilities through secure engineering practices across the hardware lifecycle. As ML and embedded ML spread, maintaining rigorous security foundations that match the field’s accelerating pace of innovation remains imperative.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/privacy_security/privacy_security.html#sec-security-and-privacy-resource",
    "href": "contents/privacy_security/privacy_security.html#sec-security-and-privacy-resource",
    "title": "14  Security & Privacy",
    "section": "14.10 Resources",
    "text": "14.10 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nSecurity.\nPrivacy.\nMonitoring after Deployment.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 14.1\nVideo 14.2\nVideo 14.3\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nEsercizio 14.1\nEsercizio 14.2\n\n\n\n\n\n\n\n\n\n\nLabs\n\n\n\n\n\nIn addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.\n\nComing soon.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html",
    "href": "contents/responsible_ai/responsible_ai.html",
    "title": "15  Responsible AI",
    "section": "",
    "text": "15.1 Introduction\nMachine learning models are increasingly used to automate decisions in high-stakes social domains like healthcare, criminal justice, and employment. However, without deliberate care, these algorithms can perpetuate biases, breach privacy, or cause other harm. For instance, a loan approval model solely trained on data from high-income neighborhoods could disadvantage applicants from lower-income areas. This motivates the need for responsible machine learning - creating fair, accountable, transparent, and ethical models.\nSeveral core principles underlie responsible ML. Fairness ensures models do not discriminate based on gender, race, age, and other attributes. Explainability enables humans to interpret model behaviors and improve transparency. Robustness and safety techniques prevent vulnerabilities like adversarial examples. Rigorous testing and validation help reduce unintended model weaknesses or side effects.\nImplementing responsible ML presents both technical and ethical challenges. Developers must grapple with defining fairness mathematically, balancing competing objectives like accuracy vs interpretability, and securing quality training data. Organizations must also align incentives, policies, and culture to uphold ethical AI.\nThis chapter will equip you to critically evaluate AI systems and contribute to developing beneficial and ethical machine learning applications by covering the foundations, methods, and real-world implications of responsible ML. The responsible ML principles discussed are crucial knowledge as algorithms mediate more aspects of human society.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html#definition",
    "href": "contents/responsible_ai/responsible_ai.html#definition",
    "title": "15  Responsible AI",
    "section": "15.2 Definition",
    "text": "15.2 Definition\nResponsible AI is about developing AI that positively impacts society under human ethics and values. There is no universally agreed-upon definition of “responsible AI,” but here is a summary of how it is commonly described. Responsible AI refers to designing, developing, and deploying artificial intelligence systems in an ethical, socially beneficial way. The core goal is to create trustworthy, unbiased, fair, transparent, accountable, and safe AI. While there is no canonical definition, responsible AI is generally considered to encompass principles such as:\n\nFairness: Avoiding biases, discrimination, and potential harm to certain groups or populations\nExplainability: Enabling humans to understand and interpret how AI models make decisions\nTransparency: Openly communicating how AI systems operate, are built, and are evaluated\nAccountability: Having processes to determine responsibility and liability for AI failures or negative impacts\nRobustness: Ensuring AI systems are secure, reliable, and behave as intended\nPrivacy: Protecting sensitive user data and adhering to privacy laws and ethics\n\nPutting these principles into practice involves technical techniques, corporate policies, governance frameworks, and moral philosophy. There are also ongoing debates around defining ambiguous concepts like fairness and determining how to balance competing objectives.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html#principles-and-concepts",
    "href": "contents/responsible_ai/responsible_ai.html#principles-and-concepts",
    "title": "15  Responsible AI",
    "section": "15.3 Principles and Concepts",
    "text": "15.3 Principles and Concepts\n\n15.3.1 Transparency and Explainability\nMachine learning models are often criticized as mysterious “black boxes” - opaque systems where it’s unclear how they arrived at particular predictions or decisions. For example, an AI system called COMPAS used to assess criminal recidivism risk in the US was found to be racially biased against black defendants. Still, the opacity of the algorithm made it difficult to understand and fix the problem. This lack of transparency can obscure biases, errors, and deficiencies.\nExplaining model behaviors helps engender trust from the public and domain experts and enables identifying issues to address. Interpretability techniques like LIME, Shapley values, and saliency maps empower humans to understand and validate model logic. Laws like the EU’s GDPR also mandate transparency, which requires explainability for certain automated decisions. Overall, transparency and explainability are critical pillars of responsible AI.\n\n\n15.3.2 Fairness, Bias, and Discrimination\nML models trained on historically biased data often perpetuate and amplify those prejudices. Healthcare algorithms have been shown to disadvantage black patients by underestimating their needs (Obermeyer et al. 2019). Facial recognition needs to be more accurate for women and people of color. Such algorithmic discrimination can negatively impact people’s lives in profound ways.\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, e Sendhil Mullainathan. 2019. «Dissecting racial bias in an algorithm used to manage the health of populations». Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\nDifferent philosophical perspectives also exist on fairness - for example, is it fairer to treat all individuals equally or try to achieve equal outcomes for groups? Ensuring fairness requires proactively detecting and mitigating biases in data and models. However, achieving perfect fairness is tremendously difficult due to contrasting mathematical definitions and ethical perspectives. Still, promoting algorithmic fairness and non-discrimination is a key responsibility in AI development.\n\n\n15.3.3 Privacy and Data Governance\nMaintaining individuals’ privacy is an ethical obligation and legal requirement for organizations deploying AI systems. Regulations like the EU’s GDPR mandate data privacy protections and rights, such as the ability to access and delete one’s data.\nHowever, maximizing the utility and accuracy of data for training models can conflict with preserving privacy - modeling disease progression could benefit from access to patients’ full genomes, but sharing such data widely violates privacy.\nResponsible data governance involves carefully anonymizing data, controlling access with encryption, getting informed consent from data subjects, and collecting the minimum data needed. Honoring privacy is challenging but critical as AI capabilities and adoption expand.\n\n\n15.3.4 Safety and Robustness\nPutting AI systems into real-world operation requires ensuring they are safe, reliable, and robust, especially for human interaction scenarios. Self-driving cars from Uber and Tesla have been involved in deadly crashes due to unsafe behaviors.\nAdversarial attacks that subtly alter input data can also fool ML models and cause dangerous failures if systems are not resistant. Deepfakes represent another emerging threat area.\nVideo 15.1 is a deepfake video of Barack Obama that went viral a few years ago.\n\n\n\n\n\n\nVideo 15.1: Fake Obama\n\n\n\n\n\n\nPromoting safety requires extensive testing, risk analysis, human oversight, and designing systems that combine multiple weak models to avoid single points of failure. Rigorous safety mechanisms are essential for the responsible deployment of capable AI.\n\n\n15.3.5 Accountability and Governance\nWhen AI systems eventually fail or produce harmful outcomes, mechanisms must exist to address resultant issues, compensate affected parties, and assign responsibility. Both corporate accountability policies and government regulations are indispensable for responsible AI governance. For instance, Illinois’ Artificial Intelligence Video Interview Act requires companies to disclose and obtain consent for AI video analysis, promoting accountability.\nWithout clear accountability, even harms caused unintentionally could go unresolved, furthering public outrage and distrust. Oversight boards, impact assessments, grievance redress processes, and independent audits promote responsible development and deployment.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html#cloud-edge-tiny-ml",
    "href": "contents/responsible_ai/responsible_ai.html#cloud-edge-tiny-ml",
    "title": "15  Responsible AI",
    "section": "15.4 Cloud, Edge & Tiny ML",
    "text": "15.4 Cloud, Edge & Tiny ML\nWhile these principles broadly apply across AI systems, certain responsible AI considerations are unique or pronounced when dealing with machine learning on embedded devices versus traditional server-based modeling. Therefore, we present a high-level taxonomy comparing responsible AI considerations across cloud, edge, and TinyML systems.\n\n15.4.1 Summary\nTabella 15.1 summarizes how responsible AI principles manifest differently across cloud, edge, and TinyML architectures and how core considerations tie into their unique capabilities and limitations. Each environment’s constraints and tradeoffs shape how we approach transparency, accountability, governance, and other pillars of responsible AI.\n\n\n\nTabella 15.1: Comparison of key principles in Cloud ML, Edge ML, and TinyML.\n\n\n\n\n\n\n\n\n\n\n\nPrinciple\nCloud ML\nEdge ML\nTinyML\n\n\n\n\nExplainability\nComplex models supported\nLightweight required\nSevere limits\n\n\nFairness\nBroad data available\nOn-device biases\nLimited data labels\n\n\nPrivacy\nCloud data vulnerabilities\nMore sensitive data\nData dispersed\n\n\nSafety\nHacking threats\nReal-world interaction\nAutonomous devices\n\n\nAccountability\nCorporate policies\nSupply chain issues\nComponent tracing\n\n\nGovernance\nExternal oversight feasible\nSelf-governance needed\nProtocol constraints\n\n\n\n\n\n\n\n\n15.4.2 Explainability\nFor cloud-based machine learning, explainability techniques can leverage significant compute resources, enabling complex methods like SHAP values or sampling-based approaches to interpret model behaviors. For example, Microsoft’s InterpretML toolkit provides explainability techniques tailored for cloud environments.\nHowever, edge ML operates on resource-constrained devices, requiring more lightweight explainability methods that can run locally without excessive latency. Techniques like LIME (Ribeiro, Singh, e Guestrin 2016) approximate model explanations using linear models or decision trees to avoid expensive computations, which makes them ideal for resource-constrained devices. However, LIME requires training hundreds to even thousands of models to generate good explanations, which is often infeasible given edge computing constraints. In contrast, saliency-based methods are often much faster in practice, only requiring a single forward pass through the network to estimate feature importance. This greater efficiency makes such methods better suited to edge devices with limited compute resources where low-latency explanations are critical.\nGiven tiny hardware capabilities, embedded systems pose the most significant challenges for explainability. More compact models and limited data make inherent model transparency easier. Explaining decisions may not be feasible on high-size and power-optimized microcontrollers. DARPA’s Transparent Computing program aims to develop extremely low overhead explainability, especially for TinyML devices like sensors and wearables.\n\n\n15.4.3 Fairness\nFor cloud machine learning, vast datasets and computing power enable detecting biases across large heterogeneous populations and mitigating them through techniques like re-weighting data samples. However, biases may emerge from the broad behavioral data used to train cloud models. Amazon’s Fairness Flow framework helps assess cloud ML fairness.\nEdge ML relies on limited on-device data, making analyzing biases across diverse groups harder. However, edge devices interact closely with individuals, providing an opportunity to adapt locally for fairness. Google’s Federated Learning distributes model training across devices to incorporate individual differences.\nTinyML poses unique challenges for fairness with highly dispersed specialized hardware and minimal training data. Bias testing is difficult across diverse devices. Collecting representative data from many devices to mitigate bias has scale and privacy hurdles. DARPA’s Assured Neuro Symbolic Learning and Reasoning (ANSR) efforts are geared toward developing fairness techniques given extreme hardware constraints.\n\n\n15.4.4 Safety\nKey safety risks for cloud ML include model hacking, data poisoning, and malware disrupting cloud services. Robustness techniques like adversarial training, anomaly detection, and diversified models aim to harden cloud ML against attacks. Redundancy can help prevent single points of failure.\nEdge ML and TinyML interact with the physical world, so reliability and safety validation are critical. Rigorous testing platforms like Foretellix synthetically generate edge scenarios to validate safety. TinyML safety is magnified by autonomous devices with limited supervision. TinyML safety often relies on collective coordination - swarms of drones maintain safety through redundancy. Physical control barriers also constrain unsafe TinyML device behaviors.\nIn summary, safety is crucial but manifests differently in each domain. Cloud ML guards against hacking, edge ML interacts physically, so reliability is key, and TinyML leverages distributed coordination for safety. Understanding the nuances guides appropriate safety techniques.\n\n\n15.4.5 Accountability\nCloud ML’s accountability centers on corporate practices like responsible AI committees, ethical charters, and processes to address harmful incidents. Third-party audits and external government oversight promote cloud ML accountability.\nEdge ML accountability is more complex with distributed devices and supply chain fragmentation. Companies are accountable for devices, but components come from various vendors. Industry standards help coordinate edge ML accountability across stakeholders.\nWith TinyML, accountability mechanisms must be traced across long, complex supply chains of integrated circuits, sensors, and other hardware. TinyML certification schemes help track component provenance. Trade associations should ideally promote shared accountability for ethical TinyML.\n\n\n15.4.6 Governance\nOrganizations institute internal governance for cloud ML, such as ethics boards, audits, and model risk management. But external governance also oversees cloud ML, like regulations on bias and transparency such as the AI Bill of Rights, General Data Protection Regulation (GDPR), and California Consumer Protection Act (CCPA). Third-party auditing supports cloud ML governance.\nEdge ML is more decentralized, requiring responsible self-governance by developers and companies deploying models locally. Industry associations coordinate governance across edge ML vendors, and open software helps align incentives for ethical edge ML.\nExtreme decentralization and complexity make external governance infeasible with TinyML. TinyML relies on protocols and standards for self-governance baked into model design and hardware. Cryptography enables the provable trustworthiness of TinyML devices.\n\n\n15.4.7 Privacy\nFor cloud ML, vast amounts of user data are concentrated in the cloud, creating risks of exposure through breaches. Differential privacy techniques add noise to cloud data to preserve privacy. Strict access controls and encryption protect cloud data at rest and in transit.\nEdge ML moves data processing onto user devices, reducing aggregated data collection but increasing potential sensitivity as personal data resides on the device. Apple uses on-device ML and differential privacy to train models while minimizing data sharing. Data anonymization and secure enclaves protect on-device data.\nTinyML distributes data across many resource-constrained devices, making centralized breaches unlikely and making scale anonymization challenging. Data minimization and using edge devices as intermediaries help TinyML privacy.\nSo, while cloud ML must protect expansive centralized data, edge ML secures sensitive on-device data, and TinyML aims for minimal distributed data sharing due to constraints. While privacy is vital throughout, techniques must match the environment. Understanding nuances allows for selecting appropriate privacy preservation approaches.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html#technical-aspects",
    "href": "contents/responsible_ai/responsible_ai.html#technical-aspects",
    "title": "15  Responsible AI",
    "section": "15.5 Technical Aspects",
    "text": "15.5 Technical Aspects\n\n15.5.1 Detecting and Mitigating Bias\nA large body of work has demonstrated that machine learning models can exhibit bias, from underperforming people of a certain identity to making decisions that limit groups’ access to important resources (Buolamwini e Gebru 2018).\nEnsuring fair and equitable treatment for all groups affected by machine learning systems is crucial as these models increasingly impact people’s lives in areas like lending, healthcare, and criminal justice. We typically evaluate model fairness by considering “subgroup attributes” unrelated to the prediction task that capture identities like race, gender, or religion. For example, in a loan default prediction model, subgroups could include race, gender, or religion. When models are trained naively to maximize accuracy, they often ignore subgroup performance. However, this can negatively impact marginalized communities.\nTo illustrate, imagine a model predicting loan repayment where the plusses (+’s) represent repayment and the circles (O’s) represent default, as shown in Figura 15.1. The optimal accuracy would be correctly classifying all of Group A while misclassifying some of Group B’s creditworthy applicants as defaults. If positive classifications allow access loans, Group A would receive many more loans—which would naturally result in a biased outcome.\n\n\n\n\n\n\nFigura 15.1: Fairness and accuracy.\n\n\n\nAlternatively, correcting the biases against Group B would likely increase “false positives” and reduce accuracy for Group A. Or, we could train separate models focused on maximizing true positives for each group. However, this would require explicitly using sensitive attributes like race in the decision process.\nAs we see, there are inherent tensions around priorities like accuracy versus subgroup fairness and whether to explicitly account for protected classes. Reasonable people can disagree on the appropriate tradeoffs. Constraints around costs and implementation options further complicate matters. Overall, ensuring the fair and ethical use of machine learning involves navigating these complex challenges.\nThus, the fairness literature has proposed three main fairness metrics for quantifying how fair a model performs over a dataset (Hardt, Price, e Srebro 2016). Given a model h and a dataset D consisting of (x,y,s) samples, where x is the data features, y is the label, and s is the subgroup attribute, and we assume there are simply two subgroups a and b, we can define the following.\n\nDemographic Parity asks how accurate a model is for each subgroup. In other words, P(h(X) = Y S = a) = P(h(X) = Y S = b)\nEqualized Odds asks how precise a model is on positive and negative samples for each subgroup. P(h(X) = y S = a, Y = y) = P(h(X) = y S = b, Y = y)\nEquality of Opportunity is a special case of equalized odds that only asks how precise a model is on positive samples. This is relevant in cases such as resource allocation, where we care about how positive (i.e., resource-allocated) labels are distributed across groups. For example, we care that an equal proportion of loans are given to both men and women. P(h(X) = 1 S = a, Y = 1) = P(h(X) = 1 S = b, Y = 1)\n\nNote: These definitions often take a narrow view when considering binary comparisons between two subgroups. Another thread of fair machine learning research focusing on multicalibration and multiaccuracy considers the interactions between an arbitrary number of identities, acknowledging the inherent intersectionality of individual identities in the real world (Hébert-Johnson et al. 2018).\n\nHébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, e Guy N. Rothblum. 2018. «Multicalibration: Calibration for the (Computationally-Identifiable) Masses». In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, a cura di Jennifer G. Dy e Andreas Krause, 80:1944–53. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\nContext Matters\nBefore making any technical decisions to develop an unbiased ML algorithm, we need to understand the context surrounding our model. Here are some of the key questions to think about:\n\nWho will this model make decisions for?\nWho is represented in the training data?\nWho is represented, and who is missing at the table of engineers, designers, and managers?\n\nWhat sort of long-lasting impacts could this model have? For example, will it impact an individual’s financial security at a generational scale, such as determining college admissions or admitting a loan for a house?\n\nWhat historical and systematic biases are present in this setting, and are they present in the training data the model will generalize from?\n\nUnderstanding a system’s social, ethical, and historical background is critical to preventing harm and should inform decisions throughout the model development lifecycle. After understanding the context, one can make various technical decisions to remove bias. First, one must decide what fairness metric is the most appropriate criterion for optimizing. Next, there are generally three main areas where one can intervene to debias an ML system.\nFirst, preprocessing is when one balances a dataset to ensure fair representation or even increases the weight on certain underrepresented groups to ensure the model performs well. Second, in processing attempts to modify the training process of an ML system to ensure it prioritizes fairness. This can be as simple as adding a fairness regularizer (Lowy et al. 2021) to training an ensemble of models and sampling from them in a specific manner (Agarwal et al. 2018).\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, e Ahmad Beirami. 2021. «Fermi: Fair empirical risk minimization via exponential Rényi mutual information».\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, e Hanna M. Wallach. 2018. «A Reductions Approach to Fair Classification». In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, a cura di Jennifer G. Dy e Andreas Krause, 80:60–69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, e Flavio Calmon. 2022. «Beyond Adult and COMPAS: Fair multi-class prediction via information projection». Adv. Neur. In. 35: 38747–60.\n\nHardt, Moritz, Eric Price, e Nati Srebro. 2016. «Equality of Opportunity in Supervised Learning». In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, a cura di Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, e Roman Garnett, 3315–23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\nFinally, post-processing debases a model after the fact, taking a trained model and modifying its predictions in a specific manner to ensure fairness is preserved (Alghamdi et al. 2022; Hardt, Price, e Srebro 2016). Post-processing builds on the preprocessing and in-processing steps by providing another opportunity to address bias and fairness issues in the model after it has already been trained.\nThe three-step process of preprocessing, in-processing, and post-processing provides a framework for intervening at different stages of model development to mitigate issues around bias and fairness. While preprocessing and in-processing focus on data and training, post-processing allows for adjustments after the model has been fully trained. Together, these three approaches give multiple opportunities to detect and remove unfair bias.\n\n\nThoughtful Deployment\nThe breadth of existing fairness definitions and debiasing interventions underscores the need for thoughtful assessment before deploying ML systems. As ML researchers and developers, responsible model development requires proactively educating ourselves on the real-world context, consulting domain experts and end-users, and centering harm prevention.\nRather than seeing fairness considerations as a box to check, we must deeply engage with the unique social implications and ethical tradeoffs around each model we build. Every technical choice about datasets, model architectures, evaluation metrics, and deployment constraints embeds values. By broadening our perspective beyond narrow technical metrics, carefully evaluating tradeoffs, and listening to impacted voices, we can work to ensure our systems expand opportunity rather than encode bias.\nThe path forward lies not in an arbitrary debiasing checklist but in a commitment to understanding and upholding our ethical responsibility at each step. This commitment starts with proactively educating ourselves and consulting others rather than just going through the motions of a fairness checklist. It requires engaging deeply with ethical tradeoffs in our technical choices, evaluating impacts on different groups, and listening to those voices most impacted.\nUltimately, responsible and ethical AI systems do not come from checkbox debiasing but from upholding our duty to assess harms, broaden perspectives, understand tradeoffs, and ensure we provide opportunity for all groups. This ethical responsibility should drive every step.\nThe connection between the paragraphs is that the first paragraph establishes the need for a thoughtful assessment of fairness issues rather than a checkbox approach. The second paragraph then expands on what that thoughtful assessment looks like in practice—engaging with tradeoffs, evaluating impacts on groups, and listening to impacted voices. Finally, the last paragraph refers to avoiding an “arbitrary debiasing checklist” and committing to ethical responsibility through assessment, understanding tradeoffs, and providing opportunity.\n\n\n\n15.5.2 Preserving Privacy\nRecent incidents have demonstrated how AI models can memorize sensitive user data in ways that violate privacy. For example, as shown in Figure XXX below, Stable Diffusion’s art generations were found to mimic identifiable artists’ styles and replicate existing photos, concerning many (Ippolito et al. 2023). These risks are amplified with personalized ML systems deployed in intimate environments like homes or wearables.\nImagine if a smart speaker uses our conversations to improve the quality of service to end users who genuinely want it. Still, others could violate privacy by trying to extract what the speaker “remembers.” Figura 15.2 below shows how diffusion models can memorize and generate individual training examples (Ippolito et al. 2023).\n\n\n\n\n\n\nFigura 15.2: Diffusion models memorizing samples from training data. Source: Ippolito et al. (2023).\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, e Nicholas Carlini. 2023. «Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy». In Proceedings of the 16th International Natural Language Generation Conference, 5253–70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nAdversaries can use these memorization capabilities and train models to detect if specific training data influenced a target model. For example, membership inference attacks train a secondary model that learns to detect a change in the target model’s outputs when making inferences over data it was trained on versus not trained on (Shokri et al. 2017).\n\nShokri, Reza, Marco Stronati, Congzheng Song, e Vitaly Shmatikov. 2017. «Membership Inference Attacks Against Machine Learning Models». In 2017 IEEE Symposium on Security and Privacy (SP), 3–18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, e Li Zhang. 2016. «Deep Learning with Differential Privacy». In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–18. CCS ’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\nML devices are especially vulnerable because they are often personalized on user data and are deployed in even more intimate settings such as the home. Private machine learning techniques have evolved to establish safeguards against adversaries, as mentioned in the Security and Privacy chapter to combat these privacy issues. Methods like differential privacy add mathematical noise during training to obscure individual data points’ influence on the model. Popular techniques like DP-SGD (Abadi et al. 2016) also clip gradients to limit what the model leaks about the data. Still, users should also be able to delete the impact of their data after the fact.\n\n\n15.5.3 Machine Unlearning\nWith ML devices personalized to individual users and then deployed to remote edges without connectivity, a challenge arises—how can models responsively “forget” data points after deployment? If users request their data be removed from a personalized model, the lack of connectivity makes retraining infeasible. Thus, efficient on-device data forgetting is necessary but poses hurdles.\nInitial unlearning approaches faced limitations in this context. Given the resource constraints, retrieving models from scratch on the device to forget data points proves inefficient or even impossible. Fully retraining also requires retaining all the original training data on the device, which brings its own security and privacy risks. Common machine unlearning techniques (Bourtoule et al. 2021) for remote embedded ML systems fail to enable responsive, secure data removal.\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, e Nicolas Papernot. 2021. «Machine Unlearning». In 2021 IEEE Symposium on Security and Privacy (SP), 141–59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\nHowever, newer methods show promise in modifying models to approximately forget data [?] without full retraining. While the accuracy loss from avoiding full rebuilds is modest, guaranteeing data privacy should still be the priority when handling sensitive user information ethically. Even slight exposure to private data can violate user trust. As ML systems become deeply personalized, efficiency and privacy must be enabled from the start—not afterthoughts.\nRecent policy discussions which include the European Union’s General Data, Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), the Act on the Protection of Personal Information (APPI), and Canada’s proposed Consumer Privacy Protection Act (CPPA), require the deletion of private information. These policies, coupled with AI incidents like Stable Diffusion memorizing artist data, have underscored the ethical need for users to delete their data from models after training.\nThe right to remove data arises from privacy concerns around corporations or adversaries misusing sensitive user information. Machine unlearning refers to removing the influence of specific points from an already-trained model. Naively, this involves full retraining without the deleted data. However, connectivity constraints often make retraining infeasible for ML systems personalized and deployed to remote edges. If a smart speaker learns from private home conversations, retaining access to delete that data is important.\nAlthough limited, methods are evolving to enable efficient approximations of retraining for unlearning. By modifying models’ inference time, they can mimic “forgetting” data without full access to training data. However, most current techniques are restricted to simple models, still have resource costs, and trade some accuracy. Though methods are evolving, enabling efficient data removal and respecting user privacy remains imperative for responsible TinyML deployment.\n\n\n15.5.4 Adversarial Examples and Robustness\nMachine learning models, especially deep neural networks, have a well-documented Achilles heel: they often break when even tiny perturbations are made to their inputs (Szegedy et al. 2014). This surprising fragility highlights a major robustness gap threatening real-world deployment in high-stakes domains. It also opens the door for adversarial attacks designed to fool models deliberately.\nMachine learning models can exhibit surprising brittleness—minor input tweaks can cause shocking malfunctions, even in state-of-the-art deep neural networks (Szegedy et al. 2014). This unpredictability around out-of-sample data underscores gaps in model generalization and robustness. Given the growing ubiquity of ML, it also enables adversarial threats that weaponize models’ blindspots.\nDeep neural networks demonstrate an almost paradoxical dual nature - human-like proficiency in training distributions coupled with extreme fragility to tiny input perturbations (Szegedy et al. 2014). This adversarial vulnerability gap highlights gaps in standard ML procedures and threats to real-world reliability. At the same time, it can be exploited: attackers can find model-breaking points humans wouldn’t perceive.\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, e Rob Fergus. 2014. «Intriguing properties of neural networks». In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, a cura di Yoshua Bengio e Yann LeCun. http://arxiv.org/abs/1312.6199.\nFigura 15.3 includes an example of a small meaningless perturbation that changes a model prediction. This fragility has real-world impacts: lack of robustness undermines trust in deploying models for high-stakes applications like self-driving cars or medical diagnosis. Moreover, the vulnerability leads to security threats: attackers can deliberately craft adversarial examples that are perceptually indistinguishable from normal data but cause model failures.\n\n\n\n\n\n\nFigura 15.3: Perturbation effect on prediction. Source: Microsoft.\n\n\n\nFor instance, past work shows successful attacks that trick models for tasks like NSFW detection (Bhagoji et al. 2018), ad-blocking (Tramèr et al. 2019), and speech recognition (Carlini et al. 2016). While errors in these domains already pose security risks, the problem extends beyond IT security. Recently, adversarial robustness has been proposed as an additional performance metric by approximating worst-case behavior.\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, e Dawn Song. 2018. «Practical black-box attacks on deep neural networks using efficient query mechanisms». In Proceedings of the European conference on computer vision (ECCV), 154–69.\n\nTramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, e Dan Boneh. 2019. «AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning». In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, 2005–21. ACM. https://doi.org/10.1145/3319535.3354222.\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, e Wenchao Zhou. 2016. «Hidden voice commands». In 25th USENIX security symposium (USENIX security 16), 513–30.\nThe surprising model fragility highlighted above casts doubt on real-world reliability and opens the door to adversarial manipulation. This growing vulnerability underscores several needs. First, moral robustness evaluations are essential for quantifying model vulnerabilities before deployment. Approximating worst-case behavior surfaces blindspots.\nSecond, effective defenses across domains must be developed to close these robustness gaps. With security on the line, developers cannot ignore the threat of attacks exploiting model weaknesses. Moreover, we cannot afford any fragility-induced failures for safety-critical applications like self-driving vehicles and medical diagnosis. Lives are at stake.\nFinally, the research community continues mobilizing rapidly in response. Interest in adversarial machine learning has exploded as attacks reveal the need to bridge the robustness gap between synthetic and real-world data. Conferences now commonly feature defenses for securing and stabilizing models. The community recognizes that model fragility is a critical issue that must be addressed through robustness testing, defense development, and ongoing research. By surfacing blindspots and responding with principled defenses, we can work to ensure reliability and safety for machine learning systems, especially in high-stakes domains.\n\n\n15.5.5 Building Interpretable Models\nAs models are deployed more frequently in high-stakes settings, practitioners, developers, downstream end-users, and increasing regulation have highlighted the need for explainability in machine learning. The goal of many interpretability and explainability methods is to provide practitioners with more information about the models’ overall behavior or the behavior given a specific input. This allows users to decide whether or not a model’s output or prediction is trustworthy.\nSuch analysis can help developers debug models and improve performance by pointing out biases, spurious correlations, and failure modes of models. In cases where models can surpass human performance on a task, interpretability can help users and researchers better understand relationships in their data and previously unknown patterns.\nThere are many classes of explainability/interpretability methods, including post hoc explainability, inherent interpretability, and mechanistic interpretability. These methods aim to make complex machine learning models more understandable and ensure users can trust model predictions, especially in critical settings. By providing transparency into model behavior, explainability techniques are an important tool for developing safe, fair, and reliable AI systems.\n\nPost Hoc Explainability\nPost hoc explainability methods typically explain the output behavior of a black-box model on a specific input. Popular methods include counterfactual explanations, feature attribution methods, and concept-based explanations.\nCounterfactual explanations, also frequently called algorithmic recourse, “If X had not occurred, Y would not have occurred” (Wachter, Mittelstadt, e Russell 2017). For example, consider a person applying for a bank loan whose application is rejected by a model. They may ask their bank for recourse or how to change to be eligible for a loan. A counterfactual explanation would tell them which features they need to change and by how much such that the model’s prediction changes.\n\nWachter, Sandra, Brent Mittelstadt, e Chris Russell. 2017. «Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR». SSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, e Dhruv Batra. 2017. «Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization». In 2017 IEEE International Conference on Computer Vision (ICCV), 618–26. IEEE. https://doi.org/10.1109/iccv.2017.74.\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, e Martin Wattenberg. 2017. «Smoothgrad: Removing noise by adding noise». ArXiv preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\nRibeiro, Marco Tulio, Sameer Singh, e Carlos Guestrin. 2016. «” Why should i trust you?” Explaining the predictions of any classifier». In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135–44.\n\nLundberg, Scott M., e Su-In Lee. 2017. «A Unified Approach to Interpreting Model Predictions». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\nFeature attribution methods highlight the input features that are important or necessary for a particular prediction. For a computer vision model, this would mean highlighting the individual pixels that contributed most to the predicted label of the image. Note that these methods do not explain how those pixels/features impact the prediction, only that they do. Common methods include input gradients, GradCAM (Selvaraju et al. 2017), SmoothGrad (Smilkov et al. 2017), LIME (Ribeiro, Singh, e Guestrin 2016), and SHAP (Lundberg e Lee 2017).\nBy providing examples of changes to input features that would alter a prediction (counterfactuals) or indicating the most influential features for a given prediction (attribution), these post hoc explanation techniques shed light on model behavior for individual inputs. This granular transparency helps users determine whether they can trust and act upon specific model outputs.\nConcept-based explanations aim to explain model behavior and outputs using a pre-defined set of semantic concepts (e.g., the model recognizes scene class “bedroom” based on the presence of concepts “bed” and “pillow”). Recent work shows that users often prefer these explanations to attribution and example-based explanations because they “resemble human reasoning and explanations” (Vikram V. Ramaswamy et al. 2023b). Popular concept-based explanation methods include TCAV (Cai et al. 2019), Network Dissection (Bau et al. 2017), and interpretable basis decomposition (Zhou et al. 2018).\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, e Olga Russakovsky. 2023b. «UFO: A unified method for controlling Understandability and Faithfulness Objectives in concept-based explanations for CNNs». ArXiv preprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, et al. 2019. «Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making». In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, a cura di Jennifer G. Dy e Andreas Krause, 80:2673–82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, e Antonio Torralba. 2017. «Network Dissection: Quantifying Interpretability of Deep Visual Representations». In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3319–27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\nZhou, Bolei, Yiyou Sun, David Bau, e Antonio Torralba. 2018. «Interpretable basis decomposition for visual explanation». In Proceedings of the European Conference on Computer Vision (ECCV), 119–34.\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, e Olga Russakovsky. 2023a. «Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability». In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10932–41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\nNote that these methods are extremely sensitive to the size and quality of the concept set, and there is a tradeoff between their accuracy and faithfulness and their interpretability or understandability to humans (Vikram V. Ramaswamy et al. 2023a). However, by mapping model predictions to human-understandable concepts, concept-based explanations can provide transparency into the reasoning behind model outputs.\n\n\nInherent Interpretability\nInherently interpretable models are constructed such that their explanations are part of the model architecture and are thus naturally faithful, which sometimes makes them preferable to post-hoc explanations applied to black-box models, especially in high-stakes domains where transparency is imperative (Rudin 2019). Often, these models are constrained so that the relationships between input features and predictions are easy for humans to follow (linear models, decision trees, decision sets, k-NN models), or they obey structural knowledge of the domain, such as monotonicity (Gupta et al. 2016), causality, or additivity (Lou et al. 2013; Beck e Jackman 1998).\n\nRudin, Cynthia. 2019. «Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead». Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x.\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, e Alexander Van Esbroeck. 2016. «Monotonic calibrated interpolated look-up tables». The Journal of Machine Learning Research 17 (1): 3790–3836.\n\nLou, Yin, Rich Caruana, Johannes Gehrke, e Giles Hooker. 2013. «Accurate intelligible models with pairwise interactions». In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, a cura di Inderjit S. Dhillon, Yehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh, Jingrui He, Robert L. Grossman, e Ramasamy Uthurusamy, 623–31. ACM. https://doi.org/10.1145/2487575.2487579.\n\nBeck, Nathaniel, e Simon Jackman. 1998. «Beyond Linearity by Default: Generalized Additive Models». Am. J. Polit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, e Percy Liang. 2020. «Concept Bottleneck Models». In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 119:5338–48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, e Jonathan Su. 2019. «This Looks Like That: Deep Learning for Interpretable Image Recognition». In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, a cura di Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, e Roman Garnett, 8928–39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\nHowever, more recent works have relaxed the restrictions on inherently interpretable models, using black-box models for feature extraction and a simpler inherently interpretable model for classification, allowing for faithful explanations that relate high-level features to prediction. For example, Concept Bottleneck Models (Koh et al. 2020) predict a concept set c that is passed into a linear classifier. ProtoPNets (Chen et al. 2019) dissect inputs into linear combinations of similarities to prototypical parts from the training set.\n\n\nMechanistic Interpretability\nMechanistic interpretability methods seek to reverse engineer neural networks, often analogizing them to how one might reverse engineer a compiled binary or how neuroscientists attempt to decode the function of individual neurons and circuits in brains. Most research in mechanistic interpretability views models as a computational graph (Geiger et al. 2021), and circuits are subgraphs with distinct functionality (Wang e Zhan 2019). Current approaches to extracting circuits from neural networks and understanding their functionality rely on human manual inspection of visualizations produced by circuits (Olah et al. 2020).\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, e Christopher Potts. 2021. «Causal Abstractions of Neural Networks». In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, a cura di Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, e Jennifer Wortman Vaughan, 9574–86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\nWang, LingFeng, e YaQing Zhan. 2019. «A conceptual peer review model for arXiv and other preprint databases». Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, e Shan Carter. 2020. «Zoom In: An Introduction to Circuits». Distill 5 (3): e00024–001. https://doi.org/10.23915/distill.00024.001.\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana Turner, Carver Middleton, Will Carroll, et al. 2023. «Closing the Wearable Gap: Footankle kinematic modeling via deep learning models based on a smart sock wearable». Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\nAlternatively, some approaches build sparse autoencoders that encourage neurons to encode disentangled interpretable features (Davarzani et al. 2023). This field is much newer than existing areas in explainability and interpretability, and as such, most works are generally exploratory rather than solution-oriented.\nThere are many problems in mechanistic interpretability, including the polysemanticity of neurons and circuits, the inconvenience and subjectivity of human labeling, and the exponential search space for identifying circuits in large models with billions or trillions of neurons.\n\n\nChallenges and Considerations\nAs methods for interpreting and explaining models progress, it is important to note that humans overtrust and misuse interpretability tools (Kaur et al. 2020) and that a user’s trust in a model due to an explanation can be independent of the correctness of the explanations (Lakkaraju e Bastani 2020). As such, it is necessary that aside from assessing the faithfulness/correctness of explanations, researchers must also ensure that interpretability methods are developed and deployed with a specific user in mind and that user studies are performed to evaluate their efficacy and usefulness in practice.\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, e Jennifer Wortman Vaughan. 2020. «Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning». In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, a cura di Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14. ACM. https://doi.org/10.1145/3313831.3376219.\n\nLakkaraju, Himabindu, e Osbert Bastani. 2020. «”How do I fool you?”: Manipulating User Trust via Misleading Black Box Explanations». In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 79–85. ACM. https://doi.org/10.1145/3375627.3375833.\nFurthermore, explanations should be tailored to the user’s expertise, the task they are using the explanation for and the corresponding minimal amount of information required for the explanation to be useful to prevent information overload.\nWhile interpretability/explainability are popular areas in machine learning research, very few works study their intersection with TinyML and edge computing. Given that a significant application of TinyML is healthcare, which often requires high transparency and interpretability, existing techniques must be tested for scalability and efficiency concerning edge devices. Many methods rely on extra forward and backward passes, and some even require extensive training in proxy models, which are infeasible on resource-constrained microcontrollers.\nThat said, explainability methods can be highly useful in developing models for edge devices, as they can give insights into how input data and models can be compressed and how representations may change post-compression. Furthermore, many interpretable models are often smaller than their black-box counterparts, which could benefit TinyML applications.\n\n\n\n15.5.6 Monitoring Model Performance\nWhile developers may train models that seem adversarially robust, fair, and interpretable before deployment, it is imperative that both the users and the model owners continue to monitor the model’s performance and trustworthiness during the model’s full lifecycle. Data is frequently changing in practice, which can often result in distribution shifts. These distribution shifts can profoundly impact the model’s vanilla predictive performance and its trustworthiness (fairness, robustness, and interpretability) in real-world data.\nFurthermore, definitions of fairness frequently change with time, such as what society considers a protected attribute, and the expertise of the users asking for explanations may also change.\nTo ensure that models keep up to date with such changes in the real world, developers must continually evaluate their models on current and representative data and standards and update models when necessary.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html#implementation-challenges",
    "href": "contents/responsible_ai/responsible_ai.html#implementation-challenges",
    "title": "15  Responsible AI",
    "section": "15.6 Implementation Challenges",
    "text": "15.6 Implementation Challenges\n\n15.6.1 Organizational and Cultural Structures\nWhile innovation and regulation are often seen as having competing interests, many countries have found it necessary to provide oversight as AI systems expand into more sectors. As illustrated in Figura 15.4, this oversight has become crucial as these systems continue permeating various industries and impacting people’s lives (see Human-Centered AI, Chapter 8 “Government Interventions and Regulations”.\n\n\n\n\n\n\nFigura 15.4: How various groups impact human-centered AI. Source: Shneiderman (2020).\n\n\nShneiderman, Ben. 2020. «Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-centered AI Systems». ACM Trans. Interact. Intell. Syst. 10 (4): 1–31. https://doi.org/10.1145/3419764.\n\n\nAmong these are:\n\nCanada’s Responsible Use of Artificial Intelligence\nThe European Union’s General Data Protection Regulation (GDPR)\nThe European Commission’s White Paper on Artificial Intelligence: a European approach to excellence and trust\nThe UK’s Information Commissioner’s Office and Alan Turing Institute’s Consultation on Explaining AI Decisions Guidance co-badged guidance by the individuals affected by them.\n\n\n\n15.6.2 Obtaining Quality and Representative Data\nAs discussed in the Data Engineering chapter, responsible AI design must occur at all pipeline stages, including data collection. This begs the question: what does it mean for data to be high-quality and representative? Consider the following scenarios that hinder the representativeness of data:\n\nSubgroup Imbalance\nThis is likely what comes to mind when hearing “representative data.” Subgroup imbalance means the dataset contains relatively more data from one subgroup than another. This imbalance can negatively affect the downstream ML model by causing it to overfit a subgroup of people while performing poorly on another.\nOne example consequence of subgroup imbalance is racial discrimination in facial recognition technology (Buolamwini e Gebru 2018); commercial facial recognition algorithms have up to 34% worse error rates on darker-skinned females than lighter-skinned males.\n\nBuolamwini, Joy, e Timnit Gebru. 2018. «Gender shades: Intersectional accuracy disparities in commercial gender classification». In Conference on fairness, accountability and transparency, 77–91. PMLR.\nNote that data imbalance goes both ways, and subgroups can also be harmful overrepresented in the dataset. For example, the Allegheny Family Screening Tool (AFST) predicts the likelihood that a child will eventually be removed from a home. The AFST produces disproportionate scores for different subgroups, one of the reasons being that it is trained on historically biased data, sourced from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs.\n\n\nQuantifying Target Outcomes\nThis occurs in applications where the ground-truth label cannot be measured or is difficult to represent in a single quantity. For example, an ML model in a mobile wellness application may want to predict individual stress levels. The true stress labels themselves are impossible to obtain directly and must be inferred from other biosignals, such as heart rate variability and user self-reported data. In these situations, noise is built into the data by design, making this a challenging ML task.\n\n\nDistribution Shift\nData may no longer represent a task if a major external event causes the data source to change drastically. The most common way to think about distribution shifts is with respect to time; for example, data on consumer shopping habits collected pre-covid may no longer be present in consumer behavior today.\nThe transfer causes another form of distribution shift. For instance, when applying a triage system that was trained on data from one hospital to another, a distribution shift may occur if the two hospitals are very different.#\n\n\nGathering Data\nA reasonable solution for many of the above problems with non-representative or low-quality data is to collect more; we can collect more data targeting an underrepresented subgroup or from the target hospital to which our model might be transferred. However, for some reasons, gathering more data is an inappropriate or infeasible solution for the task at hand.\n\nData collection can be harmful. This is the paradox of exposure, the situation in which those who stand to significantly gain from their data being collected are also those who are put at risk by the collection process (D’ignazio e Klein (2023), Chapter 4). For example, collecting more data on non-binary individuals may be important for ensuring the fairness of the ML application, but it also puts them at risk, depending on who is collecting the data and how (whether the data is easily identifiable, contains sensitive content, etc.).\nData collection can be costly. In some domains, such as healthcare, obtaining data can be costly in terms of time and money.\nBiased data collection. Electronic Health Records is a huge data source for ML-driven healthcare applications. Issues of subgroup representation aside, the data itself may be collected in a biased manner. For example, negative language (“nonadherent,” “unwilling”) is disproportionately used on black patients (Himmelstein, Bates, e Zhou 2022).\n\n\nD’ignazio, Catherine, e Lauren F Klein. 2023. Data feminism. MIT press.\n\nHimmelstein, Gracie, David Bates, e Li Zhou. 2022. «Examination of Stigmatizing Language in the Electronic Health Record». JAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\nWe conclude with several additional strategies for maintaining data quality: improving understanding of the data, data exploration, and intr. First, fostering a deeper understanding of the data is crucial. This can be achieved through the implementation of standardized labels and measures of data quality, such as in the Data Nutrition Project.\nCollaborating with organizations responsible for collecting data helps ensure the data is interpreted correctly. Second, employing effective tools for data exploration is important. Visualization techniques and statistical analyses can reveal issues with the data. Finally, establishing a feedback loop within the ML pipeline is essential for understanding the real-world implications of the data. Metrics, such as fairness measures, allow us to define “data quality” in the context of the downstream application; improving fairness may directly improve the quality of the predictions that the end users receive.\n\n\n\n15.6.3 Balancing Accuracy and Other Objectives\nMachine learning models are often evaluated on accuracy alone, but this single metric cannot fully capture model performance and tradeoffs for responsible AI systems. Other ethical dimensions, such as fairness, robustness, interpretability, and privacy, may compete with pure predictive accuracy during model development. For instance, inherently interpretable models such as small decision trees or linear classifiers with simplified features intentionally trade some accuracy for transparency in the model behavior and predictions. While these simplified models achieve lower accuracy by not capturing all the complexity in the dataset, improved interpretability builds trust by enabling direct analysis by human practitioners.\nAdditionally, certain techniques meant to improve adversarial robustness, such as adversarial training examples or dimensionality reduction, can degrade the accuracy of clean validation data. In sensitive applications like healthcare, focusing narrowly on state-of-the-art accuracy carries ethical risks if it allows models to rely more on spurious correlations that introduce bias or use opaque reasoning. Therefore, the appropriate performance objectives depend greatly on the sociotechnical context.\nMethodologies like Value Sensitive Design provide frameworks for formally evaluating the priorities of various stakeholders within the real-world deployment system. These elucidate tensions between values like accuracy, interpretation, ility, and fail and redness, which can then guide responsible tradeoff decisions. For a medical diagnosis system, achieving the highest accuracy may not be the singular goal - improving transparency to build practitioner trust or reducing bias towards minority groups could justify small losses in accuracy. Analyzing the sociotechnical context is key for setting these objectives.\nBy taking a holistic view, we can responsibly balance accuracy with other ethical objectives for model success. Ongoing performance monitoring along multiple dimensions is crucial as the system evolves after deployment.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html#ethical-considerations-in-ai-design",
    "href": "contents/responsible_ai/responsible_ai.html#ethical-considerations-in-ai-design",
    "title": "15  Responsible AI",
    "section": "15.7 Ethical Considerations in AI Design",
    "text": "15.7 Ethical Considerations in AI Design\nWe must discuss at least some of the many ethical issues at stake in designing and applying AI systems and diverse frameworks for approaching these issues, including those from AI safety, Human-Computer Interaction (HCI), and Science, Technology, and Society (STS).\n\n15.7.1 AI Safety and Value Alignment\nIn 1960, Norbert Weiner wrote, “’if we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively… we had better be quite sure that the purpose put into the machine is the purpose which we desire” (Wiener 1960).\n\nWiener, Norbert. 1960. «Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers.» Science 131 (3410): 1355–58. https://doi.org/10.1126/science.131.3410.1355.\n\nRussell, Stuart. 2021. «Human-compatible artificial intelligence». Human-like machine intelligence, 3–23.\nIn recent years, as the capabilities of deep learning models have achieved, and sometimes even surpassed, human abilities, the issue of creating AI systems that act in accord with human intentions instead of pursuing unintended or undesirable goals has become a source of concern (Russell 2021). Within the field of AI safety, a particular goal concerns “value alignment,” or the problem of how to code the “right” purpose into machines Human-Compatible Artificial Intelligence. Present AI research assumes we know the objectives we want to achieve and “studies the ability to achieve objectives, not the design of those objectives.”\nHowever, complex real-world deployment contexts make explicitly defining “the right purpose” for machines difficult, requiring frameworks for responsible and ethical goal-setting. Methodologies like Value Sensitive Design provide formal mechanisms to surface tensions between stakeholder values and priorities.\nBy taking a holistic sociotechnical view, we can better ensure intelligent systems pursue objectives that align with broad human intentions rather than maximizing narrow metrics like accuracy alone. Achieving this in practice remains an open and critical research question as AI capabilities advance rapidly.\nThe absence of this alignment can lead to several AI safety issues, as have been documented in a variety of deep learning models. A common feature of systems that optimize for an objective is that variables not directly included in the objective may be set to extreme values to help optimize for that objective, leading to issues characterized as specification gaming, reward hacking, etc., in reinforcement learning (RL).\nIn recent years, a particularly popular implementation of RL has been models pre-trained using self-supervised learning and fine-tuned reinforcement learning from human feedback (RLHF) (Christiano et al. 2017). Ngo 2022 (Ngo, Chan, e Mindermann 2022) argues that by rewarding models for appearing harmless and ethical while also maximizing useful outcomes, RLHF could encourage the emergence of three problematic properties: situationally aware reward hacking, where policies exploit human fallibility to gain high reward, misaligned internally-represented goals that generalize beyond the RLHF fine-tuning distribution, and power-seeking strategies.\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, e Dario Amodei. 2017. «Deep Reinforcement Learning from Human Preferences». In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, a cura di Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, e Roman Garnett, 4299–4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\nNgo, Richard, Lawrence Chan, e Sören Mindermann. 2022. «The alignment problem from a deep learning perspective». ArXiv preprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\nVan Noorden, Richard. 2016. «ArXiv preprint server plans multimillion-dollar overhaul». Nature 534 (7609): 602–2. https://doi.org/10.1038/534602a.\nSimilarly, Van Noorden (2016) outlines six concrete problems for AI safety, including avoiding negative side effects, avoiding reward hacking, scalable oversight for aspects of the objective that are too expensive to be frequently evaluated during training, safe exploration strategies that encourage creativity while preventing harm, and robustness to distributional shift in unseen testing environments.\n\n\n15.7.2 Autonomous Systems and Control [and Trust]\nThe consequences of autonomous systems that act independently of human oversight and often outside human judgment have been well documented across several industries and use cases. Most recently, the California Department of Motor Vehicles suspended Cruise’s deployment and testing permits for its autonomous vehicles citing “unreasonable risks to public safety”. One such accident occurred when a vehicle struck a pedestrian who stepped into a crosswalk after the stoplight had turned green, and the vehicle was allowed to proceed. In 2018, a pedestrian crossing the street with her bike was killed when a self-driving Uber car, which was operating in autonomous mode, failed to accurately classify her moving body as an object to be avoided.\nAutonomous systems beyond self-driving vehicles are also susceptible to such issues, with potentially graver consequences, as remotely-powered drones are already reshaping warfare. While such incidents bring up important ethical questions regarding who should be held responsible when these systems fail, they also highlight the technical challenges of giving full control of complex, real-world tasks to machines.\nAt its core, there is a tension between human and machine autonomy. Engineering and computer science disciplines have tended to focus on machine autonomy. For example, as of 2019, a search for the word “autonomy” in the Digital Library of the Association for Computing Machinery (ACM) reveals that of the top 100 most cited papers, 90% are on machine autonomy (Calvo et al. 2020). In an attempt to build systems for the benefit of humanity, these disciplines have taken, without question, increasing productivity, efficiency, and automation as primary strategies for benefiting humanity.\n\nMcCarthy, John. 1981. «Epistemological Problems Of Artificial Intelligence». In Readings in Artificial Intelligence, 459–65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\nThese goals put machine automation at the forefront, often at the expense of the human. This approach suffers from inherent challenges, as noted since the early days of AI through the Frame problem and qualification problem, which formalizes the observation that it is impossible to specify all the preconditions needed for a real-world action to succeed (McCarthy 1981).\nThese logical limitations have given rise to mathematical approaches such as Responsibility-sensitive safety (RSS) (Shalev-Shwartz, Shammah, e Shashua 2017), which is aimed at breaking down the end goal of an automated driving system (namely safety) into concrete and checkable conditions that can be rigorously formulated in mathematical terms. The goal of RSS is that those safety rules guarantee ADS safety in the rigorous form of mathematical proof. However, such approaches tend towards using automation to address the problems of automation and are susceptible to many of the same issues.\n\nShalev-Shwartz, Shai, Shaked Shammah, e Amnon Shashua. 2017. «On a formal model of safe and scalable self-driving cars». ArXiv preprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\nFriedman, Batya. 1996. «Value-sensitive design». Interactions 3 (6): 16–23. https://doi.org/10.1145/242485.242493.\n\nPeters, Dorian, Rafael A. Calvo, e Richard M. Ryan. 2018. «Designing for Motivation, Engagement and Wellbeing in Digital Experience». Front. Psychol. 9 (maggio): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\nRyan, Richard M., e Edward L. Deci. 2000. «Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.» Am. Psychol. 55 (1): 68–78. https://doi.org/10.1037/0003-066x.55.1.68.\nAnother approach to combating these issues is to focus on the human-centered design of interactive systems that incorporate human control. Value-sensitive design (Friedman 1996) described three key design factors for a user interface that impact autonomy, including system capability, complexity, misrepresentation, and fluidity. A more recent model, called METUX (A Model for Motivation, Engagement, and Thriving in the User Experience), leverages insights from Self-determination Theory (SDT) in Psychology to identify six distinct spheres of technology experience that contribute to the design systems that promote well-being and human flourishing (Peters, Calvo, e Ryan 2018). SDT defines autonomy as acting by one’s goals and values, which is distinct from the use of autonomy as simply a synonym for either independence or being in control (Ryan e Deci 2000).\nCalvo 2020 elaborates on METUX and its six “spheres of technology experience” in the context of AI-recommender systems (Calvo et al. 2020). They propose these spheres—adoption, Interface, Tasks, Behavior, Life, and Society—as a way of organizing thinking and evaluation of technology design in order to appropriately capture contradictory and downstream impacts on human autonomy when interacting with AI systems.\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, e Richard M Ryan. 2020. «Supporting human autonomy in AI systems: A framework for ethical enquiry». Ethics of digital well-being: A multidisciplinary approach, 31–54.\n\n\n15.7.3 Economic Impacts on Jobs, Skills, Wages\nA major concern of the current rise of AI technologies is widespread unemployment. As AI systems’ capabilities expand, many fear these technologies will cause an absolute loss of jobs as they replace current workers and overtake alternative employment roles across industries. However, changing economic landscapes at the hands of automation is not new, and historically, have been found to reflect patterns of displacement rather than replacement (Shneiderman 2022)—Chapter 4. In particular, automation usually lowers costs and increases quality, greatly increasing access and demand. The need to serve these growing markets pushes production, creating new jobs.\n\n———. 2022. Human-centered AI. Oxford University Press.\nFurthermore, studies have found that attempts to achieve “lights-out” automation – productive and flexible automation with a minimal number of human workers – have been unsuccessful. Attempts to do so have led to what the MIT Work of the Future taskforce has termed “zero-sum automation”, in which process flexibility is sacrificed for increased productivity.\nIn contrast, the task force proposes a “positive-sum automation” approach in which flexibility is increased by designing technology that strategically incorporates humans where they are very much needed, making it easier for line employees to train and debug robots, using a bottom-up approach to identifying what tasks should be automated; and choosing the right metrics for measuring success (see MIT’s Work of the Future).\nHowever, the optimism of the high-level outlook does not preclude individual harm, especially to those whose skills and jobs will be rendered obsolete by automation. Public and legislative pressure, as well as corporate social responsibility efforts, will need to be directed at creating policies that share the benefits of automation with workers and result in higher minimum wages and benefits.\n\n\n15.7.4 Scientific Communication and AI Literacy\nA 1993 survey of 3000 North American adults’ beliefs about the “electronic thinking machine” revealed two primary perspectives of the early computer: the “beneficial tool of man” perspective and the “awesome thinking machine” perspective. The attitudes contributing to the “awesome thinking machine” view in this and other studies revealed a characterization of computers as “intelligent brains, smarter than people, unlimited, fast, mysterious, and frightening” (Martin 1993). These fears highlight an easily overlooked component of responsible AI, especially amidst the rush to commercialize such technologies: scientific communication that accurately communicates the capabilities and limitations of these systems while providing transparency about the limitations of experts’ knowledge about these systems.\n\nMartin, C. Dianne. 1993. «The myth of the awesome thinking machine». Commun. ACM 36 (4): 120–33. https://doi.org/10.1145/255950.153587.\n\nHandlin, Oscar. 1965. «Science and technology in popular culture». Daedalus-us., 156–70.\nAs AI systems’ capabilities expand beyond most people’s comprehension, there is a natural tendency to assume the kinds of apocalyptic worlds painted by our media. This is partly due to the apparent difficulty of assimilating scientific information, even in technologically advanced cultures, which leads to the products of science being perceived as magic—“understandable only in terms of what it did, not how it worked” (Handlin 1965).\nWhile tech companies should be held responsible for limiting grandiose claims and not falling into cycles of hype, research studying scientific communication, especially concerning (generative) AI, will also be useful in tracking and correcting public understanding of these technologies. An analysis of the Scopus scholarly database found that such research is scarce, with only a handful of papers mentioning both “science communication” and “artificial intelligence” (Schäfer 2023).\n\nSchäfer, Mike S. 2023. «The Notorious GPT: Science communication in the age of artificial intelligence». Journal of Science Communication 22 (02): Y02. https://doi.org/10.22323/2.22020402.\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial Intelligence. Edward Elgar Publishing.\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, e Maggie Shen Qiao. 2021. «AI literacy: Definition, teaching, evaluation and ethical issues». Proceedings of the Association for Information Science and Technology 58 (1): 504–9.\nResearch that exposes the perspectives, frames, and images of the future promoted by academic institutions, tech companies, stakeholders, regulators, journalists, NGOs, and others will also help to identify potential gaps in AI literacy among adults (Lindgren 2023). Increased focus on AI literacy from all stakeholders will be important in helping people whose skills are rendered obsolete by AI automation (Ng et al. 2021).\n“But even those who never acquire that understanding need assurance that there is a connection between the goals of science and their welfare, and above all, that the scientist is not a man altogether apart but one who shares some of their value.” (Handlin, 1965)",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html#conclusion",
    "href": "contents/responsible_ai/responsible_ai.html#conclusion",
    "title": "15  Responsible AI",
    "section": "15.8 Conclusion",
    "text": "15.8 Conclusion\nResponsible artificial intelligence is crucial as machine learning systems exert growing influence across healthcare, employment, finance, and criminal justice sectors. While AI promises immense benefits, thoughtlessly designed models risk perpetrating harm through biases, privacy violations, unintended behaviors, and other pitfalls.\nUpholding principles of fairness, explainability, accountability, safety, and transparency enables the development of ethical AI aligned with human values. However, implementing these principles involves surmounting complex technical and social challenges around detecting dataset biases, choosing appropriate model tradeoffs, securing quality training data, and more. Frameworks like value-sensitive design guide balancing accuracy versus other objectives based on stakeholder needs.\nLooking forward, advancing responsible AI necessitates continued research and industry commitment. More standardized benchmarks are required to compare model biases and robustness. As personalized TinyML expands, enabling efficient transparency and user control for edge devices warrants focus. Revised incentive structures and policies must encourage deliberate, ethical development before reckless deployment. Education around AI literacy and its limitations will further contribute to public understanding.\nResponsible methods underscore that while machine learning offers immense potential, thoughtless application risks adverse consequences. Cross-disciplinary collaboration and human-centered design are imperative so AI can promote broad social benefit. The path ahead lies not in an arbitrary checklist but in a steadfast commitment to understand and uphold our ethical responsibility at each step. By taking conscientious action, the machine learning community can lead AI toward empowering all people equitably and safely.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/responsible_ai/responsible_ai.html#sec-responsible-ai-resource",
    "href": "contents/responsible_ai/responsible_ai.html#sec-responsible-ai-resource",
    "title": "15  Responsible AI",
    "section": "15.9 Resources",
    "text": "15.9 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will be adding new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nWhat am I building? What is the goal?\nWho is the audience?\nWhat are the consequences?\nResponsible Data Collection.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 15.1\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nLabs\n\n\n\n\n\nIn addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.\n\nComing soon.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html",
    "href": "contents/sustainable_ai/sustainable_ai.html",
    "title": "16  Sustainable AI",
    "section": "",
    "text": "16.1 Introduction\nThe rapid advancements in artificial intelligence (AI) and machine learning (ML) have led to many beneficial applications and optimizations for performance efficiency. However, the remarkable growth of AI comes with a significant yet often overlooked cost: its environmental impact. The most recent report released by the IPCC, the international body leading scientific assessments of climate change and its impacts, emphasized the pressing importance of tackling climate change. Without immediate efforts to decrease global \\(\\textrm{CO}_2\\) emissions by at least 43 percent before 2030, we exceed global warming of 1.5 degrees Celsius (Winkler et al. 2022). This could initiate positive feedback loops, pushing temperatures even higher. Next to environmental issues, the United Nations recognized 17 Sustainable Development Goals (SDGs), in which AI can play an important role, and vice versa, play an important role in the development of AI systems. As the field continues expanding, considering sustainability is crucial.\nAI systems, particularly large language models like GPT-3 and computer vision models like DALL-E 2, require massive amounts of computational resources for training. For example, GPT-3 was estimated to consume 1,300 megawatt-hours of electricity, which is equal to 1,450 average US households in an entire month (Maslej et al. 2023), or put another way, it consumed enough energy to supply an average US household for 120 years! This immense energy demand stems primarily from power-hungry data centers with servers running intense computations to train these complex neural networks for days or weeks.\nCurrent estimates indicate that the carbon emissions produced from developing a single, sophisticated AI model can equal the emissions over the lifetime of five standard gasoline-powered vehicles (Strubell, Ganesh, e McCallum 2019). A significant portion of the electricity presently consumed by data centers is generated from nonrenewable sources such as coal and natural gas, resulting in data centers contributing around 1% of total worldwide carbon emissions. This is comparable to the emissions from the entire airline sector. This immense carbon footprint demonstrates the pressing need to transition to renewable power sources such as solar and wind to operate AI development.\nAdditionally, even small-scale AI systems deployed to edge devices as part of TinyML have environmental impacts that should not be ignored (Prakash, Stewart, et al. 2023). The specialized hardware required for AI has an environmental toll from natural resource extraction and manufacturing. GPUs, CPUs, and chips like TPUs depend on rare earth metals whose mining and processing generate substantial pollution. The production of these components also has its energy demands. Furthermore, collecting, storing, and preprocessing data used to train both small- and large-scale models comes with environmental costs, further exacerbating the sustainability implications of ML systems.\nThus, while AI promises innovative breakthroughs in many fields, sustaining progress requires addressing sustainability challenges. AI can continue advancing responsibly by optimizing models’ efficiency, exploring alternative specialized hardware and renewable energy sources for data centers, and tracking its overall environmental impact.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#introduction",
    "href": "contents/sustainable_ai/sustainable_ai.html#introduction",
    "title": "16  Sustainable AI",
    "section": "",
    "text": "Winkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilariño, Sivan Kartha, e Joana Portugal-Pereira. 2022. «Examples of shifting development pathways: Lessons on how to enable broader, deeper, and faster climate action». Climate Action 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, et al. 2023. «Artificial intelligence index report 2023». ArXiv preprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete Warden, Brian Plancher, e Vijay Janapa Reddi. 2023. «Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers». ArXiv preprint. https://arxiv.org/abs/2301.11899.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#social-and-ethical-responsibility",
    "href": "contents/sustainable_ai/sustainable_ai.html#social-and-ethical-responsibility",
    "title": "16  Sustainable AI",
    "section": "16.2 Social and Ethical Responsibility",
    "text": "16.2 Social and Ethical Responsibility\nThe environmental impact of AI is not just a technical issue but also an ethical and social one. As AI becomes more integrated into our lives and industries, its sustainability becomes increasingly critical.\n\n16.2.1 Ethical Considerations\nThe scale of AI’s environmental footprint raises profound ethical questions about the responsibilities of AI developers and companies to minimize their carbon emissions and energy usage. As the creators of AI systems and technologies that can have sweeping global impacts, developers have an ethical obligation to consciously integrate environmental stewardship into their design process, even if sustainability comes at the cost of some efficiency gains.\nThere is a clear and present need for us to have open and honest conversations about AI’s environmental tradeoffs earlier in the development lifecycle. Researchers should feel empowered to voice concerns if organizational priorities do not align with ethical goals, as in the case of the open letter to pause giant AI experiments.\nAdditionally, there is an increasing need for AI companies to scrutinize their contributions to climate change and environmental harm. Large tech firms are responsible for the cloud infrastructure, data center energy demands, and resource extraction required to power today’s AI. Leadership should assess whether organizational values and policies promote sustainability, from hardware manufacturing through model training pipelines.\nFurthermore, more than voluntary self-regulation may be needed– -governments may need to introduce new regulations aimed at sustainable AI standards and practices if we hope to curb the projected energy explosion of ever-larger models. Reported metrics like computing usage, carbon footprint, and efficiency benchmarks could hold organizations accountable.\nThrough ethical principles, company policies, and public rules, AI technologists and corporations have a profound duty to our planet to ensure the responsible and sustainable advancement of technology positioned to transform modern society radically. We owe it to future generations to get this right.\n\n\n16.2.2 Long-term Sustainability\nThe massive projected expansion of AI raises urgent concerns about its long-term sustainability. As AI software and applications rapidly increase in complexity and usage across industries, demand for computing power and infrastructure will skyrocket exponentially in the coming years.\nTo put the scale of projected growth in perspective, the total computing capacity required for training AI models saw an astonishing 350,000x increase from 2012 to 2019 (R. Schwartz et al. 2020). Researchers forecast over an order of magnitude growth each year moving forward as personalized AI assistants, autonomous technology, precision medicine tools, and more are developed. Similar trends are estimated for embedded ML systems, with an estimated 2.5 billion AI-enabled edge devices deployed by 2030.\nManaging this expansion level requires software and hardware-focused breakthroughs in efficiency and renewable integration from AI engineers and scientists. On the software side, novel techniques in model optimization, distillation, pruning, low-precision numerics, knowledge sharing between systems, and other areas must become widespread best practices to curb energy needs. For example, realizing even a 50% reduced computational demand per capability doubling would have massive compounding on total energy.\nOn the hardware infrastructure side, due to increasing costs of data transfer, storage, cooling, and space, continuing today’s centralized server farm model at data centers is likely infeasible long-term (Lannelongue, Grealey, e Inouye 2021). Exploring alternative decentralized computing options around “edge AI” on local devices or within telco networks can alleviate scaling pressures on power-hungry hyper scale data centers. Likewise, the shift towards carbon-neutral, hybrid renewable energy sources powering leading cloud provider data centers worldwide will be essential.\n\nLannelongue, Loı̈c, Jason Grealey, e Michael Inouye. 2021. «Green Algorithms: Quantifying the Carbon Footprint of Computation». Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\n16.2.3 AI for Environmental Good\nWhile much focus goes on AI’s sustainability challenges, these powerful technologies provide unique solutions to combat climate change and drive environmental progress. For example, ML can continuously optimize smart power grids to improve renewable integration and electricity distribution efficiency across networks (Zhang, Han, e Deng 2018). Models can ingest the real-time status of a power grid and weather forecasts to allocate and shift sources responding to supply and demand.\n\nZhang, Dongxia, Xiaoqing Han, e Chunyu Deng. 2018. «Review on the research and practice of deep learning and reinforcement learning in smart grids». CSEE Journal of Power and Energy Systems 4 (3): 362–70. https://doi.org/10.17775/cseejpes.2018.00520.\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. «Learning skillful medium-range global weather forecasting». Science 382 (6677): 1416–21. https://doi.org/10.1126/science.adi2336.\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, e Anima Anandkumar. 2023. «FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators». In Proceedings of the Platform for Advanced Scientific Computing Conference, 1–11. ACM. https://doi.org/10.1145/3592979.3593412.\nFine-tuned neural networks have also proven remarkably effective at next-generation weather forecasting (Lam et al. 2023) and climate modeling (Kurth et al. 2023). They can rapidly analyze massive volumes of climate data to boost extreme event preparation and resource planning for hurricanes, floods, droughts, and more. Climate researchers have achieved state-of-the-art storm path accuracy by combining AI simulations with traditional numerical models.\nAI also enables better tracking of biodiversity (Silvestro et al. 2022), wildlife (D. Schwartz et al. 2021), ecosystems, and illegal deforestation using drones and satellite feeds. Computer vision algorithms can automate species population estimates and habitat health assessments over huge untracked regions. These capabilities provide conservationists with powerful tools for combating poaching (Bondi et al. 2018), reducing species extinction risks, and understanding ecological shifts.\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, e Alexandre Antonelli. 2022. «Improving biodiversity protection through artificial intelligence». Nature Sustainability 5 (5): 415–24. https://doi.org/10.1038/s41893-022-00851-6.\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, e Andreas Paepcke. 2021. «Deployment of Embedded Edge-AI for Wildlife Monitoring in Remote Regions». In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), 1035–42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital Shah, Robert Hannaford, Arvind Iyer, Lucas Joppa, e Milind Tambe. 2018. «Near Real-Time Detection of Poachers from Drones in AirSim». In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, a cura di Jérôme Lang, 5814–16. International Joint Conferences on Artificial Intelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\nTargeted investment in AI applications for environmental sustainability, cross-sector data sharing, and model accessibility can profoundly accelerate solutions to pressing ecological issues. Emphasizing AI for social good steers innovation in cleaner directions, guiding these world-shaping technologies towards ethical and responsible development.\n\n\n16.2.4 Case Study\nGoogle’s data centers are foundational to powering products like Search, Gmail, and YouTube, which are used by billions daily. However, keeping the vast server farms up and running requires substantial energy, particularly for vital cooling systems. Google continuously strives to improve efficiency across operations. Yet progress was proving difficult through traditional methods alone, considering the complex, custom dynamics involved. This challenge prompted an ML breakthrough, yielding potential savings.\nAfter over a decade of optimizing data center design, inventing energy-efficient computing hardware, and securing renewable energy sources, Google brought DeepMind scientists to unlock further advances. The AI experts faced intricate factors surrounding the functioning of industrial cooling apparatuses. Equipment like pumps and chillers interact nonlinearly, while external weather and internal architectural variables also change. Capturing this complexity confounded rigid engineering formulas and human intuition.\nThe DeepMind team leveraged Google’s extensive historical sensor data detailing temperatures, power draw, and other attributes as training inputs. They built a flexible system based on neural networks to model the relationships and predict optimal configurations, minimizing power usage effectiveness (PUE) (Barroso, Hölzle, e Ranganathan 2019); PUE is the standard measurement for gauging how efficiently a data center uses energy gives the proportion of total facility power consumed divided by the power directly used for computing operations. When tested live, the AI system delivered remarkable gains beyond prior innovations, lowering cooling energy by 40% for a 15% drop in total PUE, a new site record. The generalizable framework learned cooling dynamics rapidly across shifting conditions that static rules could not match. The breakthrough highlights AI’s rising role in transforming modern tech and enabling a sustainable future.\n\nBarroso, Luiz André, Urs Hölzle, e Parthasarathy Ranganathan. 2019. The Datacenter as a Computer: Designing Warehouse-Scale Machines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#energy-consumption",
    "href": "contents/sustainable_ai/sustainable_ai.html#energy-consumption",
    "title": "16  Sustainable AI",
    "section": "16.3 Energy Consumption",
    "text": "16.3 Energy Consumption\n\n16.3.1 Understanding Energy Needs\nUnderstanding the energy needs for training and operating AI models is crucial in the rapidly evolving field of A.I. With AI entering widespread use in many new fields (Bohr e Memarzadeh 2020; Sudhakar, Sze, e Karaman 2023), the demand for AI-enabled devices and data centers is expected to explode. This understanding helps us understand why AI, particularly deep learning, is often labeled energy-intensive.\n\nBohr, Adam, e Kaveh Memarzadeh. 2020. «The rise of artificial intelligence in healthcare applications». In Artificial Intelligence in Healthcare, 25–60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\nEnergy Requirements for AI Training\nThe training of complex AI systems like large deep learning models can demand startlingly high levels of computing power–with profound energy implications. Consider OpenAI’s state-of-the-art language model GPT-3 as a prime example. This system pushes the frontiers of text generation through algorithms trained on massive datasets. Yet, the energy GPT-3 consumed for a single training cycle could rival an entire small town’s monthly usage. In recent years, these generative AI models have gained increasing popularity, leading to more models being trained. Next to the increased number of models, the number of parameters in these models will also increase. Research shows that increasing the model size (number of parameters), dataset size, and compute used for training improves performance smoothly with no signs of saturation (Kaplan et al. 2020). See how, in Figura 16.1, the test loss decreases as each of the 3 increases above.\n\n\n\n\n\n\nFigura 16.1: Performance improves with compute, dataset set, and model size. Source: Kaplan et al. (2020).\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, e Dario Amodei. 2020. «Scaling Laws for Neural Language Models». ArXiv preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nWhat drives such immense requirements? During training, models like GPT-3 learn their capabilities by continuously processing huge volumes of data to adjust internal parameters. The processing capacity enabling AI’s rapid advances also contributes to surging energy usage, especially as datasets and models balloon. GPT-3 highlights a steady trajectory in the field where each leap in AI’s sophistication traces back to ever more substantial computational power and resources. Its predecessor, GPT-2, required 10x less training to compute only 1.5 billion parameters, a difference now dwarfed by magnitudes as GPT-3 comprises 175 billion parameters. Sustaining this trajectory toward increasingly capable AI raises energy and infrastructure provision challenges ahead.\n\n\nOperational Energy Use\nDeveloping and training AI models requires immense data, computing power, and energy. However, the deployment and operation of those models also incur significant recurrent resource costs over time. AI systems are now integrated across various industries and applications and are entering the daily lives of an increasing demographic. Their cumulative operational energy and infrastructure impacts could eclipse the upfront model training.\nThis concept is reflected in the demand for training and inference hardware in data centers and on the edge. Inference refers to using a trained model to make predictions or decisions on real-world data. According to a recent McKinsey analysis, the need for advanced systems to train ever-larger models is rapidly growing. However, inference computations already make up a dominant and increasing portion of total AI workloads, as shown in Figura 16.2. Running real-time inference with trained models–whether for image classification, speech recognition, or predictive analytics–invariably demands computing hardware like servers and chips. However, even a model handling thousands of facial recognition requests or natural language queries daily is dwarfed by massive platforms like Meta. Where inference on millions of photos and videos shared on social media, the infrastructure energy requirements continue to scale!\n\n\n\n\n\n\nFigura 16.2: Market size for inference and training hardware. Source: McKinsey.\n\n\n\nAlgorithms powering AI-enabled smart assistants, automated warehouses, self-driving vehicles, tailored healthcare, and more have marginal individual energy footprints. However, the projected proliferation of these technologies could add hundreds of millions of endpoints running AI algorithms continually, causing the scale of their collective energy requirements to surge. Current efficiency gains need help to counterbalance this sheer growth.\nAI is expected to see an annual growth rate of 37.3% between 2023 and 2030. Yet, applying the same growth rate to operational computing could multiply annual AI energy needs up to 1,000 times by 2030. So, while model optimization tackles one facet, responsible innovation must also consider total lifecycle costs at global deployment scales that were unfathomable just years ago but now pose infrastructure and sustainability challenges ahead.\n\n\n\n16.3.2 Data Centers and Their Impact\nAs the demand for AI services grows, the impact of data centers on the energy consumption of AI systems is becoming increasingly important. While these facilities are crucial for the advancement and deployment of AI, they contribute significantly to its energy footprint.\n\nScale\nData centers are the essential workhorses enabling the recent computational demands of advanced AI systems. For example, leading providers like Meta operate massive data centers spanning up to the size of multiple football fields, housing hundreds of thousands of high-capacity servers optimized for parallel processing and data throughput.\nThese massive facilities provide the infrastructure for training complex neural networks on vast datasets. For instance, based on leaked information, OpenAI’s language model GPT-4 was trained on Azure data centers packing over 25,000 Nvidia A100 GPUs, used continuously for over 90 to 100 days.\nAdditionally, real-time inference for consumer AI applications at scale is only made possible by leveraging the server farms inside data centers. Services like Alexa, Siri, and Google Assistant process billions of voice requests per month from users globally by relying on data center computing for low-latency response. In the future, expanding cutting-edge use cases like self-driving vehicles, precision medicine diagnostics, and accurate climate forecasting models will require significant computational resources to be obtained by tapping into vast on-demand cloud computing resources from data centers. Some emerging applications, like autonomous cars, have harsh latency and bandwidth constraints. Locating data center-level computing power on the edge rather than the cloud will be necessary.\nMIT research prototypes have shown trucks and cars with onboard hardware performing real-time AI processing of sensor data equivalent to small data centers (Sudhakar, Sze, e Karaman 2023). These innovative “data centers on wheels” demonstrate how vehicles like self-driving trucks may need embedded data center-scale compute on board to achieve millisecond system latency for navigation, though still likely supplemented by wireless 5G connectivity to more powerful cloud data centers.\n\nSudhakar, Soumya, Vivienne Sze, e Sertac Karaman. 2023. «Data Centers on Wheels: Emissions From Computing Onboard Autonomous Vehicles». IEEE Micro 43 (1): 29–39. https://doi.org/10.1109/mm.2022.3219803.\nThe bandwidth, storage, and processing capacities required to enable this future technology at scale will depend heavily on advancements in data center infrastructure and AI algorithmic innovations.\n\n\nEnergy Demand\nThe energy demand of data centers can roughly be divided into 4 components—infrastructure, network, storage, and servers. In Figura 16.3, we see that the data infrastructure (which includes cooling, lighting, and controls) and the servers use most of the total energy budget of data centers in the US (Shehabi et al. 2016). This section breaks down the energy demand for the servers and the infrastructure. For the latter, the focus is on cooling systems, as cooling is the dominant factor in energy consumption in the infrastructure.\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin, Jonathan Koomey, Eric Masanet, Nathaniel Horner, Inês Azevedo, e William Lintner. 2016. «United states data center energy usage report».\n\n\n\n\n\n\nFigura 16.3: Data centers energy consumption in the US. Source: International Energy Agency (IEA).\n\n\n\n\nServers\nThe increase in energy consumption of data centers stems mainly from exponentially growing AI computing requirements. NVIDIA DGX H100 machines that are optimized for deep learning can draw up to 10.2 kW at peak. Leading providers operate data centers with hundreds to thousands of these power-hungry DGX nodes networked to train the latest AI models. For example, the supercomputer developed for OpenAI is a single system with over 285,000 CPU cores, 10,000 GPUs, and 400 gigabits per second of network connectivity for each GPU server.\nThe intensive computations needed across an entire facility’s densely packed fleet and supporting hardware result in data centers drawing tens of megawatts around the clock. Overall, advancing AI algorithms continue to expand data center energy consumption as more DGX nodes get deployed to keep pace with projected growth in demand for AI compute resources over the coming years.\n\n\nCooling Systems\nTo keep the beefy servers fed at peak capacity and cool, data centers require tremendous cooling capacity to counteract the heat produced by densely packed servers, networking equipment, and other hardware running computationally intensive workloads without pause. With large data centers packing thousands of server racks operating at full tilt, massive industrial-scale cooling towers and chillers are required, using energy amounting to 30-40% of the total data center electricity footprint (Dayarathna, Wen, e Fan 2016). Consequently, companies are looking for alternative methods of cooling. For example, Microsoft’s data center in Ireland leverages a nearby fjord to exchange heat using over half a million gallons of seawater daily.\nRecognizing the importance of energy-efficient cooling, there have been innovations aimed at reducing this energy demand. Techniques like free cooling, which uses outside air or water sources when conditions are favorable, and the use of AI to optimize cooling systems are examples of how the industry adapts. These innovations reduce energy consumption, lower operational costs, and lessen the environmental footprint. However, exponential increases in AI model complexity continue to demand more servers and acceleration hardware operating at higher utilization, translating to rising heat generation and ever greater energy used solely for cooling purposes.\n\n\n\nThe Environmental Impact\nThe environmental impact of data centers is not only caused by the direct energy consumption of the data center itself (Siddik, Shehabi, e Marston 2021). Data center operation involves the supply of treated water to the data center and the discharge of wastewater from the data center. Water and wastewater facilities are major electricity consumers.\n\nSiddik, Md Abu Bakar, Arman Shehabi, e Landon Marston. 2021. «The environmental footprint of data centers in the United States». Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, e Max Smolaks. 2022. «Uptime Institute Global Data Center Survey 2022». Uptime Institute.\nNext to electricity usage, there are many more aspects to the environmental impacts of these data centers. The water usage of the data centers can lead to water scarcity issues, increased water treatment needs, and proper wastewater discharge infrastructure. Also, raw materials required for construction and network transmission considerably impact environmental t the environment, and components in data centers need to be upgraded and maintained. Where almost 50 percent of servers were refreshed within 3 years of usage, refresh cycles have shown to slow down (Davis et al. 2022). Still, this generates significant e-waste, which can be hard to recycle.\n\n\n\n16.3.3 Energy Optimization\nUltimately, measuring and understanding the energy consumption of AI facilitates optimizing energy consumption.\nOne way to reduce the energy consumption of a given amount of computational work is to run it on more energy-efficient hardware. For instance, TPU chips can be more energy-efficient compared to CPUs when it comes to running large tensor computations for AI, as TPUs can run such computations much faster without drawing significantly more power than CPUs. Another way is to build software systems aware of energy consumption and application characteristics. Good examples are systems works such as Zeus (You, Chung, e Chowdhury 2023) and Perseus (Chung et al. 2023), both of which characterize the tradeoff between computation time and energy consumption at various levels of an ML training system to achieve energy reduction without end-to-end slowdown. In reality, building both energy-efficient hardware and software and combining their benefits should be promising, along with open-source frameworks (e.g., Zeus) that facilitate community efforts.\n\nYou, Jie, Jae-Won Chung, e Mosharaf Chowdhury. 2023. «Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training». In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), 119–39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, e Mosharaf Chowdhury. 2023. «Perseus: Removing Energy Bloat from Large Model Training». ArXiv preprint abs/2312.06902. https://arxiv.org/abs/2312.06902.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#carbon-footprint",
    "href": "contents/sustainable_ai/sustainable_ai.html#carbon-footprint",
    "title": "16  Sustainable AI",
    "section": "16.4 Carbon Footprint",
    "text": "16.4 Carbon Footprint\nThe massive electricity demands of data centers can lead to significant environmental externalities absent an adequate renewable power supply. Many facilities rely heavily on nonrenewable energy sources like coal and natural gas. For example, data centers are estimated to produce up to 2% of total global \\(\\textrm{CO}_2\\) emissions which is closing the gap with the airline industry. As mentioned in previous sections, the computational demands of AI are set to increase. The emissions of this surge are threefold. First, data centers are projected to increase in size (Liu et al. 2020). Secondly, emissions during training are set to increase significantly (Patterson et al. 2022). Thirdly, inference calls to these models are set to increase dramatically.\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, e Yun Tian. 2020. «Energy consumption and emission mitigation prediction based on data center traffic and PUE for global data centers». Global Energy Interconnection 3 (3): 272–82. https://doi.org/10.1016/j.gloei.2020.07.008.\nWithout action, this exponential demand growth risks ratcheting up the carbon footprint of data centers further to unsustainable levels. Major providers have pledged carbon neutrality and committed funds to secure clean energy, but progress remains incremental compared to overall industry expansion plans. More radical grid decarbonization policies and renewable energy investments may prove essential to counteracting the climate impact of the coming tide of new data centers aimed at supporting the next generation of AI.\n\n16.4.1 Definition and Significance\nThe concept of a ‘carbon footprint’ has emerged as a key metric. This term refers to the total amount of greenhouse gasses, particularly carbon dioxide, emitted directly or indirectly by an individual, organization, event, or product. These emissions significantly contribute to the greenhouse effect, accelerating global warming and climate change. The carbon footprint is measured in terms of carbon dioxide equivalents (\\(\\textrm{CO}_2\\)e), allowing for a comprehensive account that includes various greenhouse gasses and their relative environmental impact. Examples of this as applied to large-scale ML tasks are shown in Figura 16.4.\n\n\n\n\n\n\nFigura 16.4: Carbon footprint of large-scale ML tasks. Source: Wu et al. (2022).\n\n\n\nConsidering the carbon footprint is especially important in AI AI’s rapid advancement and integration into various sectors, bringing its environmental impact into sharp focus. AI systems, particularly those involving intensive computations like deep learning and large-scale data processing, are known for their substantial energy demands. This energy, often drawn from power grids, may still predominantly rely on fossil fuels, leading to significant greenhouse gas emissions.\nTake, for example, training large AI models such as GPT-3 or complex neural networks. These processes require immense computational power, typically provided by data centers. The energy consumption associated with operating these centers, particularly for high-intensity tasks, results in notable greenhouse gas emissions. Studies have highlighted that training a single AI model can generate carbon emissions comparable to that of the lifetime emissions of multiple cars, shedding light on the environmental cost of developing advanced AI technologies (Dayarathna, Wen, e Fan 2016). Figura 16.5 shows a comparison from lowest to highest carbon footprints, starting with a roundtrip flight between NY and SF, human life average per year, American life average per year, US car including fuel over a lifetime, and a Transformer model with neural architecture search, which has the highest footprint.\n\n\n\n\n\n\nFigura 16.5: Carbon footprint of NLP model in lbs of \\(\\textrm{CO}_2\\) equivalent. Source: Dayarathna, Wen, e Fan (2016).\n\n\nDayarathna, Miyuru, Yonggang Wen, e Rui Fan. 2016. «Data Center Energy Consumption Modeling: A Survey». IEEE Communications Surveys &amp; Tutorials 18 (1): 732–94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nMoreover, AI’s carbon footprint extends beyond the operational phase. The entire lifecycle of AI systems, including the manufacturing of computing hardware, the energy used in data centers for cooling and maintenance, and the disposal of electronic waste, contributes to their overall carbon footprint. We have discussed some of these aspects earlier, and we will discuss the waste aspects later in this chapter.\n\n\n16.4.2 The Need for Awareness and Action\nUnderstanding the carbon footprint of AI systems is crucial for several reasons. Primarily, it is a step towards mitigating the impacts of climate change. As AI continues to grow and permeate different aspects of our lives, its contribution to global carbon emissions becomes a significant concern. Awareness of these emissions can inform decisions made by developers, businesses, policymakers, and even ML engineers and scientists like us to ensure a balance between technological innovation and environmental responsibility.\nFurthermore, this understanding stimulates the drive towards ‘Green AI’ (R. Schwartz et al. 2020). This approach focuses on developing AI technologies that are efficient, powerful, and environmentally sustainable. It encourages exploring energy-efficient algorithms, using renewable energy sources in data centers, and adopting practices that reduce A. I’m the overall environmental impact.\nIn essence, the carbon footprint is an essential consideration in developing and applying AI technologies. As AI evolves and its applications become more widespread, managing its carbon footprint is key to ensuring that this technological progress aligns with the broader environmental sustainability goals.\n\n\n16.4.3 Estimating the AI Carbon Footprint\nEstimating AI systems’ carbon footprint is critical in understanding their environmental impact. This involves analyzing the various elements contributing to emissions throughout AI technologies’ lifecycle and employing specific methodologies to quantify these emissions accurately. Many different methods for quantifying ML’s carbon emissions have been proposed.\nThe carbon footprint of AI encompasses several key elements, each contributing to the overall environmental impact. First, energy is consumed during the AI model training and operational phases. The source of this energy heavily influences the carbon emissions. Once trained, these models, depending on their application and scale, continue to consume electricity during operation. Next to energy considerations, the hardware used stresses the environment as well.\nThe carbon footprint varies significantly based on the energy sources used. The composition of the sources providing the energy used in the grid varies widely depending on geographical region and even time in a single day! For example, in the USA, roughly 60 percent of the total energy supply is still covered by fossil fuels. Nuclear and renewable energy sources cover the remaining 40 percent. These fractions are not constant throughout the day. As renewable energy production usually relies on environmental factors, such as solar radiation and pressure fields, they do not provide a constant energy source.\nThe variability of renewable energy production has been an ongoing challenge in the widespread use of these sources. Looking at Figura 16.6, which shows data for the European grid, we see that it is supposed to be able to produce the required amount of energy throughout the day. While solar energy peaks in the middle of the day, wind energy shows two distinct peaks in the mornings and evenings. Currently, we rely on fossil and coal-based energy generation methods to supply the lack of energy during times when renewable energy does not meet requirements,\nInnovation in energy storage solutions is required to enable constant use of renewable energy sources. The base energy load is currently met with nuclear energy. This constant energy source does not directly emit carbon emissions but needs to be faster to accommodate the variability of renewable energy sources. Tech companies such as Microsoft have shown interest in nuclear energy sources to power their data centers. As the demand of data centers is more constant than the demand of regular households, nuclear energy could be used as a dominant source of energy.\n\n\n\n\n\n\nFigura 16.6: Energy sources and generation capabilities. Source: Energy Charts..\n\n\n\nAdditionally, the manufacturing and disposal of AI hardware add to the carbon footprint. Producing specialized computing devices, such as GPUs and CPUs, is energy- and resource-intensive. This phase often relies on energy sources that contribute to greenhouse gas emissions. The electronics industry’s manufacturing process has been identified as one of the eight big supply chains responsible for more than 50 percent of global emissions (Challenge 2021). Furthermore, the end-of-life disposal of this hardware, which can lead to electronic waste, also has environmental implications. As mentioned, servers have a refresh cycle of roughly 3 to 5 years. Of this e-waste, currently only 17.4 percent is properly collected and recycled.. The carbon emissions of this e-waste has shown an increase of more than 50 percent between 2014 and 2020 (Singh e Ogunseitan 2022).\n\nChallenge, WEF Net-Zero. 2021. «The Supply Chain Opportunity». In World Economic Forum: Geneva, Switzerland.\n\nSingh, Narendra, e Oladele A. Ogunseitan. 2022. «Disentangling the worldwide web of e-waste and climate change co-benefits». Circular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\nAs is clear from the above, a proper Life Cycle Analysis is necessary to portray all relevant aspects of the emissions caused by AI. Another method is carbon accounting, which quantifies the amount of carbon dioxide emissions directly and indirectly associated with AI operations. This measurement typically uses \\(\\textrm{CO}_2\\) equivalents, allowing for a standardized way of reporting and assessing emissions.\n\n\n\n\n\n\nEsercizio 16.1: AI’s Carbon Footprint\n\n\n\n\n\nDid you know that the cutting-edge AI models you might use have an environmental impact? This exercise will go into an AI system’s “carbon footprint.” You’ll learn how data centers’ energy demands, large AI models’ training, and even hardware manufacturing contribute to greenhouse gas emissions. We’ll discuss why it’s crucial to be aware of this impact, and you’ll learn methods to estimate the carbon footprint of your own AI projects. Get ready to explore the intersection of AI and environmental sustainability!",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#beyond-carbon-footprint",
    "href": "contents/sustainable_ai/sustainable_ai.html#beyond-carbon-footprint",
    "title": "16  Sustainable AI",
    "section": "16.5 Beyond Carbon Footprint",
    "text": "16.5 Beyond Carbon Footprint\nThe current focus on reducing AI systems’ carbon emissions and energy consumption addresses one crucial aspect of sustainability. However, manufacturing the semiconductors and hardware that enable AI also carries severe environmental impacts that receive comparatively less public attention. Building and operating a leading-edge semiconductor fabrication plant, or “fab,” has substantial resource requirements and polluting byproducts beyond a large carbon footprint.\nFor example, a state-of-the-art fab producing state-of-the-art chips like in 5nm can require up to four million gallons of pure water each day. This water usage approaches what a city of half a million people would require for all needs. Sourcing this consistently places immense strain on local water tables and reservoirs, especially in already water-stressed regions that host many high-tech manufacturing hubs.\nAdditionally, over 250 unique hazardous chemicals are utilized at various stages of semiconductor production within fabs (Mills e Le Hunte 1997). These include volatile solvents like sulfuric acid, nitric acid, and hydrogen fluoride, along with arsine, phosphine, and other highly toxic substances. Preventing the discharge of these chemicals requires extensive safety controls and wastewater treatment infrastructure to avoid soil contamination and risks to surrounding communities. Any improper chemical handling or unanticipated spill carries dire consequences.\n\nMills, Andrew, e Stephen Le Hunte. 1997. «An overview of semiconductor photocatalysis». J. Photochem. Photobiol., A 108 (1): 1–35. https://doi.org/10.1016/s1010-6030(97)00118-4.\nBeyond water consumption and chemical risks, fab operations also depend on rare metals sourcing, generate tons of dangerous waste products, and can hamper local biodiversity. This section will analyze these critical but less discussed impacts. With vigilance and investment in safety, the harms from semiconductor manufacturing can be contained while still enabling technological progress. However, ignoring these externalized issues will exacerbate ecological damage and health risks over the long run.\n\n16.5.1 Water Usage and Stress\nSemiconductor fabrication is an incredibly water-intensive process. Based on an article from 2009, a typical 300mm silicon wafer requires 8,328 liters of water, of which 5,678 liters is ultrapure water (Cope 2009). Today, a typical fab can use up to four million gallons of pure water. To operate one facility, TSMC’s latest fab in Arizona is projected to use 8.9 million gallons daily or nearly 3 percent of the city’s current water production. To put things in perspective, Intel and Quantis found that over 97% of their direct water consumption is attributed to semiconductor manufacturing operations within their fabrication facilities (Cooper et al. 2011).\n\nCope, Gord. 2009. «Pure water, semiconductors and the recession». Global Water Intelligence 10 (10).\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien Humbert, e Lindsay Lessard. 2011. «A semiconductor company’s examination of its water footprint approach». In Proceedings of the 2011 IEEE International Symposium on Sustainable Systems and Technology, 1–6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\nThis water is repeatedly used to flush away contaminants in cleaning steps and also acts as a coolant and carrier fluid in thermal oxidation, chemical deposition, and chemical mechanical planarization processes. During peak summer months, this approximates the daily water consumption of a city with a population of half a million people.\nDespite being located in regions with sufficient water, the intensive usage can severely depress local water tables and drainage basins. For example, the city of Hsinchu in Taiwan suffered sinking water tables and seawater intrusion into aquifers due to excessive pumping to satisfy water supply demands from the Taiwan Semiconductor Manufacturing Company (TSMC) fab. In water-scarce inland areas like Arizona, massive water inputs are needed to support fabs despite already strained reservoirs.\nWater discharge from fabs risks environmental contamination besides depletion if not properly treated. While much discharge is recycled within the fab, the purification systems still filter out metals, acids, and other contaminants that can pollute rivers and lakes if not cautiously handled (Prakash, Callahan, et al. 2023). These factors make managing water usage essential when mitigating wider sustainability impacts.\n\n\n16.5.2 Hazardous Chemicals Usage\nModern semiconductor fabrication involves working with many highly hazardous chemicals under extreme conditions of heat and pressure (Kim et al. 2018). Key chemicals utilized include:\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk Park, Sangjun Choi, Seungwon Kim, Kwonchul Ha, e Won Kim. 2018. «Chemical use in the semiconductor manufacturing industry». Int. J. Occup. Env. Heal. 24 (3-4): 109–18. https://doi.org/10.1080/10773525.2018.1519957.\n\nStrong acids: Hydrofluoric, sulfuric, nitric, and hydrochloric acids rapidly eat through oxides and other surface contaminants but also pose toxicity dangers. Fabs can use thousands of metric tons of these acids annually, and accidental exposure can be fatal for workers.\nSolvents: Key solvents like xylene, methanol, and methyl isobutyl ketone (MIBK) handle dissolving photoresists but have adverse health impacts like skin/eye irritation and narcotic effects if mishandled. They also create explosive and air pollution risks.\nToxic gases: Gas mixtures containing arsine (AsH3), phosphine (PH3), diborane (B2H6), germane (GeH4), etc., are some of the deadliest chemicals used in doping and vapor deposition steps. Minimal exposures can lead to poisoning, tissue damage, and even death without quick treatment.\nChlorinated compounds: Older chemical mechanical planarization formulations incorporated perchloroethylene, trichloroethylene, and other chlorinated solvents, which have since been banned due to their carcinogenic effects and impacts on the ozone layer. However, their prior release still threatens surrounding groundwater sources.\n\nStrict handling protocols, protective equipment for workers, ventilation, filtrating/scrubbing systems, secondary containment tanks, and specialized disposal mechanisms are vital where these chemicals are used to minimize health, explosion, air, and environmental spill dangers (Wald e Jones 1987). But human errors and equipment failures still occasionally occur–highlighting why reducing fab chemical intensities is an ongoing sustainability effort.\n\nWald, Peter H., e Jeffrey R. Jones. 1987. «Semiconductor manufacturing: An introduction to processes and hazards». Am. J. Ind. Med. 11 (2): 203–21. https://doi.org/10.1002/ajim.4700110209.\n\n\n16.5.3 Resource Depletion\nWhile silicon forms the base, there is an almost endless supply of silicon on Earth. In fact, silicon is the second most plentiful element found in the Earth’s crust, accounting for 27.7% of the crust’s total mass. Only oxygen exceeds silicon in abundance within the crust. Therefore, silicon is not necessary to consider for resource depletion. However, the various specialty metals and materials that enable the integrated circuit fabrication process and provide specific properties still need to be discovered. Maintaining supplies of these resources is crucial yet threatened by finite availability and geopolitical influences (Nakano 2021).\n\nNakano, Jane. 2021. The geopolitics of critical minerals supply chains. JSTOR.\n\nChen, H.-W. 2006. «Gallium, Indium, and Arsenic Pollution of Groundwater from a Semiconductor Manufacturing Area of Taiwan». B. Environ. Contam. Tox. 77 (2): 289–96. https://doi.org/10.1007/s00128-006-1062-3.\nGallium, indium, and arsenic are vital ingredients in forming ultra-efficient compound semiconductors in the highest-speed chips suited for 5G and AI applications (Chen 2006). However, these rare elements have relatively scarce natural deposits that are being depleted. The United States Geological Survey has indium on its list of most critical at-risk commodities, estimated to have less than a 15-year viable global supply at current demand growth (Davies 2011).\nHelium is required in huge volumes for next-gen fabs to enable precise wafer cooling during operation. But helium’s relative rarity and the fact that once it vents into the atmosphere, it quickly escapes Earth make maintaining helium supplies extremely challenging long-term (Davies 2011). According to the US National Academies, substantial price increases and supply shocks are already occurring in this thinly traded market.\n\nJha, A. R. 2014. Rare Earth Materials: Properties and Applications. CRC Press. https://doi.org/10.1201/b17045.\nOther risks include China’s control over 90% of the rare earth elements critical to semiconductor material production (Jha 2014). Any supply chain issues or trade disputes can lead to catastrophic raw material shortages, given the lack of current alternatives. In conjunction with helium shortages, resolving the limited availability and geographic imbalance in accessing essential ingredients remains a sector priority for sustainability.\n\n\n16.5.4 Hazardous Waste Generation\nSemiconductor fabs generate tons of hazardous waste annually as byproducts from the various chemical processes (Grossman 2007). The key waste streams include:\n\nGrossman, Elizabeth. 2007. High tech trash: Digital devices, hidden toxics, and human health. Island press.\n\nGaseous waste: Fab ventilation systems capture harmful gases like arsine, phosphine, and germane and filter them out to avoid worker exposure. However, this produces significant quantities of dangerous condensed gas that need specialized treatment.\nVOCs: Volatile organic compounds like xylene, acetone, and methanol are used extensively as photoresist solvents and are evaporated as emissions during baking, etching, and stripping. VOCs pose toxicity issues and require scrubbing systems to prevent release.\nSpent acids: Strong acids such as sulfuric acid, hydrofluoric acid, and nitric acid get depleted in cleaning and etching steps, transforming into a corrosive, toxic soup that can dangerously react, releasing heat and fumes if mixed.\nSludge: Water treatment of discharged effluent contains concentrated heavy metals, acid residues, and chemical contaminants. Filter press systems separate this hazardous sludge.\nFilter cake: Gaseous filtration systems generate multi-ton sticky cakes of dangerous absorbed compounds requiring containment.\n\nWithout proper handling procedures, storage tanks, packaging materials, and secondary containment, improper disposal of any of these waste streams can lead to dangerous spills, explosions, and environmental releases. The massive volumes mean even well-run fabs produce tons of hazardous waste year after year, requiring extensive treatment.\n\n\n16.5.5 Biodiversity Impacts\n\nHabitat Disruption and Fragmentation\nSemiconductor fabs require large, contiguous land areas to accommodate cleanrooms, support facilities, chemical storage, waste treatment, and ancillary infrastructure. Developing these vast built-up spaces inevitably dismantles existing habitats, damaging sensitive biomes that may have taken decades to develop. For example, constructing a new fabrication module may level local forest ecosystems that species, like spotted owls and elk, rely upon for survival. The outright removal of such habitats severely threatens wildlife populations dependent on those lands.\nFurthermore, pipelines, water channels, air and waste exhaust systems, access roads, transmission towers, and other support infrastructure fragment the remaining undisturbed habitats. Animals moving daily for food, water, and spawning can find their migration patterns blocked by these physical human barriers that bisect previously natural corridors.\n\n\nAquatic Life Disturbances\nWith semiconductor fabs consuming millions of gallons of ultra-pure water daily, accessing and discharging such volumes risks altering the suitability of nearby aquatic environments housing fish, water plants, amphibians, and other species. If the fab is tapping groundwater tables as its primary supply source, overdrawing at unsustainable rates can deplete lakes or lead to stream drying as water levels drop (Davies 2011).\n\nDavies, Emma. 2011. «Endangered elements: Critical thinking». https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\nLeRoy Poff, N, MM Brinson, e JW Day. 2002. «Aquatic ecosystems & Global climate change». Pew Center on Global Climate Change.\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, e Samuel B. Fey. 2019. «Fish die-offs are concurrent with thermal extremes in north temperate lakes». Nat. Clim. Change 9 (8): 637–41. https://doi.org/10.1038/s41558-019-0520-y.\nAlso, discharging wastewater at higher temperatures to cool fabrication equipment can shift downstream river conditions through thermal pollution. Temperature changes beyond thresholds that native species evolved for can disrupt reproductive cycles. Warmer water also holds less dissolved oxygen, critical to supporting aquatic plant and animal life (LeRoy Poff, Brinson, e Day 2002). Combined with traces of residual contaminants that escape filtration systems, the discharged water can cumulatively transform environments to be far less habitable for sensitive organisms (Till et al. 2019).\n\n\nAir and Chemical Emissions\nWhile modern semiconductor fabs aim to contain air and chemical discharges through extensive filtration systems, some levels of emissions often persist, raising risks for nearby flora and fauna. Air pollutants can carry downwind, including volatile organic compounds (VOCs), nitrogen oxide compounds (NOx), particulate matter from fab operational exhausts, and power plant fuel emissions.\nAs contaminants permeate local soils and water sources, wildlife ingesting affected food and water ingest toxic substances, which research shows can hamper cell function, reproduction rates, and longevity–slowly poisoning ecosystems (Hsu et al. 2016).\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting Chan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, e Yu-Min Tzou. 2016. «Accumulation of heavy metals and trace elements in fluvial sediments received effluents from traditional and semiconductor industries». Scientific Reports 6 (1): 34250. https://doi.org/10.1038/srep34250.\nLikewise, accidental chemical spills and improper waste handling, which release acids, BODs, and heavy metals into soils, can dramatically affect retention and leeching capabilities. Flora, such as vulnerable native orchids adapted to nutrient-poor substrates, can experience die-offs when contacted by foreign runoff chemicals that alter soil pH and permeability. One analysis found that a single 500-gallon nitric acid spill led to the regional extinction of a rare moss species in the year following when the acidic effluent reached nearby forest habitats. Such contamination events set off chain reactions across the interconnected web of life. Thus, strict protocols are essential to avoid hazardous discharge and runoff.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#life-cycle-analysis",
    "href": "contents/sustainable_ai/sustainable_ai.html#life-cycle-analysis",
    "title": "16  Sustainable AI",
    "section": "16.6 Life Cycle Analysis",
    "text": "16.6 Life Cycle Analysis\nUnderstanding the holistic environmental impact of AI systems requires a comprehensive approach that considers the entire life cycle of these technologies. Life Cycle Analysis (LCA) refers to a methodological framework used to quantify the environmental impacts across all stages in a product or system’s lifespan, from raw material extraction to end-of-life disposal. Applying LCA to AI systems can help identify priority areas to target for reducing overall environmental footprints.\n\n16.6.1 Stages of an AI System’s Life Cycle\nThe life cycle of an AI system can be divided into four key phases:\n\nDesign Phase: This includes the energy and resources used in researching and developing AI technologies. It encompasses the computational resources used for algorithm development and testing contributing to carbon emissions.\nManufacture Phase: This stage involves producing hardware components such as graphics cards, processors, and other computing devices necessary for running AI algorithms. Manufacturing these components often involves significant energy for material extraction, processing, and greenhouse gas emissions.\nUse Phase: The next most energy-intensive phase involves the operational use of AI systems. It includes the electricity consumed in data centers for training and running neural networks and powering end-user applications. This is arguably one of the most carbon-intensive stages.\nDisposal Phase: This final stage covers the end-of-life aspects of AI systems, including the recycling and disposal of electronic waste generated from outdated or non-functional hardware past their usable lifespan.\n\n\n\n16.6.2 Environmental Impact at Each Stage\nDesign and Manufacturing\nThe environmental impact during these beginning-of-life phases includes emissions from energy use and resource depletion from extracting materials for hardware production. At the heart of AI hardware are semiconductors, primarily silicon, used to make the integrated circuits in processors and memory chips. This hardware manufacturing relies on metals like copper for wiring, aluminum for casings, and various plastics and composites for other components. It also uses rare earth metals and specialized alloys- elements like neodymium, terbium, and yttrium- used in small but vital quantities. For example, the creation of GPUs relies on copper and aluminum. At the same time, chips use rare earth metals, which is the mining process that can generate substantial carbon emissions and ecosystem damage.\nUse Phase\nAI computes the majority of emissions in the lifecycle due to continuous high-power consumption, especially for training and running models. This includes direct and indirect emissions from electricity usage and nonrenewable grid energy generation. Studies estimate training complex models can have a carbon footprint comparable to the lifetime emissions of up to five cars.\nDisposal Phase\nThe disposal stage impacts include air and water pollution from toxic materials in devices, challenges associated with complex electronics recycling, and contamination when improperly handled. Harmful compounds from burned e-waste are released into the atmosphere. At the same time, landfill leakage of lead, mercury, and other materials poses risks of soil and groundwater contamination if not properly controlled. Implementing effective electronics recycling is crucial.\n\n\n\n\n\n\nEsercizio 16.2: Tracking ML Emissions\n\n\n\n\n\nIn this exercise, you’ll explore the environmental impact of training machine learning models. We’ll use CodeCarbon to track emissions, learn about Life Cycle Analysis (LCA) to understand AI’s carbon footprint, and explore strategies to make your ML model development more environmentally friendly. By the end, you’ll be equipped to track the carbon emissions of your models and start implementing greener practices in your projects.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#challenges-in-lca",
    "href": "contents/sustainable_ai/sustainable_ai.html#challenges-in-lca",
    "title": "16  Sustainable AI",
    "section": "16.7 Challenges in LCA",
    "text": "16.7 Challenges in LCA\n\n16.7.1 Lack of Consistency and Standards\nOne major challenge facing life cycle analysis (LCA) for AI systems is the need for consistent methodological standards and frameworks. Unlike product categories like building materials, which have developed international standards for LCA through ISO 14040, there are no firmly established guidelines for analyzing the environmental footprint of complex information technology like AI.\nThis absence of uniformity means researchers make differing assumptions and varying methodological choices. For example, a 2021 study from the University of Massachusetts Amherst (Strubell, Ganesh, e McCallum 2019) analyzed the life cycle emissions of several natural language processing models but only considered computational resource usage for training and omitted hardware manufacturing impacts. A more comprehensive 2020 study from Stanford University researchers included emissions estimates from producing relevant servers, processors, and other components, following an ISO-aligned LCA standard for computer hardware. However, these diverging choices in system boundaries and accounting approaches reduce robustness and prevent apples-to-apples comparisons of results.\nStandardized frameworks and protocols tailored to AI systems’ unique aspects and rapid update cycles would provide more coherence. This could equip researchers and developers to understand environmental hotspots, compare technology options, and accurately track progress on sustainability initiatives across the AI field. Industry groups and international standards bodies like the IEEE or ACM should prioritize addressing this methodological gap.\n\n\n16.7.2 Data Gaps\nAnother key challenge for comprehensive life cycle assessment of AI systems is substantial data gaps, especially regarding upstream supply chain impacts and downstream electronic waste flows. Most existing studies focus narrowly on the learner or usage phase emissions from computational power demands, which misses a significant portion of lifetime emissions (Gupta et al. 2022).\nFor example, little public data from companies exists quantifying energy use and emissions from manufacturing the specialized hardware components that enable AI–including high-end GPUs, ASIC chips, solid-state drives, and more. Researchers often rely on secondary sources or generic industry averages to approximate production impacts. Similarly, on average, there is limited transparency into downstream fate once AI systems are discarded after 4-5 years of usable lifespans.\nWhile electronic waste generation levels can be estimated, specifics on hazardous material leakage, recycling rates, and disposal methods for the complex components are hugely uncertain without better corporate documentation or regulatory reporting requirements.\nThe need for fine-grained data on computational resource consumption for training different model types makes reliable per-parameter or per-query emissions calculations difficult even for the usage phase. Attempts to create lifecycle inventories estimating average energy needs for key AI tasks exist (Henderson et al. 2020; Anthony, Kanding, e Selvan 2020), but variability across hardware setups, algorithms, and input data uncertainty remains extremely high. Furthermore, real-time carbon intensity data, critical in accurately tracking operational carbon footprint, must be improved in many geographic locations, rendering existing tools for operational carbon emission mere approximations based on annual average carbon intensity values.\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, e Joelle Pineau. 2020. «Towards the systematic reporting of the energy and carbon footprints of machine learning». The Journal of Machine Learning Research 21 (1): 10039–81.\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, e Raghavendra Selvan. 2020. ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems.\nThe challenge is that tools like CodeCarbon and ML \\(\\textrm{CO}_2\\) but these are ad hoc approaches at best. Bridging the real data gaps with more rigorous corporate sustainability disclosures and mandated environmental impact reporting will be key for AI’s overall climatic impacts to be understood and managed.\n\n\n16.7.3 Rapid Pace of Evolution\nThe extremely quick evolution of AI systems poses additional challenges in keeping life cycle assessments up-to-date and accounting for the latest hardware and software advancements. The core algorithms, specialized chips, frameworks, and technical infrastructure underpinning AI have all been advancing exceptionally fast, with new developments rapidly rendering prior systems obsolete.\nFor example, in deep learning, novel neural network architectures that achieve significantly better performance on key benchmarks or new optimized hardware like Google’s TPU chips can completely change an “average” model in less than a year. These swift shifts quickly make one-off LCA studies outdated for accurately tracking emissions from designing, running, or disposing of the latest AI.\nHowever, the resources and access required to update LCAs continuously need to be improved. Frequently re-doing labor—and data-intensive life cycle inventories and impact modeling to stay current with AI’s state-of-the-art is likely infeasible for many researchers and organizations. However, updated analyses could notice environmental hotspots as algorithms and silicon chips continue rapidly evolving.\nThis presents difficulty in balancing dynamic precision through continuous assessment with pragmatic constraints. Some researchers have proposed simplified proxy metrics like tracking hardware generations over time or using representative benchmarks as an oscillating set of goalposts for relative comparisons, though granularity may be sacrificed. Overall, the challenge of rapid change will require innovative methodological solutions to prevent underestimating AI’s evolving environmental burdens.\n\n\n16.7.4 Supply Chain Complexity\nFinally, the complex and often opaque supply chains associated with producing the wide array of specialized hardware components that enable AI pose challenges for comprehensive life cycle modeling. State-of-the-art AI relies on cutting-edge advancements in processing chips, graphics cards, data storage, networking equipment, and more. However, tracking emissions and resource use across the tiered networks of globalized suppliers for all these components is extremely difficult.\nFor example, NVIDIA graphics processing units dominate much of the AI computing hardware, but the company relies on several discrete suppliers across Asia and beyond to produce GPUs. Many firms at each supplier tier choose to keep facility-level environmental data private, which could fully enable robust LCAs. Gaining end-to-end transparency down multiple levels of suppliers across disparate geographies with varying disclosure protocols and regulations poses barriers despite being crucial for complete boundary setting. This becomes even more complex when attempting to model emerging hardware accelerators like tensor processing units (TPUs), whose production networks still need to be made public.\nWithout tech giants’ willingness to require and consolidate environmental impact data disclosure from across their global electronics supply chains, considerable uncertainty will remain around quantifying the full lifecycle footprint of AI hardware enablement. More supply chain visibility coupled with standardized sustainability reporting frameworks specifically addressing AI’s complex inputs hold promise for enriching LCAs and prioritizing environmental impact reductions.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#sustainable-design-and-development",
    "href": "contents/sustainable_ai/sustainable_ai.html#sustainable-design-and-development",
    "title": "16  Sustainable AI",
    "section": "16.8 Sustainable Design and Development",
    "text": "16.8 Sustainable Design and Development\n\n16.8.1 Sustainability Principles\nAs the impact of AI on the environment becomes increasingly evident, the focus on sustainable design and development in AI is gaining prominence. This involves incorporating sustainability principles into AI design, developing energy-efficient models, and integrating these considerations throughout the AI development pipeline. There is a growing need to consider its sustainability implications and develop principles to guide responsible innovation. Below is a core set of principles. The principles flow from the conceptual foundation to practical execution to supporting implementation factors; the principles provide a full cycle perspective on embedding sustainability in AI design and development.\nLifecycle Thinking: Encouraging designers to consider the entire lifecycle of AI systems, from data collection and preprocessing to model development, training, deployment, and monitoring. The goal is to ensure sustainability is considered at each stage. This includes using energy-efficient hardware, prioritizing renewable energy sources, and planning to reuse or recycle retired models.\nFuture Proofing: Designing AI systems anticipating future needs and changes can improve sustainability. This may involve making models adaptable via transfer learning and modular architectures. It also includes planning capacity for projected increases in operational scale and data volumes.\nEfficiency and Minimalism: This principle focuses on creating AI models that achieve desired results with the least possible resource use. It involves simplifying models and algorithms to reduce computational requirements. Specific techniques include pruning redundant parameters, quantizing and compressing models, and designing efficient model architectures, such as those discussed in the Optimizations chapter.\nLifecycle Assessment (LCA) Integration: Analyzing environmental impacts throughout the development and deployment of lifecycles highlights unsustainable practices early on. Teams can then make adjustments instead of discovering issues late when they are more difficult to address. Integrating this analysis into the standard design flow avoids creating legacy sustainability problems.\nIncentive Alignment: Economic and policy incentives should promote and reward sustainable AI development. These may include government grants, corporate initiatives, industry standards, and academic mandates for sustainability. Aligned incentives enable sustainability to become embedded in AI culture.\nSustainability Metrics and Goals: It is important to establish clearly defined Metrics that measure sustainability factors like carbon usage and energy efficiency. Establishing clear targets for these metrics provides concrete guidelines for teams to develop responsible AI systems. Tracking performance on metrics over time shows progress towards set sustainability goals.\nFairness, Transparency, and Accountability: Sustainable AI systems should be fair, transparent, and accountable. Models should be unbiased, with transparent development processes and mechanisms for auditing and redressing issues. This builds public trust and enables the identification of unsustainable practices.\nCross-disciplinary Collaboration: AI researchers teaming up with environmental scientists and engineers can lead to innovative systems that are high-performing yet environmentally friendly. Combining expertise from different fields from the start of projects enables sustainable thinking to be incorporated into the AI design process.\nEducation and Awareness: Workshops, training programs, and course curricula that cover AI sustainability raise awareness among the next generation of practitioners. This equips students with the knowledge to develop AI that consciously minimizes negative societal and environmental impacts. Instilling these values from the start shapes tomorrow’s professionals and company cultures.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#green-ai-infrastructure",
    "href": "contents/sustainable_ai/sustainable_ai.html#green-ai-infrastructure",
    "title": "16  Sustainable AI",
    "section": "16.9 Green AI Infrastructure",
    "text": "16.9 Green AI Infrastructure\nGreen AI represents a transformative approach to AI that incorporates environmental sustainability as a fundamental principle across the AI system design and lifecycle (R. Schwartz et al. 2020). This shift is driven by growing awareness of AI technologies’ significant carbon footprint and ecological impact, especially the compute-intensive process of training complex ML models.\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, e Oren Etzioni. 2020. «Green AI». Commun. ACM 63 (12): 54–63. https://doi.org/10.1145/3381831.\nThe essence of Green AI lies in its commitment to align AI advancement with sustainability goals around energy efficiency, renewable energy usage, and waste reduction. The introduction of Green AI ideals reflects maturing responsibility across the tech industry towards environmental stewardship and ethical technology practices. It moves beyond technical optimizations toward holistic life cycle assessment on how AI systems affect sustainability metrics. Setting new bars for ecologically conscious AI paves the way for the harmonious coexistence of technological progress and planetary health.\n\n16.9.1 Energy Efficient AI Systems\nEnergy efficiency in AI systems is a cornerstone of Green AI, aiming to reduce the energy demands traditionally associated with AI development and operations. This shift towards energy-conscious AI practices is vital in addressing the environmental concerns raised by the rapidly expanding field of AI. By focusing on energy efficiency, AI systems can become more sustainable, lessening their environmental impact and paving the way for more responsible AI use.\nAs we discussed earlier, the training and operation of AI models, especially large-scale ones, are known for their high energy consumption, which stems from compute-intensive model architecture and reliance on vast amounts of training data. For example, it is estimated that training a large state-of-the-art neural network model can have a carbon footprint of 284 tonnes—equivalent to the lifetime emissions of 5 cars (Strubell, Ganesh, e McCallum 2019).\n\nStrubell, Emma, Ananya Ganesh, e Andrew McCallum. 2019. «Energy and Policy Considerations for Deep Learning in NLP». In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3645–50. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\nTo tackle the massive energy demands, researchers and developers are actively exploring methods to optimize AI systems for better energy efficiency while maintaining model accuracy and performance. This includes techniques like the ones we have discussed in the model optimizations, efficient AI, and hardware acceleration chapters:\n\nKnowledge distillation to transfer knowledge from large AI models to miniature versions\nQuantization and pruning approaches that reduce computational and space complexities\nLow-precision numerics–lowering mathematical precision without impacting model quality\nSpecialized hardware like TPUs, neuromorphic chips tuned explicitly for efficient AI processing\n\nOne example is Intel’s work on Q8BERT—quantizing the BERT language model with 8-bit integers, leading to a 4x reduction in model size with minimal accuracy loss (Zafrir et al. 2019). The push for energy-efficient AI is not just a technical endeavor–it has tangible real-world implications. More performant systems lower AI’s operational costs and carbon footprint, making it accessible for widespread deployment on mobile and edge devices. It also paves the path toward the democratization of AI and mitigates unfair biases that can emerge from uneven access to computing resources across regions and communities. Pursuing energy-efficient AI is thus crucial for creating an equitable and sustainable future with AI.\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, e Moshe Wasserblat. 2019. «Q8BERT: Quantized 8Bit BERT». In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS), 36–39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\n16.9.2 Sustainable AI Infrastructure\nSustainable AI infrastructure includes the physical and technological frameworks that support AI systems, focusing on environmental sustainability. This involves designing and operating AI infrastructure to minimize ecological impact, conserve resources, and reduce carbon emissions. The goal is to create a sustainable ecosystem for AI that aligns with broader environmental objectives.\nGreen data centers are central to sustainable AI infrastructure, optimized for energy efficiency, and often powered by renewable energy sources. These data centers employ advanced cooling technologies (Ebrahimi, Jones, e Fleischer 2014), energy-efficient server designs (Uddin e Rahman 2012), and smart management systems (Buyya, Beloglazov, e Abawajy 2010) to reduce power consumption. The shift towards green computing infrastructure also involves adopting energy-efficient hardware, like AI-optimized processors that deliver high performance with lower energy requirements, which we discussed in the AI. Acceleration chapter. These efforts collectively reduce the carbon footprint of running large-scale AI operations.\n\nEbrahimi, Khosrow, Gerard F. Jones, e Amy S. Fleischer. 2014. «A review of data center cooling technology, operating conditions and the corresponding low-grade waste heat recovery opportunities». Renewable Sustainable Energy Rev. 31 (marzo): 622–38. https://doi.org/10.1016/j.rser.2013.12.007.\n\nUddin, Mueen, e Azizah Abdul Rahman. 2012. «Energy efficiency and low carbon enabler green IT framework for data centers considering green metrics». Renewable Sustainable Energy Rev. 16 (6): 4078–94. https://doi.org/10.1016/j.rser.2012.03.014.\n\nBuyya, Rajkumar, Anton Beloglazov, e Jemal Abawajy. 2010. «Energy-Efficient Management of Data Center Resources for Cloud Computing: A Vision, Architectural Elements, and Open Challenges». https://arxiv.org/abs/1006.0308.\n\nChua, L. 1971. «Memristor-The missing circuit element». #IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\nIntegrating renewable energy sources, such as solar, wind, and hydroelectric power, into AI infrastructure is important for environmental sustainability (Chua 1971). Many tech companies and research institutions are investing in renewable energy projects to power their data centers. This not only helps in making AI operations carbon-neutral but also promotes the wider adoption of clean energy. Using renewable energy sources clearly shows commitment to environmental responsibility in the AI industry.\nSustainability in AI also extends to the materials and hardware used in creating AI systems. This involves choosing environmentally friendly materials, adopting recycling practices, and ensuring responsible electronic waste disposal. Efforts are underway to develop more sustainable hardware components, including energy-efficient chips designed for domain-specific tasks (such as AI accelerators) and environmentally friendly materials in device manufacturing (Cenci et al. 2021; Irimia-Vladu 2014). The lifecycle of these components is also a focus, with initiatives aimed at extending the lifespan of hardware and promoting recycling and reuse.\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula Cristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, e Pablo R. Dias. 2021. «Eco-Friendly ElectronicsA Comprehensive Review». Adv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\nIrimia-Vladu, Mihai. 2014. «“Green” electronics: Biodegradable and biocompatible materials and devices for sustainable future». Chem. Soc. Rev. 43 (2): 588–610. https://doi.org/10.1039/c3cs60235d.\nWhile strides are being made in sustainable AI infrastructure, challenges remain, such as the high costs of green technology and the need for global standards in sustainable practices. Future directions include more widespread adoption of green energy, further innovations in energy-efficient hardware, and international collaboration on sustainable AI policies. Pursuing sustainable AI infrastructure is not just a technical endeavor but a holistic approach that encompasses environmental, economic, and social aspects, ensuring that AI advances harmoniously with our planet’s health.\n\n\n16.9.3 Frameworks and Tools\nAccess to the right frameworks and tools is essential to effectively implementing green AI practices. These resources are designed to assist developers and researchers in creating more energy-efficient and environmentally friendly AI systems. They range from software libraries optimized for low-power consumption to platforms that facilitate the development of sustainable AI applications.\nSeveral software libraries and development environments are specifically tailored for Green AI. These tools often include features for optimizing AI models to reduce their computational load and, consequently, their energy consumption. For example, libraries in PyTorch and TensorFlow that support model pruning, quantization, and efficient neural network architectures enable developers to build AI systems that require less processing power and energy. Additionally, open-source communities like the Green Carbon Foundation are creating a centralized carbon intensity metric and building software for carbon-aware computing.\nEnergy monitoring tools are crucial for Green AI, as they allow developers to measure and analyze the energy consumption of their AI systems. By providing detailed insights into where and how energy is being used, these tools enable developers to make informed decisions about optimizing their models for better energy efficiency. This can involve adjustments in algorithm design, hardware selection, cloud computing software selection, or operational parameters. Figura 16.7 is a screenshot of an energy consumption dashboard provided by Microsoft’s cloud services platform.\n\n\n\n\n\n\nFigura 16.7: Microsoft Azure energy consumption dashboard. Source: Will Buchanan.\n\n\n\nWith the increasing integration of renewable energy sources in AI operations, frameworks facilitating this process are becoming more important. These frameworks help manage the energy supply from renewable sources like solar or wind power, ensuring that AI systems can operate efficiently with fluctuating energy inputs.\nBeyond energy efficiency, sustainability assessment tools help evaluate the broader environmental impact of AI systems. These tools can analyze factors like the carbon footprint of AI operations, the lifecycle impact of hardware components (Gupta et al. 2022), and the overall sustainability of AI projects (Prakash, Callahan, et al. 2023).\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, e Carole-Jean Wu. 2022. «Act: designing sustainable computer systems with an architectural carbon modeling tool». In Proceedings of the 49th Annual International Symposium on Computer Architecture, 784–99. ACM. https://doi.org/10.1145/3470496.3527408.\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, e Vijay Janapa Reddi. 2023. «CFU Playground: Full-stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration on FPGAs». In 2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). Vol. abs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\nThe availability and ongoing development of Green AI frameworks and tools are critical for advancing sustainable AI practices. By providing the necessary resources for developers and researchers, these tools facilitate the creation of more environmentally friendly AI systems and encourage a broader shift towards sustainability in the tech community. As Green AI continues to evolve, these frameworks and tools will play a vital role in shaping a more sustainable future for AI.\n\n\n16.9.4 Benchmarks and Leaderboards\nBenchmarks and leaderboards are important for driving progress in Green AI, as they provide standardized ways to measure and compare different methods. Well-designed benchmarks that capture relevant metrics around energy efficiency, carbon emissions, and other sustainability factors enable the community to track advancements fairly and meaningfully.\nExtensive benchmarks exist for tracking AI model performance, such as those extensively discussed in the Benchmarking chapter. Still, a clear and pressing need exists for additional standardized benchmarks focused on sustainability metrics like energy efficiency, carbon emissions, and overall ecological impact. Understanding the environmental costs of AI currently needs to be improved by a lack of transparency and standardized measurement around these factors.\nEmerging efforts such as the ML.ENERGY Leaderboard, which provides performance and energy consumption benchmarking results for large language models (LLMs) text generation, assists in enhancing the understanding of the energy cost of GenAI deployment.\nAs with any benchmark, Green AI benchmarks must represent realistic usage scenarios and workloads. Benchmarks that focus narrowly on easily gamed metrics may lead to short-term gains but fail to reflect actual production environments where more holistic efficiency and sustainability measures are needed. The community should continue expanding benchmarks to cover diverse use cases.\nWider adoption of common benchmark suites by industry players will accelerate innovation in Green AI by allowing easier comparison of techniques across organizations. Shared benchmarks lower the barrier to demonstrating the sustainability benefits of new tools and best practices. However, when designing industry-wide benchmarks, care must be taken around issues like intellectual property, privacy, and commercial sensitivity. Initiatives to develop open reference datasets for Green AI evaluation may help drive broader participation.\nAs methods and infrastructure for Green AI continue maturing, the community must revisit benchmark design to ensure existing suites capture new techniques and scenarios well. Tracking the evolving landscape through regular benchmark updates and reviews will be important to maintain representative comparisons over time. Community efforts for benchmark curation can enable sustainable benchmark suites that stand the test of time. Comprehensive benchmark suites owned by research communities or neutral third parties like MLCommons may encourage wider participation and standardization.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#case-study-google-4ms",
    "href": "contents/sustainable_ai/sustainable_ai.html#case-study-google-4ms",
    "title": "16  Sustainable AI",
    "section": "16.10 Case Study: Google’s 4Ms",
    "text": "16.10 Case Study: Google’s 4Ms\nOver the past decade, AI has rapidly moved from academic research to large-scale production systems powering numerous Google products and services. As AI models and workloads have grown exponentially in size and computational demands, concerns have emerged about their energy consumption and carbon footprint. Some researchers predicted runaway growth in ML’s energy appetite that could outweigh efficiencies gained from improved algorithms and hardware (Thompson et al. 2021).\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, e Gabriel F. Manso. 2021. «Deep Learning’s Diminishing Returns: The Cost of Improvement is Becoming Unsustainable». IEEE Spectr. 58 (10): 50–55. https://doi.org/10.1109/mspec.2021.9563954.\nHowever, Google’s production data reveals a different story—AI represents a steady 10-15% of total company energy usage from 2019 to 2021. This case study analyzes how Google applied a systematic approach leveraging four best practices—what they term the “4 Ms” of model efficiency, machine optimization, mechanization through cloud computing, and mapping to green locations—to bend the curve on emissions from AI workloads.\nThe scale of Google’s AI usage makes it an ideal case study. In 2021 alone, the company trained models like the 1.2 trillion-parameter GLam model. Analyzing how the application of AI has been paired with rapid efficiency gains in this environment helps us by providing a logical blueprint for the broader AI field to follow.\nBy transparently publishing detailed energy usage statistics, adopting rates of carbon-free clouds and renewables purchases, and more, alongside its technical innovations, Google has enabled outside researchers to measure progress accurately. Their study in the ACM CACM (Patterson et al. 2022) highlights how the company’s multipronged approach shows that runaway AI energy consumption predictions can be overcome by focusing engineering efforts on sustainable development patterns. The pace of improvements also suggests ML’s efficiency gains are just starting.\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, e Jeff Dean. 2022. «The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink». Computer 55 (7): 18–28. https://doi.org/10.1109/mc.2022.3148714.\n\n16.10.1 Google’s 4M Best Practices\nTo curb emissions from their rapidly expanding AI workloads, Google engineers systematically identified four best practice areas–termed the “4 Ms”–where optimizations could compound to reduce the carbon footprint of ML:\n\nModel - Selecting efficient AI model architectures can reduce computation by 5-10X with no loss in model quality. Google has extensively researched developing sparse models and neural architecture search to create more efficient models like the Evolved Transformer and Primer.\nMachine—Using hardware optimized for AI over general-purpose systems improves performance per watt by 2-5X. Google’s Tensor Processing Units (TPUs) led to 5-13X better carbon efficiency versus GPUs not optimized for ML.\nMechanization—By leveraging cloud computing systems tailored for high utilization over conventional on-premise data centers, energy costs are reduced by 1.4-2X. Google cites its data center’s power usage effectiveness as outpacing industry averages.\nMap - Choosing data center locations with low-carbon electricity reduces gross emissions by another 5-10X. Google provides real-time maps highlighting the percentage of renewable energy used by its facilities.\n\nTogether, these practices created drastic compound efficiency gains. For example, optimizing the Transformer AI model on TPUs in a sustainable data center location cut energy use by 83. It lowered \\(\\textrm{CO}_2\\) emissions by a factor of 747.\n\n\n16.10.2 Significant Results\nDespite exponential growth in AI adoption across products and services, Google’s efforts to improve the carbon efficiency of ML have produced measurable gains, helping to restrain overall energy appetite. One key data point highlighting this progress is that AI workloads have remained a steady 10% to 15% of total company energy use from 2019 to 2021. As AI became integral to more Google offerings, overall compute cycles dedicated to AI grew substantially. However, efficiencies in algorithms, specialized hardware, data center design, and flexible geography allowed sustainability to keep pace—with AI representing just a fraction of total data center electricity over years of expansion.\nOther case studies underscore how an engineering focus on sustainable AI development patterns enabled rapid quality improvements in lockstep with environmental gains. For example, the natural language processing model GPT-3 was viewed as state-of-the-art in mid-2020. Yet its successor GLaM improved accuracy while cutting training compute needs and using cleaner data center energy–cutting CO2 emissions by a factor of 14 in just 18 months of model evolution.\nSimilarly, Google found past published speculation missing the mark on ML’s energy appetite by factors of 100 to 100,000X due to a lack of real-world metrics. By transparently tracking optimization impact, Google hoped to motivate efficiency while preventing overestimated extrapolations about ML’s environmental toll.\nThese data-driven case studies show how companies like Google are steering AI advancements toward sustainable trajectories and improving efficiency to outpace adoption growth. With further efforts around lifecycle analysis, inference optimization, and renewable expansion, companies can aim to accelerate progress, giving evidence that ML’s clean potential is only just being unlocked by current gains.\n\n\n16.10.3 Further Improvements\nWhile Google has made measurable progress in restraining the carbon footprint of its AI operations, the company recognizes further efficiency gains will be vital for responsible innovation given the technology’s ongoing expansion.\nOne area of focus is showing how advances are often incorrectly viewed as increasing unsustainable computing—like neural architecture search (NAS) to find optimized models— spur downstream savings, outweighing their upfront costs. Despite expending more energy on model discovery rather than hand-engineering, NAS cuts lifetime emissions by producing efficient designs callable across countless applications.\nAdditionally, the analysis reveals that focusing sustainability efforts on data center and server-side optimization makes sense, given the dominant energy draw versus consumer devices. Though Google aims to shrink inference impacts across processors like mobile phones, priority rests on improving training cycles and data center renewables procurement for maximal effect.\nTo that end, Google’s progress in pooling computing inefficiently designed cloud facilities highlights the value of scale and centralization. As more workloads shift away from inefficient on-premise servers, internet giants’ prioritization of renewable energy—with Google and Facebook matched 100% by renewables since 2017 and 2020, respectively—unlocks compounding emissions cuts.\nTogether, these efforts emphasize that while no resting on laurels is possible, Google’s multipronged approach shows that AI efficiency improvements are only accelerating. Cross-domain initiatives around lifecycle assessment, carbon-conscious development patterns, transparency, and matching rising AI demand with clean electricity supply pave a path toward bending the curve further as adoption grows. The company’s results compel the broader field towards replicating these integrated sustainability pursuits.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#embedded-ai-internet-of-trash",
    "href": "contents/sustainable_ai/sustainable_ai.html#embedded-ai-internet-of-trash",
    "title": "16  Sustainable AI",
    "section": "16.11 Embedded AI - Internet of Trash",
    "text": "16.11 Embedded AI - Internet of Trash\nWhile much attention has focused on making the immense data centers powering AI more sustainable, an equally pressing concern is the movement of AI capabilities into smart edge devices and endpoints. Edge/embedded AI allows near real-time responsiveness without connectivity dependencies. It also reduces transmission bandwidth needs. However, the increase of tiny devices leads to other risks.\nTiny computers, microcontrollers, and custom ASICs powering edge intelligence face size, cost, and power limitations that rule out high-end GPUs used in data centers. Instead, they require optimized algorithms and extremely compact, energy-efficient circuitry to run smoothly. However, engineering for these microscopic form factors opens up risks around planned obsolescence, disposability, and waste. Figura 16.8 shows that the number of IoT devices is projected to reach 30 billion connected devices by 2030.\n\n\n\n\n\n\nFigura 16.8: Number of Internet of Things (IoT) connected devices worldwide from 2019 to 2023. Source: Statista.\n\n\n\nEnd-of-life handling of internet-connected gadgets embedded with sensors and AI remains an often overlooked issue during design. However, these products permeate consumer goods, vehicles, public infrastructure, industrial equipment, and more.\n\nE-waste\nElectronic waste, or e-waste, refers to discarded electrical equipment and components that enter the waste stream. This includes devices that have to be plugged in, have a battery, or electrical circuitry. With the rising adoption of internet-connected smart devices and sensors, e-waste volumes rapidly increase yearly. These proliferating gadgets contain toxic heavy metals like lead, mercury, and cadmium that become environmental and health hazards when improperly disposed of.\nThe amount of electronic waste being produced is growing at an alarming rate. Today, we already produce 50 million tons per year. By 2030, that figure is projected to jump to a staggering 75 million tons as consumer electronics consumption continues to accelerate. Global e-waste production will reach 120 million tonnes annually by 2050 (Un e Forum 2019). The soaring production and short lifecycles of our gadgets fuel this crisis, from smartphones and tablets to internet-connected devices and home appliances.\nDeveloping nations are being hit the hardest as they need more infrastructure to process obsolete electronics safely. In 2019, formal e-waste recycling rates in poorer countries ranged from 13% to 23%. The remainder ends up illegally dumped, burned, or crudely dismantled, releasing toxic materials into the environment and harming workers and local communities. Clearly, more needs to be done to build global capacity for ethical and sustainable e-waste management, or we risk irreversible damage.\nThe danger is that crude handling of electronics to strip valuables exposes marginalized workers and communities to noxious burnt plastics/metals. Lead poisoning poses especially high risks to child development if ingested or inhaled. Overall, only about 20% of e-waste produced was collected using environmentally sound methods, according to UN estimates (Un e Forum 2019). So solutions for responsible lifecycle management are urgently required to contain the unsafe disposal as volume soars higher.\n\nUn, e World Economic Forum. 2019. A New Circular Vision for Electronics, Time for a Global Reboot. PACE - Platform for Accelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\nDisposable Electronics\nThe rapidly falling costs of microcontrollers, tiny rechargeable batteries, and compact communication hardware have enabled the embedding of intelligent sensor systems throughout everyday consumer goods. These internet-of-things (IoT) devices monitor product conditions, user interactions, and environmental factors to enable real-time responsiveness, personalization, and data-driven business decisions in the evolving connected marketplace.\nHowever, these embedded electronics face little oversight or planning around sustainably handling their eventual disposal once the often plastic-encased products are discarded after brief lifetimes. IoT sensors now commonly reside in single-use items like water bottles, food packaging, prescription bottles, and cosmetic containers that overwhelmingly enter landfill waste streams after a few weeks to months of consumer use.\nThe problem accelerates as more manufacturers rush to integrate mobile chips, power sources, Bluetooth modules, and other modern silicon ICs, costing under US$1, into various merchandise without protocols for recycling, replacing batteries, or component reusability. Despite their small individual size, the volumes of these devices and lifetime waste burden loom large. Unlike regulating larger electronics, few policy constraints exist around materials requirements or toxicity in tiny disposable gadgets.\nWhile offering convenience when working, the unsustainable combination of difficult retrievability and limited safe breakdown mechanisms causes disposable connected devices to contribute outsized shares of future e-waste volumes needing urgent attention.\n\n\nPlanned Obsolescence\nPlanned obsolescence refers to the intentional design strategy of manufacturing products with artificially limited lifetimes that quickly become non-functional or outdated. This spurs faster replacement purchase cycles as consumers find devices no longer meet their needs within a few years. However, electronics designed for premature obsolescence contribute to unsustainable e-waste volumes.\nFor example, gluing smartphone batteries and components together hinders repairability compared to modular, accessible assemblies. Rolling out software updates that deliberately slow system performance creates a perception that upgrading devices produced only several years earlier is worth it.\nLikewise, fashionable introductions of new product generations with minor but exclusive feature additions make prior versions rapidly seem dated. These tactics compel buying new gadgets (e.g., iPhones) long before operational endpoints. When multiplied across fast-paced electronics categories, billions of barely worn items are discarded annually.\nPlanned obsolescence thus intensifies resource utilization and waste creation in making products with no intention for long lifetimes. This contradicts sustainability principles around durability, reuse, and material conservation. While stimulating continuous sales and gains for manufacturers in the short term, the strategy externalizes environmental costs and toxins onto communities lacking proper e-waste processing infrastructure.\nPolicy and consumer action are crucial to counter gadget designs that are needlessly disposable by default. Companies should also invest in product stewardship programs supporting responsible reuse and reclamation.\nConsider the real-world example. Apple has faced scrutiny over the years for allegedly engaging in planned obsolescence to encourage customers to buy new iPhone models. The company allegedly designed its phones so that performance degrades over time or existing features become incompatible with new operating systems, which critics argue is meant to spur more rapid upgrade cycles. In 2020, Apple paid a 25 million Euros fine to settle a case in France where regulators found the company guilty of intentionally slowing down older iPhones without clearly informing customers via iOS updates.\nBy failing to be transparent about power management changes that reduced device performance, Apple participated in deceptive activities that reduced product lifespan to drive sales. The company claimed it was done to “smooth out” peaks that could suddenly cause older batteries to shut down. However, this example highlights the legal risks around employing planned obsolescence and not properly disclosing when functionality changes impact device usability over time- even leading brands like Apple can run into trouble if perceived as intentionally shortening product life cycles.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#policy-and-regulatory-considerations",
    "href": "contents/sustainable_ai/sustainable_ai.html#policy-and-regulatory-considerations",
    "title": "16  Sustainable AI",
    "section": "16.12 Policy and Regulatory Considerations",
    "text": "16.12 Policy and Regulatory Considerations\n\n16.12.1 Measurement and Reporting Mandates\nOne policy mechanism that is increasingly relevant for AI systems is measurement and reporting requirements regarding energy consumption and carbon emissions. Mandated metering, auditing, disclosures, and more rigorous methodologies aligned to sustainability metrics can help address information gaps hindering efficiency optimizations.\nSimultaneously, national or regional policies require companies above a certain size to use AI in their products or backend systems to report energy consumption or emissions associated with major AI workloads. Organizations like the Partnership on AI, IEEE, and NIST could help shape standardized methodologies. More complex proposals involve defining consistent ways to measure computational complexity, data center PUE, carbon intensity of energy supply, and efficiencies gained through AI-specific hardware.\nReporting obligations for public sector users procuring AI services—such as through proposed legislation in Europe—could also increase transparency. However, regulators must balance the additional measurement burden such mandates place on organizations against ongoing carbon reductions from ingraining sustainability-conscious development patterns.\nTo be most constructive, any measurement and reporting policies should focus on enabling continuous refinement rather than simplistic restrictions or caps. As AI advancements unfold rapidly, nimble governance guardrails that embed sustainability considerations into normal evaluation metrics can motivate positive change. However, overprescription risks constraining innovation if requirements grow outdated. AI efficiency policy aims to accelerate progress industry-wide by combining flexibility with appropriate transparency guardrails.\n\n\n16.12.2 Restriction Mechanisms\nIn addition to reporting mandates, policymakers have several restriction mechanisms that could directly shape how AI systems are developed and deployed to curb emissions:\nCaps on Computing Emissions: The European Commission’s proposed AI Act takes a horizontal approach that could allow setting economy-wide caps on the volume of computing power available for training AI models. Like emissions trading systems, caps aim to disincentivize extensive computing over sustainability indirectly. However, model quality could be improved to provide more pathways for procuring additional capacity.\nConditioning Access to Public Resources: Some experts have proposed incentives like only allowing access to public datasets or computing power for developing fundamentally efficient models rather than extravagant architectures. For example, the MLCommons benchmarking consortium founded by major tech firms could formally integrate efficiency into its standardized leaderboard metrics—however, conditioned access risks limiting innovation.\nFinancial Mechanisms: Analogous to carbon taxes on polluting industries, fees applied per unit of AI-related compute consumption could discourage unnecessary model scaling while funding efficiency innovations. Tax credits could alternatively reward organizations pioneering more accurate but compact AI techniques. However, financial tools require careful calibration between revenue generation and fairness and not over-penalizing productive uses of AI.\nTechnology Bans: If measurement consistently pinned extreme emissions on specific applications of AI without paths for remediation, outright bans present a tool of last resort for policymakers. However, given AI’s dual use, defining harmful versus beneficial deployments proves complex, necessitating holistic impact assessment before concluding no redeeming value exists. Banning promising technologies risks unintended consequences and requires caution.\n\n\n16.12.3 Government Incentives\nIt is a common practice for governments to provide tax or other incentives to consumers or businesses when contributing to more sustainable technological practices. Such incentives already exist in the US for adopting solar panels or energy-efficient buildings. To the best of our knowledge, no such tax incentives exist for AI-specific development practices yet.\nAnother potential incentive program that is beginning to be explored is using government grants to fund Green AI projects. For example, in Spain, 300 million euros have been allocated to specifically fund projects in AI and sustainability. Government incentives are a promising avenue to encourage sustainable business and consumer behavior practices, but careful thought is required to determine how those incentives will fit into market demands (Cohen, Lobel, e Perakis 2016).\n\nCohen, Maxime C., Ruben Lobel, e Georgia Perakis. 2016. «The Impact of Demand Uncertainty on Consumer Subsidies for Green Technology Adoption». Manage. Sci. 62 (5): 1235–58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\n16.12.4 Self-Regulation\nComplimentary to potential government action, voluntary self-governance mechanisms allow the AI community to pursue sustainability ends without top-down intervention:\nRenewables Commitments: Large AI practitioners like Google, Microsoft, Amazon, and Facebook have pledged to procure enough renewable electricity to match 100% of their energy demands. These commitments unlock compounding emissions cuts as compute scales up. Formalizing such programs incentivizes green data center regions. However, there are critiques on whether these pledges are enough (Monyei e Jenkins 2018).\n\nMonyei, Chukwuka G., e Kirsten E. H. Jenkins. 2018. «Electrons have no identity: Setting right misrepresentations in Google and Apple’s clean energy purchasing». Energy Research &amp; Social Science 46 (dicembre): 48–51. https://doi.org/10.1016/j.erss.2018.06.015.\nInternal Carbon Prices: Some organizations use shadow prices on carbon emissions to represent environmental costs in capital allocation decisions between AI projects. If modeled effectively, theoretical charges on development carbon footprints steer funding toward efficient innovations rather than solely accuracy gains.\nEfficiency Development Checklists: Groups like the AI Sustainability Coalition suggest voluntary checklist templates highlighting model design choices, hardware configurations, and other factors architects can tune per application to restrain emissions. Organizations can drive change by ingraining sustainability as a primary success metric alongside accuracy and cost.\nIndependent Auditing: Even absent public disclosure mandates, firms specializing in technology sustainability audits help AI developers identify waste, create efficiency roadmaps, and benchmark progress via impartial reviews. Structuring such audits into internal governance procedures or the procurement process expands accountability.\n\n\n16.12.5 Global Considerations\nWhile measurement, restrictions, incentives, and self-regulation represent potential policy mechanisms for furthering AI sustainability, fragmentation across national regimes risks unintended consequences. As with other technology policy domains, divergence between regions must be carefully managed.\nFor example, due to regional data privacy concerns, OpenAI barred European users from accessing its viral ChatGPT chatbot. This came after the EU’s proposed AI Act signaled a precautionary approach, allowing the EC to ban certain high-risk AI uses and enforcing transparency rules that create uncertainty for releasing brand new models. However, it would be wise to caution against regulator action as it could inadvertently limit European innovation if regimes with lighter-touch regulation attract more private-sector AI research spending and talent. Finding common ground is key.\nThe OECD principles on AI and the United Nations frameworks underscore universally agreed-upon tenets all national policies should uphold: transparency, accountability, bias mitigation, and more. Constructively embedding sustainability as a core principle for responsible AI within international guidance can motivate unified action without sacrificing flexibility across divergent legal systems. Avoiding race-to-the-bottom dynamics hinges on enlightened multilateral cooperation.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#public-perception-and-engagement",
    "href": "contents/sustainable_ai/sustainable_ai.html#public-perception-and-engagement",
    "title": "16  Sustainable AI",
    "section": "16.13 Public Perception and Engagement",
    "text": "16.13 Public Perception and Engagement\nAs societal attention and policy efforts aimed at environmental sustainability ramp up worldwide, there is growing enthusiasm for leveraging AI to help address ecological challenges. However, public understanding and attitudes toward the role of AI systems in sustainability contexts still need to be clarified and clouded by misconceptions. On the one hand, people hope advanced algorithms can provide new solutions for green energy, responsible consumption, decarbonization pathways, and ecosystem preservation. On the other, fears regarding the risks of uncontrolled AI also seep into the environmental domain and undermine constructive discourse. Furthermore, a lack of public awareness on key issues like transparency in developing sustainability-focused AI tools and potential biases in data or modeling also threaten to limit inclusive participation and degrade public trust.\nTackling complex, interdisciplinary priorities like environmental sustainability requires informed, nuanced public engagement and responsible advances in AI innovation. The path forward demands careful, equitable collaborative efforts between experts in ML, climate science, environmental policy, social science, and communication. Mapping the landscape of public perceptions, identifying pitfalls, and charting strategies to cultivate understandable, accessible, and trustworthy AI systems targeting shared ecological priorities will prove essential to realizing sustainability goals. This complex terrain warrants a deep examination of the sociotechnical dynamics involved.\n\n16.13.1 AI Awareness\nIn May 2022, the Pew Research Center polled 5,101 US adults, finding 60% had heard or read “a little” about AI while 27% heard “a lot”–indicating decent broad recognition, but likely limited comprehension about details or applications. However, among those with some AI familiarity, concerns emerge regarding risks of personal data misuse according to agreed terms. Still, 62% felt AI could ease modern life if applied responsibly. Yet, a specific understanding of sustainability contexts still needs to be improved.\nStudies attempting to categorize online discourse sentiments find a nearly even split between optimism and caution regarding deploying AI for sustainability goals. Factors driving positivity include hopes around better forecasting of ecological shifts using ML models. Negativity arises from a lack of confidence in self-supervised algorithms avoiding unintended consequences due to unpredictable human impacts on complex natural systems during training.\nThe most prevalent public belief remains that while AI does harbor the potential for accelerating solutions on issues like emission reductions and wildlife protections, inadequate safeguarding around data biases, ethical blindspots, and privacy considerations could be more appreciated risks if pursued carelessly, especially at scale. This leads to hesitancy around unconditional support without evidence of deliberate, democratically guided development.\n\n\n16.13.2 Messaging\nOptimistic efforts are highlighting AI’s sustainability promise and emphasize the potential for advanced ML to radically accelerate decarbonization effects from smart grids, personalized carbon tracking apps, automated building efficiency optimizations, and predictive analytics guiding targeted conservation efforts. More comprehensive real-time modeling of complex climate and ecological shifts using self-improving algorithms offers hope for mitigating biodiversity losses and averting worst-case scenarios.\nHowever, cautionary perspectives, such as the Asilomar AI Principles, question whether AI itself could exacerbate sustainability challenges if improperly constrained. The rising energy demands of large-scale computing systems and the increasingly massive neural network model training conflict with clean energy ambitions. Lack of diversity in data inputs or developers’ priorities may downplay urgent environmental justice considerations. Near-term skeptical public engagement likely hinges on a need for perceivable safeguards against uncontrolled AI systems running amok on core ecological processes.\nIn essence, polarized framings either promote AI as an indispensable tool for sustainability problem-solving–if compassionately directed toward people and the planet–or present AI as an amplifier of existing harms insidiously dominating hidden facets of natural systems central to all life. Overcoming such impasses demands balancing honest trade-off discussions with shared visions for equitable, democratically governed technological progress targeting restoration.\n\n\n16.13.3 Equitable Participation\nEnsuring equitable participation and access should form a cornerstone of any sustainability initiative with the potential for major societal impacts. This principle applies equally to AI systems targeting environmental goals. However, commonly excluded voices like frontline, rural, or indigenous communities and future generations not present to consent could suffer disproportionate consequences from technology transformations. For instance, the Partnership on AI has launched events expressly targeting input from marginalized communities on deploying AI responsibly.\nEnsuring equitable access and participation should form a cornerstone of any sustainability initiative with the potential for major societal impacts, whether AI or otherwise. However, inclusive engagement in environmental AI relies partly on the availability and understanding of fundamental computing resources. As the recent OECD report on National AI Compute Capacity highlights (Oecd 2023), many countries currently lack data or strategic plans mapping needs for the infrastructure required to fuel AI systems. This policy blindspot could constrain economic goals and exacerbate barriers to entry for marginalized populations. Their blueprint urges developing national AI compute capacity strategies along dimensions of capacity, accessibility, innovation pipelines, and resilience to anchor innovation. The underlying data storage needs to be improved, and model development platforms or specialized hardware could inadvertently concentrate AI progress in the hands of select groups. Therefore, planning for a balanced expansion of fundamental AI computing resources via policy initiatives ties directly to hopes for democratized sustainability problem-solving using equitable and transparent ML tools.\n\nOecd. 2023. «A blueprint for building national compute capacity for artificial intelligence». 350. Organisation for Economic Co-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\nThe key idea is that equitable participation in AI systems targeting environmental challenges relies in part on ensuring the underlying computing capacity and infrastructure are correct, which requires proactive policy planning from a national perspective.\n\n\n16.13.4 Transparency\nAs public sector agencies and private companies alike rush towards adopting AI tools to help tackle pressing environmental challenges, calls for transparency around these systems’ development and functionality have begun to amplify. Explainable and interpretable ML features grow more crucial for building trust in emerging models aiming to guide consequential sustainability policies. Initiatives like the Montreal Carbon Pledge brought tech leaders together to commit to publishing impact assessments before launching environmental systems, as pledged below:\n*“As institutional investors, we must act in the best long-term interests of our beneficiaries. In this fiduciary role, long-term investment risks are associated with greenhouse gas emissions, climate change, and carbon regulation.\nMeasuring our carbon footprint is integral to understanding better, quantifying, and managing the carbon and climate change-related impacts, risks, and opportunities in our investments. Therefore, as a first step, we commit to measuring and disclosing the carbon footprint of our investments annually to use this information to develop an engagement strategy and identify and set carbon footprint reduction targets.”*\nWe need a similar pledge for AI sustainability and responsibility. Widespread acceptance and impact of AI sustainability solutions will partly be on deliberate communication of validation schemes, metrics, and layers of human judgment applied before live deployment. Efforts like NIST’s Principles for Explainable AI can help foster transparency into AI systems. The National Institute of Standards and Technology (NIST) has published an influential set of guidelines dubbed the Principles for Explainable AI (Phillips et al. 2020). This framework articulates best practices for designing, evaluating, and deploying responsible AI systems with transparent and interpretable features that build critical user understanding and trust.\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A Broniatowski, e Mark A Przybocki. 2020. «Four principles of explainable artificial intelligence». Gaithersburg, Maryland 18.\nIt delineates four core principles: Firstly, AI systems should provide contextually relevant explanations justifying the reasoning behind their outputs to appropriate stakeholders. Secondly, these AI explanations must communicate information meaningfully for their target audience’s appropriate comprehension level. Next is the accuracy principle, which dictates that explanations should faithfully reflect the actual process and logic informing an AI model’s internal mechanics for generating given outputs or recommendations based on inputs. Finally, a knowledge limits principle compels explanations to clarify an AI model’s boundaries in capturing the full breadth of real-world complexity, variance, and uncertainties within a problem space.\nAltogether, these NIST principles offer AI practitioners and adopters guidance on key transparency considerations vital for developing accessible solutions prioritizing user autonomy and trust rather than simply maximizing predictive accuracy metrics alone. As AI rapidly advances across sensitive social contexts like healthcare, finance, employment, and beyond, such human-centered design guidelines will continue growing in importance for anchoring innovation to public interests.\nThis applies equally to the domain of environmental ability. Responsible and democratically guided AI innovation targeting shared ecological priorities depends on maintaining public vigilance, understanding, and oversight over otherwise opaque systems taking prominent roles in societal decisions. Prioritizing explainable algorithm designs and radical transparency practices per global standards can help sustain collective confidence that these tools improve rather than imperil hopes for a driven future.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#future-directions-and-challenges",
    "href": "contents/sustainable_ai/sustainable_ai.html#future-directions-and-challenges",
    "title": "16  Sustainable AI",
    "section": "16.14 Future Directions and Challenges",
    "text": "16.14 Future Directions and Challenges\nAs we look towards the future, the role of AI in environmental sustainability is poised to grow even more significant. AI’s potential to drive advancements in renewable energy, climate modeling, conservation efforts, and more is immense. However, it is a two-sided coin, as we need to overcome several challenges and direct our efforts towards sustainable and responsible AI development.\n\n16.14.1 Future Directions\nOne key future direction is the development of more energy-efficient AI models and algorithms. This involves ongoing research and innovation in areas like model pruning, quantization, and the use of low-precision numerics, as well as developing the hardware to enable full profitability of these innovations. Even further, we look at alternative computing paradigms that do not rely on von-Neumann architectures. More on this topic can be found in the hardware acceleration chapter. The goal is to create AI systems that deliver high performance while minimizing energy consumption and carbon emissions.\nAnother important direction is the integration of renewable energy sources into AI infrastructure. As data centers continue to be major contributors to AI’s carbon footprint, transitioning to renewable energy sources like solar and wind is crucial. Developments in long-term, sustainable energy storage, such as Ambri, an MIT spinoff, could enable this transition. This requires significant investment and collaboration between tech companies, energy providers, and policymakers.\n\n\n16.14.2 Challenges\nDespite these promising directions, several challenges need to be addressed. One of the major challenges is the need for consistent standards and methodologies for measuring and reporting the environmental impact of AI. These methods must capture the complexity of the life cycles of AI models and system hardware. Next, efficient and environmentally sustainable AI infrastructure and system hardware are needed. This consists of three components. It aims to maximize the utilization of accelerator and system resources, prolong the lifetime of AI infrastructure, and design systems hardware with environmental impact in mind.\nOn the software side, we should trade off experimentation and the subsequent training cost. Techniques such as neural architecture search and hyperparameter optimization can be used for design space exploration. However, these are often very resource-intensive. Efficient experimentation can significantly reduce the environmental footprint overhead. Next, methods to reduce wasted training efforts should be explored.\nTo improve model quality, we often scale the dataset. However, the increased system resources required for data storage and ingestion caused by this scaling have a significant environmental impact (Wu et al. 2022). A thorough understanding of the rate at which data loses its predictive value and devising data sampling strategies is important.\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. 2022. «Sustainable ai: Environmental implications, challenges and opportunities». Proceedings of Machine Learning and Systems 4: 795–813.\nData gaps also pose a significant challenge. Without companies and governments openly sharing detailed and accurate data on energy consumption, carbon emissions, and other environmental impacts, it isn’t easy to develop effective strategies for sustainable AI.\nFinally, the fast pace of AI development requires an agile approach to the policy imposed on these systems. The policy should ensure sustainable development without constraining innovation. This requires experts in all domains of AI, environmental sciences, energy, and policy to work together to achieve a sustainable future.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#conclusion",
    "href": "contents/sustainable_ai/sustainable_ai.html#conclusion",
    "title": "16  Sustainable AI",
    "section": "16.15 Conclusion",
    "text": "16.15 Conclusion\nWe must address sustainability considerations as AI rapidly expands across industries and society. AI promises breakthrough innovations, yet its environmental footprint threatens its widespread growth. This chapter analyzes multiple facets, from energy and emissions to waste and biodiversity impacts, that AI/ML developers must weigh when creating responsible AI systems.\nFundamentally, we require elevating sustainability as a primary design priority rather than an afterthought. Techniques like energy-efficient models, renewable-powered data centers, and hardware recycling programs offer solutions, but the holistic commitment remains vital. We need standards around transparency, carbon accounting, and supply chain disclosures to supplement technical gains. Still, examples like Google’s 4M efficiency practices containing ML energy use highlight that we can advance AI in lockstep with environmental objectives with concerted effort. We achieve this harmonious balance by having researchers, corporations, regulators, and users collaborate across domains. The aim is not perfect solutions but continuous improvement as we integrate AI across new sectors.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/sustainable_ai/sustainable_ai.html#sec-sustainable-ai-resource",
    "href": "contents/sustainable_ai/sustainable_ai.html#sec-sustainable-ai-resource",
    "title": "16  Sustainable AI",
    "section": "16.16 Resources",
    "text": "16.16 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nTransparency and Sustainability.\nSustainability of TinyML.\nModel Cards for Transparency.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nEsercizio 16.1\nEsercizio 16.2\n\n\n\n\n\n\n\n\n\n\nLabs\n\n\n\n\n\nIn addition to exercises, we offer hands-on labs that allow students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.\n\nComing soon.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html",
    "href": "contents/robust_ai/robust_ai.html",
    "title": "17  Robust AI",
    "section": "",
    "text": "17.1 Introduction\nRobust AI refers to a system’s ability to maintain its performance and reliability in the presence of hardware, software, and errors. A robust machine learning system is designed to be fault-tolerant and error-resilient, capable of operating effectively even under adverse conditions.\nAs ML systems become increasingly integrated into various aspects of our lives, from cloud-based services to edge devices and embedded systems, the impact of hardware and software faults on their performance and reliability becomes more significant. In the future, as ML systems become more complex and are deployed in even more critical applications, the need for robust and fault-tolerant designs will be paramount.\nML systems are expected to play crucial roles in autonomous vehicles, smart cities, healthcare, and industrial automation domains. In these domains, the consequences of hardware or software faults can be severe, potentially leading to loss of life, economic damage, or environmental harm.\nResearchers and engineers must focus on developing advanced techniques for fault detection, isolation, and recovery to mitigate these risks and ensure the reliable operation of future ML systems.\nThis chapter will focus specifically on three main categories of faults and errors that can impact the robustness of ML systems: hardware faults, software faults, and human errors.\nThe specific challenges and approaches to achieving robustness may vary depending on the scale and constraints of the ML system. Large-scale cloud computing or data center systems may focus on fault tolerance and resilience through redundancy, distributed processing, and advanced error detection and correction techniques. In contrast, resource-constrained edge devices or embedded systems face unique challenges due to limited computational power, memory, and energy resources.\nRegardless of the scale and constraints, the key characteristics of a robust ML system include fault tolerance, error resilience, and performance maintenance. By understanding and addressing the multifaceted challenges to robustness, we can develop trustworthy and reliable ML systems that can navigate the complexities of real-world environments.\nThis chapter is not just about exploring ML systems’ tools, frameworks, and techniques for detecting and mitigating faults, attacks, and distributional shifts. It’s about emphasizing the crucial role of each one of you in prioritizing resilience throughout the AI development lifecycle, from data collection and model training to deployment and monitoring. By proactively addressing the challenges to robustness, we can unlock the full potential of ML technologies while ensuring their safe, reliable, and responsible deployment in real-world applications.\nAs AI continues to shape our future, the potential of ML technologies is immense. But it’s only when we build resilient systems that can withstand the challenges of the real world that we can truly harness this potential. This is a defining factor in the success and societal impact of this transformative technology, and it’s within our reach.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html#introduction",
    "href": "contents/robust_ai/robust_ai.html#introduction",
    "title": "17  Robust AI",
    "section": "",
    "text": "Hardware Faults: Transient, permanent, and intermittent faults can affect the hardware components of an ML system, corrupting computations and degrading performance.\nModel Robustness: ML models can be vulnerable to adversarial attacks, data poisoning, and distribution shifts, which can induce targeted misclassifications, skew the model’s learned behavior, or compromise the system’s integrity and reliability.\nSoftware Faults: Bugs, design flaws, and implementation errors in the software components, such as algorithms, libraries, and frameworks, can propagate errors and introduce vulnerabilities.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html#real-world-examples",
    "href": "contents/robust_ai/robust_ai.html#real-world-examples",
    "title": "17  Robust AI",
    "section": "17.2 Real-World Examples",
    "text": "17.2 Real-World Examples\nHere are some real-world examples of cases where faults in hardware or software have caused major issues in ML systems across cloud, edge, and embedded environments:\n\n17.2.1 Cloud\nIn February 2017, Amazon Web Services (AWS) experienced a significant outage due to human error during maintenance. An engineer inadvertently entered an incorrect command, causing many servers to be taken offline. This outage disrupted many AWS services, including Amazon’s AI-powered assistant, Alexa. As a result, Alexa-powered devices, such as Amazon Echo and third-party products using Alexa Voice Service, could not respond to user requests for several hours. This incident highlights the potential impact of human errors on cloud-based ML systems and the need for robust maintenance procedures and failsafe mechanisms.\nIn another example (Vangal et al. 2021), Facebook encountered a silent data corruption (SDC) issue within its distributed querying infrastructure, as shown in Figura 17.1. Facebook’s infrastructure includes a querying system that fetches and executes SQL and SQL-like queries across multiple datasets using frameworks like Presto, Hive, and Spark. One of the applications that utilized this querying infrastructure was a compression application to reduce the footprint of data stores. In this compression application, files were compressed when not being read and decompressed when a read request was made. Before decompression, the file size was checked to ensure it was greater than zero, indicating a valid compressed file with contents.\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar, Ram Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, e Chris H. Kim. 2021. «Wide-Range Many-Core SoC Design in Scaled CMOS: Challenges and Opportunities». IEEE Trans. Very Large Scale Integr. VLSI Syst. 29 (5): 843–56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\n\n\n\n\nFigura 17.1: Silent data corruption in database applications. Source: Facebook\n\n\n\nHowever, in one instance, when the file size was being computed for a valid non-zero-sized file, the decompression algorithm invoked a power function from the Scala library. Unexpectedly, the Scala function returned a zero size value for the file despite having a known non-zero decompressed size. As a result, the decompression was not performed, and the file was not written to the output database. This issue manifested sporadically, with some occurrences of the same file size computation returning the correct non-zero value.\nThe impact of this silent data corruption was significant, leading to missing files and incorrect data in the output database. The application relying on the decompressed files failed due to the data inconsistencies. In the case study presented in the paper, Facebook’s infrastructure, which consists of hundreds of thousands of servers handling billions of requests per day from their massive user base, encountered a silent data corruption issue. The affected system processed user queries, image uploads, and media content, which required fast, reliable, and secure execution.\nThis case study illustrates how silent data corruption can propagate through multiple layers of an application stack, leading to data loss and application failures in a large-scale distributed system. The intermittent nature of the issue and the lack of explicit error messages made it particularly challenging to diagnose and resolve. But this is not restricted to just Meta, even other companies such as Google that operate AI hypercomputers face this challenge. Figura 17.2 Jeff Dean, Chief Scientist at Google DeepMind and Google Research, discusses SDCS and their impact on ML systems.\n\n\n\n\n\n\nFigura 17.2: Silent data corruption (SDC) errors are a major issue for AI hypercomputers. Source: Jeff Dean at MLSys 2024, Keynote (Google)\n\n\n\n\n\n17.2.2 Edge\nRegarding examples of faults and errors in edge ML systems, one area that has gathered significant attention is the domain of self-driving cars. Self-driving vehicles rely heavily on machine learning algorithms for perception, decision-making, and control, making them particularly susceptible to the impact of hardware and software faults. In recent years, several high-profile incidents involving autonomous vehicles have highlighted the challenges and risks associated with deploying these systems in real-world environments.\nIn May 2016, a fatal accident occurred when a Tesla Model S operating on Autopilot crashed into a white semi-trailer truck crossing the highway. The Autopilot system, which relied on computer vision and machine learning algorithms, failed to recognize the white trailer against a bright sky background. The driver, who was reportedly watching a movie when the crash, did not intervene in time, and the vehicle collided with the trailer at full speed. This incident raised concerns about the limitations of AI-based perception systems and the need for robust failsafe mechanisms in autonomous vehicles. It also highlighted the importance of driver awareness and the need for clear guidelines on using semi-autonomous driving features, as shown in Figura 17.3.\n\n\n\n\n\n\nFigura 17.3: Tesla in the fatal California crash was on Autopilot. Source: BBC News\n\n\n\nIn March 2018, an Uber self-driving test vehicle struck and killed a pedestrian crossing the street in Tempe, Arizona. The incident was caused by a software flaw in the vehicle’s object recognition system, which failed to identify the pedestrians appropriately to avoid them as obstacles. The safety driver, who was supposed to monitor the vehicle’s operation and intervene if necessary, was found distracted during the crash. This incident led to widespread scrutiny of Uber’s self-driving program and raised questions about the readiness of autonomous vehicle technology for public roads. It also emphasized the need for rigorous testing, validation, and safety measures in developing and deploying AI-based self-driving systems.\nIn 2021, Tesla faced increased scrutiny following several accidents involving vehicles operating on Autopilot mode. Some of these accidents were attributed to issues with the Autopilot system’s ability to detect and respond to certain road situations, such as stationary emergency vehicles or obstacles in the road. For example, in April 2021, a Tesla Model S crashed into a tree in Texas, killing two passengers. Initial reports suggested that no one was in the driver’s seat at the time of the crash, raising questions about the use and potential misuse of Autopilot features. These incidents highlight the ongoing challenges in developing robust and reliable autonomous driving systems and the need for clear regulations and consumer education regarding the capabilities and limitations of these technologies.\n\n\n17.2.3 Embedded\nEmbedded systems, which often operate in resource-constrained environments and safety-critical applications, have long faced challenges related to hardware and software faults. As AI and machine learning technologies are increasingly integrated into these systems, the potential for faults and errors takes on new dimensions, with the added complexity of AI algorithms and the critical nature of the applications in which they are deployed.\nLet’s consider a few examples, starting with outer space exploration. NASA’s Mars Polar Lander mission in 1999 suffered a catastrophic failure due to a software error in the touchdown detection system (Figura 17.4). The spacecraft’s onboard software mistakenly interpreted the noise from the deployment of its landing legs as a sign that it had touched down on the Martian surface. As a result, the spacecraft prematurely shut down its engines, causing it to crash into the surface. This incident highlights the critical importance of robust software design and extensive testing in embedded systems, especially those operating in remote and unforgiving environments. As AI capabilities are integrated into future space missions, ensuring these systems’ reliability and fault tolerance will be paramount to mission success.\n\n\n\n\n\n\nFigura 17.4: NASA’s Failed Mars Polar Lander mission in 1999 cost over $200M. Source: SlashGear\n\n\n\nBack on earth, in 2015, a Boeing 787 Dreamliner experienced a complete electrical shutdown during a flight due to a software bug in its generator control units. The bug caused the generator control units to enter a failsafe mode, cutting power to the aircraft’s electrical systems and forcing an emergency landing. This incident underscores the potential for software faults to have severe consequences in complex embedded systems like aircraft. As AI technologies are increasingly applied in aviation, such as in autonomous flight systems and predictive maintenance, ensuring the robustness and reliability of these systems will be critical to passenger safety.\nAs AI capabilities increasingly integrate into embedded systems, the potential for faults and errors becomes more complex and severe. Imagine a smart pacemaker that has a sudden glitch. A patient could die from that effect. Therefore, AI algorithms, such as those used for perception, decision-making, and control, introduce new sources of potential faults, such as data-related issues, model uncertainties, and unexpected behaviors in edge cases. Moreover, the opaque nature of some AI models can make it challenging to identify and diagnose faults when they occur.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html#hardware-faults",
    "href": "contents/robust_ai/robust_ai.html#hardware-faults",
    "title": "17  Robust AI",
    "section": "17.3 Hardware Faults",
    "text": "17.3 Hardware Faults\nHardware faults are a significant challenge in computing systems, including traditional and ML systems. These faults occur when physical components, such as processors, memory modules, storage devices, or interconnects, malfunction or behave abnormally. Hardware faults can cause incorrect computations, data corruption, system crashes, or complete system failure, compromising the integrity and trustworthiness of the computations performed by the system (Jha et al. 2019). A complete system failure refers to a situation where the entire computing system becomes unresponsive or inoperable due to a critical hardware malfunction. This type of failure is the most severe, as it renders the system unusable and may lead to data loss or corruption, requiring manual intervention to repair or replace the faulty components.\nUnderstanding the taxonomy of hardware faults is essential for anyone working with computing systems, especially in the context of ML systems. ML systems rely on complex hardware architectures and large-scale computations to train and deploy models that learn from data and make intelligent predictions or decisions. However, hardware faults can introduce errors and inconsistencies in the MLOps pipeline, affecting the trained models’ accuracy, robustness, and reliability (G. Li et al. 2017).\nKnowing the different types of hardware faults, their mechanisms, and their potential impact on system behavior is crucial for developing effective strategies to detect, mitigate, and recover them. This knowledge is necessary for designing fault-tolerant computing systems, implementing robust ML algorithms, and ensuring the overall dependability of ML-based applications.\nThe following sections will explore the three main categories of hardware faults: transient, permanent, and intermittent. We will discuss their definitions, characteristics, causes, mechanisms, and examples of how they manifest in computing systems. We will also cover detection and mitigation techniques specific to each fault type.\n\nTransient Faults: Transient faults are temporary and non-recurring. They are often caused by external factors such as cosmic rays, electromagnetic interference, or power fluctuations. A common example of a transient fault is a bit flip, where a single bit in a memory location or register changes its value unexpectedly. Transient faults can lead to incorrect computations or data corruption, but they do not cause permanent damage to the hardware.\nPermanent Faults: Permanent faults, also called hard errors, are irreversible and persist over time. They are typically caused by physical defects or wear-out of hardware components. Examples of permanent faults include stuck-at faults, where a bit or signal is permanently set to a specific value (e.g., always 0 or always 1), and device failures, such as a malfunctioning processor or a damaged memory module. Permanent faults can result in complete system failure or significant performance degradation.\nIntermittent Faults: Intermittent faults are recurring faults that appear and disappear intermittently. Unstable hardware conditions, such as loose connections, aging components, or manufacturing defects, often cause them. Intermittent faults can be challenging to diagnose and reproduce because they may occur sporadically and under specific conditions. Examples include intermittent short circuits or contact resistance issues. Intermittent faults can lead to unpredictable system behavior and intermittent errors.\n\nBy the end of this discussion, readers will have a solid understanding of fault taxonomy and its relevance to traditional computing and ML systems. This foundation will help them make informed decisions when designing, implementing, and deploying fault-tolerant solutions, improving the reliability and trustworthiness of their computing systems and ML applications.\n\n17.3.1 Transient Faults\nTransient faults in hardware can manifest in various forms, each with its own unique characteristics and causes. These faults are temporary in nature and do not result in permanent damage to the hardware components.\n\nDefinition and Characteristics\nSome of the common types of transient faults include Single Event Upsets (SEUs) caused by ionizing radiation, voltage fluctuations (Reddi e Gupta 2013) due to power supply noise or electromagnetic interference, Electromagnetic Interference (EMI) induced by external electromagnetic fields, Electrostatic Discharge (ESD) resulting from sudden static electricity flow, crosstalk caused by unintended signal coupling, ground bounce triggered by simultaneous switching of multiple outputs, timing violations due to signal timing constraint breaches, and soft errors in combinational logic affecting the output of logic circuits (Mukherjee, Emer, e Reinhardt 2005). Understanding these different types of transient faults is crucial for designing robust and resilient hardware systems that can mitigate their impact and ensure reliable operation.\n\nReddi, Vijay Janapa, e Meeta Sharma Gupta. 2013. Resilient Architecture Design for Voltage Variation. Springer International Publishing. https://doi.org/10.1007/978-3-031-01739-1.\n\nMukherjee, S. S., J. Emer, e S. K. Reinhardt. 2005. «The Soft Error Problem: An Architectural Perspective». In 11th International Symposium on High-Performance Computer Architecture, 243–47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\nAll of these transient faults are characterized by their short duration and non-permanent nature. They do not persist or leave any lasting impact on the hardware. However, they can still lead to incorrect computations, data corruption, or system misbehavior if not properly handled.\n\n\n\nCauses of Transient Faults\nTransient faults can be attributed to various external factors. One common cause is cosmic rays, high-energy particles originating from outer space. When these particles strike sensitive areas of the hardware, such as memory cells or transistors, they can induce charge disturbances that alter the stored or transmitted data. This is illustrated in Figura 17.5. Another cause of transient faults is electromagnetic interference (EMI) from nearby devices or power fluctuations. EMI can couple with the circuits and cause voltage spikes or glitches that temporarily disrupt the normal operation of the hardware.\n\n\n\n\n\n\nFigura 17.5: Mechanism of Hardware Transient Fault Occurrence. Source: NTT\n\n\n\n\n\nMechanisms of Transient Faults\nTransient faults can manifest through different mechanisms depending on the affected hardware component. In memory devices like DRAM or SRAM, transient faults often lead to bit flips, where a single bit changes its value from 0 to 1 or vice versa. This can corrupt the stored data or instructions. In logic circuits, transient faults can cause glitches or voltage spikes propagating through the combinational logic, resulting in incorrect outputs or control signals. Transient faults can also affect communication channels, causing bit errors or packet losses during data transmission.\n\n\nImpact on ML Systems\nA common example of a transient fault is a bit flip in the main memory. If an important data structure or critical instruction is stored in the affected memory location, it can lead to incorrect computations or program misbehavior. If a transient fault occurs in the memory storing the model weights or gradients. For instance, a bit flip in the memory storing a loop counter can cause the loop to execute indefinitely or terminate prematurely. Transient faults in control registers or flag bits can alter the flow of program execution, leading to unexpected jumps or incorrect branch decisions. In communication systems, transient faults can corrupt transmitted data packets, resulting in retransmissions or data loss.\nIn ML systems, transient faults can have significant implications during the training phase (He et al. 2023). ML training involves iterative computations and updates to model parameters based on large datasets. If a transient fault occurs in the memory storing the model weights or gradients, it can lead to incorrect updates and compromise the convergence and accuracy of the training process. Figura 17.6 Show a real-world example from Google’s production fleet where an SDC anomaly caused a significant difference in the gradient norm.\n\n\n\n\n\n\nFigura 17.6: SDC in ML training phase results in anomalies in the gradient norm. Source: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nFor example, a bit flip in the weight matrix of a neural network can cause the model to learn incorrect patterns or associations, leading to degraded performance (Wan et al. 2021). Transient faults in the data pipeline, such as corruption of training samples or labels, can also introduce noise and affect the quality of the learned model.\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi, e Arijit Raychowdhury. 2021. «Analyzing and Improving Fault Tolerance of Learning-Based Navigation Systems». In 2021 58th ACM/IEEE Design Automation Conference (DAC), 841–46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\nDuring the inference phase, transient faults can impact the reliability and trustworthiness of ML predictions. If a transient fault occurs in the memory storing the trained model parameters or in the computation of the inference results, it can lead to incorrect or inconsistent predictions. For instance, a bit flip in the activation values of a neural network can alter the final classification or regression output (Mahmoud et al. 2020).\nIn safety-critical applications, such as autonomous vehicles or medical diagnosis, transient faults during inference can have severe consequences, leading to incorrect decisions or actions (G. Li et al. 2017; Jha et al. 2019). Ensuring the resilience of ML systems against transient faults is crucial to maintaining the integrity and reliability of the predictions.\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, e Stephen W. Keckler. 2017. «Understanding error propagation in deep learning neural network (DNN) accelerators and applications». In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 1–12. ACM. https://doi.org/10.1145/3126908.3126964.\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, e Yoshua Bengio. 2016. «Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1». arXiv preprint arXiv:1602.02830.\n\nAygun, Sercan, Ece Olcay Gunes, e Christophe De Vleeschouwer. 2021. «Efficient and robust bitstream processing in binarised neural networks». Electron. Lett. 57 (5): 219–22. https://doi.org/10.1049/ell2.12045.\nAt the other extreme, in resource-constrained environments like TinyML, Binarized Neural Networks [BNNs] (Courbariaux et al. 2016) have emerged as a promising solution. BNNs represent network weights in single-bit precision, offering computational efficiency and faster inference times. However, this binary representation renders BNNs fragile to bit-flip errors on the network weights. For instance, prior work (Aygun, Gunes, e De Vleeschouwer 2021) has shown that a two-hidden layer BNN architecture for a simple task such as MNIST classification suffers performance degradation from 98% test accuracy to 70% when random bit-flipping soft errors are inserted through model weights with a 10% probability.\nAddressing such issues requires considering flip-aware training techniques or leveraging emerging computing paradigms (e.g., stochastic computing) to improve fault tolerance and robustness, which we will discuss in Sezione 17.3.4. Future research directions aim to develop hybrid architectures, novel activation functions, and loss functions tailored to bridge the accuracy gap compared to full-precision models while maintaining their computational efficiency.\n\n\n\n17.3.2 Permanent Faults\nPermanent faults are hardware defects that persist and cause irreversible damage to the affected components. These faults are characterized by their persistent nature and require repair or replacement of the faulty hardware to restore normal system functionality.\n\nDefinition and Characteristics\nPermanent faults are hardware defects that cause persistent and irreversible malfunctions in the affected components. The faulty component remains non-operational until a permanent fault is repaired or replaced. These faults are characterized by their consistent and reproducible nature, meaning that the faulty behavior is observed every time the affected component is used. Permanent faults can impact various hardware components, such as processors, memory modules, storage devices, or interconnects, leading to system crashes, data corruption, or complete system failure.\nOne notable example of a permanent fault is the Intel FDIV bug, which was discovered in 1994. The FDIV bug was a flaw in certain Intel Pentium processors’ floating-point division (FDIV) units. The bug caused incorrect results for specific division operations, leading to inaccurate calculations.\nThe FDIV bug occurred due to an error in the lookup table used by the division unit. In rare cases, the processor would fetch an incorrect value from the lookup table, resulting in a slightly less precise result than expected. For instance, Figura 17.7 shows a fraction 4195835/3145727 plotted on a Pentium processor with the FDIV permanent fault. The triangular regions are where erroneous calculations occurred. Ideally, all correct values would round to 1.3338, but the erroneous results show 1.3337, indicating a mistake in the 5th digit.\nAlthough the error was small, it could compound over many division operations, leading to significant inaccuracies in mathematical calculations. The impact of the FDIV bug was significant, especially for applications that relied heavily on precise floating-point division, such as scientific simulations, financial calculations, and computer-aided design. The bug led to incorrect results, which could have severe consequences in fields like finance or engineering.\n\n\n\n\n\n\nFigura 17.7: Intel Pentium processor with the FDIV permanent fault. The triangular regions are where erroneous calculations occurred. Source: Byte Magazine\n\n\n\nThe Intel FDIV bug is a cautionary tale for the potential impact of permanent faults on ML systems. In the context of ML, permanent faults in hardware components can lead to incorrect computations, affecting the accuracy and reliability of the models. For example, if an ML system relies on a processor with a faulty floating-point unit, similar to the Intel FDIV bug, it could introduce errors in the calculations performed during training or inference.\nThese errors can propagate through the model, leading to inaccurate predictions or skewed learning. In applications where ML is used for critical tasks, such as autonomous driving, medical diagnosis, or financial forecasting, the consequences of incorrect computations due to permanent faults can be severe.\nIt is crucial for ML practitioners to be aware of the potential impact of permanent faults and to incorporate fault-tolerant techniques, such as hardware redundancy, error detection and correction mechanisms, and robust algorithm design, to mitigate the risks associated with these faults. Additionally, thorough testing and validation of ML hardware components can help identify and address permanent faults before they impact the system’s performance and reliability.\n\n\nCauses of Permanent Faults\nPermanent faults can arise from several causes, including manufacturing defects and wear-out mechanisms. Manufacturing defects are inherent flaws introduced during the fabrication process of hardware components. These defects include improper etching, incorrect doping, or contamination, leading to non-functional or partially functional components.\nOn the other hand, wear-out mechanisms occur over time as the hardware components are subjected to prolonged use and stress. Factors such as electromigration, oxide breakdown, or thermal stress can cause gradual degradation of the components, eventually leading to permanent failures.\n\n\nMechanisms of Permanent Faults\nPermanent faults can manifest through various mechanisms, depending on the nature and location of the fault. Stuck-at faults (Seong et al. 2010) are common permanent faults where a signal or memory cell remains fixed at a particular value (either 0 or 1) regardless of the inputs, as illustrated in Figura 17.8.\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, e Hsien-Hsin S. Lee. 2010. «SAFER: Stuck-at-fault Error Recovery for Memories». In 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture, 115–24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\n\n\n\n\nFigura 17.8: Stuck-at Fault Model in Digital Circuits. Source: Accendo Reliability\n\n\n\nStuck-at faults can occur in logic gates, memory cells, or interconnects, causing incorrect computations or data corruption. Another mechanism is device failures, where a component, such as a transistor or a memory cell, completely ceases to function. This can be due to manufacturing defects or severe wear-out. Bridging faults occur when two or more signal lines are unintentionally connected, causing short circuits or incorrect logic behavior.\nIn addition to stuck-at faults, there are several other types of permanent faults that can affect digital circuits that can impact an ML system. Delay faults can cause the propagation delay of a signal to exceed the specified limit, leading to timing violations. Interconnect faults, such as open faults (broken wires), resistive faults (increased resistance), or capacitive faults (increased capacitance), can cause signal integrity issues or timing violations. Memory cells can also suffer from various faults, including transition faults (inability to change state), coupling faults (interference between adjacent cells), and neighborhood pattern sensitive faults (faults that depend on the values of neighboring cells). Other permanent faults can occur in the power supply network or the clock distribution network, affecting the functionality and timing of the circuit.\n\n\nImpact on ML Systems\nPermanent faults can severely affect the behavior and reliability of computing systems. For example, a stuck-at-fault in a processor’s arithmetic logic unit (ALU) can cause incorrect computations, leading to erroneous results or system crashes. A permanent fault in a memory module, such as a stuck-at fault in a specific memory cell, can corrupt the stored data, causing data loss or program misbehavior. In storage devices, permanent faults like bad sectors or device failures can result in data inaccessibility or complete loss of stored information. Permanent interconnect faults can disrupt communication channels, causing data corruption or system hangs.\nPermanent faults can significantly affect ML systems during the training and inference phases. During training, permanent faults in processing units or memory can lead to incorrect computations, resulting in corrupted or suboptimal models (He et al. 2023). Furthermore, faults in storage devices can corrupt the training data or the stored model parameters, leading to data loss or model inconsistencies (He et al. 2023).\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, e Siddharth Garg. 2018. «Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator». In 2018 IEEE 36th VLSI Test Symposium (VTS), 1–6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\nDuring inference, permanent faults can impact the reliability and correctness of ML predictions. Faults in the processing units can produce incorrect results or cause system failures, while faults in memory storing the model parameters can lead to corrupted or outdated models being used for inference (J. J. Zhang et al. 2018).\nTo mitigate the impact of permanent faults in ML systems, fault-tolerant techniques must be employed at both the hardware and software levels. Hardware redundancy, such as duplicating critical components or using error-correcting codes (Kim, Sullivan, e Erez 2015), can help detect and recover from permanent faults. Software techniques, such as checkpoint and restart mechanisms (Egwutuoha et al. 2013), can enable the system to recover from permanent faults by returning to a previously saved state. Regular monitoring, testing, and maintenance of ML systems can help identify and replace faulty components before they cause significant disruptions.\n\nKim, Jungrae, Michael Sullivan, e Mattan Erez. 2015. «Bamboo ECC: Strong, safe, and flexible codes for reliable computer memory». In 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), 101–12. IEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, e Shiping Chen. 2013. «A survey of fault tolerance mechanisms and checkpoint/restart implementations for high performance computing systems». The Journal of Supercomputing 65 (3): 1302–26. https://doi.org/10.1007/s11227-013-0884-0.\nDesigning ML systems with fault tolerance in mind is crucial to ensure their reliability and robustness in the presence of permanent faults. This may involve incorporating redundancy, error detection and correction mechanisms, and fail-safe strategies into the system architecture. By proactively addressing the challenges posed by permanent faults, ML systems can maintain their integrity, accuracy, and trustworthiness, even in the face of hardware failures.\n\n\n\n17.3.3 Intermittent Faults\nIntermittent faults are hardware faults that occur sporadically and unpredictably in a system. An example is illustrated in Figura 17.9, where cracks in the material can introduce increased resistance in circuitry. These faults are particularly challenging to detect and diagnose because they appear and disappear intermittently, making it difficult to reproduce and isolate the root cause. Intermittent faults can lead to system instability, data corruption, and performance degradation.\n\n\n\n\n\n\nFigura 17.9: Increased resistance due to an intermittent fault – crack between copper bump and package solder. Source: Constantinescu\n\n\n\n\nDefinition and Characteristics\nIntermittent faults are characterized by their sporadic and non-deterministic nature. They occur irregularly and may appear and disappear spontaneously, with varying durations and frequencies. These faults do not consistently manifest every time the affected component is used, making them harder to detect than permanent faults. Intermittent faults can affect various hardware components, including processors, memory modules, storage devices, or interconnects. They can cause transient errors, data corruption, or unexpected system behavior.\nIntermittent faults can significantly impact the behavior and reliability of computing systems (Rashid, Pattabiraman, e Gopalakrishnan 2015). For example, an intermittent fault in a processor’s control logic can cause irregular program flow, leading to incorrect computations or system hangs. Intermittent faults in memory modules can corrupt data values, resulting in erroneous program execution or data inconsistencies. In storage devices, intermittent faults can cause read/write errors or data loss. Intermittent faults in communication channels can lead to data corruption, packet loss, or intermittent connectivity issues. These faults can cause system crashes, data integrity problems, or performance degradation, depending on the severity and frequency of the intermittent failures.\n\n———. 2015. «Characterizing the Impact of Intermittent Hardware Faults on Programs». IEEE Trans. Reliab. 64 (1): 297–310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nCauses of Intermittent Faults\nIntermittent faults can arise from several causes, both internal and external, to the hardware components (Constantinescu 2008). One common cause is aging and wear-out of the components. As electronic devices age, they become more susceptible to intermittent failures due to degradation mechanisms such as electromigration, oxide breakdown, or solder joint fatigue.\n\nConstantinescu, Cristian. 2008. «Intermittent faults and effects on reliability of integrated circuits». In 2008 Annual Reliability and Maintainability Symposium, 370–74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\nManufacturing defects or process variations can also introduce intermittent faults, where marginal or borderline components may exhibit sporadic failures under specific conditions, as shown in Figura 17.10.\nEnvironmental factors, such as temperature fluctuations, humidity, or vibrations, can trigger intermittent faults by altering the electrical characteristics of the components. Loose or degraded connections, such as those in connectors or printed circuit boards, can cause intermittent faults.\n\n\n\n\n\n\nFigura 17.10: Residue induced intermittent fault in a DRAM chip. Source: Hynix Semiconductor\n\n\n\n\n\nMechanisms of Intermittent Faults\nIntermittent faults can manifest through various mechanisms, depending on the underlying cause and the affected component. One mechanism is the intermittent open or short circuit, where a signal path or connection becomes temporarily disrupted or shorted, causing erratic behavior. Another mechanism is the intermittent delay fault (J. Zhang et al. 2018), where the timing of signals or propagation delays becomes inconsistent, leading to synchronization issues or incorrect computations. Intermittent faults can manifest as transient bit flips or soft errors in memory cells or registers, causing data corruption or incorrect program execution.\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, e Siddharth Garg. 2018. «ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Learning Accelerators». In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nImpact on ML Systems\nIn the context of ML systems, intermittent faults can introduce significant challenges and impact the system’s reliability and performance. During the training phase, intermittent faults in processing units or memory can lead to inconsistencies in computations, resulting in incorrect or noisy gradients and weight updates. This can affect the convergence and accuracy of the training process, leading to suboptimal or unstable models. Intermittent data storage or retrieval faults can corrupt the training data, introducing noise or errors that degrade the quality of the learned models (He et al. 2023).\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, e Yanjing Li. 2023. «Understanding and Mitigating Hardware Failures in Deep Learning Training Systems». In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\nDuring the inference phase, intermittent faults can impact the reliability and consistency of ML predictions. Faults in the processing units or memory can cause incorrect computations or data corruption, leading to erroneous or inconsistent predictions. Intermittent faults in the data pipeline can introduce noise or errors in the input data, affecting the accuracy and robustness of the predictions. In safety-critical applications, such as autonomous vehicles or medical diagnosis systems, intermittent faults can have severe consequences, leading to incorrect decisions or actions that compromise safety and reliability.\nMitigating the impact of intermittent faults in ML systems requires a multifaceted approach (Rashid, Pattabiraman, e Gopalakrishnan 2012). At the hardware level, techniques such as robust design practices, component selection, and environmental control can help reduce the occurrence of intermittent faults. Redundancy and error correction mechanisms can be employed to detect and recover from intermittent failures. At the software level, runtime monitoring, anomaly detection, and fault-tolerant techniques can be incorporated into the ML pipeline. This may include techniques such as data validation, outlier detection, model ensembling, or runtime model adaptation to handle intermittent faults gracefully.\n\nRashid, Layali, Karthik Pattabiraman, e Sathish Gopalakrishnan. 2012. «Intermittent Hardware Errors Recovery: Modeling and Evaluation». In 2012 Ninth International Conference on Quantitative Evaluation of Systems, 220–29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\nDesigning ML systems resilient to intermittent faults is crucial to ensuring their reliability and robustness. This involves incorporating fault-tolerant techniques, runtime monitoring, and adaptive mechanisms into the system architecture. By proactively addressing the challenges of intermittent faults, ML systems can maintain their accuracy, consistency, and trustworthiness, even in sporadic hardware failures. Regular testing, monitoring, and maintenance of ML systems can help identify and mitigate intermittent faults before they cause significant disruptions or performance degradation.\n\n\n\n17.3.4 Detection and Mitigation\nThis section explores various fault detection techniques, including hardware-level and software-level approaches, and discusses effective mitigation strategies to enhance the resilience of ML systems. Additionally, we will look into resilient ML system design considerations, present case studies and examples, and highlight future research directions in fault-tolerant ML systems.\n\nFault Detection Techniques\nFault detection techniques are important for identifying and localizing hardware faults in ML systems. These techniques can be broadly categorized into hardware-level and software-level approaches, each offering unique capabilities and advantages.\n\nHardware-level fault detection\nHardware-level fault detection techniques are implemented at the physical level of the system and aim to identify faults in the underlying hardware components. There are several hardware techniques, but broadly, we can bucket these different mechanisms into the following categories.\nBuilt-in self-test (BIST) mechanisms: BIST is a powerful technique for detecting faults in hardware components (Bushnell e Agrawal 2002). It involves incorporating additional hardware circuitry into the system for self-testing and fault detection. BIST can be applied to various components, such as processors, memory modules, or application-specific integrated circuits (ASICs). For example, BIST can be implemented in a processor using scan chains, which are dedicated paths that allow access to internal registers and logic for testing purposes.\n\nBushnell, Michael L, e Vishwani D Agrawal. 2002. «Built-in self-test». Essentials of electronic testing for digital, memory and mixed-signal VLSI circuits, 489–548.\nDuring the BIST process, predefined test patterns are applied to the processor’s internal circuitry, and the responses are compared against expected values. Any discrepancies indicate the presence of faults. Intel’s Xeon processors, for instance, include BIST mechanisms to test the CPU cores, cache memory, and other critical components during system startup.\nError detection codes: Error detection codes are widely used to detect data storage and transmission errors (Hamming 1950). These codes add redundant bits to the original data, allowing the detection of bit errors. Example: Parity checks are a simple form of error detection code shown in Figura 17.11. In a single-bit parity scheme, an extra bit is appended to each data word, making the number of 1s in the word even (even parity) or odd (odd parity).\n\nHamming, R. W. 1950. «Error Detecting and Error Correcting Codes». Bell Syst. Tech. J. 29 (2): 147–60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\n\n\n\n\nFigura 17.11: Parity bit example. Source: Computer Hope\n\n\n\nWhen reading the data, the parity is checked, and if it doesn’t match the expected value, an error is detected. More advanced error detection codes, such as cyclic redundancy checks (CRC), calculate a checksum based on the data and append it to the message. The checksum is recalculated at the receiving end and compared with the transmitted checksum to detect errors. Error-correcting code (ECC) memory modules, commonly used in servers and critical systems, employ advanced error detection and correction codes to detect and correct single-bit or multi-bit errors in memory.\nHardware redundancy and voting mechanisms: Hardware redundancy involves duplicating critical components and comparing their outputs to detect and mask faults (Sheaffer, Luebke, e Skadron 2007). Voting mechanisms, such as triple modular redundancy (TMR), employ multiple instances of a component and compare their outputs to identify and mask faulty behavior (Arifeen, Hassan, e Lee 2020).\n\nSheaffer, Jeremy W, David P Luebke, e Kevin Skadron. 2007. «A hardware redundancy and recovery mechanism for reliable scientific computation on graphics processors». In Graphics Hardware, 2007:55–64. Citeseer.\n\nArifeen, Tooba, Abdus Sami Hassan, e Jeong-A Lee. 2020. «Approximate Triple Modular Redundancy: A Survey». #IEEE_O_ACC# 8: 139851–67. https://doi.org/10.1109/access.2020.3012673.\n\nYeh, Y. C. 1996. «Triple-triple redundant 777 primary flight computer». In 1996 IEEE Aerospace Applications Conference. Proceedings, 1:293–307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\nIn a TMR system, three identical instances of a hardware component, such as a processor or a sensor, perform the same computation in parallel. The outputs of these instances are fed into a voting circuit, which compares the results and selects the majority value as the final output. If one of the instances produces an incorrect result due to a fault, the voting mechanism masks the error and maintains the correct output. TMR is commonly used in aerospace and aviation systems, where high reliability is critical. For instance, the Boeing 777 aircraft employs TMR in its primary flight computer system to ensure the availability and correctness of flight control functions (Yeh 1996).\nTesla’s self-driving computers employ a redundant hardware architecture to ensure the safety and reliability of critical functions, such as perception, decision-making, and vehicle control, as shown in Figura 17.12. One key component of this architecture is using dual modular redundancy (DMR) in the car’s onboard computer systems.\n\n\n\n\n\n\nFigura 17.12: Tesla full self-driving computer with dual redundant SoCs. Source: Tesla\n\n\n\nIn Tesla’s DMR implementation, two identical hardware units, often called “redundant computers” or “redundant control units,” perform the same computations in parallel (Bannon et al. 2019). Each unit independently processes sensor data, executes perception and decision-making algorithms, and generates control commands for the vehicle’s actuators (e.g., steering, acceleration, and braking).\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, e Emil Talpes. 2019. «Computer and Redundancy Solution for the Full Self-Driving Computer». In 2019 IEEE Hot Chips 31 Symposium (HCS), 1–22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\nThe outputs of these two redundant units are continuously compared to detect any discrepancies or faults. If the outputs match, the system assumes that both units function correctly, and the control commands are sent to the vehicle’s actuators. However, if there is a mismatch between the outputs, the system identifies a potential fault in one of the units and takes appropriate action to ensure safe operation.\nThe system may employ additional mechanisms to determine which unit is faulty in a mismatch. This can involve using diagnostic algorithms, comparing the outputs with data from other sensors or subsystems, or analyzing the consistency of the outputs over time. Once the faulty unit is identified, the system can isolate it and continue operating using the output from the non-faulty unit.\nDMR in Tesla’s self-driving computer provides an extra safety and fault tolerance layer. By having two independent units performing the same computations, the system can detect and mitigate faults that may occur in one of the units. This redundancy helps prevent single points of failure and ensures that critical functions remain operational despite hardware faults.\nFurthermore, Tesla also incorporates additional redundancy mechanisms beyond DMR. For example, they use redundant power supplies, steering and braking systems, and diverse sensor suites (e.g., cameras, radar, and ultrasonic sensors) to provide multiple layers of fault tolerance. These redundancies collectively contribute to the overall safety and reliability of the self-driving system.\nIt’s important to note that while DMR provides fault detection and some level of fault tolerance, TMR may provide a different level of fault masking. In DMR, if both units experience simultaneous faults or the fault affects the comparison mechanism, the system may be unable to identify the fault. Therefore, Tesla’s SDCs rely on a combination of DMR and other redundancy mechanisms to achieve a high level of fault tolerance.\nThe use of DMR in Tesla’s self-driving computer highlights the importance of hardware redundancy in safety-critical applications. By employing redundant computing units and comparing their outputs, the system can detect and mitigate faults, enhancing the overall safety and reliability of the self-driving functionality.\nGoogle employs redundant hot spares to deal with SDC issues within its data centers, thereby enhancing the reliability of critical functions. As illustrated in Figura 17.13, during the normal training phase, multiple synchronous training workers function flawlessly. However, if a worker becomes defective and causes SDC, an SDC checker automatically identifies the issues. Upon detecting the SDC, the SDC checker moves the training to a hot spare and sends the defective machine for repair. This redundancy safeguards the continuity and reliability of ML training, effectively minimizing downtime and preserving data integrity.\n\n\n\n\n\n\nFigura 17.13: Google employs hot spare cores to transparently handle SDCs in the data center. Source: Jeff Dean, MLSys 2024 Keynote (Google)\n\n\n\nWatchdog timers: Watchdog timers are hardware components that monitor the execution of critical tasks or processes (Pont e Ong 2002). They are commonly used to detect and recover from software or hardware faults that cause a system to become unresponsive or stuck in an infinite loop. In an embedded system, a watchdog timer can be configured to monitor the execution of the main control loop, as illustrated in Figura 17.14. The software periodically resets the watchdog timer to indicate that it functions correctly. Suppose the software fails to reset the timer within a specified time limit (timeout period). In that case, the watchdog timer assumes that the system has encountered a fault and triggers a predefined recovery action, such as resetting the system or switching to a backup component. Watchdog timers are widely used in automotive electronics, industrial control systems, and other safety-critical applications to ensure the timely detection and recovery from faults.\n\nPont, Michael J, e Royan HL Ong. 2002. «Using watchdog timers to improve the reliability of single-processor embedded systems: Seven new patterns and a case study». In Proceedings of the First Nordic Conference on Pattern Languages of Programs, 159–200. Citeseer.\n\n\n\n\n\n\nFigura 17.14: Watchdog timer example in detecting MCU faults. Source: Ablic\n\n\n\n\n\nSoftware-level fault detection\nSoftware-level fault detection techniques rely on software algorithms and monitoring mechanisms to identify system faults. These techniques can be implemented at various levels of the software stack, including the operating system, middleware, or application level.\nRuntime monitoring and anomaly detection: Runtime monitoring involves continuously observing the behavior of the system and its components during execution (Francalanza et al. 2017). It helps detect anomalies, errors, or unexpected behavior that may indicate the presence of faults. For example, consider an ML-based image classification system deployed in a self-driving car. Runtime monitoring can be implemented to track the classification model’s performance and behavior (Mahmoud et al. 2021).\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard, Ian Cassar, Dario Della Monica, e Anna Ingólfsdóttir. 2017. «A foundation for runtime monitoring». In International Conference on Runtime Verification, 8–29. Springer.\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, e Stephen W. Keckler. 2021. «Optimizing Selective Protection for CNN Resilience». In 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE), 127–38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\nChandola, Varun, Arindam Banerjee, e Vipin Kumar. 2009. «Anomaly detection: A survey». ACM Comput. Surv. 41 (3): 1–58. https://doi.org/10.1145/1541880.1541882.\nAnomaly detection algorithms can be applied to the model’s predictions or intermediate layer activations, such as statistical outlier detection or machine learning-based approaches (e.g., One-Class SVM or Autoencoders) (Chandola, Banerjee, e Kumar 2009). Figura 17.15 shows example of anomaly detection. Suppose the monitoring system detects a significant deviation from the expected patterns, such as a sudden drop in classification accuracy or out-of-distribution samples. In that case, it can raise an alert indicating a potential fault in the model or the input data pipeline. This early detection allows for timely intervention and fault mitigation strategies to be applied.\n\n\n\n\n\n\nFigura 17.15: Examples of anomaly detection. (a) Fully supervised anomaly detection, (b) normal-only anomaly detection, (c, d, e) semi-supervised anomaly detection, (f) unsupervised anomaly detection. Source: Google\n\n\n\nConsistency checks and data validation: Consistency checks and data validation techniques ensure data integrity and correctness at different processing stages in an ML system (Lindholm et al. 2019). These checks help detect data corruption, inconsistencies, or errors that may propagate and affect the system’s behavior. Example: In a distributed ML system where multiple nodes collaborate to train a model, consistency checks can be implemented to validate the integrity of the shared model parameters. Each node can compute a checksum or hash of the model parameters before and after the training iteration, as shown in Figura 17.15. Any inconsistencies or data corruption can be detected by comparing the checksums across nodes. Additionally, range checks can be applied to the input data and model outputs to ensure they fall within expected bounds. For instance, if an autonomous vehicle’s perception system detects an object with unrealistic dimensions or velocities, it can indicate a fault in the sensor data or the perception algorithms (Wan et al. 2023).\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, e Thomas B. Schon. 2019. «Data Consistency Approach to Model Validation». #IEEE_O_ACC# 7: 59788–96. https://doi.org/10.1109/access.2019.2915109.\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, e Y Zhu. 2023. «Vpp: The vulnerability-proportional protection paradigm towards reliable autonomous machines». In Proceedings of the 5th International Workshop on Domain Specific System Architecture (DOSSA), 1–6.\n\nKawazoe Aguilera, Marcos, Wei Chen, e Sam Toueg. 1997. «Heartbeat: A timeout-free failure detector for quiescent reliable communication». In Distributed Algorithms: 11th International Workshop, WDAG’97 Saarbrücken, Germany, September 2426, 1997 Proceedings 11, 126–40. Springer.\nHeartbeat and timeout mechanisms: Heartbeat mechanisms and timeouts are commonly used to detect faults in distributed systems and ensure the liveness and responsiveness of components (Kawazoe Aguilera, Chen, e Toueg 1997). These are quite similar to the watchdog timers found in hardware. For example, in a distributed ML system, where multiple nodes collaborate to perform tasks such as data preprocessing, model training, or inference, heartbeat mechanisms can be implemented to monitor the health and availability of each node. Each node periodically sends a heartbeat message to a central coordinator or its peer nodes, indicating its status and availability. Suppose a node fails to send a heartbeat within a specified timeout period, as shown in Figura 17.16. In that case, it is considered faulty, and appropriate actions can be taken, such as redistributing the workload or initiating a failover mechanism. Timeouts can also be used to detect and handle hanging or unresponsive components. For example, if a data loading process exceeds a predefined timeout threshold, it may indicate a fault in the data pipeline, and the system can take corrective measures.\n\n\n\n\n\n\nFigura 17.16: Heartbeat messages in distributed systems. Source: GeeksforGeeks\n\n\n\n\nSoftware-implemented fault tolerance (SIFT) techniques: SIFT techniques introduce redundancy and fault detection mechanisms at the software level to improve the reliability and fault tolerance of the system (Reis et al. 2005). Example: N-version programming is a SIFT technique where multiple functionally equivalent software component versions are developed independently by different teams. This can be applied to critical components such as the model inference engine in an ML system. Multiple versions of the inference engine can be executed in parallel, and their outputs can be compared for consistency. It is considered the correct result if most versions produce the same output. If there is a discrepancy, it indicates a potential fault in one or more versions, and appropriate error-handling mechanisms can be triggered. Another example is using software-based error correction codes, such as Reed-Solomon codes (Plank 1997), to detect and correct errors in data storage or transmission, as shown in Figura 17.17. These codes add redundancy to the data, enabling detecting and correcting certain errors and enhancing the system’s fault tolerance.\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, e D. I. August. 2005. «SWIFT: Software Implemented Fault Tolerance». In International Symposium on Code Generation and Optimization, 243–54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\nPlank, James S. 1997. «A tutorial on ReedSolomon coding for fault-tolerance in RAID-like systems». Software: Practice and Experience 27 (9): 995–1012.\n\n\n\n\n\n\nFigura 17.17: n-bits representation of the Reed-Solomon codes. Source: GeeksforGeeks\n\n\n\n\n\n\n\n\n\nEsercizio 17.1: Anomaly Detection\n\n\n\n\n\nIn this Colab, play the role of an AI fault detective! You’ll build an autoencoder-based anomaly detector to pinpoint errors in heart health data. Learn how to identify malfunctions in ML systems, a vital skill for creating dependable AI. We’ll use Keras Tuner to fine-tune your autoencoder for top-notch fault detection. This experience directly links to the Robust AI chapter, demonstrating the importance of fault detection in real-world applications like healthcare and autonomous systems. Get ready to strengthen the reliability of your AI creations!\n\n\n\n\n\n\n\n\n17.3.5 Summary\nTabella 17.1 provides an extensive comparative analysis of transient, permanent, and intermittent faults. It outlines the primary characteristics or dimensions that distinguish these fault types. Here, we summarize the relevant dimensions we examined and explore the nuances that differentiate transient, permanent, and intermittent faults in greater detail.\n\n\n\nTabella 17.1: Comparison of transient, permanent, and intermittent faults.\n\n\n\n\n\n\n\n\n\n\n\nDimension\nTransient Faults\nPermanent Faults\nIntermittent Faults\n\n\n\n\nDuration\nShort-lived, temporary\nPersistent, remains until repair or replacement\nSporadic, appears and disappears intermittently\n\n\nPersistence\nDisappears after the fault condition passes\nConsistently present until addressed\nRecurs irregularly, not always present\n\n\nCauses\nExternal factors (e.g., electromagnetic interference cosmic rays)\nHardware defects, physical damage, wear-out\nUnstable hardware conditions, loose connections, aging components\n\n\nManifestation\nBit flips, glitches, temporary data corruption\nStuck-at faults, broken components, complete device failures\nOccasional bit flips, intermittent signal issues, sporadic malfunctions\n\n\nImpact on ML Systems\nIntroduces temporary errors or noise in computations\nCauses consistent errors or failures, affecting reliability\nLeads to sporadic and unpredictable errors, challenging to diagnose and mitigate\n\n\nDetection\nError detection codes, comparison with expected values\nBuilt-in self-tests, error detection codes, consistency checks\nMonitoring for anomalies, analyzing error patterns and correlations\n\n\nMitigation\nError correction codes, redundancy, checkpoint and restart\nHardware repair or replacement, component redundancy, failover mechanisms\nRobust design, environmental control, runtime monitoring, fault-tolerant techniques",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html#ml-model-robustness",
    "href": "contents/robust_ai/robust_ai.html#ml-model-robustness",
    "title": "17  Robust AI",
    "section": "17.4 ML Model Robustness",
    "text": "17.4 ML Model Robustness\n\n17.4.1 Adversarial Attacks\n\nDefinition and Characteristics\nAdversarial attacks aim to trick models into making incorrect predictions by providing them with specially crafted, deceptive inputs (called adversarial examples) (Parrish et al. 2023). By adding slight perturbations to input data, adversaries can “hack” a model’s pattern recognition and deceive it. These are sophisticated techniques where slight, often imperceptible alterations to input data can trick an ML model into making a wrong prediction, as shown in Figura 17.18.\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. «Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models». ArXiv preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\n\n\n\n\nFigura 17.18: A small adversarial noise added to the original image can make the neural network classify the image as a Guacamole instead of an Egyptian cat. Source: Sutanto\n\n\n\nOne can generate prompts that lead to unsafe images in text-to-image models like DALLE (Ramesh et al. 2021) or Stable Diffusion (Rombach et al. 2022). For example, by altering the pixel values of an image, attackers can deceive a facial recognition system into identifying a face as a different person.\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, e Ilya Sutskever. 2021. «Zero-Shot Text-to-Image Generation». In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, a cura di Marina Meila e Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, e Bjorn Ommer. 2022. «High-Resolution Image Synthesis with Latent Diffusion Models». In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\nAdversarial attacks exploit the way ML models learn and make decisions during inference. These models work on the principle of recognizing patterns in data. An adversary crafts special inputs with perturbations to mislead the model’s pattern recognition---essentially ‘hacking’ the model’s perceptions.\nAdversarial attacks fall under different scenarios:\n\nWhitebox Attacks: The attacker fully knows the target model’s internal workings, including the training data, parameters, and architecture (Ye e Hamidi 2021). This comprehensive access creates favorable conditions for attackers to exploit the model’s vulnerabilities. The attacker can use specific and subtle weaknesses to craft effective adversarial examples.\nBlackbox Attacks: In contrast to white-box attacks, black-box attacks involve the attacker having little to no knowledge of the target model (Guo et al. 2019). To carry out the attack, the adversarial actor must carefully observe the model’s output behavior.\nGreybox Attacks: These fall between blackbox and whitebox attacks. The attacker has only partial knowledge about the target model’s internal design (Xu et al. 2021). For example, the attacker could have knowledge about training data but not the architecture or parameters. In the real world, practical attacks fall under black black-box box grey-boxes.\n\n\nYe, Linfeng, e Shayan Mohajer Hamidi. 2021. «Thundernna: A white box adversarial attack». arXiv preprint arXiv:2111.12305.\n\nGuo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, e Kilian Weinberger. 2019. «Simple black-box adversarial attacks». In International conference on machine learning, 2484–93. PMLR.\n\nXu, Ying, Xu Zhong, Antonio Jimeno Yepes, e Jey Han Lau. 2021. «Grey-box adversarial attack and defence for sentiment classification». arXiv preprint arXiv:2103.11576.\nThe landscape of machine learning models is complex and broad, especially given their relatively recent integration into commercial applications. This rapid adoption, while transformative, has brought to light numerous vulnerabilities within these models. Consequently, various adversarial attack methods have emerged, each strategically exploiting different aspects of different models. Below, we highlight a subset of these methods, showcasing the multifaceted nature of adversarial attacks on machine learning models:\n\nGenerative Adversarial Networks (GANs) are deep learning models that consist of two networks competing against each other: a generator and a discriminator (Goodfellow et al. 2020). The generator tries to synthesize realistic data while the discriminator evaluates whether they are real or fake. GANs can be used to craft adversarial examples. The generator network is trained to produce inputs that the target model misclassifies. These GAN-generated images can then attack a target classifier or detection model. The generator and the target model are engaged in a competitive process, with the generator continually improving its ability to create deceptive examples and the target model enhancing its resistance to such examples. GANs provide a powerful framework for crafting complex and diverse adversarial inputs, illustrating the adaptability of generative models in the adversarial landscape.\nTransfer Learning Adversarial Attacks exploit the knowledge transferred from a pre-trained model to a target model, creating adversarial examples that can deceive both models. These attacks pose a growing concern, particularly when adversaries have knowledge of the feature extractor but lack access to the classification head (the part or layer responsible for making the final classifications). Referred to as “headless attacks,” these transferable adversarial strategies leverage the expressive capabilities of feature extractors to craft perturbations while being oblivious to the label space or training data. The existence of such attacks underscores the importance of developing robust defenses for transfer learning applications, especially since pre-trained models are commonly used (Abdelkader et al. 2020).\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, e Yoshua Bengio. 2020. «Generative adversarial networks». Commun. ACM 63 (11): 139–44. https://doi.org/10.1145/3422622.\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, e Chen Zhu. 2020. «Headless Horseman: Adversarial Attacks on Transfer Learning Models». In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nMechanisms of Adversarial Attacks\n\n\n\n\n\n\nFigura 17.19: Gradient-Based Attacks. Source: Ivezic\n\n\n\nGradient-based Attacks\nOne prominent category of adversarial attacks is gradient-based attacks. These attacks leverage the gradients of the ML model’s loss function to craft adversarial examples. The Fast Gradient Sign Method (FGSM) is a well-known technique in this category. FGSM perturbs the input data by adding small noise in the gradient direction, aiming to maximize the model’s prediction error. FGSM can quickly generate adversarial examples, as shown in Figura 17.19, by taking a single step in the gradient direction.\nAnother variant, the Projected Gradient Descent (PGD) attack, extends FGSM by iteratively applying the gradient update step, allowing for more refined and powerful adversarial examples. The Jacobian-based Saliency Map Attack (JSMA) is another gradient-based approach that identifies the most influential input features and perturbs them to create adversarial examples.\nOptimization-based Attacks\nThese attacks formulate the generation of adversarial examples as an optimization problem. The Carlini and Wagner (C&W) attack is a prominent example in this category. It aims to find the smallest perturbation that can cause misclassification while maintaining the perceptual similarity to the original input. The C&W attack employs an iterative optimization process to minimize the perturbation while maximizing the model’s prediction error.\nAnother optimization-based approach is the Elastic Net Attack to DNNs (EAD), which incorporates elastic net regularization to generate adversarial examples with sparse perturbations.\nTransfer-based Attacks\nTransfer-based attacks exploit the transferability property of adversarial examples. Transferability refers to the phenomenon where adversarial examples crafted for one ML model can often fool other models, even if they have different architectures or were trained on different datasets. This enables attackers to generate adversarial examples using a surrogate model and then transfer them to the target model without requiring direct access to its parameters or gradients. Transfer-based attacks highlight the generalization of adversarial vulnerabilities across different models and the potential for black-box attacks.\nPhysical-world Attacks\nPhysical-world attacks bring adversarial examples into the realm of real-world scenarios. These attacks involve creating physical objects or manipulations that can deceive ML models when captured by sensors or cameras. Adversarial patches, for example, are small, carefully designed patches that can be placed on objects to fool object detection or classification models. When attached to real-world objects, these patches can cause models to misclassify or fail to detect the objects accurately. Adversarial objects, such as 3D-printed sculptures or modified road signs, can also be crafted to deceive ML systems in physical environments.\nSummary\nTabella 17.2 a concise overview of the different categories of adversarial attacks, including gradient-based attacks (FGSM, PGD, JSMA), optimization-based attacks (C&W, EAD), transfer-based attacks, and physical-world attacks (adversarial patches and objects). Each attack is briefly described, highlighting its key characteristics and mechanisms.\n\n\n\nTabella 17.2: Different attack types on ML models.\n\n\n\n\n\n\n\n\n\n\nAttack Category\nAttack Name\nDescription\n\n\n\n\nGradient-based\nFast Gradient Sign Method (FGSM) Projected Gradient Descent (PGD) Jacobian-based Saliency Map Attack (JSMA)\nPerturbs input data by adding small noise in the gradient direction to maximize prediction error. Extends FGSM by iteratively applying the gradient update step for more refined adversarial examples. Identifies influential input features and perturbs them to create adversarial examples.\n\n\nOptimization-based\nCarlini and Wagner (C&W) Attack Elastic Net Attack to DNNs (EAD)\nFinds the smallest perturbation that causes misclassification while maintaining perceptual similarity. Incorporates elastic net regularization to generate adversarial examples with sparse perturbations.\n\n\nTransfer-based\nTransferability-based Attacks\nExploits the transferability of adversarial examples across different models, enabling black-box attacks.\n\n\nPhysical-world\nAdversarial Patches Adversarial Objects\nSmall, carefully designed patches placed on objects to fool object detection or classification models. Physical objects (e.g., 3D-printed sculptures, modified road signs) crafted to deceive ML systems in real-world scenarios.\n\n\n\n\n\n\nThe mechanisms of adversarial attacks reveal the intricate interplay between the ML model’s decision boundaries, the input data, and the attacker’s objectives. By carefully manipulating the input data, attackers can exploit the model’s sensitivities and blind spots, leading to incorrect predictions. The success of adversarial attacks highlights the need for a deeper understanding of ML models’ robustness and generalization properties.\nDefending against adversarial attacks requires a multifaceted approach. Adversarial training is one common defense strategy in which models are trained on adversarial examples to improve robustness. Exposing the model to adversarial examples during training teaches it to classify them correctly and become more resilient to attacks. Defensive distillation, input preprocessing, and ensemble methods are other techniques that can help mitigate the impact of adversarial attacks.\nAs adversarial machine learning evolves, researchers explore new attack mechanisms and develop more sophisticated defenses. The arms race between attackers and defenders drives the need for constant innovation and vigilance in securing ML systems against adversarial threats. Understanding the mechanisms of adversarial attacks is crucial for developing robust and reliable ML models that can withstand the ever-evolving landscape of adversarial examples.\n\n\nImpact on ML Systems\nAdversarial attacks on machine learning systems have emerged as a significant concern in recent years, highlighting the potential vulnerabilities and risks associated with the widespread adoption of ML technologies. These attacks involve carefully crafted perturbations to input data that can deceive or mislead ML models, leading to incorrect predictions or misclassifications, as shown in Figura 17.20. The impact of adversarial attacks on ML systems is far-reaching and can have serious consequences in various domains.\nOne striking example of the impact of adversarial attacks was demonstrated by researchers in 2017. They experimented with small black and white stickers on stop signs (Eykholt et al. 2017). To the human eye, these stickers did not obscure the sign or prevent its interpretability. However, when images of the sticker-modified stop signs were fed into standard traffic sign classification ML models, a shocking result emerged. The models misclassified the stop signs as speed limit signs over 85% of the time.\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, e Dawn Song. 2017. «Robust Physical-World Attacks on Deep Learning Models». ArXiv preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\nThis demonstration shed light on the alarming potential of simple adversarial stickers to trick ML systems into misreading critical road signs. The implications of such attacks in the real world are significant, particularly in the context of autonomous vehicles. If deployed on actual roads, these adversarial stickers could cause self-driving cars to misinterpret stop signs as speed limits, leading to dangerous situations, as shown in Figura 17.21. Researchers warned that this could result in rolling stops or unintended acceleration into intersections, endangering public safety.\n\n\n\n\n\n\nFigura 17.20: Adversarial example generation applied to GoogLeNet (Szegedy et al., 2014a) on ImageNet. Source: Goodfellow\n\n\n\n\n\n\n\n\n\nFigura 17.21: Graffiti on a stop sign tricked a self-driving car into thinking it was a 45 mph speed limit sign. Source: Eykholt\n\n\n\nThe case study of the adversarial stickers on stop signs provides a concrete illustration of how adversarial examples exploit how ML models recognize patterns. By subtly manipulating the input data in ways that are invisible to humans, attackers can induce incorrect predictions and create serious risks, especially in safety-critical applications like autonomous vehicles. The attack’s simplicity highlights the vulnerability of ML models to even minor changes in the input, emphasizing the need for robust defenses against such threats.\nThe impact of adversarial attacks extends beyond the degradation of model performance. These attacks raise significant security and safety concerns, particularly in domains where ML models are relied upon for critical decision-making. In healthcare applications, adversarial attacks on medical imaging models could lead to misdiagnosis or incorrect treatment recommendations, jeopardizing patient well-being (M.-J. Tsai, Lin, e Lee 2023). In financial systems, adversarial attacks could enable fraud or manipulation of trading algorithms, resulting in substantial economic losses.\n\nTsai, Min-Jen, Ping-Yi Lin, e Ming-En Lee. 2023. «Adversarial Attacks on Medical Image Classification». Cancers 15 (17): 4228. https://doi.org/10.3390/cancers15174228.\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, e Evgeny Burnaev. 2021. «Adversarial Attacks on Deep Models for Financial Transaction Records». In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, 2868–78. ACM. https://doi.org/10.1145/3447548.3467145.\nMoreover, adversarial vulnerabilities undermine the trustworthiness and interpretability of ML models. If carefully crafted perturbations can easily fool models, confidence in their predictions and decisions erodes. Adversarial examples expose the models’ reliance on superficial patterns and the inability to capture the true underlying concepts, challenging the reliability of ML systems (Fursov et al. 2021).\nDefending against adversarial attacks often requires additional computational resources and can impact the overall system performance. Techniques like adversarial training, where models are trained on adversarial examples to improve robustness, can significantly increase training time and computational requirements (Bai et al. 2021). Runtime detection and mitigation mechanisms, such as input preprocessing (Addepalli et al. 2020) or prediction consistency checks, introduce latency and affect the real-time performance of ML systems.\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, e Qian Wang. 2021. «Recent advances in adversarial training for adversarial robustness». arXiv preprint arXiv:2102.01356.\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, e R. Venkatesh Babu. 2020. «Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes». In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1020–29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\nThe presence of adversarial vulnerabilities also complicates the deployment and maintenance of ML systems. System designers and operators must consider the potential for adversarial attacks and incorporate appropriate defenses and monitoring mechanisms. Regular updates and retraining of models become necessary to adapt to new adversarial techniques and maintain system security and performance over time.\nThe impact of adversarial attacks on ML systems is significant and multifaceted. These attacks expose ML models’ vulnerabilities, from degrading model performance and raising security and safety concerns to challenging model trustworthiness and interpretability. Developers and researchers must prioritize the development of robust defenses and countermeasures to mitigate the risks posed by adversarial attacks. By addressing these challenges, we can build more secure, reliable, and trustworthy ML systems that can withstand the ever-evolving landscape of adversarial threats.\n\n\n\n\n\n\nEsercizio 17.2: Adversarial Attacks\n\n\n\n\n\nGet ready to become an AI adversary! In this Colab, you’ll become a white-box hacker, learning to craft attacks that deceive image classification models. We’ll focus on the Fast Gradient Sign Method (FGSM), where you’ll weaponize a model’s gradients against it! You’ll deliberately distort images with tiny perturbations, observing how they increasingly fool the AI more intensely. This hands-on exercise highlights the importance of building secure AI – a critical skill as AI integrates into cars and healthcare. The Colab directly ties into the Robust AI chapter of your book, moving adversarial attacks from theory into your own hands-on experience.\n\nThink you can outsmart an AI? In this Colab, learn how to trick image classification models with adversarial attacks. We’ll use methods like FGSM to change images and subtly fool the AI. Discover how to design deceptive image patches and witness the surprising vulnerability of these powerful models. This is crucial knowledge for building truly robust AI systems!\n\n\n\n\n\n\n\n17.4.2 Data Poisoning\n\nDefinition and Characteristics\nData poisoning is an attack where the training data is tampered with, leading to a compromised model (Biggio, Nelson, e Laskov 2012), as shown in Figura 17.22. Attackers can modify existing training examples, insert new malicious data points, or influence the data collection process. The poisoned data is labeled in such a way as to skew the model’s learned behavior. This can be particularly damaging in applications where ML models make automated decisions based on learned patterns. Beyond training sets, poisoning tests, and validation data can allow adversaries to boost reported model performance artificially.\n\nBiggio, Battista, Blaine Nelson, e Pavel Laskov. 2012. «Poisoning Attacks against Support Vector Machines». In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\n\n\n\n\nFigura 17.22: NightShade’s poisoning effects on Stable Diffusion. Source: TOMÉ\n\n\n\nThe process usually involves the following steps:\n\nInjection: The attacker adds incorrect or misleading examples into the training set. These examples are often designed to look normal to cursory inspection but have been carefully crafted to disrupt the learning process.\nTraining: The ML model trains on this manipulated dataset and develops skewed understandings of the data patterns.\nDeployment: Once the model is deployed, the corrupted training leads to flawed decision-making or predictable vulnerabilities the attacker can exploit.\n\nThe impact of data poisoning extends beyond classification errors or accuracy drops. In critical applications like healthcare, such alterations can lead to significant trust and safety issues (Marulli, Marrone, e Verde 2022). Later, we will discuss a few case studies of these issues.\n\nMarulli, Fiammetta, Stefano Marrone, e Laura Verde. 2022. «Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain». Journal of Sensor and Actuator Networks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\nOprea, Alina, Anoop Singhal, e Apostol Vassilev. 2022. «Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?» Computer 55 (11): 94–99. https://doi.org/10.1109/mc.2022.3190787.\nThere are six main categories of data poisoning (Oprea, Singhal, e Vassilev 2022):\n\nAvailability Attacks: These attacks aim to compromise the overall functionality of a model. They cause it to misclassify most testing samples, rendering the model unusable for practical applications. An example is label flipping, where labels of a specific, targeted class are replaced with labels from a different one.\nTargeted Attacks: In contrast to availability attacks, targeted attacks aim to compromise a small number of the testing samples. So, the effect is localized to a limited number of classes, while the model maintains the same original level of accuracy for the majority of the classes. The targeted nature of the attack requires the attacker to possess knowledge of the model’s classes, making detecting these attacks more challenging.\nBackdoor Attacks: In these attacks, an adversary targets specific patterns in the data. The attacker introduces a backdoor (a malicious, hidden trigger or pattern) into the training data, such as manipulating certain features in structured data or manipulating a pattern of pixels at a fixed position. This causes the model to associate the malicious pattern with specific labels. As a result, when the model encounters test samples that contain a malicious pattern, it makes false predictions.\nSubpopulation Attacks: Attackers selectively choose to compromise a subset of the testing samples while maintaining accuracy on the rest of the samples. You can think of these attacks as a combination of availability and targeted attacks: performing availability attacks (performance degradation) within the scope of a targeted subset. Although subpopulation attacks may seem very similar to targeted attacks, the two have clear differences:\nScope: While targeted attacks target a selected set of samples, subpopulation attacks target a general subpopulation with similar feature representations. For example, in a targeted attack, an actor inserts manipulated images of a ‘speed bump’ warning sign (with carefully crafted perturbations or patterns), which causes an autonomous car to fail to recognize such a sign and slow down. On the other hand, manipulating all samples of people with a British accent so that a speech recognition model would misclassify a British person’s speech is an example of a subpopulation attack.\nKnowledge: While targeted attacks require a high degree of familiarity with the data, subpopulation attacks require less intimate knowledge to be effective.\n\nThe characteristics of data poisoning include:\nSubtle and hard-to-detect manipulations of training data: Data poisoning often involves subtle manipulations of the training data that are carefully crafted to be difficult to detect through casual inspection. Attackers employ sophisticated techniques to ensure that the poisoned samples blend seamlessly with the legitimate data, making them easier to identify with thorough analysis. These manipulations can target specific features or attributes of the data, such as altering numerical values, modifying categorical labels, or introducing carefully designed patterns. The goal is to influence the model’s learning process while evading detection, allowing the poisoned data to subtly corrupt the model’s behavior.\nCan be performed by insiders or external attackers: Data poisoning attacks can be carried out by various actors, including malicious insiders with access to the training data and external attackers who find ways to influence the data collection or preprocessing pipeline. Insiders pose a significant threat because they often have privileged access and knowledge of the system, enabling them to introduce poisoned data without raising suspicions. On the other hand, external attackers may exploit vulnerabilities in data sourcing, crowdsourcing platforms, or data aggregation processes to inject poisoned samples into the training dataset. This highlights the importance of implementing strong access controls, data governance policies, and monitoring mechanisms to mitigate the risk of insider threats and external attacks.\nExploits vulnerabilities in data collection and preprocessing: Data poisoning attacks often exploit vulnerabilities in the machine learning pipeline’s data collection and preprocessing stages. Attackers carefully design poisoned samples to evade common data validation techniques, ensuring that the manipulated data still falls within acceptable ranges, follows expected distributions, or maintains consistency with other features. This allows the poisoned data to pass through data preprocessing steps without detection. Furthermore, poisoning attacks can take advantage of weaknesses in data preprocessing, such as inadequate data cleaning, insufficient outlier detection, or lack of integrity checks. Attackers may also exploit the lack of robust data provenance and lineage tracking mechanisms to introduce poisoned data without leaving a traceable trail. Addressing these vulnerabilities requires rigorous data validation, anomaly detection, and data provenance tracking techniques to ensure the integrity and trustworthiness of the training data.\nDisrupts the learning process and skews model behavior: Data poisoning attacks are designed to disrupt the learning process of machine learning models and skew their behavior towards the attacker’s objectives. The poisoned data is typically manipulated with specific goals, such as skewing the model’s behavior towards certain classes, introducing backdoors, or degrading overall performance. These manipulations are not random but targeted to achieve the attacker’s desired outcomes. By introducing label inconsistencies, where the manipulated samples have labels that do not align with their true nature, poisoning attacks can confuse the model during training and lead to biased or incorrect predictions. The disruption caused by poisoned data can have far-reaching consequences, as the compromised model may make flawed decisions or exhibit unintended behavior when deployed in real-world applications.\nImpacts model performance, fairness, and trustworthiness: Poisoned data in the training dataset can have severe implications for machine learning models’ performance, fairness, and trustworthiness. Poisoned data can degrade the accuracy and performance of the trained model, leading to increased misclassifications or errors in predictions. This can have significant consequences, especially in critical applications where the model’s outputs inform important decisions. Moreover, poisoning attacks can introduce biases and fairness issues, causing the model to make discriminatory or unfair decisions for certain subgroups or classes. This undermines machine learning systems’ ethical and social responsibilities and can perpetuate or amplify existing biases. Furthermore, poisoned data erodes the trustworthiness and reliability of the entire ML system. The model’s outputs become questionable and potentially harmful, leading to a loss of confidence in the system’s integrity. The impact of poisoned data can propagate throughout the entire ML pipeline, affecting downstream components and decisions that rely on the compromised model. Addressing these concerns requires robust data governance, regular model auditing, and ongoing monitoring to detect and mitigate the effects of data poisoning attacks.\n\n\nMechanisms of Data Poisoning\nData poisoning attacks can be carried out through various mechanisms, exploiting different ML pipeline vulnerabilities. These mechanisms allow attackers to manipulate the training data and introduce malicious samples that can compromise the model’s performance, fairness, or integrity. Understanding these mechanisms is crucial for developing effective defenses against data poisoning and ensuring the robustness of ML systems. Data poisoning mechanisms can be broadly categorized based on the attacker’s approach and the stage of the ML pipeline they target. Some common mechanisms include modifying training data labels, altering feature values, injecting carefully crafted malicious samples, exploiting data collection and preprocessing vulnerabilities, manipulating data at the source, poisoning data in online learning scenarios, and collaborating with insiders to manipulate data.\nEach of these mechanisms presents unique challenges and requires different mitigation strategies. For example, detecting label manipulation may involve analyzing the distribution of labels and identifying anomalies (Zhou et al. 2018), while preventing feature manipulation may require secure data preprocessing and anomaly detection techniques (Carta et al. 2020). Defending against insider threats may involve strict access control policies and monitoring of data access patterns. Moreover, the effectiveness of data poisoning attacks often depends on the attacker’s knowledge of the ML system, including the model architecture, training algorithms, and data distribution. Attackers may use adversarial machine learning or data synthesis techniques to craft samples that are more likely to bypass detection and achieve their malicious objectives.\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, e Larry S. Davis. 2018. «Learning Rich Features for Image Manipulation Detection». In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1053–61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero, e Roberto Saia. 2020. «A Local Feature Engineering Strategy to Improve Network Anomaly Detection». Future Internet 12 (10): 177. https://doi.org/10.3390/fi12100177.\n\n\n\n\n\n\nFigura 17.23: Garbage In – Garbage Out. Source: Information Matters\n\n\n\nModifying training data labels: One of the most straightforward mechanisms of data poisoning is modifying the training data labels. In this approach, the attacker selectively changes the labels of a subset of the training samples to mislead the model’s learning process as shown in Figura 17.23. For example, in a binary classification task, the attacker might flip the labels of some positive samples to negative, or vice versa. By introducing such label noise, the attacker aims to degrade the model’s performance or cause it to make incorrect predictions for specific target instances.\nAltering feature values in training data: Another mechanism of data poisoning involves altering the feature values of the training samples without modifying the labels. The attacker carefully crafts the feature values to introduce specific biases or vulnerabilities into the model. For instance, in an image classification task, the attacker might add imperceptible perturbations to a subset of images, causing the model to learn a particular pattern or association. This type of poisoning can create backdoors or trojans in the trained model, which specific input patterns can trigger.\nInjecting carefully crafted malicious samples: In this mechanism, the attacker creates malicious samples designed to poison the model. These samples are crafted to have a specific impact on the model’s behavior while blending in with the legitimate training data. The attacker might use techniques such as adversarial perturbations or data synthesis to generate poisoned samples that are difficult to detect. The attacker aims to manipulate the model’s decision boundaries by injecting these malicious samples into the training data or introducing targeted misclassifications.\nExploiting data collection and preprocessing vulnerabilities: Data poisoning attacks can also exploit the data collection and preprocessing pipeline vulnerabilities. If the data collection process is not secure or there are weaknesses in the data preprocessing steps, an attacker can manipulate the data before it reaches the training phase. For example, if data is collected from untrusted sources or issues in data cleaning or aggregation, an attacker can introduce poisoned samples or manipulate the data to their advantage.\nManipulating data at the source (e.g., sensor data): In some cases, attackers can manipulate the data at its source, such as sensor data or input devices. By tampering with the sensors or manipulating the environment in which data is collected, attackers can introduce poisoned samples or bias the data distribution. For instance, in a self-driving car scenario, an attacker might manipulate the sensors or the environment to feed misleading information into the training data, compromising the model’s ability to make safe and reliable decisions.\n\n\n\n\n\n\nFigura 17.24: Data Poisoning Attack. Source: Sikandar\n\n\n\nPoisoning data in online learning scenarios: Data poisoning attacks can also target ML systems that employ online learning, where the model is continuously updated with new data in real time. In such scenarios, an attacker can gradually inject poisoned samples over time, slowly manipulating the model’s behavior. Online learning systems are particularly vulnerable to data poisoning because they adapt to new data without extensive validation, making it easier for attackers to introduce malicious samples, as shown in Figura 17.24.\nCollaborating with insiders to manipulate data: Sometimes, data poisoning attacks can involve collaboration with insiders with access to the training data. Malicious insiders, such as employees or data providers, can manipulate the data before it is used to train the model. Insider threats are particularly challenging to detect and prevent, as the attackers have legitimate access to the data and can carefully craft the poisoning strategy to evade detection.\nThese are the key mechanisms of data poisoning in ML systems. Attackers often employ these mechanisms to make their attacks more effective and harder to detect. The risk of data poisoning attacks grows as ML systems become increasingly complex and rely on larger datasets from diverse sources. Defending against data poisoning requires a multifaceted approach. ML practitioners and system designers must be aware of the various mechanisms of data poisoning and adopt a comprehensive approach to data security and model resilience. This includes secure data collection, robust data validation, and continuous model performance monitoring. Implementing secure data collection and preprocessing practices is crucial to prevent data poisoning at the source. Data validation and anomaly detection techniques can also help identify and mitigate potential poisoning attempts. Monitoring model performance for signs of data poisoning is also essential to detect and respond to attacks promptly.\n\n\nImpact on ML Systems\nData poisoning attacks can severely affect ML systems, compromising their performance, reliability, and trustworthiness. The impact of data poisoning can manifest in various ways, depending on the attacker’s objectives and the specific mechanism used. Let’s explore each of the potential impacts in detail.\nDegradation of model performance: One of the primary impacts of data poisoning is the degradation of the model’s overall performance. By manipulating the training data, attackers can introduce noise, biases, or inconsistencies that hinder the model’s ability to learn accurate patterns and make reliable predictions. This can reduce accuracy, precision, recall, or other performance metrics. The degradation of model performance can have significant consequences, especially in critical applications such as healthcare, finance, or security, where the reliability of predictions is crucial.\nMisclassification of specific targets: Data poisoning attacks can also be designed to cause the model to misclassify specific target instances. Attackers may introduce carefully crafted poisoned samples similar to the target instances, leading the model to learn incorrect associations. This can result in the model consistently misclassifying the targeted instances, even if it performs well on other inputs. Such targeted misclassification can have severe consequences, such as causing a malware detection system to overlook specific malicious files or leading to the wrong diagnosis in a medical imaging application.\nBackdoors and trojans in trained models: Data poisoning can introduce backdoors or trojans into the trained model. Backdoors are hidden functionalities that allow attackers to trigger specific behaviors or bypass normal authentication mechanisms. On the other hand, Trojans are malicious components embedded within the model that can activate specific input patterns. By poisoning the training data, attackers can create models that appear to perform normally but contain hidden vulnerabilities that can be exploited later. Backdoors and trojans can compromise the integrity and security of the ML system, allowing attackers to gain unauthorized access, manipulate predictions, or exfiltrate sensitive information.\nBiased or unfair model outcomes: Data poisoning attacks can introduce biases or unfairness into the model’s predictions. By manipulating the training data distribution or injecting samples with specific biases, attackers can cause the model to learn and perpetuate discriminatory patterns. This can lead to unfair treatment of certain groups or individuals based on sensitive attributes such as race, gender, or age. Biased models can have severe societal implications, reinforcing existing inequalities and discriminatory practices. Ensuring fairness and mitigating biases is crucial for building trustworthy and ethical ML systems.\nIncreased false positives or false negatives: Data poisoning can also impact the model’s ability to correctly identify positive or negative instances, leading to increased false positives or false negatives. False positives occur when the model incorrectly identifies a negative instance as positive, while false negatives happen when a positive instance is misclassified as negative. The consequences of increased false positives or false negatives can be significant depending on the application. For example, in a fraud detection system, high false positives can lead to unnecessary investigations and customer frustration, while high false negatives can allow fraudulent activities to go undetected.\nCompromised system reliability and trustworthiness: Data poisoning attacks can undermine ML systems’ overall reliability and trustworthiness. When models are trained on poisoned data, their predictions become reliable and trustworthy. This can erode user confidence in the system and lead to a loss of trust in the decisions made by the model. In critical applications where ML systems are relied upon for decision-making, such as autonomous vehicles or medical diagnosis, compromised reliability can have severe consequences, putting lives and property at risk.\nAddressing the impact of data poisoning requires a proactive approach to data security, model testing, and monitoring. Organizations must implement robust measures to ensure the integrity and quality of training data, employ techniques to detect and mitigate poisoning attempts, and continuously monitor the performance and behavior of deployed models. Collaboration between ML practitioners, security experts, and domain specialists is essential to develop comprehensive strategies for preventing and responding to data poisoning attacks.\n\nCase Study 1\nIn 2017, researchers demonstrated a data poisoning attack against a popular toxicity classification model called Perspective (Hosseini et al. 2017). This ML model detects toxic comments online.\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, e Radha Poovendran. 2017. «Deceiving google’s perspective api built for detecting toxic comments». ArXiv preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\nThe researchers added synthetically generated toxic comments with slight misspellings and grammatical errors to the model’s training data. This slowly corrupted the model, causing it to misclassify increasing numbers of severely toxic inputs as non-toxic over time.\nAfter retraining on the poisoned data, the model’s false negative rate increased from 1.4% to 27% - allowing extremely toxic comments to bypass detection. The researchers warned this stealthy data poisoning could enable the spread of hate speech, harassment, and abuse if deployed against real moderation systems.\nThis case highlights how data poisoning can degrade model accuracy and reliability. For social media platforms, a poisoning attack that impairs toxicity detection could lead to the proliferation of harmful content and distrust of ML moderation systems. The example demonstrates why securing training data integrity and monitoring for poisoning is critical across application domains.\n\n\nCase Study 2\n\n\n\n\n\n\nFigura 17.25: Samples of dirty-label poison data regarding mismatched text/image pairs. Source: Shan\n\n\n\nInterestingly enough, data poisoning attacks are not always malicious (Shan et al. 2023). Nightshade, a tool developed by a team led by Professor Ben Zhao at the University of Chicago, utilizes data poisoning to help artists protect their art against scraping and copyright violations by generative AI models. Artists can use the tool to make subtle modifications to their images before uploading them online, as shown in Figura 17.25.\nWhile these changes are indiscernible to the human eye, they can significantly disrupt the performance of generative AI models when incorporated into the training data. Generative models can be manipulated to generate hallucinations and weird images. For example, with only 300 poisoned images, the University of Chicago researchers could trick the latest Stable Diffusion model into generating images of dogs that look like cats or images of cows when prompted for cars.\nAs the number of poisoned images on the internet increases, the performance of the models that use scraped data will deteriorate exponentially. First, the poisoned data is hard to detect and requires manual elimination. Second, the “poison” spreads quickly to other labels because generative models rely on connections between words and concepts as they generate images. So a poisoned image of a “car” could spread into generated images associated with words like “truck,” “train,” ” bus,” etc.\nOn the other hand, this tool can be used maliciously and can affect legitimate applications of the generative models. This shows the very challenging and novel nature of machine learning attacks.\nFigura 17.26 demonstrates the effects of different levels of data poisoning (50 samples, 100 samples, and 300 samples of poisoned images) on generating images in different categories. Notice how the images start deforming and deviating from the desired category. For example, after 300 poison samples, a car prompt generates a cow.\n\n\n\n\n\n\nFigura 17.26: Data poisoning. Source: Shan et al. (2023))\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, e Ben Y Zhao. 2023. «Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models». ArXiv preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\n\n\n\n\n\n\nEsercizio 17.3: Poisoning Attacks\n\n\n\n\n\nGet ready to explore the dark side of AI security! In this Colab, you’ll learn about data poisoning – how bad data can trick AI models into making wrong decisions. We’ll focus on a real-world attack against a Support Vector Machine (SVM), observing how the AI’s behavior changes under attack. This hands-on exercise will highlight why protecting AI systems is crucial, especially as they become more integrated into our lives. Think like a hacker, understand the vulnerability, and brainstorm how to defend our AI systems!\n\n\n\n\n\n\n\n\n17.4.3 Distribution Shifts\n\nDefinition and Characteristics\nDistribution shift refers to the phenomenon where the data distribution encountered by an ML model during deployment (inference) differs from the distribution it was trained on, as shown in Figura 17.27. This is not so much an attack as it is that the model’s robustness will vary over time. In other words, the data’s statistical properties, patterns, or underlying assumptions can change between the training and test phases.\n\n\n\n\n\n\nFigura 17.27: The curly brackets enclose the distribution shift between the environments. Here, z stands for the spurious feature, and y stands for label class. Source: Xin\n\n\n\nThe key characteristics of distribution shift include:\nDomain mismatch: The input data during inference comes from a different domain or distribution than the training data. When the input data during inference comes from a domain or distribution different from the training data, it can significantly affect the model’s performance. This is because the model has learned patterns and relationships specific to the training domain, and when applied to a different domain, those learned patterns may not hold. For example, consider a sentiment analysis model trained on movie reviews. Suppose this model is applied to analyze sentiment in tweets. In that case, it may need help to accurately classify the sentiment because the language, grammar, and context of tweets can differ from movie reviews. This domain mismatch can result in poor performance and unreliable predictions, limiting the model’s practical utility.\nTemporal drift: The data distribution evolves, leading to a gradual or sudden shift in the input characteristics. Temporal drift is important because ML models are often deployed in dynamic environments where the data distribution can change over time. If the model is not updated or adapted to these changes, its performance can gradually degrade. For instance, the patterns and behaviors associated with fraudulent activities may evolve in a fraud detection system as fraudsters adapt their techniques. If the model is not retrained or updated to capture these new patterns, it may fail to detect new types of fraud effectively. Temporal drift can lead to a decline in the model’s accuracy and reliability over time, making monitoring and addressing this type of distribution shift crucial.\nContextual changes: The ML model’s context can vary, resulting in different data distributions based on factors such as location, user behavior, or environmental conditions. Contextual changes matter because ML models are often deployed in various contexts or environments that can have different data distributions. If the model cannot generalize well to these different contexts, its performance may improve. For example, consider a computer vision model trained to recognize objects in a controlled lab environment. When deployed in a real-world setting, factors such as lighting conditions, camera angles, or background clutter can vary significantly, leading to a distribution shift. If the model is robust to these contextual changes, it may be able to accurately recognize objects in the new environment, limiting its practical utility.\nUnrepresentative training data: The training data may only partially capture the variability and diversity of the real-world data encountered during deployment. Unrepresentative training data can lead to biased or skewed models that perform poorly on real-world data. Suppose the training data needs to capture the variability and diversity of the real-world data adequately. In that case, the model may learn patterns specific to the training set but needs to generalize better to new, unseen data. This can result in poor performance, biased predictions, and limited model applicability. For instance, if a facial recognition model is trained primarily on images of individuals from a specific demographic group, it may struggle to accurately recognize faces from other demographic groups when deployed in a real-world setting. Ensuring that the training data is representative and diverse is crucial for building models that can generalize well to real-world scenarios.\n\n\n\n\n\n\nFigura 17.28: Concept drift refers to a change in data patterns and relationships over time. Source: Evidently AI\n\n\n\nDistribution shift can manifest in various forms, such as:\nCovariate shift: The distribution of the input features (covariates) changes while the conditional distribution of the target variable given the input remains the same. Covariate shift matters because it can impact the model’s ability to make accurate predictions when the input features (covariates) differ between the training and test data. Even if the relationship between the input features and the target variable remains the same, a change in the distribution of the input features can affect the model’s performance. For example, consider a model trained to predict housing prices based on features like square footage, number of bedrooms, and location. Suppose the distribution of these features in the test data significantly differs from the training data (e.g., the test data contains houses with much larger square footage). In that case, the model’s predictions may become less accurate. Addressing covariate shifts is important to ensure the model’s robustness and reliability when applied to new data.\nConcept drift: The relationship between the input features and the target variable changes over time, altering the underlying concept the model is trying to learn, as shown in Figura 17.28. Concept drift is important because it indicates changes in the fundamental relationship between the input features and the target variable over time. When the underlying concept that the model is trying to learn shifts, its performance can deteriorate if not adapted to the new concept. For instance, in a customer churn prediction model, the factors influencing customer churn may evolve due to market conditions, competitor offerings, or customer preferences. If the model is not updated to capture these changes, its predictions may become less accurate and irrelevant. Detecting and adapting to concept drift is crucial to maintaining the model’s effectiveness and alignment with evolving real-world concepts.\nDomain generalization: The model must generalize to unseen domains or distributions not present during training. Domain generalization is important because it enables ML models to be applied to new, unseen domains without requiring extensive retraining or adaptation. In real-world scenarios, training data that covers all possible domains or distributions that the model may encounter is often infeasible. Domain generalization techniques aim to learn domain-invariant features or models that can generalize well to new domains. For example, consider a model trained to classify images of animals. If the model can learn features invariant to different backgrounds, lighting conditions, or poses, it can generalize well to classify animals in new, unseen environments. Domain generalization is crucial for building models that can be deployed in diverse and evolving real-world settings.\nThe presence of a distribution shift can significantly impact the performance and reliability of ML models, as the models may need help generalizing well to the new data distribution. Detecting and adapting to distribution shifts is crucial to ensure ML systems’ robustness and practical utility in real-world scenarios.\n\n\nMechanisms of Distribution Shifts\nThe mechanisms of distribution shift, such as changes in data sources, temporal evolution, domain-specific variations, selection bias, feedback loops, and adversarial manipulations, are important to understand because they help identify the underlying causes of distribution shift. By understanding these mechanisms, practitioners can develop targeted strategies to mitigate their impact and improve the model’s robustness. Here are some common mechanisms:\n\n\n\n\n\n\nFigura 17.29: Temporal evolution. Source: Białek\n\n\n\nChanges in data sources: Distribution shifts can occur when the data sources used for training and inference differ. For example, if a model is trained on data from one sensor but deployed on data from another sensor with different characteristics, it can lead to a distribution shift.\nTemporal evolution: Over time, the underlying data distribution can evolve due to changes in user behavior, market dynamics, or other temporal factors. For instance, in a recommendation system, user preferences may shift over time, leading to a distribution shift in the input data, as shown in Figura 17.29.\nDomain-specific variations: Different domains or contexts can have distinct data distributions. A model trained on data from one domain may only generalize well to another domain with appropriate adaptation techniques. For example, an image classification model trained on indoor scenes may struggle when applied to outdoor scenes.\nSelection bias: A Distribution shift can arise from selection bias during data collection or sampling. If the training data does not represent the true population or certain subgroups are over- or underrepresented, this can lead to a mismatch between the training and test distributions.\nFeedback loops: In some cases, the predictions or actions taken by an ML model can influence future data distribution. For example, in a dynamic pricing system, the prices set by the model can impact customer behavior, leading to a shift in the data distribution over time.\nAdversarial manipulations: Adversaries can intentionally manipulate the input data to create a distribution shift and deceive the ML model. By introducing carefully crafted perturbations or generating out-of-distribution samples, attackers can exploit the model’s vulnerabilities and cause it to make incorrect predictions.\nUnderstanding the mechanisms of distribution shift is important for developing effective strategies to detect and mitigate its impact on ML systems. By identifying the sources and characteristics of the shift, practitioners can design appropriate techniques, such as domain adaptation, transfer learning, or continual learning, to improve the model’s robustness and performance under distributional changes.\n\n\nImpact on ML Systems\nDistribution shifts can significantly negatively impact the performance and reliability of ML systems. Here are some key ways in which distribution shift can affect ML models:\nDegraded predictive performance: When the data distribution encountered during inference differs from the training distribution, the model’s predictive accuracy can deteriorate. The model may need help generalizing the new data well, leading to increased errors and suboptimal performance.\nReduced reliability and trustworthiness: Distribution shift can undermine the reliability and trustworthiness of ML models. If the model’s predictions become unreliable or inconsistent due to the shift, users may lose confidence in the system’s outputs, leading to potential misuse or disuse of the model.\nBiased predictions: Distribution shift can introduce biases in the model’s predictions. If the training data does not represent the real-world distribution or certain subgroups are underrepresented, the model may make biased predictions that discriminate against certain groups or perpetuate societal biases.\nIncreased uncertainty and risk: Distribution shift introduces additional uncertainty and risk into the ML system. The model’s behavior and performance may become less predictable, making it challenging to assess its reliability and suitability for critical applications. This uncertainty can lead to increased operational risks and potential failures.\nAdaptability challenges: ML models trained on a specific data distribution may need help to adapt to changing environments or new domains. The lack of adaptability can limit the model’s usefulness and applicability in dynamic real-world scenarios where the data distribution evolves.\nMaintenance and update difficulties: Distribution shift can complicate the maintenance and updating of ML models. As the data distribution changes, the model may require frequent retraining or fine-tuning to maintain its performance. This can be time-consuming and resource-intensive, especially if the shift occurs rapidly or continuously.\nVulnerability to adversarial attacks: Distribution shift can make ML models more vulnerable to adversarial attacks. Adversaries can exploit the model’s sensitivity to distributional changes by crafting adversarial examples outside the training distribution, causing the model to make incorrect predictions or behave unexpectedly.\nTo mitigate the impact of distribution shifts, it is crucial to develop robust ML systems that detect and adapt to distributional changes. Techniques such as domain adaptation, transfer learning, and continual learning can help improve the model’s generalization ability across different distributions. ML model monitoring, testing, and updating are also necessary to ensure their performance and reliability during distribution shifts.\n\n\n\n17.4.4 Detection and Mitigation\n\nAdversarial Attacks\nAs you may recall from above, adversarial attacks pose a significant threat to the robustness and reliability of ML systems. These attacks involve crafting carefully designed inputs, known as adversarial examples, to deceive ML models and cause them to make incorrect predictions. To safeguard ML systems against adversarial attacks, developing effective techniques for detecting and mitigating these threats is crucial.\n\nAdversarial Example Detection Techniques\nDetecting adversarial examples is the first line of defense against adversarial attacks. Several techniques have been proposed to identify and flag suspicious inputs that may be adversarial.\nStatistical methods aim to detect adversarial examples by analyzing the statistical properties of the input data. These methods often compare the input data distribution to a reference distribution, such as the training data distribution or a known benign distribution. Techniques like the Kolmogorov-Smirnov (Berger e Zhou 2014) test or the Anderson-Darling test can be used to measure the discrepancy between the distributions and flag inputs that deviate significantly from the expected distribution.\n\nBerger, Vance W, e YanYan Zhou. 2014. «Kolmogorovsmirnov test: Overview». Wiley statsref: Statistics reference online.\nKernel density estimation (KDE) is a non-parametric technique used to estimate the probability density function of a dataset. In the context of adversarial example detection, KDE can be used to estimate the density of benign examples in the input space. Adversarial examples often lie in low-density regions and can be detected by comparing their estimated density to a threshold. Inputs with an estimated density below the threshold are flagged as potential adversarial examples.\nAnother technique is feature squeezing (Panda, Chakraborty, e Roy 2019), which reduces the complexity of the input space by applying dimensionality reduction or discretization. The idea behind feature squeezing is that adversarial examples often rely on small, imperceptible perturbations that can be eliminated or reduced through these transformations. Inconsistencies can be detected by comparing the model’s predictions on the original input and the squeezed input, indicating the presence of adversarial examples.\n\nPanda, Priyadarshini, Indranil Chakraborty, e Kaushik Roy. 2019. «Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks». #IEEE_O_ACC# 7: 70157–68. https://doi.org/10.1109/access.2019.2919463.\nModel uncertainty estimation techniques aim to quantify the confidence or uncertainty associated with a model’s predictions. Adversarial examples often exploit regions of high uncertainty in the model’s decision boundary. By estimating the uncertainty using techniques like Bayesian neural networks, dropout-based uncertainty estimation, or ensemble methods, inputs with high uncertainty can be flagged as potential adversarial examples.\n\n\nAdversarial Defense Strategies\nOnce adversarial examples are detected, various defense strategies can be employed to mitigate their impact and improve the robustness of ML models.\nAdversarial training is a technique that involves augmenting the training data with adversarial examples and retraining the model on this augmented dataset. Exposing the model to adversarial examples during training teaches it to classify them correctly and becomes more robust to adversarial attacks. Adversarial training can be performed using various attack methods, such as the Fast Gradient Sign Method (FGSM) or Projected Gradient Descent (PGD) (Madry et al. 2017).\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, e Adrian Vladu. 2017. «Towards deep learning models resistant to adversarial attacks». arXiv preprint arXiv:1706.06083.\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, e Ananthram Swami. 2016. «Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks». In 2016 IEEE Symposium on Security and Privacy (SP), 582–97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\nDefensive distillation (Papernot et al. 2016) is a technique that trains a second model (the student model) to mimic the behavior of the original model (the teacher model). The student model is trained on the soft labels produced by the teacher model, which are less sensitive to small perturbations. Using the student model for inference can reduce the impact of adversarial perturbations, as the student model learns to generalize better and is less sensitive to adversarial noise.\nInput preprocessing and transformation techniques aim to remove or mitigate the effect of adversarial perturbations before feeding the input to the ML model. These techniques include image denoising, JPEG compression, random resizing, padding, or applying random transformations to the input data. By reducing the impact of adversarial perturbations, these preprocessing steps can help improve the model’s robustness to adversarial attacks.\nEnsemble methods combine multiple models to make more robust predictions. The ensemble can reduce the impact of adversarial attacks by using a diverse set of models with different architectures, training data, or hyperparameters. Adversarial examples that fool one model may not fool others in the ensemble, leading to more reliable and robust predictions. Model diversification techniques, such as using different preprocessing techniques or feature representations for each model in the ensemble, can further enhance the robustness.\n\n\nRobustness Evaluation and Testing\nConduct thorough evaluation and testing to assess the effectiveness of adversarial defense techniques and measure the robustness of ML models.\nAdversarial robustness metrics quantify the model’s resilience to adversarial attacks. These metrics can include the model’s accuracy on adversarial examples, the average distortion required to fool the model, or the model’s performance under different attack strengths. By comparing these metrics across different models or defense techniques, practitioners can assess and compare their robustness levels.\nStandardized adversarial attack benchmarks and datasets provide a common ground for evaluating and comparing the robustness of ML models. These benchmarks include datasets with pre-generated adversarial examples and tools and frameworks for generating adversarial attacks. Examples of popular adversarial attack benchmarks include the MNIST-C, CIFAR-10-C, and ImageNet-C (Hendrycks e Dietterich 2019) datasets, which contain corrupted or perturbed versions of the original datasets.\n\nHendrycks, Dan, e Thomas Dietterich. 2019. «Benchmarking neural network robustness to common corruptions and perturbations». arXiv preprint arXiv:1903.12261.\nPractitioners can develop more robust and resilient ML systems by leveraging these adversarial example detection techniques, defense strategies, and robustness evaluation methods. However, it is important to note that adversarial robustness is an ongoing research area, and no single technique provides complete protection against all types of adversarial attacks. A comprehensive approach that combines multiple defense mechanisms and regular testing is essential to maintain the security and reliability of ML systems in the face of evolving adversarial threats.\n\n\n\nData Poisoning\nRecall that data poisoning is an attack that targets the integrity of the training data used to build ML models. By manipulating or corrupting the training data, attackers can influence the model’s behavior and cause it to make incorrect predictions or perform unintended actions. Detecting and mitigating data poisoning attacks is crucial to ensure the trustworthiness and reliability of ML systems, as shown in Figura 17.30.\n\nAnomaly Detection Techniques for Identifying Poisoned Data\n\n\n\n\n\n\nFigura 17.30: Malicious data injection. Source: Li\n\n\n\nStatistical outlier detection methods identify data points that deviate significantly from most data. These methods assume that poisoned data instances are likely to be statistical outliers. Techniques such as the Z-score method, Tukey’s method, or the [Mahalanobis] distance can be used to measure the deviation of each data point from the central tendency of the dataset. Data points that exceed a predefined threshold are flagged as potential outliers and considered suspicious for data poisoning.\nClustering-based methods group similar data points together based on their features or attributes. The assumption is that poisoned data instances may form distinct clusters or lie far away from the normal data clusters. By applying clustering algorithms like K-means, DBSCAN, or hierarchical clustering, anomalous clusters or data points that do not belong to any cluster can be identified. These anomalous instances are then treated as potentially poisoned data.\n\n\n\n\n\n\nFigura 17.31: Autoencoder. Source: Dertat\n\n\n\nAutoencoders are neural networks trained to reconstruct the input data from a compressed representation, as shown in Figura 17.31. They can be used for anomaly detection by learning the normal patterns in the data and identifying instances that deviate from them. During training, the autoencoder is trained on clean, unpoisoned data. At inference time, the reconstruction error for each data point is computed. Data points with high reconstruction errors are considered abnormal and potentially poisoned, as they do not conform to the learned normal patterns.\n\n\nData Sanitization and Preprocessing Techniques\nData poisoning can be avoided by cleaning data, which involves identifying and removing or correcting noisy, incomplete, or inconsistent data points. Techniques such as data deduplication, missing value imputation, and outlier removal can be applied to improve the quality of the training data. By eliminating or filtering out suspicious or anomalous data points, the impact of poisoned instances can be reduced.\nData validation involves verifying the integrity and consistency of the training data. This can include checking for data type consistency, range validation, and cross-field dependencies. By defining and enforcing data validation rules, anomalous or inconsistent data points indicative of data poisoning can be identified and flagged for further investigation.\nData provenance and lineage tracking involve maintaining a record of data’s origin, transformations, and movements throughout the ML pipeline. By documenting the data sources, preprocessing steps, and any modifications made to the data, practitioners can trace anomalies or suspicious patterns back to their origin. This helps identify potential points of data poisoning and facilitates the investigation and mitigation process.\n\n\nRobust Training Techniques\nRobust optimization techniques can be used to modify the training objective to minimize the impact of outliers or poisoned instances. This can be achieved by using robust loss functions less sensitive to extreme values, such as the Huber loss or the modified Huber loss. Regularization techniques, such as L1 or L2 regularization, can also help in reducing the model’s sensitivity to poisoned data by constraining the model’s complexity and preventing overfitting.\nRobust loss functions are designed to be less sensitive to outliers or noisy data points. Examples include the modified Huber loss, the Tukey loss (Beaton e Tukey 1974), and the trimmed mean loss. These loss functions down-weight or ignore the contribution of abnormal instances during training, reducing their impact on the model’s learning process. Robust objective functions, such as the minimax or distributionally robust objective, aim to optimize the model’s performance under worst-case scenarios or in the presence of adversarial perturbations.\n\nBeaton, Albert E., e John W. Tukey. 1974. «The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data». Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\nData augmentation techniques involve generating additional training examples by applying random transformations or perturbations to the existing data Figura 17.32. This helps in increasing the diversity and robustness of the training dataset. By introducing controlled variations in the data, the model becomes less sensitive to specific patterns or artifacts that may be present in poisoned instances. Randomization techniques, such as random subsampling or bootstrap aggregating, can also help reduce the impact of poisoned data by training multiple models on different subsets of the data and combining their predictions.\n\n\n\n\n\n\nFigura 17.32: An image of the number “3” in original form and with basic augmentations applied.\n\n\n\n\n\nSecure and Trusted Data Sourcing\nImplementing the best data collection and curation practices can help mitigate the risk of data poisoning. This includes establishing clear data collection protocols, verifying the authenticity and reliability of data sources, and conducting regular data quality assessments. Sourcing data from trusted and reputable providers and following secure data handling practices can reduce the likelihood of introducing poisoned data into the training pipeline.\nStrong data governance and access control mechanisms are essential to prevent unauthorized modifications or tampering with the training data. This involves defining clear roles and responsibilities for data access, implementing access control policies based on the principle of least privilege, and monitoring and logging data access activities. By restricting access to the training data and maintaining an audit trail, potential data poisoning attempts can be detected and investigated.\nDetecting and mitigating data poisoning attacks requires a multifaceted approach that combines anomaly detection, data sanitization, robust training techniques, and secure data sourcing practices. By implementing these measures, ML practitioners can improve the resilience of their models against data poisoning and ensure the integrity and trustworthiness of the training data. However, it is important to note that data poisoning is an active area of research, and new attack vectors and defense mechanisms continue to emerge. Staying informed about the latest developments and adopting a proactive and adaptive approach to data security is crucial for maintaining the robustness of ML systems.\n\n\n\nDistribution Shifts\n\nDetecting and Mitigating Distribution Shifts\nRecall that distribution shifts occur when the data distribution encountered by a machine learning (ML) model during deployment differs from the distribution it was trained on. These shifts can significantly impact the model’s performance and generalization ability, leading to suboptimal or incorrect predictions. Detecting and mitigating distribution shifts is crucial to ensure the robustness and reliability of ML systems in real-world scenarios.\n\n\nDetection Techniques for Distribution Shifts\nStatistical tests can be used to compare the distributions of the training and test data to identify significant differences. Techniques such as the Kolmogorov-Smirnov test or the Anderson-Darling test measure the discrepancy between two distributions and provide a quantitative assessment of the presence of distribution shift. By applying these tests to the input features or the model’s predictions, practitioners can detect if there is a statistically significant difference between the training and test distributions.\nDivergence metrics quantify the dissimilarity between two probability distributions. Commonly used divergence metrics include the Kullback-Leibler (KL) divergence and the [Jensen-Shannon (JS)] divergence. By calculating the divergence between the training and test data distributions, practitioners can assess the extent of the distribution shift. High divergence values indicate a significant difference between the distributions, suggesting the presence of a distribution shift.\nUncertainty quantification techniques, such as Bayesian neural networks or ensemble methods, can estimate the uncertainty associated with the model’s predictions. When a model is applied to data from a different distribution, its predictions may have higher uncertainty. By monitoring the uncertainty levels, practitioners can detect distribution shifts. If the uncertainty consistently exceeds a predetermined threshold for test samples, it suggests that the model is operating outside its trained distribution.\nIn addition, domain classifiers are trained to distinguish between different domains or distributions. Practitioners can detect distribution shifts by training a classifier to differentiate between the training and test domains. If the domain classifier achieves high accuracy in distinguishing between the two domains, it indicates a significant difference in the underlying distributions. The performance of the domain classifier serves as a measure of the distribution shift.\n\n\nMitigation Techniques for Distribution Shifts\n\n\n\n\n\n\nFigura 17.33: Transfer learning. Source: Bhavsar\n\n\n\nTransfer learning leverages knowledge gained from one domain to improve performance in another, as shown in Figura 17.33. By using pre-trained models or transferring learned features from a source domain to a target domain, transfer learning can help mitigate the impact of distribution shifts. The pre-trained model can be fine-tuned on a small amount of labeled data from the target domain, allowing it to adapt to the new distribution. Transfer learning is particularly effective when the source and target domains share similar characteristics or when labeled data in the target domain is scarce.\nContinual learning, also known as lifelong learning, enables ML models to learn continuously from new data distributions while retaining knowledge from previous distributions. Techniques such as elastic weight consolidation (EWC) (Kirkpatrick et al. 2017) or gradient episodic memory (GEM) (Lopez-Paz e Ranzato 2017) allow models to adapt to evolving data distributions over time. These techniques aim to balance the plasticity of the model (ability to learn from new data) with the stability of the model (retaining previously learned knowledge). By incrementally updating the model with new data and mitigating catastrophic forgetting, continual learning helps models stay robust to distribution shifts.\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017. «Overcoming catastrophic forgetting in neural networks». Proc. Natl. Acad. Sci. 114 (13): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n\nLopez-Paz, David, e Marc’Aurelio Ranzato. 2017. «Gradient episodic memory for continual learning». Adv Neural Inf Process Syst 30.\nData augmentation techniques, such as those we have seen previously, involve applying transformations or perturbations to the existing training data to increase its diversity and improve the model’s robustness to distribution shifts. By introducing variations in the data, such as rotations, translations, scaling, or adding noise, data augmentation helps the model learn invariant features and generalize better to unseen distributions. Data augmentation can be performed during training and inference to improve the model’s ability to handle distribution shifts.\nEnsemble methods combine multiple models to make predictions more robust to distribution shifts. By training models on different subsets of the data, using different algorithms, or with different hyperparameters, ensemble methods can capture diverse aspects of the data distribution. When presented with a shifted distribution, the ensemble can leverage the strengths of individual models to make more accurate and stable predictions. Techniques like bagging, boosting, or stacking can create effective ensembles.\nRegularly updating models with new data from the target distribution is crucial to mitigate the impact of distribution shifts. As the data distribution evolves, models should be retrained or fine-tuned on the latest available data to adapt to the changing patterns. Monitoring model performance and data characteristics can help detect when an update is necessary. By keeping the models up to date, practitioners can ensure they remain relevant and accurate in the face of distribution shifts.\nEvaluating models using robust metrics less sensitive to distribution shifts can provide a more reliable assessment of model performance. Metrics such as the area under the precision-recall curve (AUPRC) or the F1 score are more robust to class imbalance and can better capture the model’s performance across different distributions. Additionally, using domain-specific evaluation metrics that align with the desired outcomes in the target domain can provide a more meaningful measure of the model’s effectiveness.\nDetecting and mitigating distribution shifts is an ongoing process that requires continuous monitoring, adaptation, and improvement. By employing a combination of detection techniques and mitigation strategies, ML practitioners can proactively identify and address distribution shifts, ensuring the robustness and reliability of their models in real-world deployments. It is important to note that distribution shifts can take various forms and may require domain-specific approaches depending on the nature of the data and the application. Staying informed about the latest research and best practices in handling distribution shifts is essential for building resilient ML systems.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html#software-faults",
    "href": "contents/robust_ai/robust_ai.html#software-faults",
    "title": "17  Robust AI",
    "section": "17.5 Software Faults",
    "text": "17.5 Software Faults\n\nDefinition and Characteristics\nSoftware faults refer to defects, errors, or bugs in the runtime software frameworks and components that support the execution and deployment of ML models (Myllyaho et al. 2022). These faults can arise from various sources, such as programming mistakes, design flaws, or compatibility issues (H. Zhang 2008), and can have significant implications for ML systems’ performance, reliability, and security. Software faults in ML frameworks exhibit several key characteristics:\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen, e Tommi Mikkonen. 2022. «On misbehaviour and fault tolerance in machine learning systems». J. Syst. Software 183 (gennaio): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\nZhang, Hongyu. 2008. «On the Distribution of Software Faults». IEEE Trans. Software Eng. 34 (2): 301–2. https://doi.org/10.1109/tse.2007.70771.\n\nDiversity: Software faults can manifest in different forms, ranging from simple logic and syntax mistakes to more complex issues like memory leaks, race conditions, and integration problems. The variety of fault types adds to the challenge of detecting and mitigating them effectively.\nPropagation: In ML systems, software faults can propagate through the various layers and components of the framework. A fault in one module can trigger a cascade of errors or unexpected behavior in other parts of the system, making it difficult to pinpoint the root cause and assess the full impact of the fault.\nIntermittency: Some software faults may exhibit intermittent behavior, occurring sporadically or under specific conditions. These faults can be particularly challenging to reproduce and debug, as they may manifest inconsistently during testing or normal operation.\nInteraction with ML models: Software faults in ML frameworks can interact with the trained models in subtle ways. For example, a fault in the data preprocessing pipeline may introduce noise or bias into the model’s inputs, leading to degraded performance or incorrect predictions. Similarly, faults in the model serving component may cause inconsistencies between the training and inference environments.\nImpact on system properties: Software faults can compromise various desirable properties of ML systems, such as performance, scalability, reliability, and security. Faults may lead to slowdowns, crashes, incorrect outputs, or vulnerabilities that attackers can exploit.\nDependency on external factors: The occurrence and impact of software faults in ML frameworks often depend on external factors, such as the choice of hardware, operating system, libraries, and configurations. Compatibility issues and version mismatches can introduce faults that are difficult to anticipate and mitigate.\n\nUnderstanding the characteristics of software faults in ML frameworks is crucial for developing effective fault prevention, detection, and mitigation strategies. By recognizing the diversity, propagation, intermittency, and impact of software faults, ML practitioners can design more robust and reliable systems resilient to these issues.\n\n\nMechanisms of Software Faults in ML Frameworks\nMachine learning frameworks, such as TensorFlow, PyTorch, and sci-kit-learn, provide powerful tools and abstractions for building and deploying ML models. However, these frameworks are not immune to software faults that can impact ML systems’ performance, reliability, and correctness. Let’s explore some of the common software faults that can occur in ML frameworks:\nMemory Leaks and Resource Management Issues: Improper memory management, such as failing to release memory or close file handles, can lead to memory leaks and resource exhaustion over time. This issue is compounded by inefficient memory usage, where creating unnecessary copies of large tensors or not leveraging memory-efficient data structures can cause excessive memory consumption and degrade system performance. Additionally, failing to manage GPU memory properly can result in out-of-memory errors or suboptimal utilization of GPU resources, further exacerbating the problem as shown in Figura 17.34.\n\n\n\n\n\n\nFigura 17.34: Example of GPU out-of-the-memory and suboptimal utilization issues\n\n\n\nSynchronization and Concurrency Problems: Incorrect synchronization between threads or processes can lead to race conditions, deadlocks, or inconsistent behavior in multi-threaded or distributed ML systems. This issue is often tied to improper handling of asynchronous operations, such as non-blocking I/O or parallel data loading, which can cause synchronization issues and impact the correctness of the ML pipeline. Moreover, proper coordination and communication between distributed nodes in a cluster can result in consistency or stale data during training or inference, compromising the reliability of the ML system.\nCompatibility Issues: Mismatches between the versions of ML frameworks, libraries, or dependencies can introduce compatibility problems and runtime errors. Upgrading or changing the versions of underlying libraries without thoroughly testing the impact on the ML system can lead to unexpected behavior or breakages. Furthermore, inconsistencies between the training and deployment environments, such as differences in hardware, operating systems, or package versions, can cause compatibility issues and affect the reproducibility of ML models, making it challenging to ensure consistent performance across different platforms.\nNumerical Instability and Precision Errors: Inadequate handling of numerical instabilities, such as division by zero, underflow, or overflow, can lead to incorrect calculations or convergence issues during training. This problem is compounded by insufficient precision or rounding errors, which can accumulate over time and impact the accuracy of the ML models, especially in deep learning architectures with many layers. Moreover, improper scaling or normalization of input data can cause numerical instabilities and affect the convergence and performance of optimization algorithms, resulting in suboptimal or unreliable model performance.\nInadequate Error Handling and Exception Management: Proper error handling and exception management can prevent ML systems from crashing or behaving unexpectedly when encountering exceptional conditions or invalid inputs. Failing to catch and handle specific exceptions or relying on generic exception handling can make it difficult to diagnose and recover from errors gracefully, leading to system instability and reduced reliability. Furthermore, incomplete or misleading error messages can hinder the ability to effectively debug and resolve software faults in ML frameworks, prolonging the time required to identify and fix issues.\n\n\nImpact on ML Systems\nSoftware faults in machine learning frameworks can have significant and far-reaching impacts on ML systems’ performance, reliability, and security. Let’s explore the various ways in which software faults can affect ML systems:\nPerformance Degradation and System Slowdowns: Memory leaks and inefficient resource management can lead to gradual performance degradation over time as the system becomes increasingly memory-constrained and spends more time on garbage collection or memory swapping (Maas et al. 2024). This issue is compounded by synchronization issues and concurrency bugs, which can cause delays, reduced throughput, and suboptimal utilization of computational resources, especially in multi-threaded or distributed ML systems. Furthermore, compatibility problems or inefficient code paths can introduce additional overhead and slowdowns, affecting the overall performance of the ML system.\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi Javanmard, Kathryn S. McKinley, e Colin Raffel. 2024. «Combining Machine Learning and Lifetime-Based Resource Management for Memory Allocation and Beyond». Commun. ACM 67 (4): 87–96. https://doi.org/10.1145/3611018.\nIncorrect Predictions or Outputs: Software faults in data preprocessing, feature engineering, or model evaluation can introduce biases, noise, or errors propagating through the ML pipeline and resulting in incorrect predictions or outputs. Over time, numerical instabilities, precision errors, or rounding issues can accumulate and lead to degraded accuracy or convergence problems in the trained models. Moreover, faults in the model serving or inference components can cause inconsistencies between the expected and actual outputs, leading to incorrect or unreliable predictions in production.\nReliability and Stability Issues: Software faults can cause Unparalleled exceptions, crashes, or sudden terminations that can compromise the reliability and stability of ML systems, especially in production environments. Intermittent or sporadic faults can be difficult to reproduce and diagnose, leading to unpredictable behavior and reduced confidence in the ML system’s outputs. Additionally, faults in checkpointing, model serialization, or state management can cause data loss or inconsistencies, affecting the reliability and recoverability of the ML system.\nSecurity Vulnerabilities: Software faults, such as buffer overflows, injection vulnerabilities, or improper access control, can introduce security risks and expose the ML system to potential attacks or unauthorized access. Adversaries may exploit faults in the preprocessing or feature extraction stages to manipulate the input data and deceive the ML models, leading to incorrect or malicious behavior. Furthermore, inadequate protection of sensitive data, such as user information or confidential model parameters, can lead to data breaches or privacy violations (Q. Li et al. 2023).\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, e Bingsheng He. 2023. «A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection». IEEE Trans. Knowl. Data Eng. 35 (4): 3347–66. https://doi.org/10.1109/tkde.2021.3124599.\nDifficulty in Reproducing and Debugging: Software faults can make it challenging to reproduce and debug issues in ML systems, especially when the faults are intermittent or dependent on specific runtime conditions. Incomplete or ambiguous error messages, coupled with the complexity of ML frameworks and models, can prolong the debugging process and hinder the ability to identify and fix the underlying faults. Moreover, inconsistencies between development, testing, and production environments can make reproducing and diagnosing faults in specific contexts difficult.\nIncreased Development and Maintenance Costs Software faults can lead to increased development and maintenance costs, as teams spend more time and resources debugging, fixing, and validating the ML system. The need for extensive testing, monitoring, and fault-tolerant mechanisms to mitigate the impact of software faults can add complexity and overhead to the ML development process. Frequent patches, updates, and bug fixes to address software faults can disrupt the development workflow and require additional effort to ensure the stability and compatibility of the ML system.\nUnderstanding the potential impact of software faults on ML systems is crucial for prioritizing testing efforts, implementing fault-tolerant designs, and establishing effective monitoring and debugging practices. By proactively addressing software faults and their consequences, ML practitioners can build more robust, reliable, and secure ML systems that deliver accurate and trustworthy results.\n\n\nDetection and Mitigation\nDetecting and mitigating software faults in machine learning frameworks is essential to ensure ML systems’ reliability, performance, and security. Let’s explore various techniques and approaches that can be employed to identify and address software faults effectively:\nThorough Testing and Validation: Comprehensive unit testing of individual components and modules can verify their correctness and identify potential faults early in development. Integration testing validates the interaction and compatibility between different components of the ML framework, ensuring seamless integration. Systematic testing of edge cases, boundary conditions, and exceptional scenarios helps uncover hidden faults and vulnerabilities. Continuous testing and regression testing as shown in Figura 17.35 detect faults introduced by code changes or updates to the ML framework.\n\n\n\n\n\n\nFigura 17.35: Automated regression testing. Source: UTOR\n\n\n\nStatic Code Analysis and Linting: Utilizing static code analysis tools automatically identifies potential coding issues, such as syntax errors, undefined variables, or security vulnerabilities. Enforcing coding standards and best practices through linting tools maintains code quality and reduces the likelihood of common programming mistakes. Conducting regular code reviews allows manual inspection of the codebase, identification of potential faults, and ensures adherence to coding guidelines and design principles.\nRuntime Monitoring and Logging: Implementing comprehensive logging mechanisms captures relevant information during runtime, such as input data, model parameters, and system events. Monitoring key performance metrics, resource utilization, and error rates helps detect anomalies, performance bottlenecks, or unexpected behavior. Employing runtime assertion checks and invariants validates assumptions and detects violations of expected conditions during program execution. Utilizing profiling tools identifies performance bottlenecks, memory leaks, or inefficient code paths that may indicate the presence of software faults.\nFault-Tolerant Design Patterns: Implementing error handling and exception management mechanisms enables graceful handling and recovery from exceptional conditions or runtime errors. Employing redundancy and failover mechanisms, such as backup systems or redundant computations, ensures the availability and reliability of the ML system in the presence of faults. Designing modular and loosely coupled architectures minimizes the propagation and impact of faults across different components of the ML system. Utilizing checkpointing and recovery mechanisms (Eisenman et al. 2022) allows the system to resume from a known stable state in case of failures or interruptions.\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, e Murali Annavaram. 2022. «Check-N-Run: A checkpointing system for training deep learning recommendation models». In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), 929–43.\nRegular Updates and Patches: Staying up to date with the latest versions and patches of the ML frameworks, libraries, and dependencies provides benefits from bug fixes, security updates, and performance improvements. Monitoring release notes, security advisories, and community forums inform practitioners about known issues, vulnerabilities, or compatibility problems in the ML framework. Establishing a systematic process for testing and validating updates and patches before applying them to production systems ensures stability and compatibility.\nContainerization and Isolation: Leveraging containerization technologies, such as Docker or Kubernetes, encapsulates ML components and their dependencies in isolated environments. Utilizing containerization ensures consistent and reproducible runtime environments across development, testing, and production stages, reducing the likelihood of compatibility issues or environment-specific faults. Employing isolation techniques, such as virtual environments or sandboxing, prevents faults or vulnerabilities in one component from affecting other parts of the ML system.\nAutomated Testing and Continuous Integration/Continuous Deployment (CI/CD): Implement automated testing frameworks and scripts, execute comprehensive test suites, and catch faults early in development. Integrating automated testing into the CI/CD pipeline, as shown in Figura 17.36, ensures that code changes are thoroughly tested before being merged or deployed to production. Utilizing continuous monitoring and automated alerting systems detects and notifies developers and operators about potential faults or anomalies in real-time.\n\n\n\n\n\n\nFigura 17.36: Continuous Integration/Continuous Deployment (CI/CD) procedure. Source: geeksforgeeks\n\n\n\nAdopting a proactive and systematic approach to fault detection and mitigation can significantly improve ML systems’ robustness, reliability, and maintainability. By investing in comprehensive testing, monitoring, and fault-tolerant design practices, organizations can minimize the impact of software faults and ensure their ML systems’ smooth operation in production environments.\n\n\n\n\n\n\nEsercizio 17.4: Fault Tolerance\n\n\n\n\n\nGet ready to become an AI fault-fighting superhero! Software glitches can derail machine learning systems, but in this Colab, you’ll learn how to make them resilient. We’ll simulate software faults to see how AI can break, then explore techniques to save your ML model’s progress, like checkpoints in a game. You’ll see how to train your AI to bounce back after a crash, ensuring it stays on track. This is crucial for building reliable, trustworthy AI, especially in critical applications. So gear up because this Colab directly connects with the Robust AI chapter – you’ll move from theory to hands-on troubleshooting and build AI systems that can handle the unexpected!",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html#tools-and-frameworks",
    "href": "contents/robust_ai/robust_ai.html#tools-and-frameworks",
    "title": "17  Robust AI",
    "section": "17.6 Tools and Frameworks",
    "text": "17.6 Tools and Frameworks\nGiven the significance or importance of developing robust AI systems, in recent years, researchers and practitioners have developed a wide range of tools and frameworks to understand how hardware faults manifest and propagate to impact ML systems. These tools and frameworks play a crucial role in evaluating the resilience of ML systems to hardware faults by simulating various fault scenarios and analyzing their impact on the system’s performance. This enables designers to identify potential vulnerabilities and develop effective mitigation strategies, ultimately creating more robust and reliable ML systems that can operate safely despite hardware faults. This section provides an overview of widely used fault models in the literature and the tools and frameworks developed to evaluate the impact of such faults on ML systems.\n\n17.6.1 Fault Models and Error Models\nAs discussed previously, hardware faults can manifest in various ways, including transient, permanent, and intermittent faults. In addition to the type of fault under study, how the fault manifests is also important. For example, does the fault happen in a memory cell or during the computation of a functional unit? Is the impact on a single bit, or does it impact multiple bits? Does the fault propagate all the way and impact the application (causing an error), or does it get masked quickly and is considered benign? All these details impact what is known as the fault model, which plays a major role in simulating and measuring what happens to a system when a fault occurs.\nTo effectively study and understand the impact of hardware faults on ML systems, it is essential to understand the concepts of fault models and error models. A fault model describes how a hardware fault manifests itself in the system, while an error model represents how the fault propagates and affects the system’s behavior.\nFault models can be categorized based on various characteristics:\n\nDuration: Transient faults occur briefly and then disappear, while permanent faults persist indefinitely. Intermittent faults occur sporadically and may be difficult to diagnose.\nLocation: Faults can occur in hardware parts, such as memory cells, functional units, or interconnects.\nGranularity: Faults can affect a single bit (e.g., bitflip) or multiple bits (e.g., burst errors) within a hardware component.\n\nOn the other hand, error models describe how a fault propagates through the system and manifests as an error. An error may cause the system to deviate from its expected behavior, leading to incorrect results or even system failures. Error models can be defined at different levels of abstraction, from the hardware level (e.g., register-level bitflips) to the software level (e.g., corrupted weights or activations in an ML model).\nThe fault model (or error model, typically the more applicable terminology in understanding the robustness of an ML system) plays a major role in simulating and measuring what happens to a system when a fault occurs. The chosen model informs the assumptions made about the system being studied. For example, a system focusing on single-bit transient errors (Sangchoolie, Pattabiraman, e Karlsson 2017) would not be well-suited to understand the impact of permanent, multi-bit flip errors (Wilkening et al. 2014), as it is designed assuming a different model altogether.\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva Gurumurthi, e David R. Kaeli. 2014. «Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults». In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 293–305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\nFurthermore, implementing an error model is also an important consideration, particularly regarding where an error is said to occur in the compute stack. For instance, a single-bit flip model at the architectural register level differs from a single-bit flip in the weight of a model at the PyTorch level. Although both target a similar error model, the former would usually be modeled in an architecturally accurate simulator (like gem5 [binkert2011gem5]), which captures error propagation compared to the latter, focusing on value propagation through a model.\nRecent research has shown that certain characteristics of error models may exhibit similar behaviors across different levels of abstraction (Sangchoolie, Pattabiraman, e Karlsson 2017) (Papadimitriou e Gizopoulos 2021). For example, single-bit errors are generally more problematic than multi-bit errors, regardless of whether they are modeled at the hardware or software level. However, other characteristics, such as error masking (Mohanram e Touba 2003) as shown in Figura 17.37, may not always be accurately captured by software-level models, as they can hide underlying system effects.\n\nSangchoolie, Behrooz, Karthik Pattabiraman, e Johan Karlsson. 2017. «One Bit is (Not) Enough: An Empirical Study of the Impact of Single and Multiple Bit-Flip Errors». In 2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 97–108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\nPapadimitriou, George, e Dimitris Gizopoulos. 2021. «Demystifying the System Vulnerability Stack: Transient Fault Effects Across the Layers». In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), 902–15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\nMohanram, K., e N. A. Touba. 2003. «Partial error masking to reduce soft error failure rate in logic circuits». In Proceedings. 16th IEEE Symposium on Computer Arithmetic, 433–40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\n\n\n\n\nFigura 17.37: Example of error masking in microarchitectural components (Ko 2021)\n\n\nKo, Yohan. 2021. «Characterizing System-Level Masking Effects against Soft Errors». Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nSome tools, such as Fidelity (He, Balaprakash, e Li 2020), aim to bridge the gap between hardware-level and software-level error models by mapping patterns between the two levels of abstraction (Cheng et al. 2016). This allows for more accurate modeling of hardware faults in software-based tools, essential for developing robust and reliable ML systems. Lower-level tools typically represent more accurate error propagation characteristics but must be faster in simulating many errors due to the complex nature of hardware system designs. On the other hand, higher-level tools, such as those implemented in ML frameworks like PyTorch or TensorFlow, which we will discuss soon in the later sections, are often faster and more efficient for evaluating the robustness of ML systems.\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher, Hyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. «Clear: uC/u ross u-L/u ayer uE/u xploration for uA/u rchitecting uR/u esilience - Combining hardware and software techniques to tolerate soft errors in processor cores». In Proceedings of the 53rd Annual Design Automation Conference, 1–6. ACM. https://doi.org/10.1145/2897937.2897996.\nIn the following subsections, we will discuss various hardware-based and software-based fault injection methods and tools, highlighting their capabilities, limitations, and the fault and error models they support.\n\n\n17.6.2 Hardware-based Fault Injection\nAn error injection tool is a tool that allows the user to implement a particular error model, such as a transient single-bit flip during inference Figura 17.38. Most error injection tools are software-based, as software-level tools are faster for ML robustness studies. However, hardware-based fault injection methods are still important for grounding the higher-level error models, as they are considered the most accurate way to study the impact of faults on ML systems by directly manipulating the hardware to introduce faults. These methods allow researchers to observe the system’s behavior under real-world fault conditions. Both software-based and hardware-based error injection tools are described in this section in more detail.\n\n\n\n\n\n\nFigura 17.38: Hardware errors can occur due to a variety of reasons and at different times and/or locations in a system, which can be explored when studying the impact of hardware-based errors on systems (Ahmadilivani et al. 2024)\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, e Maksim Jenihhin. 2024. «A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks». ACM Comput. Surv. 56 (6): 1–39. https://doi.org/10.1145/3638242.\n\n\n\nMethods\nTwo of the most common hardware-based fault injection methods are FPGA-based fault injection and radiation or beam testing.\nFPGA-based Fault Injection: Field-Programmable Gate Arrays (FPGAs) are reconfigurable integrated circuits that can be programmed to implement various hardware designs. In the context of fault injection, FPGAs offer high precision and accuracy, as researchers can target specific bits or sets of bits within the hardware. By modifying the FPGA configuration, faults can be introduced at specific locations and times during the execution of an ML model. FPGA-based fault injection allows for fine-grained control over the fault model, enabling researchers to study the impact of different types of faults, such as single-bit flips or multi-bit errors. This level of control makes FPGA-based fault injection a valuable tool for understanding the resilience of ML systems to hardware faults.\nRadiation or Beam Testing: Radiation or beam testing (Velazco, Foucard, e Peronnard 2010) involves exposing the hardware running an ML model to high-energy particles, such as protons or neutrons as illustrated in Figura 17.39. These particles can cause bitflips or other types of faults in the hardware, mimicking the effects of real-world radiation-induced faults. Beam testing is widely regarded as a highly accurate method for measuring the error rate induced by particle strikes on a running application. It provides a realistic representation of the faults in real-world environments, particularly in applications exposed to high radiation levels, such as space systems or particle physics experiments. However, unlike FPGA-based fault injection, beam testing could be more precise in targeting specific bits or components within the hardware, as it might be difficult to aim the beam of particles to a particular bit in the hardware. Despite being quite expensive from a research standpoint, beam testing is a well-regarded industry practice for reliability.\n\nVelazco, Raoul, Gilles Foucard, e Paul Peronnard. 2010. «Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in SRAM-Based FPGAs». IEEE Trans. Nucl. Sci. 57 (6): 3500–3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\n\n\n\n\n\nFigura 17.39: Radiation test setup for semiconductor components (Lee et al. 2022) Source: JD Instrument\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang, e Seongik Cho. 2022. «Design of Radiation-Tolerant High-Speed Signal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear Explosion». Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\n\n\nLimitations\nDespite their high accuracy, hardware-based fault injection methods have several limitations that can hinder their widespread adoption:\nCost: FPGA-based fault injection and beam testing require specialized hardware and facilities, which can be expensive to set up and maintain. The cost of these methods can be a significant barrier for researchers and organizations with limited resources.\nScalability: Hardware-based methods are generally slower and less scalable than software-based methods. Injecting faults and collecting data on hardware can take time, limiting the number of experiments performed within a given timeframe. This can be particularly challenging when studying the resilience of large-scale ML systems or conducting statistical analyses that require many fault injection experiments.\nFlexibility: Hardware-based methods may not be as flexible as software-based methods in terms of the range of fault models and error models they can support. Modifying the hardware configuration or the experimental setup to accommodate different fault models can be more challenging and time-consuming than software-based methods.\nDespite these limitations, hardware-based fault injection methods remain essential tools for validating the accuracy of software-based methods and for studying the impact of faults on ML systems in realistic settings. By combining hardware-based and software-based methods, researchers can gain a more comprehensive understanding of ML systems’ resilience to hardware faults and develop effective mitigation strategies.\n\n\n\n17.6.3 Software-based Fault Injection Tools\nWith the rapid development of ML frameworks in recent years, software-based fault injection tools have gained popularity in studying the resilience of ML systems to hardware faults. These tools simulate the effects of hardware faults by modifying the software representation of the ML model or the underlying computational graph. The rise of ML frameworks such as TensorFlow, PyTorch, and Keras has facilitated the development of fault injection tools that are tightly integrated with these frameworks, making it easier for researchers to conduct fault injection experiments and analyze the results.\n\nAdvantages and Trade-offs\nSoftware-based fault injection tools offer several advantages over hardware-based methods:\nSpeed: Software-based tools are generally faster than hardware-based methods, as they do not require the modification of physical hardware or the setup of specialized equipment. This allows researchers to conduct more fault injection experiments in a shorter time, enabling more comprehensive analyses of the resilience of ML systems.\nFlexibility: Software-based tools are more flexible than hardware-based methods in terms of the range of fault and error models they can support. Researchers can easily modify the fault injection tool’s software implementation to accommodate different fault models or to target specific components of the ML system.\nAccessibility: Software-based tools are more accessible than hardware-based methods, as they do not require specialized hardware or facilities. This makes it easier for researchers and practitioners to conduct fault injection experiments and study the resilience of ML systems, even with limited resources.\n\n\nLimitations\nSoftware-based fault injection tools also have some limitations compared to hardware-based methods:\nAccuracy: Software-based tools may not always capture the full range of effects that hardware faults can have on the system. As these tools operate at a higher level of abstraction, they may need to catch up on some of the low-level hardware interactions and error propagation mechanisms that can impact the behavior of the ML system.\nFidelity: Software-based tools may provide a different level of Fidelity than hardware-based methods in terms of representing real-world fault conditions. The accuracy of the results obtained from software-based fault injection experiments may depend on how closely the software model approximates the actual hardware behavior.\n\n\n\n\n\n\nFigura 17.40: Comparison of techniques at layers of abstraction. Source: MAVFI\n\n\n\n\n\nTypes of Fault Injection Tools\nSoftware-based fault injection tools can be categorized based on their target frameworks or use cases. Here, we will discuss some of the most popular tools in each category:\nAres (Reagen et al. 2018), a fault injection tool initially developed for the Keras framework in 2018, emerged as one of the first tools to study the impact of hardware faults on deep neural networks (DNNs) in the context of the rising popularity of ML frameworks in the mid-to-late 2010s. The tool was validated against a DNN accelerator implemented in silicon, demonstrating its effectiveness in modeling hardware faults. Ares provides a comprehensive study on the impact of hardware faults in both weights and activation values, characterizing the effects of single-bit flips and bit-error rates (BER) on hardware structures. Later, the Ares framework was extended to support the PyTorch ecosystem, enabling researchers to investigate hardware faults in a more modern setting and further extending its utility in the field.\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, e Gu-Yeon Wei. 2018. «Ares: A framework for quantifying the resilience of deep neural networks». In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\n\n\n\n\n\nFigura 17.41: Hardware bitflips in ML workloads can cause phantom objects and misclassifications, which can erroneously be used downstream by larger systems, such as in autonomous driving. Shown above is a correct and faulty version of the same image using the PyTorchFI injection framework.\n\n\n\nPyTorchFI (Mahmoud et al. 2020), a fault injection tool specifically designed for the PyTorch framework, was developed in 2020 in collaboration with Nvidia Research. It enables the injection of faults into the weights, activations, and gradients of PyTorch models, supporting a wide range of fault models. By leveraging the GPU acceleration capabilities of PyTorch, PyTorchFI provides a fast and efficient implementation for conducting fault injection experiments on large-scale ML systems, as shown in Figura 17.41. The tool’s speed and ease of use have led to widespread adoption in the community, resulting in multiple developer-led projects, such as PyTorchALFI by Intel xColabs, which focuses on safety in automotive environments. Follow-up PyTorch-centric tools for fault injection include Dr. DNA by Meta (Ma et al. 2024) (which further facilitates the Pythonic programming model for ease of use), and the GoldenEye framework (Mahmoud et al. 2022), which incorporates novel numerical datatypes (such as AdaptivFloat (Tambe et al. 2020) and BlockFloat in the context of hardware bit flips.\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez Vicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, e Siva Kumar Sastry Hari. 2020. «PyTorchFI: A Runtime Perturbation Tool for DNNs». In 2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W), 25–31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore, Sriram Sankar, e Xun Jiao. 2024. «Dr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations». In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 239–52. ACM. https://doi.org/10.1145/3620666.3651349.\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, e Gu-Yeon Wei. 2022. «GoldenEye: A Platform for Evaluating Emerging Numerical Data Formats in DNN Accelerators». In 2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 206–14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, e Gu-Yeon Wei. 2020. «Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference». In 2020 57th ACM/IEEE Design Automation Conference (DAC), 1–6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2020. «TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications». In 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE), 426–35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, e Nathan DeBardeleben. 2019. «iBinFI/i: an efficient fault injector for safety-critical machine learning systems». In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC ’19. New York, NY, USA: ACM. https://doi.org/10.1145/3295500.3356177.\nTensorFI (Chen et al. 2020), or the TensorFlow Fault Injector, is a fault injection tool developed specifically for the TensorFlow framework. Analogous to Ares and PyTorchFI, TensorFI is considered the state-of-the-art tool for ML robustness studies in the TensorFlow ecosystem. It allows researchers to inject faults into the computational graph of TensorFlow models and study their impact on the model’s performance, supporting a wide range of fault models. One of the key benefits of TensorFI is its ability to evaluate the resilience of various ML models, not just DNNs. Further advancements, such as BinFi (Chen et al. 2019), provide a mechanism to speed up error injection experiments by focusing on the “important” bits in the system, accelerating the process of ML robustness analysis and prioritizing the critical components of a model.\nNVBitFI (T. Tsai et al. 2021), a general-purpose fault injection tool developed by Nvidia for their GPU platforms, operates at a lower level compared to framework-specific tools like Ares, PyTorchFI, and TensorFlow. While these tools focus on various deep learning platforms to implement and perform robustness analysis, NVBitFI targets the underlying hardware assembly code for fault injection. This allows researchers to inject faults into any application running on Nvidia GPUs, making it a versatile tool for studying the resilience of ML systems and other GPU-accelerated applications. By enabling users to inject errors at the architectural level, NVBitFI provides a more general-purpose fault model that is not restricted to just ML models. As Nvidia’s GPU systems are commonly used in many ML-based systems, NVBitFI is a valuable tool for comprehensive fault injection analysis across various applications.\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa, e Stephen W. Keckler. 2021. «NVBitFI: Dynamic Fault Injection for GPUs». In 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 284–91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\nDomain-specific Examples\nDomain-specific fault injection tools have been developed to address various ML application domains’ unique challenges and requirements, such as autonomous vehicles and robotics. This section highlights three domain-specific fault injection tools: DriveFI and PyTorchALFI for autonomous vehicles and MAVFI for uncrewed aerial vehicles (UAVs). These tools enable researchers to inject hardware faults into these complex systems’ perception, control, and other subsystems, allowing them to study the impact of faults on system performance and safety. The development of these software-based fault injection tools has greatly expanded the capabilities of the ML community to develop more robust and reliable systems that can operate safely and effectively in the presence of hardware faults.\nDriveFI (Jha et al. 2019) is a fault injection tool designed for autonomous vehicles. It enables the injection of hardware faults into the perception and control pipelines of autonomous vehicle systems, allowing researchers to study the impact of these faults on the system’s performance and safety. DriveFI has been integrated with industry-standard autonomous driving platforms, such as Nvidia DriveAV and Baidu Apollo, making it a valuable tool for evaluating the resilience of autonomous vehicle systems.\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, e Ravishankar K. Iyer. 2019. «ML-Based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection». In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 112–24. IEEE; IEEE. https://doi.org/10.1109/dsn.2019.00025.\n\nGräfe, Ralf, Qutub Syed Sha, Florian Geissler, e Michael Paulitsch. 2023. «Large-Scale Application of Fault Injection into PyTorch Models -an Extension to PyTorchFI for Validation Efficiency». In 2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S), 56–62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\nPyTorchALFI (Gräfe et al. 2023) is an extension of PyTorchFI developed by Intel xColabs for the autonomous vehicle domain. It builds upon PyTorchFI’s fault injection capabilities. It adds features specifically tailored for evaluating the resilience of autonomous vehicle systems, such as the ability to inject faults into the camera and LiDAR sensor data.\nMAVFI (Hsiao et al. 2023) is a fault injection tool designed for the robotics domain, specifically for uncrewed aerial vehicles (UAVs). MAVFI is built on top of the Robot Operating System (ROS) framework and allows researchers to inject faults into the various components of a UAV system, such as sensors, actuators, and control algorithms. By evaluating the impact of these faults on the UAV’s performance and stability, researchers can develop more resilient and fault-tolerant UAV systems.\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, e Vijay Janapa Reddi. 2023. «MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles». In 2023 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\nThe development of software-based fault injection tools has greatly expanded the capabilities of researchers and practitioners to study the resilience of ML systems to hardware faults. By leveraging the speed, flexibility, and accessibility of these tools, the ML community can develop more robust and reliable systems that can operate safely and effectively in the presence of hardware faults.\n\n\n\n\n17.6.4 Bridging the Gap between Hardware and Software Error Models\nWhile software-based fault injection tools offer many advantages in speed, flexibility, and accessibility, they may not always accurately capture the full range of effects that hardware faults can have on the system. This is because software-based tools operate at a higher level of abstraction than hardware-based methods and may miss some of the low-level hardware interactions and error propagation mechanisms that can impact the behavior of the ML system.\nAs Bolchini et al. (2023) illustrates in their work, hardware errors can manifest in complex spatial distribution patterns that are challenging to fully replicate with software-based fault injection alone. They identify four distinct patterns: (a) single point, where the fault corrupts a single value in a feature map; (b) same row, where the fault corrupts a partial or entire row in a single feature map; (c) bullet wake, where the fault corrupts the same location across multiple feature maps; and (d) shatter glass, which combines the effects of same row and bullet wake patterns, as shown in Figura 17.42. These intricate error propagation mechanisms highlight the need for hardware-aware fault injection techniques to accurately assess the resilience of ML systems.\n\n\n\n\n\n\nFigura 17.42: Hardware errors may manifest themselves in different ways at the software level, as classified by Bolchini et al. (Bolchini et al. 2023)\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, e Alessandro Toschi. 2023. «Fast and Accurate Error Simulation for CNNs Against Soft Errors». IEEE Trans. Comput. 72 (4): 984–97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nResearchers have developed tools to address this issue by bridging the gap between low-level hardware error models and higher-level software error models. One such tool is Fidelity, designed to map patterns between hardware-level faults and their software-level manifestations.\n\nFidelity: Bridging the Gap\nFidelity (He, Balaprakash, e Li 2020) is a tool for accurately modeling hardware faults in software-based fault injection experiments. It achieves this by carefully studying the relationship between hardware-level faults and their impact on the software representation of the ML system.\n\nHe, Yi, Prasanna Balaprakash, e Yanjing Li. 2020. «FIdelity: Efficient Resilience Analysis Framework for Deep Learning Accelerators». In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 270–81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\nThe key insights behind Fidelity are:\n\nFault Propagation: Fidelity models how faults propagate through the hardware and manifest as errors in the software-visible state of the system. By understanding these propagation patterns, Fidelity can more accurately simulate the effects of hardware faults in software-based experiments.\nFault Equivalence: Fidelity identifies equivalent classes of hardware faults that produce similar software-level errors. This allows researchers to design software-based fault models that are representative of the underlying hardware faults without the need to model every possible hardware fault individually.\nLayered Approach: Fidelity employs a layered approach to fault modeling, where the effects of hardware faults are propagated through multiple levels of abstraction, from the hardware to the software level. This approach ensures that the software-based fault models are grounded in the actual behavior of the hardware.\n\nBy incorporating these insights, Fidelity enables software-based fault injection tools to capture the effects of hardware faults on ML systems accurately. This is particularly important for safety-critical applications, where the system’s resilience to hardware faults is paramount.\n\n\nImportance of Capturing True Hardware Behavior\nCapturing true hardware behavior in software-based fault injection tools is crucial for several reasons:\n\nAccuracy: By accurately modeling the effects of hardware faults, software-based tools can provide more reliable insights into the resilience of ML systems. This is essential for designing and validating fault-tolerant systems that can operate safely and effectively in the presence of hardware faults.\nReproducibility: When software-based tools accurately capture hardware behavior, fault injection experiments become more reproducible across different platforms and environments. This is important for the scientific study of ML system resilience, as it allows researchers to compare and validate results across different studies and implementations.\nEfficiency: Software-based tools that capture true hardware behavior can be more efficient in their fault injection experiments by focusing on the most representative and impactful fault models. This allows researchers to cover a wider range of fault scenarios and system configurations with limited computational resources.\nMitigation Strategies: Understanding how hardware faults manifest at the software level is crucial for developing effective mitigation strategies. By accurately capturing hardware behavior, software-based fault injection tools can help researchers identify the most vulnerable components of the ML system and design targeted hardening techniques to improve resilience.\n\nTools like Fidelity are vital in advancing the state-of-the-art in ML system resilience research. These tools enable researchers to conduct more accurate, reproducible, and efficient fault injection experiments by bridging the gap between hardware and software error models. As the complexity and criticality of ML systems continue to grow, the importance of capturing true hardware behavior in software-based fault injection tools will only become more apparent.\nOngoing research in this area aims to refine the mapping between hardware and software error models and develop new techniques for efficiently simulating hardware faults in software-based experiments. As these tools mature, they will provide the ML community with increasingly powerful and accessible means to study and improve the resilience of ML systems to hardware faults.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html#conclusion",
    "href": "contents/robust_ai/robust_ai.html#conclusion",
    "title": "17  Robust AI",
    "section": "17.7 Conclusion",
    "text": "17.7 Conclusion\nDeveloping robust and resilient AI is paramount as machine learning systems become increasingly integrated into safety-critical applications and real-world environments. This chapter has explored the key challenges to AI robustness arising from hardware faults, malicious attacks, distribution shifts, and software bugs.\nSome of the key takeaways include the following:\n\nHardware Faults: Transient, permanent, and intermittent faults in hardware components can corrupt computations and degrade the performance of machine learning models if not properly detected and mitigated. Techniques such as redundancy, error correction, and fault-tolerant designs play a crucial role in building resilient ML systems that can withstand hardware faults.\nModel Robustness: Malicious actors can exploit vulnerabilities in ML models through adversarial attacks and data poisoning, aiming to induce targeted misclassifications, skew the model’s learned behavior, or compromise the system’s integrity and reliability. Also, distribution shifts can occur when the data distribution encountered during deployment differs from those seen during training, leading to performance degradation. Implementing defensive measures, including adversarial training, anomaly detection, robust model architectures, and techniques such as domain adaptation, transfer learning, and continual learning, is essential to safeguard against these challenges and ensure the model’s reliability and generalization in dynamic environments.\nSoftware Faults: Faults in ML frameworks, libraries, and software stacks can propagate errors, degrade performance, and introduce security vulnerabilities. Rigorous testing, runtime monitoring, and adopting fault-tolerant design patterns are essential for building robust software infrastructure supporting reliable ML systems.\n\nAs ML systems take on increasingly complex tasks with real-world consequences, prioritizing resilience becomes critical. The tools and frameworks discussed in this chapter, including fault injection techniques, error analysis methods, and robustness evaluation frameworks, provide practitioners with the means to thoroughly test and harden their ML systems against various failure modes and adversarial conditions.\nMoving forward, resilience must be a central focus throughout the entire AI development lifecycle, from data collection and model training to deployment and monitoring. By proactively addressing the multifaceted challenges to robustness, we can develop trustworthy, reliable ML systems that can navigate the complexities and uncertainties of real-world environments.\nFuture research in robust ML should continue to advance techniques for detecting and mitigating faults, attacks, and distributional shifts. Additionally, exploring novel paradigms for developing inherently resilient AI architectures, such as self-healing systems or fail-safe mechanisms, will be crucial in pushing the boundaries of AI robustness. By prioritizing resilience and investing in developing robust AI systems, we can unlock the full potential of machine learning technologies while ensuring their safe, reliable, and responsible deployment in real-world applications. As AI continues to shape our future, building resilient systems that can withstand the challenges of the real world will be a defining factor in the success and societal impact of this transformative technology.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/robust_ai/robust_ai.html#sec-robust-ai-resource",
    "href": "contents/robust_ai/robust_ai.html#sec-robust-ai-resource",
    "title": "17  Robust AI",
    "section": "17.8 Resources",
    "text": "17.8 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage both students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nComing soon.\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\nEsercizio 17.1\nEsercizio 17.2\nEsercizio 17.3\nEsercizio 17.4\n\n\n\n\n\n\n\n\n\n\nLabs\n\n\n\n\n\nIn addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.\n\nComing soon.",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span>"
    ]
  },
  {
    "objectID": "contents/generative_ai/generative_ai.html",
    "href": "contents/generative_ai/generative_ai.html",
    "title": "18  Generative AI",
    "section": "",
    "text": "Coming soon!\nImagine a chapter that writes itself and adapts to your curiosity, generating new insights as you read. We’re working on something extraordinary!\nThis chapter will transform how you read and learn, dynamically generating content as you go. While we fine-tune this exciting new feature, we hope users get ready for an educational experience that’s as dynamic and unique as you are. Mark your calendars for the big reveal and bookmark this page.\nThe future of generative learning is here! — Vijay Janapa Reddi",
    "crumbs": [
      "Argomenti Avanzati",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html",
    "href": "contents/ai_for_good/ai_for_good.html",
    "title": "19  AI for Good",
    "section": "",
    "text": "19.1 Introduction\nTo give ourselves a framework around which to think about AI for social good, we will be following the UN Sustainable Development Goals (SDGs). The UN SDGs are a collection of 17 global goals, shown in Figura 19.1, adopted by the United Nations in 2015 as part of the 2030 Agenda for Sustainable Development. The SDGs address global challenges related to poverty, inequality, climate change, environmental degradation, prosperity, and peace and justice.\nWhat is special about the SDGs is that they are a collection of interlinked objectives designed to serve as a “shared blueprint for peace and prosperity for people and the planet, now and into the future.” The SDGs emphasize sustainable development’s interconnected environmental, social, and economic aspects by putting sustainability at their center.\nA recent study (Vinuesa et al. 2020) highlights the influence of AI on all aspects of sustainable development, particularly on the 17 Sustainable Development Goals (SDGs) and 169 targets internationally defined in the 2030 Agenda for Sustainable Development. The study shows that AI can act as an enabler for 134 targets through technological improvements, but it also highlights the challenges of AI on some targets. The study shows that AI can benefit 67 targets when considering AI and societal outcomes. Still, it also warns about the issues related to the implementation of AI in countries with different cultural values and wealth.\nIn our book’s context, TinyML could help advance at least some of these SDG goals.\nThe portability, lower power requirements, and real-time analytics enabled by TinyML make it well-suited for addressing several sustainability challenges developing regions face. The widespread deployment of power solutions has the potential to provide localized and cost-effective monitoring to help achieve some of the UN’s SDGs. In the rest of the sections, we will dive into how TinyML is useful across many sectors that can address the UN SDGs.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#introduction",
    "href": "contents/ai_for_good/ai_for_good.html#introduction",
    "title": "19  AI for Good",
    "section": "",
    "text": "Vinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, e Francesco Fuso Nerini. 2020. «The role of artificial intelligence in achieving the Sustainable Development Goals». Nat. Commun. 11 (1): 1–10. https://doi.org/10.1038/s41467-019-14108-y.\n\n\n\n\n\n\nFigura 19.1: United Nations Sustainable Development Goals (SDG). Source: United Nations.\n\n\n\n\n\nGoal 1 - No Poverty: TinyML could help provide low-cost solutions for crop monitoring to improve agricultural yields in developing countries.\nGoal 2 - Zero Hunger: TinyML could enable localized and precise crop health monitoring and disease detection to reduce crop losses.\nGoal 3 - Good Health and Wellbeing: TinyML could help enable low-cost medical diagnosis tools for early detection and prevention of diseases in remote areas.\nGoal 6 - Clean Water and Sanitation: TinyML could monitor water quality and detect contaminants to ensure Access to clean drinking water.\nGoal 7 - Affordable and Clean Energy: TinyML could optimize energy consumption and enable predictive maintenance for renewable energy infrastructure.\nGoal 11 - Sustainable Cities and Communities: TinyML could enable intelligent traffic management, air quality monitoring, and optimized resource management in smart cities.\nGoal 13 - Climate Action: TinyML could monitor deforestation and track reforestation efforts. It could also help predict extreme weather events.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#agriculture",
    "href": "contents/ai_for_good/ai_for_good.html#agriculture",
    "title": "19  AI for Good",
    "section": "19.2 Agriculture",
    "text": "19.2 Agriculture\nAgriculture is essential to achieving many of the UN Sustainable Development Goals, including eradicating Hunger and malnutrition, promoting economic growth, and using natural resources sustainably. TinyML can be a valuable tool to help advance sustainable agriculture, especially for smallholder farmers in developing regions.\nTinyML solutions can provide real-time monitoring and data analytics for crop health and growing conditions - all without reliance on connectivity infrastructure. For example, low-cost camera modules connected to microcontrollers can monitor for disease, pests, and nutritional deficiencies. TinyML algorithms can analyze the images to detect issues early before they spread and damage yields. Precision monitoring can optimize inputs like water, fertilizer, and pesticides - improving efficiency and sustainability.\nOther sensors, such as GPS units and accelerometers, can track microclimate conditions, soil humidity, and livestock wellbeing. Local real-time data helps farmers respond and adapt better to changes in the field. TinyML analytics at the edge avoids lag, network disruptions, and the high data costs of cloud-based systems. Localized systems allow customization of specific crops, diseases, and regional issues.\nWidespread TinyML applications can help digitize smallholder farms to increase productivity, incomes, and resilience. The low cost of hardware and minimal connectivity requirements make solutions accessible. Projects across the developing world have shown the benefits:\n\nMicrosoft’s FarmBeats project is an end-to-end approach to enable data-driven farming by using low-cost sensors, drones, and vision and machine learning algorithms. The project aims to solve the problem of limited adoption of technology in farming due to the need for more power and internet connectivity in farms and the farmers’ limited technology savviness. The project aims to increase farm productivity and reduce costs by coupling data with farmers’ knowledge and intuition about their farms. The project has successfully enabled actionable insights from data by building artificial intelligence (AI) or machine learning (ML) models based on fused data sets.\nIn Sub-Saharan Africa, off-the-shelf cameras and edge AI have cut cassava disease losses from 40% to 5%, protecting a staple crop (Ramcharan et al. 2017).\nIn Indonesia, sensors monitor microclimates across rice paddies, optimizing water usage even with erratic rains (Tirtalistyani, Murtiningrum, e Kanwar 2022).\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg, e David P. Hughes. 2017. «Deep Learning for Image-Based Cassava Disease Detection». Front. Plant Sci. 8 (ottobre): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, e Rameshwar S. Kanwar. 2022. «Indonesia Rice Irrigation System: Time for Innovation». Sustainability 14 (19): 12477. https://doi.org/10.3390/su141912477.\nWith greater investment and integration into rural advisory services, TinyML could transform small-scale agriculture and improve farmers’ livelihoods worldwide. The technology effectively brings the benefits of precision agriculture to disconnected regions most in need.\n\n\n\n\n\n\nEsercizio 19.1: Crop Yield Modeling\n\n\n\n\n\nThis exercise teaches you how to predict crop yields in Nepal by combining satellite data (Sentinel-2), climate data (WorldClim), and on-the-ground measurements. You’ll use a machine learning algorithm called XGBoost Regressor to build a model, split the data for training and testing, and fine-tune the model parameters for the best performance. This notebook lays the foundation for implementing TinyML in the agriculture domain. Consider how you could adapt this process for smaller datasets, fewer features, and simplified models to make it compatible with the power and memory constraints of TinyML devices.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#healthcare",
    "href": "contents/ai_for_good/ai_for_good.html#healthcare",
    "title": "19  AI for Good",
    "section": "19.3 Healthcare",
    "text": "19.3 Healthcare\n\n19.3.1 Expanding Access\nUniversal health coverage and quality care remain out of reach for millions worldwide. In many regions, more medical professionals are required to Access basic diagnosis and treatment. Additionally, healthcare infrastructure like clinics, hospitals, and utilities to power complex equipment needs to be improved. These gaps disproportionately impact marginalized communities, exacerbating health disparities.\nTinyML offers a promising technological solution to help expand Access to quality healthcare globally. TinyML refers to the ability to deploy machine learning algorithms on microcontrollers, tiny chips with processing power, memory, and connectivity. TinyML enables real-time data analysis and intelligence in low-powered, compact devices.\nThis creates opportunities for transformative medical tools that are portable, affordable, and accessible. TinyML software and hardware can be optimized to run even in resource-constrained environments. For example, a TinyML system could analyze symptoms or make diagnostic predictions using minimal computing power, no continuous internet connectivity, and a battery or solar power source. These capabilities can bring medical-grade screening and monitoring directly to underserved patients.\n\n\n19.3.2 Early Diagnosis\nEarly detection of diseases is one major application. Small sensors paired with TinyML software can identify symptoms before conditions escalate or visible signs appear. For instance, cough monitors with embedded machine learning can pick up on acoustic patterns indicative of respiratory illness, malaria, or tuberculosis. Detecting diseases at onset improves outcomes and reduces healthcare costs.\nA detailed example could be given for TinyML monitoring pneumonia in children. Pneumonia is a leading cause of death for children under 5, and detecting it early is critical. A startup called Respira xColabs has developed a low-cost wearable audio sensor that uses TinyML algorithms to analyze coughs and identify symptoms of respiratory illnesses like pneumonia. The device contains a microphone sensor and microcontroller that runs a neural network model trained to classify respiratory sounds. It can identify features like wheezing, crackling, and stridor that may indicate pneumonia. The device is designed to be highly accessible - it has a simple strap, requires no battery or charging, and results are provided through LED lights and audio cues.\nAnother example involves researchers at UNIFEI in Brazil who have developed a low-cost device that leverages TinyML to monitor heart rhythms. Their innovative solution addresses a critical need - atrial fibrillation and other heart rhythm abnormalities often go undiagnosed due to the prohibitive cost and limited availability of screening tools. The device overcomes these barriers through its ingenious design. It uses an off-the-shelf microcontroller that costs only a few dollars, along with a basic pulse sensor. By minimizing complexity, the device becomes accessible to under-resourced populations. The TinyML algorithm running locally on the microcontroller analyzes pulse data in real-time to detect irregular heart rhythms. This life-saving heart monitoring device demonstrates how TinyML enables powerful AI capabilities to be deployed in cost-effective, user-friendly designs.\nTinyML’s versatility also shows promise for tackling infectious diseases. Researchers have proposed applying TinyML to identify malaria-spreading mosquitoes by their wingbeat sounds. When equipped with microphones, small microcontrollers can run advanced audio classification models to determine mosquito species. This compact, low-power solution produces results in real time, suitable for remote field use. By making entomology analytics affordable and accessible, TinyML could revolutionize monitoring insects that endanger human health. TinyML is expanding healthcare access for vulnerable communities from heart disease to malaria.\n\n\n19.3.3 Infectious Disease Control\nMosquitoes remain the most deadly disease vector worldwide, transmitting illnesses that infect over one billion people annually («Vector-borne diseases», s.d.). Diseases like malaria, dengue, and Zika are especially prevalent in resource-limited regions lacking robust infrastructure for mosquito control. Monitoring local mosquito populations is essential to prevent outbreaks and properly target interventions.\n\n«Vector-borne diseases». s.d. https://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\nTraditional monitoring methods are expensive, labor-intensive, and difficult to deploy remotely. The proposed TinyML solution aims to overcome these barriers. Small microphones coupled with machine learning algorithms can classify mosquitoes by species based on minute differences in wing oscillations. The TinyML software runs efficiently on low-cost microcontrollers, eliminating the need for continuous connectivity.\nA collaborative research team from the University of Khartoum and the ICTP is exploring an innovative solution using TinyML. In a recent paper, they presented a low-cost device that can identify disease-spreading mosquito species through their wing beat sounds (Altayeb, Zennaro, e Rovai 2022).\n\nAltayeb, Moez, Marco Zennaro, e Marcelo Rovai. 2022. «Classifying mosquito wingbeat sound using TinyML». In Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 132–37. ACM. https://doi.org/10.1145/3524458.3547258.\nThis portable, self-contained system shows great promise for entomology. The researchers suggest it could revolutionize insect monitoring and vector control strategies in remote areas. TinyML could significantly bolster malaria eradication efforts by providing cheaper, easier mosquito analytics. Its versatility and minimal power needs make it ideal for field use in isolated, off-grid regions with scarce resources but high disease burden.\n\n\n19.3.4 TinyML Design Contest in Healthcare\nThe first TinyML contest in healthcare, TDC’22 (Jia et al. 2023), was held in 2022 to motivate participating teams to design AI/ML algorithms for detecting life-threatening ventricular arrhythmias (VAs) and deploy them on Implantable Cardioverter Defibrillators (ICDs). VAs are the main cause of sudden cardiac death (SCD). People at high risk of SCD rely on the ICD to deliver proper and timely defibrillation treatment (i.e., shocking the heart back into normal rhythm) when experiencing life-threatening VAs.\n\nJia, Zhenge, Dawei Li, Xiaowei Xu, Na Li, Feng Hong, Lichuan Ping, e Yiyu Shi. 2023. «Life-threatening ventricular arrhythmia detection challenge in implantable cardioverterdefibrillators». Nature Machine Intelligence 5 (5): 554–55. https://doi.org/10.1038/s42256-023-00659-9.\nAn on-device algorithm for early and timely life-threatening VA detection will increase the chances of survival. The proposed AI/ML algorithm needed to be deployed and executed on an extremely low-power and resource-constrained microcontroller (MCU) (a $10 development board with an ARM Cortex-M4 core at 80 MHz, 256 kB of flash memory and 64 kB of SRAM). The submitted designs were evaluated by metrics measured on the MCU for (1) detection performance, (2) inference latency, and (3) memory occupation by the program of AI/ML algorithms.\nThe champion, GaTech EIC Lab, obtained 0.972 in \\(F_\\beta\\) (F1 score with a higher weight to recall), 1.747 ms in latency, and 26.39 kB in memory footprint with a deep neural network. An ICD with an on-device VA detection algorithm was implanted in a clinical trial.\n\n\n\n\n\n\nEsercizio 19.2: Clinical Data: Unlocking Insights with Named Entity Recognition\n\n\n\n\n\nIn this exercise, you’ll learn about Named Entity Recognition (NER), a powerful tool for extracting valuable information from clinical text. Using Spark NLP, a specialized library for healthcare NLP, we’ll explore how NER models like BiLSTM-CNN-Char and BERT can automatically identify important medical entities such as diagnoses, medications, test results, and more. You’ll get hands-on experience applying these techniques with a special focus on oncology-related data extraction, helping you unlock insights about cancer types and treatment details from patient records.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#science",
    "href": "contents/ai_for_good/ai_for_good.html#science",
    "title": "19  AI for Good",
    "section": "19.4 Science",
    "text": "19.4 Science\nIn many scientific fields, researchers are limited by the quality and resolution of data they can collect. They often must indirectly infer the true parameters of interest using approximate correlations and models built on sparse data points. This constrains the accuracy of scientific understanding and predictions.\nThe emergence of TinyML opens new possibilities for gathering high-fidelity scientific measurements. With embedded machine learning, tiny, low-cost sensors can automatically process and analyze data locally in real-time. This creates intelligent sensor networks that capture nuanced data at much greater scales and frequencies.\nFor example, monitoring environmental conditions to model climate change remains challenging due to the need for widespread, continuous data. The Ribbit Project from UC Berkeley is pioneering a crowdsourced TinyML solution (Rao 2021). They developed an open-source CO2 sensor that uses an onboard microcontroller to process the gas measurements. An extensive dataset can be aggregated by distributing hundreds of these low-cost sensors. The TinyML devices compensate for environmental factors and provide previously impossible, granular, accurate readings.\n\nRao, Ravi. 2021. «TinyML unlocks new possibilities for sustainable development technologies». www.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\nThe potential to massively scale out intelligent sensing via TinyML has profound scientific implications. Higher-resolution data can lead to discoveries and predictive capabilities in fields ranging from ecology to cosmology. Other applications could include seismic sensors for earthquake early warning systems, distributed weather monitors to track microclimate changes, and acoustic sensors to study animal populations.\nAs sensors and algorithms continue improving, TinyML networks may generate more detailed maps of natural systems than ever before. Democratizing the collection of scientific data can accelerate research and understanding across disciplines. However, it raises new challenges around data quality, privacy, and modeling unknowns. TinyML signifies a growing convergence of AI and the natural sciences to answer fundamental questions.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#conservation-and-environment",
    "href": "contents/ai_for_good/ai_for_good.html#conservation-and-environment",
    "title": "19  AI for Good",
    "section": "19.5 Conservation and Environment",
    "text": "19.5 Conservation and Environment\nTinyML is emerging as a powerful tool for environmental conservation and sustainability efforts. Recent research has highlighted numerous applications of tiny machine learning in domains such as wildlife monitoring, natural resource management, and tracking climate change.\nOne example is using TinyML for real-time wildlife tracking and protection. Researchers have developed Smart Wildlife Tracker devices that leverage TinyML algorithms to detect poaching activities. The collars contain sensors like cameras, microphones, and GPS to monitor the surrounding environment continuously. Embedded machine learning models analyze the audio and visual data to identify threats like nearby humans or gunshots. Early poaching detection gives wildlife rangers critical information to intervene and take action.\nOther projects apply TinyML to study animal behavior through sensors. The smart wildlife collar uses accelerometers and acoustic monitoring to track elephant movements, communication, and moods (Verma 2022). The low-power TinyML collar devices transmit rich data on elephant activities while avoiding burdensome Battery changes. This helps researchers unobtrusively observe elephant populations to inform conservation strategies.\n\nVerma, Team Dual_Boot: Swapnil. 2022. «Elephant AI». Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\nOn a broader scale, distributed TinyML devices are envisioned to create dense sensor networks for environmental modeling. Hundreds of low-cost air quality monitors could map pollution across cities. Underwater sensors may detect toxins and give early warning of algal blooms. Such applications underscore TinyML’s versatility in ecology, climatology, and sustainability.\nResearchers from Moulay Ismail University of Meknes in Morocco (Bamoumen et al. 2022) have published a survey on how TinyML can be used to solve environmental issues. However, thoughtfully assessing benefits, risks, and equitable Access will be vital as TinyML expands environmental research and conservation. With ethical consideration of impacts, TinyML offers data-driven solutions to protect biodiversity, natural resources, and our planet.\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, e Yousra Chtouki. 2022. «How TinyML Can be Leveraged to Solve Environmental Problems: A Survey». In 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), 338–43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#disaster-response",
    "href": "contents/ai_for_good/ai_for_good.html#disaster-response",
    "title": "19  AI for Good",
    "section": "19.6 Disaster Response",
    "text": "19.6 Disaster Response\nIn disaster response, speed and safety are paramount. But rubble and wreckage create hazardous, confined environments that impede human search efforts. TinyML enables nimble drones to assist rescue teams in these dangerous scenarios.\nWhen buildings collapse after earthquakes, small drones can prove invaluable. Equipped with TinyML navigation algorithms, micro-sized drones like the CrazyFlie can traverse cramped voids and map pathways beyond human reach (Bardienus P. Duisterhof et al. 2019). Obstacle avoidance allows the drones to weave through unstable debris. This autonomous mobility lets them rapidly sweep areas humans cannot access. Video 19.1 presents the (Bardienus P. Duisterhof et al. 2019) paper on deep reinforcement learning using drones for source-seeking.\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R Banbury, William Fu, Aleksandra Faust, Guido CHE de Croon, e Vijay Janapa Reddi. 2019. «Learning to seek: Autonomous source seeking with deep reinforcement learning onboard a nano drone microcontroller». ArXiv preprint abs/1909.11236. https://arxiv.org/abs/1909.11236.\n\n\n\n\n\n\nVideo 19.1: Learning to Seek\n\n\n\n\n\n\nCrucially, onboard sensors and TinyML processors analyze real-time data to identify signs of survivors. Thermal cameras detect body heat, microphones pick up calls for help, and gas sensors warn of leaks (Bardienus P. Duisterhof et al. 2021). Processing data locally using TinyML allows for quick interpretation to guide rescue efforts. As conditions evolve, the drones can adapt by adjusting their search patterns and priorities. Video 19.2 is an overview of autonomous drones for gas leak detection.\n\n\n\n\n\n\nVideo 19.2\n\n\n\n\n\n\nAdditionally, coordinated swarms of drones unlock new capabilities. By collaborating and sharing insights, drone teams comprehensively view the situation. Blanketing disaster sites allows TinyML algorithms to fuse and analyze data from multiple vantage points, amplifying situational awareness beyond individual drones (Bardienus P. Duisterhof et al. 2021).\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa Reddi, e Guido C. H. E. de Croon. 2021. «Sniffy Bug: A Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments». In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 9099–9106. IEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\nMost importantly, initial drone reconnaissance enhances safety for human responders. Keeping rescue teams at a safe distance until drone surveys assess hazards saves lives. Once secured, drones can guide precise personnel placement.\nBy combining agile mobility, real-time data, and swarm coordination, TinyML-enabled drones promise to transform disaster response. Their versatility, speed, and safety make them a vital asset for rescue efforts in dangerous, inaccessible environments. Integrating autonomous drones with traditional methods can accelerate responses when it matters most.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#education-and-outreach",
    "href": "contents/ai_for_good/ai_for_good.html#education-and-outreach",
    "title": "19  AI for Good",
    "section": "19.7 Education and Outreach",
    "text": "19.7 Education and Outreach\nTinyML holds immense potential to help address challenges in developing regions, but realizing its benefits requires focused education and capacity building. Recognizing this need, academic researchers have spearheaded outreach initiatives to spread TinyML education globally.\nIn 2020, Harvard University, Columbia University, the International Centre for Theoretical Physics (ICTP), and UNIFEI jointly founded the TinyML for Developing Communities (TinyML4D) network (Zennaro, Plancher, e Reddi 2022). This network empowers universities and researchers in developing countries to harness TinyML for local impact.\n\nZennaro, Marco, Brian Plancher, e V Janapa Reddi. 2022. «TinyML: Applied AI for development». In The UN 7th Multi-stakeholder Forum on Science, Technology and Innovation for the Sustainable Development Goals, 2022–05.\nA core focus is expanding Access to applied machine learning education. The TinyML4D network provides training, curricula, and lab resources to members. Hands-on workshops and data collection projects give students practical experience. Members can share best practices and build a community through conferences and academic collaborations.\nThe network prioritizes enabling locally relevant TinyML solutions. Projects address challenges like agriculture, health, and environmental monitoring based on community needs. For example, a member university in Rwanda developed a low-cost flood monitoring system using TinyML and sensors.\nTinyML4D includes over 50 member institutions across Africa, Asia, and Latin America. However, greater investments and industry partnerships are needed to reach all underserved regions. The ultimate vision is training new generations to ethically apply TinyML for sustainable development. Outreach efforts today lay the foundation for democratizing transformative technology for the future.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#accessibility",
    "href": "contents/ai_for_good/ai_for_good.html#accessibility",
    "title": "19  AI for Good",
    "section": "19.8 Accessibility",
    "text": "19.8 Accessibility\nTechnology has immense potential to break down barriers faced by people with disabilities and bridge gaps in accessibility. TinyML specifically opens new possibilities for developing intelligent, personalized assistive devices.\nWith machine learning algorithms running locally on microcontrollers, compact accessibility tools can operate in real time without reliance on connectivity. The National Institute on Deafness and Other Communication Disorders (NIDCD) states that 20% of the world’s population has some form of hearing loss. Hearing aids leveraging TinyML could recognize multiple speakers and amplify the voice of a chosen target in crowded rooms. This allows people with hearing impairments to focus on specific conversations.\nSimilarly, mobility devices could use on-device vision processing to identify obstacles and terrain characteristics. This enables enhanced navigation and safety for the visually impaired. Companies like Envision are developing smart glasses, converting visual information into speech, with embedded TinyML to guide blind people by detecting objects, text, and traffic signals. Video 19.3 below shows the different real-life use cases of the Envision visual aid glasses.\n\n\n\n\n\n\nVideo 19.3\n\n\n\n\n\n\nTinyML could even power responsive prosthetic limbs. By analyzing nerve signals and sensory data like muscle tension, prosthetics and exoskeletons with embedded ML can move and adjust grip dynamically, making control more natural and intuitive. Companies are creating affordable, everyday bionic hands using TinyML. For those with speech difficulties, voice-enabled devices with TinyML can generate personalized vocal outputs from non-verbal inputs. Pairs by Anthropic translates gestures into natural speech tailored for individual users.\nBy enabling more customizable assistive tech, TinyML makes services more accessible and tailored to individual needs. And through translation and interpretation applications, TinyML can break down communication barriers. Apps like Microsoft Translator offer real-time translation powered by TinyML algorithms.\nWith its thoughtful and inclusive design, TinyML promises more autonomy and dignity for people with disabilities. However, developers should engage communities directly, avoid compromising privacy, and consider affordability to maximize the benefits. TinyML has huge potential to contribute to a more just, equitable world.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#infrastructure-and-urban-planning",
    "href": "contents/ai_for_good/ai_for_good.html#infrastructure-and-urban-planning",
    "title": "19  AI for Good",
    "section": "19.9 Infrastructure and Urban Planning",
    "text": "19.9 Infrastructure and Urban Planning\nAs urban populations swell, cities face immense challenges in efficiently managing resources and infrastructure. TinyML presents a powerful tool for developing intelligent systems to optimize city operations and sustainability. It could revolutionize energy efficiency in smart buildings.\nMachine learning models can learn to predict and regulate energy usage based on occupancy patterns. Miniaturized sensors placed throughout buildings can provide granular, real-time data on space utilization, temperature, and more (Seyedzadeh et al. 2018). This visibility allows TinyML systems to minimize waste by optimizing heating, cooling, lighting, etc.\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, e Marc Roper. 2018. «Machine learning for estimation of building energy consumption and performance: A review». Visualization in Engineering 6 (1): 1–20. https://doi.org/10.1186/s40327-018-0064-7.\nThese examples demonstrate TinyML’s huge potential for efficient, sustainable city infrastructure. However, urban planners must consider privacy, security, and accessibility to ensure responsible adoption. With careful implementation, TinyML could profoundly modernize urban life.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#challenges-and-considerations",
    "href": "contents/ai_for_good/ai_for_good.html#challenges-and-considerations",
    "title": "19  AI for Good",
    "section": "19.10 Challenges and Considerations",
    "text": "19.10 Challenges and Considerations\nWhile TinyML presents immense opportunities, thoughtful consideration of challenges and ethical implications will be critical as adoption spreads globally. Researchers have highlighted key factors to address, especially when deploying TinyML in developing regions.\nA foremost challenge is limited Access to training and hardware (Ooko et al. 2021). Only educational programs exist tailored to TinyML, and emerging economies often need a robust electronics supply chain. Thorough training and partnerships will be needed to nurture expertise and make devices available to underserved communities. Initiatives like the TinyML4D network help provide structured learning pathways.\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, e Marco Zennaro. 2021. «TinyML in Africa: Opportunities and Challenges». In 2021 IEEE Globecom Workshops (GC Wkshps), 1–6. IEEE; IEEE. https://doi.org/10.1109/gcwkshps52748.2021.9682107.\nData limitations also pose hurdles. TinyML models require quality localized datasets, which are scarce in under-resourced environments. Creating frameworks to crowdsource data ethically could address this. However, data collection should benefit local communities directly, not just extract value.\nOptimizing power usage and connectivity will be vital for sustainability. TinyML’s low power needs make it ideal for off-grid use cases. Integrating battery or solar can enable continuous operation. Adapting devices for low-bandwidth transmission where the internet is limited also maximizes impact.\nCultural and language barriers further complicate adoption. User interfaces and devices should account for all literacy levels and avoid excluding subgroups. Voice-controllable solutions in local dialects can improve accessibility.\nAddressing these challenges requires holistic partnerships, funding, and policy support. However, inclusively and ethically scaling TinyML has monumental potential to uplift disadvantaged populations worldwide. With thoughtful implementation, the technology could profoundly democratize opportunity.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#conclusion",
    "href": "contents/ai_for_good/ai_for_good.html#conclusion",
    "title": "19  AI for Good",
    "section": "19.11 Conclusion",
    "text": "19.11 Conclusion\nTinyML presents a tremendous opportunity to harness the power of artificial intelligence to advance the UN Sustainable Development Goals and drive social impact globally, as highlighted by examples across sectors like healthcare, agriculture, conservation, and more; embedded machine learning unlocks new capabilities for low-cost, accessible solutions tailored to local contexts. TinyML circumvents barriers like poor infrastructure, limited connectivity, and high costs that often exclude developing communities from emerging technology.\nHowever, realizing TinyML’s full potential requires holistic collaboration. Researchers, policymakers, companies, and local stakeholders must collaborate to provide training, establish ethical frameworks, co-design solutions, and adapt them to community needs. Through inclusive development and deployment, TinyML can deliver on its promise to bridge inequities and uplift vulnerable populations without leaving any behind.\nIf cultivated responsibly, TinyML could democratize opportunity and accelerate progress on global priorities from poverty alleviation to climate resilience. The technology represents a new wave of applied AI to empower societies, promote sustainability, and propel humanity toward greater justice, prosperity, and peace. TinyML provides a glimpse into an AI-enabled future that is accessible to all.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/ai_for_good/ai_for_good.html#sec-ai-for-good-resource",
    "href": "contents/ai_for_good/ai_for_good.html#sec-ai-for-good-resource",
    "title": "19  AI for Good",
    "section": "19.12 Resources",
    "text": "19.12 Resources\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will be adding new exercises soon.\n\n\n\n\n\n\nSlides\n\n\n\n\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\nTinyML for Social Impact.\n\n\n\n\n\n\n\n\n\n\nVideos\n\n\n\n\n\n\nVideo 19.1\nVideo 19.2\nVideo 19.3\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\n\nEsercizio 19.1\nEsercizio 19.2\n\n\n\n\n\n\n\n\n\n\nLabs\n\n\n\n\n\nIn addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.\n\nComing soon.",
    "crumbs": [
      "Impatto Sociale",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html",
    "href": "contents/conclusion/conclusion.html",
    "title": "20  Conclusion",
    "section": "",
    "text": "20.1 Introduction\nThis book examines the rapidly evolving field of ML systems (Capitolo 2). We focus on systems because while there are many resources on ML models and algorithms, more needs to be understood about how to build the systems that run them.\nTo draw an analogy, consider the process of building a car. While many resources are available on the various components of a car, such as the engine, transmission, and suspension, there is often a need for more understanding about how to assemble these components into a functional vehicle. Just as a car requires a well-designed and properly integrated system to operate efficiently and reliably, ML models also require a robust and carefully constructed system to deliver their full potential. Moreover, there is a lot of nuance in building ML systems, given their specific use case. For example, a Formula 1 race car must be assembled differently from an everyday Prius consumer car.\nOur journey started by tracing ML’s historical trajectory, from its theoretical foundations to its current state as a transformative force across industries (Capitolo 3). This journey has highlighted the remarkable progress in the field, challenges, and opportunities.\nThroughout this book, we have looked into the intricacies of ML systems, examining the critical components and best practices necessary to create a seamless and efficient pipeline. From data preprocessing and model training to deployment and monitoring, we have provided insights and guidance to help readers navigate the complex landscape of ML system development.\nML systems involve complex workflows, spanning various topics from data engineering to model deployment on diverse systems (Capitolo 4). By providing an overview of these ML system components, we have aimed to showcase the tremendous depth and breadth of the field and expertise that is needed. Understanding the intricacies of ML workflows is crucial for practitioners and researchers alike, as it enables them to navigate the landscape effectively and develop robust, efficient, and impactful ML solutions.\nBy focusing on the systems aspect of ML, we aim to bridge the gap between theoretical knowledge and practical implementation. Just as a healthy human body system allows the organs to function optimally, a well-designed ML system enables the models to consistently deliver accurate and reliable results. This book aims to empower readers with the knowledge and tools necessary to build ML systems that showcase the underlying models’ power and ensure smooth integration and operation, much like a well-functioning human body.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#knowing-the-importance-of-ml-datasets",
    "href": "contents/conclusion/conclusion.html#knowing-the-importance-of-ml-datasets",
    "title": "20  Conclusion",
    "section": "20.2 Knowing the Importance of ML Datasets",
    "text": "20.2 Knowing the Importance of ML Datasets\nOne of the key things we have emphasized is that data is the foundation upon which ML systems are built (Capitolo 5). Data is the new code that programs deep neural networks, making data engineering the first and most critical stage of any ML pipeline. That is why we began our exploration by diving into the basics of data engineering, recognizing that quality, diversity, and ethical sourcing are key to building robust and reliable machine learning models.\nThe importance of high-quality data must be balanced. Lapses in data quality can lead to significant negative consequences, such as flawed predictions, project terminations, and even potential harm to communities. These cascading effects, often called “Data Cascades,” highlight the need for diligent data management and governance practices. ML practitioners must prioritize data quality, ensure diversity and representativeness, and adhere to ethical data collection and usage standards. By doing so, we can mitigate the risks associated with poor data quality and build ML systems that are trustworthy, reliable, and beneficial to society.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#navigating-the-ai-framework-landscape",
    "href": "contents/conclusion/conclusion.html#navigating-the-ai-framework-landscape",
    "title": "20  Conclusion",
    "section": "20.3 Navigating the AI Framework Landscape",
    "text": "20.3 Navigating the AI Framework Landscape\nThere are many different ML frameworks. Therefore, we dove into the evolution of different ML frameworks, dissecting the inner workings of popular ones like TensorFlow and PyTorch, and provided insights into the core components and advanced features that define them (Capitolo 6). We also looked into the specialization of frameworks tailored to specific needs, such as those designed for embedded AI. We discussed the criteria for selecting the most suitable framework for a given project.\nOur exploration also touched upon the future trends expected to shape the landscape of ML frameworks in the coming years. As the field continues to evolve, we can anticipate the emergence of more specialized and optimized frameworks that cater to the unique requirements of different domains and deployment scenarios, as we saw with TensorFlow Lite for Microcontrollers. By staying abreast of these developments and understanding the tradeoffs involved in framework selection, we can make informed decisions and leverage the most appropriate tools to build efficient ML systems.\nMoreover, we expect to see a growing emphasis on framework interoperability and standardization efforts, such as the ONNX (Open Neural Network Exchange) format. This format allows models to be trained in one framework and deployed in another, facilitating greater collaboration and portability across different platforms and environments.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#understanding-ml-training-fundamentals",
    "href": "contents/conclusion/conclusion.html#understanding-ml-training-fundamentals",
    "title": "20  Conclusion",
    "section": "20.4 Understanding ML Training Fundamentals",
    "text": "20.4 Understanding ML Training Fundamentals\nAs ML practitioners who build ML systems, it is crucial to deeply understand the AI training process and the system challenges in scaling and optimizing it. By leveraging the capabilities of modern AI frameworks and staying up-to-date with the latest advancements in training techniques, we can build robust, efficient, and scalable ML systems that can tackle real-world problems and drive innovation across various domains.\nWe began by examining the fundamentals of AI training (Capitolo 7), which involves feeding data into ML models and adjusting their parameters to minimize the difference between predicted and actual outputs. This process is computationally intensive and requires careful consideration of various factors, such as the choice of optimization algorithms, learning rate, batch size, and regularization techniques. Understanding these concepts is crucial for developing effective and efficient training pipelines.\nHowever, training ML models at scale poses significant system challenges. As datasets’ size and models’ complexity grow, the computational resources required for training can become prohibitively expensive. This has led to the development of distributed training techniques, such as data and model parallelism, which allow multiple devices to collaborate in the training process. Frameworks like TensorFlow and PyTorch have evolved to support these distributed training paradigms, enabling practitioners to scale their training workloads across clusters of GPUs or TPUs.\nIn addition to distributed training, we discussed techniques for optimizing the training process, such as mixed-precision training and gradient compression. It’s important to note that while these techniques may seem algorithmic, they significantly impact system performance. The choice of training algorithms, precision, and communication strategies directly affects the ML system’s resource utilization, scalability, and efficiency. Therefore, adopting an algorithm-hardware or algorithm-system co-design approach is crucial, where the algorithmic choices are made in tandem with the system considerations. By understanding the interplay between algorithms and hardware, we can make informed decisions that optimize the model performance and the system efficiency, ultimately leading to more effective and scalable ML solutions.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#pursuing-efficiency-in-ai-systems",
    "href": "contents/conclusion/conclusion.html#pursuing-efficiency-in-ai-systems",
    "title": "20  Conclusion",
    "section": "20.5 Pursuing Efficiency in AI Systems",
    "text": "20.5 Pursuing Efficiency in AI Systems\nDeploying trained ML models is more complex than simply running the networks; efficiency is critical (Capitolo 8). In this chapter on AI efficiency, we emphasized that efficiency is not merely a luxury but a necessity in artificial intelligence systems. We dug into the key concepts underpinning AI systems’ efficiency, recognizing that the computational demands on neural networks can be daunting, even for minimal systems. For AI to be seamlessly integrated into everyday devices and essential systems, it must perform optimally within the constraints of limited resources while maintaining its efficacy.\nThroughout the book, we have highlighted the importance of pursuing efficiency to ensure that AI models are streamlined, rapid, and sustainable. By optimizing models for efficiency, we can widen their applicability across various platforms and scenarios, enabling AI to be deployed in resource-constrained environments such as embedded systems and edge devices. This pursuit of efficiency is crucial for the widespread adoption and practical implementation of AI technologies in real-world applications.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#optimizing-ml-model-architectures",
    "href": "contents/conclusion/conclusion.html#optimizing-ml-model-architectures",
    "title": "20  Conclusion",
    "section": "20.6 Optimizing ML Model Architectures",
    "text": "20.6 Optimizing ML Model Architectures\nWe then explored various model architectures, from the foundational perceptron to the sophisticated transformer networks, each tailored to specific tasks and data types. This exploration has showcased machine learning models’ remarkable diversity and adaptability, enabling them to tackle various problems across domains.\nHowever, when deploying these models on systems, especially resource-constrained embedded systems, model optimization becomes a necessity. The evolution of model architectures, from the early MobileNets designed for mobile devices to the more recent TinyML models optimized for microcontrollers, is a testament to the continued innovation.\nIn the chapter on model optimization (Capitolo 9), we looked into the art and science of optimizing machine learning models to ensure they are lightweight, efficient, and effective when deployed in TinyML scenarios. We explored techniques such as model compression, quantization, and architecture search, which allow us to reduce the computational footprint of models while maintaining their performance. By applying these optimization techniques, we can create models tailored to the specific constraints of embedded systems, enabling the deployment of powerful AI capabilities on edge devices. This opens many possibilities for intelligent, real-time processing and decision-making in IoT, robotics, and mobile computing applications. As we continue pushing the boundaries of AI efficiency, we expect to see even more innovative solutions for deploying machine learning models in resource-constrained environments.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#advancing-ai-processing-hardware",
    "href": "contents/conclusion/conclusion.html#advancing-ai-processing-hardware",
    "title": "20  Conclusion",
    "section": "20.7 Advancing AI Processing Hardware",
    "text": "20.7 Advancing AI Processing Hardware\nOver the years, we have witnessed remarkable strides in ML hardware, driven by the insatiable demand for computational power and the need to address the challenges of resource constraints in real-world deployments (Capitolo 10). These advancements have been crucial in enabling the deployment of powerful AI capabilities on devices with limited resources, opening up new possibilities across various industries.\nSpecialized hardware acceleration is essential to overcome these constraints and enable high-performance machine learning. Hardware accelerators, such as GPUs, FPGAs, and ASICs, optimize compute-intensive operations, particularly inference, by leveraging custom silicon designed for efficient matrix multiplications. These accelerators provide substantial speedups compared to general-purpose CPUs, enabling real-time execution of advanced ML models on devices with strict size, weight, and power limitations.\nWe have also explored the various techniques and approaches for hardware acceleration in embedded machine-learning systems. We discussed the tradeoffs in selecting the appropriate hardware for specific use cases and the importance of software optimizations to harness these accelerators’ capabilities fully. By understanding these concepts, ML practitioners can make informed decisions when designing and deploying ML systems.\nGiven the plethora of ML hardware solutions available, benchmarking has become essential to developing and deploying machine learning systems (Capitolo 11). Benchmarking allows developers to measure and compare the performance of different hardware platforms, model architectures, training procedures, and deployment strategies. By utilizing well-established benchmarks like MLPerf, practitioners gain valuable insights into the most effective approaches for a given problem, considering the unique constraints of the target deployment environment.\nAdvancements in ML hardware, combined with insights gained from benchmarking and optimization techniques, have paved the way for successfully deploying machine learning capabilities on various devices, from powerful edge servers to resource-constrained microcontrollers. As the field continues to evolve, we expect to see even more innovative hardware solutions and benchmarking approaches that will further push the boundaries of what is possible with embedded machine learning systems.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#embracing-on-device-learning",
    "href": "contents/conclusion/conclusion.html#embracing-on-device-learning",
    "title": "20  Conclusion",
    "section": "20.8 Embracing On-Device Learning",
    "text": "20.8 Embracing On-Device Learning\nIn addition to the advancements in ML hardware, we also explored on-device learning, where models can adapt and learn directly on the device (Capitolo 12). This approach has significant implications for data privacy and security, as sensitive information can be processed locally without the need for transmission to external servers.\nOn-device learning enhances privacy by keeping data within the confines of the device, reducing the risk of unauthorized access or data breaches. It also reduces reliance on cloud connectivity, enabling ML models to function effectively even in scenarios with limited or intermittent internet access. We have discussed techniques such as transfer learning and federated learning, which have expanded the capabilities of on-device learning. Transfer learning allows models to leverage knowledge gained from one task or domain to improve performance on another, enabling more efficient and effective learning on resource-constrained devices. On the other hand, Federated learning enables collaborative model updates across distributed devices without centralized data aggregation. This approach allows multiple devices to contribute to learning while keeping their data locally, enhancing privacy and security.\nThese advancements in on-device learning have paved the way for more secure, privacy-preserving, and decentralized machine learning applications. As we prioritize data privacy and security in developing ML systems, we expect to see more innovative solutions that enable powerful AI capabilities while protecting sensitive information and ensuring user privacy.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#streamlining-ml-operations",
    "href": "contents/conclusion/conclusion.html#streamlining-ml-operations",
    "title": "20  Conclusion",
    "section": "20.9 Streamlining ML Operations",
    "text": "20.9 Streamlining ML Operations\nEven if we got the above pieces right, challenges and considerations must be addressed to ensure ML models’ successful integration and operation in production environments. In the ML Ops chapter (Capitolo 13), we studied the practices and architectures necessary to develop, deploy, and manage ML models throughout their entire lifecycle. We looked at the phases of ML, from data collection and model training to evaluation, deployment, and ongoing monitoring.\nWe learned about the importance of automation, collaboration, and continuous improvement in ML Ops. By automating key processes, teams can streamline their workflows, reduce manual errors, and accelerate the deployment of ML models. Collaboration among diverse teams, including data scientists, engineers, and domain experts, ensures ML systems’ successful development and deployment.\nThe ultimate goal of this chapter was to provide readers with a comprehensive understanding of ML model management, equipping them with the knowledge and tools necessary to build and run ML applications that deliver sustained value successfully. By adopting best practices in ML Ops, organizations can ensure their ML initiatives’ long-term success and impact, driving innovation and delivering meaningful results.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#ensuring-security-and-privacy",
    "href": "contents/conclusion/conclusion.html#ensuring-security-and-privacy",
    "title": "20  Conclusion",
    "section": "20.10 Ensuring Security and Privacy",
    "text": "20.10 Ensuring Security and Privacy\nNo ML system is ever complete without thinking about security and privacy. They are of major importance when developing real-world ML systems. As machine learning finds increasing application in sensitive domains such as healthcare, finance, and personal data, safeguarding confidentiality and preventing the misuse of data and models becomes a critical imperative, and these were the concepts we discussed previously (Capitolo 14).\nTo build robust and responsible ML systems, practitioners must thoroughly understand the potential security and privacy risks. These risks include data leaks, which can expose sensitive information; model theft, where malicious actors steal trained models; adversarial attacks that can manipulate model behavior; bias in models that can lead to unfair or discriminatory outcomes; and unintended access to private information.\nMitigating these risks requires a deep understanding of best practices in security and privacy. Therefore, we have emphasized that security and privacy cannot be an afterthought—they must be proactively addressed at every stage of the ML system development lifecycle. From the initial stages of data collection and labeling, it is crucial to ensure that data is handled securely and that privacy is protected. During model training and evaluation, techniques such as differential privacy and secure multi-party computation can be employed to safeguard sensitive information.\nWhen deploying ML models, robust access controls, encryption, and monitoring mechanisms must be implemented to prevent unauthorized access and detect potential security breaches. Ongoing monitoring and auditing of ML systems as part of MLOps are also essential to identify and address emerging security or privacy vulnerabilities.\nBy embedding security and privacy considerations into each stage of building, deploying, and managing ML systems, we can safely unlock the benefits of AI while protecting individuals’ rights and ensuring the responsible use of these powerful technologies. Only through this proactive and comprehensive approach can we build ML systems that are not only technologically advanced but also ethically sound and worthy of public trust.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#upholding-ethical-considerations",
    "href": "contents/conclusion/conclusion.html#upholding-ethical-considerations",
    "title": "20  Conclusion",
    "section": "20.11 Upholding Ethical Considerations",
    "text": "20.11 Upholding Ethical Considerations\nAs we embrace ML advancements in all facets of our lives, it is crucial to remain mindful of the ethical considerations that will shape the future of AI (Capitolo 15). Fairness, transparency, accountability, and privacy in AI systems will be paramount as they become more integrated into our lives and decision-making processes.\nAs AI systems become more pervasive and influential, it is essential to ensure that they are designed and deployed in a manner that upholds ethical principles. This means actively mitigating biases, promoting fairness, and preventing discriminatory outcomes. It also ensures transparency in how AI systems make decisions, enabling users to understand and trust their outputs.\nAccountability is another critical ethical consideration. As AI systems take on more responsibilities and make decisions that impact individuals and society, there must be clear mechanisms for holding these systems and their creators accountable. This includes establishing frameworks for auditing and monitoring AI systems and defining liability and redress mechanisms in case of harm or unintended consequences.\nEthical frameworks, regulations, and standards will be essential to address these ethical challenges. These frameworks should guide the responsible development and deployment of AI technologies, ensuring that they align with societal values and promote the well-being of individuals and communities.\nMoreover, ongoing discussions and collaborations among researchers, practitioners, policymakers, and society will be crucial in navigating the ethical landscape of AI. These conversations should be inclusive and diverse, bringing together different perspectives and expertise to develop comprehensive and equitable solutions. As we move forward, it is the collective responsibility of all stakeholders to prioritize ethical considerations in the development and deployment of AI systems.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#promoting-sustainability-and-equity",
    "href": "contents/conclusion/conclusion.html#promoting-sustainability-and-equity",
    "title": "20  Conclusion",
    "section": "20.12 Promoting Sustainability and Equity",
    "text": "20.12 Promoting Sustainability and Equity\nThe increasing computational demands of machine learning, particularly for training large models, have raised concerns about their environmental impact due to high energy consumption and carbon emissions (Capitolo 16). As the scale and complexity of models continue to grow, addressing the sustainability challenges associated with AI development becomes imperative. To mitigate the environmental footprint of AI, the development of energy-efficient algorithms is crucial. This involves optimizing models and training procedures to minimize computational requirements while maintaining performance. Techniques such as model compression, quantization, and efficient neural architecture search can help reduce the energy consumption of AI systems.\nUsing renewable energy sources to power AI infrastructure is another important step towards sustainability. By transitioning to clean energy sources such as solar, wind, and hydropower, the carbon emissions associated with AI development can be significantly reduced. This requires a concerted effort from the AI community and support from policymakers and industry leaders to invest in and adopt renewable energy solutions. In addition, exploring alternative computing paradigms, such as neuromorphic and photonic computing, holds promise for developing more energy-efficient AI systems. By developing hardware and algorithms that emulate the brain’s processing mechanisms, we can potentially create AI systems that are both powerful and sustainable.\nThe AI community must prioritize sustainability as a key consideration in research and development. This involves investing in green computing initiatives, such as developing energy-efficient hardware and optimizing data centers for reduced energy consumption. It also requires collaboration across disciplines, bringing together AI, energy, and sustainability experts to develop holistic solutions.\nMoreover, it is important to acknowledge that access to AI and machine learning compute resources may not be equally distributed across organizations and regions. This disparity can lead to a widening gap between those who have the means to leverage advanced AI technologies and those who do not. Organizations like the Organisation for Economic Cooperation and Development (OECD) are actively exploring ways to address this issue and promote greater equity in AI access and adoption. By fostering international cooperation, sharing best practices, and supporting capacity-building initiatives, we can ensure that AI’s benefits are more widely accessible and that no one is left behind in the AI revolution.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#enhancing-robustness-and-resiliency",
    "href": "contents/conclusion/conclusion.html#enhancing-robustness-and-resiliency",
    "title": "20  Conclusion",
    "section": "20.13 Enhancing Robustness and Resiliency",
    "text": "20.13 Enhancing Robustness and Resiliency\nThe chapter on Robust AI dives into the fundamental concepts, techniques, and tools for building fault-tolerant and error-resilient ML systems (Capitolo 17). In that chapter, we explored how robust AI techniques can address the challenges posed by various types of hardware faults, including transient, permanent, and intermittent faults, as well as software issues such as bugs, design flaws, and implementation errors.\nBy employing robust AI techniques, ML systems can maintain their reliability, safety, and performance even in adverse conditions. These techniques enable systems to detect and recover from faults, adapt to changing environments, and make decisions under uncertainty.\nThe chapter empowers researchers and practitioners to develop AI solutions that can withstand the complexities and uncertainties of real-world environments. It provides insights into the design principles, architectures, and algorithms underpinning robust AI systems and practical guidance on implementing and validating these systems.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#shaping-the-future-of-ml-systems",
    "href": "contents/conclusion/conclusion.html#shaping-the-future-of-ml-systems",
    "title": "20  Conclusion",
    "section": "20.14 Shaping the Future of ML Systems",
    "text": "20.14 Shaping the Future of ML Systems\nAs we look to the future, the trajectory of ML systems points towards a paradigm shift from a model-centric approach to a more data-centric one. This shift recognizes that the quality and diversity of data are paramount to developing robust, reliable, and fair AI models.\nWe anticipate a growing emphasis on data curation, labeling, and augmentation techniques in the coming years. These practices aim to ensure that models are trained on high-quality, representative data that accurately reflects the complexities and nuances of real-world scenarios. By focusing on data quality and diversity, we can mitigate the risks of biased or skewed models that may perpetuate unfair or discriminatory outcomes.\nThis data-centric approach will be crucial in addressing the challenges of bias, fairness, and generalizability in ML systems. By actively seeking out and incorporating diverse and inclusive datasets, we can develop more robust, equitable, and applicable models for various contexts and populations. Moreover, the emphasis on data will drive advancements in techniques such as data augmentation, where existing datasets are expanded and diversified through data synthesis, translation, and generation. These techniques can help overcome the limitations of small or imbalanced datasets, enabling the development of more accurate and generalizable models.\nIn recent years, generative AI has taken the field by storm, demonstrating remarkable capabilities in creating realistic images, videos, and text. However, the rise of generative AI also brings new challenges for ML systems (Capitolo 18). Unlike traditional ML systems, generative models often demand more computational resources and pose challenges in terms of scalability and efficiency. Furthermore, evaluating and benchmarking generative models presents difficulties, as traditional metrics used for classification tasks may not be directly applicable. Developing robust evaluation frameworks for generative models is an active area of research.\nUnderstanding and addressing these system challenges and ethical considerations will be crucial in shaping the future of generative AI and its impact on society. As ML practitioners and researchers, we are responsible for advancing the technical capabilities of generative models and developing robust systems and frameworks that can mitigate potential risks and ensure the beneficial application of this powerful technology.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#applying-ai-for-good",
    "href": "contents/conclusion/conclusion.html#applying-ai-for-good",
    "title": "20  Conclusion",
    "section": "20.15 Applying AI for Good",
    "text": "20.15 Applying AI for Good\nThe potential for AI to be used for social good is vast, provided that responsible ML systems are developed and deployed at scale across various use cases (Capitolo 19). To realize this potential, it is essential for researchers and practitioners to actively engage in the process of learning, experimentation, and pushing the boundaries of what is possible.\nThroughout the development of ML systems, it is crucial to remember the key themes and lessons explored in this book. These include the importance of data quality and diversity, the pursuit of efficiency and robustness, the potential of TinyML and neuromorphic computing, and the imperative of security and privacy. These insights inform the work and guide the decisions of those involved in developing AI systems.\nIt is important to recognize that the development of AI is not solely a technical endeavor but also a deeply human one. It requires collaboration, empathy, and a commitment to understanding the societal implications of the systems being created. Engaging with experts from diverse fields, such as ethics, social sciences, and policy, is essential to ensure that the AI systems developed are technically sound, socially responsible, and beneficial. Embracing the opportunity to be part of this transformative field and shaping its future is a privilege and a responsibility. By working together, we can create a world where ML systems serve as tools for positive change and improving the human condition.",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/conclusion/conclusion.html#congratulations",
    "href": "contents/conclusion/conclusion.html#congratulations",
    "title": "20  Conclusion",
    "section": "20.16 Congratulations",
    "text": "20.16 Congratulations\nCongratulations on coming this far, and best of luck in your future endeavors! The future of AI is bright and filled with endless possibilities, and I can’t wait to see the incredible contributions you will make.\nFeel free to reach out to me anytime at vj at eecs dot harvard dot edu.\n– Prof. Vijay Janapa Reddi, Harvard University",
    "crumbs": [
      "Chiusura",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "contents/labs/labs.html",
    "href": "contents/labs/labs.html",
    "title": "Overview",
    "section": "",
    "text": "Learning Objectives\nBy completing these labs, we hope learners will:",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#learning-objectives",
    "href": "contents/labs/labs.html#learning-objectives",
    "title": "Overview",
    "section": "",
    "text": "Consiglio\n\n\n\n\nGain proficiency in setting up and deploying ML models on supported devices enabling you to tackle real-world ML deployment scenarios with confidence.\nUnderstand the steps involved in adapting and experimenting with ML models for different applications allowing you to optimize performance and efficiency.\nLearn troubleshooting techniques specific to embedded ML deployments equipping you with the skills to overcome common pitfalls and challenges.\nAcquire practical experience in deploying TinyML models on embedded devices bridging the gap between theory and practice.\nExplore various sensor modalities and their applications expanding your understanding of how ML can be leveraged in diverse domains.\nFoster an understanding of the real-world implications and challenges associated with ML system deployments preparing you for future projects.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#target-audience",
    "href": "contents/labs/labs.html#target-audience",
    "title": "Overview",
    "section": "Target Audience",
    "text": "Target Audience\nThese labs are designed for:\n\nBeginners in the field of machine learning who have a keen interest in exploring the intersection of ML and embedded systems.\nDevelopers and engineers looking to apply ML models to real-world applications using low-power, resource-constrained devices.\nEnthusiasts and researchers who want to gain practical experience in deploying AI on edge devices and understand the unique challenges involved.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#supported-devices",
    "href": "contents/labs/labs.html#supported-devices",
    "title": "Overview",
    "section": "Supported Devices",
    "text": "Supported Devices\n\n\n\n\n\n\n\n\n\nExercise\nNicla Vision (Arduino Nicla Vision)\nXIAO ESP32S3 (Seeed XIAO ESP32S3)\nRaspberry Pi (Raspberry Pi Foundation)\n\n\n\n\nInstallation & Setup\n✅\n✅\n✅\n\n\nKeyword Spotting (KWS)\n✅\n✅\n\n\n\nImage Classification\n✅\n✅\nComing soon.\n\n\nObject Detection\n✅\n✅\nComing soon.\n\n\nMotion Detection\n✅\n✅\n\n\n\nSmall Language Models (SLM)\n\n\nComing soon.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#lab-structure",
    "href": "contents/labs/labs.html#lab-structure",
    "title": "Overview",
    "section": "Lab Structure",
    "text": "Lab Structure\nEach lab follows a structured approach:\n\nIntroduction: Explore the application and its significance in real-world scenarios.\nSetup: Step-by-step instructions to configure the hardware and software environment.\nDeployment: Guidance on training and deploying the pre-trained ML models on supported devices.\nExercises: Hands-on tasks to modify and experiment with model parameters.\nDiscussion: Analysis of results, potential improvements, and practical insights.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#troubleshooting-and-support",
    "href": "contents/labs/labs.html#troubleshooting-and-support",
    "title": "Overview",
    "section": "Troubleshooting and Support",
    "text": "Troubleshooting and Support\nIf you encounter any issues during the labs, consult the troubleshooting comments or check the FAQs within each lab. For further assistance, feel free to reach out to our support team or engage with the community forums.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/labs.html#credits",
    "href": "contents/labs/labs.html#credits",
    "title": "Overview",
    "section": "Credits",
    "text": "Credits\nSpecial credit and thanks to Prof. Marcelo Rovai Mjrovai for his valuable contributions to the development and continuous refinement of these labs.",
    "crumbs": [
      "LABS",
      "Overview"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html",
    "href": "contents/labs/getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Hardware Requirements\nTo follow along with the hands-on labs, you’ll need the following hardware:\nThe Arduino Nicla Vision is tailored for professional-grade applications, offering advanced features and performance suitable for demanding industrial projects. On the other hand, the Seeed Studio XIAO ESP32S3 Sense is geared towards makers, hobbyists, and students who want to explore edge AI applications in a more accessible and beginner-friendly format. Both boards have their strengths and target audiences, allowing users to choose the one that best fits their needs and skill level.",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#hardware-requirements",
    "href": "contents/labs/getting_started.html#hardware-requirements",
    "title": "Getting Started",
    "section": "",
    "text": "Arduino Nicla Vision board\n\nThe Arduino Nicla Vision is a powerful, compact board designed for professional-grade computer vision and audio applications. It features a high-quality camera module, a digital microphone, and an IMU, making it suitable for demanding projects in industries such as robotics, automation, and surveillance.\nArduino Nicla Vision specifications\nArduino Nicla Vision pinout diagram\n\nXIAO ESP32S3 Sense board\n\nThe Seeed Studio XIAO ESP32S3 Sense is a tiny, feature-packed board designed for makers, hobbyists, and students interested in exploring edge AI applications. It comes with a camera, microphone, and IMU, making it easy to get started with projects like image classification, keyword spotting, and motion detection.\nXIAO ESP32S3 Sense specifications\nXIAO ESP32S3 Sense pinout diagram\n\nAdditional accessories\n\nUSB-C cable for programming and powering the boards\nBreadboard and jumper wires (optional, for connecting additional sensors)",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#software-requirements",
    "href": "contents/labs/getting_started.html#software-requirements",
    "title": "Getting Started",
    "section": "Software Requirements",
    "text": "Software Requirements\nTo program the boards and develop embedded machine learning projects, you’ll need the following software:\n\nArduino IDE\n\nDownload and install\n\nInstall Arduino IDE\nArduino CLI\n\nEdge Impulse Studio\n\nFollow the installation guide for your specific OS.\nConfigure the Arduino IDE for the Arduino Nicla Vision and XIAO ESP32S3 Sense boards.\n\n\nOpenMV IDE (optional)\n\nDownload and install the OpenMV IDE for your operating system.\nConfigure the OpenMV IDE for the Arduino Nicla Vision.\n\nEdge Impulse Studio\n\nSign up for a free account on the Edge Impulse Studio.\nInstall Edge Impulse CLI\nFollow the guides to connect your Arduino Nicla Vision and XIAO ESP32S3 Sense boards to Edge Impulse Studio.",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#network-connectivity",
    "href": "contents/labs/getting_started.html#network-connectivity",
    "title": "Getting Started",
    "section": "Network Connectivity",
    "text": "Network Connectivity\nSome projects may require internet connectivity for data collection or model deployment. Ensure that your development environment has a stable internet connection, either through Wi-Fi or Ethernet.\n\nFor the Arduino Nicla Vision, you can use the onboard Wi-Fi module to connect to a wireless network.\nFor the XIAO ESP32S3 Sense, you can use the onboard Wi-Fi module or connect an external Wi-Fi or Ethernet module using the available pins.",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/getting_started.html#conclusion",
    "href": "contents/labs/getting_started.html#conclusion",
    "title": "Getting Started",
    "section": "Conclusion",
    "text": "Conclusion\nWith your hardware and software set up, you’re now ready to embark on your embedded machine learning journey. The hands-on labs will guide you through various projects, covering topics such as image classification, object detection, keyword spotting, and motion classification.\nIf you encounter any issues or have questions, don’t hesitate to consult the troubleshooting guides, forums, or reach out to the community for support.\nLet’s dive in and unlock the potential of ML on real (tiny) systems!",
    "crumbs": [
      "LABS",
      "Getting Started"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Introduction\nThe Arduino Nicla Vision (sometimes called NiclaV) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the Nicla Sense ME and the Nicla Voice. The Niclas can efficiently run processes created with TensorFlow Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference), while the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the management of WiFi and Bluetooth Low Energy (BLE) connectivity simultaneously.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#hardware",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#hardware",
    "title": "Setup",
    "section": "Hardware",
    "text": "Hardware\n\nTwo Parallel Cores\nThe central processor is the dual-core STM32H747, including a Cortex M7 at 480 MHz and a Cortex M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\nArduino sketches on top of the Arm Mbed OS\nNative Mbed applications\nMicroPython / JavaScript via an interpreter\nTensorFlow Lite\n\n\n\n\nMemory\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1MB, shared by both processors. This MCU also has incorporated 2MB of FLASH, mainly for code storage.\n\n\nSensors\n\nCamera: A GC2145 2 MP Color CMOS Camera.\nMicrophone: The MP34DT05 is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n6-Axis IMU: 3D gyroscope and 3D accelerometer data from the LSM6DSOX 6-axis IMU.\nTime of Flight Sensor: The VL53L1CBV0FY Time-of-Flight sensor adds accurate and low power-ranging capabilities to the Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#arduino-ide-installation",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#arduino-ide-installation",
    "title": "Setup",
    "section": "Arduino IDE Installation",
    "text": "Arduino IDE Installation\nStart connecting the board (microUSB) to your computer:\n\nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to Tools &gt; Board &gt; Board Manager, look for Arduino Nicla Vision on the search window, and install the board.\n\nNext, go to Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards and select Arduino Nicla Vision. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\nOpen the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n\nTesting the Microphone\nOn Arduino IDE, go to Examples &gt; PDM &gt; PDMSerialPlotter, open and run the sketch. Open the Plotter and see the audio representation from the microphone:\n\n\nVary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n\n\nTesting the IMU\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. For that, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test (you can also run Gyro and board temperature):\n\n\n\nTesting the ToF (Time of Flight) Sensor\nAs we did with IMU, it is necessary to install the VL53L1X ToF library. For that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n\nNext, run the sketch proximity_detection.ino:\n\nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4m).\n\n\n\nTesting the Camera\nWe can also test the camera using, for example, the code provided on Examples &gt; Camera &gt; CameraCaptureRawBytes. We cannot see the image directly, but it is possible to get the raw image data generated by the camera.\nAnyway, the best test with the camera is to see a live image. For that, we will use another IDE, the OpenMV.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#installing-the-openmv-ide",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#installing-the-openmv-ide",
    "title": "Setup",
    "section": "Installing the OpenMV IDE",
    "text": "Installing the OpenMV IDE\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras like the one on the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\nGo to the OpenMV IDE page, download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n\nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from Files &gt; Examples &gt; HelloWord &gt; helloword.py\n\nAny messages sent through a serial connection (using print() or error messages) will be displayed on the Serial Terminal during run time. The image captured by a camera will be displayed in the Camera Viewer Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\n\nBefore connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on Examples &gt; STM_32H747_System STM32H747_manageBootloader. Upload the code to your board. The Serial Monitor will guide you.\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n\nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select Install the latest release firmware (vX.Y.Z). This action will install the latest OpenMV firmware on the Nicla Vision.\n\nYou can leave the option Erase internal file system unselected and click [OK].\nNicla’s green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n\nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, “DFU firmware update complete!”. Press [OK].\n\nA green play button appears when the Nicla Vison connects to the Tool Bar.\n\nAlso, note that a drive named “NO NAME” will appear on your computer.:\n\nEvery time you press the [RESET] button on the board, it automatically executes the main.py script stored on it. You can load the main.py code on the IDE (File &gt; Open File...).\n\n\nThis code is the “Blink” code, confirming that the HW is OK.\n\nFor testing the camera, let’s run helloword_1.py. For that, select the script on File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nWhen clicking the green play button, the MicroPython script (hellowolrd.py) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 14fps.\n\nHere is the helloworld.py script:\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\nIn GitHub, you can find the Python scripts used here.\nThe code can be split into two parts:\n\nSetup: Where the libraries are imported, initialized and the variables are defined and initiated.\nLoop: (while loop) part of the code that runs continually. The image (img variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red [X] button.\n\nNote: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the GitHub, You can find other Python scripts. Try to test the onboard sensors.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "title": "Setup",
    "section": "Connecting the Nicla Vision to Edge Impulse Studio",
    "text": "Connecting the Nicla Vision to Edge Impulse Studio\nWe will need the Edge Impulse Studio later in other exercises. Edge Impulse is a leading development platform for machine learning on edge devices.\nEdge Impulse officially supports the Nicla Vision. So, for starting, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\nDownload the most updated EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary arduino-nicla-vision.bin to your board.\n\nGo to your project on the Studio, and on the Data Acquisition tab, select WebUSB (1). A window will pop up; choose the option that shows that the Nicla is paired (2) and press [Connect] (3).\n\nIn the Collect Data section on the Data Acquisition tab, you can choose which sensor data to pick.\n\nFor example. IMU data:\n\nOr Image (Camera):\n\nAnd so on. You can also test an external sensor connected to the ADC (Nicla pin 0) and the other onboard sensors, such as the microphone and the ToF.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#expanding-the-nicla-vision-board-optional",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#expanding-the-nicla-vision-board-optional",
    "title": "Setup",
    "section": "Expanding the Nicla Vision Board (optional)",
    "text": "Expanding the Nicla Vision Board (optional)\nA last item to be explored is that sometimes, during prototyping, it is essential to experiment with external sensors and devices, and an excellent expansion to the Nicla is the Arduino MKR Connector Carrier (Grove compatible).\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\nNote that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n\nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n\nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n\nThe Grove Light Sensor would be connected to one of the single Analog pins (A0/PC4), the LoRaWAN device to the UART, and the OLED to the I2C connector.\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying “Hello Word” on the Serial Monitor. Here is the code.\n\nBelow is the Hello World code to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nFinally, here is a simple script to read the ADC value on pin “PC4” (Nicla pin A0):\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\nThe ADC can be used for other sensor variables, such as Temperature.\n\nNote that the above scripts (downloaded from Github) introduce only how to connect external devices with the Nicla Vision board using MicroPython.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#conclusion",
    "title": "Setup",
    "section": "Conclusion",
    "text": "Conclusion\nThe Arduino Nicla Vision is an excellent tiny device for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\nOn the GitHub repository, you will find the last version of all the codeused or commented on in this hands-on exercise.",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/setup/setup.html#resources",
    "href": "contents/labs/arduino/nicla_vision/setup/setup.html#resources",
    "title": "Setup",
    "section": "Resources",
    "text": "Resources\n\nMicropython codes\nArduino Codes",
    "crumbs": [
      "Nicla Vision",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Introduction\nAs we initiate our studies into embedded machine learning or TinyML, it’s impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\nIn the “bullseye” of the Radar is the Edge Computer Vision, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is Image Classification, a kind of ML “Hello World”!\nThis exercise will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow’s robust ecosystem, we’ll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be on optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\nWe’ll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you’ll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#computer-vision",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#computer-vision",
    "title": "Image Classification",
    "section": "Computer Vision",
    "text": "Computer Vision\nAt its core, computer vision aims to enable machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system’s ability to understand, interpret, and react to visual stimuli.\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are Image Classification and Object Detection.\n\nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-project-goal",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-project-goal",
    "title": "Image Classification",
    "section": "Image Classification Project Goal",
    "text": "Image Classification Project Goal\nThe first step in any ML project is to define the goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). Also, we will collect images of a background where those two objects are absent.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#data-collection",
    "title": "Image Classification",
    "section": "Data Collection",
    "text": "Data Collection\nOnce you have defined your Machine Learning project goal, the next and most crucial step is the dataset collection. You can use the Edge Impulse Studio, the OpenMV IDE we installed, or even your phone for the image capture. Here, we will use the OpenMV IDE for that.\n\nCollecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, “data.” Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\nThe IDE will ask you to open the file where your data will be saved and choose the “data” folder that was created. Note that new icons will appear on the Left panel.\n\nUsing the upper icon (1), enter with the first class name, for example, “periquito”:\n\nRunning the dataset_capture_script.py and clicking on the camera icon (2), will start capturing images:\n\nRepeat the same procedure with the other classes\n\n\nWe suggest around 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and the RGB565 (color pixel format).\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\nOn your computer, you will end with a dataset that contains three classes: periquito, robot, and background.\n\nYou should return to Edge Impulse Studio and upload the dataset to your project.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio for training our model. Enter your account credentials and create a new project:\n\n\nHere, you can clone a similar project: NICLA-Vision_Image_Classification.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#dataset",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#dataset",
    "title": "Image Classification",
    "section": "Dataset",
    "text": "Dataset\nUsing the EI Studio (or Studio), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into Training, Validation, and Test. The Test Set will be divided from the beginning, and a part will reserved to be used only in the Test phase after training. The Validation Set will be used during training.\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about that specific data:\n\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a very simple project, the data seems OK.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#the-impulse-design",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#the-impulse-design",
    "title": "Image Classification",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model, in this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 27,640 features (96x96x3).\n\nPress [Save parameters] and Generate all features:\n\n\n\nModel Design\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to improve performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is that of thinning a network uniformly at each layer.\nEdge Impulse Studio can use both MobileNetV1 (96x96 images) and V2 (96x96 or 160x160 images), with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3MB RAM and 2.6MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\nWe will use MobileNetV2 96x96 0.1 for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-training",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-training",
    "title": "Image Classification",
    "section": "Model Training",
    "text": "Model Training\nAnother valuable technique to be used with Deep Learning is Data Augmentation. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n\nThe result is excellent, with 77ms of latency, which should result in 13fps (frames per second) during inference.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-testing",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#model-testing",
    "title": "Image Classification",
    "section": "Model Testing",
    "text": "Model Testing\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input:\n\nThe result is, again, excellent.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#deploying-the-model",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#deploying-the-model",
    "title": "Image Classification",
    "section": "Deploying the model",
    "text": "Deploying the model\nAt this point, we can deploy the trained model as.tflite and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n\n\nArduino Library\nFirst, Let’s deploy it as an Arduino Library:\n\nYou should install the library as.zip on the Arduino IDE and run the sketch nicla_vision_camera.ino available in Examples under your library name.\n\nNote that Arduino Nicla Vision has, by default, 512KB of RAM allocated for the M7 core and an additional 244KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nThe result is good, with 86ms of measured latency.\n\nHere is a short video showing the inference results: \n\n\nOpenMV\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware.\nThree files are generated as a library: the trained.tflite model, a list with labels, and a simple MicroPython script that can make inferences using the model.\n\nRunning this model as a .tflite directly in the Nicla was impossible. So, we can sacrifice the accuracy using a smaller model or deploy the model as an OpenMV Firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let’s explore this option.\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\nOn your computer, you will find a ZIP file. Open it:\n\nUse the Bootloader tool on the OpenMV IDE to load the FW on your board:\n\nSelect the appropriate file (.bin for Nicla-Vision):\n\nAfter the download is finished, press OK:\n\nIf a message says that the FW is outdated, DO NOT UPGRADE. Select [NO].\n\nNow, open the script ei_image_classification.py that was downloaded from the Studio and the.bin file for the Nicla.\n\nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n\n\nChanging the Code to add labels\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\nUpload the code from GitHub, or modify it as below:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nHere you can see the result:\n\nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference:\n\nThe latency will drop to only 71 ms.\n\n\nThe NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n\n\nPost-Processing with LEDs\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n\nTo accomplish that, we should upload the code from GitHub or change the last code to include the LEDs:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val &lt; 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\nLed Red 0n: Uncertain (no class is over 0.8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nAll LEDs Off: Background &gt; 0.8\n\nHere is the result:\n\nIn more detail",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-non-official-benchmark",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#image-classification-non-official-benchmark",
    "title": "Image Classification",
    "section": "Image Classification (non-official) Benchmark",
    "text": "Image Classification (non-official) Benchmark\nSeveral development boards can be used for embedded machine learning (TinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n\nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino’s Library:",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\nAutonomous Vehicles: Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\nHealthcare: Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\nRetail: Automated checkout systems that identify products as they pass through a scanner.\nSecurity and Surveillance: Facial recognition, anomaly detection, and object tracking in real-time video feeds.\nAugmented Reality: Object detection and classification to overlay digital information in the real world.\nIndustrial Automation: Visual inspection of products, predictive maintenance, and robot and drone guidance.\nAgriculture: Drone-based crop monitoring and automated harvesting.\nNatural Language Processing: Image captioning and visual question answering.\nGesture Recognition: For gaming, sign language translation, and human-machine interaction.\nContent Recommendation: Image-based recommendation systems in e-commerce.",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#resources",
    "href": "contents/labs/arduino/nicla_vision/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nMicropython codes\nDataset\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "Introduction\nThis is a continuation of CV on Nicla Vision, now exploring Object Detection on microcontrollers.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#introduction",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#introduction",
    "title": "Object Detection",
    "section": "",
    "text": "Object Detection versus Image Classification\nThe main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:\n\nBut what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:\n\nAnd what happens if there is not a dominant category on the image?\n\nThe model identifies the above image completely wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in all previous examples is the MobileNet, trained with a large dataset, the ImageNet.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\nThose models used for Object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually is lower than 1M Bytes.\n\n\nAn innovative solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).\nIn this Hands-On exercise, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-object-detection-project-goal",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-object-detection-project-goal",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nBox\nWheel\n\nHere are some not labeled image samples that we should use to detect the objects (wheels and boxes):\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#data-collection",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nWe can use the Edge Impulse Studio, the OpenMV IDE, your phone, or other devices for the image capture. Here, we will use again the OpenMV IDE for our purpose.\n\nCollecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, “data.” Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\nEdge impulse suggests that the objects should be of similar size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try with mixed sizes and positions to see the result.\n\nWe will not create separate folders for our images because each contains multiple labels.\n\nConnect the Nicla Vision to the OpenMV IDE and run the dataset_capture_script.py. Clicking on the Capture Image button will start capturing images:\n\nWe suggest around 50 images mixing the objects and varying the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#edge-impulse-studio",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#edge-impulse-studio",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\nHere, you can clone the project developed for this hands-on: NICLA_Vision_Object_Detection.\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Nicla Vision as your Target Device:\n\n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer files captured.\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually.\n\n\nAll the not labeled images (51) were uploaded but they still need to be labeled appropriately before using them as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link Labeling queue (51).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels was wrong, you can edit it using the three dots menu after the sample name:\n\nYou will be guided to replace the wrong label, correcting the dataset.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-impulse-design",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#the-impulse-design",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterwards, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n\n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, which is suitable for use with FOMO models and Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nOne of the samples (46) apparently is in the wrong space, but clicking on it can confirm that the labeling is correct.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#model-design-training-and-test",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#model-design-training-and-test",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions which have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35`. This model uses around 250KB RAM and 80KB of ROM (Flash), which suits well with our board since it has 1MB of RAM and ROM.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with practically 1.00 in the F1 score, with a similar result when using the Test data.\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\nTest model with “Live Classification”\nSince Edge Impulse officially supports the Nicla Vision, let’s connect it to the Studio. For that, follow the steps:\n\nDownload the last EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader related to your OS:\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary (arduino-nicla-vision.bin) to your board.\n\nGo to Live classification section at EI Studio, and using webUSB, connect your Nicla Vision:\n\nOnce connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the set-up). Try with 0.8 or more.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#deploying-the-model",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#deploying-the-model",
    "title": "Object Detection",
    "section": "Deploying the Model",
    "text": "Deploying the Model\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\nWhen you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option Load a specific firmware instead.\n\nYou will find a ZIP file on your computer from the Studio. Open it:\n\nLoad the .bin file to your board:\n\nAfter the download is finished, a pop-up message will be displayed. Press OK, and open the script ei_object_detection.py downloaded from the Studio.\nBefore running the script, let’s change a few lines. Note that you can leave the window definition as 240 x 240 and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse\n# Edge Impulse - OpenMV Object Detection Example\n\nimport sensor, image, time, os, tf, math, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\nRedefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.\nmin_confidence = 0.8\nChange if necessary, the color of the circles that will be used to display the detected object’s centroid for a better contrast.\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\ncolors = [ # Add more colors if you are detecting more than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nKeep the remaining code as it is and press the green Play button to run the code:\n\nOn the camera view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window (240X240).\n\nBe ware that the coordinate origin is in the upper left corner.\n\n\nNote that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)\nHere is a short video showing the inference results:",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices, for example, to explore the Nicla doing sensor fusion (camera + microphone) and object detection. This can be very useful on projects involving bees, for example.",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#resources",
    "href": "contents/labs/arduino/nicla_vision/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Introduction\nHaving already explored the Nicla Vision board in the Image Classification and Object Detection applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).\nAs introduced in the Feature Engineering for Audio Classification Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#how-does-a-voice-assistant-work",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#how-does-a-voice-assistant-work",
    "title": "Keyword Spotting (KWS)",
    "section": "How does a voice assistant work?",
    "text": "How does a voice assistant work?\nAs said, voice assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as ” Hey Google” on the first one and “Alexa” on the second.\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\nStage 1: A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the TinyML device (Stage 1).\n\n\nTo explore the above Google Assistant project, please see the tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the Nicla Vision, which has a digital microphone that will be used to spot the keyword.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#the-kws-hands-on-project",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#the-kws-hands-on-project",
    "title": "Keyword Spotting (KWS)",
    "section": "The KWS Hands-On Project",
    "text": "The KWS Hands-On Project\nThe diagram below gives an idea of how the final KWS application should work (during inference):\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no words spoken; only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nFor real-world projects, it is always advisable to include other sounds besides the keywords, such as “Noise” (or Background) and “Unknown.”\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#dataset",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of any Machine Learning Workflow is the dataset. Once we have decided on specific keywords, in our case (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as yes and no, we can get 1,500 samples.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file to a location of your choice.\n\n\nUploading the dataset to the Edge Impulse Studio\nInitiate a new project at Edge Impulse Studio (EIS) and select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\nDefine the Label, select Automatically split between train and test, and Upload data to the EIS. Repeat for all classes.\n\nThe dataset will now appear in the Data acquisition section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets.\n\n\n\nCapturing additional Audio Data\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of sound, this is optional because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.\n\nWhen we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16KHz frequency with 16-bit per sample amplitude.\nSo, any device that can generate audio data with this basic specification (16KHz/16bits) will work fine. As a device, we can use the NiclaV, a computer, or even your mobile phone.\n\n\nUsing the NiclaV and the Edge Impulse Studio\nAs we learned in the chapter Setup Nicla Vision, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:\n\nDownload the last updated EIS Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\nPut the NiclaV in Boot Mode by pressing the reset button twice.\nUpload the binary arduino-nicla-vision.bin to your board by running the batch code corresponding to your OS.\n\nGo to your project on EIS, and on the Data Acquisition tab, select WebUSB. A window will pop up; choose the option that shows that the Nicla is paired and press [Connect].\nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab. Select: Built-in microphone, define your label (for example, yes), the sampling Frequency[16000Hz], and the Sample length (in milliseconds), for example [10s]. Start sampling.\n\nData on Pete’s dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on three dots after the sample name and select Split sample.\nA window will pop up with the Split tool.\n\nOnce inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.\n\n\nUsing a smartphone and the EI Studio\nYou can also use your PC or smartphone to capture audio data, using a sampling frequency of 16KHz and a bit depth of 16.\nGo to Devices, scan the QR Code using your phone, and click on the link. A data Collection app will appear in your browser. Select Collecting Audio, and define your Label, data capture Length, and Category.\n\nRepeat the same procedure used with the NiclaV.\n\nNote that any app, such as Audacity, can be used for audio recording, provided you use 16KHz/16-bit depth samples.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#creating-impulse-pre-process-model-definition",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#creating-impulse-pre-process-model-definition",
    "title": "Keyword Spotting (KWS)",
    "section": "Creating Impulse (Pre-Process / Model definition)",
    "text": "Creating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\nImpulse Design\n\nFirst, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‘zeros’ samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). As discussed in the Feature Engineering for Audio Classification Hands-On tutorial, we will use Audio (MFCC), which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are well suited for the human voice, our case here.\nNext, we select the Classification block to build our model from scratch using a Convolution Neural Network (CNN).\n\nAlternatively, you can use the Transfer Learning (Keyword Spotting) block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.\n\n\n\nPre-Processing (MFCC)\nThe following step is to create the features to be trained in the next phase:\nWe could keep the default parameter values, but we will use the DSP Autotune parameters option.\n\nWe will take the Raw features (our 1-second, 16KHz sampled audio data) and use the MFCC processing block to calculate the Processed features. For every 16,000 raw features (16,000 x 1 second), we will get 637 processed features (13 x 49).\n\nThe result shows that we only used a small amount of memory to pre-process data (16KB) and a latency of 34ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64MHz), the same pre-process will take around 480ms. The parameters chosen, such as the FFT length [512], will significantly impact the latency.\nNow, let’s Save parameters and move to the Generated features tab, where the actual features will be generated. Using UMAP, a dimension reduction technique, the Feature explorer shows how the features are distributed on a two-dimensional plot.\n\nThe result seems OK, with a visually clear separation between yes features (in red) and no features (in blue). The unknown features seem nearer to the no space than the yes. This suggests that the keyword no has more propensity to false positives.\n\n\nGoing under the hood\nTo understand better how the raw sound is preprocessed, look at the Feature Engineering for Audio Classification chapter. You can play with the MFCC features generation by downloading this notebook from GitHub or [Opening it In Colab]",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#model-design-and-training",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#model-design-and-training",
    "title": "Keyword Spotting (KWS)",
    "section": "Model Design and Training",
    "text": "Model Design and Training\nWe will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:\n\nAs hyper-parameters, we will have a Learning Rate of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on SpecAugment. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.\n\n\nUsing 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.\n\nIt is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for yes is 95%, and for no, 91%. That was expected by what we saw with the Feature Explorer (no and unknown at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.\n\nListen to the samples that went wrong. For example, for yes, most of the mistakes were related to a yes pronounced as “yeh”. You can acquire additional samples and then retrain your model.\n\nGoing under the hood\nIf you want to understand what is happening “under the hood,” you can download the pre-processed dataset (MFCC training data) from the Dashboard tab and run this Jupyter Notebook, playing with the code or [Opening it In Colab]. For example, you can analyze the accuracy by each epoch:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#testing",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.\n\nInspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary “trigger” for our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.\nFor NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.\n\nLive Classification\nWe can proceed to the project’s next step but also consider that it is possible to perform Live Classification using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#deploy-and-inference",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the Deployment section, select Arduino Library, and at the bottom, choose Quantized (Int8) and press Build.\n\nWhen the Build button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the Sketch tab, select the option Add .ZIP Library, and Choose the .zip file downloaded by EIS:\n\nNow, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let’s use the NiclaV code example created when we deployed the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab, look for your project, and select nicla-vision/nicla-vision_microphone (or nicla-vision_microphone_continuous)\n\nPress the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#post-processing",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#post-processing",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-processing",
    "text": "Post-processing\nNow that we know the model is working since it detects our keywords, let’s modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOW, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF.\nWe should modify one of the code examples. Let’s do it now with the nicla-vision_microphone_continuous.\nStart with initializing the LEDs:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n    \n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreate two functions, turn_off_leds() function , to turn off all RGB LEDs\n**\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nAnother turn_on_led() function is used to turn on the RGB LEDs according to the most probable result of the classifier.\n/**\n * @brief      turn_on_leds function used to turn on the RGB LEDs\n * @param[in]  pred_index     \n *             no:       [0] ==&gt; Red ON\n *             noise:    [1] ==&gt; ALL OFF \n *             unknown:  [2] ==&gt; Blue ON\n *             Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n    \n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nAnd change the // print the predictions portion of the code on loop():\n...\n\n    if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n        // print the predictions\n        ei_printf(\"Predictions \");\n        ei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n            result.timing.dsp, result.timing.classification, result.timing.anomaly);\n        ei_printf(\": \\n\");\n\n        int pred_index = 0;     // Initialize pred_index\n        float pred_value = 0;   // Initialize pred_value\n\n        for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n            if (result.classification[ix].value &gt; pred_value){\n                pred_index = ix;\n                pred_value = result.classification[ix].value;\n            }\n            // ei_printf(\"    %s: \", result.classification[ix].label);\n            // ei_printf_float(result.classification[ix].value);\n            // ei_printf(\"\\n\");\n        }\n        ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\", \n                  result.classification[pred_index].label, pred_value);\n        turn_on_leds (pred_index);\n\n        \n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nYou can find the complete code on the project’s GitHub.\nUpload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can “trigger” an external device to perform a desired action instead of turning on an LED, as we saw in the introduction.",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#conclusion",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusion",
    "text": "Conclusion\n\nYou will find the notebooks and codeused in this hands-on tutorial on the GitHub repository.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection, Gunshot)\nIndustry (Anomaly Detection)\nMedical (Snore, Cough, Pulmonary diseases)\nNature (Beehive control, insect sound, pouching mitigation)",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/kws/kws.html#resources",
    "href": "contents/labs/arduino/nicla_vision/kws/kws.html#resources",
    "title": "Keyword Spotting (KWS)",
    "section": "Resources",
    "text": "Resources\n\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Introduction\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers’ safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\nBy the end of this tutorial, you’ll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#introduction",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#introduction",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nSetting up the Arduino Nicla Vision Board\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#imu-installation-and-testing",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#imu-installation-and-testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "IMU Installation and testing",
    "text": "IMU Installation and testing\nFor this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, Setup Nicla Vision, the Nicla Vision Board has an onboard 6-axis IMU: 3D gyroscope and 3D accelerometer, the LSM6DSOX. Let’s verify if the LSM6DSOX IMU library is installed. If not, install it.\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n\n\nDefining the Sampling frequency:\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you’re interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\nNature of the Motion: Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\nHardware Limitations: The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\nComputational Resources: Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\nBattery Life: A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\nData Storage: More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, sampling rates of around 50 Hz to 100 Hz are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\nLet’s define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ----------------------------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines -------------------------------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n    \n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2); \n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2); \n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2); \n    }\n  }\n}\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n\n\nNote that with the Nicla board resting on a table (with the camera facing down), the z-axis measures around 9.8m/s\\(^2\\), the expected earth acceleration.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#the-case-study-simulated-container-transportation",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#the-case-study-simulated-container-transportation",
    "title": "Motion Classification and Anomaly Detection",
    "section": "The Case Study: Simulated Container Transportation",
    "text": "The Case Study: Simulated Container Transportation\nWe will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we’ll capture motion data by manually simulating the conditions of:\n\nTerrestrial Transportation (by road or train)\nMaritime-associated Transportation\nVertical Movement via Fork-Lift\nStationary (Idle) period in a Warehouse\n\n\nFrom the above images, we can define for our simulation that primarily horizontal movements (x or y axis) should be associated with the “Terrestrial class,” Vertical movements (z-axis) with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#data-collection",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#data-collection",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection",
    "text": "Data Collection\nFor data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\nConnecting the device to Edge Impulse\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\nDownload the latest firmware and connect it directly to the Data Collection section.\nUse the CLI Data Forwarder tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the Setup Nicla Vision hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let’s do it with the last one.\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\nInstall the Edge Impulse CLI and the Node.js into your computer.\nUpload a sketch for data capture (the one discussed previously in this tutorial).\nUse the CLI Data Forwarder to capture data from the Nicla’s accelerometer and send it to the Studio, as shown in this diagram:\n\n\nStart the CLI Data Forwarder on your terminal, entering (if it is the first time) the following command:\n$ edge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables (for example, accX, accY, and accZ), and device name (for example, NiclaV:\n\nGo to the Devices section on your EI Project and verify if the device is connected (the dot should be green):\n\n\nYou can clone the project developed for this hands-on: NICLA Vision Movement Classification.\n\n\n\nData Collection\nOn the Data Acquisition section, you should see that your board [NiclaV] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50Hz]. The Studio suggests a sample length of [10000] ms (10s). The last thing left is defining the sample label. Let’s start with[terrestrial]:\n\nTerrestrial (palettes in a Truck or Train), moving horizontally. Press [Start Sample]and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n\nAs expected, the movement was captured mainly in the Y-axis (green). In the blue, we see the Z axis, around -10 m/s\\(^2\\) (the Nicla has the camera facing up).\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\nMaritime (pallets in boats into an angry ocean). The movement is captured on all three axes:\n\nLift (Palettes being handled vertically by a Forklift). Movement captured only in the Z-axis:\n\nIdle (Paletts in a warehouse). No movement detected by the accelerometer:\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the three dots menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab. Below, you can see the resulting dataset:\n\nOnce you have captured your dataset, you can explore it in more detail using the Data Explorer, a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as PCA or t-SNE to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n\nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only “vertical”.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#impulse-design",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#impulse-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Impulse Design",
    "text": "Impulse Design\nThe next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a learning block to classify new data. Go to Impulse Design and Create Impulse. The Studio will suggest the basic design. Let’s also add a second Learning Block for Anomaly Detection.\n\nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n\nThe sampling frequency should be automatically captured, if not, enter it: [50]Hz. The Studio suggests a Window Size of 2 seconds ([2000] ms) with a sliding window of [20]ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n\nLet’s dig into those steps and parameters to understand better what we are doing here.\n\nData Pre-Processing Overview\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations.\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or windows. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window increase) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of “cycles of data”.\n\nWith a sampling rate (SR) of 50Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis x 2 seconds x 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\n\nOnce the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n\nEI Studio Spectral Features\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block.\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as FFT or Wavelets.\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of [32].\nThe per axis/channel Time Domain Statistical features are:\n\nRMS: 1 feature\nSkewness: 1 feature\nKurtosis: 1 feature\n\nThe per axis/channel Frequency Domain Spectral features are:\n\nSpectral Power: 16 features (FFT Length/2)\nSkewness: 1 feature\nKurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\nYou can learn more about how each feature is calculated by downloading the notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis or opening it directly on Google CoLab.\n\n\n\nGenerating features\nOnce we understand what the pre-processing does, it is time to finish the job. So, let’s take the raw data (time-series type) and convert it to tabular data. For that, go to the Spectral Features section on the Parameters tab, define the main parameters as discussed in the previous section ([FFT] with [32] points), and select[Save Parameters]:\n\nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#models-training",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#models-training",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Models Training",
    "text": "Models Training\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\nAs hyperparameters, we will use a Learning Rate of [0.005], a Batch size of [32], and [20]% of data for validation for [30] epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n\nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be [32], as suggested by the Studio:",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#testing",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Testing",
    "text": "Testing\nWe can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\nThe default minimum threshold for a considered uncertain result is [0.6] for classification and [0.3] for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can Set confidence thresholds on the three dots menu, besides the Classy all button.\n\nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#deploy",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#deploy",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Deploy",
    "text": "Deploy\nIt is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, you can choose Quantized (Int8) or Unoptimized (float32) and [Build]. A Zip file will be created and downloaded to your computer.\n\nOn your Arduino IDE, go to the Sketch tab, select Add.ZIP Library, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: Library installed.\n\nInference\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select Nicla_vision_fusion:\n\nNote that the code created by Edge Impulse considers a sensor fusion approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n/* Includes ---------------------------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt; \n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nYou can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla [RESET] button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\nIdle and lift classes:\n\n\n\nmaritime and terrestrial:\n\n\nNote that in all situations above, the value of the anomaly score was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, “rolling” the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\nanomaly detection:\n\n\nIn this case, the anomaly is much bigger, over 1.00\n\n\nPost-processing\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if terrestrial is detected, the Green LED will light; if maritime, the Red LED will light, if it is a lift, the Blue LED will light; and if no movement is detected (idle), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously).",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#conclusion",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#conclusion",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe notebooks and codeused in this hands-on tutorial will be found on the GitHub repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\nCase Applications\n\nIndustrial and Manufacturing\n\nPredictive Maintenance: Detecting anomalies in machinery motion to predict failures before they occur.\nQuality Control: Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\nWarehouse Logistics: Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n\n\nHealthcare\n\nPatient Monitoring: Detecting falls or abnormal movements in the elderly or those with mobility issues.\nRehabilitation: Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\nActivity Recognition: Classifying types of physical activity for fitness applications or patient monitoring.\n\n\n\nConsumer Electronics\n\nGesture Control: Interpreting specific motions to control devices, such as turning on lights with a hand wave.\nGaming: Enhancing gaming experiences with motion-controlled inputs.\n\n\n\nTransportation and Logistics\n\nVehicle Telematics: Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\nCargo Monitoring: Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n\n\nSmart Cities and Infrastructure\n\nStructural Health Monitoring: Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\nTraffic Management: Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n\n\nSecurity and Surveillance\n\nIntruder Detection: Detecting motion patterns typical of unauthorized access or other security breaches.\nWildlife Monitoring: Detecting poachers or abnormal animal movements in protected areas.\n\n\n\nAgriculture\n\nEquipment Monitoring: Tracking the performance and usage of agricultural machinery.\nAnimal Behavior Analysis: Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n\n\nEnvironmental Monitoring\n\nSeismic Activity: Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\nOceanography: Studying wave patterns or marine movements for research and safety purposes.\n\n\n\n\nNicla 3D case\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: Arduino Nicla Voice and Vision Wearable Case.\n\nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited.",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#resources",
    "href": "contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html#resources",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Resources",
    "text": "Resources\n\nArduino Code\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project",
    "crumbs": [
      "Nicla Vision",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Introduction\nThe XIAO ESP32S3 Sense is Seeed Studio’s affordable development board, which integrates a camera sensor, digital microphone, and SD card support. Combining embedded ML computing power and photography capability, this development board is a great tool to start with TinyML (intelligent voice and vision AI).\nXIAO ESP32S3 Sense Main Features\nBelow is the general board pinout:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#introduction",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#introduction",
    "title": "Setup",
    "section": "",
    "text": "Powerful MCU Board: Incorporate the ESP32S3 32-bit, dual-core, Xtensa processor chip operating up to 240 MHz, mounted multiple development ports, Arduino / MicroPython supported\nAdvanced Functionality: Detachable OV2640 camera sensor for 1600 * 1200 resolution, compatible with OV5640 camera sensor, integrating an additional digital microphone\nElaborate Power Design: Lithium battery charge management capability offers four power consumption models, which allows for deep sleep mode with power consumption as low as 14μA\nGreat Memory for more Possibilities: Offer 8MB PSRAM and 8MB FLASH, supporting SD card slot for external 32GB FAT memory\nOutstanding RF performance: Support 2.4GHz Wi-Fi and BLE dual wireless communication, support 100m+ remote communication when connected with U.FL antenna\nThumb-sized Compact Design: 21 x 17.5mm, adopting the classic form factor of XIAO, suitable for space-limited projects like wearable devices\n\n\n\n\n\nFor more details, please refer to the Seeed Studio WiKi page:  https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "title": "Setup",
    "section": "Installing the XIAO ESP32S3 Sense on Arduino IDE",
    "text": "Installing the XIAO ESP32S3 Sense on Arduino IDE\nOn Arduino IDE, navigate to File &gt; Preferences, and fill in the URL:\nhttps://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_dev_index.json\non the field ==&gt; Additional Boards Manager URLs\n\nNext, open boards manager. Go to Tools &gt; Board &gt; Boards Manager… and enter with esp32. Select and install the most updated and stable package (avoid alpha versions) :\n\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nOn Tools, select the Board (XIAO ESP32S3):\n\nLast but not least, choose the Port where the ESP32S3 is connected.\nThat is it! The device should be OK. Let’s do some tests.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-board-with-blink",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-board-with-blink",
    "title": "Setup",
    "section": "Testing the board with BLINK",
    "text": "Testing the board with BLINK\nThe XIAO ESP32S3 Sense has a built-in LED that is connected to GPIO21. So, you can run the blink sketch as it is (using the LED_BUILTIN Arduino constant) or by changing the Blink sketch accordingly:\n#define LED_BUILT_IN 21 \n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pin work with inverted logic\n// LOW to Turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNote that the pins work with inverted logic: LOW to Turn on and HIGH to turn off.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#connecting-sense-module-expansion-board",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#connecting-sense-module-expansion-board",
    "title": "Setup",
    "section": "Connecting Sense module (Expansion Board)",
    "text": "Connecting Sense module (Expansion Board)\nWhen purchased, the expansion board is separated from the main board, but installing the expansion board is very simple. You need to align the connector on the expansion board with the B2B connector on the XIAO ESP32S3, press it hard, and when you hear a “click,” the installation is complete.\nAs commented in the introduction, the expansion board, or the “sense” part of the device, has a 1600x1200 OV2640 camera, an SD card slot, and a digital microphone.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#microphone-test",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#microphone-test",
    "title": "Setup",
    "section": "Microphone Test",
    "text": "Microphone Test\nLet’s start with sound detection. Go to the GitHub project and download the sketch: XIAOEsp2s3_Mic_Test and run it on the Arduino IDE:\n\nWhen producing sound, you can verify it on the Serial Plotter.\nSave recorded sound (.wav audio files) to a microSD card.\nNow, the onboard SD Card reader can save .wav audio files. To do that, we need to habilitate the XIAO PSRAM.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. This can be insufficient for some purposes, so up to 16 MB of external PSRAM (pseudo-static RAM) can be connected with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\n\nDownload the sketch Wav_Record, which you can find on GitHub.\nTo execute the code (Wav Record), it is necessary to use the PSRAM function of the ESP-32 chip, so turn it on before uploading.: Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\n\nRun the code Wav_Record.ino\nThis program is executed only once after the user **turns on the serial monitor. It records for 20 seconds and saves the recording file to a microSD card as “arduino_rec.wav.”\nWhen the “.” is output every 1 second in the serial monitor, the program execution is finished, and you can play the recorded sound file with the help of a card reader.\n\n\nThe sound quality is excellent!\n\nThe explanation of how the code works is beyond the scope of this tutorial, but you can find an excellent description on the wiki page.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-camera",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-the-camera",
    "title": "Setup",
    "section": "Testing the Camera",
    "text": "Testing the Camera\nTo test the camera, you should download the folder take_photos_command from GitHub. The folder contains the sketch (.ino) and two .h files with camera details.\n\nRun the code: take_photos_command.ino. Open the Serial Monitor and send the command capture to capture and save the image on the SD Card:\n\n\nVerify that [Both NL & CR] are selected on Serial Monitor.\n\n\nHere is an example of a taken photo:",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-wifi",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#testing-wifi",
    "title": "Setup",
    "section": "Testing WiFi",
    "text": "Testing WiFi\nOne of the XIAO ESP32S3’s differentiators is its WiFi capability. So, let’s test its radio by scanning the Wi-Fi networks around it. You can do this by running one of the code examples on the board.\nGo to Arduino IDE Examples and look for WiFI ==&gt; WiFIScan\nYou should see the Wi-Fi networks (SSIDs and RSSIs) within your device’s range on the serial monitor. Here is what I got in the lab:\n\nSimple WiFi Server (Turning LED ON/OFF)\nLet’s test the device’s capability to behave as a WiFi Server. We will host a simple page on the device that sends commands to turn the XIAO built-in LED ON and OFF.\nLike before, go to GitHub to download the folder using the sketch SimpleWiFiServer.\nBefore running the sketch, you should enter your network credentials:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nYou can monitor how your server is working with the Serial Monitor.\n\nTake the IP address and enter it on your browser:\n\nYou will see a page with links that can turn the built-in LED of your XIAO ON and OFF.\nStreaming video to Web\nNow that you know that you can send commands from the webpage to your device, let’s do the reverse. Let’s take the image captured by the camera and stream it to a webpage:\nDownload from GitHub the folder that contains the code: XIAO-ESP32S3-Streeming_Video.ino.\n\nRemember that the folder contains the.ino file and a couple of .h files necessary to handle the camera.\n\nEnter your credentials and run the sketch. On the Serial monitor, you can find the page address to enter in your browser:\n\nOpen the page on your browser (wait a few seconds to start the streaming). That’s it.\n\nStreamlining what your camera is “seen” can be important when you position it to capture a dataset for an ML project (for example, using the code “take_phots_commands.ino”.\nOf course, we can do both things simultaneously: show what the camera sees on the page and send a command to capture and save the image on the SD card. For that, you can use the code Camera_HTTP_Server_STA, which can be downloaded from GitHub.\n\nThe program will do the following tasks:\n\nSet the camera to JPEG output mode.\nCreate a web page (for example ==&gt; http://192.168.4.119//). The correct address will be displayed on the Serial Monitor.\nIf server.on (“/capture”, HTTP_GET, serverCapture), the program takes a photo and sends it to the Web.\nIt is possible to rotate the image on webPage using the button [ROTATE]\nThe command [CAPTURE] only will preview the image on the webpage, showing its size on the Serial Monitor\nThe [SAVE] command will save an image on the SD Card and show the image on the browser.\nSaved images will follow a sequential naming (image1.jpg, image2.jpg.\n\n\n\nThis program can capture an image dataset with an image classification project.\n\nInspect the code; it will be easier to understand how the camera works. This code was developed based on the great Rui Santos Tutorial ESP32-CAM Take Photo and Display in Web Server, which I invite all of you to visit.\nUsing the CameraWebServer\nIn the Arduino IDE, go to File &gt; Examples &gt; ESP32 &gt; Camera, and select CameraWebServer\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nDo not forget the Tools to enable the PSRAM.\nEnter your wifi credentials and upload the code to the device:\n\nIf the code is executed correctly, you should see the address on the Serial Monitor:\n\nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. Using the [Save] button, you can save an image to your computer download area.\n\nThat’s it! You can save the images directly on your computer for use on projects.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#conclusion",
    "title": "Setup",
    "section": "Conclusion",
    "text": "Conclusion\nThe XIAO ESP32S3 Sense is flexible, inexpensive, and easy to program. With 8 MB of RAM, memory is not an issue, and the device can handle many post-processing tasks, including communication.\nYou will find the last version of the codeon the GitHub repository: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/setup/setup.html#resources",
    "title": "Setup",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Code",
    "crumbs": [
      "XIAO ESP32S3",
      "Setup"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Introduction\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\nAt the forefront of the Emerging Technologies Radar is the universal language of Edge Computer Vision. When we look into Machine Learning (ML) applied to vision, the first concept that greets us is Image Classification, a kind of ML’ Hello World ’ that is both simple and profound!\nThe Seeed Studio XIAO ESP32S3 Sense is a powerful tool that combines camera and SD card support. With its embedded ML computing power and photography capability, it is an excellent starting point for exploring TinyML vision AI.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#a-tinyml-image-classification-project---fruits-versus-veggies",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#a-tinyml-image-classification-project---fruits-versus-veggies",
    "title": "Image Classification",
    "section": "A TinyML Image Classification Project - Fruits versus Veggies",
    "text": "A TinyML Image Classification Project - Fruits versus Veggies\n\nThe whole idea of our project will be to train a model and proceed with inference on the XIAO ESP32S3 Sense. For training, we should find some data (in fact, tons of data!).\nBut first of all, we need a goal! What do we want to classify?\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We will differentiate apples from bananas and potatoes (you can try other categories).\nSo, let’s find a specific dataset that includes images from those categories. Kaggle is a good start:\nhttps://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition\nThis dataset contains images of the following food items:\n\nFruits - banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango.\nVegetables - cucumber, carrot, capsicum, onion, potato, lemon, tomato, radish, beetroot, cabbage, lettuce, spinach, soybean, cauliflower, bell pepper, chili pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant.\n\nEach category is split into the train (100 images), test (10 images), and validation (10 images).\n\nDownload the dataset from the Kaggle website and put it on your computer.\n\n\nOptionally, you can add some fresh photos of bananas, apples, and potatoes from your home kitchen, using, for example, the codediscussed in the setup lab.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. As you may know, Edge Impulse is a leading development platform for machine learning on edge devices.\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\n\nData Acquisition\nNext, on the UPLOAD DATA section, upload from your computer the files from chosen categories:\n\nIt would be best if you now had your training dataset split into three classes of data:\n\n\nYou can upload extra data for further model testing or split the training data. I will leave it as it is to use the most data possible.\n\n\n\nImpulse Design\n\nAn impulse takes raw data (in this case, images), extracts features (resize pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common use of deep learning, but a lot of data should be used to accomplish this task. We have around 90 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach or model” to differentiate an apple from a banana. But, we can solve this issue by re-training a previously trained model with thousands of images. We call this technique “Transfer Learning” (TL).\n\nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\nSo, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:\n\n\nPre-processing (Feature Generation)\nBesides resizing the images, we can change them to Grayscale or keep the actual RGB color depth. Let’s start selecting Grayscale. Doing that, each one of our data samples will have dimension 9, 216 features (96x96x1). Keeping RGB, this dimension would be three times bigger. Working with Grayscale helps to reduce the amount of final memory needed for inference.\n\nRemember to [Save parameters]. This will generate the features to be used in training.\n\n\nModel Design\nTransfer Learning\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be smaller and faster. MobileNet introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.\nEdge Impulse Studio has MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images) available, with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency.\nThe smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).\nFor this first pass, we will use MobileNet V1 and α=0.10.\n\n\n\nTraining\nData Augmentation\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models, creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nUnder the rood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 16 neurons with a 10% dropout for overfitting prevention. Here is the Training output:\n\nThe result could be better. The model reached around 77% accuracy, but the amount of RAM expected to be used during the inference is relatively tiny (about 60 KBytes), which is very good.\n\n\nDeployment\nThe trained model will be deployed as a .zip Arduino library:\n\nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Please select the file you download from Edge Impulse Studio, and that’s it!\n\nUnder the Examples tab on Arduino IDE, you should find a sketch code under your project name.\n\nOpen the Static Buffer example:\n\nYou can see that the first line of code is exactly the calling of a library with all the necessary stuff for running inference on your device.\n#include &lt;XIAO-ESP32S3-CAM-Fruits-vs-Veggies_inferencing.h&gt;\nOf course, this is a generic code (a “template”) that only gets one sample of raw data (stored on the variable: features = {} and runs the classifier, doing the inference. The result is shown on the Serial Monitor.\nWe should get the sample (image) from the camera and pre-process it (resizing to 96x96, converting to grayscale, and flatting it). This will be the input tensor of our model. The output tensor will be a vector with three values (labels), showing the probabilities of each one of the classes.\n\nReturning to your project (Tab Image), copy one of the Raw Data Sample:\n\n9, 216 features will be copied to the clipboard. This is the input tensor (a flattened image of 96x96x1), in this case, bananas. Past this Input tensor onfeatures[] = {0xb2d77b, 0xb5d687, 0xd8e8c0, 0xeaecba, 0xc2cf67, ...}\n\nEdge Impulse included the library ESP NN in its SDK, which contains optimized NN (Neural Network) functions for various Espressif chips, including the ESP32S3 (running at Arduino IDE).\nWhen running the inference, you should get the highest score for “banana.”\n\nGreat news! Our device handles an inference, discovering that the input image is a banana. Also, note that the inference time was around 317ms, resulting in a maximum of 3 fps if you tried to classify images from a video.\nNow, we should incorporate the camera and classify images in real time.\nGo to the Arduino IDE Examples and download from your project the sketch esp32_camera:\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n\nThe modified sketch can be downloaded from GitHub: xiao_esp32s3_camera.\n\nNote that you can optionally keep the pins as a .h file as we did in the Setup Lab.\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start classifying your fruits and vegetables! You can check the result on Serial Monitor.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-the-model-inference",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-the-model-inference",
    "title": "Image Classification",
    "section": "Testing the Model (Inference)",
    "text": "Testing the Model (Inference)\n\nGetting a photo with the camera, the classification result will appear on the Serial Monitor:\n\nOther tests:",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-with-a-bigger-model",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#testing-with-a-bigger-model",
    "title": "Image Classification",
    "section": "Testing with a Bigger Model",
    "text": "Testing with a Bigger Model\nNow, let’s go to the other side of the model size. Let’s select a MobilinetV2 96x96 0.35, having as input RGB images.\n\nEven with a bigger model, the accuracy could be better, and the amount of memory necessary to run the model increases five times, with latency increasing seven times.\n\nNote that the performance here is estimated with a smaller device, the ESP-EYE. The actual inference with the ESP32S3 should be better.\n\nTo improve our model, we will need to train more images.\nEven though our model did not improve accuracy, let’s test whether the XIAO can handle such a bigger model. We will do a simple inference test with the Static Buffer sketch.\nLet’s redeploy the model. If the EON Compiler is enabled when you generate the library, the total memory needed for inference should be reduced, but it does not influence accuracy.\n\n⚠️ Attention - The Xiao ESP32S3 with PSRAM enable has enought memory to run the inference, even in such bigger model. Keep the EON Compiler NOT ENABLED.\n\n\nDoing an inference with MobilinetV2 96x96 0.35, having as input RGB images, the latency was 219ms, which is great for such a bigger model.\n\nFor the test, we can train the model again, using the smallest version of MobileNet V2, with an alpha of 0.05. Interesting that the result in accuracy was higher.\n\n\nNote that the estimated latency for an Arduino Portenta (ou Nicla), running with a clock of 480MHz is 45ms.\n\nDeploying the model, we got an inference of only 135ms, remembering that the XIAO runs with half of the clock used by the Portenta/Nicla (240MHz):",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#running-inference-on-the-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#running-inference-on-the-sensecraft-web-toolkit",
    "title": "Image Classification",
    "section": "Running inference on the SenseCraft-Web-Toolkit",
    "text": "Running inference on the SenseCraft-Web-Toolkit\nOne significant limitation of viewing inference on Arduino IDE is that we can not see what the camera focuses on. A good alternative is the SenseCraft-Web-Toolkit, a visual model deployment tool provided by SSCMA(Seeed SenseCraft Model Assistant). This tool allows you to deploy models to various platforms easily through simple operations. The tool offers a user-friendly interface and does not require any coding.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn the Dashboard, download the model (“block output”): Transfer learning model - TensorFlow Lite (int8 quantized).\n\n\n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n\n\nNote that you should use the labels trained on EI Studio, entering them in alphabetic order (in our case: apple, banana, potato).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\nThe Classification result will be at the top of the image. You can also select the Confidence of your inference cursor Confidence.\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, the same that we have done with the Arduino IDE:\n\nOn Device Log, you will get information as:\n\n\nPreprocess time (image capture and Crop): 4ms;\nInference time (model latency): 106ms,\nPostprocess time (display of the image and inclusion of data): 0ms.\nOutput tensor (classes), for example: [[89,0]]; where 0 is Apple (and 1is banana and 2 is potato)\n\nHere are other screenshots:",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nThe XIAO ESP32S3 Sense is very flexible, inexpensive, and easy to program. The project proves the potential of TinyML. Memory is not an issue; the device can handle many post-processing tasks, including communication.\nYou will find the last version of the codeon the GitHub repository: XIAO-ESP32S3-Sense.",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nDataset\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Image Classification"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "Introduction\nIn the last section regarding Computer Vision (CV) and the XIAO ESP32S3, Image Classification, we learned how to set up and classify images with this remarkable development board. Continuing our CV journey, we will explore Object Detection on microcontrollers.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#introduction",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#introduction",
    "title": "Object Detection",
    "section": "",
    "text": "Object Detection versus Image Classification\nThe main task with Image Classification models is to identify the most probable object category present on an image, for example, to classify between a cat or a dog, dominant “objects” in an image:\n\nBut what happens if there is no dominant category in the image?\n\nAn image classification model identifies the above image utterly wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in the previous images is MobileNet, which is trained with a large dataset, ImageNet, running on a Raspberry Pi.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually has, at most, a few MB as in the case of the XIAO ESP32S3.\n\n\nAn Innovative Solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, such as the Nicla Vision and Portenta (Cortex M7), on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM, ESP-EYE and XIAO ESP32S3 Sense).\nIn this Hands-On project, we will explore Object Detection using FOMO.\n\nTo understand more about FOMO, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-object-detection-project-goal",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-object-detection-project-goal",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial or rural facility and must sort and count oranges (fruits) and particular frogs (bugs).\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nFruit\nBug\n\nHere are some not labeled image samples that we should use to detect the objects (fruits and bugs):\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the XIAO ESP32S3 for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#data-collection",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#data-collection",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nYou can capture images using the XIAO, your phone, or other devices. Here, we will use the XIAO with code from the Arduino IDE ESP32 library.\n\nCollecting Dataset with the XIAO ESP32S3\nOpen the Arduino IDE and select the XIAO_ESP32S3 board (and the port where it is connected). On File &gt; Examples &gt; ESP32 &gt; Camera, select CameraWebServer.\nOn the BOARDS MANAGER panel, confirm that you have installed the latest “stable” package.\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nAnd on Tools, enable the PSRAM. Enter your wifi credentials and upload the code to the device:\n\nIf the code is executed correctly, you should see the address on the Serial Monitor:\n\nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. You can save an image on your computer download area using the [Save] button.\n\nEdge impulse suggests that the objects should be similar in size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe do not need to create separate folders for our images because each contains multiple labels.\n\nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, [Stop Stream] and move your images to a folder.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#edge-impulse-studio",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\nHere, you can clone the project developed for this hands-on: XIAO-ESP32S3-Sense-Object_Detection\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Espressif ESP-EYE (most similar to our board) as your Target Device:\n\n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload files captured as a folder from your computer.\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually. We will upload all of them as training.\n\n\nAll the not-labeled images (47) were uploaded but must be labeled appropriately before being used as a project dataset. The Studio has a tool for that purpose, which you can find in the link Labeling queue (47).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, you can edit it using the three dots menu after the sample name:\n\nYou will be guided to replace the wrong label and correct the dataset.\n\n\n\nBalancing the dataset and split Train/Test\nAfter labeling all data, it was realized that the class fruit had many more samples than the bug. So, 11 new and additional bug images were collected (ending with 58 images). After labeling them, it is time to select some images and move them to the test dataset. You can do it using the three-dot menu after the image name. I selected six images, representing 13% of the total dataset.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#the-impulse-design",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n\n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nSome samples seem to be in the wrong space, but clicking on them confirms the correct labeling.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#model-design-training-and-test",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#model-design-training-and-test",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250KB of RAM and 80KB of ROM (Flash), which suits well with our board.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an overall F1 score of 85%, similar to the result when using the test data (83%).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\nTest model with “Live Classification”\nOnce our model is trained, we can test it using the Live Classification tool. On the correspondent section, click on Connect a development board icon (a small MCU) and scan the QR code with your phone.\n\nOnce connected, you can use the smartphone to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the setup). Try with 0.8 or more.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-arduino-ide",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-arduino-ide",
    "title": "Object Detection",
    "section": "Deploying the Model (Arduino IDE)",
    "text": "Deploying the Model (Arduino IDE)\nSelect the Arduino Library and Quantized (int8) model, enable the EON Compiler on the Deploy Tab, and press [Build].\n\nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Select the file you download from Edge Impulse Studio, and that’s it!\n\nUnder the Examples tab on Arduino IDE, you should find a sketch code (esp32 &gt; esp32_camera) under your project name.\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start detecting fruits and bugs. You can check the result on Serial Monitor.\nBackground\n\nFruits\n\nBugs\n\nNote that the model latency is 143ms, and the frame rate per second is around 7 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around five times higher (around 1.5 fps).",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-sensecraft-web-toolkit",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#deploying-the-model-sensecraft-web-toolkit",
    "title": "Object Detection",
    "section": "Deploying the Model (SenseCraft-Web-Toolkit)",
    "text": "Deploying the Model (SenseCraft-Web-Toolkit)\nAs discussed in the Image Classification chapter, verifying inference with Image models on Arduino IDE is very challenging because we can not see what the camera focuses on. Again, let’s use the SenseCraft-Web Toolkit.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn Dashboard, download the model (“block output”): Object Detection model - TensorFlow Lite (int8 quantized)\n\n\n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n\n\nNote that you should use the labels trained on EI Studio and enter them in alphabetic order (in our case, background, bug, fruit).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\nThe detected objects will be marked (the centroid). You can select the Confidence of your inference cursor Confidence. and IoU, which is used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, as we did with the Arduino IDE.\n\nOn Device Log, you will get information as:\n\nPreprocess time (image capture and Crop): 3 ms;\nInference time (model latency): 115 ms,\nPostprocess time (display of the image and marking objects): 1 ms.\nOutput tensor (boxes), for example, one of the boxes: [[30,150, 20, 20,97, 2]]; where 30,150, 20, 20 are the coordinates of the box (around the centroid); 97 is the inference result, and 2 is the class (in this case 2: fruit)\n\n\nNote that in the above example, we got 5 boxes because none of the fruits got 3 centroids. One solution will be post-processing, where we can aggregate close centroids in one.\n\nHere are other screenshots:",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices.",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Object Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "Introduction\nKeyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and achievable on smaller, low-power devices. This lab will guide you through implementing a KWS system using TinyML on the XIAO ESP32S3 microcontroller board.\nThe XIAO ESP32S3, equipped with Espressif’s ESP32-S3 chip, is a compact and potent microcontroller offering a dual-core Xtensa LX7 processor, integrated Wi-Fi, and Bluetooth. Its balance of computational power, energy efficiency, and versatile connectivity make it a fantastic platform for TinyML applications. Also, with its expansion board, we will have access to the “sense” part of the device, which has a 1600x1200 OV2640 camera, an SD card slot, and a digital microphone. The integrated microphone and the SD card will be essential in this project.\nWe will use the Edge Impulse Studio, a powerful, user-friendly platform that simplifies creating and deploying machine learning models onto edge devices. We’ll train a KWS model step-by-step, optimizing and deploying it onto the XIAO ESP32S3 Sense.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions (in the case of “YES”), bringing your projects to life with voice-activated commands.\nLeveraging our experience with TensorFlow Lite for Microcontrollers (the engine “under the hood” on the EI Studio), we’ll create a KWS system capable of real-time machine learning on the device.\nAs we progress through the lab, we’ll break down each process stage - from data collection and preparation to model training and deployment - to provide a comprehensive understanding of implementing a KWS system on a microcontroller.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#introduction",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#introduction",
    "title": "Keyword Spotting (KWS)",
    "section": "",
    "text": "How does a voice assistant work?\nKeyword Spotting (KWS) is critical to many voice assistants, enabling devices to respond to specific words or phrases. To start, it is essential to realize that Voice Assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as “ Hey Google” on the first one and “Alexa” on the second.\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\nStage 1: A smaller microprocessor inside the Echo Dot or Google Home continuously listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example where I emulate a Google Assistant on a Raspberry Pi (Stage 2), having an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n\n\nIf you want to go deeper on the full project, please see my tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this lab, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone for spotting the keyword.\n\n\nThe KWS Project\nThe below diagram will give an idea of how the final KWS application should work (during inference):\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no keywords spoken, only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nOptionally for real-world projects, it is always advised to include different words than keywords, such as “Noise” (or Background) and “Unknow.”\n\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#dataset",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of Machine Learning Workflow is the dataset. Once we have decided on specific keywords (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In other words, we can get 1,500 samples of yes and no.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file in a location of your choice.\n\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor was essential. In the case of sound, it is different because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.\n\nThe sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16KHz with a 16-bit depth.\nSo, any device that can generate audio data with this basic specification (16Khz/16bits) will work fine. As a device, we can use the proper XIAO ESP32S3 Sense, a computer, or even your mobile phone.\n\nCapturing online Audio Data with Edge Impulse and a smartphone\nIn the lab Motion Classification and Anomaly Detection, we connect our device directly to Edge Impulse Studio for data capturing (having a sampling frequency of 50Hz to 100Hz). For such low frequency, we could use the EI CLI function Data Forwarder, but according to Jan Jongboom, Edge Impulse CTO, audio (16KHz) goes too fast for the data forwarder to be captured. So, once we have the digital data captured by the microphone, we can turn it into a WAV file to be sent to the Studio via Data Uploader (same as we will do with Pete’s dataset).\n\nIf we want to collect audio data directly on the Studio, we can use any smartphone connected online with it. We will not explore this option here, but you can easily follow EI documentation.\n\n\nCapturing (offline) Audio Data with the XIAO ESP32S3 Sense\nThe built-in microphone is the MSM261D3526H1CPM, a PDM digital output MEMS microphone with Multi-modes. Internally, it is connected to the ESP32S3 via an I2S bus using pins IO41 (Clock) and IO41 (Data).\n\nWhat is I2S?\nI2S, or Inter-IC Sound, is a standard protocol for transmitting digital audio from one device to another. It was initially developed by Philips Semiconductor (now NXP Semiconductors). It is commonly used in audio devices such as digital signal processors, digital audio processors, and, more recently, microcontrollers with digital audio capabilities (our case here).\nThe I2S protocol consists of at least three lines:\n\n1. Bit (or Serial) clock line (BCLK or CLK): This line toggles to indicate the start of a new bit of data (pin IO42).\n2. Word select line (WS): This line toggles to indicate the start of a new word (left channel or right channel). The Word select clock (WS) frequency defines the sample rate. In our case, L/R on the microphone is set to ground, meaning that we will use only the left channel (mono).\n3. Data line (SD): This line carries the audio data (pin IO41)\nIn an I2S data stream, the data is sent as a sequence of frames, each containing a left-channel word and a right-channel word. This makes I2S particularly suited for transmitting stereo audio data. However, it can also be used for mono or multichannel audio with additional data lines.\nLet’s start understanding how to capture raw data using the microphone. Go to the GitHub projectand download the sketch: XIAOEsp2s3_Mic_Test:\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nThis code is a simple microphone test for the XIAO ESP32S3 using the I2S (Inter-IC Sound) interface. It sets up the I2S interface to capture audio data at a sample rate of 16 kHz with 16 bits per sample and then continuously reads samples from the microphone and prints them to the serial monitor.\nLet’s dig into the code’s main parts:\n\nInclude the I2S library: This library provides functions to configure and use the I2S interface, which is a standard for connecting digital audio devices.\nI2S.setAllPins(-1, 42, 41, -1, -1): This sets up the I2S pins. The parameters are (-1, 42, 41, -1, -1), where the second parameter (42) is the PIN for the I2S clock (CLK), and the third parameter (41) is the PIN for the I2S data (DATA) line. The other parameters are set to -1, meaning those pins are not used.\nI2S.begin(PDM_MONO_MODE, 16000, 16): This initializes the I2S interface in Pulse Density Modulation (PDM) mono mode, with a sample rate of 16 kHz and 16 bits per sample. If the initialization fails, an error message is printed, and the program halts.\nint sample = I2S.read(): This reads an audio sample from the I2S interface.\n\nIf the sample is valid, it is printed on the Serial Monitor and Plotter.\nBelow is a test “whispering” in two different tones.\n\n\n\nSave recorded sound samples (dataset) as .wav audio files to a microSD card\nLet’s use the onboard SD Card reader to save .wav audio files; we must habilitate the XIAO PSRAM first.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. It can be insufficient for some purposes so that ESP32-S3 can use up to 16 MB of external PSRAM (Psuedostatic RAM) connected in parallel with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\nTurn the PSRAM function of the ESP-32 chip on (Arduino IDE): Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\nDownload the sketch Wav_Record_dataset,which you can find on the project’s GitHub.\n\nThis code records audio using the I2S interface of the Seeed XIAO ESP32S3 Sense board, saves the recording as a.wav file on an SD card, and allows for control of the recording process through commands sent from the serial monitor. The name of the audio file is customizable (it should be the class labels to be used with the training), and multiple recordings can be made, each saved in a new file. The code also includes functionality to increase the volume of the recordings.\nLet’s break down the most essential parts of it:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nThose are the necessary libraries for the program. I2S.h allows for audio input, FS.h provides file system handling capabilities, SD.h enables the program to interact with an SD card, and SPI.h handles the SPI communication with the SD card.\n#define RECORD_TIME   10  \n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nHere, various constants are defined for the program.\n\nRECORD_TIME specifies the length of the audio recording in seconds.\nSAMPLE_RATE and SAMPLE_BITS define the audio quality of the recording.\nWAV_HEADER_SIZE specifies the size of the .wav file header.\nVOLUME_GAIN is used to increase the volume of the recording.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nThese variables keep track of the current file number (to create unique file names), the base file name, and whether the system is currently recording.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n  \n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n  \n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n  Serial.printf(\"Enter with the label name\\n\");\n}\nThe setup function initializes the serial communication, I2S interface for audio input, and SD card interface. If the I2S did not initialize or the SD card fails to mount, it will print an error message and halt execution.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n  }\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\" + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files at once\n    isRecording = false;\n  }\n}\nIn the main loop, the program waits for a command from the serial monitor. If the command is rec, the program starts recording. Otherwise, the command is assumed to be the base name for the .wav files. If it’s currently recording and a base file name is set, it records the audio and saves it as a.wav file. The file names are generated by appending the file number to the base file name.\nvoid record_wav(String fileName)\n{\n  ...\n  \n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                    rec_buffer, \n                    record_size, \n                    &sample_size, \n                    portMAX_DELAY);\n  ...\n}\nThis function records audio and saves it as a.wav file with the given name. It starts by initializing the sample_size and record_size variables. record_size is calculated based on the sample rate, size, and desired recording time. Let’s dig into the essential sections;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nThis section of the code opens the file on the SD card for writing and then generates the .wav file header using the generate_wav_header function. It then writes the header to the file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize() - ESP.getFreePsram());\nThe ps_malloc function allocates memory in the PSRAM for the recording. If the allocation fails (i.e., rec_buffer is NULL), it prints an error message and halts execution.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n         rec_buffer, \n         record_size, \n         &sample_size, \n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nThe i2s_read function reads audio data from the microphone into rec_buffer. It prints an error message if no data is read (sample_size is 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nThis section of the code increases the recording volume by shifting the sample values by VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter a new label\\n\\n\");\nFinally, the audio data is written to the .wav file. If the write operation fails, it prints an error message. After writing, the memory allocated for rec_buffer is freed, and the file is closed. The function finishes by printing a completion message and prompting the user to send a new command.\nvoid generate_wav_header(uint8_t *wav_header,  \n             uint32_t wav_size, \n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nThe generate_wav_header function creates a.wav file header based on the parameters (wav_size and sample_rate). It generates an array of bytes according to the .wav file format, which includes fields for the file size, audio format, number of channels, sample rate, byte rate, block alignment, bits per sample, and data size. The generated header is then copied into the wav_header array passed to the function.\nNow, upload the code to the XIAO and get samples from the keywords (yes and no). You can also capture noise and other words.\nThe Serial monitor will prompt you to receive the label to be recorded.\n\nSend the label (for example, yes). The program will wait for another command: rec\n\nAnd the program will start recording new samples every time a command rec is sent. The files will be saved as yes.1.wav, yes.2.wav, yes.3.wav, etc., until a new label (for example, no) is sent. In this case, you should send the command rec for each new sample, which will be saved as no.1.wav, no.2.wav, no.3.wav, etc.\n\nUltimately, we will get the saved files on the SD card.\n\nThe files are ready to be uploaded to Edge Impulse Studio\n\n\nCapturing (offline) Audio Data Apps\nAlternatively, you can also use your PC or smartphone to capture audio data with a sampling frequency 16KHz and a bit depth of 16 Bits. A good app for that is Voice Recorder Pro (IOS). You should save your records as .wav files and send them to your computer.\n\n\nNote that any app, such as Audacity, can be used for audio recording or even your computer.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#training-model-with-edge-impulse-studio",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#training-model-with-edge-impulse-studio",
    "title": "Keyword Spotting (KWS)",
    "section": "Training model with Edge Impulse Studio",
    "text": "Training model with Edge Impulse Studio\n\nUploading the Data\nWhen the raw dataset is defined and collected (Pete’s dataset + recorded keywords), we should initiate a new project at Edge Impulse Studio:\n\nOnce the project is created, select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\nAnd upload them to the Studio (You can automatically split data in train/test). Repete to all classes and all raw data.\n\nThe samples will now appear in the Data acquisition section.\n\nAll data on Pete’s dataset have a 1s length, but the samples recorded in the previous section have 10s and must be split into 1s samples to be compatible.\nClick on three dots after the sample name and select Split sample.\n\nOnce inside the tool, split the data into 1-second records. If necessary, add or remove segments:\n\nThis procedure should be repeated for all samples.\n\nNote: For longer audio files (minutes), first, split into 10-second segments and after that, use the tool again to get the final 1-second splits.\n\nSuppose we do not split data automatically in train/test during upload. In that case, we can do it manually (using the three dots menu, moving samples individually) or using Perform Train / Test Split on Dashboard - Danger Zone.\n\nWe can optionally check all datasets using the tab Data Explorer.\n\n\n\nCreating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\nFirst, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, I reduced the 1000 ms window on the split tool to avoid noises and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). We will use MFCC, which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are great for the human voice.\n\nNext, we select KERAS for classification and build our model from scratch by doing Image Classification using Convolution Neural Network).\n\n\nPre-Processing (MFCC)\nThe next step is to create the images to be trained in the next phase:\nWe can keep the default parameter values or take advantage of the DSP Autotuneparameters option, which we will do.\n\nThe result will not spend much memory to pre-process data (only 16KB). Still, the estimated processing time is high, 675 ms for an Espressif ESP-EYE (the closest reference available), with a 240KHz clock (same as our device), but with a smaller CPU ( XTensa LX6, versus the LX7 on the ESP32S). The real inference time should be smaller.\nSuppose we need to reduce the inference time later. In that case, we should return to the pre-processing stage and, for example, reduce the FFT length to 256, change the Number of coefficients, or another parameter.\nFor now, let’s keep the parameters defined by the Autotuning tool. Save parameters and generate the features.\n\n\nIf you want to go further with converting temporal serial data into images using FFT, Spectrogram, etc., you can play with this CoLab: Audio Raw Data Analysis.\n\n\n\nModel Design and Training\nWe will use a Convolution Neural Network (CNN) model. The basic architecture is defined with two blocks of Conv1D + MaxPooling (with 8 and 16 neurons, respectively) and a 0.25 Dropout. And on the last layer, after Flattening four neurons, one for each class:\n\nAs hyper-parameters, we will have a Learning Rate of 0.005 and a model that will be trained by 100 epochs. We will also include data augmentation, as some noise. The result seems OK:\n\nIf you want to understand what is happening “under the hood,” you can download the dataset and run a Jupyter Notebook playing with the code. For example, you can analyze the accuracy by each epoch:\n\nThis CoLab Notebook can explain how you can go further: KWS Classifier Project - Looking “Under the hood Training/xiao_esp32s3_keyword_spotting_project_nn_classifier.ipynb).”",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#testing",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data put apart before training (Test Data), we got an accuracy of approximately 87%.\n\nInspecting the F1 score, we can see that for YES. We got 0.95, an excellent result once we used this keyword to “trigger” our postprocessing stage (turn on the built-in LED). Even for NO, we got 0.90. The worst result is for unknown, what is OK.\nWe can proceed with the project, but it is possible to perform Live Classification using a smartphone before deployment on our device. Go to the Live Classification section and click on Connect a Development board:\n\nPoint your phone to the barcode and select the link.\n\nYour phone will be connected to the Studio. Select the option Classification on the app, and when it is running, start testing your keywords, confirming that the model is working with live and real data:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#deploy-and-inference",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and press the button Build.\n\nNow it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the ESP32 code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab look for your project, and select esp32/esp32_microphone:\n\nThis code was created for the ESP-EYE built-in microphone, which should be adapted for our device.\nStart changing the libraries to handle the I2S bus:\n\nBy:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInitialize the IS2 microphone at setup(), including the lines:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nOn the static void capture_samples(void* arg) function, replace the line 153 that reads data from I2S mic:\n\nBy:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, \n                 (void*)sampleBuffer, \n                 i2s_bytes_to_read, \n                 &bytes_read, 100);\nOn function static bool microphone_inference_start(uint32_t n_samples), we should comment or delete lines 198 to 200, where the microphone initialization function is called. This is unnecessary because the I2S microphone was already initialized during the setup().\n\nFinally, on static void microphone_inference_end(void) function, replace line 243:\n\nBy:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#postprocessing",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#postprocessing",
    "title": "Keyword Spotting (KWS)",
    "section": "Postprocessing",
    "text": "Postprocessing\nNow that we know the model is working by detecting our keywords, let’s modify the code to see the internal LED going on every time a YES is detected.\nYou should initialize the LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nAnd change the // print the predictions portion of the previous code (on loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification, result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:\n\nThe idea is that the LED will be ON whenever the keyword YES is detected. In the same way, instead of turning on an LED, this could be a “trigger” for an external device, as we saw in the introduction.",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#conclusion",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusion",
    "text": "Conclusion\nThe Seeed XIAO ESP32S3 Sense is a giant tiny device! However, it is powerful, trustworthy, not expensive, low power, and has suitable sensors to be used on the most common embedded machine learning applications such as vision and sound. Even though Edge Impulse does not officially support XIAO ESP32S3 Sense (yet!), we realized that using the Studio for training and deployment is straightforward.\n\nOn my GitHub repository, you will find the last version all the codeused on this project and the previous ones of the XIAO ESP32S3 series.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection)\nIndustry (Anomaly Detection)\nMedical (Snore, Toss, Pulmonary diseases)\nNature (Beehive control, insect sound)",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/kws/kws.html#resources",
    "title": "Keyword Spotting (KWS)",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS CNN training Colab Notebook\nXIAO ESP32S3 Post-processing Code\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Keyword Spotting (KWS)"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html",
    "title": "Motion Classification and Anomaly Detection",
    "section": "",
    "text": "Introduction\nThe XIAO ESP32S3 Sense, with its built-in camera and mic, is a versatile device. But what if you need to add another type of sensor, such as an IMU? No problem! One of the standout features of the XIAO ESP32S3 is its multiple pins that can be used as an I2C bus (SDA/SCL pins), making it a suitable platform for sensor integration.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#installing-the-imu",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#installing-the-imu",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Installing the IMU",
    "text": "Installing the IMU\nWhen selecting your IMU, the market offers a wide range of devices, each with unique features and capabilities. You could choose, for example, the ADXL362 (3-axis), MAX21100 (6-axis), MPU6050 (6-axis), LIS3DHTR (3-axis), or the LCM20600Seeed Grove— (6-axis), which is part of the IMU 9DOF (lcm20600+AK09918). This variety allows you to tailor your choice to your project’s specific needs.\nFor this project, we will use an IMU, the MPU6050 (or 6500), a low-cost (less than 2.00 USD) 6-axis Accelerometer/Gyroscope unit.\n\nAt the end of the lab, we will also comment on using the LCM20600.\n\nThe MPU-6500 is a 6-axis Motion Tracking device that combines a 3-axis gyroscope, 3-axis accelerometer, and a Digital Motion ProcessorTM (DMP) in a small 3x3x0.9mm package. It also features a 4096-byte FIFO that can lower the traffic on the serial bus interface and reduce power consumption by allowing the system processor to burst read sensor data and then go into a low-power mode.\nWith its dedicated I2C sensor bus, the MPU-6500 directly accepts inputs from external I2C devices. MPU-6500, with its 6-axis integration, on-chip DMP, and run-time calibration firmware, enables manufacturers to eliminate the costly and complex selection, qualification, and system-level integration of discrete devices, guaranteeing optimal motion performance for consumers. MPU-6500 is also designed to interface with multiple non-inertial digital sensors, such as pressure sensors, on its auxiliary I2C port.\n\n\nUsually, the libraries available are for MPU6050, but they work for both devices.\n\nConnecting the HW\nConnect the IMU to the XIAO according to the below diagram:\n\nMPU6050 SCL –&gt; XIAO D5\nMPU6050 SDA –&gt; XIAO D4\nMPU6050 VCC –&gt; XIAO 3.3V\nMPU6050 GND –&gt; XIAO GND\n\n\nInstall the Library\nGo to Arduino Library Manager and type MPU6050. Install the latest version.\n\nDownload the sketch MPU6050_Acc_Data_Acquisition.in:\n/*\n * Based on I2C device class (I2Cdev) Arduino sketch for MPU6050 class \n   by Jeff Rowberg &lt;jeff@rowberg.net&gt;\n * and Edge Impulse Data Forwarder Exampe (Arduino) \n   - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * \n * Developed by M.Rovai @11May23\n */\n\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n\n// convert factor g to m/s2 ==&gt; [-32768, +32767] ==&gt; [-2g, +2g]\n#define CONVERT_G_TO_MS2    (9.81/(16384.0/(1.+ACC_RANGE))) \n\nstatic unsigned long last_interval_ms = 0;\n\nMPU6050 imu;\nint16_t ax, ay, az;\n\nvoid setup() {\n  \n    Serial.begin(115200);\n\n    \n    // initialize device\n    Serial.println(\"Initializing I2C devices...\");\n    Wire.begin();\n    imu.initialize();\n    delay(10);\n    \n//    // verify connection\n//    if (imu.testConnection()) {\n//      Serial.println(\"IMU connected\");\n//    }\n//    else {\n//      Serial.println(\"IMU Error\");\n//    }\n    delay(300);\n    \n    //Set MCU 6050 OffSet Calibration \n    imu.setXAccelOffset(-4732);\n    imu.setYAccelOffset(4703);\n    imu.setZAccelOffset(8867);\n    imu.setXGyroOffset(61);\n    imu.setYGyroOffset(-73);\n    imu.setZGyroOffset(35);\n    \n    /* Set full-scale accelerometer range.\n     * 0 = +/- 2g\n     * 1 = +/- 4g\n     * 2 = +/- 8g\n     * 3 = +/- 16g\n     */\n    imu.setFullScaleAccelRange(ACC_RANGE);\n}\n\nvoid loop() {\n\n      if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n        last_interval_ms = millis();\n        \n        // read raw accel/gyro measurements from device\n        imu.getAcceleration(&ax, &ay, &az);\n\n        // converting to m/s2\n        float ax_m_s2 = ax * CONVERT_G_TO_MS2;\n        float ay_m_s2 = ay * CONVERT_G_TO_MS2;\n        float az_m_s2 = az * CONVERT_G_TO_MS2;\n\n        Serial.print(ax_m_s2); \n        Serial.print(\"\\t\");\n        Serial.print(ay_m_s2); \n        Serial.print(\"\\t\");\n        Serial.println(az_m_s2); \n      }\n}\nSome comments about the code:\nNote that the values generated by the accelerometer and gyroscope have a range: [-32768, +32767], so for example, if the default accelerometer range is used, the range in Gs should be: [-2g, +2g]. So, “1G” means 16384.\nFor conversion to m/s2, for example, you can define the following:\n#define CONVERT_G_TO_MS2 (9.81/16384.0)\nIn the code, I left an option (ACC_RANGE) to be set to 0 (+/-2G) or 1 (+/- 4G). We will use +/-4G; that should be enough for us. In this case.\nWe will capture the accelerometer data on a frequency of 50Hz, and the acceleration data will be sent to the Serial Port as meters per squared second (m/s2).\nWhen you ran the code with the IMU resting over your table, the accelerometer data shown on the Serial Monitor should be around 0.00, 0.00, and 9.81. If the values are a lot different, you should calibrate the IMU.\nThe MCU6050 can be calibrated using the sketch: mcu6050-calibration.ino.\nRun the code. The following will be displayed on the Serial Monitor:\n\nSend any character (in the above example, “x”), and the calibration should start.\n\nNote that A message MPU6050 connection failed. Ignore this message. For some reason, imu.testConnection() is not returning a correct result.\n\nIn the end, you will receive the offset values to be used on all your sketches:\n\nTake the values and use them on the setup:\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\nNow, run the sketch MPU6050_Acc_Data_Acquisition.in:\nOnce you run the above sketch, open the Serial Monitor:\n\nOr check the Plotter:\n\nMove your device in the three axes. You should see the variation on Plotter:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#the-tinyml-motion-classification-project",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#the-tinyml-motion-classification-project",
    "title": "Motion Classification and Anomaly Detection",
    "section": "The TinyML Motion Classification Project",
    "text": "The TinyML Motion Classification Project\nFor our lab, we will simulate mechanical stresses in transport. Our problem will be to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (palettes in a Truck or Train)\nLift (Palettes being handled by Fork-Lift)\nIdle (Palettes in Storage houses)\n\nSo, to start, we should collect data. Then, accelerometers will provide the data on the palette (or container).\n\nFrom the above images, we can see that primarily horizontal movements should be associated with the “Terrestrial class,” Vertical movements with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#connecting-the-device-to-edge-impulse",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#connecting-the-device-to-edge-impulse",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Connecting the device to Edge Impulse",
    "text": "Connecting the device to Edge Impulse\nFor data collection, we should first connect our device to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\nFollow the instructions hereto install the Node.jsand Edge Impulse CLI on your computer.\n\nOnce the XIAO ESP32S3 is not a fully supported development board by Edge Impulse, we should, for example, use the CLI Data Forwarder to capture data from our sensor and send it to the Studio, as shown in this diagram:\n\n\nYou can alternately capture your data “offline,” store them on an SD card or send them to your computer via Bluetooth or Wi-Fi. In this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\nConnect your device to the serial port and run the previous code to capture IMU (Accelerometer) data, “printing them” on the serial. This will allow the Edge Impulse Studio to “capture” them.\nGo to the Edge Impulse page and create a project.\n\n\nThe maximum length for an Arduino library name is 63 characters. Note that the Studio will name the final library using your project name and include “_inference” to it. The name I chose initially did not work when I tried to deploy the Arduino library because it resulted in 64 characters. So, I need to change it by taking out the “anomaly detection” part.\n\nStart the CLI Data Forwarderon your terminal, entering (if it is the first time) the following command:\nedge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables, and device names:\n\nGo to your EI Project and verify if the device is connected (the dot should be green):",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-collection",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-collection",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection",
    "text": "Data Collection\nAs discussed before, we should capture data from all four Transportation Classes. Imagine that you have a container with a built-in accelerometer:\n\nNow imagine your container is on a boat, facing an angry ocean, on a truck, etc.:\n\nMaritime (pallets in boats)\n\nMove the XIAO in all directions, simulating an undulatory boat movement.\n\nTerrestrial (palettes in a Truck or Train)\n\nMove the XIAO over a horizontal line.\n\nLift (Palettes being handled by\n\nMove the XIAO over a vertical line.\n\nIdle (Palettes in Storage houses)\n\nLeave the XIAO over the table.\n\n\n\nBelow is one sample (raw data) of 10 seconds:\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds each) for the four classes. Using the “3 dots” after each one of the samples, select 2, moving them for the Test set (or use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab). Below, you can see the result datasets:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-pre-processing",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#data-pre-processing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data type captured by the accelerometer is a “time series” and should be converted to “tabular data”. We can do this conversion using a sliding window over the sample data. For example, in the below figure,\n\nWe can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50Hz. A 2-second window will capture 300 data points (3 axis x 2 seconds x 50 samples). We will slide this window each 200ms, creating a larger dataset where each instance has 300 raw features.\n\nYou should use the best SR for your case, considering Nyquist’s theorem, which states that a periodic signal must be sampled at more than twice the signal’s highest frequency component.\n\nData preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.\nOn the Studio, this dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as “FFT” or “Wavelets”. In the most common case, FFT, the Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness\nKurtosis\n\nFor example, for an FFT length of 32 points, the Spectral Analysis Block’s resulting output will be 21 features per axis (a total of 63 features).\nThose 63 features will be the Input Tensor of a Neural Network Classifier and the Anomaly Detection model (K-Means).\n\nYou can learn more by digging into the lab DSP Spectral Features",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#model-design",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#model-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Model Design",
    "text": "Model Design\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#impulse-design",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#impulse-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Impulse Design",
    "text": "Impulse Design\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\nWe also take advantage of a second model, the K-means, that can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean).\n\n\nImagine our XIAO rolling or moving upside-down, on a movement complement different from the one trained\n\n\nBelow is our final Impulse design:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#generating-features",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#generating-features",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Generating features",
    "text": "Generating features\nAt this point in our project, we have defined the pre-processing method and the model designed. Now, it is time to have the job done. First, let’s take the raw data (time-series type) and convert it to tabular data. Go to the Spectral Features tab and select Save Parameters:\n\nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but also for general non-linear dimension reduction.\n\nThe visualization allows one to verify that the classes present an excellent separation, which indicates that the classifier should work well.\n\nOptionally, you can analyze the relative importance of each feature for one class compared with other classes.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#training",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#training",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Training",
    "text": "Training\nOur model has four layers, as shown below:\n\nAs hyperparameters, we will use a Learning Rate of 0.005 and 20% of data for validation for 30 epochs. After training, we can see that the accuracy is 97%.\n\nFor anomaly detection, we should choose the suggested features that are precisely the most important in feature extraction. The number of clusters will be 32, as suggested by the Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#testing",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Testing",
    "text": "Testing\nUsing 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was not that good (8%), mainly due to the terrestrial class. Once we have four classes (which output should add 1.0), we can set up a lower threshold for a class to be considered valid (for example, 0.4):\n\nNow, the Test accuracy will go up to 97%.\n\nYou should also use your device (which is still connected to the Studio) and perform some Live Classification.\n\nBe aware that here you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#deploy",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#deploy",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Deploy",
    "text": "Deploy\nNow it is time for magic˜! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and Build. A Zip file will be created and downloaded to your computer.\n\nOn your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#inference",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#inference",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Inference",
    "text": "Inference\nNow, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select nano_ble_sense_accelerometer:\n\nOf course, this is not your board, but we can have the code working with only a few changes.\nFor example, at the beginning of the code, you have the library related to Arduino Sense IMU:\n/* Includes --------------------------------------------------------------- */\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nChange the “includes” portion with the code related to the IMU:\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\nChange the Constant Defines\n/* Constant defines ------------------------------------------------------- */\nMPU6050 imu;\nint16_t ax, ay, az;\n\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n#define CONVERT_G_TO_MS2    (9.81/(16384/(1.+ACC_RANGE)))\n#define MAX_ACCEPTED_RANGE  (2*9.81)+(2*9.81)*ACC_RANGE\nOn the setup function, initiate the IMU set the off-set values and range:\n// initialize device\nSerial.println(\"Initializing I2C devices...\");\nWire.begin();\nimu.initialize();\ndelay(10);\n\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\n\nimu.setFullScaleAccelRange(ACC_RANGE);\nAt the loop function, the buffers buffer[ix], buffer[ix + 1], and buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. On the original code, you have the line:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nChange it with this block of code:\nimu.getAcceleration(&ax, &ay, &az);       \nbuffer[ix + 0] = ax;\nbuffer[ix + 1] = ay;\nbuffer[ix + 2] = az;\nYou should change the order of the following two blocks of code. First, you make the conversion to raw data to “Meters per squared second (ms2)”, followed by the test regarding the maximum acceptance range (that here is in ms2, but on Arduino, was in Gs):\nbuffer[ix + 0] *= CONVERT_G_TO_MS2;\nbuffer[ix + 1] *= CONVERT_G_TO_MS2;\nbuffer[ix + 2] *= CONVERT_G_TO_MS2;\n\nfor (int i = 0; i &lt; 3; i++) {\n     if (fabs(buffer[ix + i]) &gt; MAX_ACCEPTED_RANGE) {\n        buffer[ix + i] = ei_get_sign(buffer[ix + i]) * MAX_ACCEPTED_RANGE;\n     }\n}\nAnd that is it! You can now upload the code to your device and proceed with the inferences. The complete code is available on the project’s GitHub.\nNow you should try your movements, seeing the result of the inference of each class on the images:\n\n\n\n\nAnd, of course, some “anomaly”, for example, putting the XIAO upside-down. The anomaly score will be over 1:",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#conclusion",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#conclusion",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\nRegarding the IMU, this project used the low-cost MPU6050 but could also use other IMUs, for example, the LCM20600 (6-axis), which is part of the Seeed Grove - IMU 9DOF (lcm20600+AK09918). You can take advantage of this sensor, which has integrated a Grove connector, which can be helpful in the case you use the XIAO with an extension board, as shown below:\n\nYou can follow the instructions here to connect the IMU with the MCU. Only note that for using the Grove ICM20600 Accelerometer, it is essential to update the files I2Cdev.cpp and I2Cdev.h that you will download from the library provided by Seeed Studio. For that, replace both files from this link. You can find a sketch for testing the IMU on the GitHub project: accelerometer_test.ino.\n\nOn the projet’s GitHub repository, you will find the last version of all codeand other docs: XIAO-ESP32S3 - IMU.",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#resources",
    "href": "contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html#resources",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project",
    "crumbs": [
      "XIAO ESP32S3",
      "Motion Classification and Anomaly Detection"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "Introduction\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with “under-the-hood” mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don’t understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction",
    "title": "KWS Feature Engineering",
    "section": "",
    "text": "This tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#the-kws",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#the-kws",
    "title": "KWS Feature Engineering",
    "section": "The KWS",
    "text": "The KWS\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition aims to transcribe all spoken words into text, Keyword Spotting focuses on detecting specific “keywords” or “wake words” in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n\n\nApplications of KWS\n\nVoice Assistants: In devices like Amazon’s Alexa or Google Home, KWS is used to detect the wake word (“Alexa” or “Hey Google”) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like “Start engine” or “Turn off lights.”\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\nDifferences from General Speech Recognition\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction-to-audio-signals",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction-to-audio-signals",
    "title": "KWS Feature Engineering",
    "section": "Introduction to Audio Signals",
    "text": "Introduction to Audio Signals\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals’ tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16KHz. Altough music tones can be heard at frequencies up to 20 kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8 kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs. Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal’s tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal’s constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal’s spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n\n\nWhy Not Raw Audio?\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn’t inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction-to-mfccs",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#introduction-to-mfccs",
    "title": "KWS Feature Engineering",
    "section": "Introduction to MFCCs",
    "text": "Introduction to MFCCs\n\nWhat are MFCCs?\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal’s spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n\n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\nWhy are MFCCs important?\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear’s response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\nComputing MFCCs\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let’s walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: y(t) = x(t) - \\(\\alpha\\) x(t-1) , where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n\n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f) = |X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n\n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear’s response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n\n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "title": "KWS Feature Engineering",
    "section": "Hands-On using Python",
    "text": "Hands-On using Python\nLet’s apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#conclusion",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#conclusion",
    "title": "KWS Feature Engineering",
    "section": "Conclusion",
    "text": "Conclusion\nWhat Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\nMFCCs are particularly strong for\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\nSpectrograms or MFEs are often more suitable for\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts.",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#resources",
    "href": "contents/labs/shared/kws_feature_eng/kws_feature_eng.html#resources",
    "title": "KWS Feature Engineering",
    "section": "Resources",
    "text": "Resources\n\nAudio_Data_Analysis Colab Notebook",
    "crumbs": [
      "Shared Labs",
      "KWS Feature Engineering"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html",
    "title": "DSP Spectral Features",
    "section": "",
    "text": "Introduction\nTinyML projects related to motion (or vibration) involve data from IMUs (usually accelerometers and Gyroscopes). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block for Inertial sensors.\nBut how does it work under the hood? Let’s dig into it.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "title": "DSP Spectral Features",
    "section": "Extracting Features Review",
    "text": "Extracting Features Review\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations. Here’s a high-level overview of the process:\nData collection: First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It’s essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\nData preprocessing: Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\nThe Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentation: Depending on the nature of the data and the application, dividing the data into smaller segments or windows may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window span) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of “data cycles.”\nFeature extraction: Once the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\nLet’s explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "title": "DSP Spectral Features",
    "section": "A TinyML Motion Classification project",
    "text": "A TinyML Motion Classification project\n\nIn the hands-on project, Motion Classification and Anomaly Detection, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (pallets in a Truck or Train)\nLift (pallets being handled by Fork-Lift)\nIdle (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n\nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50Hz:\n\n\nThe result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5Hz instead of 50Hz.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "title": "DSP Spectral Features",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data captured by the accelerometer (a “time series” data) should be converted to “tabular data” using one of the typical Feature Extraction methods described in the last section.\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis x 2 seconds x 62.5 samples). The window is slid every 80ms, creating a larger dataset where each instance has 375 “raw features.”\n\nOn the Studio, the previous version (V1) of the Spectral Analysis Block extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n\nThose 33 features were the Input tensor of a Neural Network Classifier.\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\nEdge Impulse - Spectral Analysis Block V.2 under the hood\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness (in the next version)\nKurtosis (in the next version)\n\nIn this link, we can have more details about the feature extraction.\n\nClone the public project. You can also follow the explanation, playing with the code using my Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nStart importing the libraries:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nFrom the studied project, let’s choose a data sample from accelerometers as below:\n\nWindow size of 2 seconds: [2,000] ms\nSample frequency: [62.5] Hz\nWe will choose the [None] filter (for simplicity) and a\nFFT length: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n\nSelecting the Raw Features on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n\nPaste the data points to a new variable data:\ndata=[-5.6330, 0.2376, 9.8701, -5.9442, 0.4830, 9.8701, -5.4217, ...]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nThe total raw features are 375, but we will work with each axis individually, where N= 125 (number of samples per axis).\nWe aim to understand how Edge Impulse gets the processed features.\n\nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\nfeatures = [2.7322, -0.0978, -0.3813, 2.3980, 3.8924, 24.6841, 9.6303, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nThe total number of processed features is 39, which means 13 features/axis.\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n[rms] [skew] [kurtosis]\n\nand 10 for the frequency domain (we will return to this later).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSplitting raw data per sensor\nThe data has samples from all axes; let’s split and plot them separately:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ] \nplot_data(sensors, axis, 'Raw Features')\n\nSubtracting the mean\nNext, we should subtract the mean from the data. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\nHere are some specific reasons why subtracting the mean can be helpful:\n\nIt simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\nIt removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\nIt can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\nIt can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\ndtmean = [(sum(x)/len(x)) for x in sensors]\n[print('mean_'+x+'= ', round(y, 4)) for x,y in zip(axis, dtmean)][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "title": "DSP Spectral Features",
    "section": "Time Domain Statistical features",
    "text": "Time Domain Statistical features\nRMS Calculation\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the “value of the direct current that dissipates the same power in a resistor.”\nIn the case of a set of n values {𝑥1, 𝑥2, …, 𝑥𝑛}, the RMS is:\n\n\nNOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n# Using numpy and standartized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nCompared with Edge Impulse result features:\n[2.7322, 0.7833, 0.1383]\nSkewness and kurtosis calculation\nIn statistics, skewness and kurtosis are two ways to measure the shape of a distribution.\nHere, we can see the sensor values distribution:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n\nSkewness is a measure of the asymmetry of a distribution. This value can be positive or negative.\n\n\nA negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\nA positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\nA zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4)) for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nCompared with Edge Impulse result features:\n[-0.0978, 0.1735, 6.8629]\nKurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n\n\nThe kurtosis of a normal distribution is zero.\nIf a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\nIf a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4)) for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nCompared with Edge Impulse result features:\n[-0.3813, 1.1696, 65.3726]",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "title": "DSP Spectral Features",
    "section": "Spectral features",
    "text": "Spectral features\nThe filtered signal is passed to the Spectral power section, which computes the FFT to generate the spectral features.\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or “sub-windows”), and the FFT is calculated over each frame.\nFFT length - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\nThe total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\nSpectral Power - Welch’s method\nWe should use Welch’s method to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len, and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplying the above function to 3 signals:\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\nWe can plot the Power Spectrum P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n\nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nLet’s now list all Spectral features per axis and compare them with EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) #13: 3+N_feat_axis;  26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "title": "DSP Spectral Features",
    "section": "Time-frequency domain",
    "text": "Time-frequency domain\n\nWavelets\nWavelet is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a wavelet function, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\nLet’s select Wavelet on the Spectral Features block in the same project:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n\nThe Wavelet Function\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n\nAs we did before, let’s copy and past the Processed Features:\n\nfeatures = [3.6251, 0.0615, 0.0615, -7.3517, -2.7641, 2.8462, 5.0924, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse computes the Discrete Wavelet Transform (DWT) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\nIn the case of Wavelets, the extracted features are basic statistical values, crossing values, and entropy. There are, in total, 14 features per layer as below:\n\n[11] Statiscal Features: n5, n25, n75, n95, mean, median, standard deviation (std), variance (var) root mean square (rms), kurtosis, and skewness (skew).\n[2] Crossing Features: Zero crossing rate (zcross) and mean crossing rate (mcross) are the times that the signal passes through the baseline (y = 0) and the average level (y = u) per unit of time, respectively\n[1] Complexity Feature: Entropy is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\nThe total number of features varies depending on how you set the filter and the number of layers. For example, with [None] filtering and Level[1], the number of features per axis will be 14 x 2 (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n\n\nWavelet Analysis\nWavelet analysis decomposes the signal (accX, accY, and accZ) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as accX_l1, accY_l1, accZ_l1 and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as accX_d1, accY_d1, accZ_d1, permitting the extraction of features for further analysis or classification.\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the “Multilevel 1D Discrete Wavelet Transform”, the result will be a list (for detail, please see: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n\n\n\nFeature Extraction\nLet’s start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n \nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nThe Skelness and Kurtosis:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal’s frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\nMean crossing (mcross), on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) &lt; 0).su    m())/len(arr)))\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nIn wavelet analysis, entropy refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal’s uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nLet’s now list all the wavelet features and create a list by layers.\nL1_features_names = [\"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\", \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\", \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"]\n\nL0_features_names = [\"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\", \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\", \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = stat_feat_l0[i]+[skew_l0[i]]+[kurtosis_l0[i]]+[cross_l0[0][i]]+[cross_l0[1][i]]+[entropy_l0[i]]\n    [print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L0_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\nall_feat_l0 = [item for sublist in all_feat_l0 for item in sublist]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\nfeat_l1 = stat_feat_l1[i]+[skew_l1[i]]+[kurtosis_l1[i]]+[cross_l1[0][i]]+[cross_l1[1][i]]+[entropy_l1[i]]\n[print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L1_features_names, feat_l1)][0]\nall_feat_l1.append(feat_l1)\nall_feat_l1 = [item for sublist in all_feat_l1 for item in sublist]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "href": "contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "title": "DSP Spectral Features",
    "section": "Conclusion",
    "text": "Conclusion\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\nDaniel Situnayake wrote in his blog: “Raw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.” I recommend you read Dan’s excellent post in its totality: nn to cpp: What you need to know about porting deep learning models to the edge.",
    "crumbs": [
      "Shared Labs",
      "DSP Spectral Features"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya\nMironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with\nDifferential Privacy.” In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Security, 308–18. CCS\n’16. New York, NY, USA: ACM. https://doi.org/10.1145/2976749.2978318.\n\n\nAbdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi\nSchwarzschild, Manli Shu, Christoph Studer, and Chen Zhu. 2020.\n“Headless Horseman: Adversarial Attacks on Transfer\nLearning Models.” In ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\n3087–91. IEEE. https://doi.org/10.1109/icassp40776.2020.9053181.\n\n\nAddepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, and\nR. Venkatesh Babu. 2020. “Towards Achieving Adversarial Robustness\nby Enforcing Feature Consistency Across Bit Planes.” In 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 1020–29. IEEE. https://doi.org/10.1109/cvpr42600.2020.00110.\n\n\nAdolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, and David\nBrooks. 2016. “Fathom: Reference Workloads for Modern\nDeep Learning Methods.” In 2016 IEEE International Symposium\non Workload Characterization (IISWC), 1–10. IEEE; IEEE. https://doi.org/10.1109/iiswc.2016.7581275.\n\n\nAgarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, and\nHanna M. Wallach. 2018. “A Reductions Approach to Fair\nClassification.” In Proceedings of the 35th International\nConference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,\nSweden, July 10-15, 2018, edited by Jennifer G. Dy and Andreas\nKrause, 80:60–69. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/agarwal18a.html.\n\n\nAgnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta,\nAustin Jiao, Ben Keller, Brucek Khailany, and Haoxing Ren. 2023.\n“AutoDMP: Automated DREAMPlace-Based Macro\nPlacement.” In Proceedings of the 2023 International\nSymposium on Physical Design, 149–57. ACM. https://doi.org/10.1145/3569052.3578923.\n\n\nAgrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, and\nBerk Sunar. 2007. “Trojan Detection Using\nIC Fingerprinting.” In 2007 IEEE Symposium on\nSecurity and Privacy (SP ’07), 29–45. Springer; IEEE. https://doi.org/10.1109/sp.2007.36.\n\n\nAhmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud\nDaneshtalab, and Maksim Jenihhin. 2024. “A Systematic Literature\nReview on Hardware Reliability Assessment Methods for Deep Neural\nNetworks.” ACM Comput. Surv. 56 (6): 1–39. https://doi.org/10.1145/3638242.\n\n\nAledhari, Mohammed, Rehma Razzak, Reza M. Parizi, and Fahad Saeed. 2020.\n“Federated Learning: A Survey on Enabling\nTechnologies, Protocols, and Applications.” #IEEE_O_ACC#\n8: 140699–725. https://doi.org/10.1109/access.2020.3013541.\n\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak,\nShahab Asoodeh, and Flavio Calmon. 2022. “Beyond Adult and\nCOMPAS: Fair Multi-Class Prediction via\nInformation Projection.” Adv. Neur. In. 35: 38747–60.\n\n\nAltayeb, Moez, Marco Zennaro, and Marcelo Rovai. 2022.\n“Classifying Mosquito Wingbeat Sound Using\nTinyML.” In Proceedings of the 2022 ACM\nConference on Information Technology for Social Good, 132–37. ACM.\nhttps://doi.org/10.1145/3524458.3547258.\n\n\nAmiel, Frederic, Christophe Clavier, and Michael Tunstall. 2006.\n“Fault Analysis of DPA-Resistant Algorithms.”\nIn International Workshop on Fault Diagnosis and Tolerance in\nCryptography, 223–36. Springer.\n\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain,\nMichael Voznesensky, Bin Bao, et al. 2024. “PyTorch\n2: Faster Machine Learning Through Dynamic Python Bytecode\nTransformation and Graph Compilation.” In Proceedings of the\n29th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume 2, edited by\nHanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd’Alché-Buc, Emily B. Fox, and Roman Garnett, 8024–35. ACM. https://doi.org/10.1145/3620665.3640366.\n\n\nAnthony, Lasse F. Wolff, Benjamin Kanding, and Raghavendra Selvan. 2020.\nICML Workshop on Challenges in Deploying and monitoring Machine Learning\nSystems.\n\n\nAntol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv\nBatra, C. Lawrence Zitnick, and Devi Parikh. 2015.\n“VQA: Visual Question Answering.”\nIn 2015 IEEE International Conference on Computer Vision\n(ICCV), 2425–33. IEEE. https://doi.org/10.1109/iccv.2015.279.\n\n\nAntonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie\nBursztein, Jaime Cochran, Zakir Durumeric, et al. 2017.\n“Understanding the Mirai Botnet.” In 26th USENIX\nSecurity Symposium (USENIX Security 17), 1093–1110.\n\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer,\nMichael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and\nGregor Weber. 2020. “Common Voice: A\nMassively-Multilingual Speech Corpus.” In Proceedings of the\nTwelfth Language Resources and Evaluation Conference, 4218–22.\nMarseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.520.\n\n\nArifeen, Tooba, Abdus Sami Hassan, and Jeong-A Lee. 2020.\n“Approximate Triple Modular Redundancy: A\nSurvey.” #IEEE_O_ACC# 8: 139851–67. https://doi.org/10.1109/access.2020.3012673.\n\n\nAsonov, D., and R. Agrawal. 2004. “Keyboard Acoustic\nEmanations.” In IEEE Symposium on Security and Privacy, 2004.\nProceedings. 2004, 3–11. IEEE; IEEE. https://doi.org/10.1109/secpri.2004.1301311.\n\n\nAteniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,\nDomenico Vitali, and Giovanni Felici. 2015. “Hacking Smart\nMachines with Smarter Ones: How to Extract Meaningful Data\nfrom Machine Learning Classifiers.” Int. J. Secur. Netw.\n10 (3): 137. https://doi.org/10.1504/ijsn.2015.071829.\n\n\nAttia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman,\nSuraj Kapa, Paul A. Friedman, and Peter A. Noseworthy. 2018.\n“Noninvasive Assessment of Dofetilide Plasma Concentration Using a\nDeep Learning (Neural Network) Analysis of the Surface\nElectrocardiogram: A Proof of Concept Study.”\nPLoS One 13 (8): e0201059. https://doi.org/10.1371/journal.pone.0201059.\n\n\nAygun, Sercan, Ece Olcay Gunes, and Christophe De Vleeschouwer. 2021.\n“Efficient and Robust Bitstream Processing in Binarised Neural\nNetworks.” Electron. Lett. 57 (5): 219–22. https://doi.org/10.1049/ell2.12045.\n\n\nBai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.\n“Recent Advances in Adversarial Training for Adversarial\nRobustness.” arXiv Preprint arXiv:2102.01356.\n\n\nBains, Sunny. 2020. “The Business of Building Brains.”\nNature Electronics 3 (7): 348–51. https://doi.org/10.1038/s41928-020-0449-1.\n\n\nBamoumen, Hatim, Anas Temouden, Nabil Benamar, and Yousra Chtouki. 2022.\n“How TinyML Can Be Leveraged to Solve Environmental\nProblems: A Survey.” In 2022 International\nConference on Innovation and Intelligence for Informatics, Computing,\nand Technologies (3ICT), 338–43. IEEE; IEEE. https://doi.org/10.1109/3ict56508.2022.9990661.\n\n\nBank, Dor, Noam Koenigstein, and Raja Giryes. 2023.\n“Autoencoders.” Machine Learning for Data Science\nHandbook: Data Mining and Knowledge Discovery Handbook, 353–74.\n\n\nBannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes.\n2019. “Computer and Redundancy Solution for the Full Self-Driving\nComputer.” In 2019 IEEE Hot Chips 31 Symposium (HCS),\n1–22. IEEE Computer Society; IEEE. https://doi.org/10.1109/hotchips.2019.8875645.\n\n\nBarenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro\nPellicioli, and Gerardo Pelosi. 2010. “Low Voltage Fault Attacks\nto AES.” In 2010 IEEE International Symposium on\nHardware-Oriented Security and Trust (HOST), 7–12. IEEE; IEEE. https://doi.org/10.1109/hst.2010.5513121.\n\n\nBarroso, Luiz André, Urs Hölzle, and Parthasarathy Ranganathan. 2019.\nThe Datacenter as a Computer: Designing Warehouse-Scale\nMachines. Springer International Publishing. https://doi.org/10.1007/978-3-031-01761-2.\n\n\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.\n2017. “Network Dissection: Quantifying\nInterpretability of Deep Visual Representations.” In 2017\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n3319–27. IEEE. https://doi.org/10.1109/cvpr.2017.354.\n\n\nBeaton, Albert E., and John W. Tukey. 1974. “The Fitting of Power\nSeries, Meaning Polynomials, Illustrated on Band-Spectroscopic\nData.” Technometrics 16 (2): 147. https://doi.org/10.2307/1267936.\n\n\nBeck, Nathaniel, and Simon Jackman. 1998. “Beyond Linearity by\nDefault: Generalized Additive Models.” Am. J.\nPolit. Sci. 42 (2): 596. https://doi.org/10.2307/2991772.\n\n\nBender, Emily M., and Batya Friedman. 2018. “Data Statements for\nNatural Language Processing: Toward Mitigating System Bias\nand Enabling Better Science.” Transactions of the Association\nfor Computational Linguistics 6 (December): 587–604. https://doi.org/10.1162/tacl_a_00041.\n\n\nBerger, Vance W, and YanYan Zhou. 2014.\n“Kolmogorovsmirnov Test:\nOverview.” Wiley Statsref: Statistics Reference\nOnline.\n\n\nBeyer, Lucas, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and\nAäron van den Oord. 2020. “Are We Done with Imagenet?”\nArXiv Preprint abs/2006.07159. https://arxiv.org/abs/2006.07159.\n\n\nBhagoji, Arjun Nitin, Warren He, Bo Li, and Dawn Song. 2018.\n“Practical Black-Box Attacks on Deep Neural Networks Using\nEfficient Query Mechanisms.” In Proceedings of the European\nConference on Computer Vision (ECCV), 154–69.\n\n\nBhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel\nHernández-Lobato, and Gu-Yeon Wei. 2020. “A Comprehensive\nMethodology to Determine Optimal Coherence Interfaces for\nMany-Accelerator SoCs.” In Proceedings of the\nACM/IEEE International Symposium on Low Power Electronics and\nDesign, 145–50. ACM. https://doi.org/10.1145/3370748.3406564.\n\n\nBianco, Simone, Remi Cadene, Luigi Celona, and Paolo Napoletano. 2018.\n“Benchmark Analysis of Representative Deep Neural Network\nArchitectures.” IEEE Access 6: 64270–77.\n\n\nBiega, Asia J., Peter Potash, Hal Daumé, Fernando Diaz, and Michèle\nFinck. 2020. “Operationalizing the Legal Principle of Data\nMinimization for Personalization.” In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, edited by Jimmy Huang, Yi Chang, Xueqi\nCheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, 399–408.\nACM. https://doi.org/10.1145/3397271.3401034.\n\n\nBiggio, Battista, Blaine Nelson, and Pavel Laskov. 2012.\n“Poisoning Attacks Against Support Vector Machines.” In\nProceedings of the 29th International Conference on Machine\nLearning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1,\n2012. icml.cc / Omnipress. http://icml.cc/2012/papers/880.pdf.\n\n\nBiggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony\nSou, Catherine Ramsdale, Ken Williamson, Richard Price, and Scott White.\n2021. “A Natively Flexible 32-Bit Arm Microprocessor.”\nNature 595 (7868): 532–36. https://doi.org/10.1038/s41586-021-03625-w.\n\n\nBinkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt,\nAli Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. “The Gem5\nSimulator.” ACM SIGARCH Computer Architecture News 39\n(2): 1–7. https://doi.org/10.1145/2024716.2024718.\n\n\nBohr, Adam, and Kaveh Memarzadeh. 2020. “The Rise of Artificial\nIntelligence in Healthcare Applications.” In Artificial\nIntelligence in Healthcare, 25–60. Elsevier. https://doi.org/10.1016/b978-0-12-818438-7.00002-2.\n\n\nBolchini, Cristiana, Luca Cassano, Antonio Miele, and Alessandro Toschi.\n2023. “Fast and Accurate Error Simulation for CNNs\nAgainst Soft Errors.” IEEE Trans. Comput. 72 (4):\n984–97. https://doi.org/10.1109/tc.2022.3184274.\n\n\nBondi, Elizabeth, Ashish Kapoor, Debadeepta Dey, James Piavis, Shital\nShah, Robert Hannaford, Arvind Iyer, Lucas Joppa, and Milind Tambe.\n2018. “Near Real-Time Detection of Poachers from Drones in\nAirSim.” In Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artificial Intelligence, edited\nby Jérôme Lang, 5814–16. International Joint Conferences on Artificial\nIntelligence Organization. https://doi.org/10.24963/ijcai.2018/847.\n\n\nBourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo,\nHengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas\nPapernot. 2021. “Machine Unlearning.” In 2021 IEEE\nSymposium on Security and Privacy (SP), 141–59. IEEE; IEEE. https://doi.org/10.1109/sp40001.2021.00019.\n\n\nBreier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang\nLiu. 2018. “Deeplaser: Practical Fault Attack on Deep\nNeural Networks.” ArXiv Preprint abs/1806.05859. https://arxiv.org/abs/1806.05859.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language\nModels Are Few-Shot Learners.” In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.” In Conference on Fairness, Accountability\nand Transparency, 77–91. PMLR.\n\n\nBurnet, David, and Richard Thomas. 1989. “Spycatcher:\nThe Commodification of Truth.” J. Law Soc.\n16 (2): 210. https://doi.org/10.2307/1410360.\n\n\nBurr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng,\nJau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. “Recent\nProgress in Phase-Change?Pub _Newline ?Memory\nTechnology.” IEEE Journal on Emerging and Selected Topics in\nCircuits and Systems 6 (2): 146–62. https://doi.org/10.1109/jetcas.2016.2547718.\n\n\nBushnell, Michael L, and Vishwani D Agrawal. 2002. “Built-in\nSelf-Test.” Essentials of Electronic Testing for Digital,\nMemory and Mixed-Signal VLSI Circuits, 489–548.\n\n\nBuyya, Rajkumar, Anton Beloglazov, and Jemal Abawajy. 2010.\n“Energy-Efficient Management of Data Center Resources for Cloud\nComputing: A Vision, Architectural Elements, and Open\nChallenges.” https://arxiv.org/abs/1006.0308.\n\n\nCai, Carrie J., Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel\nSmilkov, Martin Wattenberg, et al. 2019. “Human-Centered Tools for\nCoping with Imperfect Algorithms During Medical Decision-Making.”\nIn Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems, edited by Jennifer G. Dy and Andreas Krause,\n80:2673–82. Proceedings of Machine Learning Research. ACM. https://doi.org/10.1145/3290605.3300234.\n\n\nCai, Han, Chuang Gan, Ligeng Zhu, and Song Han. 2020.\n“TinyTL: Reduce Memory, Not Parameters\nfor Efficient on-Device Learning.” In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html.\n\n\nCai, Han, Ligeng Zhu, and Song Han. 2019.\n“ProxylessNAS: Direct Neural\nArchitecture Search on Target Task and Hardware.” In 7th\nInternational Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HylVB3AqYm.\n\n\nCalvo, Rafael A, Dorian Peters, Karina Vold, and Richard M Ryan. 2020.\n“Supporting Human Autonomy in AI Systems:\nA Framework for Ethical Enquiry.” Ethics of\nDigital Well-Being: A Multidisciplinary Approach, 31–54.\n\n\nCarlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah\nSherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. “Hidden\nVoice Commands.” In 25th USENIX Security Symposium (USENIX\nSecurity 16), 513–30.\n\n\nCarta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero,\nand Roberto Saia. 2020. “A Local Feature Engineering Strategy to\nImprove Network Anomaly Detection.” Future Internet 12\n(10): 177. https://doi.org/10.3390/fi12100177.\n\n\nCavoukian, Ann. 2009. “Privacy by Design.” Office of\nthe Information and Privacy Commissioner.\n\n\nCenci, Marcelo Pilotto, Tatiana Scarazzato, Daniel Dotto Munchen, Paula\nCristina Dartora, Hugo Marcelo Veit, Andrea Moura Bernardes, and Pablo\nR. Dias. 2021. “Eco-Friendly\nElectronicsA Comprehensive Review.”\nAdv. Mater. Technol. 7 (2): 2001263. https://doi.org/10.1002/admt.202001263.\n\n\nChallenge, WEF Net-Zero. 2021. “The Supply Chain\nOpportunity.” In World Economic Forum: Geneva,\nSwitzerland.\n\n\nChandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. “Anomaly\nDetection: A Survey.” ACM Comput. Surv. 41 (3): 1–58. https://doi.org/10.1145/1541880.1541882.\n\n\nChapelle, O., B. Scholkopf, and A. Zien Eds. 2009.\n“Semi-Supervised Learning (Chapelle, O.\nEt Al., Eds.; 2006) [Book Reviews].” IEEE Trans.\nNeural Networks 20 (3): 542–42. https://doi.org/10.1109/tnn.2009.2015974.\n\n\nChen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and\nJonathan Su. 2019. “This Looks Like That: Deep\nLearning for Interpretable Image Recognition.” In Advances in\nNeural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, edited by Hanna M. Wallach, Hugo Larochelle,\nAlina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman\nGarnett, 8928–39. https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html.\n\n\nChen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, and Pranav\nRajpurkar. 2023. “A Framework for Integrating Artificial\nIntelligence for Clinical Care with Continuous Therapeutic\nMonitoring.” Nat. Biomed. Eng., November. https://doi.org/10.1038/s41551-023-01115-0.\n\n\nChen, H.-W. 2006. “Gallium, Indium, and Arsenic Pollution of\nGroundwater from a Semiconductor Manufacturing Area of\nTaiwan.” B. Environ. Contam. Tox. 77 (2):\n289–96. https://doi.org/10.1007/s00128-006-1062-3.\n\n\nChen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,\nHaichen Shen, Meghan Cowan, et al. 2018. “TVM:\nAn Automated End-to-End Optimizing Compiler for Deep\nLearning.” In 13th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 18), 578–94.\n\n\nChen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\n“Training Deep Nets with Sublinear Memory Cost.” ArXiv\nPreprint abs/1604.06174. https://arxiv.org/abs/1604.06174.\n\n\nChen, Zhiyong, and Shugong Xu. 2023. “Learning\nDomain-Heterogeneous Speaker Recognition Systems with Personalized\nContinual Federated Learning.” EURASIP Journal on Audio,\nSpeech, and Music Processing 2023 (1): 33. https://doi.org/10.1186/s13636-023-00299-2.\n\n\nChen, Zitao, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben.\n2019. “iBinFI/i: An Efficient Fault\nInjector for Safety-Critical Machine Learning Systems.” In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis. SC ’19. New York, NY,\nUSA: ACM. https://doi.org/10.1145/3295500.3356177.\n\n\nChen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik\nPattabiraman, and Nathan DeBardeleben. 2020.\n“TensorFI: A Flexible Fault Injection\nFramework for TensorFlow Applications.” In 2020\nIEEE 31st International Symposium on Software Reliability Engineering\n(ISSRE), 426–35. IEEE; IEEE. https://doi.org/10.1109/issre5003.2020.00047.\n\n\nCheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher,\nHyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. “Clear:\nuC/u Ross u-l/u Ayer uE/u Xploration for uA/u Rchitecting uR/u Esilience\n- Combining Hardware and Software Techniques to Tolerate Soft Errors in\nProcessor Cores.” In Proceedings of the 53rd Annual Design\nAutomation Conference, 1–6. ACM. https://doi.org/10.1145/2897937.2897996.\n\n\nCheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2018. “Model\nCompression and Acceleration for Deep Neural Networks: The\nPrinciples, Progress, and Challenges.” IEEE Signal Process\nMag. 35 (1): 126–36. https://doi.org/10.1109/msp.2017.2765695.\n\n\nChi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu,\nYu Wang, and Yuan Xie. 2016. “Prime: A Novel Processing-in-Memory\nArchitecture for Neural Network Computation in ReRAM-Based Main\nMemory.” ACM SIGARCH Computer Architecture News 44 (3):\n27–39. https://doi.org/10.1145/3007787.3001140.\n\n\nChollet, François. 2018. “Introduction to Keras.” March\n9th.\n\n\nChristiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\nand Dario Amodei. 2017. “Deep Reinforcement Learning from Human\nPreferences.” In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, edited by Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S.\nV. N. Vishwanathan, and Roman Garnett, 4299–4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\n\n\nChu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton,\nPieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew\nHoward. 2021. “Discovering Multi-Hardware Mobile Models via\nArchitecture Search.” In 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), 3022–31. IEEE. https://doi.org/10.1109/cvprw53098.2021.00337.\n\n\nChua, L. 1971. “Memristor-the Missing Circuit Element.”\n#IEEE_J_CT# 18 (5): 507–19. https://doi.org/10.1109/tct.1971.1083337.\n\n\nChung, Jae-Won, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and\nMosharaf Chowdhury. 2023. “Perseus: Removing Energy\nBloat from Large Model Training.” ArXiv Preprint\nabs/2312.06902. https://arxiv.org/abs/2312.06902.\n\n\nCohen, Maxime C., Ruben Lobel, and Georgia Perakis. 2016. “The\nImpact of Demand Uncertainty on Consumer Subsidies for Green Technology\nAdoption.” Manage. Sci. 62 (5): 1235–58. https://doi.org/10.1287/mnsc.2015.2173.\n\n\nColeman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter\nBailis, Alexander C. Berg, Robert D. Nowak, Roshan Sumbaly, Matei\nZaharia, and I. Zeki Yalniz. 2022. “Similarity Search for\nEfficient Active Learning and Search of Rare Concepts.” In\nThirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022,\nThirty-Fourth Conference on Innovative Applications of Artificial\nIntelligence, IAAI 2022, the Twelveth Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March\n1, 2022, 6402–10. AAAI Press. https://ojs.aaai.org/index.php/AAAI/article/view/20591.\n\n\nColeman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao,\nJian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia.\n2019. “Analysis of DAWNBench, a Time-to-Accuracy\nMachine Learning Performance Benchmark.” ACM SIGOPS Operating\nSystems Review 53 (1): 14–25. https://doi.org/10.1145/3352020.3352024.\n\n\nConstantinescu, Cristian. 2008. “Intermittent Faults and Effects\non Reliability of Integrated Circuits.” In 2008 Annual\nReliability and Maintainability Symposium, 370–74. IEEE; IEEE. https://doi.org/10.1109/rams.2008.4925824.\n\n\nCooper, Tom, Suzanne Fallender, Joyann Pafumi, Jon Dettling, Sebastien\nHumbert, and Lindsay Lessard. 2011. “A Semiconductor Company’s\nExamination of Its Water Footprint Approach.” In Proceedings\nof the 2011 IEEE International Symposium on Sustainable Systems and\nTechnology, 1–6. IEEE; IEEE. https://doi.org/10.1109/issst.2011.5936865.\n\n\nCope, Gord. 2009. “Pure Water, Semiconductors and the\nRecession.” Global Water Intelligence 10 (10).\n\n\nCourbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and\nYoshua Bengio. 2016. “Binarized Neural Networks:\nTraining Deep Neural Networks with Weights and Activations\nConstrained to+ 1 or-1.” arXiv Preprint\narXiv:1602.02830.\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism.\nMIT press.\n\n\nDarvish Rouhani, Bita, Azalia Mirhoseini, and Farinaz Koushanfar. 2017.\n“TinyDL: Just-in-time\nDeep Learning Solution for Constrained Embedded Systems.” In\n2017 IEEE International Symposium on Circuits and Systems\n(ISCAS), 1–4. IEEE. https://doi.org/10.1109/iscas.2017.8050343.\n\n\nDavarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana\nTurner, Carver Middleton, Will Carroll, et al. 2023. “Closing the\nWearable Gap: Footankle\nKinematic Modeling via Deep Learning Models Based on a Smart Sock\nWearable.” Wearable Technologies 4. https://doi.org/10.1017/wtc.2023.3.\n\n\nDavid, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat\nJeffries, Jian Li, Nick Kreeger, et al. 2021. “Tensorflow Lite\nMicro: Embedded Machine Learning for Tinyml\nSystems.” Proceedings of Machine Learning and Systems 3:\n800–811.\n\n\nDavies, Emma. 2011. “Endangered Elements: Critical\nThinking.” https://www.rsc.org/images/Endangered\\%20Elements\\%20-\\%20Critical\\%20Thinking\\_tcm18-196054.pdf.\n\n\nDavies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya,\nYongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018.\n“Loihi: A Neuromorphic Manycore Processor with\non-Chip Learning.” IEEE Micro 38 (1): 82–99. https://doi.org/10.1109/mm.2018.112130359.\n\n\nDavies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya,\nGabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, and Sumedh R.\nRisbud. 2021. “Advancing Neuromorphic Computing with Loihi:\nA Survey of Results and Outlook.” Proc.\nIEEE 109 (5): 911–34. https://doi.org/10.1109/jproc.2021.3067593.\n\n\nDavis, Jacqueline, Daniel Bizo, Andy Lawrence, Owen Rogers, and Max\nSmolaks. 2022. “Uptime Institute Global Data Center Survey\n2022.” Uptime Institute.\n\n\nDayarathna, Miyuru, Yonggang Wen, and Rui Fan. 2016. “Data Center\nEnergy Consumption Modeling: A Survey.” IEEE\nCommunications Surveys &Amp; Tutorials 18 (1): 732–94. https://doi.org/10.1109/comst.2015.2481183.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc\nV. Le, Mark Z. Mao, et al. 2012. “Large Scale Distributed Deep\nNetworks.” In Advances in Neural Information Processing\nSystems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake\nTahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando\nC. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q.\nWeinberger, 1232–40. https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li.\n2009. “ImageNet: A Large-Scale\nHierarchical Image Database.” In 2009 IEEE Conference on\nComputer Vision and Pattern Recognition, 248–55. IEEE. https://doi.org/10.1109/cvpr.2009.5206848.\n\n\nDesai, Tanvi, Felix Ritchie, Richard Welpton, et al. 2016. “Five\nSafes: Designing Data Access for Research.”\nEconomics Working Paper Series 1601: 28.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding.” In\nProceedings of the 2019 Conference of the North, 4171–86.\nMinneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423.\n\n\nDhar, Sauptik, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh\nKurup, and Mohak Shah. 2021. “A Survey of on-Device Machine\nLearning: An Algorithms and Learning Theory Perspective.” ACM\nTransactions on Internet of Things 2 (3): 1–49. https://doi.org/10.1145/3450494.\n\n\nDong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T.\nKung, and Ziyun Li. 2022. “SplitNets:\nDesigning Neural Architectures for Efficient Distributed\nComputing on Head-Mounted Systems.” In 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\n12549–59. IEEE. https://doi.org/10.1109/cvpr52688.2022.01223.\n\n\nDongarra, Jack J. 2009. “The Evolution of High Performance\nComputing on System z.” IBM J. Res. Dev. 53: 3–4.\n\n\nDuarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi,\nShvetank Prakash, and Vijay Janapa Reddi. 2022.\n“FastML Science Benchmarks: Accelerating\nReal-Time Scientific Edge Machine Learning.” ArXiv\nPreprint abs/2207.07958. https://arxiv.org/abs/2207.07958.\n\n\nDuchi, John C., Elad Hazan, and Yoram Singer. 2010. “Adaptive\nSubgradient Methods for Online Learning and Stochastic\nOptimization.” In COLT 2010 - the 23rd Conference on Learning\nTheory, Haifa, Israel, June 27-29, 2010, edited by Adam Tauman\nKalai and Mehryar Mohri, 257–69. Omnipress. http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.\n\n\nDuisterhof, Bardienus P, Srivatsan Krishnan, Jonathan J Cruz, Colby R\nBanbury, William Fu, Aleksandra Faust, Guido CHE de Croon, and Vijay\nJanapa Reddi. 2019. “Learning to Seek: Autonomous\nSource Seeking with Deep Reinforcement Learning Onboard a Nano Drone\nMicrocontroller.” ArXiv Preprint abs/1909.11236. https://arxiv.org/abs/1909.11236.\n\n\nDuisterhof, Bardienus P., Shushuai Li, Javier Burgues, Vijay Janapa\nReddi, and Guido C. H. E. de Croon. 2021. “Sniffy Bug:\nA Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in\nCluttered Environments.” In 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), 9099–9106.\nIEEE; IEEE. https://doi.org/10.1109/iros51168.2021.9636217.\n\n\nDürr, Marc, Gunnar Nissen, Kurt-Wolfram Sühs, Philipp Schwenkenbecher,\nChristian Geis, Marius Ringelstein, Hans-Peter Hartung, et al. 2021.\n“CSF Findings in Acute NMDAR and LGI1 Antibody–Associated\nAutoimmune Encephalitis.” Neurology Neuroimmunology &Amp;\nNeuroinflammation 8 (6). https://doi.org/10.1212/nxi.0000000000001086.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n“Calibrating Noise to Sensitivity in Private Data\nAnalysis.” In Theory of Cryptography, edited by Shai\nHalevi and Tal Rabin, 265–84. Berlin, Heidelberg: Springer Berlin\nHeidelberg.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations\nof Differential Privacy.” Foundations and Trends\nin Theoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nEbrahimi, Khosrow, Gerard F. Jones, and Amy S. Fleischer. 2014. “A\nReview of Data Center Cooling Technology, Operating Conditions and the\nCorresponding Low-Grade Waste Heat Recovery Opportunities.”\nRenewable Sustainable Energy Rev. 31 (March): 622–38. https://doi.org/10.1016/j.rser.2013.12.007.\n\n\nEgwutuoha, Ifeanyi P., David Levy, Bran Selic, and Shiping Chen. 2013.\n“A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart\nImplementations for High Performance Computing Systems.” The\nJournal of Supercomputing 65 (3): 1302–26. https://doi.org/10.1007/s11227-013-0884-0.\n\n\nEisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere,\nRaghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and\nMurali Annavaram. 2022. “Check-n-Run: A Checkpointing\nSystem for Training Deep Learning Recommendation Models.” In\n19th USENIX Symposium on Networked Systems Design and Implementation\n(NSDI 22), 929–43.\n\n\nEldan, Ronen, and Mark Russinovich. 2023. “Who’s Harry Potter?\nApproximate Unlearning in LLMs.” ArXiv\nPreprint abs/2310.02238. https://arxiv.org/abs/2310.02238.\n\n\nEl-Rayis, A. O. 2014. “Reconfigurable Architectures for the Next\nGeneration of Mobile Device Telecommunications Systems.” :\nhttps://www.researchgate.net/publication/292608967.\n\n\nEshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor\nLenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu.\n2023. “Training Spiking Neural Networks Using Lessons from Deep\nLearning.” Proc. IEEE 111 (9): 1016–54. https://doi.org/10.1109/jproc.2023.3308088.\n\n\nEsteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M.\nSwetter, Helen M. Blau, and Sebastian Thrun. 2017.\n“Dermatologist-Level Classification of Skin Cancer with Deep\nNeural Networks.” Nature 542 (7639): 115–18. https://doi.org/10.1038/nature21056.\n\n\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,\nChaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017.\n“Robust Physical-World Attacks on Deep Learning Models.”\nArXiv Preprint abs/1707.08945. https://arxiv.org/abs/1707.08945.\n\n\nFahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo\nJindariani, Nhan Tran, Luca P. Carloni, et al. 2021. “Hls4ml:\nAn Open-Source Codesign Workflow to Empower Scientific\nLow-Power Machine Learning Devices.” https://arxiv.org/abs/2103.05579.\n\n\nFarah, Martha J. 2005. “Neuroethics: The Practical\nand the Philosophical.” Trends Cogn. Sci. 9 (1): 34–40.\nhttps://doi.org/10.1016/j.tics.2004.12.001.\n\n\nFarwell, James P., and Rafal Rohozinski. 2011. “Stuxnet and the\nFuture of Cyber War.” Survival 53 (1): 23–40. https://doi.org/10.1080/00396338.2011.555586.\n\n\nFowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill,\nMing Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. “A Configurable\nCloud-Scale DNN Processor for Real-Time\nAI.” In 2018 ACM/IEEE 45th Annual International\nSymposium on Computer Architecture (ISCA), 1–14. IEEE; IEEE. https://doi.org/10.1109/isca.2018.00012.\n\n\nFrancalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard,\nIan Cassar, Dario Della Monica, and Anna Ingólfsdóttir. 2017. “A\nFoundation for Runtime Monitoring.” In International\nConference on Runtime Verification, 8–29. Springer.\n\n\nFrankle, Jonathan, and Michael Carbin. 2019. “The Lottery Ticket\nHypothesis: Finding Sparse, Trainable Neural\nNetworks.” In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net. https://openreview.net/forum?id=rJl-b3RcF7.\n\n\nFriedman, Batya. 1996. “Value-Sensitive Design.”\nInteractions 3 (6): 16–23. https://doi.org/10.1145/242485.242493.\n\n\nFurber, Steve. 2016. “Large-Scale Neuromorphic Computing\nSystems.” J. Neural Eng. 13 (5): 051001. https://doi.org/10.1088/1741-2560/13/5/051001.\n\n\nFursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun,\nRodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey\nZaytsev, and Evgeny Burnaev. 2021. “Adversarial Attacks on Deep\nModels for Financial Transaction Records.” In Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery &Amp; Data\nMining, 2868–78. ACM. https://doi.org/10.1145/3447548.3467145.\n\n\nGale, Trevor, Erich Elsen, and Sara Hooker. 2019. “The State of\nSparsity in Deep Neural Networks.” ArXiv Preprint\nabs/1902.09574. https://arxiv.org/abs/1902.09574.\n\n\nGandolfi, Karine, Christophe Mourtel, and Francis Olivier. 2001.\n“Electromagnetic Analysis: Concrete Results.”\nIn Cryptographic Hardware and Embedded SystemsCHES\n2001: Third International Workshop Paris, France, May 1416,\n2001 Proceedings 3, 251–61. Springer.\n\n\nGannot, G., and M. Ligthart. 1994. “Verilog HDL Based\nFPGA Design.” In International Verilog HDL\nConference, 86–92. IEEE. https://doi.org/10.1109/ivc.1994.323743.\n\n\nGao, Yansong, Said F. Al-Sarawi, and Derek Abbott. 2020. “Physical\nUnclonable Functions.” Nature Electronics 3 (2): 81–91.\nhttps://doi.org/10.1038/s41928-020-0372-5.\n\n\nGates, Byron D. 2009. “Flexible Electronics.”\nScience 323 (5921): 1566–67. https://doi.org/10.1126/science.1171230.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Commun. ACM 64 (12):\n86–92. https://doi.org/10.1145/3458723.\n\n\nGeiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts. 2021.\n“Causal Abstractions of Neural Networks.” In Advances\nin Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 9574–86. https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.\n\n\nGholami, Dong Kim, Mahoney Yao, and Keutzer. 2021. “A Survey of\nQuantization Methods for Efficient Neural Network Inference).”\nArXiv Preprint. https://arxiv.org/abs/2103.13630.\n\n\nGlorot, Xavier, and Yoshua Bengio. 2010. “Understanding the\nDifficulty of Training Deep Feedforward Neural Networks.” In\nProceedings of the Thirteenth International Conference on Artificial\nIntelligence and Statistics, 249–56. http://proceedings.mlr.press/v9/glorot10a.html.\n\n\nGnad, Dennis R. E., Fabian Oboril, and Mehdi B. Tahoori. 2017.\n“Voltage Drop-Based Fault Attacks on FPGAs Using\nValid Bitstreams.” In 2017 27th International Conference on\nField Programmable Logic and Applications (FPL), 1–7. IEEE; IEEE.\nhttps://doi.org/10.23919/fpl.2017.8056840.\n\n\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.\n“Generative Adversarial Networks.” Commun. ACM 63\n(11): 139–44. https://doi.org/10.1145/3422622.\n\n\nGoodyear, Victoria A. 2017. “Social Media, Apps and Wearable\nTechnologies: Navigating Ethical Dilemmas and\nProcedures.” Qualitative Research in Sport, Exercise and\nHealth 9 (3): 285–302. https://doi.org/10.1080/2159676x.2017.1303790.\n\n\nGoogle. n.d. “Information Quality Content Moderation.” https://blog.google/documents/83/.\n\n\nGordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang,\nand Edward Choi. 2018. “MorphNet: Fast\n&Amp; Simple Resource-Constrained Structure Learning of Deep\nNetworks.” In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 1586–95. IEEE. https://doi.org/10.1109/cvpr.2018.00171.\n\n\nGräfe, Ralf, Qutub Syed Sha, Florian Geissler, and Michael Paulitsch.\n2023. “Large-Scale Application of Fault Injection into\nPyTorch Models -an Extension to PyTorchFI for\nValidation Efficiency.” In 2023 53rd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks -\nSupplemental Volume (DSN-s), 56–62. IEEE; IEEE. https://doi.org/10.1109/dsn-s58398.2023.00025.\n\n\nGreengard, Samuel. 2015. The Internet of Things. The MIT Press.\nhttps://doi.org/10.7551/mitpress/10277.001.0001.\n\n\nGrossman, Elizabeth. 2007. High Tech Trash: Digital\nDevices, Hidden Toxics, and Human Health. Island press.\n\n\nGruslys, Audrunas, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex\nGraves. 2016. “Memory-Efficient Backpropagation Through\nTime.” In Advances in Neural Information Processing Systems\n29: Annual Conference on Neural Information Processing Systems 2016,\nDecember 5-10, 2016, Barcelona, Spain, edited by Daniel D. Lee,\nMasashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett,\n4125–33. https://proceedings.neurips.cc/paper/2016/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html.\n\n\nGu, Ivy. 2023. “Deep Learning Model Compression (Ii) by Ivy Gu\nMedium.” https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453.\n\n\nGuo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian\nWeinberger. 2019. “Simple Black-Box Adversarial Attacks.”\nIn International Conference on Machine Learning, 2484–93. PMLR.\n\n\nGuo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia,\nLi Yan, et al. 2019. “Mobile Photoplethysmographic Technology to\nDetect Atrial Fibrillation.” J. Am. Coll. Cardiol. 74\n(19): 2365–75. https://doi.org/10.1016/j.jacc.2019.08.019.\n\n\nGupta, Maanak, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and\nLopamudra Praharaj. 2023. “From ChatGPT to\nThreatGPT: Impact of Generative\nAI in Cybersecurity and Privacy.”\n#IEEE_O_ACC# 11: 80218–45. https://doi.org/10.1109/access.2023.3300381.\n\n\nGupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin\nCanini, Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van\nEsbroeck. 2016. “Monotonic Calibrated Interpolated Look-up\nTables.” The Journal of Machine Learning Research 17\n(1): 3790–3836.\n\n\nGupta, Udit, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S. Lee,\nDavid Brooks, and Carole-Jean Wu. 2022. “Act: Designing\nSustainable Computer Systems with an Architectural Carbon Modeling\nTool.” In Proceedings of the 49th Annual International\nSymposium on Computer Architecture, 784–99. ACM. https://doi.org/10.1145/3470496.3527408.\n\n\nGwennap, Linley. n.d. “Certus-NX Innovates\nGeneral-Purpose FPGAs.”\n\n\nHaensch, Wilfried, Tayfun Gokmen, and Ruchir Puri. 2019. “The Next\nGeneration of Deep Learning Hardware: Analog\nComputing.” Proc. IEEE 107 (1): 108–22. https://doi.org/10.1109/jproc.2018.2871057.\n\n\nHamming, R. W. 1950. “Error Detecting and Error Correcting\nCodes.” Bell Syst. Tech. J. 29 (2): 147–60. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x.\n\n\nHan, Song, Huizi Mao, and William J Dally. 2015. “Deep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.” arXiv Preprint\narXiv:1510.00149.\n\n\nHan, Song, Huizi Mao, and William J. Dally. 2016. “Deep\nCompression: Compressing Deep Neural Networks with Pruning,\nTrained Quantization and Huffman Coding.” https://arxiv.org/abs/1510.00149.\n\n\nHandlin, Oscar. 1965. “Science and Technology in Popular\nCulture.” Daedalus-Us., 156–70.\n\n\nHardt, Moritz, Eric Price, and Nati Srebro. 2016. “Equality of\nOpportunity in Supervised Learning.” In Advances in Neural\nInformation Processing Systems 29: Annual Conference on Neural\nInformation Processing Systems 2016, December 5-10, 2016, Barcelona,\nSpain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett, 3315–23. https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html.\n\n\nHawks, Benjamin, Javier Duarte, Nicholas J. Fraser, Alessandro\nPappalardo, Nhan Tran, and Yaman Umuroglu. 2021. “Ps and Qs: Quantization-aware Pruning for Efficient Low\nLatency Neural Network Inference.” Frontiers in Artificial\nIntelligence 4 (July). https://doi.org/10.3389/frai.2021.676564.\n\n\nHazan, Avi, and Elishai Ezra Tsur. 2021. “Neuromorphic Analog\nImplementation of Neural Engineering Framework-Inspired Spiking Neuron\nfor High-Dimensional Representation.” Front. Neurosci.\n15 (February): 627221. https://doi.org/10.3389/fnins.2021.627221.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.\n“Delving Deep into Rectifiers: Surpassing Human-Level Performance\non ImageNet Classification.” In 2015 IEEE International\nConference on Computer Vision (ICCV), 1026–34. IEEE. https://doi.org/10.1109/iccv.2015.123.\n\n\n———. 2016. “Deep Residual Learning for Image Recognition.”\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 770–78. IEEE. https://doi.org/10.1109/cvpr.2016.90.\n\n\nHe, Yi, Prasanna Balaprakash, and Yanjing Li. 2020.\n“FIdelity: Efficient Resilience Analysis\nFramework for Deep Learning Accelerators.” In 2020 53rd\nAnnual IEEE/ACM International Symposium on Microarchitecture\n(MICRO), 270–81. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00033.\n\n\nHe, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju,\nNishant Patil, and Yanjing Li. 2023. “Understanding and Mitigating\nHardware Failures in Deep Learning Training Systems.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1–16. IEEE; ACM. https://doi.org/10.1145/3579371.3589105.\n\n\nHébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, and Guy N.\nRothblum. 2018. “Multicalibration: Calibration for\nthe (Computationally-Identifiable) Masses.” In\nProceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,\n2018, edited by Jennifer G. Dy and Andreas Krause, 80:1944–53.\nProceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v80/hebert-johnson18a.html.\n\n\nHegde, Sumant. 2023. “An Introduction to Separable Convolutions -\nAnalytics Vidhya.” https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/.\n\n\nHenderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky,\nand Joelle Pineau. 2020. “Towards the Systematic Reporting of the\nEnergy and Carbon Footprints of Machine Learning.” The\nJournal of Machine Learning Research 21 (1): 10039–81.\n\n\nHendrycks, Dan, and Thomas Dietterich. 2019. “Benchmarking Neural\nNetwork Robustness to Common Corruptions and Perturbations.”\narXiv Preprint arXiv:1903.12261.\n\n\nHendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn\nSong. 2021. “Natural Adversarial Examples.” In 2021\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 15262–71. IEEE. https://doi.org/10.1109/cvpr46437.2021.01501.\n\n\nHennessy, John L., and David A. Patterson. 2019. “A New Golden Age\nfor Computer Architecture.” Commun. ACM 62 (2): 48–60.\nhttps://doi.org/10.1145/3282307.\n\n\nHimmelstein, Gracie, David Bates, and Li Zhou. 2022. “Examination\nof Stigmatizing Language in the Electronic Health Record.”\nJAMA Network Open 5 (1): e2144967. https://doi.org/10.1001/jamanetworkopen.2021.44967.\n\n\nHinton, Geoffrey. 2005. “Van Nostrand’s Scientific Encyclopedia.” Wiley.\nhttps://doi.org/10.1002/0471743984.vse0673.\n\n\n———. 2017. “Overview of Minibatch Gradient Descent.”\nUniversity of Toronto; University Lecture.\n\n\nHo Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song,\nJun Yeong Seok, Kyung Jean Yoon, et al. 2012. “Frontiers in\nElectronic Materials.” Wiley. https://doi.org/10.1002/9783527667703.ch67.\n\n\nHoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and\nAlexandra Peste. 2021. “Sparsity in Deep Learning: Pruning and\nGrowth for Efficient Inference and Training in Neural Networks,”\nJanuary. http://arxiv.org/abs/2102.00554v1.\n\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia\nChmielinski. 2020. “The Dataset Nutrition Label: A Framework to\nDrive Higher Data Quality Standards.” In Data Protection and\nPrivacy. Hart Publishing. https://doi.org/10.5040/9781509932771.ch-001.\n\n\nHong, Sanghyun, Nicholas Carlini, and Alexey Kurakin. 2023.\n“Publishing Efficient on-Device Models Increases Adversarial\nVulnerability.” In 2023 IEEE Conference on Secure and\nTrustworthy Machine Learning (SaTML), 271–90. IEEE; IEEE. https://doi.org/10.1109/satml54575.2023.00026.\n\n\nHosseini, Hossein, Sreeram Kannan, Baosen Zhang, and Radha Poovendran.\n2017. “Deceiving Google’s Perspective Api Built for Detecting\nToxic Comments.” ArXiv Preprint abs/1702.08138. https://arxiv.org/abs/1702.08138.\n\n\nHoward, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.\n“MobileNets: Efficient Convolutional\nNeural Networks for Mobile Vision Applications.” ArXiv\nPreprint. https://arxiv.org/abs/1704.04861.\n\n\nHsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman\nMahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay\nJanapa Reddi. 2023. “MAVFI: An\nEnd-to-End Fault Analysis Framework with Anomaly Detection and Recovery\nfor Micro Aerial Vehicles.” In 2023 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1–6. IEEE; IEEE. https://doi.org/10.23919/date56975.2023.10137246.\n\n\nHsu, Liang-Ching, Ching-Yi Huang, Yen-Hsun Chuang, Ho-Wen Chen, Ya-Ting\nChan, Heng Yi Teah, Tsan-Yao Chen, Chiung-Fen Chang, Yu-Ting Liu, and\nYu-Min Tzou. 2016. “Accumulation of Heavy Metals and Trace\nElements in Fluvial Sediments Received Effluents from Traditional and\nSemiconductor Industries.” Scientific Reports 6 (1):\n34250. https://doi.org/10.1038/srep34250.\n\n\nHu, Jie, Li Shen, and Gang Sun. 2018. “Squeeze-and-Excitation\nNetworks.” In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 7132–41. IEEE. https://doi.org/10.1109/cvpr.2018.00745.\n\n\nHu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, and Jian Shi. 2023.\n“Halide Perovskite Semiconductors.” Wiley. https://doi.org/10.1002/9783527829026.ch13.\n\n\nHuang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi\nSekitani, Takao Someya, and Kwang-Ting Cheng. 2011.\n“Pseudo-CMOS: A Design Style for\nLow-Cost and Robust Flexible Electronics.” IEEE Trans.\nElectron Devices 58 (1): 141–50. https://doi.org/10.1109/ted.2010.2088127.\n\n\nHutter, Michael, Jorn-Marc Schmidt, and Thomas Plos. 2009.\n“Contact-Based Fault Injections and Power Analysis on\nRFID Tags.” In 2009 European Conference on\nCircuit Theory and Design, 409–12. IEEE; IEEE. https://doi.org/10.1109/ecctd.2009.5275012.\n\n\nIandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf,\nWilliam J Dally, and Kurt Keutzer. 2016. “SqueezeNet:\nAlexnet-level Accuracy with 50x Fewer\nParameters and 0.5 MB Model Size.” ArXiv\nPreprint abs/1602.07360. https://arxiv.org/abs/1602.07360.\n\n\nIgnatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim\nHartley, and Luc Van Gool. 2018. “AI Benchmark:\nRunning Deep Neural Networks on Android\nSmartphones,” 0–0.\n\n\nImani, Mohsen, Abbas Rahimi, and Tajana S. Rosing. 2016.\n“Resistive Configurable Associative Memory for Approximate\nComputing.” In Proceedings of the 2016 Design, Automation\n&Amp; Test in Europe Conference &Amp; Exhibition (DATE),\n1327–32. IEEE; Research Publishing Services. https://doi.org/10.3850/9783981537079_0454.\n\n\nIntelLabs. 2023. “Knowledge Distillation - Neural Network\nDistiller.” https://intellabs.github.io/distiller/knowledge_distillation.html.\n\n\nIppolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew\nJagielski, Katherine Lee, Christopher Choquette Choo, and Nicholas\nCarlini. 2023. “Preventing Generation of Verbatim Memorization in\nLanguage Models Gives a False Sense of Privacy.” In\nProceedings of the 16th International Natural Language Generation\nConference, 5253–70. Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.inlg-main.3.\n\n\nIrimia-Vladu, Mihai. 2014.\n““Green” Electronics:\nBiodegradable and Biocompatible Materials and Devices for\nSustainable Future.” Chem. Soc. Rev. 43 (2): 588–610. https://doi.org/10.1039/c3cs60235d.\n\n\nIsscc. 2014. “Computing’s Energy Problem (and What We Can Do about\nIt).” https://ieeexplore.ieee.org/document/6757323.\n\n\nJacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,\nAndrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018.\n“Quantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference.” In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\n2704–13.\n\n\nJaderberg, Max, Valentin Dalibard, Simon Osindero, Wojciech M.\nCzarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, et al. 2017.\n“Population Based Training of Neural Networks.” arXiv\nPreprint arXiv:1711.09846, November. http://arxiv.org/abs/1711.09846v2.\n\n\nJanapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler,\nDaniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. “Edge\nImpulse: An MLOps Platform for Tiny Machine\nLearning.” Proceedings of Machine Learning and Systems\n5.\n\n\nJha, A. R. 2014. Rare Earth Materials: Properties and\nApplications. CRC Press. https://doi.org/10.1201/b17045.\n\n\nJha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B.\nSullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, and Ravishankar K.\nIyer. 2019. “ML-Based Fault Injection for Autonomous\nVehicles: A Case for Bayesian Fault\nInjection.” In 2019 49th Annual IEEE/IFIP International\nConference on Dependable Systems and Networks (DSN), 112–24. IEEE;\nIEEE. https://doi.org/10.1109/dsn.2019.00025.\n\n\nJia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\nLong, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014.\n“Caffe: Convolutional Architecture for Fast Feature\nEmbedding.” In Proceedings of the 22nd ACM International\nConference on Multimedia, 675–78. ACM. https://doi.org/10.1145/2647868.2654889.\n\n\nJia, Zhe, Marco Maggioni, Benjamin Staiger, and Daniele P. Scarpazza.\n2018. “Dissecting the NVIDIA Volta\nGPU Architecture via Microbenchmarking.” ArXiv\nPreprint. https://arxiv.org/abs/1804.06826.\n\n\nJia, Zhenge, Dawei Li, Xiaowei Xu, Na Li, Feng Hong, Lichuan Ping, and\nYiyu Shi. 2023. “Life-Threatening Ventricular Arrhythmia Detection\nChallenge in Implantable\nCardioverterdefibrillators.” Nature Machine\nIntelligence 5 (5): 554–55. https://doi.org/10.1038/s42256-023-00659-9.\n\n\nJia, Zhihao, Matei Zaharia, and Alex Aiken. 2019. “Beyond Data and\nModel Parallelism for Deep Neural Networks.” In Proceedings\nof Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA,\nMarch 31 - April 2, 2019, edited by Ameet Talwalkar, Virginia\nSmith, and Matei Zaharia. mlsys.org. https://proceedings.mlsys.org/book/265.pdf.\n\n\nJin, Yilun, Xiguang Wei, Yang Liu, and Qiang Yang. 2020. “Towards\nUtilizing Unlabeled Data in Federated Learning: A Survey\nand Prospective.” arXiv Preprint arXiv:2002.11545.\n\n\nJohnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur\nSridhar, Karl Rosaen, and Ram Vasudevan. 2017. “Driving in the\nMatrix: Can Virtual Worlds Replace Human-Generated\nAnnotations for Real World Tasks?” In 2017 IEEE International\nConference on Robotics and Automation (ICRA), 746–53. Singapore,\nSingapore: IEEE. https://doi.org/10.1109/icra.2017.7989092.\n\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. “In-Datacenter\nPerformance Analysis of a Tensor Processing Unit.” In\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture, 1–12. ISCA ’17. New York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\n———, et al. 2017b. “In-Datacenter Performance Analysis of a Tensor\nProcessing Unit.” In Proceedings of the 44th Annual\nInternational Symposium on Computer Architecture, 1–12. ISCA ’17.\nNew York, NY, USA: ACM. https://doi.org/10.1145/3079856.3080246.\n\n\nJouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng\nNai, Nishant Patil, et al. 2023. “TPU V4:\nAn Optically Reconfigurable Supercomputer for Machine\nLearning with Hardware Support for Embeddings.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture. ISCA ’23. New York, NY, USA: ACM. https://doi.org/10.1145/3579371.3589350.\n\n\nJoye, Marc, and Michael Tunstall. 2012. Fault Analysis in\nCryptography. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-29656-7.\n\n\nKairouz, Peter, Sewoong Oh, and Pramod Viswanath. 2015. “Secure\nMulti-Party Differential Privacy.” In Advances in Neural\nInformation Processing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, edited by Corinna Cortes, Neil D. Lawrence, Daniel\nD. Lee, Masashi Sugiyama, and Roman Garnett, 2008–16. https://proceedings.neurips.cc/paper/2015/hash/a01610228fe998f515a72dd730294d87-Abstract.html.\n\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das,\nKunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019.\n“A Study of BFLOAT16 for Deep Learning\nTraining.” https://arxiv.org/abs/1905.12322.\n\n\nKao, Sheng-Chun, Geonhwa Jeong, and Tushar Krishna. 2020.\n“ConfuciuX: Autonomous Hardware Resource\nAssignment for DNN Accelerators Using Reinforcement\nLearning.” In 2020 53rd Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO), 622–36. IEEE; IEEE. https://doi.org/10.1109/micro50266.2020.00058.\n\n\nKao, Sheng-Chun, and Tushar Krishna. 2020. “Gamma: Automating the\nHW Mapping of DNN Models on Accelerators via Genetic Algorithm.”\nIn Proceedings of the 39th International Conference on\nComputer-Aided Design, 1–9. ACM. https://doi.org/10.1145/3400302.3415639.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. “Scaling Laws for Neural Language Models.”\nArXiv Preprint abs/2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nKarargyris, Alexandros, Renato Umeton, Micah J Sheller, Alejandro\nAristizabal, Johnu George, Anna Wuest, Sarthak Pati, et al. 2023.\n“Federated Benchmarking of Medical Artificial Intelligence with\nMedPerf.” Nature Machine Intelligence 5\n(7): 799–810. https://doi.org/10.1038/s42256-023-00652-2.\n\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna\nWallach, and Jennifer Wortman Vaughan. 2020. “Interpreting\nInterpretability: Understanding Data Scientists’ Use of\nInterpretability Tools for Machine Learning.” In Proceedings\nof the 2020 CHI Conference on Human Factors in Computing Systems,\nedited by Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh\nAndres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14.\nACM. https://doi.org/10.1145/3313831.3376219.\n\n\nKawazoe Aguilera, Marcos, Wei Chen, and Sam Toueg. 1997.\n“Heartbeat: A Timeout-Free Failure Detector for\nQuiescent Reliable Communication.” In Distributed Algorithms:\n11th International Workshop, WDAG’97 Saarbrücken, Germany, September\n2426, 1997 Proceedings 11, 126–40. Springer.\n\n\nKhan, Mohammad Emtiyaz, and Siddharth Swaroop. 2021.\n“Knowledge-Adaptation Priors.” In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nVirtual, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, 19757–70. https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html.\n\n\nKiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\nZhengxuan Wu, Bertie Vidgen, et al. 2021. “Dynabench:\nRethinking Benchmarking in NLP.” In\nProceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language\nTechnologies, 4110–24. Online: Association for Computational\nLinguistics. https://doi.org/10.18653/v1/2021.naacl-main.324.\n\n\nKim, Jungrae, Michael Sullivan, and Mattan Erez. 2015. “Bamboo\nECC: Strong, Safe, and Flexible Codes for\nReliable Computer Memory.” In 2015 IEEE 21st International\nSymposium on High Performance Computer Architecture (HPCA), 101–12.\nIEEE; IEEE. https://doi.org/10.1109/hpca.2015.7056025.\n\n\nKim, Sunju, Chungsik Yoon, Seunghon Ham, Jihoon Park, Ohun Kwon, Donguk\nPark, Sangjun Choi, Seungwon Kim, Kwonchul Ha, and Won Kim. 2018.\n“Chemical Use in the Semiconductor Manufacturing Industry.”\nInt. J. Occup. Env. Heal. 24 (3-4): 109–18. https://doi.org/10.1080/10773525.2018.1519957.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A Method for\nStochastic Optimization.” Edited by Yoshua Bengio and Yann LeCun,\nDecember. http://arxiv.org/abs/1412.6980v9.\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\nGuillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017.\n“Overcoming Catastrophic Forgetting in Neural Networks.”\nProc. Natl. Acad. Sci. 114 (13): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n\n\nKo, Yohan. 2021. “Characterizing System-Level Masking Effects\nAgainst Soft Errors.” Electronics 10 (18): 2286. https://doi.org/10.3390/electronics10182286.\n\n\nKocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss,\nWerner Haas, Mike Hamburg, et al. 2019a. “Spectre Attacks:\nExploiting Speculative Execution.” In 2019 IEEE\nSymposium on Security and Privacy (SP). IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\n———, et al. 2019b. “Spectre Attacks: Exploiting\nSpeculative Execution.” In 2019 IEEE Symposium on Security\nand Privacy (SP). IEEE. https://doi.org/10.1109/sp.2019.00002.\n\n\nKocher, Paul, Joshua Jaffe, and Benjamin Jun. 1999. “Differential\nPower Analysis.” In Advances in\nCryptologyCRYPTO’99: 19th Annual International Cryptology\nConference Santa Barbara, California, USA, August 1519,\n1999 Proceedings 19, 388–97. Springer.\n\n\nKocher, Paul, Joshua Jaffe, Benjamin Jun, and Pankaj Rohatgi. 2011.\n“Introduction to Differential Power Analysis.” Journal\nof Cryptographic Engineering 1 (1): 5–27. https://doi.org/10.1007/s13389-011-0006-y.\n\n\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma\nPierson, Been Kim, and Percy Liang. 2020. “Concept Bottleneck\nModels.” In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\n119:5338–48. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v119/koh20a.html.\n\n\nKoh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin\nZhang, Akshay Balsubramani, Weihua Hu, et al. 2021.\n“WILDS: A Benchmark of in-the-Wild\nDistribution Shifts.” In Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, edited by Marina Meila and Tong Zhang,\n139:5637–64. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/koh21a.html.\n\n\nKoren, Yehuda, Robert Bell, and Chris Volinsky. 2009. “Matrix\nFactorization Techniques for Recommender Systems.”\nComputer 42 (8): 30–37. https://doi.org/10.1109/mc.2009.263.\n\n\nKrishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh\nDwivedi, André van Schaik, Mahesh Mehendale, and Chetan Singh Thakur.\n2023. “RAMAN: A Re-Configurable and\nSparse TinyML Accelerator for Inference on Edge.” https://arxiv.org/abs/2306.06493.\n\n\nKrishnamoorthi. 2018. “Quantizing Deep Convolutional Networks for\nEfficient Inference: A Whitepaper.” ArXiv\nPreprint. https://arxiv.org/abs/1806.08342.\n\n\nKrishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022.\n“Self-Supervised Learning in Medicine and Healthcare.”\nNat. Biomed. Eng. 6 (12): 1346–52. https://doi.org/10.1038/s41551-022-00914-1.\n\n\nKrishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang,\nIzzeddin Gur, Vijay Janapa Reddi, and Aleksandra Faust. 2022.\n“Multi-Agent Reinforcement Learning for Microprocessor Design\nSpace Exploration.” https://arxiv.org/abs/2211.16385.\n\n\nKrishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour,\nIkechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023.\n“ArchGym: An Open-Source Gymnasium for\nMachine Learning Assisted Architecture Design.” In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture, 1–16. ACM. https://doi.org/10.1145/3579371.3589049.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012.\n“ImageNet Classification with Deep Convolutional\nNeural Networks.” In Advances in Neural Information\nProcessing Systems 25: 26th Annual Conference on Neural Information\nProcessing Systems 2012. Proceedings of a Meeting Held December 3-6,\n2012, Lake Tahoe, Nevada, United States, edited by Peter L.\nBartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou,\nand Kilian Q. Weinberger, 1106–14. https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\n\n———. 2017. “ImageNet Classification with Deep\nConvolutional Neural Networks.” Edited by F. Pereira, C. J.\nBurges, L. Bottou, and K. Q. Weinberger. Commun. ACM 60 (6):\n84–90. https://doi.org/10.1145/3065386.\n\n\nKung, Hsiang Tsung, and Charles E Leiserson. 1979. “Systolic\nArrays (for VLSI).” In Sparse Matrix Proceedings\n1978, 1:256–82. Society for industrial; applied mathematics\nPhiladelphia, PA, USA.\n\n\nKurth, Thorsten, Shashank Subramanian, Peter Harrington, Jaideep Pathak,\nMorteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Anima\nAnandkumar. 2023. “FourCastNet:\nAccelerating Global High-Resolution Weather Forecasting\nUsing Adaptive Fourier Neural Operators.” In\nProceedings of the Platform for Advanced Scientific Computing\nConference, 1–11. ACM. https://doi.org/10.1145/3592979.3593412.\n\n\nKuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,\nand Tijmen Blankevoort. 2022. “FP8 Quantization:\nThe Power of the Exponent.” https://arxiv.org/abs/2208.09225.\n\n\nKuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\nKrasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. “The Open\nImages Dataset V4: Unified Image Classification, Object\nDetection, and Visual Relationship Detection at Scale.”\nInternational Journal of Computer Vision 128 (7): 1956–81.\n\n\nKwon, Jisu, and Daejin Park. 2021. “Hardware/Software\nCo-Design for TinyML Voice-Recognition Application on\nResource Frugal Edge Devices.” Applied Sciences 11 (22):\n11073. https://doi.org/10.3390/app112211073.\n\n\nKwon, Sun Hwa, and Lin Dong. 2022. “Flexible Sensors and Machine\nLearning for Heart Monitoring.” Nano Energy 102\n(November): 107632. https://doi.org/10.1016/j.nanoen.2022.107632.\n\n\nKwon, Young D, Rui Li, Stylianos I Venieris, Jagmohan Chauhan, Nicholas\nD Lane, and Cecilia Mascolo. 2023. “TinyTrain:\nDeep Neural Network Training at the Extreme Edge.”\nArXiv Preprint abs/2307.09988. https://arxiv.org/abs/2307.09988.\n\n\nLai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018a. “Cmsis-Nn:\nEfficient Neural Network Kernels for Arm Cortex-m\nCpus.” ArXiv Preprint abs/1801.06601. https://arxiv.org/abs/1801.06601.\n\n\n———. 2018b. “CMSIS-NN:\nEfficient Neural Network Kernels for Arm Cortex-m\nCPUs.” https://arxiv.org/abs/1801.06601.\n\n\nLakkaraju, Himabindu, and Osbert Bastani. 2020.\n“”How Do i Fool You?”:\nManipulating User Trust via Misleading Black Box Explanations.”\nIn Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety, 79–85. ACM. https://doi.org/10.1145/3375627.3375833.\n\n\nLam, Remi, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger,\nMeire Fortunato, Ferran Alet, Suman Ravuri, et al. 2023. “Learning\nSkillful Medium-Range Global Weather Forecasting.”\nScience 382 (6677): 1416–21. https://doi.org/10.1126/science.adi2336.\n\n\nLannelongue, Loı̈c, Jason Grealey, and Michael Inouye. 2021. “Green\nAlgorithms: Quantifying the Carbon Footprint of\nComputation.” Adv. Sci. 8 (12): 2100707. https://doi.org/10.1002/advs.202100707.\n\n\nLeCun, Yann, John Denker, and Sara Solla. 1989. “Optimal Brain\nDamage.” Adv Neural Inf Process Syst 2.\n\n\nLee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang,\nand Seongik Cho. 2022. “Design of Radiation-Tolerant High-Speed\nSignal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear\nExplosion.” Electronics 11 (18): 2970. https://doi.org/10.3390/electronics11182970.\n\n\nLeRoy Poff, N, MM Brinson, and JW Day. 2002. “Aquatic Ecosystems\n& Global Climate Change.” Pew Center on Global Climate\nChange.\n\n\nLi, En, Liekang Zeng, Zhi Zhou, and Xu Chen. 2020. “Edge\nAI: On-demand Accelerating Deep\nNeural Network Inference via Edge Computing.” IEEE Trans.\nWireless Commun. 19 (1): 447–57. https://doi.org/10.1109/twc.2019.2946140.\n\n\nLi, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai,\nKarthik Pattabiraman, Joel Emer, and Stephen W. Keckler. 2017.\n“Understanding Error Propagation in Deep Learning Neural Network\n(DNN) Accelerators and Applications.” In\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, 1–12. ACM. https://doi.org/10.1145/3126908.3126964.\n\n\nLi, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, and\nZedong Nie. 2021. “Non-Invasive Monitoring of Three Glucose Ranges\nBased on ECG by Using\nDBSCAN-CNN.” #IEEE_J_BHI# 25\n(9): 3340–50. https://doi.org/10.1109/jbhi.2021.3072628.\n\n\nLi, Mu, David G. Andersen, Alexander J. Smola, and Kai Yu. 2014.\n“Communication Efficient Distributed Machine Learning with the\nParameter Server.” In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Montreal, Quebec,\nCanada, edited by Zoubin Ghahramani, Max Welling, Corinna Cortes,\nNeil D. Lawrence, and Kilian Q. Weinberger, 19–27. https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html.\n\n\nLi, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu,\nand Bingsheng He. 2023. “A Survey on Federated Learning Systems:\nVision, Hype and Reality for Data Privacy and\nProtection.” IEEE Trans. Knowl. Data Eng. 35 (4):\n3347–66. https://doi.org/10.1109/tkde.2021.3124599.\n\n\nLi, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.\n“Federated Learning: Challenges, Methods, and Future\nDirections.” IEEE Signal Process Mag. 37 (3): 50–60. https://doi.org/10.1109/msp.2020.2975749.\n\n\nLi, Xiang, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016.\n“LightRNN: Memory and\nComputation-Efficient Recurrent Neural Networks.” In Advances\nin Neural Information Processing Systems 29: Annual Conference on Neural\nInformation Processing Systems 2016, December 5-10, 2016, Barcelona,\nSpain, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett, 4385–93. https://proceedings.neurips.cc/paper/2016/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html.\n\n\nLi, Yuhang, Xin Dong, and Wei Wang. 2020. “Additive Powers-of-Two\nQuantization: An Efficient Non-Uniform Discretization for\nNeural Networks.” In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net. https://openreview.net/forum?id=BkgXT24tDS.\n\n\nLi, Zhizhong, and Derek Hoiem. 2018. “Learning Without\nForgetting.” IEEE Trans. Pattern Anal. Mach. Intell. 40\n(12): 2935–47. https://doi.org/10.1109/tpami.2017.2773081.\n\n\nLin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han.\n2020. “MCUNet: Tiny Deep Learning on\nIoT Devices.” In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nVirtual, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song\nHan. 2022. “On-Device Training Under 256kb Memory.”\nAdv. Neur. In. 35: 22941–54.\n\n\nLin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, and Song Han. 2023.\n“Tiny Machine Learning: Progress and Futures Feature.”\nIEEE Circuits Syst. Mag. 23 (3): 8–34. https://doi.org/10.1109/mcas.2023.3302182.\n\n\nLin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona,\nDeva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.\n“Microsoft Coco: Common Objects in Context.”\nIn Computer VisionECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part v 13,\n740–55. Springer.\n\n\nLindgren, Simon. 2023. Handbook of Critical Studies of Artificial\nIntelligence. Edward Elgar Publishing.\n\n\nLindholm, Andreas, Dave Zachariah, Petre Stoica, and Thomas B. Schon.\n2019. “Data Consistency Approach to Model Validation.”\n#IEEE_O_ACC# 7: 59788–96. https://doi.org/10.1109/access.2019.2915109.\n\n\nLindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008.\n“NVIDIA Tesla: A Unified Graphics and\nComputing Architecture.” IEEE Micro 28 (2): 39–55. https://doi.org/10.1109/mm.2008.31.\n\n\nLin, Tang Tang, Dang Yang, and Han Gan. 2023. “AWQ:\nActivation-aware Weight Quantization for\nLLM Compression and Acceleration.” ArXiv\nPreprint. https://arxiv.org/abs/2306.00978.\n\n\nLiu, Yanan, Xiaoxia Wei, Jinyu Xiao, Zhijie Liu, Yang Xu, and Yun Tian.\n2020. “Energy Consumption and Emission Mitigation Prediction Based\non Data Center Traffic and PUE for Global Data\nCenters.” Global Energy Interconnection 3 (3): 272–82.\nhttps://doi.org/10.1016/j.gloei.2020.07.008.\n\n\nLiu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella\nJensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022.\n“Monitoring Gait at Home with Radio Waves in\nParkinson’s Disease: A Marker of Severity,\nProgression, and Medication Response.” Sci. Transl. Med.\n14 (663): eadc9669. https://doi.org/10.1126/scitranslmed.adc9669.\n\n\nLoh, Gabriel H. 2008. “3D-Stacked Memory\nArchitectures for Multi-Core Processors.” ACM SIGARCH\nComputer Architecture News 36 (3): 453–64. https://doi.org/10.1145/1394608.1382159.\n\n\nLopez-Paz, David, and Marc’Aurelio Ranzato. 2017. “Gradient\nEpisodic Memory for Continual Learning.” Adv Neural Inf\nProcess Syst 30.\n\n\nLou, Yin, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013.\n“Accurate Intelligible Models with Pairwise Interactions.”\nIn Proceedings of the 19th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, edited by Inderjit S. Dhillon,\nYehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh,\nJingrui He, Robert L. Grossman, and Ramasamy Uthurusamy, 623–31. ACM. https://doi.org/10.1145/2487575.2487579.\n\n\nLowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, and\nAhmad Beirami. 2021. “Fermi: Fair Empirical Risk\nMinimization via Exponential Rényi Mutual Information.”\n\n\nLubana, Ekdeep Singh, and Robert P Dick. 2020. “A Gradient Flow\nFramework for Analyzing Network Pruning.” arXiv Preprint\narXiv:2009.11839.\n\n\nLuebke, David. 2008. “CUDA: Scalable\nParallel Programming for High-Performance Scientific Computing.”\nIn 2008 5th IEEE International Symposium on Biomedical Imaging: From\nNano to Macro, 836–38. IEEE. https://doi.org/10.1109/isbi.2008.4541126.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to\nInterpreting Model Predictions.” In Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett,\n4765–74. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html.\n\n\nMa, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore,\nSriram Sankar, and Xun Jiao. 2024. “Dr.\nDNA: Combating Silent Data Corruptions in Deep\nLearning Using Distribution of Neuron Activations.” In\nProceedings of the 29th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Systems,\nVolume 3, 239–52. ACM. https://doi.org/10.1145/3620666.3651349.\n\n\nMaas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi\nJavanmard, Kathryn S. McKinley, and Colin Raffel. 2024. “Combining\nMachine Learning and Lifetime-Based Resource Management for Memory\nAllocation and Beyond.” Commun. ACM 67 (4): 87–96. https://doi.org/10.1145/3611018.\n\n\nMaass, Wolfgang. 1997. “Networks of Spiking Neurons:\nThe Third Generation of Neural Network Models.”\nNeural Networks 10 (9): 1659–71. https://doi.org/10.1016/s0893-6080(97)00011-7.\n\n\nMadry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,\nand Adrian Vladu. 2017. “Towards Deep Learning Models Resistant to\nAdversarial Attacks.” arXiv Preprint arXiv:1706.06083.\n\n\nMahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez\nVicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, and Siva\nKumar Sastry Hari. 2020. “PyTorchFI: A\nRuntime Perturbation Tool for DNNs.” In 2020\n50th Annual IEEE/IFIP International Conference on Dependable Systems and\nNetworks Workshops (DSN-w), 25–31. IEEE; IEEE. https://doi.org/10.1109/dsn-w50199.2020.00014.\n\n\nMahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher,\nSarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael\nB. Sullivan, Timothy Tsai, and Stephen W. Keckler. 2021.\n“Optimizing Selective Protection for CNN\nResilience.” In 2021 IEEE 32nd International Symposium on\nSoftware Reliability Engineering (ISSRE), 127–38. IEEE. https://doi.org/10.1109/issre52982.2021.00025.\n\n\nMahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, and\nGu-Yeon Wei. 2022. “GoldenEye: A\nPlatform for Evaluating Emerging Numerical Data Formats in\nDNN Accelerators.” In 2022 52nd Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks (DSN),\n206–14. IEEE. https://doi.org/10.1109/dsn53405.2022.00031.\n\n\nMarković, Danijela, Alice Mizrahi, Damien Querlioz, and Julie Grollier.\n2020. “Physics for Neuromorphic Computing.” Nature\nReviews Physics 2 (9): 499–510. https://doi.org/10.1038/s42254-020-0208-2.\n\n\nMartin, C. Dianne. 1993. “The Myth of the Awesome Thinking\nMachine.” Commun. ACM 36 (4): 120–33. https://doi.org/10.1145/255950.153587.\n\n\nMarulli, Fiammetta, Stefano Marrone, and Laura Verde. 2022.\n“Sensitivity of Machine Learning Approaches to Fake and Untrusted\nData in Healthcare Domain.” Journal of Sensor and Actuator\nNetworks 11 (2): 21. https://doi.org/10.3390/jsan11020021.\n\n\nMaslej, Nestor, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy,\nKatrina Ligett, Terah Lyons, James Manyika, et al. 2023.\n“Artificial Intelligence Index Report 2023.” ArXiv\nPreprint abs/2310.03715. https://arxiv.org/abs/2310.03715.\n\n\nMattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg\nDiamos, David Kanter, Paulius Micikevicius, et al. 2020a.\n“MLPerf: An Industry Standard Benchmark\nSuite for Machine Learning Performance.” IEEE Micro 40\n(2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\n———, et al. 2020b. “MLPerf: An Industry\nStandard Benchmark Suite for Machine Learning Performance.”\nIEEE Micro 40 (2): 8–16. https://doi.org/10.1109/mm.2020.2974843.\n\n\nMazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan\nManuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021.\n“Multilingual Spoken Words Corpus.” In Thirty-Fifth\nConference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2).\n\n\nMcCarthy, John. 1981. “Epistemological Problems of Artificial\nIntelligence.” In Readings in Artificial Intelligence,\n459–65. Elsevier. https://doi.org/10.1016/b978-0-934613-03-3.50035-0.\n\n\nMcMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise\nAgüera y Arcas. 2017. “Communication-Efficient Learning of Deep\nNetworks from Decentralized Data.” In Proceedings of the 20th\nInternational Conference on Artificial Intelligence and Statistics,\nAISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, edited by\nAarti Singh and Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine\nLearning Research. PMLR. http://proceedings.mlr.press/v54/mcmahan17a.html.\n\n\nMiller, Charlie. 2019. “Lessons Learned from Hacking a\nCar.” IEEE Design &Amp; Test 36 (6): 7–9. https://doi.org/10.1109/mdat.2018.2863106.\n\n\nMiller, Charlie, and Chris Valasek. 2015. “Remote Exploitation of\nan Unaltered Passenger Vehicle.” Black Hat USA 2015 (S\n91): 1–91.\n\n\nMiller, D. A. B. 2000. “Optical Interconnects to Silicon.”\n#IEEE_J_JSTQE# 6 (6): 1312–17. https://doi.org/10.1109/2944.902184.\n\n\nMills, Andrew, and Stephen Le Hunte. 1997. “An Overview of\nSemiconductor Photocatalysis.” J. Photochem. Photobiol.,\nA 108 (1): 1–35. https://doi.org/10.1016/s1010-6030(97)00118-4.\n\n\nMirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang,\nEbrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. “A Graph\nPlacement Methodology for Fast Chip Design.” Nature 594\n(7862): 207–12. https://doi.org/10.1038/s41586-021-03544-w.\n\n\nMishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan\nStosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021.\n“Accelerating Sparse Deep Neural Networks.” CoRR\nabs/2104.08378. https://arxiv.org/abs/2104.08378.\n\n\nMittal, Sparsh, Gaurav Verma, Brajesh Kaushik, and Farooq A. Khanday.\n2021. “A Survey of SRAM-Based in-Memory Computing\nTechniques and Applications.” J. Syst. Architect. 119\n(October): 102276. https://doi.org/10.1016/j.sysarc.2021.102276.\n\n\nModha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos,\nRathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta,\net al. 2023. “Neural Inference at the Frontier of Energy, Space,\nand Time.” Science 382 (6668): 329–35. https://doi.org/10.1126/science.adh1174.\n\n\nMohanram, K., and N. A. Touba. 2003. “Partial Error Masking to\nReduce Soft Error Failure Rate in Logic Circuits.” In\nProceedings. 16th IEEE Symposium on Computer Arithmetic,\n433–40. IEEE; IEEE Comput. Soc. https://doi.org/10.1109/dftvs.2003.1250141.\n\n\nMonyei, Chukwuka G., and Kirsten E. H. Jenkins. 2018. “Electrons\nHave No Identity: Setting Right Misrepresentations in\nGoogle and Apple’s Clean Energy Purchasing.”\nEnergy Research &Amp; Social Science 46 (December): 48–51.\nhttps://doi.org/10.1016/j.erss.2018.06.015.\n\n\nMoshawrab, Mohammad, Mehdi Adda, Abdenour Bouzouane, Hussein Ibrahim,\nand Ali Raad. 2023. “Reviewing Federated Learning Aggregation\nAlgorithms; Strategies, Contributions, Limitations and Future\nPerspectives.” Electronics 12 (10): 2287. https://doi.org/10.3390/electronics12102287.\n\n\nMukherjee, S. S., J. Emer, and S. K. Reinhardt. 2005. “The Soft\nError Problem: An Architectural Perspective.” In\n11th International Symposium on High-Performance Computer\nArchitecture, 243–47. IEEE; IEEE. https://doi.org/10.1109/hpca.2005.37.\n\n\nMunshi, Aaftab. 2009. “The OpenCL\nSpecification.” In 2009 IEEE Hot Chips 21 Symposium\n(HCS), 1–314. IEEE. https://doi.org/10.1109/hotchips.2009.7478342.\n\n\nMusk, Elon et al. 2019. “An Integrated Brain-Machine Interface\nPlatform with Thousands of Channels.” J. Med. Internet\nRes. 21 (10): e16194. https://doi.org/10.2196/16194.\n\n\nMyllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen,\nand Tommi Mikkonen. 2022. “On Misbehaviour and Fault Tolerance in\nMachine Learning Systems.” J. Syst. Software 183\n(January): 111096. https://doi.org/10.1016/j.jss.2021.111096.\n\n\nNakano, Jane. 2021. The Geopolitics of Critical Minerals Supply\nChains. JSTOR.\n\n\nNarayanan, Arvind, and Vitaly Shmatikov. 2006. “How to Break\nAnonymity of the Netflix Prize Dataset.” arXiv Preprint\nCs/0610105.\n\n\nNg, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, and Maggie Shen\nQiao. 2021. “AI Literacy: Definition,\nTeaching, Evaluation and Ethical Issues.” Proceedings of the\nAssociation for Information Science and Technology 58 (1): 504–9.\n\n\nNgo, Richard, Lawrence Chan, and Sören Mindermann. 2022. “The\nAlignment Problem from a Deep Learning Perspective.” ArXiv\nPreprint abs/2209.00626. https://arxiv.org/abs/2209.00626.\n\n\nNguyen, Ngoc-Bao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and\nNgai-Man Cheung. 2023. “Re-Thinking Model Inversion Attacks\nAgainst Deep Neural Networks.” In 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 16384–93. IEEE. https://doi.org/10.1109/cvpr52729.2023.01572.\n\n\nNorrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li,\nJames Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021.\n“The Design Process for Google’s Training Chips:\nTpuv2 and TPUv3.” IEEE Micro\n41 (2): 56–63. https://doi.org/10.1109/mm.2021.3058217.\n\n\nNorthcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021.\n“Pervasive Label Errors in Test Sets Destabilize Machine Learning\nBenchmarks.” arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2103.14749\narXiv-issued DOI via DataCite.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil\nMullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used\nto Manage the Health of Populations.” Science 366\n(6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nOecd. 2023. “A Blueprint for Building National Compute Capacity\nfor Artificial Intelligence.” 350. Organisation for Economic\nCo-Operation; Development (OECD). https://doi.org/10.1787/876367e3-en.\n\n\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael\nPetrov, and Shan Carter. 2020. “Zoom in: An\nIntroduction to Circuits.” Distill 5 (3): e00024–001. https://doi.org/10.23915/distill.00024.001.\n\n\nOliynyk, Daryna, Rudolf Mayer, and Andreas Rauber. 2023. “I Know\nWhat You Trained Last Summer: A Survey on Stealing Machine\nLearning Models and Defences.” ACM Comput. Surv. 55\n(14s): 1–41. https://doi.org/10.1145/3595292.\n\n\nOoko, Samson Otieno, Marvin Muyonga Ogore, Jimmy Nsenga, and Marco\nZennaro. 2021. “TinyML in Africa:\nOpportunities and Challenges.” In 2021 IEEE\nGlobecom Workshops (GC Wkshps), 1–6. IEEE; IEEE. https://doi.org/10.1109/gcwkshps52748.2021.9682107.\n\n\nOprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022.\n“Poisoning Attacks Against Machine Learning: Can\nMachine Learning Be Trustworthy?” Computer 55 (11):\n94–99. https://doi.org/10.1109/mc.2022.3190787.\n\n\nPan, Sinno Jialin, and Qiang Yang. 2010. “A Survey on Transfer\nLearning.” IEEE Trans. Knowl. Data Eng. 22 (10):\n1345–59. https://doi.org/10.1109/tkde.2009.191.\n\n\nPanda, Priyadarshini, Indranil Chakraborty, and Kaushik Roy. 2019.\n“Discretization Based Solutions for Secure Machine Learning\nAgainst Adversarial Attacks.” #IEEE_O_ACC# 7: 70157–68.\nhttps://doi.org/10.1109/access.2019.2919463.\n\n\nPapadimitriou, George, and Dimitris Gizopoulos. 2021.\n“Demystifying the System Vulnerability Stack:\nTransient Fault Effects Across the Layers.” In\n2021 ACM/IEEE 48th Annual International Symposium on Computer\nArchitecture (ISCA), 902–15. IEEE; IEEE. https://doi.org/10.1109/isca52012.2021.00075.\n\n\nPapernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram\nSwami. 2016. “Distillation as a Defense to Adversarial\nPerturbations Against Deep Neural Networks.” In 2016 IEEE\nSymposium on Security and Privacy (SP), 582–97. IEEE; IEEE. https://doi.org/10.1109/sp.2016.41.\n\n\nParrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max\nBartolo, Oana Inel, Juan Ciro, et al. 2023. “Adversarial Nibbler:\nA Data-Centric Challenge for Improving the Safety of\nText-to-Image Models.” ArXiv Preprint abs/2305.14384. https://arxiv.org/abs/2305.14384.\n\n\nPatterson, David A, and John L Hennessy. 2016. Computer Organization\nand Design ARM Edition: The Hardware Software\nInterface. Morgan kaufmann.\n\n\nPatterson, David, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang,\nLluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and\nJeff Dean. 2022. “The Carbon Footprint of Machine Learning\nTraining Will Plateau, Then Shrink.” Computer 55 (7):\n18–28. https://doi.org/10.1109/mc.2022.3148714.\n\n\nPeters, Dorian, Rafael A. Calvo, and Richard M. Ryan. 2018.\n“Designing for Motivation, Engagement and Wellbeing in Digital\nExperience.” Front. Psychol. 9 (May): 797. https://doi.org/10.3389/fpsyg.2018.00797.\n\n\nPhillips, P Jonathon, Carina A Hahn, Peter C Fontana, David A\nBroniatowski, and Mark A Przybocki. 2020. “Four Principles of\nExplainable Artificial Intelligence.” Gaithersburg,\nMaryland 18.\n\n\nPlank, James S. 1997. “A Tutorial on\nReedSolomon Coding for Fault-Tolerance in\nRAID-Like Systems.” Software: Practice and\nExperience 27 (9): 995–1012.\n\n\nPont, Michael J, and Royan HL Ong. 2002. “Using Watchdog Timers to\nImprove the Reliability of Single-Processor Embedded Systems:\nSeven New Patterns and a Case Study.” In\nProceedings of the First Nordic Conference on Pattern Languages of\nPrograms, 159–200. Citeseer.\n\n\nPrakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan\nV. Green, Pete Warden, Tim Ansell, and Vijay Janapa Reddi. 2023.\n“CFU Playground: Full-stack Open-Source Framework for Tiny Machine\nLearning (TinyML) Acceleration on\nFPGAs.” In 2023 IEEE International Symposium on\nPerformance Analysis of Systems and Software (ISPASS). Vol.\nabs/2201.01863. IEEE. https://doi.org/10.1109/ispass57527.2023.00024.\n\n\nPrakash, Shvetank, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete\nWarden, Brian Plancher, and Vijay Janapa Reddi. 2023. “Is\nTinyML Sustainable? Assessing the Environmental Impacts of\nMachine Learning on Microcontrollers.” ArXiv Preprint.\nhttps://arxiv.org/abs/2301.11899.\n\n\nPsoma, Sotiria D., and Chryso Kanthou. 2023. “Wearable Insulin\nBiosensors for Diabetes Management: Advances and\nChallenges.” Biosensors 13 (7): 719. https://doi.org/10.3390/bios13070719.\n\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022.\n“Data Cards: Purposeful and Transparent Dataset\nDocumentation for Responsible AI.” In 2022 ACM\nConference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nPutnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros\nConstantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. “A\nReconfigurable Fabric for Accelerating Large-Scale Datacenter\nServices.” ACM SIGARCH Computer Architecture News 42\n(3): 13–24. https://doi.org/10.1145/2678373.2665678.\n\n\nQi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang,\nand Honggang Zhang. 2021. “An Efficient Pruning Scheme of Deep\nNeural Networks for Internet of Things Applications.” EURASIP\nJournal on Advances in Signal Processing 2021 (1): 31. https://doi.org/10.1186/s13634-021-00744-4.\n\n\nQian, Yu, Xuegong Zhou, Hao Zhou, and Lingli Wang. 2024. “An\nEfficient Reinforcement Learning Based Framework for Exploring Logic\nSynthesis.” ACM Trans. Des. Autom. Electron. Syst. 29\n(2): 1–33. https://doi.org/10.1145/3632174.\n\n\nR. V., Rashmi, and Karthikeyan A. 2018. “Secure Boot of Embedded\nApplications - a Review.” In 2018 Second International\nConference on Electronics, Communication and Aerospace Technology\n(ICECA), 291–98. IEEE. https://doi.org/10.1109/iceca.2018.8474730.\n\n\nRachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler,\nMorgane Ayle, and Stephan Günnemann. 2022. “Winning the Lottery\nAhead of Time: Efficient Early Network Pruning.” In\nInternational Conference on Machine Learning, 18293–309. PMLR.\n\n\nRaina, Rajat, Anand Madhavan, and Andrew Y. Ng. 2009. “Large-Scale\nDeep Unsupervised Learning Using Graphics Processors.” In\nProceedings of the 26th Annual International Conference on Machine\nLearning, edited by Andrea Pohoreckyj Danyluk, Léon Bottou, and\nMichael L. Littman, 382:873–80. ACM International Conference Proceeding\nSeries. ACM. https://doi.org/10.1145/1553374.1553486.\n\n\nRamaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, and Olga Russakovsky.\n2023a. “Overlooked Factors in Concept-Based Explanations:\nDataset Choice, Concept Learnability, and Human\nCapability.” In 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 10932–41. IEEE. https://doi.org/10.1109/cvpr52729.2023.01052.\n\n\nRamaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, and Olga Russakovsky.\n2023b. “UFO: A Unified Method for\nControlling Understandability and Faithfulness Objectives in\nConcept-Based Explanations for CNNs.” ArXiv\nPreprint abs/2303.15632. https://arxiv.org/abs/2303.15632.\n\n\nRamcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed,\nJames Legg, and David P. Hughes. 2017. “Deep Learning for\nImage-Based Cassava Disease Detection.” Front. Plant\nSci. 8 (October): 1852. https://doi.org/10.3389/fpls.2017.01852.\n\n\nRamesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,\nAlec Radford, Mark Chen, and Ilya Sutskever. 2021. “Zero-Shot\nText-to-Image Generation.” In Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, edited by Marina Meila and Tong Zhang,\n139:8821–31. Proceedings of Machine Learning Research. PMLR. http://proceedings.mlr.press/v139/ramesh21a.html.\n\n\nRanganathan, Parthasarathy. 2011. “From Microprocessors to\nNanostores: Rethinking Data-Centric Systems.”\nComputer 44 (1): 39–48. https://doi.org/10.1109/mc.2011.18.\n\n\nRao, Ravi. 2021. “TinyML Unlocks New Possibilities\nfor Sustainable Development Technologies.”\nWww.wevolver.com. https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies.\n\n\nRashid, Layali, Karthik Pattabiraman, and Sathish Gopalakrishnan. 2012.\n“Intermittent Hardware Errors Recovery: Modeling and\nEvaluation.” In 2012 Ninth International Conference on\nQuantitative Evaluation of Systems, 220–29. IEEE; IEEE. https://doi.org/10.1109/qest.2012.37.\n\n\n———. 2015. “Characterizing the Impact of Intermittent Hardware\nFaults on Programs.” IEEE Trans. Reliab. 64 (1):\n297–310. https://doi.org/10.1109/tr.2014.2363152.\n\n\nRatner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and\nChristopher Ré. 2018. “Snorkel MeTaL: Weak\nSupervision for Multi-Task Learning.” In Proceedings of the\nSecond Workshop on Data Management for End-to-End Machine Learning.\nACM. https://doi.org/10.1145/3209889.3209898.\n\n\nReagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu\nLee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. “Ares:\nA Framework for Quantifying the Resilience of Deep Neural\nNetworks.” In 2018 55th ACM/ESDA/IEEE Design Automation\nConference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465834.\n\n\nReagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael\nGelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. “A\nCase for Efficient Accelerator Design Space Exploration via\nBayesian Optimization.” In 2017 IEEE/ACM\nInternational Symposium on Low Power Electronics and Design\n(ISLPED), 1–6. IEEE; IEEE. https://doi.org/10.1109/islped.2017.8009208.\n\n\nReddi, Sashank J., Satyen Kale, and Sanjiv Kumar. 2019. “On the\nConvergence of Adam and Beyond.” arXiv Preprint\narXiv:1904.09237, April. http://arxiv.org/abs/1904.09237v1.\n\n\nReddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson,\nGuenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020.\n“MLPerf Inference Benchmark.” In 2020\nACM/IEEE 47th Annual International Symposium on Computer Architecture\n(ISCA), 446–59. IEEE; IEEE. https://doi.org/10.1109/isca45697.2020.00045.\n\n\nReddi, Vijay Janapa, and Meeta Sharma Gupta. 2013. Resilient\nArchitecture Design for Voltage Variation. Springer International\nPublishing. https://doi.org/10.1007/978-3-031-01739-1.\n\n\nReis, G. A., J. Chang, N. Vachharajani, R. Rangan, and D. I. August.\n2005. “SWIFT: Software Implemented Fault\nTolerance.” In International Symposium on Code Generation and\nOptimization, 243–54. IEEE; IEEE. https://doi.org/10.1109/cgo.2005.34.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016.\n“” Why Should i Trust You?” Explaining\nthe Predictions of Any Classifier.” In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 1135–44.\n\n\nRobbins, Herbert, and Sutton Monro. 1951. “A Stochastic\nApproximation Method.” The Annals of Mathematical\nStatistics 22 (3): 400–407. https://doi.org/10.1214/aoms/1177729586.\n\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and\nBjorn Ommer. 2022. “High-Resolution Image Synthesis with Latent\nDiffusion Models.” In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/cvpr52688.2022.01042.\n\n\nRosa, Gustavo H. de, and João P. Papa. 2021. “A Survey on Text\nGeneration Using Generative Adversarial Networks.” Pattern\nRecogn. 119 (November): 108098. https://doi.org/10.1016/j.patcog.2021.108098.\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and\nRecognizing Automaton Project Para. Cornell Aeronautical\nLaboratory.\n\n\nRoskies, Adina. 2002. “Neuroethics for the New Millenium.”\nNeuron 35 (1): 21–23. https://doi.org/10.1016/s0896-6273(02)00763-8.\n\n\nRuder, Sebastian. 2016. “An Overview of Gradient Descent\nOptimization Algorithms.” ArXiv Preprint abs/1609.04747\n(September). http://arxiv.org/abs/1609.04747v2.\n\n\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning\nModels for High Stakes Decisions and Use Interpretable Models\nInstead.” Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,\nSean Ma, Zhiheng Huang, et al. 2015. “ImageNet Large\nScale Visual Recognition Challenge.” Int. J. Comput.\nVision 115 (3): 211–52. https://doi.org/10.1007/s11263-015-0816-y.\n\n\nRussell, Stuart. 2021. “Human-Compatible Artificial\nIntelligence.” Human-Like Machine Intelligence, 3–23.\n\n\nRyan, Richard M., and Edward L. Deci. 2000. “Self-Determination\nTheory and the Facilitation of Intrinsic Motivation, Social Development,\nand Well-Being.” Am. Psychol. 55 (1): 68–78. https://doi.org/10.1037/0003-066x.55.1.68.\n\n\nSamajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar\nKrishna. 2018. “Scale-Sim: Systolic Cnn Accelerator\nSimulator.” ArXiv Preprint abs/1811.02883. https://arxiv.org/abs/1811.02883.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,\nPraveen Paritosh, and Lora M Aroyo. 2021a.\n““Everyone Wants to Do the Model Work,\nNot the Data Work”: Data Cascades in\nHigh-Stakes AI.” In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems, 1–15.\n\n\n———. 2021b. ““Everyone Wants to Do the\nModel Work, Not the Data Work”: Data Cascades\nin High-Stakes AI.” In Proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSangchoolie, Behrooz, Karthik Pattabiraman, and Johan Karlsson. 2017.\n“One Bit Is (Not) Enough: An Empirical\nStudy of the Impact of Single and Multiple Bit-Flip Errors.” In\n2017 47th Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 97–108. IEEE; IEEE. https://doi.org/10.1109/dsn.2017.30.\n\n\nSchäfer, Mike S. 2023. “The Notorious GPT:\nScience Communication in the Age of Artificial\nIntelligence.” Journal of Science Communication 22 (02):\nY02. https://doi.org/10.22323/2.22020402.\n\n\nSchizas, Nikolaos, Aristeidis Karras, Christos Karras, and Spyros\nSioutas. 2022. “TinyML for Ultra-Low Power\nAI and Large Scale IoT Deployments:\nA Systematic Review.” Future Internet 14\n(12): 363. https://doi.org/10.3390/fi14120363.\n\n\nSchuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker\nMitchell, Prasanna Date, and Bill Kay. 2022. “Opportunities for\nNeuromorphic Computing Algorithms and Applications.” Nature\nComputational Science 2 (1): 10–19. https://doi.org/10.1038/s43588-021-00184-y.\n\n\nSchwartz, Daniel, Jonathan Michael Gomes Selman, Peter Wrege, and\nAndreas Paepcke. 2021. “Deployment of Embedded\nEdge-AI for Wildlife Monitoring in Remote Regions.”\nIn 2021 20th IEEE International Conference on Machine Learning and\nApplications (ICMLA), 1035–42. IEEE; IEEE. https://doi.org/10.1109/icmla52953.2021.00170.\n\n\nSchwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020.\n“Green AI.” Commun. ACM 63 (12):\n54–63. https://doi.org/10.1145/3381831.\n\n\nSegal, Mark, and Kurt Akeley. 1999. “The OpenGL\nGraphics System: A Specification (Version 1.1).”\n\n\nSegura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, and P. W. C.\nPrasad. 2017. “Ethical Implications of User Perceptions of\nWearable Devices.” Sci. Eng. Ethics 24 (1): 1–28. https://doi.org/10.1007/s11948-017-9872-8.\n\n\nSeide, Frank, and Amit Agarwal. 2016. “Cntk: Microsoft’s\nOpen-Source Deep-Learning Toolkit.” In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, 2135–35. ACM. https://doi.org/10.1145/2939672.2945397.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna\nVedantam, Devi Parikh, and Dhruv Batra. 2017.\n“Grad-CAM: Visual Explanations from Deep\nNetworks via Gradient-Based Localization.” In 2017 IEEE\nInternational Conference on Computer Vision (ICCV), 618–26. IEEE.\nhttps://doi.org/10.1109/iccv.2017.74.\n\n\nSeong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers,\nand Hsien-Hsin S. Lee. 2010. “SAFER: Stuck-at-fault Error Recovery for\nMemories.” In 2010 43rd Annual IEEE/ACM International\nSymposium on Microarchitecture, 115–24. IEEE; IEEE. https://doi.org/10.1109/micro.2010.46.\n\n\nSeyedzadeh, Saleh, Farzad Pour Rahimian, Ivan Glesk, and Marc Roper.\n2018. “Machine Learning for Estimation of Building Energy\nConsumption and Performance: A Review.”\nVisualization in Engineering 6 (1): 1–20. https://doi.org/10.1186/s40327-018-0064-7.\n\n\nShalev-Shwartz, Shai, Shaked Shammah, and Amnon Shashua. 2017. “On\na Formal Model of Safe and Scalable Self-Driving Cars.” ArXiv\nPreprint abs/1708.06374. https://arxiv.org/abs/1708.06374.\n\n\nShan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y\nZhao. 2023. “Prompt-Specific Poisoning Attacks on Text-to-Image\nGenerative Models.” ArXiv Preprint abs/2310.13828. https://arxiv.org/abs/2310.13828.\n\n\nShastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H.\nP. Pernice, Harish Bhaskaran, C. D. Wright, and Paul R. Prucnal. 2021.\n“Photonics for Artificial Intelligence and Neuromorphic\nComputing.” Nat. Photonics 15 (2): 102–14. https://doi.org/10.1038/s41566-020-00754-y.\n\n\nSheaffer, Jeremy W, David P Luebke, and Kevin Skadron. 2007. “A\nHardware Redundancy and Recovery Mechanism for Reliable Scientific\nComputation on Graphics Processors.” In Graphics\nHardware, 2007:55–64. Citeseer.\n\n\nShehabi, Arman, Sarah Smith, Dale Sartor, Richard Brown, Magnus Herrlin,\nJonathan Koomey, Eric Masanet, Nathaniel Horner, Inês Azevedo, and\nWilliam Lintner. 2016. “United States Data Center Energy Usage\nReport.”\n\n\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichael W. Mahoney, and Kurt Keutzer. 2020. “Q-BERT:\nHessian Based Ultra Low Precision Quantization of\nBERT.” Proceedings of the AAAI Conference on\nArtificial Intelligence 34 (05): 8815–21. https://doi.org/10.1609/aaai.v34i05.6409.\n\n\nSheng, Victor S., and Jing Zhang. 2019. “Machine Learning with\nCrowdsourcing: A Brief Summary of the Past Research and\nFuture Directions.” Proceedings of the AAAI Conference on\nArtificial Intelligence 33 (01): 9837–43. https://doi.org/10.1609/aaai.v33i01.33019837.\n\n\nShi, Hongrui, and Valentin Radu. 2022. “Data Selection for\nEfficient Model Update in Federated Learning.” In Proceedings\nof the 2nd European Workshop on Machine Learning and Systems,\n72–78. ACM. https://doi.org/10.1145/3517207.3526980.\n\n\nShneiderman, Ben. 2020. “Bridging the Gap Between Ethics and\nPractice: Guidelines for Reliable, Safe, and Trustworthy Human-Centered\nAI Systems.” ACM Trans. Interact. Intell. Syst. 10 (4):\n1–31. https://doi.org/10.1145/3419764.\n\n\n———. 2022. Human-Centered AI. Oxford University\nPress.\n\n\nShokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.\n2017. “Membership Inference Attacks Against Machine Learning\nModels.” In 2017 IEEE Symposium on Security and Privacy\n(SP), 3–18. IEEE; IEEE. https://doi.org/10.1109/sp.2017.41.\n\n\nSiddik, Md Abu Bakar, Arman Shehabi, and Landon Marston. 2021.\n“The Environmental Footprint of Data Centers in the United\nStates.” Environ. Res. Lett. 16 (6): 064017. https://doi.org/10.1088/1748-9326/abfba1.\n\n\nSilvestro, Daniele, Stefano Goria, Thomas Sterner, and Alexandre\nAntonelli. 2022. “Improving Biodiversity Protection Through\nArtificial Intelligence.” Nature Sustainability 5 (5):\n415–24. https://doi.org/10.1038/s41893-022-00851-6.\n\n\nSingh, Narendra, and Oladele A. Ogunseitan. 2022. “Disentangling\nthe Worldwide Web of e-Waste and Climate Change Co-Benefits.”\nCircular Economy 1 (2): 100011. https://doi.org/10.1016/j.cec.2022.100011.\n\n\nSkorobogatov, Sergei. 2009. “Local Heating Attacks on Flash Memory\nDevices.” In 2009 IEEE International Workshop on\nHardware-Oriented Security and Trust, 1–6. IEEE; IEEE. https://doi.org/10.1109/hst.2009.5225028.\n\n\nSkorobogatov, Sergei P, and Ross J Anderson. 2003. “Optical Fault\nInduction Attacks.” In Cryptographic Hardware and Embedded\nSystems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA,\nAugust 1315, 2002 Revised Papers 4, 2–12. Springer.\n\n\nSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin\nWattenberg. 2017. “Smoothgrad: Removing Noise by\nAdding Noise.” ArXiv Preprint abs/1706.03825. https://arxiv.org/abs/1706.03825.\n\n\nSnoek, Jasper, Hugo Larochelle, and Ryan P. Adams. 2012.\n“Practical Bayesian Optimization of Machine Learning\nAlgorithms.” In Advances in Neural Information Processing\nSystems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake\nTahoe, Nevada, United States, edited by Peter L. Bartlett, Fernando\nC. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q.\nWeinberger, 2960–68. https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html.\n\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever,\nand Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent\nNeural Networks from Overfitting.” J. Mach. Learn. Res.\n15 (1): 1929–58. https://doi.org/10.5555/2627435.2670313.\n\n\nStm32L4Q5Ag. 2021. STMicroelectronics.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy\nand Policy Considerations for Deep Learning in NLP.”\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 3645–50. Florence, Italy: Association\nfor Computational Linguistics. https://doi.org/10.18653/v1/p19-1355.\n\n\nSuda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma,\nSarma Vrudhula, Jae-sun Seo, and Yu Cao. 2016.\n“Throughput-Optimized OpenCL-Based FPGA\nAccelerator for Large-Scale Convolutional Neural Networks.” In\nProceedings of the 2016 ACM/SIGDA International Symposium on\nField-Programmable Gate Arrays, 16–25. ACM. https://doi.org/10.1145/2847263.2847276.\n\n\nSudhakar, Soumya, Vivienne Sze, and Sertac Karaman. 2023. “Data\nCenters on Wheels: Emissions from Computing Onboard\nAutonomous Vehicles.” IEEE Micro 43 (1): 29–39. https://doi.org/10.1109/mm.2022.3219803.\n\n\nSze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017.\n“Efficient Processing of Deep Neural Networks: A\nTutorial and Survey.” Proc. IEEE 105 (12): 2295–2329. https://doi.org/10.1109/jproc.2017.2761740.\n\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\nDumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.\n“Intriguing Properties of Neural Networks.” In 2nd\nInternational Conference on Learning Representations, ICLR 2014, Banff,\nAB, Canada, April 14-16, 2014, Conference Track Proceedings, edited\nby Yoshua Bengio and Yann LeCun. http://arxiv.org/abs/1312.6199.\n\n\nTambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa\nReddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020.\n“Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings\nfor Resilient Deep Learning Inference.” In 2020 57th ACM/IEEE\nDesign Automation Conference (DAC), 1–6. IEEE; IEEE. https://doi.org/10.1109/dac18072.2020.9218516.\n\n\nTan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler,\nAndrew Howard, and Quoc V. Le. 2019. “MnasNet: Platform-aware Neural Architecture Search for\nMobile.” In 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2820–28. IEEE. https://doi.org/10.1109/cvpr.2019.00293.\n\n\nTan, Mingxing, and Quoc V. Le. 2023. “Demystifying Deep\nLearning.” Wiley. https://doi.org/10.1002/9781394205639.ch6.\n\n\nTang, Xin, Yichun He, and Jia Liu. 2022. “Soft Bioelectronics for\nCardiac Interfaces.” Biophysics Reviews 3 (1). https://doi.org/10.1063/5.0069516.\n\n\nTang, Xin, Hao Shen, Siyuan Zhao, Na Li, and Jia Liu. 2023.\n“Flexible Braincomputer Interfaces.”\nNature Electronics 6 (2): 109–18. https://doi.org/10.1038/s41928-022-00913-9.\n\n\nTarun, Ayush K, Vikram S Chundawat, Murari Mandal, and Mohan\nKankanhalli. 2022. “Deep Regression Unlearning.” ArXiv\nPreprint abs/2210.08196. https://arxiv.org/abs/2210.08196.\n\n\nTeam, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad\nAlmahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et\nal. 2016. “Theano: A Python Framework for Fast\nComputation of Mathematical Expressions.” https://arxiv.org/abs/1605.02688.\n\n\n“The Ultimate Guide to Deep Learning Model Quantization and\nQuantization-Aware Training.” n.d. https://deci.ai/quantization-and-quantization-aware-training/.\n\n\nThompson, Neil C., Kristjan Greenewald, Keeheon Lee, and Gabriel F.\nManso. 2021. “Deep Learning’s Diminishing Returns:\nThe Cost of Improvement Is Becoming Unsustainable.”\nIEEE Spectr. 58 (10): 50–55. https://doi.org/10.1109/mspec.2021.9563954.\n\n\nTill, Aaron, Andrew L. Rypel, Andrew Bray, and Samuel B. Fey. 2019.\n“Fish Die-Offs Are Concurrent with Thermal Extremes in North\nTemperate Lakes.” Nat. Clim. Change 9 (8): 637–41. https://doi.org/10.1038/s41558-019-0520-y.\n\n\nTirtalistyani, Rose, Murtiningrum Murtiningrum, and Rameshwar S. Kanwar.\n2022. “Indonesia Rice Irrigation System:\nTime for Innovation.” Sustainability 14\n(19): 12477. https://doi.org/10.3390/su141912477.\n\n\nTokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa,\nShunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki\nYamazaki Vincent. 2019. “Chainer: A Deep Learning Framework for\nAccelerating the Research Cycle.” In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery &Amp;\nData Mining, 5:1–6. ACM. https://doi.org/10.1145/3292500.3330756.\n\n\nTramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, and Dan\nBoneh. 2019. “AdVersarial: Perceptual Ad Blocking\nMeets Adversarial Machine Learning.” In Proceedings of the\n2019 ACM SIGSAC Conference on Computer and Communications Security,\n2005–21. ACM. https://doi.org/10.1145/3319535.3354222.\n\n\nTran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022.\n“Pruning Has a Disparate Impact on Model Accuracy.” Adv\nNeural Inf Process Syst 35: 17652–64.\n\n\nTsai, Min-Jen, Ping-Yi Lin, and Ming-En Lee. 2023. “Adversarial\nAttacks on Medical Image Classification.” Cancers 15\n(17): 4228. https://doi.org/10.3390/cancers15174228.\n\n\nTsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa,\nand Stephen W. Keckler. 2021. “NVBitFI:\nDynamic Fault Injection for GPUs.” In\n2021 51st Annual IEEE/IFIP International Conference on Dependable\nSystems and Networks (DSN), 284–91. IEEE; IEEE. https://doi.org/10.1109/dsn48987.2021.00041.\n\n\nUddin, Mueen, and Azizah Abdul Rahman. 2012. “Energy Efficiency\nand Low Carbon Enabler Green IT Framework for Data Centers\nConsidering Green Metrics.” Renewable Sustainable Energy\nRev. 16 (6): 4078–94. https://doi.org/10.1016/j.rser.2012.03.014.\n\n\nUn, and World Economic Forum. 2019. A New Circular Vision for\nElectronics, Time for a Global Reboot. PACE - Platform for\nAccelerating the Circular Economy. https://www3.weforum.org/docs/WEF\\_A\\_New\\_Circular\\_Vision\\_for\\_Electronics.pdf.\n\n\nValenzuela, Christine L, and Pearl Y Wang. 2000. “A Genetic\nAlgorithm for VLSI Floorplanning.” In Parallel\nProblem Solving from Nature PPSN VI: 6th International Conference Paris,\nFrance, September 1820, 2000 Proceedings 6, 671–80.\nSpringer.\n\n\nVan Noorden, Richard. 2016. “ArXiv Preprint Server\nPlans Multimillion-Dollar Overhaul.” Nature 534 (7609):\n602–2. https://doi.org/10.1038/534602a.\n\n\nVangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar,\nRam Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, and\nChris H. Kim. 2021. “Wide-Range Many-Core SoC Design\nin Scaled CMOS: Challenges and\nOpportunities.” IEEE Trans. Very Large Scale Integr. VLSI\nSyst. 29 (5): 843–56. https://doi.org/10.1109/tvlsi.2021.3061649.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Adv Neural Inf Process\nSyst 30.\n\n\n“Vector-Borne Diseases.” n.d.\nhttps://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases.\n\n\nVelazco, Raoul, Gilles Foucard, and Paul Peronnard. 2010.\n“Combining Results of Accelerated Radiation Tests and Fault\nInjections to Predict the Error Rate of an Application Implemented in\nSRAM-Based FPGAs.” IEEE Trans.\nNucl. Sci. 57 (6): 3500–3505. https://doi.org/10.1109/tns.2010.2087355.\n\n\nVerma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay,\nLung-Yen Chen, Bonan Zhang, and Peter Deaville. 2019. “In-Memory\nComputing: Advances and Prospects.” IEEE\nSolid-State Circuits Mag. 11 (3): 43–55. https://doi.org/10.1109/mssc.2019.2922889.\n\n\nVerma, Team Dual_Boot: Swapnil. 2022. “Elephant\nAI.” Hackster.io. https://www.hackster.io/dual\\_boot/elephant-ai-ba71e9.\n\n\nVinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam,\nVirginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans,\nMax Tegmark, and Francesco Fuso Nerini. 2020. “The Role of\nArtificial Intelligence in Achieving the Sustainable Development\nGoals.” Nat. Commun. 11 (1): 1–10. https://doi.org/10.1038/s41467-019-14108-y.\n\n\nVivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar\nFuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021.\n“IntAct: A 96-Core Processor with Six\nChiplets 3D-Stacked on an Active Interposer with\nDistributed Interconnects and Integrated Power Management.”\nIEEE J. Solid-State Circuits 56 (1): 79–97. https://doi.org/10.1109/jssc.2020.3036341.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.\n“Counterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR.”\nSSRN Electronic Journal 31: 841. https://doi.org/10.2139/ssrn.3063289.\n\n\nWald, Peter H., and Jeffrey R. Jones. 1987. “Semiconductor\nManufacturing: An Introduction to Processes and\nHazards.” Am. J. Ind. Med. 11 (2): 203–21. https://doi.org/10.1002/ajim.4700110209.\n\n\nWan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi,\nand Arijit Raychowdhury. 2021. “Analyzing and Improving Fault\nTolerance of Learning-Based Navigation Systems.” In 2021 58th\nACM/IEEE Design Automation Conference (DAC), 841–46. IEEE; IEEE. https://doi.org/10.1109/dac18074.2021.9586116.\n\n\nWan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, and Y Zhu. 2023.\n“Vpp: The Vulnerability-Proportional Protection\nParadigm Towards Reliable Autonomous Machines.” In\nProceedings of the 5th International Workshop on Domain Specific\nSystem Architecture (DOSSA), 1–6.\n\n\nWang, LingFeng, and YaQing Zhan. 2019a. “A Conceptual Peer Review\nModel for arXiv and Other Preprint\nDatabases.” Learn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\n\n———. 2019b. “A Conceptual Peer Review Model for arXiv and Other Preprint Databases.”\nLearn. Publ. 32 (3): 213–19. https://doi.org/10.1002/leap.1229.\n\n\nWang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang,\nYujun Lin, and Song Han. 2020. “APQ:\nJoint Search for Network Architecture, Pruning and\nQuantization Policy.” In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2075–84. IEEE. https://doi.org/10.1109/cvpr42600.2020.00215.\n\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for\nLimited-Vocabulary Speech Recognition.” arXiv Preprint\narXiv:1804.03209.\n\n\nWarden, Pete, and Daniel Situnayake. 2019. Tinyml:\nMachine Learning with Tensorflow Lite on Arduino and\nUltra-Low-Power Microcontrollers. O’Reilly Media.\n\n\nWeik, Martin H. 1955. A Survey of Domestic Electronic Digital\nComputing Systems. Ballistic Research Laboratories.\n\n\nWeiser, Mark. 1991. “The Computer for the 21st Century.”\nSci. Am. 265 (3): 94–104. https://doi.org/10.1038/scientificamerican0991-94.\n\n\nWess, Matthias, Matvey Ivanov, Christoph Unger, and Anvesh Nookala.\n2020. “ANNETTE: Accurate Neural Network\nExecution Time Estimation with Stacked Models.” IEEE. https://doi.org/10.1109/ACCESS.2020.3047259.\n\n\nWiener, Norbert. 1960. “Some Moral and Technical Consequences of\nAutomation: As Machines Learn They May Develop Unforeseen Strategies at\nRates That Baffle Their Programmers.” Science 131\n(3410): 1355–58. https://doi.org/10.1126/science.131.3410.1355.\n\n\nWilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva\nGurumurthi, and David R. Kaeli. 2014. “Calculating Architectural\nVulnerability Factors for Spatial Multi-Bit Transient Faults.” In\n2014 47th Annual IEEE/ACM International Symposium on\nMicroarchitecture, 293–305. IEEE; IEEE. https://doi.org/10.1109/micro.2014.15.\n\n\nWinkler, Harald, Franck Lecocq, Hans Lofgren, Maria Virginia Vilariño,\nSivan Kartha, and Joana Portugal-Pereira. 2022. “Examples of\nShifting Development Pathways: Lessons on How to Enable\nBroader, Deeper, and Faster Climate Action.” Climate\nAction 1 (1). https://doi.org/10.1007/s44168-022-00026-1.\n\n\nWong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu,\nPang-Shiu Chen, Byoungil Lee, Frederick T. Chen, and Ming-Jinn Tsai.\n2012. “MetalOxide\nRRAM.” Proc. IEEE 100 (6): 1951–70. https://doi.org/10.1109/jproc.2012.2190369.\n\n\nWu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, and Yangqing Jia. 2019.\n“FBNet: Hardware-aware\nEfficient ConvNet Design via Differentiable Neural\nArchitecture Search.” In 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 10734–42. IEEE. https://doi.org/10.1109/cvpr.2019.01099.\n\n\nWu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha\nArdalani, Kiwan Maeng, Gloria Chang, et al. 2022. “Sustainable Ai:\nEnvironmental Implications, Challenges and\nOpportunities.” Proceedings of Machine Learning and\nSystems 4: 795–813.\n\n\nWu, Zhang Judd, and Micikevicius Isaev. 2020. “Integer\nQuantization for Deep Learning Inference: Principles and\nEmpirical Evaluation).” ArXiv Preprint. https://arxiv.org/abs/2004.09602.\n\n\nXiao, Seznec Lin, Demouth Wu, and Han. 2022.\n“SmoothQuant: Accurate and Efficient\nPost-Training Quantization for Large Language Models.” ArXiv\nPreprint. https://arxiv.org/abs/2211.10438.\n\n\nXie, Cihang, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and\nQuoc V. Le. 2020. “Adversarial Examples Improve Image\nRecognition.” In 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 816–25. IEEE. https://doi.org/10.1109/cvpr42600.2020.00090.\n\n\nXie, Saining, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He.\n2017. “Aggregated Residual Transformations for Deep Neural\nNetworks.” In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 1492–1500. IEEE. https://doi.org/10.1109/cvpr.2017.634.\n\n\nXinyu, Chen. n.d.\n\n\nXiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei\nCao, Xuegong Zhou, et al. 2021. “MRI-Based Brain\nTumor Segmentation Using FPGA-Accelerated Neural\nNetwork.” BMC Bioinf. 22 (1): 421. https://doi.org/10.1186/s12859-021-04347-6.\n\n\nXiu, Liming. 2019. “Time Moore: Exploiting Moore’s Law from the Perspective of Time.”\nIEEE Solid-State Circuits Mag. 11 (1): 39–55. https://doi.org/10.1109/mssc.2018.2882285.\n\n\nXu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong\nWang, and Hongbin Zha. 2018. “Alternating Multi-Bit Quantization\nfor Recurrent Neural Networks.” In 6th International\nConference on Learning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net. https://openreview.net/forum?id=S19dR9x0b.\n\n\nXu, Hu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes,\nVasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph\nFeichtenhofer. 2023. “Demystifying CLIP Data.”\nArXiv Preprint abs/2309.16671. https://arxiv.org/abs/2309.16671.\n\n\nXu, Ying, Xu Zhong, Antonio Jimeno Yepes, and Jey Han Lau. 2021.\n“Grey-Box Adversarial Attack and Defence for\nSentiment Classification.” arXiv Preprint\narXiv:2103.11576.\n\n\nXu, Zheng, Yanxiang Zhang, Galen Andrew, Christopher A Choquette-Choo,\nPeter Kairouz, H Brendan McMahan, Jesse Rosenstock, and Yuanbo Zhang.\n2023. “Federated Learning of Gboard Language Models with\nDifferential Privacy.” ArXiv Preprint abs/2305.18465. https://arxiv.org/abs/2305.18465.\n\n\nYang, Tien-Ju, Yonghui Xiao, Giovanni Motta, Françoise Beaufays, Rajiv\nMathews, and Mingqing Chen. 2023. “Online Model Compression for\nFederated Learning with Large Models.” In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 1–5. IEEE; IEEE. https://doi.org/10.1109/icassp49357.2023.10097124.\n\n\nYao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric\nTan, Leyuan Wang, et al. 2021. “Hawq-V3: Dyadic\nNeural Network Quantization.” In International Conference on\nMachine Learning, 11875–86. PMLR.\n\n\nYe, Linfeng, and Shayan Mohajer Hamidi. 2021. “Thundernna:\nA White Box Adversarial Attack.” arXiv Preprint\narXiv:2111.12305.\n\n\nYeh, Y. C. 1996. “Triple-Triple Redundant 777 Primary Flight\nComputer.” In 1996 IEEE Aerospace Applications Conference.\nProceedings, 1:293–307. IEEE; IEEE. https://doi.org/10.1109/aero.1996.495891.\n\n\nYik, Jason, Soikat Hasan Ahmed, Zergham Ahmed, Brian Anderson, Andreas\nG. Andreou, Chiara Bartolozzi, Arindam Basu, et al. 2023.\n“NeuroBench: Advancing Neuromorphic\nComputing Through Collaborative, Fair and Representative\nBenchmarking.” https://arxiv.org/abs/2304.04640.\n\n\nYou, Jie, Jae-Won Chung, and Mosharaf Chowdhury. 2023. “Zeus:\nUnderstanding and Optimizing GPU Energy\nConsumption of DNN Training.” In 20th USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 23),\n119–39. Boston, MA: USENIX Association. https://www.usenix.org/conference/nsdi23/presentation/you.\n\n\nYou, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.\n2017. “ImageNet Training in Minutes,” September. http://arxiv.org/abs/1709.05011v10.\n\n\nYoung, Tom, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018.\n“Recent Trends in Deep Learning Based Natural Language Processing\n[Review Article].” IEEE Comput. Intell.\nMag. 13 (3): 55–75. https://doi.org/10.1109/mci.2018.2840738.\n\n\nYu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy\nDavis, Jeff Dean, et al. 2018. “Dynamic Control Flow in\nLarge-Scale Machine Learning.” In Proceedings of the\nThirteenth EuroSys Conference, 265–83. ACM. https://doi.org/10.1145/3190508.3190551.\n\n\nZafrir, Ofir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019.\n“Q8BERT: Quantized 8Bit\nBERT.” In 2019 Fifth Workshop on Energy\nEfficient Machine Learning and Cognitive Computing - NeurIPS Edition\n(EMC2-NIPS), 36–39. IEEE; IEEE. https://doi.org/10.1109/emc2-nips53020.2019.00016.\n\n\nZeiler, Matthew D. 2012. “ADADELTA: An Adaptive Learning Rate\nMethod,” December, 119–49. https://doi.org/10.1002/9781118266502.ch6.\n\n\nZennaro, Marco, Brian Plancher, and V Janapa Reddi. 2022.\n“TinyML: Applied AI for\nDevelopment.” In The UN 7th Multi-Stakeholder Forum on\nScience, Technology and Innovation for the Sustainable Development\nGoals, 2022–05.\n\n\nZhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason\nOptimizing Cong. 2015. “FPGA-Based Accelerator Design\nfor Deep Convolutional Neural Networks Proceedings of the 2015\nACM.” In SIGDA International Symposium on\nField-Programmable Gate Arrays-FPGA, 15:161–70.\n\n\nZhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna\nGoldie, and Azalia Mirhoseini. 2022. “A Full-Stack Search\nTechnique for Domain Optimized Deep Learning Accelerators.” In\nProceedings of the 27th ACM International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems, 27–42. ASPLOS ’22. New York, NY, USA: ACM. https://doi.org/10.1145/3503222.3507767.\n\n\nZhang, Dongxia, Xiaoqing Han, and Chunyu Deng. 2018. “Review on\nthe Research and Practice of Deep Learning and Reinforcement Learning in\nSmart Grids.” CSEE Journal of Power and Energy Systems 4\n(3): 362–70. https://doi.org/10.17775/cseejpes.2018.00520.\n\n\nZhang, Hongyu. 2008. “On the Distribution of Software\nFaults.” IEEE Trans. Software Eng. 34 (2): 301–2. https://doi.org/10.1109/tse.2007.70771.\n\n\nZhang, Jeff Jun, Tianyu Gu, Kanad Basu, and Siddharth Garg. 2018.\n“Analyzing and Mitigating the Impact of Permanent Faults on a\nSystolic Array Based Neural Network Accelerator.” In 2018\nIEEE 36th VLSI Test Symposium (VTS), 1–6. IEEE; IEEE. https://doi.org/10.1109/vts.2018.8368656.\n\n\nZhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. 2018.\n“ThUnderVolt: Enabling Aggressive\nVoltage Underscaling and Timing Error Resilience for Energy Efficient\nDeep Learning Accelerators.” In 2018 55th ACM/ESDA/IEEE\nDesign Automation Conference (DAC), 1–6. IEEE. https://doi.org/10.1109/dac.2018.8465918.\n\n\nZhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu.\n2020. “Fast Hardware-Aware Neural Architecture Search.” In\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW). IEEE. https://doi.org/10.1109/cvprw50498.2020.00354.\n\n\nZhang, Qingxue, Dian Zhou, and Xuan Zeng. 2017. “Highly Wearable\nCuff-Less Blood Pressure and Heart Rate Monitoring with Single-Arm\nElectrocardiogram and Photoplethysmogram Signals.” BioMedical\nEngineering OnLine 16 (1): 23. https://doi.org/10.1186/s12938-017-0317-z.\n\n\nZhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai\nHelen Li, and Yiran Chen. 2020. “AutoShrink:\nA Topology-Aware NAS for Discovering Efficient\nNeural Architecture.” In The Thirty-Fourth AAAI Conference on\nArtificial Intelligence, AAAI 2020, the Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, the Tenth\nAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, 6829–36. AAAI Press.\nhttps://aaai.org/ojs/index.php/AAAI/article/view/6163.\n\n\nZhao, Mark, and G. Edward Suh. 2018. “FPGA-Based\nRemote Power Side-Channel Attacks.” In 2018 IEEE Symposium on\nSecurity and Privacy (SP), 229–44. IEEE; IEEE. https://doi.org/10.1109/sp.2018.00049.\n\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas\nChandra. 2018. “Federated Learning with Non-Iid Data.”\nArXiv Preprint abs/1806.00582. https://arxiv.org/abs/1806.00582.\n\n\nZhou, Bolei, Yiyou Sun, David Bau, and Antonio Torralba. 2018.\n“Interpretable Basis Decomposition for Visual Explanation.”\nIn Proceedings of the European Conference on Computer Vision\n(ECCV), 119–34.\n\n\nZhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat,\nXavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian,\nManuel Le Gallo, and Paul N. Whatmough. 2021.\n“AnalogNets: Ml-hw\nCo-Design of Noise-Robust TinyML Models and Always-on\nAnalog Compute-in-Memory Accelerator.” https://arxiv.org/abs/2111.06503.\n\n\nZhou, Peng, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2018.\n“Learning Rich Features for Image Manipulation Detection.”\nIn 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1053–61. IEEE. https://doi.org/10.1109/cvpr.2018.00116.\n\n\nZhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand\nJayarajan, Amar Phanishayee, Bianca Schroeder, and Gennady Pekhimenko.\n2018. “Benchmarking and Analyzing Deep Neural Network\nTraining.” In 2018 IEEE International Symposium on Workload\nCharacterization (IISWC), 88–100. IEEE; IEEE. https://doi.org/10.1109/iiswc.2018.8573476.\n\n\nZhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang\nGan, and Song Han. 2023. “PockEngine:\nSparse and Efficient Fine-Tuning in a Pocket.” In\n56th Annual IEEE/ACM International Symposium on\nMicroarchitecture. ACM. https://doi.org/10.1145/3613424.3614307.\n\n\nZhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. 2021. “A Comprehensive Survey on\nTransfer Learning.” Proc. IEEE 109 (1): 43–76. https://doi.org/10.1109/jproc.2020.3004555.\n\n\nZoph, Barret, and Quoc V. Le. 2016. “Neural Architecture Search\nwith Reinforcement Learning,” November, 367–92. https://doi.org/10.1002/9781394217519.ch17.",
    "crumbs": [
      "REFERENCES",
      "References"
    ]
  },
  {
    "objectID": "contents/tools.html",
    "href": "contents/tools.html",
    "title": "Appendice A: Tools",
    "section": "",
    "text": "A.1 Hardware Kits",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "contents/tools.html#hardware-kits",
    "href": "contents/tools.html#hardware-kits",
    "title": "Appendice A: Tools",
    "section": "",
    "text": "A.1.1 Microcontrollers and Development Boards\n\n\n\n\n\n\n\n\n\n\nNo\nHardware\nProcessor\nFeatures\nTinyML Compatibility\n\n\n\n\n1\nArduino Nano 33 BLE Sense\nARM Cortex-M4\nOnboard sensors, Bluetooth connectivity\nTensorFlow Lite Micro\n\n\n2\nRaspberry Pi Pico\nDual-core Arm Cortex-M0+\nLow-cost, large community support\nTensorFlow Lite Micro\n\n\n3\nSparkFun Edge\nAmbiq Apollo3 Blue\nUltra-low power consumption, onboard microphone\nTensorFlow Lite Micro\n\n\n4\nAdafruit EdgeBadge\nATSAMD51 32-bit Cortex M4\nCompact size, integrated display and microphone\nTensorFlow Lite Micro\n\n\n5\nGoogle Coral Development Board\nNXP i.MX 8M SOC (quad Cortex-A53, Cortex-M4F)\nEdge TPU, Wi-Fi, Bluetooth\nTensorFlow Lite for Coral\n\n\n6\nSTM32 Discovery Kits\nVarious (e.g., STM32F7, STM32H7)\nDifferent configurations, Cube.AI software support\nSTM32Cube.AI\n\n\n7\nArduino Nicla Vision\nSTM32H747AII6 Dual Arm Cortex M7/M4\nIntegrated camera, low power, compact design\nTensorFlow Lite Micro\n\n\n8\nArduino Nicla Sense ME\n64 MHz Arm Cortex M4 (nRF52832)\nMulti-sensor platform, environment sensing, BLE, Wi-Fi\nTensorFlow Lite Micro\n\n\n9\nXIAO ESP32S3 Sense\nXtensa LX7 dual-core (ESP32-S3R8)\nIntegrated camera, microphone, BLE, Wi-Fi and the most compact design\nTensorFlow Lite Micro",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "contents/tools.html#software-tools",
    "href": "contents/tools.html#software-tools",
    "title": "Appendice A: Tools",
    "section": "A.2 Software Tools",
    "text": "A.2 Software Tools\n\nA.2.1 Machine Learning Frameworks\n\n\n\n\n\n\n\n\n\nNo\nMachine Learning Framework\nDescription\nUse Cases\n\n\n\n\n1\nTensorFlow Lite\nLightweight library for running machine learning models on constrained devices\nImage recognition, voice commands, anomaly detection\n\n\n2\nEdge Impulse\nA platform providing tools for creating machine learning models optimized for edge devices\nData collection, model training, deployment on tiny devices\n\n\n3\nONNX Runtime\nA performance-optimized engine for running ONNX models, fine-tuned for edge devices\nCross-platform deployment of machine learning models\n\n\n\n\n\nA.2.2 Libraries and APIs\n\n\n\n\n\n\n\n\n\nNo\nLibrary/API\nDescription\nUse Cases\n\n\n\n\n1\nCMSIS-NN\nA collection of efficient neural network kernels optimized for Cortex-M processors\nEmbedded vision and AI applications\n\n\n2\nARM NN\nAn inference engine for CPUs, GPUs, and NPUs, enabling the translation of neural network frameworks\nAccelerating machine learning model inference on ARM-based devices",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "contents/tools.html#ides-and-development-environments",
    "href": "contents/tools.html#ides-and-development-environments",
    "title": "Appendice A: Tools",
    "section": "A.3 IDEs and Development Environments",
    "text": "A.3 IDEs and Development Environments\n\n\n\n\n\n\n\n\n\nNo\nIDE/Development Environment\nDescription\nFeatures\n\n\n\n\n1\nPlatformIO\nAn open-source ecosystem for IoT development catering to various boards & platforms\nCross-platform build system, continuous testing, firmware updates\n\n\n2\nEclipse Embedded CDT\nA plugin for Eclipse facilitating embedded systems development\nSupports various compilers and debuggers, integrates with popular build tools\n\n\n3\nArduino IDE\nOfficial development environment for Arduino supporting various boards & languages\nUser-friendly interface, large community support, extensive library collection\n\n\n4\nMbed Studio\nARM’s IDE for developing robust embedded software with Mbed OS\nIntegrated debugger, Mbed OS integration, version control support\n\n\n5\nSegger Embedded Studio\nA powerful IDE for ARM microcontrollers supporting a wide range of development boards\nAdvanced code editor, project management, debugging capabilities",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "contents/zoo_datasets.html",
    "href": "contents/zoo_datasets.html",
    "title": "Appendice B: Datasets",
    "section": "",
    "text": "Google Speech Commands Dataset\n\nDescription: A set of one-second .wav audio files, each containing a single spoken English word.\nLink to the Dataset\n\nVisualWakeWords Dataset\n\nDescription: A dataset tailored for TinyML vision applications, consisting of binary labeled images indicating whether a person is in the image or not.\nLink to the Dataset\n\nEMNIST Dataset\n\nDescription: A dataset containing 28x28 pixel images of handwritten characters and digits, which is an extension of the MNIST dataset but includes letters.\nLink to the Dataset\n\nUCI Machine Learning Repository: Human Activity Recognition Using Smartphones\n\nDescription: A dataset with the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors.\nLink to the Dataset\n\nPlantVillage Dataset\n\nDescription: A dataset comprising of images of healthy and diseased crop leaves categorized based on the crop type and disease type, which could be used in a TinyML agricultural project.\nLink to the Dataset\n\nGesture Recognition using 3D Motion Sensing (3D Gesture Database)\n\nDescription: This dataset contains 3D gesture data recorded using a Leap Motion Controller, which might be useful for gesture recognition projects.\nLink to the Dataset\n\nMultilingual Spoken Words Corpus\n\nDescription: A dataset containing recordings of common spoken words in various languages, useful for speech recognition projects targeting multiple languages.\nLink to the Dataset\n\nWake Vision\n\nDescription: A dataset containing over 6 million images for binary person classification. In addition, it includes a fine-grain benchmark suite for evaluating the fairness and robustness of models.\nLink to the Dataset\n\n\nRemember to verify the dataset’s license or terms of use to ensure it can be used for your intended purpose.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html",
    "href": "contents/learning_resources.html",
    "title": "Appendice D: Resources",
    "section": "",
    "text": "D.1 Books\nHere is a list of recommended books for learning about TinyML or embedded AI:\nThese books cover a range of topics related to TinyML and embedded AI, including:\nIn addition to the above books, there are a number of other resources available for learning about TinyML and embedded AI, including online courses, tutorials, and blog posts. Some of these are listed below. Another great way to learn is by joining the community of embedded AI developers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html#books",
    "href": "contents/learning_resources.html#books",
    "title": "Appendice D: Resources",
    "section": "",
    "text": "TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers by Pete Warden and Daniel Situnayake\nAI at the Edge: Solving Real-World Problems with Embedded Machine Learning by Daniel Situnayake and Jenny Plunkett\nTinyML Cookbook: Combine artificial intelligence and ultra-low-power embedded devices to make the world smarter by Gian Marco Iodice\nIntroduction to TinyML by Rohit Sharma\nIntegrated camera, microphone, BLE, Wi-Fi and the most compact design by Lei Feng(Seeed Studio), Marcelo Rovai\n\n\n\nThe fundamentals of machine learning and TinyML\nHow to choose the right hardware and software for your project\nHow to train and deploy TinyML models on embedded devices\nReal-world examples of TinyML applications",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html#tutorials",
    "href": "contents/learning_resources.html#tutorials",
    "title": "Appendice D: Resources",
    "section": "D.2 Tutorials",
    "text": "D.2 Tutorials",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html#frameworks",
    "href": "contents/learning_resources.html#frameworks",
    "title": "Appendice D: Resources",
    "section": "D.3 Frameworks",
    "text": "D.3 Frameworks\n\nGitHub Description: There are various GitHub repositories dedicated to TinyML where you can contribute or learn from existing projects. Some popular organizations/repos to check out are:\n\n\nTensorFlow Lite Micro: GitHub Repository\nTinyML4D: GitHub Repository\nEdge Impulse Expert Network: Repository\n\n\nStack Overflow Tags: tinyml Description: Use the “tinyml” tag on Stack Overflow to ask technical questions and find answers from the community.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/learning_resources.html#courses-and-learning-platforms",
    "href": "contents/learning_resources.html#courses-and-learning-platforms",
    "title": "Appendice D: Resources",
    "section": "D.4 Courses and Learning Platforms",
    "text": "D.4 Courses and Learning Platforms\n\nCoursera Course: Introduction to Embedded Machine Learning Description: A dedicated course on Coursera to learn the basics and advances of TinyML.\nEdX Course: Intro to TinyML Description: Learn about TinyML with this HarvardX course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "contents/community.html",
    "href": "contents/community.html",
    "title": "Appendice E: Communities",
    "section": "",
    "text": "E.1 Online Forums",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/community.html#online-forums",
    "href": "contents/community.html#online-forums",
    "title": "Appendice E: Communities",
    "section": "",
    "text": "TinyML Forum Website: TinyML Forum Description: A dedicated forum for discussions, news, and updates on TinyML.\nReddit Subreddits: r/TinyML Description: Reddit community discussing various topics related to TinyML.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/community.html#blogs-and-websites",
    "href": "contents/community.html#blogs-and-websites",
    "title": "Appendice E: Communities",
    "section": "E.2 Blogs and Websites",
    "text": "E.2 Blogs and Websites\n\nTinyML Foundation Website: TinyML Foundation Description: The official website offers a wealth of information including research, news, and events.\nEdge Impulse Blog Website: Blog Description: Contains several articles, tutorials, and resources on TinyML.\nTiny Machine Learning Open Education Initiative (TinyMLedu) Website: TinyML Open Education Initiative Description: The website offers links to educational materials on TinyML, training events and research papers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/community.html#social-media-groups",
    "href": "contents/community.html#social-media-groups",
    "title": "Appendice E: Communities",
    "section": "E.3 Social Media Groups",
    "text": "E.3 Social Media Groups\n\nLinkedIn Groups Description: Join TinyML groups on LinkedIn to connect with professionals and enthusiasts in the field.\nTwitter Description: Follow TinyML enthusiasts, organizations, and experts on Twitter for the latest news and updates. Example handles to follow:\n\nTwitter\nEdgeImpulse",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/community.html#conferences-and-meetups",
    "href": "contents/community.html#conferences-and-meetups",
    "title": "Appendice E: Communities",
    "section": "E.4 Conferences and Meetups",
    "text": "E.4 Conferences and Meetups\n\nTinyML Summit Website: TinyML Summit Description: Annual event where professionals and enthusiasts gather to discuss the latest developments in TinyML.\nMeetup Website: Meetup Description: Search for TinyML groups on Meetup to find local or virtual gatherings.\n\nRemember to always check the credibility and activity level of the platforms and groups before diving in to ensure a productive experience.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Communities</span>"
    ]
  },
  {
    "objectID": "contents/case_studies.html",
    "href": "contents/case_studies.html",
    "title": "Appendice F: Case Studies",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nComing soon.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html",
    "title": "Nicla Vision",
    "section": "",
    "text": "Pre-requisites",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#pre-requisites",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#pre-requisites",
    "title": "Nicla Vision",
    "section": "",
    "text": "Nicla Vision Board: Ensure you have the Nicla Vision board.\nUSB Cable: For connecting the board to your computer.\nNetwork: With internet access for downloading necessary software.",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#setup",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#setup",
    "title": "Nicla Vision",
    "section": "Setup",
    "text": "Setup\n\nSetup Nicla Vision",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/arduino/nicla_vision/nicla_vision.html#exercises",
    "href": "contents/labs/arduino/nicla_vision/nicla_vision.html#exercises",
    "title": "Nicla Vision",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "Nicla Vision"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "Pre-requisites",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#pre-requisites",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#pre-requisites",
    "title": "XIAO ESP32S3",
    "section": "",
    "text": "XIAO ESP32S3 Sense Board: Ensure you have the XIAO ESP32S3 Sense Board.\nUSB-C Cable: This is for connecting the board to your computer.\nNetwork: With internet access for downloading necessary software.\nSD Card and an SD card Adapter: This saves audio and images (optional).",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#setup",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#setup",
    "title": "XIAO ESP32S3",
    "section": "Setup",
    "text": "Setup\n\nSetup XIAO ESP32S3",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#exercises",
    "href": "contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html#exercises",
    "title": "XIAO ESP32S3",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nModality\nTask\nDescription\nLink\n\n\n\n\nVision\nImage Classification\nLearn to classify images\nLink\n\n\nVision\nObject Detection\nImplement object detection\nLink\n\n\nSound\nKeyword Spotting\nExplore voice recognition systems\nLink\n\n\nIMU\nMotion Classification and Anomaly Detection\nClassify motion data and detect anomalies\nLink",
    "crumbs": [
      "XIAO ESP32S3"
    ]
  },
  {
    "objectID": "contents/labs/shared/shared.html",
    "href": "contents/labs/shared/shared.html",
    "title": "Shared Labs",
    "section": "",
    "text": "The labs in this section cover topics and techniques that are applicable across different hardware platforms. These labs are designed to be independent of specific boards, allowing you to focus on the fundamental concepts and algorithms used in (tiny) ML applications.\nBy exploring these shared labs, you’ll gain a deeper understanding of the common challenges and solutions in embedded machine learning. The knowledge and skills acquired here will be valuable regardless of the specific hardware you work with in the future.\n\n\n\nExercise\nNicla Vision\nXIAO ESP32S3\n\n\n\n\nKWS Feature Engineering\n✔ Link\n✔ Link\n\n\nDSP Spectral Features Block\n✔ Link\n✔ Link",
    "crumbs": [
      "Shared Labs"
    ]
  }
]